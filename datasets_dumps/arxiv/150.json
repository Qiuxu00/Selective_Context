{"entry_id": "http://arxiv.org/abs/2303.07146v1", "published": "20230313141659", "title": "NeuroQL: A Neuro-Symbolic Language and Dataset for Inter-Subjective Reasoning", "authors": ["Nick Papoulias"], "primary_category": "cs.PL", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.SE"], "text": "\n\nEnhanced Vibrational Stability in Glass Droplets\n    Smarajit Karmakar\n    March 30, 2023\n================================================\n\n\n\n\n  We present a new AI task and baseline solution for Inter-Subjective Reasoning. We define inter-subjective information,  to be a mixture of objective and subjective information possibly shared by different parties. Examples may include commodities and their objective properties as reported by IR (Information Retrieval) systems, that need to be cross-referenced with subjective user reviews from an online forum. For an AI system to successfully reason about both, it needs to be able to combine symbolic reasoning of objective facts with the shared consensus found on subjective user reviews. To this end we introduce the NeuroQL dataset and DSL (Domain-specific Language) as a baseline solution for this problem. NeuroQL is a neuro-symbolic language that extends logical unification with neural primitives for extraction and retrieval. It can function as a target for automatic translation of inter-subjective questions (posed in natural language) into the neuro-symbolic code that can answer them.\n\n \n\n\n\u00a7 INTRODUCTION\n\n\n\nOur digital world comprises of information sources with varying degrees of objectivity and subjectivity. Structured\ninformation stored in databases or other IR systems (such as for the physical properties and prices of products) are presented as objective facts to potential customers, while their accompanying free-form reviews  fall by definition in the subjective part of this spectrum. Yet, as users of these systems we are interested in questions involving both aspects. These are questions where both the authoritative structured data of a product and the unstructured opinions of the general public are pertinent. Consider for example the search query: \n\u201cHow is the bass for headphones at around 30 dollars having minimum 14K reviews that is not discontinued?\u201d which we depict in Fig. <ref>: \n\n \n\nWe call this type of questions inter-subjective given that they involve a mixture of subjective and objective information that may be shared by different parties. Currently, answers to such queries are treated with a semi-manual two stage process by information systems. In the first step the user needs to pinpoint a single product of interest using a query involving only its objective properties and description (e.g a query involving the title, price, number of reviews as in the example above). Then in the second step, the user needs to scan the reviews of this product either manually or with a similarity search in order to find relevant subjective opinions regarding the quality of the product (e.g. the quality of the bass in the above example). With the advent of deep-learning <cit.> for natural-language processing <cit.> and more specifically deep-learning Q&A (Question & Answering) models <cit.>, this second step can now be supplemented with neural retrieval and neural comprehension of reviews. \n\nIn this work we investigate the possibility of automating and merging these two stages that involve both symbolic reasoning (over structured factual information) and neuronal reasoning (over unstructured subjective sources). The problem of interfacing symbolic and neuronal reasoning, is a known open problem in AI literature involving neuro-symbolic systems <cit.>. Our contribution is to propose a new AI task and baseline solution for inter-subjective queries and reasoning, as the one we saw above. \n\nTo this end we introduce the NeuroQL dataset and DSL (Domain-specific Language <cit.>) as a baseline solution for this problem. Our dataset extends previous work on Q&A systems focused on metadata <cit.> and subjective <cit.> information to include inter-subjective questions and their translation into neuro-symbolic queries.  Our queries are expressed in NeuroQL which is a neuro-symbolic language that we embed inside the Python <cit.> runtime environment. This embedded DSL implements and extends logical unification <cit.> with neural primitives for extraction and retrieval. NeuroQL can function as a target for automatic translation of inter-subjective questions (posed in natural language) into the neuro-symbolic code that can answer them, as we show in Figure <ref>: \n\n\n\n\nIn this figure we show our goals and hypotheses regarding NeuroQL. Firstly, we hypothesize  that: \n\n  (H1): It is possible to translate inter-subjective questions (as the one shown on top of Figure <ref>), comprising of different subjective and objective components, into neuro-symbolic queries expressed in NeuroQL. \n\n\nTo test (H1) we experiment with a neural translation solution fine-tuned to this domain, aiming to distinguish not only between objective and subjective components, but also between different kinds of sub-query categories (opinion, title, price, reviews, manufacturing ). These sub-query categories have different translations in NeuroQL (as shown in the bottom part of Figure <ref>) which are highlighted to match their equivalent categories in natural language. \n\nFurthermore, we hypothesize that: \n\n \n  (H2): Symbolic reasoning through unification (covering structured objective facts), similar to the one found in prolog <cit.> and datalog <cit.> systems, can be extended with neuronal reasoning (for unstructured subjective data) in a disciplined manner, producing satisfactory answers for inter-subjective queries.\n  \n\nMeaning, that the neuro-symbolic synthesis can be expressed in a concise syntactic and semantic form for NeuroQL users, while still being able to answer inter-subjective queries in a satisfactory way. To test (H2) we measure recall, em and f1 scores <cit.> of translated NeuroQL queries against previously unseen inter-subjective questions.\n\nThe rest of this paper is organized as follows: Section <ref> presents the results of our effort including: a hands-on walk-through of NeuroQL (sub-section <ref>), a description of the NeuroQL inference pipeline (sub-section <ref>), a description of the NeuroQL dataset (sub-section <ref>), our translation, recall, em and f1 experiments (sub-section <ref>) as well as a further review of related work (sub-section <ref>). Section <ref> discusses the implications of our contribution, possible threats to validity and future perspectives. Finally, Section <ref> concludes the paper with pertinent methodological comments.\n\nAll data and examples from this paper are available at: <https://orgdlabs.com/neuroQL>.\n\n\n\n\u00a7 RESULTS\n \n\n\n\n \u00a7.\u00a7 NeuroQL by Example\n \n\nYou can load the NeuroQL language inside a python environment as the one we provide for our readers at: <https://orgdlabs.com/neuroql_eg> by running the following import statement (shown in line 1 of listing <ref>):\n\nstyle=mystyle2\n[label=lst:import, caption=Importing NeuroQL into Python and loading product properties and reviews]\nfrom NeuroQL import *\nNeuroQL.load('asin_key_properties.csv','asin_reviews.csv')\n\n\nThen as shown in line 2 of listing <ref> you can load data (from csv files) to populate your knowledge base. In this case we load product properties and reviews in line 2, which have the following general format:\n\nstyle=datastyle\n[label=lst:props_eg, caption=Product properties excerpt]\n  ...\nB00001P4ZH,title,koss portapro headphones with case\nB00001P4ZH,price,39.36\nB00001P4ZH,brand,koss\nB00001P4ZH,stars,4.7\nB00001P4ZH,total_reviews,14549\nB00001P4ZH,item_weight,2.79 \nB00001P4ZH,item_weight_units,ounces\nB00001P4ZH,item_model_number,6303157\nB00001P4ZH,is_discontinued_by_manufacturer,no\n  ...\n\n\nThis data consists of a product identifier (like B00001P4ZH), which is called an asin (Amazon Standard Identification Number) in the <amazon.com> product catalog <cit.>. The identifier is followed by the name of a property (such as title, or price) together with that property's value (39.36 in the case of the price property on line 3 above). NeuroQL is able to load arbitrarily nested n-tuples of any length, but for the sake of simplicity we use here a flat representation of the form: object.property = value familiar to programmers from OO paradigms as well as engineers working with triple-based knowledge systems (subject,predicate,object) <cit.>. If we need to load new items explicitly (instead from a pre-saved format) we can invoke the NeuroQL [style=datastyle]fact primitive as follows: [style=datastyle]fact(('B00001P4ZH','price',39.36)). Alternatively we can declare the id: [style=datastyle]B00001P4ZH = ids() and simply state [style=datastyle]B00001P4ZH.price == 39.36. \n\nAfter having loaded the data, we can perform our first simple query using NeuroQL as follows:\n\nstyle=datastyle\n[label=lst:search_id, caption=Retrieving all available product properties by id]\nB00001P4ZH = ids()\n_property, _value = vars()\nsearch(B00001P4ZH._property == _value)\n\n\nHere in listing <ref> we define an id of interest on line 1 and declare our variables of interest on line 2. In this case we would like to retrieve all available properties and their values regarding a particular id. To do so, on line 3 we invoke the search NeuroQL primitive, which takes as input a NeuroQL expression, and returns all possible combinations of values from our knowledge base that satisfy our input expression.  \n\nThus our search results always consist of a list of mappings (such as dictionaries) that bind our unknown variables (seen on listing <ref>) to the values that satisfy our expression:\n\nstyle=datastyle\n[label=lst:search_id_results, caption=Product properties retrieved by id]\n'?property': 'stars', '?value': 4.7,\n'?property': 'item_weight', '?value': 2.79,\n'?property': 'total_reviews', '?value': 14549,\n'?property': 'title', '?value': 'koss portapro ..',\n'?property': 'price', '?value': 39.36,\n'?property': 'review', '?value': '882b1e2745a47...',\n'?property': 'brand', '?value': 'koss',\n'?property': 'is_discontinued_by_manufacturer','?value': 'no',\n'?property': 'item_model_number', '?value': 6303157,\n'?property': 'review', '?value': 'ce76793f036494...',\n'?property': 'item_weight_units', '?value': 'ounces',\n'?property': 'review', '?value': 'd040f2713caa2...'\n\n\nThis type of binding is achieved through symbolic unification <cit.>, which we will be extending later on with a set of neuronal primitives. \n\nContinuing with our examples, NeuroQL also allows for the creation of more complex queries from simpler ones, with additional means of combination as shown in listing <ref>:\n\nstyle=mystyle2\n[label=lst:match_title_price, caption=Searching for headphones with a price around 30$] \n_asin,_total_reviews,_price,_title,\n_review,_review_text,_answers=vars()\n\nsearch(\n    bm25_match(_asin.title==_title,'headphones',80),\n    _asin.price==_price, \n    op_filter(lambda e: abs(e['?price'] - 30) < 10),\n)\n\n\nHere on lines 1 and 2 we define a set of variables that we want to find the bindings of (for this and subsequent examples). Then on lines 4 through 8 we are asking for the ids of headphones whose price is around 30 dollars. To express this query in NeuroQL we form a conjunction of expressions (by passing each expression as a separate argument to our search primitive). The first such expression performs a bm25 <cit.> matching against all product titles, returning the top 80 results (line 5).\nSimilarly, on line 6 we are asking for the _price variable to be bound to the price of the products that we have found thus far. Finally on line 7, we are filtering our results to only include products whose price is within 10 dollars of our target price.  \n\nNotice here, that the id (the _asin), the product _title as well as the product _price, are all variables. These are the variables which we are asking NeuroQL to bound for us. Each binding will correspond to specific values from our knowledge base that jointly satisfy our expressions, getting us the following results (listing <ref>): \n\nstyle=datastyle\n[label=lst:match_title_price_results, caption=Output example for a simple query (showing 2 sample results out of a total of 10)]\n...\n'?asin': 'B00001P4ZH',\n'?title': 'koss portapro headphones with case',\n'?price': 39.36,\n...\n'?asin': 'B000AJIF4E',\n'?title': 'sony mdr7506 professional large diaphragm headphone',\n'?price': 29.99\n...\n\n\nBuilding towards our complete example from Figures <ref> and <ref> we can now ask (listing <ref>) for headphones at around 30 dollars, having minimum 14K reviews, that are not discontinued by the manufacturer:\n\nstyle=mystyle2\n[label=lst:inc_reviews_manufacturing, caption=Refining our search to include total number of reviews and manifacturing details] \nsearch(\n    bm25_match(_asin.title==_title,'headphones',80),\n    _asin.price==_price, \n    op_filter(lambda e: abs(e['?price'] - 30) < 10),\n    _asin.total_reviews==_total_reviews,  \n    op_filter(lambda e: e['?total_reviews'] >= 14000),\n    _asin.is_discontinued_by_manufacturer=='no',\n)\n\n\nThis is achieved by extending our previous query (of listing <ref>) with lines 5 through 7 on listing <ref>. Describing a _total_reviews variable to be bound (for our already retrieved products) on line 5. Filtering the results further depending on the total number of reviews on line 6. Then finally, on line 7 adding the additional constraint that the products we are looking for should not have been discontinued. The results from this extended query are seen in listing <ref>:\n\nstyle=datastyle\n[label=lst:results_inc_reviews_manufacturing, caption=Output example for a refined query (showing all 3 results)]\n'?asin': 'B00001P4ZH',\n'?title': 'koss portapro headphones with case',\n'?price': 39.36,\n'?total_reviews': 14549,\n'?asin': 'B0007XJSQC',\n'?title': 'sennheiser hd201 lightweight over-ear .. headphones',\n'?price': 24.95,\n'?total_reviews': 14980,\n'?asin': 'B000AJIF4E',\n'?title': 'sony mdr7506 professional large diaphragm headphone',\n'?price': 29.99,\n'?total_reviews': 22071\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Neuro-Symbolic Composition\n\n\nUp to this point we have been seeing how NeuroQL can handle the objective components of natural language questions. Now we will extend our previous example (listing <ref>) to handle the entirety of an inter-subjective question as the one we described in Figures <ref> and <ref>. Namely, \u201cHow is the bass for headphones at around 30 dollars having minimum 14K reviews that is not discontinued?\u201d. To do so, we start by extending our previous example on line 8 of listing <ref> where we bind a _review variable to the reviews of the products we have found so far. \n\nThen on line 9 we invoke the neural primitive [style=mystyle2]neural_match, taking the following algorithmic steps to extend classical unification: \n\n\n  * [style=mystyle2]neural_match will receive the sub-query [style=mystyle2]_review.text==_review_text as its first argument. It will use this sub-query to create a set of (id, document) pairs (in this case [style=mystyle2]_review,_review_text pairs) binding the variable [style=mystyle2]_review_text in the process.\n\n  * It will then try to match the bindings of [style=mystyle2]_review_text against the subjective component of the initial query, in this case: [style=mystyle2]'how is the bass?', which is the second argument we passed to our [style=mystyle2]neural_match primitive. With default settings this match will be performed by a DPR (Dense Passage Retriever) <cit.>, using question & context encoders trained with the Natural Questions dataset <cit.>. \n\n  * Finally, [style=mystyle2]neural_match will filter all query bindings thus far (up to line 11 of listing <ref>) to include only the top 5 results or our DPR [style=mystyle2]_review_text match (using the third argument on line 10).\n\n\n \nstyle=mystyle2\n[label=lst:full-eg, caption=Creating an inter-subjective query by filtering and scanning reviews to answer a particular question] \nsearch(\n    bm25_match(_asin.title==_title,'headphones',80),\n    _asin.price==_price, \n    op_filter(lambda e: abs(e['?price'] - 30) < 10),\n    _asin.total_reviews==_total_reviews,  \n    op_filter(lambda e: e['?total_reviews'] >= 14000),\n    _asin.is_discontinued_by_manufacturer=='no',\n    _asin.review==_review, \n    neural_match(\n      _review.text==_review_text,'how is the bass?',5\n    ),\n    neural_extract(\n      _answers, _review.text==_review_text,'how is the bass?',2\n    )\n)\n\n\nThus far we have the top 5 reviews that match our question for the exact subset of products satisfying our objective constraints (up to line 11). We can now proceed to extract relevant opinions for our inter-subjective question using the [style=mystyle2]neural_extract primitive (lines 12 to 14 of listing <ref>), as follows: \n\n\n  \n  * [style=mystyle2]neural_extract will first receive the name of a new variable to bind (in this case the variable [style=mystyle2]_answers) with the extracted text that matches its targeted question.\n  \n  * It will then use the sub-query [style=mystyle2]_review.text==_review_text passed as a second argument, to create a new set of (ids, documents) matching the sub-query (as we did before).\n  \n  * Then with the third argument (the subjective sub-component [style=mystyle2]'how is the bass?') it will try to extract the answer to this question from the [style=mystyle2]_review_text documents. It will do so using a Reader model, such as MiniLM <cit.> initially trained on the SQuAD 2.0 dataset <cit.> and further fine-tuned on the NeuroQL training set to improve its performance (as we describe in Section <ref>).\n  \n  * Finally, [style=mystyle2]neural_extract will filter all query bindings to include only the top 2 results extracted by the Reader (using the forth argument on line 13).\n\n\n\n\nThe final results of our inter-subjective query are seen in listing <ref> with a sample of the final variable bindings. These now include both objective (title, price, total_reviews, manufacturing) and subjective (review, answers) components that jointly satisfied our query's constraints. As an example the most pertinent opinion for product [style=datastyle]B000AJIF4E (satisfying our constraints regarding title, price, total_reviews, manufacturing )  is that the [style=datastyle]'Bass is amazing', while for product [style=datastyle]B00001P4ZH is that the [style=datastyle]'Bass is weak as expected'.\n\nstyle=datastyle\n[label=lst:full-eg-results, caption=Output example for an inter-subjective query]\n'?asin': 'B000AJIF4E',\n'?title': 'sony mdr7506 professional large diaphragm headphone',\n'?price': 29.99,\n'?total_reviews': 22071,\n'?review': '5e96b0052898fe667cf622888fc5af69',\n...\n'?answers': 'answer': 'Bass is amazing',...\n...\n'?asin': 'B00001P4ZH',\n'?title': 'koss portapro headphones with case',\n'?price': 39.36,\n'?total_reviews': 14549,\n'?review': 'd040f2713caa2aff0ce95affb40e12c2',\n...\n'?answers': 'answer': 'Bass is weak as expected',...\n\n\nWe complete our examples by showing how to define and use inference rules in NeuroQL [A formal model of the syntax and semantics of inference rules in NeuroQL will be part of a follow up paper.]. Lines 1 through 6 of listing <ref> define an inference rule using the primitive [style=mystyle2]rule. The first argument on line 1 is the sub-query ([style=mystyle2]_asin.well_ranked == True) that defines the conclusion that the rule can infer if the rest of the arguments are satisfied. The rest of the arguments being sub-queries themselves forming a conjunction.\n\nWhenever the head of this rule unifies with a sub-query that we are currently looking to bind (like the sub-query on line 11 of our [style=mystyle2]search primitive) the rule will be tested. This means that there will be a nested search that will try to satisfy the rule's conjunction given the current bindings and return new bindings if needed. \n\nstyle=mystyle2\n[label=lst:rull-eg, caption=Incorporating a rule to infer well ranked products during search] \nrule(_asin.well_ranked == True,\n  _asin.total_reviews==_total_reviews,\n  op_filter(lambda e: e['?total_reviews'] >= 20000),\n  _asin.stars==_stars,\n  op_filter(lambda e: e['?stars'] >= 4.0)\n)\nsearch(\n    bm25_match(_asin.title==_title,'headphones',80),\n    _asin.price==_price, \n    op_filter(lambda e: abs(e['?price'] - 30) < 10),\n    _asin.well_ranked == True,\n    _asin.is_discontinued_by_manufacturer=='no',\n    _asin.review==_review, \n    neural_match(\n      _review.text==_review_text,'how is the bass?',5\n    ),\n    neural_extract(\n      _answers, _review.text==_review_text,'how is the bass?',2\n    )\n)\n\n\nIn plain english this rule states that a product is well ranked if it has at least 20K reviews and a rating of at least 4.0 stars. By using this rule on line 11 of listing <ref> instead of our previous less strict criteria (on lines 5 and 6 of listing <ref>) we get a more constraint result (seen on listing <ref>).\n\nstyle=datastyle\n[label=lst:rule-eg-results, caption=Inter-subjective query results using rules for well ranked products during inference]\n...\n'?asin': 'B000AJIF4E',\n'?title': 'sony mdr7506 professional large diaphragm headphone',\n'?price': 29.99,\n'?total_reviews': 22071,\n'?review': '5e96b0052898fe667cf622888fc5af69',\n...\n'?answers': 'answer': 'Bass is amazing',...\n\n\n\n\n\n \u00a7.\u00a7 The NeuroQL Architecture\n \n\n\n\nOur two main hypotheses (as detailed in Section <ref>) are that (H1) it is possible to automatically translate inter-subjective questions from natural language into NeuroQL and (H2) that NeuroQL can concisely extend unification with neural reasoning to produce satisfactory answers for inter-subjective questions. To test (H1) and (H2) we devised the following architecture (seen in Figure <ref>) that serves as a baseline solution for our inter-subjective Q&A task. Starting at the top left corner of Figure <ref> we see a user submitting an inter-subjective question in natural language [black]A. This question during inference [orange]B2 will be passed to a translation model (in our case a fine-tuned CodeT5 model <cit.>) that will attempt to translate the question into a NeuroQL query [orange]C. The translation model is trained [orange]B1 for this downstream task using the NeuroQL question/query dataset (detailed in Sections <ref> and <ref>). Subsequently, our architecture will attempt to execute this query [blue]D which (as we saw in Section <ref>) means finding all possible bindings within the NeuroQL database that satisfies the query's main conjunction. The NeuroQL database itself is an in-memory n-tuple store upon which our extended unification algorithm is applied. These extensions include the [style=mystyle2]neural_match [orange]E and [style=mystyle2]neural_extract [orange]F2 primitives. These primitives are based on a retriever model (in our case a DPR <cit.> trained with the Natural Questions dataset <cit.>) and a reader model (a MiniLM <cit.> initially trained on the SQuAD 2.0 dataset <cit.>). Using these models our unification engine will attempt to further bind and filter the neuronal sub-queries of a logical conjunction (as we saw in Section <ref>). During training our reader has been further fine-tuned [orange]F1 on the NeuroQL training set (using our review/answer pairs) to improve its performance (as we further describe in Sections <ref> and <ref>). Finally, when all possible bindings have been found [blue]G an answer [black]H will be returned to our user.\n\nListings <ref> and <ref> show step by step how we can first translate a natural language question into NeuroQL (lines 1 to 3 of listing <ref>) invoking our primitive [style=mystyle2]NeuroQL.translate. Then, how to dynamically evaluate the resulting query (in line 1 of listing <ref>) to get our results (using [style=mystyle2]NeuroQL.eval). This is done with both cases matching our previous results from listings <ref> and <ref>. Finally, listing <ref> show us how both translation and evaluation can be expressed in a single step (line 1 of listing <ref>) by simply invoking our primitive [style=mystyle2]NeuroQL.answer.\n\n\n\nstyle=mystyle2\n[label=lst:translate, caption=Translating an inter-subjective question into a NeuroQL query] \n  query = NeuroQL.translate(\n    'How is the bass for headphones at around [...] ?'\n  )\n  print(query)\n\n  Output: #########################################\n  search(\n    bm25_match(_asin.title==_title,'headphones',80),\n    _asin.price == _price, \n    op_filter(lambda e: abs(e['?price'] - 30) < 10),\n    _asin.total_reviews == _total_reviews, \n    op_filter(lambda e: e['?total_reviews'] >= 14000),\n    _asin.is_discontinued_by_manufacturer=='no',\n    _asin.review == _review, \n    neural_match(\n      _review.text == _review_text,'how is the bass?',5\n    ),\n    neural_extract(\n      _answers, _review.text==_review_text,'how is the bass?',2\n    )\n  )\n  ###################################################\n \n\nstyle=datastyle\n[label=lst:dynEval, caption=Dynamically evaluating a generated NeuroQL query]] \n  NeuroQL.eval(query)\n\n  Output: #########################################\n'?asin': 'B000AJIF4E',\n'?title': 'sony mdr7506 professional large diaphragm headphone',\n'?price': 29.99,\n'?total_reviews': 22071,\n'?review': '5e96b0052898fe667cf622888fc5af69',\n...\n'?answers': 'answer': 'Bass is amazing',...\n...\n'?asin': 'B00001P4ZH',\n'?title': 'koss portapro headphones with case',\n'?price': 39.36,\n'?total_reviews': 14549,\n'?review': 'd040f2713caa2aff0ce95affb40e12c2',\n...\n'?answers': 'answer': 'Bass is weak as expected',...\n  ###################################################\n\n\nstyle=datastyle\n[label=lst:answer, caption=Combining translation and evaluation of inter-subjective questions in a single call] \n  NeuroQL.answer(\n    'How is the bass for headphones at around [...] ?'\n  )\n  \n  Output: #########################################\n'?asin': 'B000AJIF4E',\n'?title': 'sony mdr7506 professional large diaphragm headphone',\n'?price': 29.99,\n'?total_reviews': 22071,\n'?review': '5e96b0052898fe667cf622888fc5af69',\n...\n'?answers': 'answer': 'Bass is amazing',...\n...\n'?asin': 'B00001P4ZH',\n'?title': 'koss portapro headphones with case',\n'?price': 39.36,\n'?total_reviews': 14549,\n'?review': 'd040f2713caa2aff0ce95affb40e12c2',\n...\n'?answers': 'answer': 'Bass is weak as expected',...\n  ###################################################\n\n\n\n\n\n \u00a7.\u00a7 The NeuroQL Dataset\n   The NeuroQL dataset extends previous work on Q&A systems focused on metadata <cit.> and subjective <cit.> information to include 1505 inter-subjective questions and their translation into neuro-symbolic queries (in listing <ref>). These (question, query) pairs are coupled with a detailed knowledge base of 4250 properties (in listing <ref>) for 500 different products, including 1583 reviews and 1627 ground truths for Q&A extraction. \n\nstyle=datastyle\n[label=lst:qq-eg, caption=A (question, query) pair from the NeuroQL dataset]\nB00001P4ZH,question,0514ee34 ...\n0514ee34 ..., text,For headphones model number 6303157 ...\n0514ee34 ..., query,\"search(\n  bm25_match( ...\n  ...\n  neural_match( ...\n  neural_extract( ... \n)\"\n \n\nOn Table <ref> we list all the categories of objective properties that we included in our dataset with a short description for each. Each category is followed by either its domain or a sample value for reference. Finally on Figure <ref> we present a breakdown of our dataset in terms of questions involving a specific objective property (on the left of Figure <ref>). On the right (similarly to <cit.>) we give statistics regarding the first word of the subjective component present in our inter-subjective questions.\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Experiments and Validation\n \n\nThere are two distinct experimental tasks that the NeuroQL dataset makes possible. The first is the NeuroQL Translation Task where the goal is to translate inter-subjective questions posed in natural language into the neuro-symbolic code that can answer them. The task takes into consideration (question, query) pairs and can be evaluated using metrics such as sacreBleu <cit.> and four-gram or tri-gram precision, common in machine translation tasks <cit.>.\n\nThe second task is the NeuroQL Query Task where the goal is to fine-tune neural primitives such as the [style=mystyle2]neural_extract sub-query to produce as many accurate results as possible. The task takes into consideration (query, answers) pairs and can be evaluated using metrics such as recall, em and f1 common in Q&A extraction tasks <cit.>.\n\nFor our NeuroQL Translation Task we fine-tune a CodeT5 model <cit.> for 20 epochs aiming to translate inter-subjective questions into NeuroQL code.  We use a maximum input/output length of 512 tokens that fits our (question,query) pairs, a batch size of 64 and an 1e-4 learning rate with a linear scheduler and an AdamW optimizer <cit.>. As we can see in the the left part of Figure <ref>, the model is significantly improving up until epoch 11 (without over-fitting), after which point the returns are diminishing leading to a near perfect sacreBleu score at epoch 20 for both training and validation sets. The test set evaluation after epoch 20 confirms our model's performance. We split our dataset between training (80%), validation (10%) and test (10%) sets using unique product ids for each slice to ensure no leakage. The four-gram and tri-gram precisions reported on the right side of Figure <ref> follow our sacreBleu observations as expected. Given these results our initial hypothesis (H1) regarding the feasibility of translating inter-subjective questions into neuro-symbolic queries has been validated.\n\n\n\n\nFor our NeuroQL Query Task, we first measure the impact our DPR <cit.> model trained with the Natural Questions dataset <cit.>) has on our test set, for different values of top_k results returned by the retriever.\nWe then fine-tune our reader model (a MiniLM <cit.> initially trained on the SQuAD 2.0 dataset <cit.>) to improve its performance for different values of top_k results returned by the reader. On the left part of Figure <ref> we can see that our DPR model performs quite well at 70% recall even in the case of only the top_2 results returned. Yet, it does not surpass the 90% mark before top_13 results are returned, reaching a 94% recall score at top_20. Using this last recall score from our retriever, we then evaluate our fine-tuned reader model, observing as expected an increase at em and f1 scores as top_k increases. Yet, the best values observed were 0.20 for em and 0.33 for f1. These were both reached at top_8 results returned, being comparable to scores for extractive Q&A on user reviews reported by <cit.>, using the same model. \n\n\n\nPrior to this evaluation we fine-tuned our MiniLM <cit.> reader model for 3 epochs in order to improve its extraction accuracy. We used a 384-token sequence length with a 128-token document stride and a batch size of 16, including the ability to return no answers when predicting results. We set the learning rate at 1e-5 with a 0.2 warmup, using the same dataset split as before. Given the above results our initial hypothesis (H2) regarding the feasibility of extending unification with neural primitives for answering inter-subjective questions has only been partially validated. Neuro-symbolic unification as presented in this work, can indeed be used to execute inter-subjective neuro-symbolic queries. Yet these results present only a first baseline. There is more work needed to improve the accuracy of extracted answers, particularly regarding the [style=mystyle2]neural_extract primitive.\n\n\n\n\n\n \n\n\n \u00a7.\u00a7 Related Work\n \n\nThere is a significant body of work dedicated to translating natural language questions to queries for established symbolic languages such as sql <cit.> and sparql <cit.>. These solutions have only one neuronal component for translation, going from: neural translation \u22b3  symbolic code \u22b3  symbolic reasoning. Our contribution takes a step further by translating natural language into neuro-symbolic code, closing the loop between symbolic and neuronal reasoning: neural translation \u22b3  neuro-symbolic code \u22b3 neuro-symbolic reasoning. \n\nDeep-prolog <cit.> and TensorLog <cit.> are two other neuro-symbolic and logic-oriented programming languages that aim to integrate neuronal and symbolic reasoning. Both rely on probabilistic logic and thus try to integrate neural primitives as predicates with probabilities that can be parameterized by neural networks. Similarly, Scallop <cit.> takes a weighted graph approach for knowledge graphs, where the weights between entities are learned. Finally, SQLFlow <cit.> focuses not on execution but on orchestration (using Kubernetes) for ML workloads using an SQL-like language. In our case the integration between neuronal and symbolic reasoning targets execution and occurs at a higher semantic level than probabilistic approaches. In NeuroQL reasoning tasks such as neuronal extraction and matching are expressed as filtering and binding operations for unification <cit.>. This allows us to create a concise neuro-symbolic query language that can act as a target for automatic translation of natural language questions.\n\nA number of DSLs for neuro-symbolic programming <cit.> target specific domains such as symbolic regression <cit.>, behavior analysis <cit.> and program synthesis <cit.>. Ours is the first to address the problem of inter-subjective reasoning using neural translation, extending prior work on metadata <cit.> and subjectivity <cit.>. \n\nFinally, our work is related to the wider domain of retrieval augmented  <cit.> and tool augmented <cit.> extraction and generation of answers. In our case though we proposed a rich intermediate representation for our task that takes the form of neuro-symbolic code. This allowed us to integrate neuronal and symbolic reasoning in an interpretable and explicit way.   \n\n\n\n\u00a7 DISCUSSION\n \n\nWe believe that the translation of natural language into neuro-symbolic instead of simply symbolic code is a step forward in the direction of closing the loop between neuronal and symbolic reasoning. The tasks, dataset and DSL for inter-subjective reasoning that we have introduced in this work provide a case-study close to real-world needs with clear neuro-symbolic characteristics. As such we believe it can be used as a baseline to evaluate future work.\n\nOur results regarding hypothesis (H1) for the NeuroQL Translation Task, were surprising. For this task the network needs to learn not only to distinguish between objective and subjective components, but also between different kinds of sub-query categories (opinion, title, price, reviews, manufacturing ) as shown in Figure <ref>. Given the near perfect sacreBleu score for this task, we believe that the close-world assumption (the fact that all queries concerned products and their objective or subjective properties) needs to be relaxed in future iterations of the dataset. For example we can consider extending the dataset with sufficiently different domains of inquiry, such as for news articles and their commentary. Moreover, automatically paraphrasing <cit.> our questions to create a larger more diverse dataset is a promising direction for future work. \n\nOur results regarding hypothesis (H2) for the NeuroQL Query Task were closer to our expectations given that there were comparable to a simpler extractive Q&A task on user reviews reported by <cit.>. While these results can serve as a baseline, there is clearly more work needed to improve accuracy.\n\nRegarding possible threats to validity, we note that the language used in parts of our questions comes directly from real-world usage and is unedited. This at first glance is a strength of the dataset (models need to learn to adapt to real-world noise). Nevertheless, examples where there are syntactic or semantic mistakes (from non-native speakers) can also affect the quality of labeling, making it harder to pinpoint valid answers. Here also, an automatic paraphrasing <cit.> and auto-correction approach might help us control for and measure the impact of linguistic coherence in our dataset.\n\n\n\n\u00a7 METHODS\n \n\n\n\n  \nDomain-Specific Language: We implemented NeuroQL as a DSL <cit.> using reflection and meta-programming <cit.>, allowing us to change the normal semantics of Python only for NeuroQL expressions (leaving the rest of the runtime unchanged). These changes include (i) the redefinition of variable declaration (when using the [style=mystyle2]vars() and [style=mystyle2]ids() primitives) (ii) the redefinition of the dot, assignment and equality operators for NeuroQL variables and NeuroQL ids. These changes make NeuroQL an embedded pidgin language according to the categorization of DSLs provided in <cit.>.\n\n\n\n  \nUnification: Our base unification algorithm <cit.> is implemented as a generalization of pattern matching <cit.>, where both the pattern and the target may contain variables. The generalization receives a set of bindings as input (usually referred to as frames) and returns possible augmentations that contain matched values of \u2013 as yet \u2013 unresolved variables. By allowing for sub-queries to be matched either jointly or in parallel we can implement logical operations such as conjunction and disjunction. Unification with inference rules can then be achieved through nested pattern matching that tries to satisfy a rule's body given the current bindings. \n\n\n\n  \nBM25: We use a slight variation of the BM25 (Best Match 25) algorithm in our work to compensate for the most common cases of the \u201cexact overlapp\u201d problem of the initial algorithm <cit.>. The BM25 is itself an improvement over TF-IDF (Term Frequency-Inverse Document Frequency) <cit.>, with both algorithms being used as ranking methods for sparse retrievers. In our case we pre-process both the documents and the query to be matched by tokenizing and stemming the inputs. We don't provide a further solution for synonyms using BM25, since in this case the more accurate DPR primitive can be used (see below). The two methods (BM25 and DPR) provide a trade-off for our users between speed and accuracy, with BM25 being the fastest and DPR being the most accurate.\n\n\n\n  \nDense Passage Retrieval: DPR (Dense Passage Retrieval) is a method that uses dense text embeddings for both query and documents that need to be matched. It is based on a BERT bi-encoder architecture that computes a dot product similarity between a document and a query. Our DPR is based on <cit.>, using question & context encoders trained with the Natural Questions dataset <cit.>. This DPR is then used as a backend for of our [style=mystyle2]neural_match primitive. The [style=mystyle2]neural_match primitive receives a sub-query whose results are used to create a set of (id, document) pairs. It then tries to match the documents against a target query text, using our DPR to return the top_k results that matched (see full example of [style=mystyle2]neural_match in Section <ref>). \n  \n\n\n\n  \nReader Model: \nA Reader or reading comprehension model is a neural network that can perform extractive Q&A by returning relevant text intervals of documents. In our work we use the MiniLM <cit.> model initially trained on the SQuAD 2.0 dataset <cit.> and further fine-tuned on the NeuroQL training set to improve its performance (as we describe in Section <ref>). This Reader is then used as a backend for of our [style=mystyle2]neural_extract primitive. The [style=mystyle2]neural_extract primitive receives the name of a new variable to bind for extracted answers, a query to create (id, document) pairs and finally a target query text. It then tries to extract relevant text intervals from our documents using the Reader to return the top_k results found. We fine-tuned our reader model for 3 epochs, using a 384-token sequence length with a 128-token document stride. We used a batch size of 16, learning rate of 1e-5 with a 0.2 warmup and included the ability to return no answers when predicting results.\n\n\n\n  \nTranslation Model: A translation model is a sequence-to-sequence neural network trained over pairs of input and target sequences. In our case we fine-tuned a CodeT5 model <cit.>) using the NeuroQL question/query dataset in order to translate inter-subjective questions posed in natural language into the NeuroQL query that can answer them (as detailed in Sections <ref> and <ref>). This translation model is then used as a backend for our [style=mystyle2]NeuroQL.translate and [style=mystyle2]NeuroQL.answer primitives, which as their name suggests can translate questions into NeuroQL code and attempt to answer them.\nOur translation model was fine-tuned for 20 epochs with a maximum input/output length of 512 tokens, a batch size of 64 and an 1e-4 learning rate using a linear scheduler and an AdamW optimizer <cit.>.\n \n\n\n  \nSacreBleu, Recall, EM & F1 Scores: In machine translation tasks the sacreBleu <cit.> score is used for the evaluation of generated translations focusing on reproducibility and comparability of reported results. It is itself a standardization of the Bleu <cit.> score that compares the n-grams of the generated sequences to those of the reference translations. In our work we use sacreBleu to evaluate the  quality of our translation model (as detailed in Section <ref>). Recall: is a metric used to evaluate retrieval methods such as our [style=mystyle2]neural_match primitive that is using a DPR model. Recall represents the percentage of relevant documents retrieved among the top_k results returned by a retriever (as we report in Section <ref>). EM & F1 Scores: are used to evaluate Q&A extraction methods such as our [style=mystyle2]neural_extract primitive that is using a Reader model. EM represents the percentage of exact extracted matches, while F1 measures the harmonic mean of precision and recall (reported in Section <ref>).\n\n\n\n\n\u00a7 CODE & DATA AVAILABILITY\n \n\nAll data and examples from this paper are available at: <https://orgdlabs.com/neuroQL>\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}