{"entry_id": "http://arxiv.org/abs/2303.07212v1", "published": "20230313154711", "title": "Distributed Non-Bayesian Learning for Games with Incomplete Information", "authors": ["Shijie Huang", "Jinlong Lei", "Yiguang Hong"], "primary_category": "math.OC", "categories": ["math.OC"], "text": "\n\nParallel Vertex Diffusion for Unified Visual Grounding\n    \nZesen Cheng^1\u00a0Corresponding Author.   Kehan Li^1   Peng Jin^1   Xiangyang Ji^3\n\nLi Yuan^1,2   Chang Liu^3\u00a0   Jie Chen^1,2\u00a0^1 School of Electronic and Computer Engineering, Peking University \n\n^2 Peng Cheng Laboratory   ^3 Tsinghua University   \n\n\n\n    March 30, 2023\n============================================================================================================================================================================================================================================================\n\n\n\n\tWe consider distributed learning problem in games with an unknown cost-relevant parameter, and aim to find the Nash equilibrium while learning the true parameter. Inspired by the social learning literature, we propose a distributed non-Bayesian rule to learn the parameter (each agent maintains a belief on the parameter and updates the belief according to the received noisy cost), combined with best response dynamics for strategy update. The difficulty of the analysis lies mainly in the fact that the parameter learning process and strategy update process are coupled. We first prove that agents' beliefs converge to a common belief and the strategy profiles converge to a Nash equilibrium under this common belief. On this basis, we further show that the beliefs eventually concentrate on the true parameter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nGame model has attracted much research attention due to its wide applications in smart grid <cit.>, wireless communication network <cit.>, traffic control <cit.> and machine learning <cit.>. Equilibrium learning is a promising topic in game theory because it provides a rational way for agents to make decisions. Particularly, in some complex situations, agents usually need to make decisions when faced with an unknown environment. For example, the true value of a stock is unknown in a stock market <cit.>; the delay of each route is unknown in a traffic routing problem <cit.>; the location of target is unknown in a robotic target covering problem <cit.>. Therefore, developing learning algorithms in such game models with incomplete information is crutial and has gained increasing popularity in recent years. \n\nThe common framework of learning dynamics in games with an unknown cost-relevant environment (parameter) consists of two processes: parameter learning and strategy update. In the parameter learning process, agents update the estimates of the parameter according to received historical information. In the strategy update process, agents make decision given the current estimates of the parameter. Distinct from the learning dynamics for misspecified game model <cit.>, parameter learning and strategy update are coupled in this framework, which brings additional difficulty to the convergence analysis. Inspired by the social learning in networks <cit.>, the authors in <cit.> studied the limit behaviour of rational learning process in linear games with an unknown parameter. Along another line of research, <cit.> and <cit.> designed learning dynamics for stochastic routing games with unknown latency function and stochastic network aggregative games with unknown network parameter, respectively, by utilizing the least-squares estimation technique. \n\nThe aforementioned works assume that the unknown parameter influences the cost function in a special structure (such as a linear structure). Learning dynamics for game models with general parameter structure are rare in the literature. To the best of our knowledge, there are two works devoted to this kind of problem. On the premise that the parameter learning process converges at a rate faster than O(log t/t), <cit.> designed a distributed fictitious play dynamics for finite potential games with incomplete information. More recently, <cit.> proposed a Bayesian learning dynamics that does not depend on the structure of parameter affecting the cost. The learning process in <cit.> assumes the existence of an information system for centrally updating the belief about the parameter. However, such an information system may not exsit in some industries. This may be the case, for example, using a central system to collect information from all traders in the stock market may be impossible. A more rational way is that traders estimate the true value of the stock distributedly based on local information. This paper addresses distributed learning problem in games with incomplete information, where each agent updates the belief about the parameter according to private signals.   \n\nIn the absence of a central system, it is computationally expensive for each agent to deduce other agents' information in a fully Bayesian fashion. In contrast, non-Bayesian learning rules are usually more effective when dealing with learning problmes in large-scale networks because of its low computational burden <cit.>. Following up on the seminal work of Ali Jadbabaie et al. <cit.> on distributed learning, various non-Bayesian learning rules have been developed in social learning or distributed parameter estimation literature. This type of learning rules share a common framework, where each agent first performs a Bayesian update based on the private information and then incorporates the neighbors' beliefs in a simple way. For example, <cit.> designed a non-Bayesian update rule for distributed hypothesis testing by averaging the neighbors' log-beliefs and analyzed the convergence rate. To reduce the communication burden, <cit.> developed a switching learning rule between Bayesian and non-Bayesian regimes. Furthermore, <cit.> studied the performance of non-Bayesian rule proposed by <cit.> in random digraphs. <cit.> and <cit.> proposed adaptive non-Bayesian rules to deal with social learning problems with disparate hypotheses and variable true hypotheses, respectively. In addition, <cit.> considered the scenerio where the agents might have incosistent hypotheses from an optimization point of view and proposed an effective non-Bayesian learning rule for time-varying communication graphs.   \n\nIn this paper, we design learning dynamics for game models with incomplete information by leveraging the idea of non-Bayesian learning. In our learning process, each agent first performs a local tempered Bayesian update to form a posterior belief according to received noisy cost. Then, agents exchange the posterior belief through a communication network and geometrically average the neighbors' beliefs to achieve belief consensus. Based on the current belief, agents select best response strategies for minimizing the expected cost function. Our contributions are as follows:\n\n\t\n  * We propose a novel distributed non-Bayesian learning dynamics for NE seeking in games with incomplete information, which generalizes the learning rule for social learning considered in <cit.>. Moreover, our dynamics does not require an information system to collect the noisy costs of all agents as in <cit.>.\n\t\n  * We prove that agents' beliefs about the parameter converge to a common belief and that the strategy profiles converge to a Nash equilibrium under this common belief. Compared with <cit.>, the beliefs do not necessarily converge to the common belief faster than O(log t/t) in our algorithm.\n\t\n  * We overcome the difficulty of coupling parameter learning and strategy update by combining the strong law of large numbers and Toeplitz' lemma, thus futher showing that the beliefs about the parameter converge to the true parameter.\n \n\nNotations. Denote by \u211d^n the n-dimensional real Euclidean space and 1_n the n-dimensional vector of all ones. For a matrix A, A(i,j) denotes the element in the ith row and jth column. \u0394(S) denotes the set of probability distributions on a set S. For a random variable x with probability distribution \u03bc, denote by \ud835\udd3c_\u03bc[x] the expectation of x. Denote by D_KL(\u03bc_1\u03bc_2) the Kullback-Leibler divergence between two probability distributions \u03bc_1 and \u03bc_2. An undirected graph is characterized by the 2-tuple \ud835\udca2 = (\ud835\udcb1,\u2130), where \ud835\udcb1 = {1,\u2026,n} is the set of nodes and \u2130\u2282\ud835\udcb1\u00d7\ud835\udcb1 is the set of edges. A path from i_1 to i_p is an alternating sequence i_1e_1\u22ef i_p-1e_p-1i_p such that e_r = (i_r,i_r+1)\u2208\u2130 (r = 1,\u2026,p-1). An undirected graph is connected if there is a path between any pair of distinct nodes.\n\n\n\n\u00a7 PROBLEM FORMULATION\n\nConsider a group of N agents that repeatedly play a stage game G = (\ud835\udca9,{\ud835\udcb3_i},{u_i},\u0398) with incomplete information. In game G, the strategy variable of each player i\u2208\ud835\udca9:={1,\u2026,N} is denoted by x_i belonging to a convex and compact set \ud835\udcb3_i. Define x := col{x_1,\u2026,x_N}\u2208\ud835\udcb3:=\u220f_i=1^N\ud835\udcb3_i and x_-i:=col{x_1,\u2026,x_i-1,x_i+1,\u2026,x_N}\u2208\u220f_j\u2260 i\ud835\udcb3_j, respectively, as the strategy profile and the strategies of all agents except i. The important feature of this game is that the cost function of each agent u_i:\ud835\udcb3\u00d7\u0398\u2192\u211d depends not only on a strategy profile, but also on an unknown parameter \u03b8 in a finite set \u0398:={\u03b8_1,\u2026,\u03b8_M}. Assume that the costs are realized with noises given a strategy profile x and a parameter \u03b8. Specifically, let the realized cost of agent i be\n\n    y_i = u_i(x,\u03b8) + \u03f5_i(x,\u03b8),\n\nwhere \u03f5_i(x,\u03b8) is the noise term with zero mean. Denote by f_i(y_i|x,\u03b8) the likelihood function of y_i. We make the following assumption on the likelihood function.\n\n\tThere exists a positive constant L such that\n\t\n    max_imax_\u03b8',\u03b8\u201d\u2208\u0398max_x\u2208\ud835\udcb3sup_y_i|logf_i(y_i|x, \u03b8')/f_i(y_i|x,\u03b8\u201d)|\u2264 L.\n\n\tIn addition, for each i\u2208\ud835\udca9, f_i(y_i|x,\u03b8) is continuous in x for all \u03b8\u2208\u0398.\n\nAssumption <ref> requires that every realized cost has bounded information content, which is a standard assumption for the convergence of the beliefs in the social learning literature <cit.>. Also, the continuity assumption is crutial to ensure the convergence of the strategies in the game setting <cit.>.\n\nMoreover, we denote the true parameter by \u03b8^\u2217\u2208\u0398. Then, the associated Nash equilibrium is defined by a strategy profile x^\u2217 such that no agent can benefit from deviating unilaterally, i.e., for i\u2208\ud835\udca9,\n\n    u_i(x_i^\u2217,x_-i^\u2217,\u03b8^\u2217)\u2264 u_i(x_i,x_-i^\u2217,\u03b8^\u2217), for all x_i\u2208\ud835\udcb3_i.\n\n\n\nTo measure the explanatory quality of the parameters in the set \u0398, we consider the Kullback\u2013Leibler (KL) divergence between the distribution of the realized costs conditioned over the strategies and parameters. Recall that the KL divergence between any two probability distributions P and P' is defined by\n\n    D_KL(PP'):= \ud835\udd3c_P[logP/P'],\n\nwhich equals to 0 if and only if P = P' with probability 1. Inspired by <cit.>, for each agent i\u2208\ud835\udca9 and a given strategy profile x\u2208\ud835\udcb3, we define a set of cost-equivalent parameters\n\n    \u0398\u0305_i(x):= {\u03b8\u2208\u0398: D_KL(f_i(y_i|x,\u03b8^\u2217)f_i(y_i|x,\u03b8)) = 0}.\n\nIn other words, given a strategy profile x, the parameters in the set \u0398\u0305_i(x) are locally indistinguishable to agent i. To learn the true parameter, we generalize the global identifiability assumption in the social learning literature <cit.> to the game setting.\n\n\tFor every \u03b8\u2260\u03b8^\u2217, there is at least one agent i\u2208\ud835\udca9 for which the KL divergence D_KL(f_i(y_i|x,\u03b8^\u2217)f_i(y_i|x,\u03b8)) is strictly positive for all x\u2208\ud835\udcb3. \n\nAssumption <ref> guarantees that for all x\u2208\ud835\udcb3,\n\n    \u0398\u0305_1(x)\u2229\u0398\u0305_2(x)\u2229\u22ef\u2229\u0398\u0305_N(x) = {\u03b8^\u2217}.\n\nTherefore, although a single agent may not distinguish \u03b8^\u2217 from all other parameters, the true parameter is globally identifiable from the standpoint of the global game model. \n\nThe problem to be addressed in this paper is how agents learn the Nash equilibrium distributedly when the true parameter is unknown. Each agent forms its own belief on the parameter and can exchange information with other agents through a communication network. Agents attempts to learn the true parameter \u03b8^\u2217 by combining their local knowledge (realized costs) with the information received from the network. \n\nWe model the communication network via an undirected graph \ud835\udca2(\ud835\udca9,\u2130), where \ud835\udca9 is the node set and \u2130 is the edge set. Denote by \ud835\udca9_i:={j\u2208\ud835\udca9: (j,i)\u2208\u2130} the set of neighbors of agent i. Each agent can receive information from its neighbors. Moreover, we let W = [w_ij]\u2208\u211d^N\u00d7 N denote the associated adjacency matrix, which defines the weights that each agent assigns to neighbors' information such that w_ij > 0 if and only if j\u2208\ud835\udca9_i. We require the following assumptions which have been previously used in distributed optimization <cit.> and game theory <cit.>.\n\n\tThe undirected communication graph \ud835\udca2(\ud835\udca9,\u2130) is connected and W is doubly stochastic, i.e., \u2211_j=1^Nw_ij = \u2211_i=1^Nw_ij = 1.\n\nThe following lemma <cit.> provides a mixing rate of the adjacency matrix and plays an important role in the convergence analysis.\n\n\tDenote by \u03bb_i(W) the i-largest eigenvalue of the mixing matrix. Under Assumption <ref>, W satisfies\n\t\n    |W^t(i,j) - 1/N|\u2264\u03bb_max(W)^t,\n\n\twhere \u03bb_max(W)\u225cmax{|\u03bb_N(W)|,|\u03bb_2(W)|}\u2208 (0,1). \n \n\n\n\u00a7 ALGORITHM DESIGN\n\nOur learning process consists of two parts: the evolution of agents' beliefs about the unknown parameter and the update of agents' strategies. At each time step t, each agent i maintains a private belief \u03bc_i^(t)\u2208\u0394(\u0398) and takes action x_i^(t). The cost y_i^(t) realized according to f_i(y_i^(t)|x^(t),\u03b8) when the parameter is \u03b8 and each agent forms a posterior belief b_i^(t) based on the cost. Then, agents exchange the posterior beliefs with their neighbors and updates the private beliefs using a non-Bayesian rule.\n\nGiven the current private belief \u03bc_i^(t+1), agent i evaluates its expected cost by\n\n    u_i(x,\u03bc_i^(t+1)):= \u2211_\u03b8\u2208\u0398\u03bc_i^(t+1)(\u03b8)u_i(x,\u03b8).\n\nFurther, assuming that the rivals' strategies are fixed as x_-i^(t), the best-response mapping of agent i is defined by\n\n    BR_i(x_-i^(t),\u03bc_i^(t+1)):=min_x_i\u2208\ud835\udcb3_iu_i(x_i,x_-i^(t),\u03bc_i^(t+1)).\n\nWe thoroughly describe our learning process in Algorithm <ref>.\n\nNote that the update rule (<ref>) of posterior belief is different from the traditional Bayes rule. Such a generalized posterior is called tempered posterior distribution, which is easier to theoretical analysis <cit.> and more robust to model misspecification <cit.>. Tempered posterior has also been used in social learning <cit.> and stochastic bandit problem <cit.>.\n\n\n\n\n\tCompared to the non-Bayesian learning algorithm for social learning <cit.>, in a game setting, the likelihood functions of the realized costs {y_i^(t)}_t\u2265 0 depends not only on the unknown parameter, but also on the strategy profile x^(t). And hence, the costs are not independent and identically distributed with respect to time t. In addition, different from the learning algorithm in <cit.> that requires an information system to centrally perform Bayesian update, agents employ a non-Bayesian rule to distributedly learn the unknown parameter in our algorithm.   \n\n\n\n\u00a7 MAIN RESULTS\n\nIn this section, we present the convergence of agents' beliefs to the true parameter and the convergence of the strategy profile to the Nash equilibrium.\n\n\n \u00a7.\u00a7 Belief Convergence\n\nIn this part, we show that agents' beliefs reach consensus to a common belief \u03bc. Before presenting the proof, we make the following assumption on the step-size sequence. \n\n\tThe step-size sequence {\u03b1^(t)} satisfies 0<\u03b1^(t)<1, \u2211_t=0^\u221e\u03b1^(t) = \u221e, and \u2211_t=0^\u221e(\u03b1^(t))^2 < \u221e.\n\nFirst, we prove the convergence of the average belief ratio 1/N\u2211_i=1^N\u03bc_i^(t)(\u03b8)/\u03bc_i^(t)(\u03b8^\u2217). Let \u2131_t:=\u03c3{\u03bc_i^(0),x_i^(s),y_i^(s),0\u2264 s\u2264 t-1, i\u2208\ud835\udca9} denote the \u03c3-field containing the past information about all agents. \n\n\tLet Assumption <ref> hold. Then, the sequence 1/N\u2211_i=1^N\u03bc_i^(t)(\u03b8_k)/\u03bc_i^(t)(\u03b8^\u2217) converges with probability 1 to some non-negative random variable \u03bd_k for all \u03b8_k\u2208\u0398.\n\n\n\tBy the belief update rules (<ref>) and (<ref>),\n\t\n    \u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217)   =exp( \u2211_j=1^Nw_ijlogb_j^(t)(\u03b8_k)/b_j^(t)(\u03b8^\u2217))\n       \u2264\u2211_j=1^Nw_ijf_j(y_j^(t)|x^(t),\u03b8_k)^\u03b1^(t)\u03bc_j^(t)(\u03b8_k)/f_j(y_j^(t)|x^(t),\u03b8^\u2217)^\u03b1^(t)\u03bc_j^(t)(\u03b8^\u2217),\n\n\twhere the inequality is followed by \u2211_j=1^Nw_ij = 1 and the Jensen's inequality. Furthermore, using \u2211_i=1^Nw_ij = 1, we get\n\t\n    \u2211_i=1^N\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217)\u2264\u2211_i=1^Nf_j(y_i^(t)|x^(t),\u03b8_k)^\u03b1^(t)\u03bc_i^(t)(\u03b8_k)/f_i(y_i^(t)|x^(t),\u03b8^\u2217)^\u03b1^(t)\u03bc_i^(t)(\u03b8^\u2217).\n\n\tThus, by taking conditional expectation and noting that \u03bc_i^(t)(\u03b8) is \u2131_t-measurable for all \u03b8\u2208\u0398, we derive\n\t\n    \ud835\udd3c[\u2211_i=1^N\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217)|\u2131_t] \u2264\u2211_i=1^N\u03bc_i^(t)(\u03b8_k)/\u03bc_i^(t)(\u03b8^\u2217)\ud835\udd3c[(f_i(y_i^(t)|x^(t),\u03b8_k)/f_i(y_i^(t)|x^(t),\u03b8^\u2217))^\u03b1^(t)|\u2131_t],\n\n\twhere the expectation is taken on the probability distribution \u220f_j=1^Nf_j(y_j|x,\u03b8^\u2217). Since x^\u03b1 is a concave function when 0<\u03b1<1, Jensen's inequality implies that\n\t\n    \ud835\udd3c[\u2211_i=1^N\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217)|\u2131_t]    \u2264\u2211_i=1^N\u03bc_i^(t)(\u03b8_k)/\u03bc_i^(t)(\u03b8^\u2217)\ud835\udd3c[f_i(y_i^(t)|x^(t),\u03b8_k)/f_i(y_i^(t)|x^(t),\u03b8^\u2217)|\u2131_t]^\u03b1^(t)\n       =  \u2211_i=1^N\u03bc_i^(t)(\u03b8_k)/\u03bc_i^(t)(\u03b8^\u2217)(\u222b_y_i^(t)f_i(y_i^(t)|x^(t),\u03b8^\u2217)f_i(y_i^(t)|x^(t),\u03b8_k)/f_i(y_i^(t)|x^(t),\u03b8^\u2217)dy_i^(t))^\u03b1^(t)\n       = \u2211_i=1^N\u03bc_i^(t)(\u03b8_k)/\u03bc_i^(t)(\u03b8^\u2217),\n\n\twhere the first equality follows from that conditioned on the current action profile x^(t), f_i(y_i^(t)|x^(t),\u03b8_k)/f_i(y_i^(t)|x^(t),\u03b8^\u2217) is independent of \u2131_t. Therefore, \u2211_i=1^N\u03bc_i^(t)(\u03b8_k)/\u03bc_i^(t)(\u03b8^\u2217) is a positive supermartingale. By the supermartingale convergence theorem <cit.>, we conclude that 1/N\u2211_i=1^N\u03bc_i^(t)(\u03b8)/\u03bc_i^(t)(\u03b8^\u2217) converges with probability 1.\n\nNext, we establish the consensus of the log-belief ratio log\u03bc_i^(t)(\u03b8)/\u03bc_i^(t)(\u03b8^\u2217).\n\n\tLet Assumptions <ref>,<ref>,<ref> hold. Then the agents' log-belief ratios reach consensus, i.e., for all \u03b8_k\u2208\u0398,\n\t\n    |log\u03bc_i^(t)(\u03b8_k)/\u03bc_i^(t)(\u03b8^\u2217) - 1/N\u2211_i=1^Nlog\u03bc_i^(t)(\u03b8_k)/\u03bc_i^(t)(\u03b8^\u2217)|\u2192 0.\n \n\n\n\tAgain, by the belief update rules (<ref>) and (<ref>), we derive\n\t\n    log\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217)   = \u2211_j=1^Nw_ijlogb_j^(t)(\u03b8_k)/b_j^(t)(\u03b8^\u2217)\n       = \u2211_j=1^Nw_ijlog\u03bc_j^(t)(\u03b8_k)/\u03bc_j^(t)(\u03b8^\u2217) + \u03b1^(t)\u2211_j=1^Nw_ijlogf_j(y_j^(t)|x^(t),\u03b8_k)/f_j(y_j^(t)|x^(t),\u03b8^\u2217)\n       = \u2211_j=1^N\u2211_\u03c4=1^tW^\u03c4(i,j)\u03b1^(t-\u03c4 +1)logf_j(y_j^(t-\u03c4+1)|x^(t-\u03c4 +1),\u03b8_k)/f_j(y_j^(t-\u03c4+1)|x^(t-\u03c4 +1),\u03b8^\u2217) + \u2211_j=1^NW^t+1(i,j)log\u03bc_j^(0)(\u03b8_k)/\u03bc_j^(0)(\u03b8^\u2217)\n       = \u2211_j=1^N\u2211_\u03c4=1^tW^\u03c4(i,j)\u03b1^(t-\u03c4 +1)logf_j(y_j^(t-\u03c4+1)|x^(t-\u03c4 +1),\u03b8_k)/f_j(y_j^(t-\u03c4+1)|x^(t-\u03c4 +1),\u03b8^\u2217),\n\n\twhere the last equality follows from \u03bc_i^(0) = 1/M1_M. By the double stochasticity of W, \n\t\n    1/N\u2211_i=1^Nlog\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217) = 1/N\u2211_i=1^N\u2211_\u03c4=1^t\u03b1^(t-\u03c4 +1)logf_i(y_i^(t-\u03c4+1)|x^(t-\u03c4 +1),\u03b8_k)/f_i(y_i^(t-\u03c4+1)|x^(t-\u03c4 +1),\u03b8^\u2217),\n\n\tAs a result,\n\t\n    |log\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217) - 1/N\u2211_i=1^Nlog\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217)|\n       \u2264\u2211_j=1^N\u2211_\u03c4=1^t\u03b1^(t-\u03c4+1)|W^\u03c4(i,j) - 1/N||logf_j(y_j^(t-\u03c4+1)|x^(t-\u03c4+1),\u03b8_k)/f_j(y_j^(t-\u03c4+1)|x^(t-\u03c4+1),\u03b8^\u2217)|.\n\n\tBy the connectivity of the communication graph, |W^\u03c4(i,j) - 1/N|\u2264\u03bb_max(W)^\u03c4. Thus, by Assumption <ref>,\n\t\n    |log\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217) - 1/N\u2211_i=1^Nlog\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217)|\u2264 NL\u2211_\u03c4 = 1^t\u03b1^(t-\u03c4+1)\u03bb_max(W)^\u03c4,\n\n\twhich converges to 0 by Lemma 7[Let 0<\u03b2<1 and let {\u03b3_k} be a positive scalar sequence. Assume that lim_k\u2192\u221e\u03b3_k = 0. Then lim_k\u2192\u221e\u2211_l=0^k\u03b2^k-l\u03b3_l = 0.] of <cit.>. \n\nLemma 3 provides the reason why we consider a tempered posterior distribution in Algorithm <ref>. Based on Lemma 2 and Lemma 3, we now present our first main result in the following theorem.\n\n\tLet Assumptions <ref>, <ref>, <ref> hold. The belief sequence {\u03bc_i^(t)}_t\u2265 0 of each agent generated by Algorithm <ref> converges almost surely to a common belief \u03bc with the form\n\t\n    \u03bc(\u03b8_k) = \u03bd_k/\u2211_k=1^M\u03bd_k, for k = 1,\u2026,M\n\n\twhere \u03bd_k is defined in Lemma 2.\n\n\n\tPerforming an exponential operation on Lemma 3 yields\n\t\n    \u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217)\u00b71/exp(1/N\u2211_i=1^Nlog\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217))\u2192 1.\n\n\tTaking the average, we get\n\t\n    1/N\u2211_i=1^N\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217)\u00b71/exp(1/N\u2211_i=1^Nlog\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217))\u2192 1.\n\n\tFurthermore, taking the logarithm of both sides, we obtain\n\t\n    log(1/N\u2211_i=1^N\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217)) - 1/N\u2211_i=1^Nlog\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217)\u2192 0.\n\n\tThus, by Lemma 2, \n\t\n    1/N\u2211_i=1^Nlog\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217)\u2192log\u03bd_k, a.s.\n\n\tUsing Lemma 3 again, we get\n\t\n    log\u03bc_i^(t+1)(\u03b8_k)/\u03bc_i^(t+1)(\u03b8^\u2217)\u2192log\u03bd_k, a.s.\n\n\tOn the other hand, by the belief update rules,\n\t\n    \u03bc_i^(t+1)(\u03b8^\u2217)    (<ref>)=exp(\u2211_j=1^Nw_ijlog b_j^(t)(\u03b8^\u2217))/\u2211_\u03b8\u2208\u0398exp(\u2211_j=1^Nw_ijlog b_j^(t)(\u03b8))\n       = (1 + \u2211_\u03b8\u2260\u03b8^\u2217exp(\u2211_j=1^Nw_ijlogb_j^(t)(\u03b8)/b_j^(t)(\u03b8^\u2217)))^-1\n       (<ref>)=(1 + \u2211_\u03b8\u2260\u03b8^\u2217exp(\u2211_j=1^Nw_ij\u03b1^(t)logf_j(y_j^(t)|x^(t),\u03b8)/f_j(y_j^(t)|x^(t),\u03b8^\u2217) + \u2211_j=1^Nw_ijlog\u03bc_j^(t)(\u03b8)/\u03bc_j^(t)(\u03b8^\u2217)))^-1\n\n\tWithout loss of generality, we let \u03b8^\u2217 = \u03b8_1. By Assumption <ref> and Assumption <ref>, logf_j(y_j^(t)|x^(t),\u03b8)/f_j(y_j^(t)|x^(t),\u03b8^\u2217) is bounded and \u03b1^(t)\u2192 0. Thus, for all \u03b8\u2208\u0398,\n\t\n    exp(\u2211_j=1^Nw_ij\u03b1^(t)logf_j(y_j^(t)|x^(t),\u03b8)/f_j(y_j^(t)|x^(t),\u03b8^\u2217))\u2192 1.\n\n\tAnd by (<ref>),\n\t\n    exp(\u2211_j=1^Nw_ijlog\u03bc_j^(t)(\u03b8_k)/\u03bc_j^(t)(\u03b8^\u2217))\u2192\u03bd_k,   a.s.\n\n\tSubstituting (<ref>) and (<ref>) into (<ref>), we obtain \n\t\n    \u03bc_i^t+1(\u03b8^\u2217)\u2192 (1 + \u2211_k=2^M\u03bd_k)^-1, a.s.\n\n\tFurther, applying (<ref>) yields\n\t\n    \u03bc_i^t+1(\u03b8_k)\u2192\u03bd_k/1 + \u2211_k=2^M\u03bd_k, a.s.\n\n\tThe assertion follows by noting that \u03bd_1 = 1 when \u03b8^\u2217 = \u03b8_1.\n\nTheorem <ref> shows that agents' beliefs converge to a common belief, which is not necessarily the true belief. Distinct from the centralized Bayesian update in <cit.> whose belief convergence is directly obtained through the martingale convergence theorem, we also need to prove that the beliefs of different agents reach consensus.\n\n\n \u00a7.\u00a7 Strategy Convergence\n\nSimilar to (<ref>), we define the Nash equilibrium of the game with a common belief \u03bc by a strategy profile x^\u2217(\u03bc) satisfying\n\n    u_i(x_i^\u2217(\u03bc),x_-i^\u2217(\u03bc),\u03bc)\u2264 u_i(x_i,x_-i^\u2217(\u03bc),\u03bc), for all x_i\u2208\ud835\udcb3_i\n\nwhere the expected cost is defined by (<ref>). Moreover, we consider an auxiliary trajectory x\u0305(t) generated by the following continuous-time best-response dynamics under the common belief \u03bc \n    dx\u0305_i(t)/dt\u2208BR_i(x\u0305_-i(t),\u03bc) - x\u0305_i(t),  x\u0305_i(0) = x_i^(0).\n\nTo study the strategy convergence of Algorithm <ref>, we require the following assumption on the cost function.\n\n\tThe cost function u_i(x_i,x_-i,\u03b8) is continuous in x and strictly convex in x_i for any \u03b8\u2208\u0398.\n\nAssumption <ref> guarantees that there exists a unique Nash equilibrium for the game with a common belief <cit.> and for any \u03bc'\u2208\u0394(\u0398) and any x_-i\u2208\ud835\udcb3_-i, the best response mapping BR_i(x_-i,\u03bc') is single-valued. Further, from Berge's maximum theorem <cit.>, BR_i(x_-i,\u03bc') is continuous in both x_-i and \u03bc'. \nIn addition, inspired by <cit.>, we impose the following assumption on the best-response dynamics (<ref>), which holds for potential games and dominance solvable games <cit.>.\n\n\tFor any x^(0)\u2208\ud835\udcb3, the solution of (<ref>) converges to the Nash equilibrium of the game with the common belief \u03bc, i.e., lim_t\u2192\u221ex\u0305(t) = x^\u2217(\u03bc). \n\nOur next main result is based on the following stochstic approximation conclusion <cit.>.\n\n\tConsider a stochstic approximation scheme\n\t\n    z_n+1 = z_n + \u03b3_n(h(z_n) + \u03be_n),\n\n\twhere {\u03be_n} is a bounded random sequence with \u03be_n\u2192 0 almost surely. Assume that h is Lipschitz continuous and the step-size sequence \u03b3_n satisfies \u2211_n\u03b3_n = \u221e and \u2211_n\u03b3_n^2<\u221e. If sup_nz_n<\u221e almost surely, then the sequence {z_n} converges almost surely to the set of asymptotically stable equilibria of the dynamics dz(t)/dt = h(z(t)). \n\nWe proceed to derive the convergence of the strategy profiles {x^(t)}_t=1^\u221e.\n\n\tLet Assumptions <ref>, <ref>-<ref> hold. Then, the strategy profile x^(t) generated by Algorithm <ref>  converges almost surely to x^\u2217(\u03bc).\n\n\n\tBy (<ref>), we get\n\t\n    x_i^(t+1)   = (1 - \u03b1^(t))x_i^(t) + \u03b1^(t)BR_i(x_-i^(t),\u03bc_i^(t+1))\n       =x_i^(t) + \u03b1^(t)(BR_i(x_-i^(t),\u03bc_i^(t+1)) - x_i^(t))\n       = x_i^(t) + \u03b1^(t)(BR_i(x_-i^(t),\u03bc) - x_i^(t) + BR_i(x_-i^(t),\u03bc_i^(t+1)) - BR_i(x_-i^(t),\u03bc))\n\n\tNote by Theorem 1 that \u03bc_i^(t+1)\u2192\u03bc almost surely. Thus, using the continuity of BR_i, we obtain\n\t\n    BR_i(x_-i^(t),\u03bc_i^(t+1)) - BR_i(x_-i^(t),\u03bc)\u2192 0, a.s.\n\n\tThe conlusion is followed by applying Assumption <ref> and Assumption <ref> to Lemma 4.\n\nThe strategy update rule (<ref>) is similar to the best-response strategy considered in <cit.>, except that each agent uses its own belief in our algorithm. In fact, the equilibrium strategy and the corresponding convergence result in <cit.> are also applicable since we have obtained \u03bc_i^(t)\u2192\u03bc, a.s..\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Convergence to the True Parameter\n\nIn this part, we further prove that agents' beliefs converge to the true belief by combining Theorem 1 and Theorem 2. In general, the strong law of large numbers and McDiarmid\u2019s inequality are employed in the social learning literature <cit.> to show the convergence of agents' beliefs to the true parameter. However, all these results rely on an assumption that the private signals (realized costs in our setting) are independent and identically distributed across the time t. And hence, these techniques cannot be directly applied to the game setting as the realized costs are not i.i.d due to the influence of the strategies. We instead use the following Toeplitz's lemma <cit.> to develop a similar convergence result.\n\n\tLet {A_nk,1\u2264 k\u2264 k_n}_n\u2265 1 be a double array of positive numbers such that for fixed k, A_nk\u2192 0 when n\u2192\u221e. Let {Y_n}_n\u2265 1 be a sequence of real numbers. If Y_n\u2192 y and \u2211_k=1^k_nA_nk\u2192 1 when n\u2192\u221e, then lim_n\u2192\u221e\u2211_k=1^k_nA_nkY_k = y. \n\t\n \nNow we state our final main result in the following theorem.\n\n\tSuppose that Assumptions <ref>-<ref> hold. Let {\u03bc_i^(t)}_t\u2265 0 be the belief sequence generated by Algorithm <ref>. Then, for each agent i\u2208\ud835\udca9, it holds that \n\t\n    lim_T\u2192\u221e1/\u2211_t=1^T\u03b1^(t)log\u03bc_i^(T+1)(\u03b8^\u2217)/\u03bc_i^(T+1)(\u03b8_k)\n    \t= Z(\u03b8^\u2217,\u03b8_k)   a.s.\n\n\twhere Z(\u03b8^\u2217,\u03b8_k)=1/N\u2211_j=1^ND_KL(f_j(y_j|x^\u2217(\u03bc),\u03b8^\u2217)f_j(y_j|x^\u2217(\u03bc),\u03b8_k)) is the network divergence and x^\u2217(\u03bc) is defined in Assumption <ref>. In particular, we have \u03bc_i^(t)(\u03b8^\u2217)\u2192 1 almost surely.\n\n\n\tSimilar to (<ref>), we get\n\t\n    lim_T\u2192\u221e1/\u2211_t=1^T\u03b1^(t)log\u03bc_i^(T+1)(\u03b8^\u2217)/\u03bc_i^(T+1)(\u03b8_k)\n       = lim_T\u2192\u221e1/\u2211_t=1^T\u03b1^(t)\u2211_j=1^N\u2211_t=1^TW^t(i,j)\u03b1^(T-t+1)logf_j(y_j^(T-t+1)|x^(T-t+1),\u03b8^\u2217)/f_j(y_j^T-t+1|x^(T-t+1),\u03b8_k)\n       = lim_T\u2192\u221e1/\u2211_t=1^T\u03b1^(t)\u2211_j=1^N\u2211_t=1^T\u03b1^(T-t+1)(W^t(i,j) - 1/N)logf_j(y_j^(T-t+1)|x^(T-t+1),\u03b8^\u2217)/f_j(y_j^(T-t+1)|x^(T-t +1),\u03b8_k)\n          + lim_T\u2192\u221e1/\u2211_t=1^T\u03b1^(t)1/N\u2211_j=1^N\u2211_t=1^T\u03b1^(T-t+1)logf_j(y_j^(T-t+1)|x^(T-t+1),\u03b8^\u2217)/f_j(y_j^(T-t+1)|x^(T-t +1),\u03b8_k).\n\n\tNote by (<ref>) and (<ref>) that\n\t\n    lim_T\u2192\u221e1/\u2211_t=1^T\u03b1^(t)\u2211_j=1^N\u2211_t=1^T\u03b1^(T-t+1)(W^t(i,j) - 1/N)logf_j(y_j^(T-t+1)|x^(T-t+1),\u03b8^\u2217)/f_j(y_j^(T-t+1)|x^(T-t +1),\u03b8_k)\n       = lim_T\u2192\u221e1/\u2211_t=1^T\u03b1^(t)(log\u03bc_i^(T+1)(\u03b8^\u2217)/\u03bc_i^(T+1)(\u03b8_k) - 1/N\u2211_i=1^Nlog\u03bc_i^(T+1)(\u03b8^\u2217)/\u03bc_i^(T+1)(\u03b8_k))\n       = 0,\n\n\twhere the last equality follows from Lemma 3 and Assumption <ref>. As a result, combining the above two relations and denoting by z_j^(t)(\u03b8^\u2217,\u03b8_k) = logf_j(y_j^(t)|x^(t),\u03b8^\u2217)/f_j(y_j^(t)|x^(t),\u03b8_k) and s^(T) = \u2211_t=1^T\u03b1^(t), we further derive\n\t\n    lim_T\u2192\u221e1/\u2211_t=1^T\u03b1^(t)log\u03bc_i^(T+1)(\u03b8^\u2217)/\u03bc_i^(T+1)(\u03b8_k)\n       = lim_T\u2192\u221e1/\u2211_t=1^T\u03b1^(t)1/N\u2211_j=1^N\u2211_t=1^T\u03b1^(T-t+1)logf_j(y_j^(T-t+1)|x^(T-t+1),\u03b8^\u2217)/f_j(y_j^(T-t+1)|x^(T-t +1),\u03b8_k)\n       = 1/N\u2211_j=1^Nlim_T\u2192\u221e1/s^(T)\u2211_t=1^T\u03b1^(t)z_j^(t)(\u03b8^\u2217,\u03b8_k)\n\n\tTo consider the convergence of the weighted average of the random variables z_j^(t)(\u03b8^\u2217,\u03b8_k), we first study the convergence of 1/T\u2211_t=1^Tz_j^(t)(\u03b8^\u2217,\u03b8_k). By Theorem 2, x^(t)\u2192 x^\u2217(\u03bc) almost surely. We define the following cumulative distribution functions\n\t\n    F_j^(t)(z)\u225c P{z_j^(t)(\u03b8^\u2217,\u03b8_k)\u2264 z},   F_j^\u2217(z)\u225c P{logf_j(y_j|x^\u2217(\u03bc),\u03b8^\u2217)/f_j(y_j|x^\u2217(\u03bc),\u03b8_k)\u2264 z}.\n\n\tWe establish the relationship between F_j^(t)(z) and F_j^\u2217(z) in two steps.\n\n\tStep 1: From F_j^(t)(z) to uniform distribution on [0,1]\n\n\tDefine\n\t\n    \u0394_j^(t)\u225c F_j^(t)(z_j^(t)(\u03b8^\u2217,\u03b8_k)).\n\n\tThen, \u0394_j^(t)\u2208[0,1] and for any \u03b4\u2208[0,1],\n\t\n    P{\u0394_j^(t)\u2264\u03b4} = P{z_j^(t)(\u03b8^\u2217,\u03b8_k)\u2264 (F_j^(t))^-1(\u03b4)} = F_j^(t)(F_j^(t))^-1(\u03b4) = \u03b4.\n\n\tThus, \u0394_j^(t) has a uniform distribution on [0,1] for all t.\n\n\tStep 2: From uniform distribution on [0,1] to F_j^\u2217(z)\n\n\tDefine\n\t\n    \u03b7_j^(t)\u225c (F_j^\u2217)^-1(\u0394_j^(t)).\n\n\tSince \u0394_j^(t) has a uniform distribution, we get \n\t\n    P{\u03b7_j^(t)\u2264 z} = P{\u0394_j^(t)\u2264 F_j^\u2217(z)} = F_j^\u2217(z).\n\n\tNote that conditioned on the action profiles, the sequence {z_j^(t)(\u03b8^\u2217,\u03b8_k)} is independent over t. Therefore, the sequence {\u03b7_j^(t)}_t\u2265 1 is independent and identically distributed over t and has the cumulative distribution function F_j^\u2217(z). Furthermore, by the strong law of large numbers,\n\t\n    lim_T\u2192\u221e1/T\u2211_t=1^T\u03b7_j^(t) = \ud835\udd3c[logf_j(y_j|x^\u2217(\u03bc),\u03b8^\u2217)/f_j(y_j|x^\u2217(\u03bc),\u03b8_k)]   a.s.\n\n\t\n\tOn the other hand, since x^(t)\u2192 x^\u2217(\u03bc) a.s., we get by the continuity of the likelihood function (Assumption 1) that z_j^(t)(\u03b8^\u2217,\u03b8_k)\u2192logf_j(y_j|x^\u2217(\u03bc),\u03b8^\u2217)/f_j(y_j|x^\u2217(\u03bc),\u03b8_k), a.s. Thus, the convergence also holds in distribution, i.e., for all z\u2208\u211d,\n\t\n    lim_t\u2192\u221eF_j^(t)(z) = lim_t\u2192\u221eF_j^\u2217(z).\n\n\tMoreover, since the set of discontinuous points of a monotone function is at most countable <cit.>, we get\n\t\n    lim_t\u2192\u221e(z_j^(t)(\u03b8^\u2217,\u03b8_k) - \u03b7_j^(t)) = lim_t\u2192\u221e(z_j^(t)(\u03b8^\u2217,\u03b8_k) - (F_j^\u2217)^-1F_j^(t)(z_j^(t)(\u03b8^\u2217,\u03b8_k))) = 0   a.s.\n \n\tBy (<ref>)-(<ref>) and using Toeplitz's lemma (Lemma 4) with A_Tt = 1/T (t = 1,\u2026,T) and Y_t = z_j^(t)(\u03b8^\u2217,\u03b8_k) - \u03b7_j^(t), we obtain\n\t\n    lim_T\u2192\u221e1/T\u2211_t=1^Tz_j^(t)(\u03b8^\u2217,\u03b8_k) = \ud835\udd3c[logf_j(y_j|x^\u2217(\u03bc),\u03b8^\u2217)/f_j(y_j|x^\u2217(\u03bc),\u03b8_k)]   a.s.\n\n\t\n\tNext, we associate the weighted sum (<ref>) with lim_T\u2192\u221e1/T\u2211_t=1^Tz_j^(t)(\u03b8^\u2217,\u03b8_k). From (<ref>), we further derive \n\t\n    lim_T\u2192\u221e1/\u2211_t=1^T\u03b1^(t)log\u03bc_i^(T+1)(\u03b8^\u2217)/\u03bc_i^(T+1)(\u03b8_k)\n       = 1/N\u2211_j=1^Nlim_T\u2192\u221e1/s^(T)(T\u03b1^(T)\u00b71/T\u2211_t=1^Tz_j^(t)(\u03b8^\u2217,\u03b8_k) + \u2211_t=1^T-1t(\u03b1^(t) - \u03b1^(t+1))\u00b71/t\u2211_\u03c4=1^tz_j^(\u03c4)(\u03b8^\u2217,\u03b8_k)).\n\n\tNotice that T\u03b1^(T) + \u2211_t=1^T-1t(\u03b1^(t) - \u03b1^(t+1)) = s^(T). As a result, by s^(T)\u2192\u221e (T\u2192\u221e), applying Toeplitz's lemma again with A_Tt = t(\u03b1^(t) - \u03b1^(t+1))/s^(T) (t = 1,\u2026,T-1), A_TT = T\u03b1^(T)/s^(T), and Y_t = 1/t\u2211_\u03c4=1^tz_j^(\u03c4)(\u03b8^\u2217,\u03b8_k) yields\n\t\n    lim_T\u2192\u221e1/\u2211_t=1^T\u03b1^(t)log\u03bc_i^(T+1)(\u03b8^\u2217)/\u03bc_i^(T+1)(\u03b8_k)   = 1/N\u2211_j=1^Nlim_T\u2192\u221e1/T\u2211_t=1^Tz_j^(t)(\u03b8^\u2217,\u03b8_k)\n       = \ud835\udd3c[1/N\u2211_j=1^Nlogf_j(y_j|x^\u2217(\u03bc),\u03b8^\u2217)/f_j(y_j|x^\u2217(\u03bc),\u03b8_k)]   a.s.\n\n\t\n\tRecall that Z(\u03b8^\u2217,\u03b8_k)\u225c1/N\u2211_j=1^ND_KL(f_j(y_j|x^\u2217(\u03bc),\u03b8^\u2217)f_j(y_j|x^\u2217(\u03bc),\u03b8_k)). From the definition (<ref>) of KL divergence,\n\t\n    Z(\u03b8^\u2217,\u03b8_k) = \ud835\udd3c[1/N\u2211_j=1^Nlogf_j(y_j|x^\u2217(\u03bc),\u03b8^\u2217)/f_j(y_j|x^\u2217(\u03bc),\u03b8_k)].\n\n\tBy Assumption <ref>, Z(\u03b8^\u2217,\u03b8_k) is strictly positive. Additionally, from (<ref>), for all \u03f5 > 0, there exists T' such that for all T\u2265 T',\n\t\n    |1/s^(T)log\u03bc_i^(T+1)(\u03b8^\u2217)/\u03bc_i^(T+1)(\u03b8_k) - Z(\u03b8^\u2217,\u03b8_k)|\u2264\u03f5   a.s.\n\n\tTherefore,\n\t\n    \u03bc_i^(T+1)(\u03b8_k)/\u03bc_i^(T+1)(\u03b8^\u2217)\u2264exp(-s^(T)(Z(\u03b8^\u2217,\u03b8_k) - \u03f5))   a.s.\n\n\tUsing \u2211_k=1^M\u03bc_i^(T+1)(\u03b8_k) = 1, we get\n\t\n    1/\u03bc_i^(T+1)(\u03b8^\u2217) - 1\u2264\u2211_\u03b8_k\u2260\u03b8^\u2217exp(-s^(T)(Z(\u03b8^\u2217,\u03b8_k) - \u03f5))   a.s.\n\n\tFurthermore, we derive\n\t\n    1/1 + \u2211_\u03b8_k\u2260\u03b8^\u2217exp(-s^(T)(Z(\u03b8^\u2217,\u03b8_k) - \u03f5))\u2264\u03bc_i^(T+1)(\u03b8^\u2217)\u2264 1   a.s.\n\n\tHence we have that \u03bc_i^(t)(\u03b8^\u2217)\u2192 1 a.s., and the assertion is proved.\n\nTheorem 3 establishes the convergence of the beliefs to the true parameter and the rate is given by (<ref>). Combining Theorem 3 and Theorem 2, we further obtain that the strategy profile converges to the true Nash equilibrium. \n\n\tThe equation (<ref>) shows that the convergence rate depends on the step-size sequence {\u03b1^(t)}. Combined with Theorem 1 in <cit.>, we know that if we choose an appropriate sequence {\u03b1^(t)} such that \u03bc_i^(t) converges to \u03bc at a rate faster than O(log t/t), agents may replace the best response strategies with a distributed fictitious play scheme. \n\n\n\n\u00a7 CONCLUSION\n\nIn this paper, we studied a Nash equilibrium seeking problem in games with a cost-relevant parameter. In order to learn the true parameter while finding the Nash equilibrium, we introduced a diminishing step-size sequence on the basis of the traditional non-Bayesian learning rule to ensure that the beliefs of agents on the parameter reach consensus. Then we combined the best response dynamics to update the strategies, and thus, proposed a novel distributed non-Bayesian rule. Using the strong law of large numbers and Toeplitz's lemma, we showed that agents' beliefs concentrate on the true parameter and the strategy profile converges to the true Nash equilibrium.  \nIEEEtran\n\n"}