{"entry_id": "http://arxiv.org/abs/2303.06661v1", "published": "20230312134332", "title": "Bayesian Size-and-Shape regression modelling", "authors": ["Antonio Di Noia", "Gianluca Mastrantonio", "Giovanna Jona Lasinio"], "primary_category": "stat.ME", "categories": ["stat.ME"], "text": "\n\t\n\n\n\n\n\n\n\n\n\n\n\n\nDepartment of Computer Science, ETH Zurich & Faculty of Economics, University of Lugano\n\nDepartment of Mathematical Sciences, Polytechnic of Turin\n\nDepartment of Statistical Sciences, Sapienza University\n\n\n\n\n\t\n\t\n\t\n\tBayesian Size-and-Shape regression modelling\n    Giovanna Jona Lasinio\n    \n============================================\n\n\n\n\t\n\t\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding on <cit.>, this note presents the Bayesian estimation of a regression model for size-and-shape response variables with Gaussian landmarks. Our proposal fits into the framework of Bayesian latent variable models and allows a highly flexible modelling framework.\n\n\n\n\nBayesian Size-and-Shape regression modelling\n    Giovanna Jona Lasinio\n    \n============================================\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nShape data naturally arise in many research fields whenever our attention focuses on the shape structure of objects. For example, in biology, there is great interest in modelling the shape features of organisms, relating them to environmental conditions. In <cit.>, a study about the sex differences in the crania of a macaque species is reported, and environmental features could be included as well. Magnetic Resonance data can be seen as size-and-shape data. Their analysis is helpful to assess specific features such as the Fetal alcohol spectrum disorder or Schizophrenia Magnetic Resonance images (see ).\nImage analysis and computer vision are other natural fields of application. Generally speaking, image recognition could be approached in different ways, and, among others, the shape analysis approach has often given successful results, e.g., digit recognition ().\nIn genetics, it is common to use electrophoretic gel images (), and shape analysis could be successfully adopted to analyse them efficiently.\nIn chemistry, it is fundamental to assess the geometric structure of molecules by using three-dimensional coordinates, e.g., it has been used to evaluate shape features of steroid molecules ().\nAnd in bio-informatics, an important task is Protein matching, i.e. aligning molecules to find common geometrical structures in them. Shape analysis could be adopted to deal successfully with these tasks ().\n\n\n\n\n\t\n\n\t\n\n\n\n\n\n\t\n\n\nRecent advances rely on the development of methodologies to capture and explain changes in the shape of objects. Some smoothing methods have been proposed in <cit.>, but to our knowledge, the only regression methodology for size-and-shape data  has been proposed by <cit.> in the marginal likelihood estimation framework.\nThis paper proposes an alternative Bayesian regression methodology for size-and-shape response data. The model fits into the Bayesian latent variable models' framework and should be an appealing alternative since it is not affected by numerical issues e.g. local optima of optimization procedures. The latter is because it relies on a simple and efficient MCMC posterior inference procedure. Moreover, it potentially allows a more flexible model's specification, for example, by adding dependence between landmarks. The posterior distribution estimation is carried out by means of Gibbs sampling and Metropolis-within-Gibbs algorithm. Theoretical results are derived and discussed, some simulation results are provided, and some concluding remarks are given.\n\n\n\n\u00a7 A BAYESIAN MODEL FOR SIZE-AND-SHAPE\n\nLet \ud835\udc17\u0303_i\u2208\u211d^(k+1)\u00d7 p, with i=1,\u2026 , n  be a (k+1)\u00d7 p dimensional configuration matrix, with k\u2265 p, that represents the Euclidean coordinates of k+1 landmarks (points of interest an object) in dimension p for the i-th recorded object, where p is usually equal to 2 or 3. To perform any size-and-shape inference, we have to remove information about the objects' location and orientation to model the data. Location information is usually removed post-multiplying by the Helmert submatrix  \ud835\udc07 <cit.>, with dimension (k+1)\u00d7 (k+1), which has non-zero entries only on the lower triangular part and\nhas j-th row equal to (-d_j,-d_j, \u2026, -d_j,jd_j, 0,\u2026, 0) where d_j = 1/\u221a(j(j+1)), obtaining the Helmertized configuration\n\n    \ud835\udc17\u0306_i = \ud835\udc07\ud835\udc17\u0303_i.\n\nThe matrix \ud835\udc17\u0306_i \u2208\u211d^k\u00d7 p is also called pre-form matrix. Following <cit.>, if we decompose \ud835\udc17\u0306_i using the singular value decomposition, i.e. according to\n\n    \ud835\udc17\u0306_i = \ud835\udc14_i\u0394_i\ud835\udc11\u0306_i^\u22a4\n\nwhere \ud835\udc11\u0306_i \u2208 O(p) i.e. belongs to the space of the p \u00d7 p orthogonal matrices, it could be readily proven that \ud835\udc11\u0306_i contains all the information about orientation and \ud835\udc18_i=\ud835\udc14_i\u0394_i  is the size-and-shape version of the original configuration  \ud835\udc17\u0306_i,  it represents the object of the inference and the data we are modelling.  An important point is that allowing \ud835\udc11\u0306_i \u2208 O(p) the reflection information is lost, and if we want to retain it, we have to assume that \ud835\udc11\u0306_i \u2208SO(p), i.e., \ud835\udc11\u0306_i is a rotation matrix with |\ud835\udc11\u0306_i| = 1, and SO(p) is the p-dimensional Special Orthogonal group or rotation group.\n\n\n\n\n \u00a7.\u00a7 The model\n\nLet us suppose that for each \ud835\udc18_i we have an associated vector of d covariates \ud835\udc33_i = (z_i1,z_i2, \u2026 , z_id)^\u22a4 and we are interested in the relation between \ud835\udc33_i and  \ud835\udc18_i. Unfortunately, the size-and-shape space, where \ud835\udc18_i lives, is a non-Euclidean manifold with a very complicated geometric structure that is not easy to handle, and even a distribution for \ud835\udc18_i cannot be easily specified. The idea proposed in <cit.>, which here we extend to a Bayesian setting, \nis to model this relation using the latent variable \ud835\udc11_i \u2208SO(p) \n\nand to define a regressive-type relation between \ud835\udc17_i=\ud835\udc18_i\ud835\udc11_i and \ud835\udc33_i in the following way:\n\n    vec(\ud835\udc17_i) \u223c\ud835\udca9_k p(  vec(\u2211_h=1^dz_ih\ud835\udc01_h), \ud835\udc08_p \u2297\u03a3),   i = 1, \u2026 , n,\n\nwith  \ud835\udc17_i \u22a5\ud835\udc17_i' if i \u2260 i', where  vec(\u00b7)  indicates the vectorization of a matrix, \u03a3  is a non singular k \u00d7 k covariance matrix and \ud835\udc01_h, h = 1, \u2026 , d, is a k\u00d7 p matrix of regressive coefficients. It should be noted that \ud835\udc11_i is latent and hence a non-observable variable, which must not be confused with \ud835\udc11\u0306_i which is the rotation matrix of the original data. \n\nThis latent  variable approach  avoids dependence on the rotation and leads to a proper inference that depends only on \ud835\udc18_1, \u2026, \ud835\udc18_n.\n\nTo specify our Bayesian proposal of the model, we need to introduce the prior distribution for the model's unknown. To facilitate the notation and the derivation of the full conditionals, we use \ud835\udc17_i,l and \ud835\udc01_i,l to indicate the l-th column of \ud835\udc17_i and \ud835\udc01_i, respectively, we let \n\u03b2_l=(\ud835\udc01_1,l^\u22a4,\ud835\udc01_2,l^\u22a4, \u2026, \ud835\udc01_d,l^\u22a4)^\u22a4\n\u03b2 = (\u03b2_1^\u22a4,\u2026 , \u03b2_p^\u22a4)^\u22a4 and we introduce the design matrix \ud835\udc19_i = \ud835\udc08_k\u2297\ud835\udc33_\ud835\udc22^\u22a4, such that \n\n    ((\ud835\udc19_i\u03b2_1)^\u22a4, (\ud835\udc19_i\u03b2_2)^\u22a4, \u2026 , (\ud835\udc19_i\u03b2_p)^\u22a4)^\u22a4 = vec(\u2211_h=1^dz_ih\ud835\udc01_h).\n\nWe then define the Bayesian model in the following way:\n\n    \ud835\udc17_i,l| \u03b2,\u03a3   \u223c\ud835\udca9_k(  \ud835\udc19_i\u03b2_l, \u03a3),      i = 1, \u2026 , n,     l = 1,\u2026,p, \n    \u03b2_l    \u223c N_kd(\ud835\udc0c_l, \ud835\udc15_l),\n    \u03a3   \u223c IW(\u03bd, \u03a8),\n\nwith \ud835\udc17_i,l|\u03b2,\u03a3\u22a5\ud835\udc17_i',l'|\u03b2,\u03a3 if i\u2260 i' or l \u2260 l' and where IW denotes the Inverse Wishart distribution.\nIt should be noted that (<ref>) and (<ref>), conditioning on \u03b2,\u03a3, identify the same distribution over the entire \ud835\udc17_1,\u2026 , \ud835\udc17_n, the conditioning is needed in (<ref>) since they are considered as random variables. \n\nIt must be remarked that\nan identification problem arises from the introduced model specification, which has not been pointed out by <cit.>. To make it evident, we show that the set of parameters {\ud835\udc01_1, \u2026, \ud835\udc01_d, \u03a3} and {\ud835\udc01_1\u039b, \u2026, \ud835\udc01_d\u039b, \u03a3} where \u039b\u2208SO(p) is a rotation matrix, induce the same density over (\ud835\udc14_1, \u0394_1, \u2026 , \ud835\udc14_n, \u0394_n). Let  \u03bc_i = \u2211_j=1^dz_ij\ud835\udc01_j and f indicates a density, we need to prove that \n\n    f(\ud835\udc14_i ,\u0394_i; \u03bc_i, \u03a3)/f(\ud835\udc14_i ,\u0394_i; \u03bc_i\u039b, \u03a3) = 1,     \u2200   i = 1,\u2026 , n.\n\nThe density of (\ud835\udc14_i, \u0394_i) is derived in Theorem 1 by <cit.> which  allows  us to compute (<ref>) that is \n\n    f(\ud835\udc14_i ,\u0394_i; \u03bc_i, \u03a3)/f(\ud835\udc14_i ,\u0394_i; \u03bc_i\u039b, \u03a3) = exp(-tr(\u03bc_i^\u22a4\u03a3^-1\u03bc_i) -  tr(\u039b^\u22a4\u03bc_i^\u22a4\u03a3^-1\u03bc_i\u039b)  /2).\n\nFrom the property of the trace operator, and since \u039b^\u22a4 = \u039b^-1, we have that \n\n    tr(\u039b^\u22a4\u03bc_i^\u22a4\u03a3^-1\u03bc_i\u039b) = tr(\u03bc_i^\u22a4\u03a3^-1\u03bc_i\u039b\u039b^\u22a4) = tr(\u03bc_i^\u22a4\u03a3^-1\u03bc_i),\n\nwhich shows that (<ref>) holds true for any i=1,\u2026,n. For this reason, an identification constraint is needed to  prevent any arbitrary rotation and/or reflection of \u03bc_i. This can be achieved by assuming that for one of the \ud835\udc01_h, e.g., the first, we have that \n\n    [\ud835\udc01_h]_wl = 0,     l>w,      [\ud835\udc01_h]_ll\u2265 0,     l = 1,\u2026, p-1.\n\nEquation (<ref>) is also a version of the transformation proposed by <cit.> to identify and isolate the size-and-shape information of the mean configuration \u03bc_i.\nThe constraints can be imposed in two different ways. The first one is by changing the prior distributions over the parameters that identify the constraints in (<ref>) accordingly. The other way is to let the Markov chain Monte Carlo (MCMC) algorithm explore freely the posterior and then remap  each \ud835\udc01_h^b, where b indicates the b-th posterior samples, to an identified version \ud835\udc01\u0303_h^b, via the map \n\ud835\udc01_h^b\u21a6\ud835\udc01\u0303_h^b:=\ud835\udc01_h^b\u039b^b,\nwhere \u039b^b = g(\ud835\udc01_h^b) \u2208SO(p) is an appropriate rotation matrix defined by a function g: \u211d^k\u00d7 p\u2192SO(p), such that  \ud835\udc01\u0303_h^b satisfies (<ref>). We prefer the latter since it allows us to define a more straightforward MCMC algorithm, as shown below. \n\n\n\n\n\n\n \u00a7.\u00a7 The Markov chain Monte Carlo algorithm\n\n\n\nTo implement the MCMC algorithm, we derive the full conditional distributions of \u03b2, \u03a3, \ud835\udc11_1, \ud835\udc11_2, \u2026, \ud835\udc11_n, i.e., the distribution of one parameter given all the others and the data. Owing to the model specification given in  (<ref>), we can easily see that the full conditional of \u03b2 and \u03a3 are the same that we would obtain in the case of a standard  Bayesian regression  (see, e.g. ), that is \n\n    \u03b2_l|\u03b2_-l,\u03a3, \ud835\udc11_1, \u2026 , \ud835\udc11_n ,\ud835\udc18_1, \u2026 ,   \ud835\udc18_n  \u223c N_kd(\ud835\udc0c_l^*, \ud835\udc15_l^*),\n\n\t\n    \ud835\udc0c_l^*  =   \ud835\udc15_l^* ( \u2211_i=1^n \ud835\udc19_i^\u22a4\u03a3^-1\ud835\udc17_i,l  +  \ud835\udc15_l^-1\ud835\udc0c_l ),   \ud835\udc15_l^*  = ( \u2211_i=1^n \ud835\udc19_i^\u22a4\u03a3^-1\ud835\udc19_i  + \ud835\udc15_l^-1)^-1\n\nwhere \u03b2_-l denotes the vector \u03b2 without the element \u03b2_l,\nand \n\n    \u03a3|\u03b2, \ud835\udc11_1, \u2026 , \ud835\udc11_n ,\ud835\udc18_1, \u2026 ,   \ud835\udc18_n   \u223c IW(\u03bd^*, \u03a8^*),\n\n\t\t\n    \u03bd^*  =  \u03bd + np,   \u03a8^*  = \u03a8 + \u2211_i=1^n \u2211_l=1^p(\ud835\udc17_i,l-\ud835\udc19_i\u03b2_l)(\ud835\udc17_i,l-\ud835\udc19_i\u03b2_l)^\u22a4.\n\n\t\nFor the derivation of \ud835\udc11_i's full conditional, we can resort to the computation reported in <cit.>, which show, in Theorem 1, that such distribution is Matrix Fisher  <cit.> with parameter \ud835\udc00_i = \u03bc_i^\u22a4\u03a3^-1\ud835\udc18_i,  and density \n\n    f(\ud835\udc11_i|\u03b2, \u03a3,\ud835\udc11_1, \u2026 , \ud835\udc11_i-1, \ud835\udc11_i+1, \u2026, \ud835\udc11_n ,\ud835\udc18_1, \u2026 ,   \ud835\udc18_n ) \u221dexp(tr(\ud835\udc11_i\ud835\udc00_i^\u22a4)).\n\nSampling from a Matrix Fisher distribution is generally not easy and requires acceptance-rejection methods, as described in <cit.>.  Interestingly, the case p=2 can be easily handled by expressing the rotation matrix as a function of the rotation angle \u03b8_i \u2208 [0, 2\u03c0), which can be shown to be von-Mises distributed with parameters (\u03b7_i,\u03ba_i ) that can be derived setting the equation \ud835\udc11_i\ud835\udc00_i^\u22a4 = \u03ba_i cos (\u03b8_i - \u03b7_i). The case p=3 can be handled by expressing \ud835\udc11_i with the Euler angles \u03b8_1\u2208[0,2\u03c0), \u03b8_2\u2208 [0,\u03c0), \u03b8_3\u2208[0,2\u03c0) by means of a 3-dimensional rotation map, and using a Metropolis step. \n\nRemark that the full conditionals are easily obtained and in the setting that we propose, any identifiability issue is solved relatively straightforwardly. This is a clear advantage of the Bayesian approach, which allows much more numerically stable estimation procedures. \n\n\n\n\u00a7 SIMULATION EXPERIMENTS\n\nBy means of a simulation experiment, we test the algorithm  when p =2 and 3, with  k=3  and d=1, meaning that \ud835\udc33_i=1, \u2200 i=1,\u2026,n and  \u03bc_i = \u03bc, with \nvec(\u03bc)=(\u03b2_1^\u22a4,\u03b2_2^\u22a4)^\u22a4 if p=2 and\nvec(\u03bc)=(\u03b2_1^\u22a4,\u03b2_2^\u22a4,\u03b2_3^\u22a4)^\u22a4 if p=3.\nHence, we are modeling the data assuming that the linear model includes only an intercept and \u03bc can be seen as a mean configuration. \nWe set \u03b2_1 = (60,1,100)^\u22a4, \u03b2_2= (10,30,180)^\u22a4, \u03b2_3= (20,400,0.5)^\u22a4 and \u03a3=\u03ba\ud835\udc08_k and, for each p, we generate  scenarios with all possible combination of \u03ba=0.1,0.3 and sample sizes n=20,50,100,300. \n\nThe algorithm is implemented assuming \ud835\udc0c_l = 0_3d,  \ud835\udc15_l = 10^6\ud835\udc08_3d, \u03bd = k+1 and  \u03a8 = I_k and estimated using   5000 iterations of which 2000 are kept for inferential purposes. The rotation matrices are simulated using a Metropolis step.\n\nTo evaluate the performance of the method, we report the size-and-shape Riemannian distances (see ) denoted by \u03c1_p between the true mean configuration \u03bc and the estimated one, i.e., the posterior mean of \u03bc.\n\nThe results are reported in Table <ref>, where we empirically verified the consistency of the model and, as expected, the error tends to increase as the variability of the data increases. Moreover, for all datasets, the true value of parameters for each element of \u03b2_l and \u03a3 was included in the associated 95% credible interval.\n\n\n\n\n\u00a7 FURTHER DEVELOPMENTS\n\n\nIn this paper, we showed how a size-and-shape regression model is easily implemented in a Bayesian latent variable framework, we discussed the model's identifiability issues and how to solve them.\nThe present work opens new possibilities in modelling size-and-shape data, especially in the formalization of complex dependence structures, which are possible to handle under the Bayesian setting. Hence, we can  imagine dependence structures among landmarks and future developments will see us introducing lattice-type modelling approaches into this field, where landmarks will be treated as locations. In this way, their dependencies will be described using spatially structured covariance matrices. We will also try to account for temporal dependence between observations, allowing us to inspect and model the temporal evolution of shape <cit.>. To conclude, as the linearity assumption could be too restrictive, a more flexible functional relation between the covariates and mean configuration could be introduced in order to account for deviations from linearity. \nelsarticle-harv \n\n\n"}