{"entry_id": "http://arxiv.org/abs/2303.06748v1", "published": "20230312205126", "title": "DTT: An Example-Driven Tabular Transformer by Leveraging Large Language Models", "authors": ["Arash Dargahi Nobari", "Davood Rafiei"], "primary_category": "cs.DB", "categories": ["cs.DB"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndargahi@ualberta.ca\n\n\n\n\n\n  University of Alberta\n\n  Edmonton\n  Alberta\n  Canada\n\n\n\n\ndrafiei@ualberta.ca\n\n\tUniversity of Alberta\n\tEdmonton\n\tAlberta\n\tCanada\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMany organizations rely on data from government and third-party sources, and those sources and organizations do not follow the same data formatting. This introduces challenges in integrating data from multiple sources. Commercial database systems do not offer adequate support for integrating data from heterogeneous sources, and manual integration is both time-consuming and inefficient. While state-of-the-art approaches rely on similarity functions and textual transformations, they often fail to handle challenging cases where multiple mappings are required, or the mappings go beyond simple textual transformations.\n\nIn this paper, we study the potential of deep neural models for transforming tables for joinability. In particular, we cast the problem as a prediction task and develop a framework that leverages large deep-learning language models to transform tabular data from a source formatting to a desired target representation. Our framework can efficiently learn the pattern for mapping the source formatting into the expected target using just a few examples, which can then be used for table joining, filling in missing values, and error detection.\nCompared to state-of-the-art mapping and joining approaches, our framework delivers noticeably more accurate and scalable performance on both real-world and synthetic datasets. Our experimental evaluation also shows that the performance of the proposed framework using our fine-tuned model is at par or better than large language models such as GPT-3, despite the significant difference in size, and that integrating large language models into our framework improves their performance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDTT: An Example-Driven Tabular Transformer by Leveraging Large Language Models \n    Davood Rafiei\n    March 30, 2023\n===============================================================================\n\n\n\n\n\n\n\n\nPVLDB Reference Format:\n\n. . PVLDB, (): , .\n\nhttps://doi.org/doi:\n[This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit <https://creativecommons.org/licenses/by-nc-nd/4.0/> to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing mailto:info@vldb.orginfo@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. \n\nProceedings of the VLDB Endowment, Vol. , No.  \nISSN 2150-8097. \n\nhttps://doi.org/doi: \n\n]footnote-1\n\n\n\n\n\n\nPVLDB Artifact Availability:\n\nThe source code, data, and/or other artifacts have been made available at <>.\n\n\n\n \n\n\n\n\u00a7 INTRODUCTION\n\nThe drive towards data publishing and sharing by entities and governments over the past couple of years has led many organizations to rely on data from third-party sources. However, gathering data from multiple sources inevitably leads to data mismatches. Converting data from one format to another has long been a challenge in data integration and management\u00a0<cit.>, with traditional approaches relying on manual development of guidelines and rules for integration or transformation\u00a0<cit.>. However, the sheer size and complexity of modern databases make manual transformation and integration impractical, fueling a significant research on automatic data transformation\u00a0<cit.>. \n\n\n\n\nOur focus in this paper is on automated transformation of tabular data such as spreadsheets, web tables, and relational databases, which is widely adopted by both organizations and governments for representing, storing, and sharing data.\n\nIn particular, given a few examples of matched rows between a source and a target, the goal is to learn a mapping. Those mappings can then be used to transform arbitrary rows in the source formatting into the formatting of the target, with applications in joining data from different sources\u00a0<cit.>, filling in the missing values and auto-completion\u00a0<cit.>, and error detection and correction\u00a0<cit.>.\n\n\n\n\nFigure\u00a0<ref> depicts a source and a target, representing the same entities (people) but in different formattings. The source table shows the names of the individuals, while the target table shows their corresponding user ids. Suppose that the target column is incomplete or unavailable, and the objective is to predict the missing values of the target column, based on a few examples of source-target pairs. Or, one may want to join the two columns despite the differences in formatting. The transformation process requires some reformatting rules and the choice of a rule may be conditional on the input. For example, in the first, third, and seventh rows, the reformatting rule involves concatenating the initial and the last name, converting them to lowercase, and using a period as a separator. However, in the second row, there is a middle name, while the last row lacks a first name, and these variations can affect the transformations.\nIn general, detecting such transformations from a set of examples is not straightforward and must account for various challenges.\n\n\nChallenges\nThe search space for possible transformations is huge. If each transformation is synthesized into a sequence of edit operations, the search space grows exponentially with the number of edit operations in a transformation and the parameter space of the operations. Also, both the search space and the runtime for many state-of-the-art approaches\u00a0<cit.> further grow with the input size, such as the number of rows and the length of a row. Despite some attempts to reduce the search space by limiting the number of operations (e.g. split and substring) within a transformation\u00a0<cit.>, sampling the input\u00a0<cit.>, and applying pruning strategies\u00a0<cit.>, the search space still remains relatively large. Hence the time needed to find a mapping is often much more than what may be considered as acceptable, for example, in an online setting. \nAlso, some of these improvements and prunings are lossy, and they can miss transformations that are of a better quality than those found.\n\n\nAnother challenge is the availability of input examples and noise handling.\nThe examples are usually either user-provided or automatically generated from input. In the former, the examples are (extremely) limited but accurate, whereas in the latter, the examples can be extensive, but less accurate. In real-world settings, noise is usually unavoidable and inconsistencies can exist in data. Also when examples are automatically generated, some of them can be incorrect. \nA good model should perform well with only a limited number of examples, and it should also benefit from the abundance of examples, maybe ignoring those that are less useful.\nA good model should also be robust against any possible noise in data and deal with inaccuracy in the provided examples.\n\nExisting Approaches\nA wide array of studies target the problem of matching entity descriptions or records that describe the same real-world entities but differ in terms of formatting or representation\u00a0<cit.>. Traditional approaches rely on textual and semantic similarity, whereas more recent approaches incorporate machine learning and deep neural models. While these models provide effective solutions to the problem of formatting mismatch in joining tabular data, their use cases are limited. For example, these models cannot predict or rectify missing values, provide suggestions, or detect outliers in a table. This has led to another line of research where the focus is on finding a mapping between source and target tables and leveraging this mapping to transform the source formatting into that of the target.\n\nThe majority of approaches aimed at detecting a mapping between two tables rely heavily on a limited set of string-based transformations\u00a0<cit.> and an exhaustive search of the parameter space. \nWhile the search space can be bounded by limiting the number of string-based transformations, this can negatively impact the accuracy.\nConsider the source and target tables in Figure\u00a0<ref>, where different rows may require different formatting rules. To transform all rows in the source to their corresponding target values, six distinct textual transformations may be needed, as illustrated in Example\u00a0<ref>. Some studies\u00a0<cit.> limit their search space and find a single transformation that covers all input rows, which will not be effective in this scenario. Other studies\u00a0<cit.> can produce more than one transformation, but the problem of selecting a transformation from the set to apply to an arbitrary row is left unanswered. For instance, Nobari\u00a0et\u00a0al.\u00a0<cit.> provide a set of transformations that are required for a mapping but do not provide much hint on how to select a transformation for an input row.\nFurthermore, many state-of-the-art methods\u00a0<cit.> exhaustively search the transformation space, and despite their pruning strategies, their runtimes increase dramatically when the input size grows\u00a0<cit.>.\n\nOur Approach\nIn this paper, we introduce Deep Tabular Transformer (DTT), a novel framework for transforming tabular data into a joinable format using the power of deep learning for language modeling. Unlike traditional approaches that rely on a limited set of pre-defined string-based transformations and an exhaustive search process, DTT overcomes these limitations by leveraging advanced deep learning techniques. DTT predicts an expected output row in the target table for each row of the source table, enabling easy and efficient data joining.  \nOur experimental results show that DTT outperforms existing state-of-the-art approaches in terms of accuracy, is applicable to a larger set of tables, and maintains an outstanding runtime performance even when dealing with large input size.  Remarkably, the performance of DTT is at par or better than large language models such as GPT-3 despite having an order of magnitude less parameters and requiring dramatically less resources during inference. We are releasing DTT as a pretrained model, which demonstrates exceptional performance across multiple domains without the need for fine-tuning. Our hope is that this release will drive further advancements in the field.\n\t\n\nOur contributions can be summarized as follows:\n\n\t\n  * We propose DTT, a novel example-driven approach for tabular data transformation leveraging pretrained language models.\n\t\n\t\n  * We develop a diverse dataset for training our model, comprising of synthetic examples. Our experiments demonstrate that our model performs exceptionally well on real-world data from various domains.\n\t\n\t\n  * We present an end-to-end framework for tabular data transformation which includes a decomposer, serializer, model, and aggregator. As an application of our framework, we demonstrate its effectiveness in table joining.\n\t\n\t\n  * We conduct extensive evaluation on a wide range of datasets from different domains and show that our approach outperforms existing state-of-the-art baselines in terms of both accuracy and runtime.\n\t\n\t\n  * We make all our resources, including our code, framework, pretrained model, synthetic data generator, and real-world benchmarks publicly available for the research community.[https://github.com/arashdn/dtt]\n\t\n\n\n\n\n\n\u00a7 PROBLEM DEFINITION\n\n\nWe want to transform tables from a source formatting to a target formatting using a few provided examples.\nLet S={s_1, s_2, \u2026} denote a set of values in the source. For a small subset S^'\u2282 S, let E={(s_i,t_i) | s_i \u2208 S^'} denote a set of k examples where the target values are given to guide the process for finding a transformation. The aim is to find the target formatting of every value in the source, i.e.\n\n    R = { (s_i, f(s_i)) | s_i \u2208 S \u2227\u2200 s_j \u2208 S^' ((s_j, f(s_j) \u2208 E)}.\n\n\n\nAs an example, suppose we have a source table S that lists the recent prime ministers of Canada and an example set E that consists of three rows:\n\n\n \n\nOur aim is to find the target formatting  for any arbitrary value in S. For instance, the values  and  may be mapped to  and  respectively.\n\n\nTables can be transformed for joinability, for example, allowing a column of a source table to be joined with a column in the target. Tables may also be transformed to fill in missing values in a target column. In both cases, S can be the set of all values in the source column.\nIn this study, we assume the source values and examples are provided. This a common practice to limit the scope of the problem and focus on data transformations\u00a0<cit.>. If user-provided examples are not available, an unequal joining method\u00a0<cit.> or token-based example generation\u00a0<cit.> may be used to provide a set of examples, with the caveat that the automatically generated examples may contain noise and invalid pairs. We will discuss how our approach can deal with such noisy examples.\n\n\n\n\n\u00a7 BACKGROUND AND RELATED WORK\n\nOur work is related to the lines of work on (1) example-driven tabular data transformation, (2) language modeling and text-to-text transformers, and (3) language models applied to tabular data. We review the relevant recent works in these areas while also providing some background (when necessary).\n\n\n\n \u00a7.\u00a7 Example-Driven Tabular Data Transformation\n \nThis is probably the closest line of work to ours.\nThere are numerous studies in this area\u00a0<cit.>, and FlashFill\u00a0<cit.> and BlinkFill\u00a0<cit.> are among the pioneers, with a focus on spreadsheet data. These two approaches construct an input graph based on a given set of user-provided examples, which is then traversed to generate a sequence of substring-based textual transformations that map source values to their corresponding targets in the input examples. However, FlashFill and BlinkFill heavily rely on the accuracy of the provided examples and are unable to handle noise in the examples. To address this issue, Zhu\u00a0et\u00a0al. propose a method called Auto-join\u00a0<cit.>, which uses a set of pre-defined string-based transformation units, such as substring and split, to describe the transformations. The examples are automatically generated by token matching, and the method creates several subsets of the input examples to handle noise. A recursive backtracking algorithm is then applied to each subset to find the best transformation. While Auto-join is able to handle minor noise in the input and limits the search space by using pre-defined transformation units, it is a backtracking method and needs to search the entire transformation space in the worst case, which can be computationally expensive. Also, it may not perform well if the noise level in the examples is significant. \n\nIn a more recent work by Nobari\u00a0et\u00a0al., referred to as Common String-based Transformer (CST), the search space for string-based transformations is further constrained by considering common text sequences between source and target examples as textual evidence to form the skeleton of transformations. CST uses the same string-based transformation units as Auto-join, but transformations for each row are generated independently to better handle input noise. The transformations are then ranked based on their coverage to build a final transformation set. While CST offers better noise handling and runtime performance compared to Auto-join, it is still limited to substring-based transformation units and performs well only when long matching sequences exist between source and target examples. \nWhile the pruning conditions in Auto-join and CST limit their search space and improve their runtime, they can end up missing some transformations, particularly those that cannot be covered by a small set of pre-defined transformation units. Our aim is to overcome these limitations on the search space by utilizing a Language Model (LM) to transform source values into the desired target representation. \n\n\n\n\n \u00a7.\u00a7 Language Modeling and Text to Text Transformers\n\nWith large language models forming an integral component of our framework, we provide a brief background of those models.\nA vast majority of machine-learned LMs are based on the concept of masked language modeling, where some tokens in a given sequence are masked (or corrupted), and the model is trained to predict those masked tokens.\nWord2Vec\u00a0<cit.> and GloVe\u00a0<cit.> are among the earliest models for pretraining, which generate static vectorized embeddings of each word using a shallow neural network. A later work, ELMo\u00a0<cit.> uses two layers of bidirectional LSTM\u00a0<cit.> to observe the context before and after the word and generates contextualized embedding of the words, unlike the static embeddings in Word2Vec. In recent years, Vaswani\u00a0et\u00a0al.\u00a0<cit.> introduce transformers that use self\u00a0attention\u00a0<cit.>, allowing the model to parallelize better than LSTM models and not giving more weight to nearby words. Transformer-based models consist mostly of an encoder, a decoder\u00a0<cit.>, or both. Encoder-only models, such as BERT\u00a0<cit.>, aim to learn the natural languages and generate a latent representation of the input that can be used for tasks requiring an understanding of the input. Decoder-only models, such as GPT-2\u00a0<cit.> and GPT-3\u00a0<cit.>, are widely used to generate natural language text given a context. Finally, Encoder-Decoder models, also referred to as sequence-to-sequence or text-to-text models, such as T5\u00a0<cit.> and BART\u00a0<cit.>, use an encoder to create a latent representation of the input, which is passed to the decoder to generate a new text for a desired task.\n\n\n\n \u00a7.\u00a7 Language Models Applied to Tabular Data\n \nThe rise of pretrained language models has led to their increasing use in various tasks, including those involving tabular data. In particular, these models are being applied to \ntasks such as entity matching\u00a0<cit.>,\ntext to SQL\u00a0<cit.>,\nquestion answering\u00a0<cit.> and \ndata to text\u00a0<cit.>. \nSince many deep learning and NLP models can only process data as a sequence of tokens, \ntable serialization has become a common module in many of these tasks.\nSeveral serialization techniques have been developed to transform tables into sequences of tokens\u00a0<cit.>, while preserving the structural relationships that may be needed for these tasks. Since the relationships that need to be preserved can be task-dependent, various serialization methods are used in the literature. For example, Iida\u00a0et\u00a0al.\u00a0<cit.> pass the rows and columns as two separate sequences to two transformer blocks and average the row and column values for each cell to generate a cell representation. In RPT\u00a0<cit.>, tables are serialized using two special tokens,  and , to encode attribute names and their corresponding values respectively. While this serialization keeps the structural information about the tables, it is not very efficient as the attribute names are repeated in each row of the table. Our aim is not to generate a dense representation of the entire table, and this requires a different serialization approach, which we discuss in Section\u00a0<ref>.\n\n\n\n\n\n\n\u00a7 APPROACH\n\nAs depicted in Figure\u00a0<ref>, our framework consists of a few components: (1) a decomposer and serializer, which decomposes the problem into smaller subtasks and performs an input serialization; (2) a tokenizer, which performs tokenization to obtain a vectorized representation of the input; (3) a sequence-to-sequence model, which predicts an output for each subtask; and (4) an aggregator, which is responsible for combining the predictions of the subtasks to generate a  final prediction.\n\nIn the rest of this section, we will discuss the details of those components.\n\n\n\n \u00a7.\u00a7 Decomposer and Serializer\n\n\n\n\nGiven a set of rows S from a source table and an example set E of source-target row pairs, the aim is to transform every row s_i \u2208 S to a target based on the examples. Many large language models impose a limit on the length of the input. This limit usually varies from 512 tokens (e.g. BERT) to 2048 tokens (e.g. GPT-3) and is associated with the quadratic memory requirement of the self-attention mechanism with input length\u00a0<cit.>. However, the encoding of a table can be much longer for large tables and when there are many examples. To reduce this dependency on input length, we decompose the problem into smaller tasks, with each task being small enough to easily fit the input length requirement of many language models. This decomposition process is discussed in this section and the aggregation of the results is discussed in Section\u00a0<ref>.\n\n\nSuppose the number of examples that describe the context in a sub-problem is set to two. For any arbitrary row in the source table, any subset of E of size two can be selected as the context. Let E^2 denote the set of all subsets of E\nof size two, i.e.\n\n    E^2 = {(s_1, o_1), (s_2, o_2) | (s_1, o_1) \u2208 E \u2227 (s_2, o_2) \u2208 E \u2227 s_1 < s_2}.\n\n\n\n\nFor each input row s_i \u2208 S to be transformed, there are |E^2| possible contexts that can be chosen.\nAs an example, consider sets S and E in Section\u00a0<ref>. The set E^2 of all subsets of size two of E will be\n\nand an encoding of the input  \u2208 S using one of these contexts is \n.\nEach input row s_i can be fed to the model multiple times, each time with a different context. If the input is passed to the model n times, each time with a different context, the model will predict n possible targets, which can be aggregated as discussed in Section\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n\n\n\nIt is common to use special tokens to mark the beginning and the end of sentences in natural language as it helps with training a model. The same convention is followed in encoding tabular data to describe the relationships between different input fields.\nFollowing this convention, we separate the source and target in an example with a  token and two examples with . We also mark the beginning of input with  and the end of input with .\nWith these symbols, our example given earlier can be encoded as:\n, and the expected label is .\n\nIn general, the size of a sub-problem can vary depending on the lengths of the records in source and target, the length limitation of the large language model being employed, and maybe the complexity of transformations. In our case, each example consists of two rows, a source and a target. Assuming that the input consists of k examples and a source row to be transformed, and using a language model that takes 512 tokens, the length of each row is limited to \u230a 512/(2k+1)\u230b tokens, ignoring special tokens and separators. Also, more complex transformations generally require more examples to better describe the operations. For instance, consider the example  in the context of the example given in Section\u00a0<ref>. With only one example, one cannot tell if the letter  in the target is derived from  or . However, with two examples, there is less chance of an ambiguity. Unless explicitly stated otherwise, we set the number of examples in our contexts to two.\n\n\n\n \u00a7.\u00a7 Tokenizer and Model\n\n\nAs large language models expect vectors as inputs, the input needs to be tokenized and each token is assigned a vector. Even though tokenization is considered as the first step in many NLP pipelines, the choice of tokenization is less obvious when working with tabular data.\nConventional NLP models (e.g. word2vec, GloVe) use the vocabulary words as the unit for tokenization, and out-of-vocabulary tokens are usually all collapsed into an unknown token. Recent deep models utilize a more efficient tokenization that may better handle the morphological structure of the words, out-of-vocabulary words, or low-resource languages. Transformer-based language models\u00a0<cit.> generally use either WordPiece\u00a0<cit.> or Byte Pair Encoding (BPE)\u00a0<cit.> tokenizations, where more frequent consecutive byte pairs are represented as new symbols in the vocabulary (analogous to text compression techniques). In both cases, words are broken down into subword tokens based on the frequency of each subword in the language. As a result, common words are represented as a single token whereas rare words are broken down into multiple tokens.\n\n\nHowever, in our problem setting, a subword-level tokenizer may not be the right choice. Subword tokenizers are mainly optimized to pick subwords of natural language or words in the input domain while in tabular data, the values can be from any domain including words not from a specific language. Understanding meaning and semantics of the words or splitting them into smaller meaningful parts is not essentially helpful in predicting the output as each character may independently contribute to the output value.\nFor instance, consider the pair , where the first character  in  is producing the letter  in . In a different example pair, , the word  is used in the output as a single token. A pretrained subword-level tokenizer may not be the best choice for such input tokenization. \nA similar problem arises in low-resource languages that lack enough data for training a tokenizer. It is shown that character- or byte-level tokenizers work better in those settings\u00a0<cit.>.\nOn the same basis, we adopt byte-level tokenizer ByT5\u00a0<cit.> in our work.\nRecent work has shown that byte-level models are competitive with their subword-level counterparts\u00a0<cit.>, especially in tasks dealing with short-length texts. \n\nGenerally, table cells store short-length content and our serialization technique also generates short-length contexts with only two examples. Taking this into account, we use a byte-level UTF-8 encoder as the tokenizer, which benefits from the accuracy of character-level tokenizers and maintains a relatively short input length passed to the model.\n\nWith the input represented as a sequence of tokens, the problem becomes a text-to-text transformation, where a suitable choice for the model architecture is a sequence-to-sequence model that comprises an encoder and a decoder block. Recent models are stacking a same number of transformer\u00a0<cit.> layers for both encoder and decoder. However, it is shown that when the input is a sequence of characters, using a deeper encoder containing more layers of transformers, referred to as unbalanced architecture, performs better than a balanced model\u00a0<cit.>. ByT5\u00a0<cit.> is a recent byte-level text-to-text model with an encoder block three times deeper than the decoder block, which we use as a starting point for the training process. Unlike the original model, which masks parts of the output, we mask all characters in the target, and the training objective is to predict the masked bytes. The decoder is an auto-regressive decoder, and only the initial token, , is passed to the decoder.\nIn the next sections, we will delve into the details of passing the input and predicting an output.\n\n\n\n\n \u00a7.\u00a7 Aggregator\n\n\nWe have decomposed the problem of transforming a table into a set of smaller tasks (see Section\u00a0<ref>), where each of these tasks is carried out using a sequence-to-sequence model, as discussed in Section\u00a0<ref>. To exploit all provided examples in the prediction process, each input is fed into the model multiple times, each time with a different context. If we denote the number of trials with n, the model will predict n target candidates, denoted as O_i = {o_i1\u2026 o_in}, for each row s_i \u2208 S in the source.\n\nIn an ideal setting where there is no noise or inconsistency in the data and the model performs with no error, all of the predicted values for a specific source s_i should be the same, i.e. o_i1 = o_i2 = \u2026 = o_in. However, inaccurate examples, noisy input rows, and inconsistencies among multiple rows can lead to different predictions for a particular source row. It should be noted that due to the limitations in the model's input length, it is not feasible to pass the entire example set to the model, and instead, we create various subsets, each of which is treated as an independent problem. While noise in the examples may affect the output in some subsets, we ensemble the outputs generated under different contexts to obtain the best possible output. Consequently, the predicted target t_i for the source s_i can be estimated as\n\n    t_i = o_ij\u2208 O_iargmax  P(C_i | o_ij),\n\nwhere C_i \u2286 C is a subset of contexts that may include example sets that are relevant to source s_i, and C_i may also be limited in size, for example to n. By applying Bayes' theorem, we have\n\n    P(C_i | o_ij) = P(o_ij | C_i) P(C_i) /P(o_ij).\n\nAssuming a uniform prior probability P(o_ij) for the predictions and treating P(C_i) the same for all predictions, these terms can be ignored, and P(C_i | o_ij) \u221d P(o_ij | C_i) can be used as a proxy for finding the argmax. Also, assuming independence among predictions, it is possible to use the maximum likelihood estimation to calculate P(o_ij | C_i), i.e.\n \n    t_i = o_ij\u2208 O_iargmax  P(o_ij | C_i) \u221d|o_ij|/|O_i|\n\nwhere |o_ij| is the frequency of o_ij in O_i and |O_i| is number of possible predictions.\n\n\n\n \u00a7.\u00a7 Downstream Tasks\n\n\nGiven a source row and a set of source-target example pairs, the proposed model generates a target row following the examples. This framework can be useful in many downstream tasks such as auto-completion and auto-filling spreadsheets\u00a0<cit.>, predicting missing values, error correction\u00a0<cit.>, and joining\u00a0<cit.>. In this section, we review two particular tasks: (1) filling missing values, and (2) joining heterogeneous tables.\n\n\n\n  \u00a7.\u00a7.\u00a7 Filling missing values\n\nIt is common to have missing values in real-world tables\u00a0<cit.>, and sometimes those missing values can be predicted from the values of other columns\u00a0<cit.>.\nConsider a scenario where two columns s and t are given, and column t has some missing or incorrect values. Those columns can be from the same or different tables. If there exists a mapping relationship from s to t, our approach may be used to fill in the missing values. In this case, the given samples in s and t where the values of both columns are present may serve as examples from which a mapping is learned. The examples can then be utilized by the model as context to find the missing or incorrect values in s. \n\n\n\n\n  \u00a7.\u00a7.\u00a7 Joining heterogeneous tables\n\nConsider a join scenario where a source table S and a target table T must be joined, based on some columns that are not formatted the same but there is a mapping from source to target. Examples of mappings may be provided by the user or obtained automatically\u00a0<cit.>. The model can be invoked to generate a target f(s_i) for each source row s_i, utilizing the examples as discussed earlier.\nUnlike the case for filling missing values where an exact prediction of the missing value is needed, an exact prediction is not necessary for a join.\nInstead, the goal is to use f(s_i) as a bridge to establish a connection between rows in S and T. For instance, under the setting where each value in the source is matched with a single value in the target (e.g. a primary-foreign key relationship), one needs to find the closest match in T for f(s_i). \nThis allows small discrepancies between predicted and target values, without affecting the join. \nThere is a great chance that a significant string similarity exists between a model predicted value f(s_i) and the corresponding value in T. In many cases, this similarity is enough to perform the join. Therefore, for each (s_i, f(s_i)) pair, we can select t_j \u2208 T such that it yields the minimum edit distance between the two strings. This can be formalized as follows:\n \n    m_i = t^j \u2208 Targmin  edit_dist(f(s_i), t^j)\n\nwhere m_i is considered a match in the target for s_i. The approach can be generalized to cases where a value in the source is matched with either no values or multiple values in the target. To allow such many-to-many joins, one may set lower and upper bounds for the edit distance instead of aiming for the minimum distance.\n\n\n\n\u00a7 EXPERIMENTS AND ANALYSIS\n\nIn this section, we evaluate our proposed model and analyze its performance under different settings. We also discuss our training data generation and the process of training our model.\n\n\n\n \u00a7.\u00a7 Dataset for Training DTT\n\n\nPretrained language models are generally trained on large text corpora, and a common approach for training them involves  masking out a word and predicting it, as seen in popular models such as T5\u00a0<cit.>, BERT\u00a0<cit.> and GPT-2\u00a0<cit.>. By using this approach, large amounts of unlabeled data can be utilized to train the model. Nonetheless, our particular task requires a vast set of source and target examples, grouped according to the transformations that map source examples to their corresponding targets. To the best of our knowledge, such a dataset is not currently available, and our experiments have shown that even advanced generative models pretrained on natural language text, such as T5 and GPT-2, are not capable of performing this task without extensive fine-tuning and training. This is because entries in real-world tables are typically short and have little relevance to other entries in the same column, aside from sharing the same domain (e.g. individual names). As a result, the prior language knowledge of these general models is less likely to be useful for this task.\nTo address this challenge, we propose generating synthetic data to train the model. Before delving into the details of data generation, however, it is important to first review the desired features of the training data.\n\n\n\n  \u00a7.\u00a7.\u00a7 Training data features\n\nThe training data must possess several key features.\nFirst, it should be organized as source-target pairs, categorized by their corresponding transformations, as previously discussed. It is worth noting that the mapping function can be general, as the model does not need to know the function itself; rather, it only requires the output of the function for the source examples and that the generated examples by the same mapping are grouped together.\nSecond, the dataset must be sufficiently large to train a model with hundreds of millions of parameters, which is typical for many language models.\nThird, the dataset should cover a broad range of textual transformation patterns and various input lengths.\nFinally, the generated data should not be limited to the words in any specific language since many terms \n in table cells are not dictionary words and may not be limited to a particular language. \n \n\nOverall, the primary purpose of training data in our case is guiding the model to understand the mapping corresponding to a set of source-target example pairs. In this context, different combinations of edit operations can be performed on the source examples to generate the target outputs. Unlike NLP models that rely on understanding the syntax and the semantics of input, our model primarily focuses on discovering textual patterns and string operations. Hence,  character-level tokenization is preferred in our case. In the rest of this section, we will delve into the process of generating a synthetic dataset to train our model. \n\n\n\n\n  \u00a7.\u00a7.\u00a7 Training data generation\n\nTo generate our synthetic dataset we first build a set of textual transformations, denoted as T, each consisting of a sequence of basic transformation units. \nWe use the basic transformation units in Nobari\u00a0et\u00a0al.\u00a0<cit.>, which include , , , , and .  These units have their natural meanings:  selects a portion of the input based on start and end parameters,  breaks the input by a given  character and selects one part,  returns a constant, and  and  return the lowercase and uppercase forms of the input respectively. Each unit copies either parts of the input or a literal to the output, and the output of a transformation is the concatenation of the outputs of its units.\nWe randomly choose the units, parameters, and the length of each transformation in terms of the number of units, to provide a diverse set of examples. While the aforementioned transformations are expected to cover many cases of transformations in real-world settings\u00a0<cit.>, our aim is not to limit the model to learning a fixed set of transformations. Our findings indicate that, with sufficient independent training examples, the model can learn to find any necessary transformation even with a limited set of pre-defined transformations.  The construction of transformations mainly helps us group input examples that follow a same mapping, but the model is not aware of the transformations and uses regular cross-entropy loss at the character level to learn a mapping that transforms the source into the target. \n\nThe transformations in Nobari\u00a0et\u00a0al.\u00a0<cit.> and Zhu\u00a0et\u00a0al.\u00a0<cit.> do not allow stacking of the units where one unit is applied on top of another unit. For the same reason, they introduce complex transformation units such as  which stacks substring on top of split, with the output of one operation fed to the other. \nInstead of introducing many such new units, we allow random stacking of up to three transformation units. The stacking here refers to passing the output of one transformation unit to another one. Since our units include lower case and upper case transformations, the case of input may change in some transformations and not others. \n\nFor each transformation tr \u2208 T, a set of examples is generated. To create these examples, a source text is randomly generated consisting of a mix of alphabetic and numeric characters, symbols, and special characters. The length of the input is selected at random. The transformation tr is then applied to source texts to generate a set of examples, denoted as I_tr = { (s_i, t_i)}_1 \u2264 i \u2264 u. Using random text instead of dictionary words avoids any potential bias towards natural language words and grammatical structures. To form example sets, subsets of size 3 are selected from I_tr. Each example set is then serialized, as discussed in Section\u00a0<ref>, with the target of the last example masked and labeled as the target for use in forming context sets for model training. \n\n\n\n\n\n\n \u00a7.\u00a7 Dataset for Evaluation\n\nTo evaluate the effectiveness of our approach and compare its performance with state-of-the-art baselines, we use two real-world datasets as well as four synthetic datasets. In what follows, we provide a detailed explanation of each dataset.\n\nWeb Tables Dataset (WT) This benchmark was initially introduced by Zhu\u00a0et\u00a0al.\u00a0<cit.> and was also used as a benchmark in Nobari\u00a0et\u00a0al.\u00a0<cit.>. The dataset includes 31 pairs of tables from 17 distinct topics, with an average of 92.13 rows per table and an average length of 31 characters per input source. The tables were sampled from Google Fusion tables by identifying tables that appear in the results of the same queries but are formatted differently. This benchmark contains natural noise and inconsistencies, and not all entities can be transformed using traditional string-based transformations, which makes this dataset a relatively challenging benchmark\u00a0<cit.>.\n\nSpreadsheet Dataset (SS) This dataset includes 108 pairs of tables, sourced from Microsoft Excel product team and user help forums, specifically focused on users' data cleaning issues. The tables are comprised of spreadsheet pages that present the same information in different formats. The dataset encompasses the public benchmarks presented in FlashFill\u00a0<cit.> and BlinkFill\u00a0<cit.>, and was published in 2016 Syntax-Guided Synthesis Competition (SyGuS-Comp)\u00a0<cit.>. On average, each table in the dataset contains 34.43 rows and 19 characters per input source. Compared to web tables, this dataset features considerably less noise and inconsistency. \n\nGeneral Synthetic Dataset (Syn) This is a synthetic dataset that contains 10 table pairs. Each pair is generated by applying a randomly generated textual transformation to a set of random input sources to create the output table. The transformations are constructed by putting together a random sequence of 3 to 6 units, the same as those discussed in Section\u00a0<ref>, with random parameter sets. Unless stated differently, the dataset contains 10 tables, each of which contains 100 rows. Input length is randomly chosen in the range of 8 to 35, and no artificial noise is added to the dataset. While the model has been exposed to the units during the training, the transformations, the parameter sets of the units, and the inputs are unseen during the training process. \n\nEasy Synthetic Dataset (Syn-RP) This is a synthetic dataset containing 5 pairs of tables. Each pair is formed by randomly replacing one character with another (for example, the character `/' might be replaced with `-' for all rows). This dataset resembles simple formatting changes such as replacing a slash in a phone number with a hyphen. This replacement operation is not a transformation unit that exists in the model's training data and is thus unseen by the trained model.\nEach table contains 50 rows, and the length of input sources is randomly selected from a range of 8 to 35, unless stated otherwise. Since our model is generating the output character-by-character, we measure the difficulty of datasets based on the number of required edit operations. Accordingly, this is an easy dataset considering that only a few characters in the input need to be changed to generate the desired output.\n\nMedium Synthetic Dataset (Syn-ST) This synthetic dataset is similar to the previous one in terms of the number of table pairs and the input length. Each table pair is constructed by applying a single substring transformation unit to the input, with the start and end parameters selected randomly. Substring is one of the units included in the model's training data. In terms of difficulty, this dataset is considered to be medium-level based on the number of edit operations required.\n\nDifficult Synthetic Dataset (Syn-RV) This synthetic dataset consists of 5 tables, each containing 50 rows with input sources randomly selected to have a length between 8 to 35 characters. In this dataset, the target output is obtained by reversing all characters in the source (for instance, \u201cHello\u201d is changed to \u201colleH\u201d). This benchmark is considered difficult since almost all characters in the input source must be changed to generate the expected target. \n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Experimental Setup\n\nOur model, DTT, was trained on a synthetic dataset containing 2000 groupings of transformations, each corresponding to a transformation, as discussed in Section\u00a0<ref>. For each grouping, we generated 10 pairs of source-target samples with randomly chosen input lengths ranging from 8 to 35. \n80% of the samples were used for training and the other 20% were the validation set. We also conducted experiments with other sample sizes and input lengths for training the model, and the results are discussed in Section\u00a0<ref>.\n\nTo evaluate the performance of our model, we divided the rows of each input table in our datasets into two equal-sized sets, denoted as S_e and S_t. The former provided context examples to be passed to the model, while the latter was used for testing. Since DTT is an example-driven method, the selection of these examples is critical to the model's performance. To ensure the robustness of our predictions, we employ a technique where each input is fed to the model five times, and each time a distinct set of randomly chosen examples from S_e were given as context. The results of those trials were aggregated, as discussed in Section\u00a0<ref>, to produce a final prediction.  \n\n\n\n\n \u00a7.\u00a7 Evaluation Metrics\n\nWe evaluate the performance of our models based on precision, recall, and F1-Score. This evaluation is in the context of heterogeneous join, as discussed in Section\u00a0<ref>, where for a given source-target sample (s,t), we consider a model prediction correct if it has the minimum edit distance with the target t. In our case, precision represents the fraction of correct predictions that join with the target, recall measures the fraction of source rows that are correctly mapped, and F1-score is the harmonic mean of the two. It is important to note that not all source rows may be mapped due to various reasons[For AFJ, a threshold for similarity distance is set and based on that threshold, some source rows will not have a match. In CST, a match may not still be found after applying the detected transformations to all input rows. The language models may just return <eos> with no prediction.]. \nIn addition to the above metrics, we also report the Average Edit Distance(AED) and Average Normalized Edit Distance (ANED), which indicates the extent to which a prediction may differ from the given target. The normalization is performed based on the target length, enabling comparability across different datasets and lengths.\nAll reported metrics for each dataset are the average over all tables in the dataset.\n\n\n\n\n\n \u00a7.\u00a7 Performance Compared to Heterogeneous Join Baselines\n\nIn this section, we evaluate the performance of our model on the end-to-end task of heterogeneous or unequal table join. The task simulates the scenario where source and target columns are in two different tables that need to be joined.\nTo provide a point of reference, we compare the performance of our model to two current state-of-the-art baselines:\nCommon String-based Transformer (CST)\u00a0<cit.> and Auto-FuzzyJoin (AFJ)\u00a0<cit.>. CST finds a set of textual transformations given a set of examples to transform tables for joinability and AFJ uses a set of similarity functions to detect the most probable rows to be joined.\n\nTable\u00a0<ref> summarizes the performance of DTT and the baselines, in terms of precision, recall, and F1-score, (denoted as P, R, and F respectively).  \nThe results show that DTT outperforms the baselines on all real-world datasets in terms of F1-Score and recall. On the synthetic datasets, our approach outperforms the baselines on three out of four datasets. On Syn-RP and Syn-ST datasets, our approach is either comparable or slightly worse than the baselines. The reason is that these datasets are relatively easy, with a significant textual similarity between the source and target. CST exhaustively searches the space for substring transformation, which is the only transformation used in the Syn-ST dataset. Moreover, AFJ is based on the textual similarity, and every target in Syn-ST is a substring of the source, leading to a significant similarity between source and target. Therefore, these datasets favor the baselines. Nevertheless, DTT still achieves an F1-score of 88% on the Syn-ST dataset and a perfect F-score of 100% on Syn-RP, which is equal to AFJ and better than CST.\n\nThere are significant differences between DTT and the baselines\nCST is limited in its ability to extract transformations, and cannot perform a join when there is no clear copying relationship between the source and target, as is the case with the Syn-RV dataset where the target is obtained by reversing the input. As a result, CST achieves a 0% F1-score on this dataset.\nAFJ, on the other hand, employs similarity functions to determine if source and target values can be joined. However, this method struggles when there is not much similarity between the source and target, as demonstrated by its performance on the Syn-RV dataset. Such challenges are common in real-world data.\nDTT, in contrast, leverages the provided examples to generate the desired output without relying on textual similarity or being bounded by the length of transformations. Hence, DTT performs significantly better than the baselines on more challenging datasets, such as the real-world WT dataset and the synthetic Syn and Syn-RV datasets.\nFor instance, DTT outperforms the baselines by a large margin on Syn-RV, where the target is obtained by reversing the order of characters in the input.\n\n\n\n\n\nTwo more interesting observations can be made here. Firstly, to achieve a good performance on the join, it is not necessary to predict every single character correctly. Our framework can tolerate inaccuracies by aggregating results from multiple examples and using an edit-distance-based metric to form join candidates. For example, in Syn-RV dataset, while the average normalized edit distance is more than 80%, the F1-score for join prediction is 63%. Secondly, our model performs very well on all real-world datasets and two synthetic datasets Syn-RP and Syn-RV, despite the fact that our training data did not include any operation that simulates reversing the input or replacing a character, and no real-world examples or transformations were included in the training data. \nThis highlights that the model is not limited to a given set of transformations, but rather focuses on extracting character-level patterns from the given set of input examples.\n\nFinally, in terms of comparing the runtime of DTT and our baselines, a direct comparison is not possible since DTT requires a GPU architecture whereas our baselines require CPU and memory. That said, some observations can be made on the scalability of the models\nThe time required to predict a mapping for each row in DTT is independent of the number of rows and grows linearly with the length of the rows, whereas this time grows quadratically with the number of rows and polynomially with the length in CST.\nWhile the edit distance calculation in the joining process depends on the number of rows, our experiments suggest the growth in the runtime of DTT is noticeably less than CST when input length increases. \nFor instance, with our machine setup\u00a0[Our experiments were conducted on a machine with Nvidia RTX 3090 GPU and AMD EPYC 7601 CPU with 64GB RAM.], processing a table with row length set to 5 characters from our synthetic dataset takes 5 seconds for DTT and 3 seconds for CST.\nHowever, when the input length increases to 50 characters, DTT needs less than 17 seconds, while CST takes around 90 seconds to complete the join. It should be noted that the runtimes reported for DTT are the summation of decomposition, all 5 trails, and the aggregation time. \nFor scalability in terms of the number of rows, we compared their performance on two tables from our spreadsheet dataset, \u201cphone-10-short\u201d and \u201cphone-10-long\u201d, both with an average of 17 characters per row. The former has 7 rows, while the latter has 100. DTT takes 3 and 22 seconds respectively for short and long tables, while the same experiments require 4 and 366 seconds for CST, and 4 and 38 seconds for AFJ. This indicates how our framework scales better in terms of runtime when the input grows either horizontally or vertically.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Performance Compared to Large Language Model Baselines\n\t\nLarge Language Models (LLM) can be employed in many downstream tasks including joining heterogeneous tables. It has been shown that the recent models perform relatively well under zero or few shot settings\u00a0<cit.>, hence they set a strong baseline. In this section, we compare the performance of our model to GPT-3\u00a0<cit.>, a state-of-the-art LLM with exceptional performance on many tasks. Compared to our ByT5-base\u00a0<cit.> model that is fine-tuned on only 20,000 synthetically-generated samples and contains near 582M parameters, GPT-3 models are trained on billions of documents and resources such as web tables and has at least one to two orders of magnitude more parameters.\nOur experiment with GPT-3 is under few-shot setting with 1, 2, 3, 5, and 10 randomly selected samples from each table given as examples.\nZero-shot setting is not applicable in our case since an input source row can be mapped to an unlimited number of targets without any examples. \n\nAt the time of writing, GPT-3 models are not published publicly and are only accessible through OpenAI commercial API[https://openai.com]. We use the Curie[The complete model name is \u201ctext-curie-001\u201d] model of GPT-3 from the API. Curie is claimed to be extremely powerful, fast, and capable of many advanced tasks[Based on platform documentations on openai.com]. Nevertheless, the model specification and the number of parameters are not publicly announced. \nComparing the general performance of the Curie model with the performance reported for various sizes of GPT-3\u00a0<cit.>, it can be assumed that the Curie model has about 7B parameters\n\nand is trained on a huge text data from different datasets.\n\nWe run two sets of experiments to analyze the performance of GPT-3 for unequal join. First, as the common method of using LLMs, we pass our examples as an input sequence to the GPT-3 and consider the model output as the expected target. The serialization used for GPT-3 is the same as DTT, as discussed in Section\u00a0<ref>. \n\nIn the second experiment, we use GPT-3 as a replacement for our fine-tuned ByT5 model (and byte-level tokenizer) inside our framework, keeping the serializer and aggregator from DTT. Figure\u00a0<ref> depicts the F1-Score of the model with 1 and 2 examples under both experimental settings for all datasets compared to DTT and Table\u00a0<ref> reports the F1-score and ANED of GPT-3 model for all 1, 3, and 5 input examples.\n\n\nAs shown in Figure\u00a0<ref>, GPT-3 struggles to perform well on the task with just one example despite some recent work suggesting that LLMs are capable of one-shot table reasoning\u00a0<cit.>. However, providing two examples significantly boost its performance, especially on real-world data, bringing it on par with DTT. \n\nIn our synthetic datasets, however, DTT performs significantly better than GPT-3. The lack of publicly available data on GPT-3 Curie model's size and specification makes it challenging to conduct a more in-depth comparison.\n\nIt can be noted that GPT-3 is trained on numerous web resources, including web tables, which increases the likelihood that it has encountered various representations of common entities and tables on the web. Since our real-world datasets are gathered from tables on web pages, this could explain why the model performs significantly better on WT and SS datasets than on synthetic datasets. Conversely, synthetic datasets consist of sequences of random characters that may not be tokens from natural language, and GPT-3 may not have encountered them during its training. Consequently, its performance on most of the synthetic datasets is weak and, in some cases, significantly inferior to DTT, especially in the Syn-RV dataset, where the target and source are substantially different.\nOur ByT5-based model, however, is trained to extract patterns among a sequence of characters, allowing it to perform better on more challenging synthetic datasets. \n\n\n\nIn our second set of experiments with GPT-3, we used our framework and replaced the LLM module with GPT-3. By default, our framework employs two context example pairs because ByT5 has a maximum limit of 512 character-level tokens, and if a longer sequence is given to the model, it will be truncated. However, the limit in GPT-3 Curie model is 2048 subword-level tokens. This allows us to increase the number of example pairs that are given to the model. In our experiment with GPT-3 integrated into the DTT framework, we varied the number of examples from one to five. \nAs demonstrated in Figure\u00a0<ref> and Table\u00a0<ref>, using GPT-3 within our framework boosts its performance, in terms of both the F1-score and ANED, on nearly all datasets when the same number of examples were provided. For instance, the average F1-score across all datasets of the GPT-3 model increased from 0.624 to 0.667 with two examples and from 0.734 to 0.760 with five examples when integrated into the DTT framework. This demonstrates how the model inside the DTT framework can be substituted with other larger models and gain a performance boost.\n\n\n\n\n\n\n \u00a7.\u00a7 Performance Varying the Number and Length of Training Samples\n\n\n\n\n\n\nOur trained model has two important parameters: the number of samples and their length. To gain a deeper insight into the relationship between these parameters and the model's performance, we conducted an experiment where we varied the number of training samples from 0 to 10,000. Each sample here is a grouping of transformations that consists of 10 source-target pairs, and we kept the sequence length consistent with our other experiments, ranging between 8 and 35.\nWhen the number of samples was set to zero, the ByT5 model did not undergo any fine-tuning.\n\n\nAs shown in the top left panel of Figure\u00a0<ref>, the F1-Score of the model is typically less than 0.5 when no fine-tuning is performed. Also on all datasets, over 80% of characters are predicted incorrectly (i.e. ANED > 0.8) when the model is not fine-tuned, as indicated by the 0 training samples in the figure. For example, in the Syn-ST dataset, over 84% of output characters are predicated incorrectly by the ByT5 model without fine-tuning. However, this error is reduced to 27% after a proper fine-tuning of the model.\nThis finding suggests that, unlike GPT-3, the ByT5 model without fine-tuning struggles to perform well for unequal join. Nevertheless, our fine-tuning plays a crucial role in significantly improving the performance of the model.\n\n\nThe general expectation for the model is to perform better when more training samples are provided and the trend for our experiments is not much different. However, some observations should be taken into account. As shown in Figure\u00a0<ref>, when the number of training samples surpasses 2,000 [This number refers to the number of transformation groupings, and it translates to 20,000 source-target examples of which 16,000 examples are used for training and the remaining 4,000 is kept as the validation set.], the model performance does not significantly change, and it reaches its optimal performance level on our datasets. Beyond this point, a slight decrease in the performance can be observed on real-world data and synthetic datasets that contain transformations not covered in the training data. This behavior can be attributed to the bias that the model acquires from seeing more transformations of the same type, which hinders its ability to effectively use its prior knowledge of real-world data. Our extensive experiments show that even with a significantly larger training dataset, the decrease in performance is not significant. Thus the model performance will converge when 2000 or more training samples are provided.\n\n\nTo examine how the length of input affects the training process of the model, we conducted another experiment where we changed the length range of the training samples by randomly selecting values between 5 and 60 characters.\nThe right panel of Figure\u00a0<ref> shows the performance when the model is trained with sequences that are generally longer and have an extended range. \nIncreasing the length range of input sample pairs does not lead to any noticeable improvement on the performance of the model. That being said, increasing the length is expected to have an impact on how the model performs on longer inputs, which is discussed next.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Performance Varying the Input Length\n\n\nIn this section, we explore how the model performs under different input lengths. We also investigate how the length of input data during training affects the model's ability to handle longer input during inference time.  \n\nTo conduct our experiments, we regenerated synthetic datasets Syn-RP, Syn-ST, and Syn-RV, this time with the input lengths varying from 5 to 50 characters. We utilized two versions of the model in our analysis. The first version was trained on input examples with lengths randomly sampled between 8 and 35, while the second version was trained on examples with extended lengths selected randomly between 5 to 60 characters.\n\n\nAs shown in Figure\u00a0<ref>, when the benchmark dataset is easy in terms of edit distance between source and target, such as Syn-RP, the performance of the model is not significantly influenced by the length of the input. Both the model trained on short input examples and the model trained on long input examples deliver the highest F1-Score and near zero edit distance across almost all lengths of input. On the medium dataset (i.e., Syn-ST), the models start with almost perfect performance and the performance is sustained for input lengths that are shorter than the length of the majority of samples used in training. However, the performance begins to decrease once the input length surpasses this threshold. Nonetheless, even with an increase in ANED, the model still manages to predict a reasonable portion of the expected characters in the output. Interestingly, the drop in performance does not occur when the model is trained on longer input samples. \nIt should be noted that if the input is too short (not a typical real-world scenario), such as when it contains only 5 characters, there may be a slight decrease in performance as the model may not fully comprehend the relationship between the source and target with such limited information. \n\nOn the other hand, on more challenging datasets, such as Syn-RV, the performance drops even for input lengths that are shorter than the majority of training samples. This behavior is not unexpected for Auto-regressive models since a single incorrect prediction can influence the prediction of subsequent characters.\nThe results of our experiment suggest that the extent of the decrease in performance is influenced by the training data of the model. When trained on shorter-length data, there is a significant decrease in both F1-Score and ANED as the input length increases. However, when trained on lengthier data, the decrease is relatively minimal.\nOverall, such cases are not very common in real-world datasets, and our experiments demonstrate that our model can perform well under various input lengths in real-world settings. Based on our experiment, we can assume that the model can accurately detect transformation patterns for various input lengths when the difficulty level of the transformation is reasonable.\n\n\n\n\n\n\u00a7 CONCLUSION AND FUTURE WORK\n\nWe have studied the problem of mapping tabular data from a source formatting to a desired target formatting using a set of few examples. Tables may be transformed to enable joining heterogeneous tables, filling missing values, data corrections, and other data integration tasks. To address this challenge, we proposed a framework that leverages the power of large language models. We generated the required training data and fine-tuned a character-level LLM based on ByT5 for this task. Our extensive experiments demonstrate that our model achieves impressive performance on a wide range of real-world and synthetic datasets, outperforming state-of-the-art models in the field. \n\n\nOur work suggests several possible avenues for future research. One potential direction is to explore the use of synthetic data generation to enhance model training for a variety of data integration tasks. Additionally, there is value in investigating the challenges and limitations of synthetic data in model training, as well as strategies for addressing those challenges.\nFurthermore, given concerns around privacy, federated learning may be a preferred approach for table transformation tasks. As such, an exploration of federated learning methods for this purpose is yet another promising direction for future research. \n\n\n\n\n\nACM-Reference-Format\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}