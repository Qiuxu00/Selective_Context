{"entry_id": "http://arxiv.org/abs/2303.06766v1", "published": "20230312220535", "title": "Next-Best-View Selection for Robot Eye-in-Hand Calibration", "authors": ["Jun Yang", "Jason Rebello", "Steven L. Waslander"], "primary_category": "cs.RO", "categories": ["cs.RO"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext-Best-View Selection for Robot Eye-in-Hand Calibration\n    Jun Yang, Jason Rebello, Steven L. Waslander\nInstitute for Aerospace Studies and Robotics Institute\n\nUniversity of Toronto\n\nToronto, Canada\n\n{jun.yang, jason.rebello, steven.waslander}@robotics.utias.utoronto.ca\n\n    \n========================================================================================================================================================================================================================\n\n\n\n\n\nRobotic eye-in-hand calibration is the task of determining the rigid 6-DoF pose of the camera with respect to the robot end-effector frame. In this paper, we formulate this task as a non-linear optimization problem and introduce an active vision approach to strategically select the robot pose for maximizing calibration accuracy. Specifically, given an initial collection of measurement sets, our system first computes the calibration parameters and estimates the parameter uncertainties. We then predict the next robot pose from which to collect the next measurement that brings about the maximum information gain (uncertainty reduction) in the calibration parameters. We test our approach on a simulated dataset and validate the results on a real 6-axis robot manipulator. The results demonstrate that our approach can achieve accurate calibrations using many fewer viewpoints than other commonly used baseline calibration methods.\n\n\n\n\n\nHand-eye calibration; active vision; next-best-view; non-linear optimization\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nWith the rapid advance in 3D computer vision techniques, perception systems have become an essential component in many industrial applications, such as robotic bin-picking\u00a0<cit.> and on-machine inspection\u00a0<cit.>. Historically, for these applications, the sensor is installed in a static position. For example, in robotic bin-picking, the camera is mounted above the bin. However, the camera often fails to acquire a complete representation of the task space from a single viewpoint due to occlusions and limited sensor resolution. To overcome these limitations, we can attach the camera to the end-effector of a robotic manipulator, which is also known as the eye-in-hand system.\n\nThe proper functioning of an eye-in-hand system relies on accurate calibration. The goal is to estimate a rigid 6-DoF pose of the camera with respect to the robot end-effector frame. As introduced in\u00a0<cit.>, this problem can be solved with a hand-eye calibration formulation \ud835\udc00\ud835\udc17=\ud835\udc17\ud835\udc01, where \ud835\udc00 and \ud835\udc01 represent the relative robotic arm and camera motions between two different time instants, respectively, and \ud835\udc17 is the unknown rigid transformation from the robot end-effector frame to the camera coordinate frame. To use this formulation, we need to convert absolute poses into relative ones and solve the \ud835\udc00\ud835\udc17=\ud835\udc17\ud835\udc01 problem using closed-form approaches\u00a0<cit.> or optimization-based solutions\u00a0<cit.>. Alternatively, the eye-in-hand calibration can be solved with the robot-world-hand\u2013eye formulation \ud835\udc00\ud835\udc17=\ud835\udc18\ud835\udc01\u00a0<cit.>. In this formulation, \ud835\udc18 represents the transformation from the robot end-effector to the camera frame, and \ud835\udc17 is the transformation from the robot base to the world frame. The transformations \ud835\udc00 and \ud835\udc01 represent the absolute transformation from the robot base to the robot end-effector, as well as the transformation from the camera to the world coordinate. The goal of robot-world-hand\u2013eye calibration is to determine both \ud835\udc17 and \ud835\udc18 from either closed-form solutions\u00a0<cit.> or optimization-based approaches\u00a0<cit.>.\n\n\n\nAs studied in\u00a0<cit.>, optimization-based approaches with \ud835\udc00\ud835\udc17=\ud835\udc18\ud835\udc01 formulation achieve the highest accuracy since they use absolute measurements and are more tolerant to the measurement noise and errors. These approaches generally take the initial estimates from any fast closed-form methods, such as Tsai\u00a0<cit.> or Zhuang\u00a0<cit.>, and iteratively minimize the formulated cost function (e.g., 2-D reprojection errors) with non-linear optimizers. Although the optimization-based solution can provide accurate calibration, it requires sufficient measurement sets to ensure high accuracy. As illustrated in Figure\u00a0<ref>, different robot poses are needed to create measurement sets for the optimization-based methods, and the calibration accuracy heavily relies on the selection of the measurement sets. Manual selection of the robot poses has two major limitations. First, it requires a large volume of the robot motions to collect sufficient measurements for accurate calibration. Second, even if a large volume of measurement sets is collected, it does not guarantee that a good calibration can be estimated. The addition of poor measurements may even degrade the calibration accuracy due to the biased distribution of the robot poses and measurement noise.\n\nTo this end, we propose an active vision method for eye-in-hand calibration. Our method follows the \ud835\udc00\ud835\udc17=\ud835\udc18\ud835\udc01 formulation and uses an information theoretic next-best-view (NBV) policy\u00a0<cit.>. For each iteration, we determine the next best robot pose from where to collect measurements, such that it brings about the maximum reduction in calibration parameter uncertainty. We evaluate our approach on a simulated dataset and verify it on a real 6-axis robot manipulator. The results demonstrate that our next-best-view approach can achieve high calibration accuracy using significantly fewer viewpoints when compared with heuristic-based baselines, such as random and maximum distance sampling strategies. The contributions are summarized as follows:\n\n    \n  * An information-theoretic formulation for the estimation of the eye-in-hand calibration parameters and prediction of the information gain for a future viewpoint.\n    \n  * An active vision system that exploits the proposed information-theoretic formulation for rapid and accurate eye-in-hand calibration.\n\n\nThe rest of the paper is structured as follows. Section <ref> reviews the relevant literature. Section <ref> formulates our eye-in-hand calibration problem. Section <ref> describes the autonomous active calibration system. Section <ref> presents the evaluation results, and section <ref> concludes the paper.\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n \u00a7.\u00a7 Robotic Hand-Eye Calibration\n\nIn robotics, hand-eye calibration is the problem of determining the rigid transformation between a camera and a robot reference frame and includes two different camera setups: eye-in-hand and eye-to-hand. The eye-in-hand calibration is the process of estimating the relative 6D pose of a robot-mounted camera with respect to the robot\u2019s end-effector. In comparison, for eye-to-hand calibration, the camera is mounted statically, and the calibration determines the 6D pose of the camera with respect to the robot\u2019s base. Both camera setups employ the \ud835\udc00\ud835\udc17=\ud835\udc17\ud835\udc01 hand\u2013eye formulation\u00a0<cit.> or \ud835\udc00\ud835\udc17=\ud835\udc18\ud835\udc01 robot-world-hand\u2013eye formulation\u00a0<cit.>. For both the \ud835\udc00\ud835\udc17=\ud835\udc17\ud835\udc01 or \ud835\udc00\ud835\udc17=\ud835\udc18\ud835\udc01 formulation, the earliest approaches solved the problem in the closed-form manner by estimating the rotation and translation separately\u00a0<cit.>, which is also known as the separable closed-form method. Another class of closed-form methods solves the rotation and translation simultaneously\u00a0<cit.>. To further improve the calibration accuracy for high-precision tasks, such as robot pick-and-assembly, some recent works proposed optimization-based approaches\u00a0<cit.>. These approaches take the initial estimates from a fast closed-form method\u00a0<cit.>, and iteratively minimize a cost function to yield better calibration accuracy. While the core problem has been well addressed, the majority of existing works perform the calibration in a passive way, where the robot motions and camera poses are given in advance, which limits the resulting calibration accuracy. Only a few approaches exist\u00a0<cit.> that can perform hand-eye calibration in an active manner. \n\n\n\n\n \u00a7.\u00a7 Active Vision and Next-Best-View\n\nActive vision\u00a0<cit.>, and more specifically\nNext-Best-View (NBV)\u00a0<cit.> refers to the approach of actively manipulating the camera to gather more informative measurements in a greedy fashion. Active vision technologies have been successfully applied to a wide range of robotic applications, such as localization\u00a0<cit.>, 3D reconstruction\u00a0<cit.>, robot grasping\u00a0<cit.> and calibration\u00a0<cit.>. Among these works, a typical scheme for NBV is to maximize the Fisher information or minimize the entropy for the robot state parameters\u00a0<cit.>. Specifically, for a robot state estimation problem, the \"informativeness\" of a viewpoint (i.e., how much parameter uncertainties can be reduced) is quantified by the Fisher information. In\u00a0<cit.>, the authors leverage the Fisher information to select informative trajectories to improve localization quality and avoid pose tracking loss. From these examples, the closest one to our work is from\u00a0<cit.>, where the Fisher information is used to select the next best robot mechanism inputs for calibrating a dynamic camera cluster with one static camera\u00a0<cit.>. In this work, we formulate the active robot eye-in-hand calibration problem with the Fisher information objective and automate the calibration process using a next-best-view strategy. \n\n\n\n\n\n\u00a7 EYE-IN-HAND CALIBRATION FORMULATION\n\n\nIn this section, we present our problem formulation for eye-in-hand calibration. We use the robot-world-hand\u2013eye calibration formulation, \ud835\udc00\ud835\udc17=\ud835\udc18\ud835\udc01, and solve the problem using an iterative non-linear optimization-based approach. We define the rigid transformation \ud835\udc13 in the \ud835\udd4a\ud835\udd3c(3) Lie Group:\n\n    \ud835\udd4a\ud835\udd3c(3) := {\ud835\udc13=[   \ud835\udc11   \ud835\udc2d; 0^T   1 ]  |    \ud835\udc11\u2208\ud835\udd4a\ud835\udd46(3),\ud835\udc2d\u2208\u211d^3\n    }\n\nwhere \ud835\udd4a\ud835\udd46(3) is the special orthogonal group (i.e., \ud835\udc11\ud835\udc11^T = \ud835\udc08, \ud835\uddbd\ud835\uddbe\ud835\uddcd(\ud835\udc11)=1). The associated Lie Algebra space to the \ud835\udd4a\ud835\udd3c(3) Lie Group is indicated as \ud835\udd30\ud835\udd22(3). The calibration process aims to determine two rigid transformations in \ud835\udd4a\ud835\udd3c(3): the transformation \ud835\udc13_ce from the robot end-effector frame, \u2131_e, to the camera frame, \u2131_c, as well as the transformation \ud835\udc13_bw from the world frame, \u2131_w, to the robot base frame, \u2131_b. Note that the world frame, \u2131_w, is defined by a static calibration board with a known marker size (e.g., a checkerboard). For each robot pose, \ud835\udc13_eb, the camera captures the pixel measurements, \ud835\udc2e\u2208\u211d^2. We define the measurement set, \ud835\udc19_k, from the k^th robot pose as:\n\n    \ud835\udc19_k = {\ud835\udc2e_k , \ud835\udc13_eb,k}\n\nwhere the robot pose, \ud835\udc13_eb,k, represents the transformation from the robot base frame, \u2131_b, to the robot end-effector frame, \u2131_e, which is obtained from the robot's forward kinematics. The pixel measurement, \ud835\udc2e_k \u2208\u211d^2, is the set of the 2D projections of the 3D marker points, \ud835\udc0f_w\u2208\u211d^3, from the calibration board. \n\nGiven the measurement set, \ud835\udc19_k, estimating the transformations \ud835\udc13_ce and \ud835\udc13_bw can be formulated as a non-linear optimization problem. We name these two unknown transformations as the set of calibration parameters and represent them as:\n\n    \u0398 = {\ud835\udc13_ce  ,  \ud835\udc13_bw}\n\nThe optimization is constructed by defining the re-projection error between the measured pixel positions, \ud835\udc2e_k, and the projected target locations, \ud835\udc2e_k^', through the calibration parameters on the image plane:\n\n    r^j(\u0398, \ud835\udc19_k )    = u_k^j - u_k^j'\n       = u_k^j - \u03c0(T_cw, kP_w^j)\n       = u_k^j - \u03c0(T_ceT_eb, kT_bwP_w^j)\n\nwhere u_k^j is the measurement of the j^th 3D marker point P_w^j from the k^th robot pose, and r^j(\u0398, \ud835\udc19_k ) is the residual between them. \u03c0 represents the perspective projection function for a camera model. The loss function is then defined as:\n\n    L(\u0398)    = \u2211_k \u2211_j=1^| P_w |r^j(\u0398, \ud835\udc19_k )^T (\u03a3_k^j)^-1 r^j(\u0398, \ud835\udc19_k )\n\nwhere \u03a3_k^j is the corresponding measurement covariance matrix. For a calibration process, the calibration target is usually provided with high contrast, and the 2D measurements (e.g., corner detection) of the 3D marker points can be obtained accurately. Therefore, we assume the measurement noise is constant for different marker positions across all the camera viewpoints and change the loss function in Equation\u00a0(<ref>) to:\n\n    L(\u0398)    \u2248\u2211_k \u2211_j=1^| P_w |r^j(\u0398, \ud835\udc19_k )^T  r^j(\u0398, \ud835\udc19_k )\n\n\nTo compute the optimal calibration parameters \u0398^*, we perform an unconstrained optimization for Equation\u00a0(<ref>) and minimize the re-projection errors over all the collected measurement sets, \ud835\udc19_1:K. To ensure a stable solution, we can provide a good initial estimate \u0398 from any closed-form methods\u00a0<cit.> for the non-linear optimization. The iterative algorithms, such as Levenberg\u2013Marquardt or Gauss-Newton, can finally be used to optimize the calibration parameters. For example, the update equation with the Gauss-Newton method is:\n\n    ( J_\u0398, Z_1:K^T J_\u0398, Z_1:K) \u03b4\u0398 = J_\u0398, Z_1:K^T   r(\u0398, Z_1:K)\n\nwhere r(\u0398, Z_1:K) is the residual vector over all the collected measurements Z_1:K. The stacked Jacobian matrix J_\u0398, Z_1:K of the measurements is represented as:\n\n    J_\u0398, Z_1:K =\n        [ J_\u0398, Z_1;        \u22ee; J_\u0398, Z_K ]\n\n\nEach row-block of Equation\u00a0(<ref>), J_\u0398, Z_k, corresponds to the Jacobian matrix with the k^th measurement set. It composes of two parts:\n\n    J_\u0398, Z_k = [F_k   E_k]\n\nwhere Jacobian matrices F_k and E_k are the derivative of the overall cost function with respect to the poses \u03be_ce and \u03be_bw on the tangent space:\n\n    F_k = -\u2202u_k^'/\u2202\u03be_ce = -\u2202u_k^'/\u2202P_c,k\u2202P_c,k/\u2202\u03be_ce\n\n\n    E_k = -\u2202u_k^'/\u2202\u03be_bw = -\u2202u_k^'/\u2202P_c,k\u2202P_c,k/\u2202P_e,k\u2202P_e,k/\u2202P_b,k\u2202P_b,k/\u2202\u03be_bw\n\nwhere \u03be_ce , \u03be_bw\u2208\ud835\udd30\ud835\udd22(3) are the Lie algebra representations of the transformations \ud835\udc13_ce and \ud835\udc13_bw, respectively. The Hessian, J_\u0398, Z_k^T J_\u0398, Z_k is constructed by:\n\n    J_\u0398, Z_k^T J_\u0398, Z_k = [ F_k^T F_k F_k^T E_k; E_k^T F_k E_k^T E_k ]\n\nNote that, for the multiple measurement sets, Z_1:K, the Hessian, J_\u0398, Z_1:K^T J_\u0398, Z_1:K, is a 12 \u00d7 12, square matrix. The operation complexity of its inversion is constant.\n\n\n\n\n\n\u00a7 ACTIVE CALIBRATION USING NEXT-BEST-VIEW\n\n\nIn Section\u00a0<ref>, we formulate the eye-in-hand calibration as a non-linear optimization problem and solve it using the iterative approaches. However, the calibration accuracy relies heavily on the collected measurement sets from the selected robot poses. Even if a large volume of the measurement sets is collected, it does not ensure calibration accuracy due to the biased distribution of the robot poses, redundant measurements, and possible measurement noises. Hence, in this section, we present our active calibration process that can estimate the uncertainty of the calibration parameters and predict the next-best-view for maximizing the information gain (uncertainty reduction). An overview of our proposed active calibration system is illustrated in Figure\u00a0<ref>.\n\n\n\n\n\n\n \u00a7.\u00a7 Initialization and Uncertainty Estimation\n\nWe initialize our active calibration with a collection of measurement sets \ud835\udc19_1:K from K viewpoints. To uniquely determine the set of the calibration parameters \u0398 and provide the initial estimate, at least K=3 sets with non-parallel rotation axes are required\u00a0<cit.>. We then perform the iterative optimization, described in Section\u00a0<ref>, to refine the calibration parameters. To assess the calibration quality with collected measurement sets, we assume the set of the calibration parameter is distributed with a uni-modal Gaussian and compute its covariance matrix, \u03a3_\u0398, Z_1:K, with a first-order approximation of the Fisher information matrix (FIM):\n\n    \u03a3_\u0398, Z_1:K   = ( J_\u0398, Z_1:K^T  J_\u0398, Z_1:K)^-1\n\n\nSuch an inverse of the FIM defines the Cramer-Rao lower bound, which is the smallest covariance that can be achieved by an unbiased estimator. To quantify the calibration parameter uncertainty, we use the differential entropy h_e( \u03a3_\u0398, Z_1:K):\n\n    h_e( \u03a3_\u0398, Z_1:K) = 1/2ln((2\u03c0 e)^n | \u03a3_\u0398, Z_1:K| )\n\nNote that the uncertainty computation is not limited to the differential entropy and can be superseded by other metrics, such as the trace\u00a0<cit.> or the sum of the eigenvalues\u00a0<cit.> of the covariance matrix.\n\n\n\n\n \u00a7.\u00a7 Next-Best-View Prediction\n\nTo improve the quality of the calibration parameters, we aim to find the next best robot forward kinematics \u03b8^* from a set of candidates, {\u03b8}, that governs the robot pose \ud835\udc13_eb and will be used to maximize the information gain of the calibration parameters. The information gain is defined as the uncertainty (entropy) reduction after including the measurement set from a candidate robot kinematics. For a candidate robot kinematics, \u03b8, after including the measurement set, \ud835\udc19, the stacked Jacobian matrix becomes:\n\n    J_\u0398, Z =\n        [ J_\u0398, Z_1:K;     J_\u0398, Z ]\n\n\n\n\n\nwhere Z = {Z_1:K, Z} represents the set of measurements from the robot forward kinematics \u03b8_1:K and the candidate kinematics \u03b8. Using the FIM approximation, the parameter covariance can be calculated as follows:\n\n    \u03a3_\u0398, Z = ( J_\u0398, Z^T   J_\u0398, Z)^-1\n\nNote that we compute the Jacobian matrix (Equation\u00a0(<ref>)) and the predicted parameter covariance (Equation\u00a0(<ref>)) before actually applying the robot kinematics \u03b8 and including the measurement \ud835\udc19. The computation of the Equation\u00a0(<ref>) is based on the estimated calibration parameters using measurement sets Z_1:K only.\n\n\n\nFor a robot kinematics \u03b8, we define the corresponding information gain I_\u0398, Z as the entropy reduction of the parameter covariance:\n\n    I_\u0398, Z = h_e(\u03a3_\u0398, Z_1:K) - h_e(\u03a3_\u0398, Z)\n\nTo find the next-best-view, \u03b8^*, we maximize the information gain over the entire candidate set, {\u03b8}:\n\n    \u03b8^* = _\u03b8I_\u0398, Z\n\n\nOnce the next-best-view, \u03b8^*, is determined, we apply the robot motion. A new measurement set \ud835\udc19^* is collected and the total collection \ud835\udc19_1:K+1 will be updated as:\n\n    \ud835\udc19_1:K\u222a\ud835\udc19^*\u2192\ud835\udc19_1:K+1\n\nWe finally optimize the calibration parameters \u0398^* using the newly updated measurement sets, \ud835\udc19_1:K+1, and predict the next-best-view again. We repeat this process until it reaches user-defined criteria, i.e., after a fixed number of iterations or when the highest information gain of a subsequent robot kinematics falls below a user-defined threshold:\n\n    I_\u0398, Z < I_\u03c4\n\nThe architecture of NBV for eye-in-hand calibration is shown in Algorithm <ref>.\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n \u00a7.\u00a7 Datasets\n\nReal-World Dataset. To demonstrate the effectiveness of our proposed approach, we capture a real dataset using an EPSON C4L 6-Axis robot manipulator with an industrial Ensenso N35 camera. As shown in Figure\u00a0<ref>, we mount the camera on the end-effector of the robot arm and move the robot to capture the monochrome images with varying robot poses. In our experiments, we use a precisely manufactured calibration board with a 4\u00d74 symmetrical circle grid pattern. We calibrated the camera's intrinsic parameters using the calibration toolbox provided by the camera vendor. We program the robot arm to move the camera to different viewpoints. Since the calibration pattern is symmetrical, we make the robot end-effector stay pointed towards the center of the workstation, and the camera in-plane rotation remains unchanged. The examples of the captured calibration images are shown in Figure\u00a0<ref>-<ref>. With such a workstation setup, we captured a total of seven sets. Each calibration set includes one training set and one validation set. We perform the calibration using the training set only and evaluate the calibration results on the validation set.\n\n\n\nSimulated Dataset. The real dataset can represent the true uncertainties (e.g., measurement noise) for a calibration system in the real world. However, it is not possible to acquire the ground truth information for quantifying the absolute pose errors. The main advantage of using a simulated dataset is the availability of ground truth transformations (e.g., robot end-effector to camera frame, robot base to world frame). In our experiments, we use a public dataset\u00a0<cit.>. It includes three simulated datasets with a different number of robot poses and synthetic images. The dataset is injected with pseudo-realistic robot pose noise and visual noise for synthetic images.\n\n\n\n\n\n \u00a7.\u00a7 Baselines and Evaluation Metrics\n\n\nFor robot hand-eye calibration, existing works use heuristic-based policies, such as random selection or linear spacing, to select viewpoints. To demonstrate the effectiveness of our approach, we compare our system against two heuristic-based viewpoint selection strategies. The first baseline, \"Random\", selects random robot poses from the set of candidates. The second baseline, \"Maximum Distance\", moves the robot end-effector to the position of the furthest distance from previous positions in the Euclidean space \u211d^3. Figure\u00a0<ref> depicts the trajectories generated by our approach and these two baselines. In our experiments, we assume the robot can teleport from one location to the next without the path planning restriction. This will allow us to explore the theoretical upper bounds of our method.\n\nTo evaluate the results on the simulated datasets, we take two metrics from\u00a0<cit.>. We use absolute translation error (mm), e_\ud835\udc1a\ud835\udc2d, and absolute rotation error (deg), e_\ud835\udc1a\ud835\udc11, to quantify the absolute pose errors:\n\n    \u0394\ud835\udc13_ce =  \u0164_ce^-1 \ud835\udc13_ce\n\n\n    e_\ud835\udc1a\ud835\udc2d = d(\u0394\ud835\udc13_ce),     e_\ud835\udc1a\ud835\udc11 = \u2220(\u0394\ud835\udc13_ce)\n\nwhere \u0164_ce and \ud835\udc13_ce are the estimated and ground truth transformation (from robot end-effector to camera frame), respectively. The function d( \u0394\ud835\udc13) extracts the translation part from \u0394\ud835\udc13 and computes the Euclidean norm. \u2220( \u0394\ud835\udc13) extracts the rotation part from \u0394\ud835\udc13 and gets the absolute angle value with angle-axis representation with Lie algebra representation in \ud835\udd30\ud835\udd2c(3). \n\nFor the real dataset, due to the missing ground truth poses, we use three metrics from\u00a0<cit.> for the evaluation: relative translation error (mm), e_\ud835\udc2b\ud835\udc2d, relative rotation error (deg), e_\ud835\udc2b\ud835\udc11, and reprojection error (px), e_\ud835\udc11\ud835\udc0c\ud835\udc12\ud835\udc04. The relative translation and rotation errors are derived from \ud835\udc00\ud835\udc17=\ud835\udc18\ud835\udc01 formulation:\n\n    \u0394\ud835\udc13_k = \u0164_bw \ud835\udc13_wc,k \u0164_ce \ud835\udc13_eb,k\n\n\n    e_\ud835\udc2b\ud835\udc2d =  1/K\u2211_k=1^K d( \u0394\ud835\udc13_k ),     e_\ud835\udc2b\ud835\udc11 =  1/K\u2211_k=1^K \u2220( \u0394\ud835\udc13_k )\n\nwhere \u0164_bw and \u0164_ce are the estimated calibration parameters. The per-frame camera pose \ud835\udc13_wc,k is obtained by solving the perspective-n-point (PnP) problem. The metric re-projection error, e_\ud835\udc11\ud835\udc0c\ud835\udc12\ud835\udc04, is the re-projection root mean squared error between the measured pixel positions and the projected locations over the test set:\n\n    e_\ud835\udc11\ud835\udc0c\ud835\udc12\ud835\udc04 = \u221a(1/K-1\u2211_k=1^K \u2016u_k - \u03c0(T_ce^*T_eb, kT_bw^*P_w) \u2016_2^2 )\n\nwhere u_k is the pixel measurement of the 3D marker point set P_w from the robot pose T_eb, k.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Results\n\nFor our system and the baselines, we initialize the calibration parameters with the same set of robot poses (3 sets in our experiments). Table\u00a0<ref> shows the experimental results on the simulated dataset\u00a0<cit.>. We can observe that, although the proposed approach is not advantageous to the baselines in terms of the absolute rotation error, it can achieve much higher accuracy for the translation estimation using the same number of the collected measurement sets (28.4% and 37.7% error reduction compared to the \"random\" and \"max-distance\" baselines, respectively).\n\n\n\n\n\nFor the real dataset, we performed the evaluation on the collected validation sets, which were not used in the calibration process. Figure\u00a0<ref> presents the results for the baselines and our proposed NBV system when using different metrics. It can be seen that, with the first few additional viewpoints, the \"max-distance\" achieves higher accuracy than the \"random\" policy and has a similar performance to our proposed NBV approach. However, the performance using both \"max-distance\" and \"random\" policies becomes less obvious when visiting more viewpoints. Compared to the baselines, our approach is able to achieve the same level of parameter uncertainties and accuracy with much fewer robot poses. Moreover, we measure the Pearson correlation coefficient\u00a0<cit.> between our predicted information gain and the reduction of the re-projection error. As illustrated in Figure\u00a0<ref>, the predicted information gain accurately reflects the true error reduction, thus making them well suited to our goal. Table\u00a0<ref> further demonstrates the advantage of our approach with the same number of additional robot poses. To obtain the results, we use 5 additional measurement sets for each view selection strategy. We can see that, compared to the baselines, our proposed NBV achieves the lowest error for all three metrics.\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nIn this work, we have presented a viewpoint selection framework for eye-in-hand calibration. We first formulate the calibration problem with the \ud835\udc00\ud835\udc17=\ud835\udc18\ud835\udc01, robot-world-hand\u2013eye representation, and estimate the set of calibration parameters using a non-linear optimization process. Our approach estimates the parameter uncertainty and predicts the information gain of future measurement sets. This allows us to reduce the parameter uncertainty by selecting the robot kinematics with the highest information gain. We evaluate our method on a synthetic dataset and a real robot manipulator. The results demonstrate that our proposed next-best-view approach can achieve high calibration accuracy with much fewer robot poses when compared against baselines that use heuristic-based policies.\n\n\n\n\n\n\n\n\n\n\n\u00a7 ACKNOWLEDGMENT\n\nThis work was supported by Epson Canada Ltd.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nieeetr\n\n\n\n\n\n\n"}