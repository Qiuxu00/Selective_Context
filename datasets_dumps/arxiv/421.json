{"entry_id": "http://arxiv.org/abs/2303.06734v1", "published": "20230312192931", "title": "Environmental variability and network structure determine the optimal plasticity mechanisms in embodied agents", "authors": ["Emmanouil Giannakakis", "Sina Khajehabdollahi", "Anna Levina"], "primary_category": "q-bio.NC", "categories": ["q-bio.NC"], "text": "\n\n[\n    Daodao Yang\n    \n===============\n\n\nThe evolutionary balance between innate and learned behaviors is highly intricate, and different organisms have found different solutions to this problem. We hypothesize that the emergence and exact form of learning behaviors is naturally connected with the statistics of environmental fluctuations and tasks an organism needs to solve. Here, we study how different aspects of simulated environments shape an evolved synaptic plasticity rule in static and moving artificial agents. We demonstrate that environmental fluctuation and uncertainty control the reliance of artificial organisms on plasticity. \nInterestingly, the form of the emerging plasticity rule is additionally determined by the details of the task the artificial organisms are aiming to solve. Moreover, we show that co-evolution between static connectivity and interacting plasticity mechanisms in distinct sub-networks changes the function and form of the emerging plasticity rules in embodied agents performing a foraging task.\n\n\n\n\u00a7 INTRODUCTION\n\n\nOne of the defining features of living organisms is their ability to adapt to their environment and incorporate new information to modify their behavior. It is unclear how the ability to learn first evolved <cit.>, but its utility appears evident. Natural environments are too complex for all the necessary information to be hardcoded genetically <cit.> and more importantly, they keep changing during an organism\u2019s lifetime in ways that cannot be anticipated <cit.>. The link between learning and environmental uncertainty and fluctuation has been extensively demonstrated in both natural <cit.>, and artificial environments <cit.>. \n\nNevertheless, the ability to learn does not come without costs. For the capacity to learn to be beneficial in evolutionary terms, a costly nurturing period is often required, a phenomenon observed in both biological <cit.>, and artificial organisms <cit.>. Additionally, it has been shown that in some complex environments, hardcoded behaviors may be superior to learned ones given limits in the agent's lifetime and environmental uncertainty <cit.>.\n\nThe theoretical investigation of the optimal balance between learned and innate behaviors in natural and artificial systems goes back several decades. However, it has recently found also a wide range of applications in applied AI systems <cit.>.\nMost AI systems are trained for specific tasks, and have no need for modification after their training has been completed. Still, technological advances and the necessity to solve broad families of tasks make discussions about life-like AI systems relevant to a wide range of potential application areas. \nThus the idea of open-ended AI agents <cit.> that can continually interact with and adapt to changing environments has become particularly appealing.\n\nMany different approaches for introducing lifelong learning in artificial agents have been proposed. \nSome of them draw direct inspiration from actual biological systems <cit.>. Among them, the most biologically plausible solution is to equip artificial neural networks with some local neural plasticity <cit.>, similar to the large variety of synaptic plasticity mechanisms <cit.> that performs the bulk of the learning in the brains of living organisms <cit.>. The artificial plasticity mechanisms can be optimized to modify the connectivity of the artificial neural networks toward solving a particular task. The optimization can use a variety of approaches, most commonly evolutionary computation.  \n\nThe idea of meta-learning or optimizing synaptic plasticity rules to perform specific functions has been recently established as an engineering tool that can compete with state-of-the-art machine learning algorithms on various complex tasks <cit.>.\nAdditionally, it can be used to reverse engineer actual plasticity mechanisms found in biological neural networks and uncover their functions <cit.>.\n\nHere, we study the effect that different factors (environmental fluctuation and reliability, task complexity) \nhave on the form of evolved functional reward-modulated plasticity rules. We investigate the evolution of plasticity rules in static, single-layer simple networks. Then we increase the complexity by switching to moving agents performing a complex foraging task. In both cases, we study the impact of different environmental parameters  on the form of the evolved plasticity mechanisms and the interaction of learned and static network connectivity. Interestingly, we find that different environmental conditions and different combinations of static and plastic connectivity have a very large impact on the resulting plasticity rules.\n\n\n\n\u00a7 METHODS\n\n\n\n \u00a7.\u00a7 Environment\n\nWe imagine an agent who must forage to survive in an environment presenting various types of complex food particles. Each food particle is composed of various amounts and combinations of N ingredients that can have positive (food) or negative (poison) values.  The value of a food particle is a weighted sum of its ingredients. To predict the reward value of a given resource, the agent must learn the values of these ingredients by interacting with the environment. The priors could be generated by genetic memory, but the exact values are subject to change.\n\nTo introduce environmental variability, we stochastically change the values of the ingredients. \nMore precisely, we define two ingredient-value distributions E_1 and E_2 <cit.> and switch between them, with  probability p_tr for every time step. \nWe control how (dis)similar the environments are by parametrically setting E_2 = (1 - 2d_e)E_1, with d_e \u2208 [0,1] serving as a distance proxy for the environments; when d_e = 0, the environment remains unchanged, and when d_e = 1 the value of each ingredient fully reverses when the environmental transition happens. For simplicity, we take values of the ingredients in E_1 equally spaced between -1 and 1 (for the visualization, see <ref>a, b).\n\n\n\n\n \u00a7.\u00a7 Static agent\n\nThe static agent receives  passively presented food as a vector of ingredients and can assess its compound value using the linear summation of its sensors with the (learned or evolved) weights, see <ref>.\nThe network consists of N sensory neurons that are projecting to a single post-synaptic neuron.  At each time step, an input X_t = (x_1, \u2026, x_N) is presented, were the value x_i,  i \u2208{1, \u2026, N} represents the quantity of the  ingredient i. We draw x_i independently form a uniform distribution on the [0,1] interval (x_i \u223c U(0, 1)). The value of each ingredient w_i^c is determined by the environment (E_1 or E_2). \n\nThe postsynaptic neuron outputs a prediction of the food X_t value as y_t = g(W X_t^T). Throughout the paper, g will be either the identity function, in which case the prediction neuron is linear, or a step-function; however, it could be any other nonlinearity, such as a sigmoid or ReLU. After outputting the prediction, the neuron receives feedback in the form of the real value of the input R_t. The real value is computed as R_t = W^c X_t^T + \u03be, where  W^c = (w_1^c, \u2026, w_N^c) is the actual value of the ingredients, and \u03be is a term summarizing the noise of reward and sensing system \u03be\u223c\ud835\udca9(0, \u03c3). \n\n\n\n\nFor the evolutionary adjustment of the agent's parameters, the loss of the static agent is the sum of the mean squared errors (MSE) between its prediction y_t and the reward R_t over the lifetime of the agent. The agent's initial weights are set to the average of the two ingredient value distributions, which is the optimal initial value for the case of symmetric switching of environments that we consider here.\n\n\n\n\n \u00a7.\u00a7 Moving Agent\n\nAs a next step, we incorporate the sensory network of static agents into embodied agents that can move around in an environment scattered with food.\nTo this end, we merge the static agent's network with a second, non-plastic motor network that is responsible for controlling the motion of the agent in the environment. Specifically, the original plastic network now provides the agent with information about the value of the nearest food. The embodied agent has additional sensors for the distance from the nearest food, the angle between the current velocity and the nearest food direction, its own velocity, and its own energy level (sum of consumed food values). These inputs are processed by two hidden layers (of 30 and 15 neurons) with tanh activation. The network's outputs are angular and linear acceleration, <ref>. \n\n\n\n\nThe embodied agents spawn in a 2D space with periodic boundary conditions along with a number of food particles that are selected such that the mean of the food value distribution is \u223c 0. An agent can eat food by approaching it sufficiently closely, and each time a food particle is eaten, it is re-spawned with the same value somewhere randomly on the grid (following the setup of <cit.>). After 5000 time steps, the cumulative reward of the agent (the sum of the values of all the food it consumed) is taken as its fitness. During the evolutionary optimization, the parameters for both the motor network (connections) and plastic network (learning rule parameters) are co-evolved, and so agents must simultaneously learn to move and discriminate good/bad food.\n\n\n\n\n\n \u00a7.\u00a7 Plasticity rule parametrization\n \n\nReward-modulated plasticity is one of the most promising explanations for biological credit assignment <cit.>. In our network, the plasticity rule that updates the weights of the linear sensor network is a reward-modulated rule which is parameterized as a linear combination of the input, the output, and the reward at each time step:\n\n    \u0394 W_t = \u03b7_p [ R_t \u00b7 (\u03b8_1 X_t y_t + \u03b8_2 y_t  +   \u03b8_3 X_t + \u03b8_4)^Reward Modulated\n     +  (\u03b8_5 X_t y_t + \u03b8_6 y_t +   \u03b8_7 X_t  + \u03b8_8)_Hebbian ].\n\nAdditionally, after each plasticity step, the weights are normalized by mean subtraction, an important step for the stabilization of Hebbian-like plasticity rules <cit.>.\n\nWe use a genetic algorithm to optimize the learning rate \u03b7_p and amplitudes of different terms \u03b8 = (\u03b8_1, \u2026, \u03b8_8). The successful plasticity rule after many food presentations must converge to a weight vector that predicts the correct food values (or allows the agent to correctly decide whether to eat a food or avoid it). \n\nTo have comparable results, we divide \u03b8 = (\u03b8_1, \u2026, \u03b8_8) by  \u03b8_max = max_k|\u03b8_k|. So that \u03b8 / \u03b8_max = \u03b8^norm\u2208 [-1,1]^8. We then multiply the learning rate \u03b7_p with \u03b8_max to maintain the rule's evolved form unchanged, \u03b7_p^norm = \u03b7_p \u00b7\u03b8_max. In the following, we always use normalized \u03b7_p and \u03b8, omitting ^norm.\n\n\n\n\n \u00a7.\u00a7 Evolutionary Algorithm\n\nTo evolve the plasticity rule and the moving agents' motor networks, we use a simple genetic algorithm with elitism <cit.>. The agents' parameters are initialized at random (drawn from a Gaussian distribution), then the sensory network is trained by the plasticity rule and finally, the agents are evaluated. After each generation, the best-performing agents (top 10 % of the population size) are selected and copied into the next generation. The remaining 90 % of the generation is repopulated with mutated copies of the best-performing agents. We mutate agents by adding independent Gaussian noise (\u03c3 = 0.1) to its parameters.\n\n\n\n\u00a7 RESULTS\n\n\n\n\n \u00a7.\u00a7 Environmental and reward variability control the evolved learning rates of the static agents\n\nTo start with, we consider a static agent whose goal is to identify the value of presented food correctly. \nThe static reward-prediction network quickly evolves the parameters of the learning rule, successfully solving the prediction task. We first look at the evolved learning rate \u03b7_p, which determines how fast (if at all) the network's weight vector is updated during the lifetime of the agents. \nWe identify three factors that control the learning rate parameter the EA converges to: the distance between the environments, the noisiness of the reward, and the rate of environmental transition. \n\nThe first natural factor is the distance d_e between the two environments, with a larger distance requiring a higher learning rate, <ref>c. \nThis is an expected result since the convergence time to the  \u201ccorrect\u201d weights is highly dependent on the initial conditions.  \nIf an agent is born at a point very close to optimality, which naturally happens if the environments are similar, the distance it needs to traverse on the fitness landscape is small.  Therefore it can afford to have a small learning rate, which leads to a more stable convergence and is not affected by noise.\n\nA second parameter that impacts the learning rate is the variance of the rewards. The reward an agent receives for the plasticity step contains a noise term \u03be that is drawn from a zero mean Gaussian distribution with standard deviation \u03c3. This parameter controls the unreliability of the agent's sensory system, i.e., higher \u03c3 means that the information the agent gets about the value of the foods it consumes cannot be fully trusted to reflect the actual value of the foods. As \u03c3 increases, the learning rate \u03b7_p decreases, which means that the more unreliable an environment becomes, the less an agent relies on plasticity to update its weights,  <ref>c. Indeed for some combinations of relatively small distance d_e and high reward variance \u03c3, the EA converges to a learning rate of \u03b7_p \u2248 0. This means that the agent opts to have no adaptation during its lifetime and remain at the mean of the two environments. It is an optimal solution when the expected loss due to ignoring the environmental transitions is, on average, lower than the loss the plastic network will incur by learning via the (often misleading because of the high \u03c3) environmental cues.\n\nA final factor that affects the learning rate the EA will converge to is the frequency of environmental change during an agent's lifetime. Since the environmental change is modeled as a simple, two-state Markov process (<ref>a), the control parameter is the transition probability p_tr. \n\n\nWhen keeping everything else the same, the learning rate rapidly rises as we increase the transition probability from 0, and after reaching a peak, it begins to decline slowly, eventually reaching zero (<ref>d). This means that when environmental transition is very rare, agents opt for a very low learning rate, allowing a slow and stable convergence to an environment-appropriate weight vector that leads to very low losses while the agent remains in that environment. As the rate of environmental transition increases, faster learning is required to speed up convergence in order to exploit the (comparatively shorter) stays in each environment. Finally, as the environmental transition becomes too fast, the agents opt for slower or even no learning, which keeps them near the middle of the two environments, ensuring that the average loss of the two environments is minimal (<ref>d).\n\n\n\n\n\n \u00a7.\u00a7 The form of the evolved learning rule depends on the task: Decision vs. Prediction\n\n\n \nThe plasticity parameters  \u03b8 = (\u03b8_1, \u2026, \u03b8_8) for the reward-prediction task converge on approximately the same point, regardless of the environmental parameters (<ref>a). In particular, \u03b8_3 \u2192 1, \u03b8_5 \u2192 -1, \u03b8_i \u2192 0 for all other i, and thus the learning rule converges to:\n\n    \u0394 W_t  = \u03b7_p [\u03b8_3 X_tR_t + \u03b8_5 X_ty_t] \u2248\u03b7_p X_t (R_t - y_t).\n\nSince by definition y_t = g(W_t X_t^T) = W_t X_t^T (g(x) = x in this experiment) and R_t = W^c X_t^T + \u03be we get: \n\n    \u0394 W_t = \u03b7_p X_t(W^c - W_t)X_t^T + \u03b7_p \u03be X_t^T.\n\nThus the distribution of \u0394 W_t converges to a distribution with  mean 0 and variance depending on \u03b7_p and \u03c3 and W converges to W^c. \nSo this learning rule will match the agent's weight vector with the vector of ingredient values in the environment. \n\n\nWe examine the robustness of the learning rule the EA discovers by considering a slight modification of our task. Instead of predicting the expected food value, the agent now needs to decide whether to eat the presented food or not. This is done by introducing a step-function nonlinearity (g(x) = 1 if x \u2265 1 and 0 otherwise). Then the output y(t) is computed as:\n\n    y_t =\n        \n            1,    if   W_tX_t^T \u2265 0, \n    \n            0,    if   W_tX_t^T < 0.\n\nInstead of the MSE loss between prediction and actual value, the fitness of the agent is now defined as the sum of the food values it chose to consume (by giving y_t = 1). Besides these two changes, the setup of the experiments remains exactly the same.\n\nThe qualitative relation between \u03b7_p and parameters of environment d_e, \u03c3 and p_tr is preserved in the changed experiment. However, the resulting learning rule is significantly different (<ref>). The evolution converges to the following learning rule:\n\n    \u0394 W_t =  \n        \u03b7_p X_t [\u03b8_3 R_t + \u03b8_7],  y_t = 0, \n    \u03b7_p X_t[(\u03b8_1 + \u03b8_3) R_t + (\u03b8_5 + \u03b8_7)],   y_t = 1.\n\nIn both cases, the rule has the form \u0394 W_t =  \u03b7_p X_t [\u03b1_y R_t + \u03b2_y]. Thus, the \u0394 W_t is positive or negative depending on whether the reward R_t is above or below a threshold (\u03b3 = -\u03b2_y / \u03b1_y) that depends on the output decision of the network (y_t = 0 or 1). \n\nBoth learning rules (for the reward-prediction and decision tasks) have a clear Hebbian form (coordination of pre- and post-synaptic activity) and use the incoming reward signal as a threshold. These similarities indicate some common organizing principles of reward-modulated learning rules, but their significant differences highlight the sensitivity of the optimization process to task details.\n\n\n\n\n \u00a7.\u00a7 The learning rate of embodied agents depends on environmental variability\n\n\n\n\nWe now turn to the moving embodied agents in the 2D environment. To optimize these agents, both the motor network's connections and the sensory network's plasticity parameters evolve simultaneously. Since the motor network is initially random and the agent has to move to find food, the number of interactions an agent experiences in its lifetime can be small, slowing down the learning.\nHowever, having the larger motor network also has benefits for evolution because it allows the output of the plastic network to be read out and transformed in different ways, resulting in a broad set of solutions.\n\nThe agents can solve the task effectively by evolving a functional motor network and a plasticity rule that converges to interpretable weights (<ref>a). After \u223c 100 evolutionary steps (<ref>d), the agents can learn the ingredient value distribution using the plastic network and reliably move towards foods with positive values while avoiding the ones with negative values.\n\nWe compare the dependence of the moving and the static agents on the parameters of the environment: d_e and the state transition probability p_tr. At first, in order to simplify the experiment, we set the transition probability to 0, but fixed the initial weights to be the average of E_1 and E_2, while the real state is E_2. In this experiment, the distance between states d_e indicates twice the distance between the agent's initial weights and the optimal weights (the environment's ingredient values) since the agent is initialized at the mean of the two environment distributions. Same as for the static agent, the learning rate increases with the distance d_e  (<ref>b).\n\nThen, we examine the effect of the environmental transition probability p_tr on the evolved learning rate \u03b7_p. In order for an agent to get sufficient exposure to each environment, we scale down the probability p_tr from the equivalent experiment for the static agents. We find that as the probability of transition increases, the evolved learning rate \u03b7_p decreases (<ref>c). This fits with the larger trend for the static agent, although there is a clear difference when it comes to the increase for very small transition probabilities that were clearly identifiable in the static but not the moving agents. This could be due to much sparser data and possibly the insufficiently long lifetime of the moving agent (the necessity of scaling makes direct comparisons difficult). Nevertheless, overall we see that the associations observed in the static agents between environmental distance d_e and transition probability p_tr and the evolved learning rate \u03b7_p are largely maintained in the moving agents. Still, more data would be needed to make any conclusive assertions about the exact effect of these environmental parameters on the emerging plasticity mechanisms.\n\n\n\n \u00a7.\u00a7 Rule redundancy in the embodied agents\n\n\nA crucial difference between the static and the moving agents is the function the plasticity has to perform. While in the static agents, the plasticity has to effectively identify the exact value distribution of the environment in order to produce accurate predictions, in the embodied agents, the plasticity has to merely produce a representation of the environment that the motor network can evolve to interpret adequately enough to make decisions about which food  to consume.\n\n\n\n\nTo illustrate the difference, we plot the Pearson correlation coefficient between an agent's weights and the ingredient values of the environment it is moving in (<ref>e). We use the correlation instead of the MSE loss (which we used for the static agents in <ref>e) because the amplitude of the weight vector varies a lot for different agents and meaningful conclusions cannot be drawn from the MSE loss. For many agents, the learned weights are consistently anti-correlated with the actual ingredient values (an example of such an agent is shown in <ref>e). This means that the output of the sensory network will have the opposite sign from the actual food value. While in the static network, this would lead to very bad predictions and high loss, in the foraging task, these agents perform exactly as well as the ones where the weights and ingredients values are positively correlated, since the motor network can simply learn to move towards food  for which it gets a negative instead of a positive sensory input.\n\nThis additional step of the output of the plastic network going through the motor network before producing any behavior has a strong effect on the plasticity rules that the embodied agents evolve. Specifically, if we look at the emerging rules the top performing agents have evolved (<ref>a), it becomes clear that, unlike the very well-structured rules of the static agents (<ref>a), there is now virtually no discernible pattern or structure. The difference becomes even clearer if we look at the learned weights (at the end of a simulation) of the best-performing agents (<ref>c). While there is some correlation with the environment's ingredient value distribution, the variance is very large, and they do not seem to converge on the  \u201ccorrect\u201d values in any way. This is to some extent expected since, unlike the static agents where the network's output has to be exactly correct, driving the evolution of rules that converge to the precise environmental distribution, in the embodied networks, the bulk of the processing is done by the motor network which can evolve to interpret the scalar value of the sensory network's output in a variety of ways. Thus, as long as the sensory network's plasticity rule co-evolves with the motor network, any plasticity rule that learns to produce consistent information about the value of encountered food  can potentially be selected.  \n\n\nTo further test this assumption, we introduce a bottleneck of information propagation between the sensory and motor networks by using a step-function nonlinearity on the output of the sensory network (Eq.\u00a0<ref>). Similarly to the decision task of the static network, the output of the sensory network now becomes binary. This effectively reduces the flow of information from the sensory to the motor network, forcing the sensory network to consistently decide whether food should be consumed (with the caveat that the motor network can still interpret the binary sign in either of two ways, either consuming food  marked with 1 or the ones marked with 0 by the sensory network). The agents perform equally well in this variation of the task as before  (<ref>d), but now, the evolved plasticity rules seem to be more structured (<ref>b). Moreover, the variance of the learned weights in the best-performing agents is significantly reduced (<ref>d), which indicates that the bottleneck in \n the sensory network is increasing selection pressure for rules that learn the environment's food distribution  accurately.\n \n\n\n\u00a7 DISCUSSION\n\n\nWe find that different sources of variability have a strong impact on the extent to which evolving agents will develop neuronal plasticity mechanisms for adapting to their environment. A diverse environment, a reliable sensory system, and a rate of environmental change that is neither too large nor too small are necessary conditions for an agent to be able to effectively adapt via synaptic plasticity. Additionally, we find that minor variations of the task an agent has to solve or the parametrization of the network can give rise to significantly different plasticity rules. \n\nOur results partially extend to embodied artificial agents performing a foraging task. We show that environmental variability also pushes the development of plasticity in such agents. Still, in contrast to the static agents, we find that the interaction of a static motor network with a plastic sensory network gives rise to a much greater variety of well-functioning learning rules. We propose a potential cause of this degeneracy; as the relatively complex motor network is allowed to read out and process the outputs from the plastic network, any consistent information coming out of these outputs can be potentially interpreted in a behaviorally useful way. Reducing the information the motor network can extract from the sensory system significantly limits learning rule variability.\n\nOur findings on the effect of environmental variability concur with the findings of previous studies <cit.> that have identified  the constraints that environmental variability places on the evolutionary viability of learning behaviors. We extend these findings in a mechanistic model which uses a biologically plausible learning mechanism (synaptic plasticity). We show how a simple evolutionary algorithm can optimize the different parameters of a simple reward-modulated plasticity rule for solving simple prediction and decision tasks. Reward-modulated plasticity has been extensively studied as a plausible mechanism for credit assignment in the brain <cit.>  and has found several applications in artificial intelligence and robotics tasks <cit.>. Here, we demonstrate how such rules can be very well-tuned to take into account different environmental parameters and produce optimal behavior in simple systems.\n\nAdditionally, we demonstrate how the co-evolution of plasticity and static functional connectivity in different sub-networks fundamentally changes the evolutionary pressures on the resulting plasticity rules, allowing for greater diversity in the form of the learning rule and the resulting learned connectivity. Several studies have demonstrated how, in biological networks, synaptic plasticity heavily interacts with <cit.> and is driven by network topology <cit.>. Moreover, it has been recently demonstrated that biological plasticity mechanisms are highly redundant in the sense that any observed neural connectivity or recorded activity can be achieved with a variety of distinct, unrelated learning rules\n<cit.>. This observed redundancy of learning rules in biological settings complements our results and suggests that the function of plasticity rules cannot be studied independently of the connectivity and topology of the networks they are acting on. \n\nThe optimization of functional plasticity in neural networks is a promising research direction both as a means to understand biological learning processes and as a tool for building more autonomous artificial systems. Our results suggest that reward-modulated plasticity is highly adaptable to different environments and can be incorporated into larger systems that solve complex tasks.\n\n\n\n\n\u00a7 FUTURE WORK\n\n\nThis work studies a simplified toy model of neural network learning in stochastic environments. Future work could be built on this basic framework to examine more complex reward distributions and sources of environmental variability. Moreover, a greater degree of biological realism could be added by studying more plausible network architectures (multiple plastic layers, recurrent and feedback connections) and more sophisticated plasticity rule parametrizations. \n\nAdditionally, our foraging simulations were constrained by limited computational resources and were far from exhaustive. Further experiments can investigate environments with different constraints, food distributions, multiple seasons, more complex motor control systems and interactions of those systems with different sensory networks as well as the inclusion of plasticity on the motor parts of the artificial organisms. \n\n \n\n\u00a7 ACKNOWLEDGEMENTS\n\nThis work was supported by a Sofja Kovalevskaja Award from the Alexander von Humboldt Foundation. EG and SK thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for their support. We acknowledge the support from the BMBF through the T\u00fcbingen AI Center (FKZ: 01IS18039A). AL is a member of the Machine Learning Cluster of Excellence, EXC number 2064/1 \u2013 Project number 39072764. \n\n\napalike\n\n\n\n"}