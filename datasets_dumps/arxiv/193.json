{"entry_id": "http://arxiv.org/abs/2303.07084v1", "published": "20230313130820", "title": "The challenge of representation learning: Improved accuracy in deep vision models does not come with better predictions of perceptual similarity", "authors": ["Fritz G\u00fcnther", "Marco Marelli", "Marco Alessandro Petilli"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\n\nThe challenge of representation learning: Improved accuracy in deep vision models does not come with better predictions\nof perceptual similarity\n    Fritz G\u00fcnther\n\nHumboldt-Universit\u00e4t zu Berlin\n\nUnter den Linden 6, 10117 Berlin, Germany\n\nfritz.guenther@hu-berlin.de\n\n\n\n\n\nMarco Marelli\n\nUniversity of Milano-Bicocca\n\nPiazza dell'Ateneo Nuovo 1, 20126 Milano, Italy\n\nmarco.marelli@unimib.it\nMarco Alessandro Petilli\n\nUniversity of Milano-Bicocca\n\nPiazza dell'Ateneo Nuovo 1, 20126 Milano, Italy\n\nmarco.petilli@unimib.it\n\n    March 30, 2023\n======================================================================================================================================================================================================================================================================================================================================================================================\n\n\nempty\n\n\n\n\n   Over the last years, advancements in deep learning models for computer vision have led to a dramatic improvement in their image classification accuracy. However, models with a higher accuracy in the task they were trained on do not necessarily develop better image representations that allow them to also perform better in other tasks they were not trained on. In order to investigate the representation learning capabilities of prominent high-performing computer vision models, we investigated how well they capture various indices of perceptual similarity from large-scale behavioral datasets. We find that higher image classification accuracy rates are not associated with a better performance on these datasets, and in fact we observe no improvement in performance since GoogLeNet (released 2015) and VGG-M (released 2014). We speculate that more accurate classification may result from hyper-engineering towards very fine-grained distinctions between highly similar classes, which does not incentivize the models to capture overall perceptual similarities.    \n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nOver the last decade, following the seminal work by Krizhevsky, Sutskever, and Hinton <cit.>, computer vision models based on deep neural network architectures have become increasingly powerful, and nowadays achieve very high levels of performance <cit.>. This performance is typically assessed on the very task used in model training, most often as the accuracy in image classification (using measures such as top-1 error or top-5 error <cit.>).\n\nAs these models achieve higher and higher performance in such scenarios, they also tend to become increasingly sophisticated and complex in terms of model architecture and the numbers of parameters to be estimated. However, this additional complexity does not necessarily imply that these models generally perform better, also on domains they are not trained on: such an approach runs the risk of having systems that are over-optimized for a particular (set of) tasks, without gaining much in terms of transfer and generalizability <cit.>.\n\n\nThese aspects play an important role in machine learning, often discussed under the label of representation learning <cit.>). However, the point is even more relevant when these systems are used as general-level vision models for research purposes. In that respect, an emerging line of research in the domains of  computational neuroscience and cognitive science has started to investigate and employ computer vision models (originally designed and trained for image classification)  as models for human visual representation and processing, with very promising results from recent studies <cit.>. These works also provide us with rich, large-scale datasets of human behavioral data that allow us to investigate to which extent current computer-vision models can serve as general-level vision models, with much wider scientific applications than being pure image classifiers <cit.>. Following these developments, in the present study, we will systematically examine which models perform best when tested against a battery of behavioral datasets, and if such models also turn out to be the most complex and best-performing image classifiers.\n\n\n\n\n\u00a7 RELATED WORK\n\n\nIn the development of language models, human behavioral data have long been established as a gold standard for model evaluation (e.g. <cit.>). The most prominent example are ratings of word similarity, with widely-used datasets such as WordSim353 <cit.>, SimLex999 <cit.>, or MEN <cit.>.\n\nAnalogously, ratings of image similarity are widely employed to evaluate and compare the performance of computer vision models. This includes pairs of different naturalistic images  <cit.>, as well as comparisons between real images and their distorted versions <cit.>. In a recent study, Roads and Love collected similarity ratings for a very large collection of 50,000 ImageNet images, which were not only used for evaluation but also to enrich computer vision with participant-sourced information <cit.>.\n\nMore recently, G\u00fcnther al al.  <cit.> released a collection of several large-scale data sets, comprising rating data as well as on-line processing data in the form of response times, which were used to evaluate a VGG-based vision model <cit.>. These will constitute the gold standard datasets for our present study, where we systematically evaluate the performance of a wide range of models against data that are cognitively relevant, but relatively atypical for the computer vision domain, and far from the tasks on which systems are typically optimized.\n\n\n\n\n\u00a7 DATASETS\n\n\nWe considered the following metrics (see <cit.> for detailed descriptions of the data collection procedures):\n\n\n\n\n  * Ratings of\n\n\n  * image similarity [IMG] for 3,000 pairs of naturalistic ImageNet images. Data were collected from 480 participants, with 30 observations per image pair.\n\n\n  * visual word similarity [WORD] for 3,000 word pairs (image labels of the aforementioned 3,000 image pairs), where participants were asked to judge how similar of the objects denoted by the words (i.e., the word referents) look like. Thus, unlike other word-based ratings, <cit.>, these data focus on the visual domain. Data were collected from 480 participants, with 30 observations per word pair.\n\n\n  * typicality ratings [TYP] for 7,500 word-image pairs (1,500 sets of image labels and five images tagged with that label), where participants were asked to indicate the most and least typical image for the category denoted by the presented label. Data were collected from 902 participants, with 30 observations per word-image pair.\n\n\n\n\nAll ratings were collected using the best-worst method <cit.>, so participants were always presented with a set of stimuli and asked to pick the most and least relevant for the given task. Responses were then scored on a continuous scale using the Value learning algorithm <cit.>. As a result, the datasets contain exactly one rating score between 0 (completely dissimilar) and 1 (identical) for each word pair in the WORD dataset  and each image pair in the IMG dataset, and one score between 0 (very atypical) and 1 (very typical) for each word-image pair in the TYP dataset. Examples for items with very high and very low ratings are presented in Figure\u00a0<ref>\n\n\n\n\n\n\n\n  * Processing time data\n\n\n  * discrimination task [DIS]  for the same 3,000 image pairs of the IMG dataset. In a discrimination task, two stimuli (here: images) are presented in very rapid succession, and participants have to indicate whether they are identical or different by pressing one of two buttons (see Figure\u00a0<ref>, upper panel for a schematic representation of an experimental trial). Responses are typically slower for more visually similar stimuli, which are harder to discriminate from the actual stimulus. Data were collected from 750 participants, with 30 observations per image pair.\n\n\n  * priming task [PRIM] for the same 3,000 image pairs of the IMG and DIS datasets. In a priming task, two stimuli (here: images) are presented in quick succession, and participants have to perform a task on the second image only (here: judge whether a real or scrambled image has been presented by pressing one of two buttons); see Figure\u00a0<ref>, lower panel for a schematic representation of an experimental trial. Responses are typically faster when the stimulus was preceded by a more visually similar stimulus, which primes (= facilitates processing of) the target. Data were collected from 750 participants, with 30 observations per image pair.\n\n\n\n\n\n\n\n\n\n\nThe target variable in these processing time studies is the mean response time for each image pair, after removing erroneous trials and outliers with far too slow or fast responses.\n\nAll datasets are publicly available in an OSF repository associated to the original study <cit.> at  <https://doi.org/10.17605/OSF.IO/QVW9C>.\n\n\n\n\u00a7 VISION MODELS\n\n\n\n\n \u00a7.\u00a7 Models employed\n\n\nFor this study, we considered all pre-trained vision models available in the MatConvNet <cit.> and Deep Learning Toolbox (<https://github.com/matlab-deep-learning/MATLAB-Deep-Learning-Model-Hub>) packages for MATLAB. A full list of models is provided in Table\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 General setup: Image and prototype representations\n\n\nIn line with previous studies <cit.>, we extracted the activation values in each convolutional and fully-connected layer of a model for a given input image (i.e., image embeddings) as representations for that image. In addition, we constructed prototype vectors for image labels as the centroid of 100\u2013200 image embeddings of images tagged with that label (using the very same method presented in <cit.>). For each image label, we obtain such a prototype representation for each layer of each considered model.\n\nWe used the cosine similarity metric to compute similarities between these image embeddings (at the same layer of the same model). In this manner, we can obtain a metric for the similarity between two individual images (for the IMG, DIS and PRIM datasets), the overall visual similarity between two categories denoted by their respective image labels (for the WORD dataset), and the similarity between an individual image and its category (for the TYP dataset) at each layer for each model.\n\n\n\n\n\n\u00a7 RESULTS\n\n\nSince relations between the model-derived similarities and the behavioral outcome variables are mostly non-linear <cit.>, performance was assessed using Spearman rank correlations. All predictors (i.e., similarities based on each layer of each model) were ranked in terms of performance on each behavioral dataset, and these ranks were used to calculate three general-level evaluation metrics: \n\n\n\n    \n  * The rating performance as the mean rank across the three rating datasets (IMG, WORD, and TYP)\n\n    \n  * The processing time performance as the mean rank across the two processing-time datasets (DIS and PRIM)\n\n        \n  * The overall performance as the mean rank across all behavioral datasets  (compare <cit.>)\n\n\n    \nThe results for the best-performing layers for each evaluation metric are displayed in Table\u00a0<ref>. We include the best-performing layer in the paper by G\u00fcnther et al. <cit.> (VGG-F, fully-connected layer 6) as a reference condition. Note that, for the PRIM dataset, participants tend to respond faster (that is, lower response times) if the two images are more similar; therefore, the target metric here is a more negative correlation. \n\n\n\n\nAs can be seen in Table\u00a0<ref>, the overall best-performing representations (i.e., the model estimates that are most associated with behavioral variables) are provided by the GoogLeNet model, more specifically one of the representations in the 5th layer of the model (5a_3x3_reduce). These representations are also best-performing when it comes to predicting the arguably most fundamental types of behavioral data, similarity judgments within a given modality (i.e., between two different images [IMG] and between two different categories [WORD]).\n\nFocusing only on the explicit rating data ([IMG], [WORD], and [TYP]), the best-performing representations are provided by the 7th layer (a fully-connected layer) of the VGG-M-1024 variant, closely followed by the same layer of the VGG-M-2048 variant. Although these perform slightly worse for the [IMG] dataset than the best-performing GoogLeNet layer (and very marginally worse for the [WORD] dataset), they make up for this with a near top-level performance in the [TYP] dataset (with the 50th convolutional layer of the DarkNet-53 model as the top-performer). However, these models fall behind a mean rank of 240 for the processing time data.\n\nWhen focusing only on the processing time data ([DIS] and [PRIM]), the 6th layer (again, a fully-connected layer) of the VGG-M model (standard variant) performs best, with near top-level performance in both individual datasets (those top-performers being a layer of the EfficientNet B2 model and of the ResNet-50 model, respectively). However, conversely to the 7th layers in the VGG-M-1024 and VGG-M-2048 variants, these representations in turn fall behind for the rating data, with a mean rank of 157. \n\n\n\n\n \u00a7.\u00a7 Comparison with model characteristics\n\n\nIn an additional step, we assessed the relation between the characteristics of a model (more specifically, their number of parameters and their top-1 classification accuracy <cit.>; see Table\u00a0<ref>) and its performance on the behavioral datasets tested here. To this end, we equated the overall model performance with the performance of its best-performing layer, as measured by the mean rank.\n\nWe estimated two separate non-linear statistical models (GAMs; <cit.>), modelling mean rank as a function of model accuracy and number of parameters, the results of which are depicted in Figure\u00a0<ref>. Note that a lower mean rank indicates better performance. As can be seen in these plots, medium levels of classification accuracy tend to be associated with better performance against behavioral data (with two local minima around 65% and around 75%). Regarding the number of parameters, either very low or very high numbers tend to be associated with better model performance; however, a closer inspection of the individual data points indicates that the best-performing models all have a rather low number of parameters.\n\n\n\n\n\n\n\n\n\u00a7 DISCUSSION\n\n\n\n\n \u00a7.\u00a7 Implications of the results\n\n\nIn the present study, we investigated which representations obtained from different computer-vision models best predict a battery of five large-scale behavioral datasets, including both rating data and processing time data. We find that the overall best-performing models are in fact quite \u201cold\u201d models, given the pace of the research cycle within the field: A layer of the GoogLeNet model (published 2015, <cit.>) displays the overall highest performance across all five datasets, and different layers (of different variants) of the VGG-M model (published 2014, <cit.>) display the best overall performance for the rating data and processing time data, respectively. Note that the differences in performance between the individual representations are meaningful and not trivial: For example, the difference in performance for the [IMG] dataset between the overall best GoogLeNet layer (0.774) and the overall second-best DarkNet-19 layer (0.740) is already more than three percentage points.\n\nOver the last years, a lot of effort has gone into developing systems with ever better performance than these \u201colder\u201d models. With respect to the task these models are designed for \u2013 most prominently, image classification \u2013 this effort has reached impressive successes: As can be seen in Table\u00a0<ref>, the top-1 accuracy for the ILSVRC 2012 validation data <cit.> has increased dramatically, from around 60% in 2014/2015 to around 80%. In comparison, GoogLeNet (66.3 %) and especially VGG-M (around 60 % for all variants) definitely fall on the lower end of this scale. This however reveals an interesting rift opening with respect to model performance: Even though more recent models get better and better on their target tasks, this improvement in classification accuracy does not go along with improvements in predicting other types of data  (in fact the contrary, compare Figure\u00a0<ref>, upper panel). This is not to say that more recent models show low performance on this type of data: Representations from recent models and highly accurate models like DarkNet-19 <cit.> are among the best-performing representations available. The critical point however still remains that the strong improvement in classification accuracy has not been accompanied by an improvement in predicting other types of data.\n\nOn the other hand, we find no clear connection between model complexity and top performance in the behavioral dataset: The GoogLeNet model is relatively small in terms of parameters (7 mio.) and comes with an intermediate number of layers (22), while the VGG-M models are quite large  (around 90 to 100 mio. parameters) but have only a few layers (8). Therefore, one can neither conclude that a model needs to be very large and complex for top-level performance on behavioral data (consider especially the better performance of the  VGG-M model vis-\u00e0-vis the conceptually and architecturally similar VGG-16 and VGG-19 models), nor that it needs to be particularly small and efficient (compare also Figure\u00a0<ref>, lower panel). \n\n\n\nAt this point, we can only speculate why more recent and more classification-accurate models don't perform better in accounting for behavioral data. One possible explanation may be that the models are optimized to predict a very specific image class, and only exact matches as a hit when calculating accuracy \u2013 with the mis-classification of a spotted salamander as a European fire salamander treated as a miss in the same way a mis-classification as a  toaster is. This may lead the models to weight relatively specific details to a similar or maybe even larger degree than the overall structure/\u201cgestalt\u201d of the depicted object. Human judgments and responses, on the other hand, are more driven by these general-level similarities (e.g. <cit.>) rather than details (even if those are very informative for classification); this might lead to the observed discrepancy between classification accuracy and performance on behavioral datasets. However, we want to stress again that this is speculation on an open question, and more research is necessary to properly investigate and explain this discrepancy.\n\n\n\n\n \u00a7.\u00a7 Limitations and future directions\n\n\nAt this point, we need to emphasize that all the issues discussed so far are based on the results of our evaluation, and therefore necessarily restricted to the models analysed here. However, there may well be models we did not consider here which contradict our findings (i.e., a high-accuracy model that simultaneously has a higher performance on behavioral data than the best-performing models identified here). In fact, in the context of successful transfer learning, we would consider this highly desirable, and hope that our study can give an impetus to systematically consider behavioral data in the search for an overall well-performing model.\n\nWhile one may dismiss the behavioral data analysed here as not relevant for evaluating the performance of computer vision models, we argue that at the very least recognizing which images are more or less similar to one another should be considered one of the core prerequisites for a general-level vision model, analogous to semantic models predicting semantic similarity and relatedness data in the NLP domain <cit.>.\n\nIn general, a desirable direction for future work in the field would be to develop general-level models that do not only excel in one particular task, but perform well across a range of different tasks. Ideally, in the spirit of successful transfer learning, this would not simply mean optimizing a single model for a range of different tasks, but instead testing such a model on a battery of tasks it was not optimized for <cit.>. Following up on our suspicion that the lack of improvement in representation learning could be the result of hyper-engineering to distinguish very specific (and somewhat arbitrary) categories, we speculate that possible routes of advancement to achieve models representations that better capture a general similarity structure could be as follows: On the one hand, the training objective of the models could be altered to not only consider exact hits among a set of candidate categories, but to also partially reward close hits, for example based on their word embedding similarity or their WordNet distance to the correct target  (thus rewarding the classification of a poodle as a dalmatian or as a dog more than as a Persian cat, and that more than as a pillow; see also <cit.>). On the other hand, the training sets of the models could be altered to more closely approximate human visual experience rather than over-representing certain categories <cit.>, or to include more than one correct label per image <cit.>. \n\nWe argue that such developments would be interesting from an engineering/transfer learning viewpoint (since a successful general-level model could be applied to new tasks that it was not originally optimized for), but also for the application of such systems as models of human visual representations in cognitive (neuro)science.\n\n\n\n\u00a7 DATA AVAILABILITY\n\n\nData and the analysis script for this study are available at <https://osf.io/sx5u3/?view_only=09c05b84a52246d5b8b061cbbee10350>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nieee_fullname\n\n\n\n"}