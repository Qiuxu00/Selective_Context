{"entry_id": "http://arxiv.org/abs/2303.06904v1", "published": "20230313074641", "title": "Contextually-rich human affect perception using multimodal scene information", "authors": ["Digbalay Bose", "Rajat Hebbar", "Krishna Somandepalli", "Shrikanth Narayanan"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.CL"], "text": "\n\nEffects of Nb Doping on the Charge-Density Wave and Electronic Correlations in the Kagome Metal Cs(V_1-xNb_x)_3Sb_5\n    Hai-Hu\u00a0Wen\n    March 30, 2023\n===================================================================================================================\n\n\u2020The work was done while the author was at USCfootnote\n\n\n\n\nThe process of human affect understanding involves the ability to infer person specific emotional states from various sources including images, speech, and language. Affect perception from images has predominantly focused on expressions extracted from salient face crops. However, emotions perceived by humans rely on multiple contextual cues including social settings, foreground interactions, and ambient visual scenes. In this work, we leverage pretrained vision-language (VLN) models to extract descriptions of foreground context from images. Further, we propose a multimodal context fusion (MCF) module to combine foreground cues with the visual scene and person-based contextual information for emotion prediction. We show the effectiveness of our proposed modular design on two datasets associated with natural scenes and TV shows.\n\n\n\nEmotion recognition, Context understanding, Multimedia, Multimodal vision-language pretrained models, Multimodal interaction modeling\n\n\n\n\u00a7 INTRODUCTION\n\n\nThere has been increased interest in understanding the affective processes associated with various facets of human emotions <cit.>. An integral part of affective understanding is the ability to infer expressions of human emotions from various sources like images <cit.>, speech <cit.> and language use. Affect recognition systems have enabled multiple human-centered applications, notably in healthcare (depression detection <cit.>, autism-spectrum diagnosis <cit.>) and learning <cit.>. \nAffect recognition from images has largely focused on facial expressions <cit.> along a fixed set of categories. Moreover, facial expression based methods typically consider crops of a single face, which might provide ambiguous signals for classifying perceived emotions. Emotion perception in humans typically relies on multimodal behavioral cues, that go beyond facial expressions, such as voice and language <cit.>. However, are there additional contextual cues beyond behavioral expressions, such as of face and language, that mediate human emotion perception? Studies have shown that contextual information including the social setting, interaction type, and ambient location can play a key role <cit.>. Context in images is driven by visual scenes <cit.> or specific locations such as outdoor, indoor,  kitchen, living room etc and the interactions between the various entities in the scene.\n\nAn example is shown in Fig.<ref> with respect to both (a) positive and (b) negative emotions. In Fig.<ref> (a), the face crop provides a negative signal whereas the overall scene including the generated caption from a pretrained vision-language model OFA <cit.> indicates a positive event associated with the person. In Fig. <ref>(b), while the face crop provides a noisy, incomplete signal for perceiving the expressed emotional state, the overall context of the visual scene plus its descriptive caption indicates the distressing situation associated with street flooding. \n\nRecent advances in multimodal vision-language (VLN) pretraining <cit.> has resulted in task-agnostic transformer-based models that can be used for variety of tasks such as image captioning and visual question answering. In this work, we employ the pretrained VLN models as experts for describing foreground context in terms of captions. Further we consider contextual information in terms of the individual persons (whole-body appearance or face) and the visual scenes. In order to effectively leverage multiple contextual sources we propose attention-based multimodal context fusion (MCF) module to predict both discrete emotion labels and continuous-valued arousal, valence and dominance values. We show the effectiveness of our methods on two publicly available datasets: EMOTIC <cit.> (natural scenes) and CAER-S <cit.>(TV shows).\nCode:https://github.com/usc-sail/mica-context-emotion-recognition\n\n\n\u00a7 RELATED WORK\n\n\nContext in Emotion Perception: The role of context extraneous to a person (beyond their traits and behavior) in the perception of their expressed emotion has been studied from the perspective of scenes <cit.>, and cultures <cit.>. In <cit.>, the perceivable-encoding context and the prior knowledge available with the perceivers are reported as the major sources of context for influencing emotion perception.  \nSituational context like reactions from other people has been considered in <cit.> as a means to decode emotional states of persons in consideration.\n\nContext-based image datasets: Image-based emotion recognition datasets like AffectNet <cit.>, FER <cit.>, DFEW <cit.> primarily focus on signals encoded in facial expressions. Since emotion perception depends on where the facial configuration is present, datasets like EMOTIC <cit.> and CAER <cit.> have been proposed to incorporate contextual information in terms of visual scenes and social interactions. In the case of EMOTIC, annotators have marked person instances in unconstrained environments with apparent emotional states based on the existing scene context. However, the annotation process in CAER revolves around TV shows with primary focus on interactions-driven context. \n\n\nContext modeling approaches: \n<cit.> explore context modeling in terms of dual stream models for processing body and whole image streams. <cit.> also uses a dual stream network with context stream, modeled using an affective graph composed of region proposals.<cit.> uses depth and pose as additional contextual signals with existing streams like scene, face for predicting person specific emotion .\n<cit.> explores contextual modeling in short movie clips by considering scene and action characteristics along with body (including face) based signals. In contrast, our approach uses natural language descriptions to describe the foreground context along with scene and person specific streams.\n\n\nMultimodal vision-language models: Vision language (VLN) models like OFA <cit.>, VL-T5 <cit.>, ALBEF <cit.> are pretrained on large-scale image-text pairs curated from the web, thus enabling its usage in diverse tasks like image captioning, retrieval, visual-question answering etc. In our formulation, we harness the capabilities of VLN models to generate descriptive captions since they contain condensed description of the foreground context in terms of entities including persons.\n\n\n\u00a7 PROBLEM FORMULATION\n\n Given an image I and a person-specific context in the form of bounding box [x,y,w,h], the task is to predict the emotional state p associated with a person as p_disc, p_cont = F(I,[x,y,w,h]). p_disc and p_cont refer to the predicted set of discrete emotion categories and continuous arousal, valence and dominance values, respectively. F refers to the deep neural network used for estimating the discrete and continuous affective states. The design of F is dependent on extraction of multiple contextual information from the given image I, that are listed as below:\n\n Visual scene context: The underlying visual scene (VS) (e.g.,  kitchen, bar, football field etc) plays a role in influencing the emotional state of a person. Here we use a ViT <cit.> model (f_VS) finetuned on Places365 <cit.> as the backbone network for extracting visual scene representations (e_VS) from I. \n\n Person context: The person-specific context is extracted using a whole-body or facial bounding box, denoted by [x,y,w,h] from image I. The cropped person instance is passed through a person encoder (f_PE) i.e. Resnet34\u00a0<cit.> for extracting person-centric representations (e_PE). \n\n Language driven foreground context: Natural language description of image I provides foreground (FG) context in terms of entities including persons and their interactions. We use a 12-layer transformer encoder-decoder model OFA_large <cit.> as f_expert to extract the foreground specific captions for image I. For extracting text representations (e_FG) of the captions, we use BERT's <cit.> pretrained encoder (f_FG) from HuggingFace <cit.>.\n\n\n\u00a7 MULTIMODAL CONTEXT FUSION (MCF) MODULE\n\n\nThe multimodal context fusion module is composed of two parallel streams, associated with foreground and visual scene based contexts. The basic operation in individual streams is a cross-modal encoder block CMenc composed of L encoder layers. As shown in Fig <ref>, we consider two designs for the encoder layer i.e., MHAenc and SAG-MHAenc.  \nThe set of operations in encoder MHAenc layer for query (Q), key (K) and value (V) representations are listed as follows:\n\n    Q^'    = LN(Q + Dropout(MHA(Q,K,V))) \n     \n    Q^*    =LN(Dropout(FFN(Q^'))+Q^')\n\nHere MHA, LN and FFN refer to Multi-head attention, layer-norm operation and feed-forward neural network respectively. The SAG-MHAenc layer consists of a multi-head attention based transformation of the query representations followed by input to the MHAenc layer. The design of SAG-MHAenc is inspired from multimodal co-attention layer proposed in <cit.> for visual question answering task.\n\n    Q^'    = LN(Q + Dropout(MHA(Q,Q,Q))) \n     \n    Q^*    = MHA_enc( Q^',K,V)\n\nIn CMenc, the output from the ith encoder layer Enc_i is passed as query (Q) to the subsequent layer with the key (K) and value (V) remaining the same. Here, Enc_i layer can be either MHAenc or SAG-MHAenc.\n\n    Q_i   =Enc_i(Q_i-1,K,V)      i > 0 \n    \n        Q_0   =Enc_0(Q,K,V)\n\nWe use separate CMenc blocks for processing the foreground and visual scene guided context streams. MCF (MHAenc) and MCF (SAG-MHAenc) consists of 4 MHAenc layers (8 heads and hidden dimension=512) and 3 SAG-MHAenc layers (8 heads and hidden dimension=768) in the CMenc blocks respectively.\n\nThe details of the respective context-guided streams are listed as follows:\n\nForeground-guided context stream: We use e_PE as query (Q) and e_FG as key and value inputs to the CMenc encoder.\n\nVisual-scene guided context stream: Similar to the foreground-guided context stream, we use e_PE as query (Q) and e_VS as key and value inputs to the CMenc encoder.\n\nContext fusion:\nThe outputs from the context-guided streams i.e., e_PE_FG and e_PE_VS are average pooled and concatenated to obtain a fused embedding as: e_fusion=[e_FG_avg;e_VS_avg]\n\n\n\n\n\u00a7 EXPERIMENTS\n\nEMOTIC: We use a non-intersecting split of 13584 and 4389 images for training and validation. For testing we use the publicly available split of 5108 images. For joint prediction of 26 discrete emotion classes and the continuous-valued AVD ratings, we use multiple fully-connected (FC) heads in the MCF module with e_fusion as input (Fig <ref>). The person specific instance in each image is defined by the ground truth person box. We do not consider face as a part of person-specific context since approx 25% of images do not have visible faces. \n\nFor training MCF with MHAenc and SAG-MHAenc layers, we use AdamW <cit.> (lr=2e-5) and Adam <cit.> (lr=2e-4,  exp(\u03b3=0.90)) with batch sizes 32 and 64 respectively. [exp is exponential scheduler ]. We use SGD(lr=1e-2, exp(\u03b3=0.90)) with a batch size of 64 while training the person-crop only Resnet34 model (PO_R34) in Table <ref>. For training all the models associated with EMOTIC, we use a weighted combination of binary-cross entropy (BCE) and mean squared error (MSE) losses.\n\n    Loss =\u03bb_1 BCE(p_disc,y_disc) + \u03bb_2 MSE(p_cont,y_cont)\n\nHere y_disc and y_cont refer to ground truth discrete emotion labels and continuous arousal valence dominance ratings. The optimal weights \u03bb_1 and \u03bb_2 are tuned using the validation split. \n\n\n\nCAER-S: We use a non-intersecting split of 39099 and 9769 video frames across 79 TV shows for training and validation. For testing we use the public split of 20913 video frames. Since face is a dominant signal for persons in TV shows, we use MTCNN <cit.> [https://github.com/timesler/facenet-pytorch] to obtain face crops. We have a single fully-connected (FC) head with e_fusion as input for predicting 7 discrete emotion classes. \n\nFor training MCF with MHAenc and SAG-MHAenc layers, we use Adam (lr=2e-4, exp(\u03b3=0.90)) with a batch size of 64. We use Adam (lr=1e-4, exp(\u03b3=0.75)) with a batch size of 64 while training the face-crop only Resnet34 model (FO_R34) in Table <ref>. For training all the models associated with CAER-S, we use multi-class cross entropy loss.\n\n\n\nWe conduct our experiments using the Pytorch <cit.> library. We set maximum sequence length T_FG as 512 for the captions (Fig <ref>). For visual scene and person representations we use T_VS=197 and T_PE=49 respectively.\n\n\n\u00a7 RESULTS\n\n\n\n \u00a7.\u00a7 Comparison with state of the art\n\nWe compare performance of MCF (Enc) under two settings where Enc refers to the encoder layer used i.e., MHAenc and SAG-MHAenc with existing methods in Table <ref>. We can see that MCF under both settings performs better than prior methods like  <cit.>, <cit.>, and <cit.> that rely on dual stream (person + whole image approach) and do not use explicit pose information. Furthermore, in contrast to previous methods, we consider language driven foreground (captions) and visual scene contexts instead of end-to-end modeling of whole image based information. For a fair comparison with <cit.>, the current MCF framework can be potentially expanded to include other person specific streams like face and explicit pose information.\n\nFrom Table <ref>, we can see that in CAER-S, a fully finetuned Resnet34 model trained on face crops (FO_R34) obtains a high accuracy of 77.35 since facial expressions provide dominant signals for emotion classification in TV shows. However, inclusion of both foreground context through captions and visual scene information in MCF (MHAenc) results in better performance (79.63) as compared to both FO_R34 and baseline attention fusion method CAER-Net-S. \n\n\n\n\n\n \u00a7.\u00a7 Ablation studies\n\n\nWe analyze the importance of different input context streams and associated models in EMOTIC. From Table\u00a0<ref>, we can see that the Resnet34 model (PO_R34) fully finetuned using person specific crops performs worst, thus indicating the need of additional contextual information. Inclusion of scene representation (cls token) from ViT pretrained on Places2 dataset with PO_R34 via late fusion (LF) improves the mAP to 25.53. Freezing PO_R34 model followed by cross modal interaction through CM_enc composed of MHAenc layers (visual-scene guided context stream in Fig.\u00a0<ref>) further increases the mAP to 27.37. Further we can see that fusion of both foreground and visual scene based context information through MCF (MHAenc) results in the best performance (29.53). For both CMEnc (MHAenc + VS) and MCF (MHAenc), we freeze the Resnet34 model for extracting representations from person crops. \n\nFor CAER-S, we can see from Table <ref> that inclusion of SAG-MHAenc layer instead of MHAenc improves the accuracy from 76.89 to 79.63.  This can be attributed to the self-attention based augmentation operation for the query features i.e. face representations from Resnet34 in SAG-MHAenc layer (Fig <ref>). For both MCF (MHAenc) and MCF (SAG-MHAenc), we finetune the Resnet34 model completely with MCF for extracting representations from face crops. \n\n\n\n\n\n \u00a7.\u00a7 Qualitative examples\n\nIn Fig <ref> (a) and (b), we can see that the inclusion of foreground context through captions like basketball team celebrating and man crouching down with his hands on his face results in consistent performance of MCF (MHAenc) as compared to PO_R34 (Resnet34 finetuned on person crops).  Similarly, for TV shows in Fig <ref> (c) while the face crop based prediction from FO_R34 is sad, inclusion of foreground context with visual scene information gives a correct prediction for MCF (SAG-MHAenc). In Fig <ref> (d), the act of gesturing with hand enables MCF (SAG-MHAenc) to make a correct prediction (Angry) as compared to FO_R34 (Resnet34 finetuned on face crops).\n\n\n\u00a7 CONCLUSION\n\nIn this work, we explore the role of contextual information in estimating human emotions with respect to the domains of natural scenes (EMOTIC) and TV shows (CAER-S). Since multimodal-VLN models are pretrained on large-scale image-text pairs from the web, we utilize their capabilities to obtain foreground context information in terms of descriptive captions. Further, we propose a purely attention-based multimodal context fusion (MCF) module to combine person-specific information with the visual scene and foreground context representations. Future work involves the extension of the MCF module to include geometric aspects of person context including pose information and evaluation using media-centered data like movies and advertisements.\n\n\n\u00a7 ACKNOWLEDGEMENT\n\nWe would like to thank the Center for Computational Media Intelligence at USC for supporting this study. \n\n\n\n\nIEEEbib\n\n\n"}