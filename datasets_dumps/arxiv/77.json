{"entry_id": "http://arxiv.org/abs/2303.07265v1", "published": "20230313164428", "title": "Multimodal Reinforcement Learning for Robots Collaborating with Humans", "authors": ["Afagh Mehri Shervedani", "Siyu Li", "Natawut Monaikul", "Bahareh Abbasi", "Barbara Di Eugenio", "Milos Zefran"], "primary_category": "cs.RO", "categories": ["cs.RO"], "text": "\n\nCrysFieldExplorer: a software for rapid optimization of crystal field Hamiltonian\n    Huibo Cao*\n    March 30, 2023\n=================================================================================\n\n\n\n\n\n\nRobot assistants for older adults and people with disabilities need to interact with their users in collaborative tasks. The core component of these systems is an interaction manager whose job is to observe and assess the task, and infer the state of the human and their intent to choose the best course of action for the robot. Due to the sparseness of the data in this domain, the policy for such multi-modal systems is often crafted by hand; as the complexity of interactions grows this process is not scalable. In this paper, we propose a reinforcement learning (RL) approach to learn the robot policy. In contrast to the dialog systems, our agent is trained with a simulator developed by using human data and can deal with multiple modalities such as language and physical actions. We conducted a human study to evaluate the performance of the system in the interaction with a user. Our designed system shows promising preliminary results when it is used by a real user.\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\n\nAssistive robots built with the intention of helping older people and people with disabilities with activities of daily living (ADLs) such as cooking or cleaning must be able to work in tandem with their users to complete their tasks. These interactions are typically multimodal (involving speech, gestures and physical actions) and collaborative (requiring some level of engagement from both participants). As with other autonomous robots, these assistive robots can be decomposed into three main components that perform the typical sense-plan-act cycle throughout the task: a perception module that collects and processes sensory data (potentially from multiple modalities) to understand the environment and the user's actions; an interaction manager that then determines an appropriate response for the robot; and an execution module that enables the robot to perform this action (Fig.\u00a0<ref>). \n\n\n\nOne way of designing an effective interaction manager for such a robot is to study how two humans interact with each other in collaborative tasks. We previously proposed a novel architecture called a Hierarchical Bipartite Action-Transition Networks (HBATNs) for multimodal human-robot interaction management\u00a0<cit.>, which was developed largely based on our corpus of interactions between elderly individuals (labeled ELD) and nursing students (labeled HEL) assisting them in completing ADLs\u00a0<cit.>. In particular, we targeted what we call the Find task, in which two participants collaborate to locate in the environment an object that is not visible. We observed the participants in this task interacting via speech, pointing gestures, and haptic-ostensive (H-O) actions that bring physical objects into conversational focus through touch.\n\n\n\n\nWe showed that HBATNs can effectively provide a policy for the robot, but while the construction of the HBATN for the Find task was completely grounded in our human-human interaction data, it was ultimately crafted by hand and it would need to be re-designed for new tasks. In this paper, we propose a more scalable interaction manager that employs reinforcement learning (RL) to automatically extract an optimal policy for a robot participating in a collaborative task. With RL, the robot learns to differentiate desirable actions from undesirable actions through trial and error with feedback from its interactive environment in the form of rewards and penalties\u00a0<cit.>. We demonstrate the effectiveness of this RL-based interaction manager by training the agent for the robot assuming the HEL role that collaborates with a human assuming the ELD role in the Find task.\n\n\n \n\n \n\n A major challenge in training this agent is in building an interactive environment that can provide the agent with rewards. Here, the environment includes not only the physical space in which the agent acts but also the human user acting as ELD. To be able to perform RL at scale, we developed a simulated environment with a neural network-based user simulator that was inspired by Behavioral Cloning\u00a0<cit.> and was based on Find task data from our ELD-HEL interaction corpus. Additionally, because our corpus is relatively small for RL-based training and because interactions involving a robot are more prone to error than between two humans due to imperfect sensors and algorithms, we augmented our data with synthetic, data-driven misunderstandings and equipped our environment with an error-injecting module.\n \n\n\n\n\n\n\n\nOur contributions in this paper are twofold. Firstly, we propose an interpretable RL-based interaction manager for multimodal collaborative robots. The reward function is simple, and some preconditions are enforced to prevent unwanted policy searches and speed up the training process. Secondly, the proposed neural network-based, end-to-end user simulator provides a training environment in RL training. On the one hand, it handles \"erupt\" interactions, like suddenly changing the goal object. On the other hand, it can provide realistic actions when the dataset or the HBATN framework is not considered. Moreover, our preliminary user study showed that the robot agent meets high accuracy and satisfaction.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe rest of the paper is organized as follows. Section\u00a0<ref> reviews related literature, Section\u00a0<ref> presents our user simulator, Section\u00a0<ref> outlines our RL framework, and Section\u00a0<ref> describes a user study we carried out to evaluate the performance of our learned interaction manager.\n\nIn our previous study, motivated by our corpus of human-human interactions between elderly individuals (elder role: ELD) and nursing students (helper role: HEL) assisting in ADLs\u00a0<cit.>, we developed a Hierarchical Bipartite Action-Transition Network (HBATN) as the main core of our multimodal Human-Robot Interaction Manager (MIM) for assistive robots\u00a0<cit.>. The task that was studied in detail in our work was the Find task, an interaction scenario in which a human and a robot work together to find an object in the environment. The Find task is decomposed into a set of subtasks to identify two main unknowns: the target object (O) and its location (L). The four main subtasks are determining the desired object type (Det(O_T)), determining a potential location to check (Det(L)), opening the location (Open(L)), and determining the actual object (Det(O)). These are modeled as ActNets. Later on, we generalized our model to enable the robot to be either the ELD or HEL by decomposing the subtasks into what we call primitive subtasks\u00a0<cit.>.\n\nThe core of our framework is Hierarchical Bipartite Action-Transition Networks (HBATNs) that model both agents simultaneously to maintain the state of a task-driven multimodal interaction and plan subsequent robot moves.\n\nAlthogh the HBATN ultimately provides the robot agent policy, it is a rule-based handcrafted network. We would like to make our agent intelligent enough to extract the optimal policy automatically. In this study, we propose our Multimodal Reinforcement Learning Framework which enables our robot agent to extract and perform the optimal policy automatically.\n\nHowever, there are some critical challenges we need to tackle in order to make our robot agent intelligent. The first and foremost challenge would be the small size of our ELDERLY-AT-HOME corpus\u00a0<cit.> on which the HBATN is developed. HBATN does a great job replicating the interactions of the Find-task but when it comes to developing an intelligent agent based on the same data, it will get a lot more complex.\n\nFor compensating the small amount of data, we could perform a Data Augmentation on our current data. When two humans collaborate on a multimodal task, errors such as mishearing the utterances and misinterpreting the gestures are inevitable. Hence, another concern arises about applying the augmentation on all modalities in such a way that it models those errors the same way a human naturally acts. \n\nThe next challenge would be the need to have another agent acting as a human while training our robot agent. Inspired by Behavioral Cloning (BC) concept\u00a0<cit.>, we tackle this problem by proposing our Black-Box Human Agent. However, we don't exactly perform BC for developing and training our human agent since we need to map the current state of human agent to it next (state, action) pair.\n\n\n\n\n\n\nInstead of ActNet, use HBATN. HBATN is much more than ActNet.\n\nHBATN ultimately provides robot agent policy.\n\nThe premise is that HBATN is handcrafted, now we can extract/synthesize automatically.\n\nRL needs data and we don't have lots of it. User-agent simulator+reward function (high-level) compensate for the lack of data, but they are grounded in the data we have.\n\nHow can we use our (small) ELDERLY-AT-HOME corpus to the maximum extent?\n\nHighlight the challenges of data augmentation given our interaction is multimodal.\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\nMachine learning has become an indispensable tool in robotics. The critical aspect of interaction with the environment makes RL a good candidate for learning a policy from sequential human-robot interaction data. After theoretical guarantees for Q-learning were published in the '80s <cit.>, RL algorithms have developed greatly, including Deep-Q-Learning, Actor-Critic, and Trust Region Policy Optimization. <cit.>. For example, in <cit.>, the authors demonstrate how RL agents can outperform humans in the Atari game environment. Inverse-RL is also popular since there is limited size data, and human guidance can significantly reduce the set of states needed to be explored. As an example, in\u00a0<cit.> the authors introduce a preference-inference Inverse-RL model for assistive robots that learns different preferences by observing users performing tasks. In this work, we are interested in learning an agent for an intelligent assistive robot and to that end, we propose a Deep-Q-Learning (DQL) framework with a DAGGER warm-up<cit.> which helps us tune the RL reward function more efficiently.\n\nIn RL, a mandatory component to complete the feedback loop for training is a simulated user or an interactive environment. In human-robot interaction, the simulated user (which mimics humans) should be reliable but also generate a variety of actions to ensure the RL exploration. The authors of\u00a0<cit.> examine whether the human-given reward is compatible with the traditional RL reward signal. \n\nIn\u00a0<cit.>, authors assume the human agent is an expert in interacting with the robot, i.e., humans will always give actions designed to be easy-understood by the robot. In this case, the human agent's policy is a fixed and deterministic function of the robot's movements. In our case, this expert assumption for the user simulator is inaccurate since the user, who might have a disability and need help, can not be seen as an expert. Instead of giving the best decision, our proposed user simulator can capture the user's decision preference.\n\nOne can personalize the user simulator by training it based on their own collected data. Our RL agent can always successfully assist these personalized users by acquiring information or taking the initiative. \n\n\n\n Several works exist that propose RL frameworks for training interactive dialogue systems to assist users\u00a0<cit.>. For example, in\u00a0<cit.>, a Deep Q-learning network architecture is proposed for a dialogue system that can help humans in accessing information or completing tasks. The agent is trained with a language generation machine as the user simulator. One of the main differences between this work and ours is that we work with actual human data to develop our user simulator. Besides, in dialogue systems, the agent is limited to a single modality (language), while in this paper, we deal with a multi-modal system which makes it more complex. Another related work is\u00a0<cit.> in which Q-learning is employed to learn a personalized policy based on children's facial reactions and pose data in the class. Although they work with actual human data, they are only concerned with visual data from poses and faces captured by a camera. Throughout the recent literature, there is not much work that considers a multi-modal setting with gestures, physical actions, and dialogue while training the intelligent agent based on actual human data. \n\nIn this paper, we build upon the HBATN framework proposed in\u00a0<cit.>\n\nwhich allowed the agent to manipulate the objects in the environment and interact with users through pointing gestures, haptic-ostensive (H-O) actions <cit.>,\nand speech. This data-driven model was developed based on our previously collected corpus of human-human\u00a0<cit.>. The task studied in detail was the Find task, an interaction scenario in which a human and a robot work together to find an object of interest in\nthe environment. HBATNs model both agents simultaneously to maintain the state of a task-driven multi-modal interaction and plan the subsequent robot moves. Although this model provides an interpretable decision-making process (it is rule-based and constructed by hand), it lacks scalability. Also, extracting the policy manually is a daunting task.  One of our main contributions in this paper is to automate this policy extraction process using RL.  \n\n\n\n\n\n \n\n\n\n\n\u00a7 USER SIMULATOR\n\n\nAs the assistive robot can be explained in the See-Think-Act cycle in <ref>, our user simulator will take the place of the human agent on the left side, and the perception module and RL robot agent will be the interaction manager module. Especially, the user simulator takes a vector of Helper's dialogue, actions, and environment information as input and outputs the elder's dialogue and action to the RL robot agent.\n\n\n\n\n\nA critical component of the RL training cycle is the environment in which the agent to be trained can perform actions and receive feedback. In a collaborative setting, the environment also includes the participant that the agent interacts with. As illustrated in Fig.\u00a0<ref>, our HEL agent decides on its next action based on the Perceived Human Intent. As a result, we need to provide the HEL agent with this input in the RL training cycle as well.\n\nBecause the number of trials required to effectively perform RL grows significantly with the size of the agent's search space \nand putting a human user in the training cycle to provide the Perceived Human Intent input to the RL agent is neither realistic nor feasible we developed a user simulator. We focus on the Find task, for which we have annotated human-human interaction data. To train an agent to perform the role of HEL in the Find task using RL, we thus require a user simulator that can act as ELD and provide the same kind of response we observed in our data.\n\n\n\nWe previously developed an HBATN to represent the states and actions of both participants in the Find (and more generally, a collaborative) task\u00a0<cit.>. The task was decomposed into subtasks with the goal of identifying a target object type O_T, identifying locations L potentially containing the object, and, ultimately, locating the target object O. In\u00a0<cit.>, we showed that our HBATN, equipped with a trained classifier that determines which subtask the participants are currently in, can model and perform both HEL and ELD behavior; however, training an agent using RL requires the agent to be able to explore its space of states and actions, some of which are never seen in our data, which can lead to erratic behavior that the subtask classifier was not trained to handle. We, therefore, propose an end-to-end neural network-based user simulator that can reasonably engage with the agent as it undergoes RL. \n\n\n\n\n\nAs depicted in Fig\u00a0<ref>, for training our Reinforcement Learning (RL) agent in an RL cycle, we need a User Simulator acting as the ELD agent. To build the user simulator, ELD Agent, we use the same ELDERLY-AT-HOME data on which we previously developed our HBATN\u00a0<cit.>. We develop a Black-Box User Simulator which can take care of all sub-tasks at once and predict the state of the world, i.e. the state in which the ELD is as well as the ELD\u2019s next move. The ELD agent is supposed to predict if and what move the ELD is going to take, the dialogue act (DA) and action, as well as the state of the world. The state of the world is determined by the ELD\u2019s belief of the HEL\u2019s object type (O_T), location (L), and object (O).\n\n\n\n \u00a7.\u00a7 Feature Extraction\n\n\nWe require a state representation that can be the input to the user simulator (ELD) to reasonably determine an output as an appropriate response to the agent (HEL) action. We consider three main parameters: (1) ELD's belief of HEL's knowledge of O_T, which can be one of three values (ELD believes HEL does not know the target O_T, ELD believes HEL does know the target O_T, or ELD believes HEL is thinking of a different O_T), (2) ELD's belief of HEL's knowledge of L, and (3) ELD's belief of HEL's knowledge of O. We supplement these with additional features representing HEL's action that ELD responds to: (4) what or where HEL pointed to, if a pointing gesture was performed, (5) what H-O action HEL performed and on what, if an H-O action was performed, and (6) the Dialogue Act (DA) \u2013 roughly, intent indicated through speech \u2013 of HEL's utterance, which we have extensively studied and built classifiers for\u00a0<cit.>.\n\nWe assume that both ELD and HEL have a finite set of actions. We developed a list of actions for each role based on the Find task corpus so that HEL's action label could be input to the user simulator and so that the user simulator can generate its action labels as output. ELD actions include providing the O_T or the L and giving an affirmative or negative response, while HEL actions include requesting the O_T or the L and verifying a potential O_T, L, or O. \n\nWe designed our user simulator to output ELD's next action label, the DA tag associated with ELD's action (to simulate speech recognition and classification when the agent interacts with a human user), and the updated state representation of ELD's beliefs of HEL's current knowledge of the task. The action label and DA tag can then be passed to the agent during training, while the updated state representation of ELD's beliefs get passed into the user simulator at the subsequent step. When interacting with a human user, the agent will need to be equipped with classifiers to determine the user's action label and DA given sensory input. This paper addresses the RL cycle, and we plan to implement and test a full human-robot interaction in our future work.\n\n\n\nWe first determine the information our model needs to know in order to decide which state the ELD is in and what action the ELD is going to take. Then, we\u2019ll extract the features from the data.\n\nThe features extracted and used to train the ELD Agent are as follows: (1) whether the HEL is going to take consecutive moves; (2) whether or not an object type has been uttered; (3) whether or not a location is uttered; (4) the ELD\u2019s belief of the HEL\u2019s object type, could be 0, 1, 2. 0 is when it has not been determined yet, 1 is when it matches the ELD\u2019s object type, 2 is when it mismatches; (4) the ELD\u2019s belief of the HEL\u2019s location, the same as previous feature could be 0, 1, 2; (5) the ELD\u2019s belief of the HEL\u2019s object, the same as previous feature could be 0, 1, 2; (6) whether or not a pointing gesture has been performed, If so, it is targeting an object or a location; (7) whether or not an H-O action has been performed, if so, it is targeting an object or a location and what kind of H-O it is; (8) the current HEL\u2019s action tag; (9) the current HEL\u2019s utterance, the DA tag.\n\n\n\n \u00a7.\u00a7 Data Annotation\n\nWe turn to the Find task data available in the ELDERLY-AT-HOME corpus\u00a0<cit.>. This data was previously transcribed and annotated for DAs, pointing gestures, and H-O actions. To train our user simulator with this data, we supplied additional annotations around ELD beliefs of HEL's knowledge of O_T, L, and O, and ELD and HEL actions using our list of actions. \n\nELD beliefs were annotated based on the heuristic that ELD updates their belief whenever HEL gives some indication of knowing or not knowing O_T, L, or O. For example, ELD believes HEL does not know the L until HEL has acknowledged ELD giving the L or performs an action on the intended L, but ELD believes HEL is thinking of a different L if HEL asks for verification of the wrong L (e.g., ELD says, \"Check down there,\" while pointing to a cabinet, but HEL asks, \"Here?\" while pointing to a different cabinet). ELD and HEL actions were labeled by two annotators. Because these action labels did not follow a strict heuristic and were, therefore, more subjective, we measured inter-annotator agreement on both types of actions using Cohen's kappa. We chose 40 random ELD actions and 40 random HEL actions from the corpus and had both annotators independently label them prior to annotating the remaining actions. We found a high level of agreement between the annotators for both ELD actions (\u03ba=1.0) and HEL actions (\u03ba=0.81), indicating that we have reliable action labels.\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Data Augmentation\n\nThe Find task data provided a strong foundation for training our user simulator, but there were no instances in which ELD believed HEL had the wrong O_T or L in mind. The data also contained very few instances in which ELD believed HEL did not know O_T. Only in the beginning of each trial did this occur; after ELD provided the O_T, HEL never needed to ask for O_T again. \n\nThe lack of these states is unsurprising given that the interactions were between humans who could hear and see each other well. However, when we train our HEL agent with RL, we expect that the agent will make mistakes, misinterpreting or not even interpreting O_T or L, so the user simulator needs to be able to respond appropriately. We, therefore, augment our data with synthetic but grounded examples that cover these missing or infrequent states.\n\n\n\n\n\nIn the real world, the cases where the human misinterprets another human's actions would happen frequently. As a result, we expect that to be the case for ELD\u2019s belief of the HEL\u2019s object type (O_T), location (L), and object (O) in our data too. However, due to the small amount of data, there are not enough samples of these error cases in our data. To compensate for that and balance the data, we executed a data augmentation procedure before developing and training the model.\n\n\n\nTo increase the number of states in which ELD believes HEL does not know O_T, we sample instances in which ELD believes HEL knows the O_T (regardless of whether ELD believes HEL knows the L that ELD wants to search) and replace HEL's utterance with an utterance from the data that corresponds with the action of requesting the O_T (see Fig.\u00a0<ref>). The subsequent ELD action would then be to give the O_T again.\n\nTo increase the number of states in which ELD believes HEL has the wrong O_T in mind, we sample instances in which HEL mentions the target O_T in their utterance and replace the O_T in HEL's utterance with an incorrect O_T (see Fig.\u00a0<ref>). The subsequent ELD action would then be to give the O_T again. Analogously, we increase the number of states in which ELD believes HEL is not thinking of the same L or O by sampling instances in which HEL performs an action on an object or location and replacing the object or location with an incorrect one, for which the subsequent ELD action would be to give the L or the O_T again (see Fig.\u00a0<ref>).\n\n    \n  * (O_T, L, O) = (0, 0, 0) and (0, 1, 0); we take random HEL's actions and replace utterance with a random (0, 0, 0) /(0, 1, 0) utterance. \n\n    \n  * (O_T, L, O) = (2, 0, 0) and (2, 1, 0); we take random HEL's actions containing the target O_T and replace O_T in utterance with a random different O_T. \n    \n    \n  * (O_T, L, O) = (1, 2, 0) and (1, 1, 2); we randomly replace L and O in pointing and H-O actions with a different L or O.  \n\n\n\n\nWith this data augmentation scheme, we increase the number of data points we have from 693 to 1101.\n\n\n\n \u00a7.\u00a7 Model Architecture and Training\n\n\n\n\nOur designed network includes two fully connected (dense) layers with ReLU activation function, and a dropout layer (ratio=0.5) for better generalization and to avoid over-fitting. Cross-Entropy loss function and Adam optimizer are used for optimization. The model is trained for at most 100 epochs. However, evaluating the model on the validation set is performed alongside the training so training can be stopped early. The inputs to the network are the features described in the previous section and the outputs are the ELD's next state, DA, and action\n\nwhich are manually annotated in the data. While training, we adopt sample re-weighting to resolve the class imbalance issue. Pythorch library is used for implementation\u00a0<cit.>.\n\n\n\n\n\n\n\nIn addition to overall accuracy, we evaluated the model on the classification accuracy of each individual output of the model; i.e. the classification accuracy for: (1) the predicted belief ELD holds of HEL's knowledge of O_T, L, and O; (2) the predicted DA; (3) the predicted action. The accuracy results are provided in tables <ref> and <ref>.\n\n\n\nPythorch library is used for the model implementation. The design of the network is as follows: a two-layer feed-forward Pythorch network; dropout=0.5 for better generalization and to avoid over-fitting; and Relu activation function.\n\n\n\n\n\n\n\n\n\nThe relatively low overall accuracy results from the low action classification accuracy and DA classification accuracy. The main reason is still the small amount of our data. We do not want to overdo the data augmentation since having a large proportion of data generated this way becomes counterproductive. Moreover, our action accuracy and DA accuracy are overly strict since the classifier output is considered correct only when the predicted tag matches the ground-truth label. However, the predicted DA could be acceptable if it still conveys the right intent. For instance, consider the case where the HEL utters: \u201cWhere should I look for the cup?\u201d and the simulator predicts \u201cInstruct\u201d and \u201cGive O_T, L\u201d for the DA and action, respectively, while the labels are \u201cReply-w\u201d (meaning that the user replies to a Where or What question) and \u201cGive L\u201d. The simulator output would be labeled as wrong, but it does convey the right intent. We thus consider our current user simulator to be adequate because what is important is the interaction between the user simulator and the RL agent. As long as the simulator conveys the user's intent correctly to the RL agent, the essential prerequisites for training the RL agent are taken care of. Improving the accuracy of the user simulator is an important research problem, but it is beyond our current study and will be considered in the future.\n\n\n\u00a7 REINFORCEMENT LEARNING FRAMEWORK\n\n\n\n\n\nIn this RL problem, we employ a Deep-Q-Network\u00a0<cit.> model to act as our assistive robot (HEL) agent by having it interact with the interaction environment containing the user simulator proposed in the previous section. To enhance the training, we warm up the agent before starting the training loop using DAGGER, an Imitation Learning (IL) algorithm\u00a0<cit.>. The warm-up phase happens before initiating the RL training cycles. This step is crucial due to the fact that learning the appropriate agent behaviors with a simple reward function is extremely difficult and training such an agent from scratch is tedious.\n\n\n\n\n\nThe flow of the interactions between the HEL agent and the user simulator while training the HEL agent is illustrated in Fig.\u00a0<ref>. The state of the HEL agent consists of three variables: the state of HEL's O_T, HEL's L, and HEL's O. The possible values for each variable are 0, 1, or 2. The variable is encoded with 0 when it has not been determined yet, with 1 when it matches the user simulator\u2019s belief about it, and with 2 when there is a mismatch between the HEL value and user simulator\u2019s belief.\n\nAs depicted in Fig.\u00a0<ref>, the HEL agent takes the user simulator's action and the previous HEL's state as input and outputs a DA tag and an action vector encoding the HEL's utterance and physical action, respectively.\n\nWe develop and train a Deep-Q-Network\u00a0<cit.> acting as our HEL agent by having it interact with the ELD agent we proposed in the previous section. We also need to warm up the RL agent before starting the RL training using an Imitation Learning (IL) algorithm. We do the warm-up phase before RL training because modeling human behavior using a simple reward function and training from scratch is very hard and not efficient at all. As a result, we take advantage of DAGGER IL algorithm\u00a0<cit.> in this regard.\n\nThe flow of the interactions between HEL and ELD agents while training the HEL agent is illustrated in Fig.\u00a0<ref>. The state of the HEL agent consists of three variables: (1) the state of HEL's O_T could be 0 when it has not been determined yet, 1 when it matches the ELD\u2019s object type, 2 is when it mismatches the ELD\u2019s object type; (2) the state of HEL's L could be 0, 1, 2, same conditions as HEL's O_T apply; (3) the state of HEL's O could be 0, 1, 2 the same as HEL's O_T, L. \n\n\n\n\n \u00a7.\u00a7 Model Architecture\n\n\n\n\n\n\nThe HEL agent network includes two fully connected layers \nfollowed by a dropout layer (ratio=0.1). Then the output of the dropout layer is fed to the output layer followed by ReLU activation and gives a vector encoding HEL's (DA, action) output pair. For implementation, we used the Pythorch library\u00a0<cit.>. \n\n\nPythorch library is used for implementing this framework. The design of the HEL agent's network is as follows: a two-layer feed-forward Pythorch network; dropout=0.1 for better generalization and avoiding over-fitting; and Relu activation function. \n\n\n\n\n\n \u00a7.\u00a7 DAGGER Warm-up\n\n\n\n\n\n\nFirst, we have our HEL agent interact with the user simulator and run the DAGGER algorithm as a warm-up stage. We don't let the agent get fully trained, we only use it to obtain a good initial guess for the subsequent RL training. Using DAGGER only, without subsequent RL training, results in poorer performance of the HEL agent.\n\nThe human simulator initiates the interaction, the state of the HEL agent is updated according to the received input and it picks an action according to its current state and the user simulator's action. An error module is deployed before passing the HEL's state to the HEL agent.\n\n\nThe error module is necessary due to the fact that in the data when O_T/L/O information is given to the HEL by the ELD, most of the time the state corresponding to that changes to 1, i.e. the HEL's understanding of O_T/L/O is the same as what human has uttered. However, we would like to generalize the framework better so that it also covers the states when the HEL's understanding of O_T/L/O does not match the human's utterances. Thus, the error module in 25% of the cases where HEL's understanding of O_T/L/O is 1, changes that to 2.\n\nFirst, we have our HEL agent interact with the human simulator and run the DAGGER algorithm as a warm-up stage. At this stage we don't let the agent get fully trained, we only push the policy network weights toward the expert's policy extracted from the ELDERLY-AT-HOME data.\n\nThe previously trained ELD agent initiates the interaction by taking a random action, based on the action ELD takes, the state of the HEL agent is updated. Subsequent to that, the HEL agent takes an action getting its current state and the ELD's action as the inputs. An error module is deployed before inputting the HEL's state to the HEL agent to add errors to HEL's (O_T, L, O) state. The error module is devised here due to the fact that in the data when O_T, L, O are uttered to the HEL, most of the time the state corresponding to the changes to 1, i.e. the HEL's understanding of O_T, L, O is the same as what ELD has uttered. However, we would like to generalize the framework better so that it also covers the states when the HEL's understanding of O_T, L, O doesn't match the ELD's utterances. Thus, the error module in 25% of the cases where HEL's understanding of O_T or L or O is 1, changes that to 2.\n\nFor training the HEL agent using DAGGER algorithm, we need to run the algorithm for N iterations. Here we call each iteration one Episode since the interactions between the ELD and the HEL are defined to be episodic. During each episode, one entire interaction consisting of at most M turns between the HEL agent and the user simulator takes place. That interaction is successful if the HEL agent finds the object the user simulator requested before reaching turn M. Otherwise, the interaction is unsuccessful.\n\nDuring each episode, the HEL agent executes the current learned policy. Throughout execution, at each turn, the expert's action, which we get from the roll-outs extracted from our ELDERLY-AT-HOME data, is also recorded but not executed. After sufficient data is collected, it is aggregated together with all of the data that was previously collected. Eventually, the cross-entropy algorithm generates a new policy by attempting to optimize performance on the aggregated data. This process of execution of the current policy, correction by the expert, and data aggregation and training is repeated.\n\nFig.\u00a0<ref> shows training loss, success rate, and average turns during each episode for DAGGER training on the agent for 50 episodes. However, as mentioned before, we do not want a fully DAGGER trained HEL agent. Therefore, the HEL agent's network weights are saved at episode 8 (a good mid-point where the loss is not minimized and the success rate is not maximized yet) to be used later on for training the HEL agent using the Deep-Q-Learning (DQL) algorithm. \n\n\n\n\n\n\n\n \u00a7.\u00a7 Deep-Q-Learning\n\nFor training our HEL agent in the RL cycle, we need to initialize a Policy Network and a Target Network. The former network's weights will be optimized for obtaining the optimal policy, and the latter network will be used to track the target Q-values associated with each individual action for input states\u00a0<cit.>. \nThe policy we extracted at episode 8 of DAGGER training is used to initialize the weights of the Policy Network and the Target Network for DQL. The model architectures for the Policy Network and the Target Network are exactly the same as the model used previously for DAGGER training due to the fact that we simply copy the weights extracted from DAGGER network into these two networks. \nThis makes running the DQL algorithm on our RL framework \nmuch more efficient in terms of time and space. Another advantage of warming up the HEL agent by DAGGER training is that by pushing the weights of the policy toward the expert's policy, we don't need to hard code complex human behavior in the Reward Function. A simple Reward Function combined with DAGGER-half-trained HEL agent makes the DQL algorithm on our HEL agent run much faster. \n\n\n\n\nFor our reward function, we considered a small negative reward, -r, for each HEL agent's move. This is to motivate the agent to finish the task sooner than later. If the interaction is unsuccessful and ends by getting to turn M, the transition is penalized by -2   r. If the interaction successfully ends before reaching turn M, that transition is rewarded as 2  r. We also set some ground rules as Preconditions. The Preconditions are as follows: (1) the HEL agent must not take the action of verifying O_T before O_T is uttered by the user simulator; (2) the HEL agent must not take the action of verifying L before L is uttered by the user simulator; (3) the HEL agent must not take the action of verifying O before both O_T and L are uttered by the user simulator. If any of the Preconditions are violated, that action is penalized with a very large negative number -Z.\n\nEvery action the HEL agent takes is rewarded with a small negative number, -r. If the interaction is unsuccessful and ends by getting to turn M, that transition is rewarded as -2*r. If the interaction is successfully ended before reaching to turn M, that transition is rewarded as 2*r. We also set some ground rules as Preconditions. The Preconditions are as follows: (1) the HEL agent must not take the action of verifying O_T before O_T is uttered by the ELD agent; (2) the HEL agent must not take the action of verifying L before L is uttered by the ELD agent; (3) the HEL agent must not take the action of verifying O before O_T or L are uttered by the ELD agent. If any of the aforementioned Preconditions are violated, that action is rewarded a very large negative number, -inf. \n\nThe interactions between the HEL agent and the user simulator while running the DQL algorithm also follow the flow in Fig.\u00a0<ref>. A tuple of (state, reward, action, next state) is stored in the memory at each turn. Every C episodes, the weights of the Policy Network are optimized using the Mean-Squared-Error cost, and every m   C episodes, the weights of the Policy Network are copied into the Target Network.\n\nFig.\u00a0<ref> shows training loss, average reward, success rate, and average turns at each episode for the full DQL algorithm training of the HEL agent. The reason that average turns start to increase and the success rate gradually decreases after about episode 1250 is that the memory of the agent has been filled up, so part of the memory is emptied and starts to get filled with new samples. The agent has been over-fitted at this point. We thus use the policy obtained just before this phenomenon occurs. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTAL EVALUATIONS\n\n\nTo evaluate our framework in the real world, we designed a user interface to have humans interact with the HEL agent obtained through RL. In the following section, we explain the details of our interface as well as the evaluation procedure. \n\n\n\n\n \u00a7.\u00a7 User Interface Implementation\n\nSince our framework is trained on multi-modal human data, it is capable of interpreting and executing multi-modal actions, and in particular language and physical actions. However, in this work, we focus on language so the Perceived Human Intent vector that results from human subjects is limited to their utterances. The HEL agent is also capable of performing pointing and H-O actions in addition to communicating through language. However, in our evaluation, the agent generates the speech based on its predicted DA, and then also informs the human about the other modalities through speech. For instance, when the agent is verifying if the human uttered \u201cdrawer\" with a pointing gesture it announces: \u201cThe agent points to the drawer.\"\n\n\n\nFor the human subjects to be able to have a smooth interaction with our agent, we developed and implemented a friendly user interface following the architecture for the Perception Module and the Execution Module in Fig.\u00a0<ref>. Since the HEL agent is trained based on DA tags and the action vector generated by the user simulator, it expects to get DA tags and action vectors of the human subject in the test phase as well. As a result, we need to extract the DA tags and the action vectors from the human utterances. Therefore, we have Speech-to-Text, Action Extractor, and DA Classifier components which are explained in detail later on. And for the agent to respond back to the human we first need to transform its DA tag and action vector into a sentence and a textual description of the action, respectively. To this end, we developed a rule-based text generator inspired by the ELDERLY-AT-HOME corpus. Finally, the Pyttsx3 Python library\u00a0<cit.> \nis used to transform the agent's sentences into speech.\n\nHuman speech is passed to the Google Cloud Speech-to-Text API\u00a0<cit.> for speech recognition. Then, the transcribed text is passed to a DA classifier as well as an action extractor. We developed our DA classifier by deploying Sentence-BERT\u00a0<cit.> and extracting embeddings from our data as the input to the classifier. We then trained a two-layer (one hidden layer followed by the output layer) feed-forward Pythorch neural network, \nwith a dropout layer (ratio=0.1), and ReLU activation function after output layer. We used the gold-standard DA tags as the ground-truth labels and optimized the weights by employing the cross-entropy loss function and Adam optimizer.\n\nThe accuracy of our Sentence-BERT-based DA classifier tested on 10% of ELDERLY-AT-HOME data is 67.23%. Our action extractor module works based on the objects and locations it extracts from human utterances. For example, if it detects one of the objects in the utterance, that's labeled as \u201cGive O_T\" action; if it detects one of the locations in the utterance, that's labeled as \u201cGive L\" action; if it detects one of the objects as well as one of the locations in the utterance, that's labeled as \u201cGive O_T, L\" action. For extracting these words of interest, we built a dictionary based on NLTK dictionary\u00a0<cit.> consisting of all the words that could be pronounced similarly or close to our words of interest (to compensate for the speech recognition returning similarly sounding words that do not make sense in the context). \n\n\n\n \u00a7.\u00a7 User Study\n\n\nWe performed a human user study where 9 healthy adults were recruited to interact with our HEL agent. Each subject performed 4 to 5 trials (entire interactions) with the HEL agent adding up to a total of 42 trials. \n\nThe hypothetical experiment environment would be a room with a drawer, a shelf, and a cabinet. The user can choose between red, green, and yellow cups and  red, green, yellow, and white balls. At the beginning of each trial, objects are randomly scattered in different locations. The user only knows there are aforementioned locations and objects in the room but doesn't know which item is located where.\n\nThe subjects were instructed to \nchoose the object of interest at the beginning of the trial, \nand guide the agent through different locations to find the object of interest. \n\n\n\nThe data collected in this user study will be added to our publicly available ELDERLY-AT-HOME corpus.\n\n\n\n\n\n \u00a7.\u00a7 Results\n\n\nInspired by our previous study\u00a0<cit.>, we measured the performance of the system by calculating the accuracy of each component independently as well as the overall quality of the system. For evaluating speech recognition we measured Speech-to-Text (STT) accuracy which is the percentage of words transcribed correctly. We report the average Speech-to-Text confidence score as well. \n\nTo evaluate the DA classification, we calculated the accuracy based on the percentage of the DAs classified correctly. For obtaining the ground-truth gold-standard DA labels, two annotators annotated the texts that were previously extracted from the Speech-to-Text component. We chose 40 random sentences and had both annotators independently label them prior to annotating the remaining sentences. We calculated Cohen's kappa and found a moderate level of agreement between the two annotators (\u03ba=0.6). Although it's not a high level of agreement, the DA accuracy still is consistent with the test accuracy (67.23%) we got from ELDERLY-AT-HOME data before.\n\nFor evaluating the HEL agent itself, i.e. whether or not the pair of (DA, action) outputs of the HEL agent makes sense according to the DA tag and the action vector it gets as inputs, we compare its actions to those of HBATN's (as our ground truth labels) and report the action accuracy. Moreover, we calculate and report the percentage of the HEL's non-eligible actions with respect to the human's original speech. We had two human transcribers listen to the audio data recorded from human subjects. Their transcriptions from audio data matched about 99.2%. A non-eligible action is one where the agent's response does not correspond to the human's previous action. For example, if the human says: \u201cPlease get a cup.\" and the agent responds with: \u201cDid you say inside the cabinet?\", this is a non-eligible action. In other words, non-eligible actions are those actions that do not make sense to the human participants in the study based on the speech they uttered. Non-eligible actions could result from speech recognition errors, DA classification errors, and/or wrong actions by the agent itself. \n\nWe also report the average length of interactions as well as the success rate over all trials. A successful trial is a trial in which the agent is able to find the human subject's object of interest in less than 15 turns. If the agent reaches the 15th turn without finding the object, that trial is unsuccessful.\n\nEventually, we asked the participants to rate their experience on a Likert scale of 1-10. A score of 1 meant \u201csignificantly worse than expected\" and a score of 10 meant \u201csignificantly better than expected\". \n\nThe results of our preliminary user study in Table\u00a0<ref> show that our robot agent does very well in extracting and performing the optimal policy. Two very important metrics to evaluate how well our overall framework performs are the average success rate of 92.86% and the number of average turns of 8.38. There are a couple of reasons why the success rate doesn't reach 100%. The main reason is that our framework is not solely the robot agent itself. The user interface is implemented on top of the agent network and each component of this user interface introduces errors to the overall framework. These errors make our robot agent take some wrong actions occasionally which leads to taking more turns to complete the task.\n\nThe average non-eligible action rate shows that the HEL agent in 30.57% of the cases takes an action that does not make sense to the human at that point. \n\nThe action accuracy of 90.2% indicates that 9.8% of the actions the robot agent takes are wrong actions. This small rate of wrong actions by the robot agent itself is due to the fact that compared to HBATN, our agent is trained through machine learning, not rule-based. It covers a broader set of states while getting trained and as a result when it comes to testing it, the process of making decisions is more complicated.\nAbout 20% of all actions are non-eligible actions resulting from errors in other components. To investigate this more, we calculate the contribution of each component to those wrong actions.\n\nTable\u00a0<ref> summarizes the contribution of each component to those wrong actions taken by the robot agent. 46.73% of non-eligible actions (corresponding to about 14% of all actions) resulted from errors made by the speech recognition component. The DA classifier also causes 21.49% of non-eligible actions (corresponding to about 6% of all actions).\n\nAll in all, we argue that the success rate of 92.86% is remarkable, and the average number of turns of 8.38, and the average Likert score of 7.9 lead us to conclude that our RL framework for multimodal human-robot interaction performs exceptionally well.\n\n\nThings to discuss that are not covered in the evaluation/results section.\n\nAction accuracy: the match is 90%, what about the remaining 10%. It comes from the robot. Some thoughts on how to make it better.\n\nThought: we are trying to make the agent better, but we lack data. That is not quite enough, HBATN helps with that, but RL needs a bit more help. One thing: hard to express human behavior with a simple reward, we don't want to hand-craft it. But inverse reinforcement learning may be useful. In that case, how to get around not having enough data? Is human simulator similar to imitation learning/behavior cloning?\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\n\nIn this paper, we propose a Reinforcement Learning (RL) approach to automate the policy synthesis for a multimodal interactive service robot. The agent trained by this algorithm acts as the interaction manager of our assistive robot in collaborative tasks. The main advantage of this approach over the hand-crafted rules is that it can be applied to other service robots only by adapting the features according to the available data and the required task. Moreover, this approach is more efficient in terms of time and space. We employ the Deep Q-Learning network with a DAGGER warm-up to train the model. During the RL training process, the agent interacts with a human simulator through multiple modalities. In contrast to most of the RL-based models in dialog systems, which mainly work with a single modality (speech), we developed our own human simulator using our previously collected data, the ELDERLY-AT-HOME corpus. Our human simulator is an end-to-end neural network predicting the human's state and action according to the human's previous state, HEL's Dialogue Act, and HEL's action. To evaluate our trained model, we have conducted a human study and assessed user satisfaction and system performance.\n\n\n\n\n\nIEEEtran\n\n\n"}