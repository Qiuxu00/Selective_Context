{"entry_id": "http://arxiv.org/abs/2303.12728v1", "published": "20230313063545", "title": "LocalEyenet: Deep Attention framework for Localization of Eyes", "authors": ["Somsukla Maiti", "Akshansh Gupta"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.LG"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIR-Central Electronics Engineering Research Institute, Pilani, Rajasthan, India-333031\n\nsomsukla@ceeri.res.in, somsuklamaiti@gmail.com; akshanshgupta@ceeri.res.in\n\n\nDevelopment of human machine interface has become a necessity for modern day machines to catalyze more autonomy and more efficiency. Gaze driven human intervention is an effective and convenient option for creating an interface to alleviate human errors. Facial landmark detection is very crucial for designing a robust gaze detection system.  Regression based  methods capacitate good spatial localization of the landmarks corresponding to different parts of the faces. But there are still scope of improvements which have been addressed by incorporating attention. \n\nIn this paper, we have proposed a deep coarse-to-fine architecture called LocalEyenet for localization of only the eye regions that can be trained end-to-end. The model architecture, build on stacked hourglass backbone, learns the self-attention in feature maps  which aids in preserving global as well as local spatial dependencies in face image. We have incorporated deep layer aggregation in each hourglass to minimize the loss of attention over the depth of architecture. Our model shows good generalization ability in cross-dataset evaluation and in real-time localization of eyes.\n\n\n\nFacial landmark detection Attention model Deep Learning Convolutional Neural Network Deep Layer Aggregation Human Machine Interaction Gaze Controlled Interface\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nModern machines are user-friendly and keep the possibility of human machine interaction open. Human interaction provides an effective way of controlling the machine with ease and reduces the risk of error. Gaze driven human machine interfaces have become essential for smooth controlling of machine parts with no physical intervention. The need of gaze control has been addressed since the past two decades in assistive robotic systems <cit.> for disable persons and in controlling the robotic arms in robotic surgical system <cit.>. Tracking the eye gaze requires accurate localization of the facial landmarks. Face landmark detection has been playing a significant role in current human machine interface applications, such as gaze tracking for autonomous vehicles <cit.>, face tracking <cit.><cit.><cit.> and facial expression analysis <cit.><cit.>.\n\nLocalization of landmark points on the face helps us in performing face alignment in an effective manner. While designing a robust Gaze controlled interface, it is mandatory to perform precise localization of eyes with high accuracy and low latency. Simultaneously, landmark detection methods are required to handle different real-time challenges, such as low lighting condition, face occlusion and fast movement of head that lead to variation in pose.\nSince past two decades, with the evolution of new methods in deep learning, there has been a significant improvement in developing more robust solutions. Convolutional neural networks (CNNs) have been widely used for facial landmark detection for different applications. Availability of large annotated face databases in different environmental conditions has made the task easier. Even with small dataset, different data augmentation techniques have been proved really efficient in providing better generalization to the solution. \nCoarse to fine regression type of architectures has shown the most prominent localization performance. These techniques learns the coarse features over a shape space at the shallow layers and subsequently learns the fine features at deeper layers <cit.> by cascading several deep CNN <cit.>. Cascaded hourglass and Unet architectures have been able to provide good generalization but still have some issues. The heatmap generated using most of these state of the art architectures do not compute the local attention after each layer. This affects the learning of correlation between features from the shallow layers and the deep layers. \n\nWe here aim to provide a solution that solves the following issues by designing different modules.\n\n\t\n  * We have defined a self-attention module between the individual hourglasss modules, which learns the local spatial attention and the global attention to estimate the precise location of the landmarks.  \n\t\n  * We have designed a deep layer aggregation module to learn the feature dependencies over the depth of architecture while parsing the features across network.\n\t\n  * We have used differentiable Soft-argmax <cit.> <cit.> to make the framework an end-to-end trainable architecture to determine the coordinates (x,y) of each facial landmark from the attention heatmaps.\n\n\n\n\n\u00a7 RELATED WORK\n\nFacial landmark detection and localization has been an active area of research for aligning faces. There have been three broad working techniques <cit.>, a. holistic techniques, b. Constrained Local Model (CLM) based techniques  and c. regression-based techniques. Holistic models <cit.> generate appearance model of the face images by first building a shape model and then mapping the features in the shape model. Active appearance models <cit.> <cit.> have been one of the widely used holistic approach that computes and updates the shape and appearance coefficients and the parameters for affine transformations for detection of the facial landmarks. \nCLM based techniques generate global shape model and local appearance models for each landmark points <cit.>. Local appearance models help in minimizing the error due to variation in illumination and occlusion. Baltrusaitis et. al presented a Constrained Local Neural Field (CLNF) <cit.> that take care of the feature detection problem in wild scenario with less illumination and blurred faces. Recently Weighted Iterative Closest Point (ICP) based surface matching <cit.> and hierarchical filtering strategy <cit.> have been used to reduce the effect of noise in face registration. Researchers have recently proposed a semi-supervised method <cit.>, called self-calibrated pose attention network (SCPAN), that computes Boundary-Aware\nLandmark Intensity (BALI) fields corresponding to a boundary and the landmarks closest to the boundary. They have also extended their work by proposing an implicit multiorder correlating geometry-aware (IMCG) model <cit.> <cit.>, that uses spatial and channel correlations to attain the local as well as global features.\n\nOn the other hand, the regression based techniques mainly focuses on learning a model that maps the facial landmarks on the images, in spite of developing global and local face models.   The regression models can be mainly classified into two categories, viz. i. Cascaded regression techniques and ii. Coarse-to-fine techniques.\n\n\n\n \u00a7.\u00a7 Cascaded regression techniques\n\nCascaded regression methods have been proved to be very efficient and it mainly relies on extracting features at each stage and optimizing the regression parameters for estimating the positions of facial landmarks. Most of the cascaded regression methods relies on hand-crafted features. Local binary features (LBF) <cit.> and Histogram of Oriented Gaussians (HoGs) have been the most used hand-crafted features that are used for the facial landmarks. \nIn <cit.>, LBFs have been extracted using random forests and linear regression model is learned  for each landmark position. However development of regression models based on dedicated feature extraction methods tend to make the solution more sensitive to the given data. Thus most of the time, it fails to provide a generalized solution in scenarios with varied illumination conditions, occlusions and varied poses. The current cascaded regression models aim to develop a deep cascaded end-to-end architecture to determine the landmark coordinates <cit.> <cit.> <cit.>. In <cit.>, the authors have developed a Deformable Transformer Landmark Detector (DTLD) model that preserves the local spatial structure of the face images and improves the localization accuracy. Lai et al.<cit.> have included recurrent neural network (RNN) modules to learn the dependency of the features generated by the cascaded CNN modules. Weng et al have developed a cascaded deep autoencoder network (CDAN) <cit.>, that learns the feature representation at the global and the local stages simultaneously in cascaded manner. \n\n\n\n \u00a7.\u00a7 Coarse-to-fine techniques\n\nCoarse-to-fine techniques have been immensely used by developing different deep learning architectures <cit.> <cit.> <cit.> <cit.> <cit.> for generating accurate prediction of the landmark points. Dapogny et al. has defined spatial softargmax <cit.> to generate landmark-specific attention. Kim et al developed an extended version of MTCNN model, called EMTCNN model <cit.> where they have dilated convolution and CoordConv to improve the localization accuracy.  Several works has been performed, where facial recognition, head pose determination and other activities has been performed along with facial landmark detection. In such applications, shared CNN <cit.> have been developed that uses same set of  feature maps and enables better representation learning. Hannane et al <cit.> learned a FLM topological model that performs divide-conquer search for different patches of the face using coarse to fine CNN techniques and subsequently refines the landmarks positions by using a shallow cascaded CNN regression. Gao has developed a supervised encoder-decoder architecture <cit.> based on EfficientNet-B0 where the dark knowledge extracted from teacher network is used to supervise the training of a small student network and patch similarity (PS) distillation is used learn the structural information of the face. \n\nFor a given face, generated by a face detection technique, the regression model tries to estimate the position of the landmark points accurately. Thus researchers have also dedicated their time to solve the problem of face initialization. Lv et al. has developed a deep regression network with two-stage re-initialization <cit.> of faces at global and local scale   to unify the different face bounding boxes obtained using different face detection methods. \nHeatmap regression methods generate coarse attention maps by developing a heatmap for each landmark position <cit.>. These methods <cit.> <cit.> <cit.> preserves the spatial features and thus provides better performance. UNet, Hourglass, Encoder-Decoder <cit.> <cit.> <cit.> are the most widely used fully convolutional networks for generating high resolution heatmaps. Stacked hourglass network <cit.><cit.> have a been popular architecture to generate accurate heatmaps that can also improve the localization in case of partial occlusions. Stacking of several hourglass models improves the spatial mapping of the features in subsequent hourglasses. Stacked U-Nets have been another very popular model where the features are extracted and then attention maps are generated by performing deconvolution. Stacked densely connected U-Nets have been developed in <cit.> that parses the global and local features across the U-Nets. Guo et al <cit.> has defined channel aggregation block (CAB) to improve the capacity of the stacked U-Net model in parsing the features across network. In <cit.> landmark-guided self attention (LGSA) block has been introduced that process the output feature map one hourglass to improve the spatial structure of the landmarks and sends it to the next hourglass. Xiong proposed a Gaussian vector <cit.> to encode landmark coordinates that provides a better convergence result. As the heatmap-based methods requires a post-processing step which is non-differentiable, Jin has defined a single-stage pixel-in-pixel regression <cit.> where for each heatmap a grid is detected on heatmap and then offsets are determined for precise localization of the landmarks.\n\n\n\n \u00a7.\u00a7 Attention models \n\nPrediction of the face landmark coordinates, using the regression techniques, generates a model that consider the locations of the landmarks. Attention methods have recently gained popularity due to the better performance in localization. The attention methods focuses on the most salient part of the face and thus reduces the unnecessary complexity due to the irrelevant parts of the image. \nDifferent attention models have been developed that define attention block that learns feature channel attention across the architecture, viz., squeeze-and-excitation block in <cit.>. Learning the spatial attention is very useful for facial landmark localization due to the structure of the face. Spatial attention block has been designed using non-local block in Grad-CAM <cit.> <cit.>. Woo <cit.> has designed CBAM that combines both spatial and channel attention. Wang  has used non-local neural net <cit.> to learn self-attention that is most suitable due to the spatial geometry of face.\nResearchers have also used adversarial training methods <cit.> by introducing a discriminator module <cit.> to improve the performance by generating more accurate heatmaps, followed by a attention module that optimizes the spatial correlations <cit.> between the facial landmarks. \n\n\n\n\u00a7 METHODOLOGY\n\nWe aim to develop a gaze controlled human machine interface for controlling the navigation of robotic platform and thus the primary objective is to localize the eyes of user. Thus instead of detecting 68 landmark points, we have devised our problem to detect the landmarks present only in the eye region of the face. We have selected 12 landmark points out of the 68 point landmark annotations, with indices 37 to 48. \n\nAttention-driven facial landmark detection is usually performed by generating ground truth heatmap. We have generated 12 groundtruth heatmaps, each for a landmark position, corresponding to 6 landmarks for each eye, by applying a gaussian filter centered at each landmark position. Standard deviation of each gaussian filter is set at 5. We have proposed an attention based model for localization of the eye heatmaps and detection of the landmarks in the eye region. The architecture of the model has been discussed in the following section.\n\n\n\n\n \u00a7.\u00a7 Network Architecture\n\nWe have proposed a deep attention model called LocalEyenet by using Stacked hourglass(HG) architecture as backbone for detection of the eye landmark positions. We have stacked 3 hourglass modules in a linear fashion as shown in Fig.<ref>.\n\nThe hourglass architecture has been designed in such as way that  it combines the coarse and fine features in an efficient manner over the depth of the model. In standard hourglass architecture, the lower layer features are combined with the higher layer features using residual blocks and upsampling. The residual blocks use a skip connection to pass the low level features to the deeper layers. This alleviates the problem of vanishing gradients and also retains features even after few layers of downsampling. But this also provides a shallow aggregation between the layers and does not retain overall information across the network.\n\n\n\n  \u00a7.\u00a7.\u00a7 Deep Hourglass architecture\n\nThe concept of layer aggregation has been incorporated by designing deep layer aggregation schematics such as Iterative Deep Aggregation (IDA) as proposed in <cit.>. We have adopted the concept and designed a Deep layer aggregation unit (DLAU) that maps the features in an iterative manner. Merging the features from all modules and channels makes the feature combination more appropriate and the loss of attention over depth is being reduced. The structure of the DLAU has been described in Fig. <ref>.  \n \n\nFor designing our model, we have replaced the residual blocks in the hourglass modules by DLAU. Depth-wise convolution of the deep layer features are concatenated with the shallow features. This provides a deep aggregation between the features and the correlation between features across the network is preserved in this manner. The architecture of each deep hourglass module has been shown in Fig. <ref>.\n\n\nThe heatmaps generated by the LocalEyenet has been passed to the post-processing stage where the positions of the landmarks are estimated from the heatmaps. Argmax is the most commonly used mathematical tool that computes the positions of the landmarks by computing the locations with the highest probability in the attention heatmap. Since argmax is non-differentiable, we have used the softargmax <cit.> method which is a continuous function at each point and is thus differentiable. This allows us to train the network end to end and estimates the probability of the landmark positions on the attention heatmap in a single go.\n\n\n\n  \u00a7.\u00a7.\u00a7 Attention between Hourglass modules\n\nThe attention in feature maps, generated by intermediate hourglass modules, are computed and incorporated with the coarse predictions at each stage. This helps in understanding the spatial dependencies and correlation in images. We have adopted the concept of non-local neural network and modified it to compute the self-attention at each location of the feature map. \n\nThe feature map F_i obtained from the deep hourglass module is passed through a residual block to let the spatial dependencies flow across layers. The updated feature map F_i^r is passed through a convolution layer with kernel size 1\u00d71 and 12 filters, each for a landmark point, to obtain feature map F_i^rc. The convolved feature map F_i^rc is then passed though the softargmax operator to generate a coarse prediction of the attention map Map_i, as defined in Equation [<ref>], where W\u00d7 H is the size of the attention map. The similarity between the initial prediction of attention heatmap and the residual feature map F_i^r is computed to rectify the attention heatmap by incorporating the spatial association by computing the element-wise dot product, as defined in Equation [<ref>].\n\n    Map_i =\u2211_x=1^W\u2211_y=1^Hx/Wy/Hexp(F_i^rc(x,y))/\u2211_k=1^W\u2211_l=1^Hexp(F_i^rc(k,l))\n\n\n    Map_i^1=Map_i\u2299F_i^r\n\n\nThe convolved feature map F_i^rc is passed through the self-attention module. The non-local neural network has been instantiated by defining embedded Gaussian at each pixel of the image in form \u03d5(x_i)=W_\u03d5x_i, \u03b8(x_j)=W_\u03b8x_j and g(x_i)=W_gx_i. The parameters W_\u03d5, W_\u03b8 and W_g are learned through backpropagation. The spatial similarity between the embedded gaussians at each location is estimated by Equation [<ref>] . The similarity between the similarity map and the convolved feature map is computed by Equation [<ref>].\n\n    f(i,j)= e^\u03d5(x_i) \u03b8(x_j)^T\n\n\n    S_i=softargmax(f)\u2299 g\n\nThe non-local operation is completed by applying a transform W on the self-attention map S_i. The residual connection F_i^rc is added to the non-local output to generate the local attention map S_i^' as defined in Equation [<ref>]. The updated feature map F_i^' is computed by adding the coarse prediction of the attention map Map_i, which incorporates the global attention that has been learned in the last hourglass module, to the attention map Att_i as defined in Equation [<ref>]. The architecture of the complete attention block is shown in Fig. <ref>.\n\n    Att_i=WS_i+ F_i^rc\n\n\n    F_i^' =Att_i +Map_i^1\n\n\n     \n\n\n\n  \u00a7.\u00a7.\u00a7 Loss function\n\nTo compute the performance of the deep modules, we have used softargmax operation to determine the coordinates (x,y) of facial landmarks from the attention heatmaps. The location with the highest value of softargmax are the most probable estimates of landmark positions.\n\nThe deviation of the estimated landmark positions from the groundtruth locations is used to optimize the parameters of the model. Developed attention model aims to find local minima in the loss plot. Different loss function influences the optimized value of parameters and are computed using RMSprop optimization. We have used 3 different type of loss functions and compared the performance of the models.\n\nAs the problem is devised as a regression problem, we have first optimized the mean square error (MSE) loss as defined in Equation [<ref>], where L=number of landmark points. The MSE loss is computed as normalized squared L2 norm of the error function d as defined in Equation [<ref>], where gt is the groundtruth vector and pr is the predicted values of the landmark points.\n\n    d=gt-pr\n\n\n    MSE=1/L\u2211_i=1^Ld_i^2\n\nPresence of outliers increases the error and impairs the performance of the model. We have tried to minimize the risk of training the outliers, by learning the optimization of Huber loss, defined by Equation [<ref>] that computes the mean absolute error (MAE) for errors with significantly higher values.\n\n    L_\u03b4(d)=\n    \td^2/2,    if | d|\u2264\u03b4\n    \u03b4(| d|-\u03b4/2),    otherwise\n\nRecently, researchers have used <cit.><cit.> a loss function called wing loss that works well with the facial landmark detection. This loss compensates for large as well as small errors by defining a piece-wise linear and nonlinear loss function as defined in Equation [<ref>], where the constant C smoothly connects the nonlinear and linear function.\n\n\n    Wing(d)=\n    \t\n    \t\twln(1+| d|/\u03f5),    if | d|\u2264\u03b4\n    ( | d|-C) ,    otherwise\n\n\n    C=w-wln(1+w/\u03f5)\n\n\nThe developed coarse-to-fine architectures models have been optimized using different loss functions and the performance have been discussed in the subsequent results section.\n\n\n\n\u00a7 RESULTS AND DISCUSSION\n\nThe faces are first detected and cropped for detection of facial landmarks. High resolution face images are downsampled and smaller images are upsampled to the size 256\u00d7256. The resized images are passed through developed deep learning framework.\n\n\n\n\n \u00a7.\u00a7 Dataset\n\nWe have used 2 public domain datasets in this paper, 300W and 300VW. The results of the designed attention model architecture has been trained on the images from the 300W dataset and has been tested on both the datasets.\n\n\n  \u00a7.\u00a7.\u00a7 300W\n\nThe 300W dataset <cit.> <cit.> <cit.> contains facial images captured in two environmental settings, namely indoor and outdoor conditions. This ensures the varied illumination conditions, large variation in expression, pose and occlusion that are being covered in the dataset formation. The dataset contains total 600 images with 300 in indoor and 300 in outdoor categories. Faces in the images are annotated using 68 landmark points using a semi-automatic methodology. The images with multiple number of faces have been annotated separately with different annotation files. \n\n\n\n  \u00a7.\u00a7.\u00a7 300VW\n\nThe 300VW dataset <cit.><cit.><cit.> contains facial videos of 113 subjects recorded in wild scenarios at varied frame rate of 25-30 fps, where each video is around 1 minute long. All the frames have been annotated using 68 landmark points. The videos are recorded under three different scenarios such as well-lit conditions, different illumination conditions and completely unconstrained conditions including occlusions, make-up, expression, head pose, etc. We have extracted frames from the videos and cropped the faces from each frame. The faces has been reshaped later and the annotations have been updated accordingly as discussed in the data pre-processing section.\n\n\n\n \u00a7.\u00a7 Data Preprocessing\n\nThe images are of different resolution and the face contains a small part of the image in most of the cases. Some of the images contain more than one faces and thus we cannot use the whole image for designing the architecture. Thus faces are cropped from the images and resized to the resolution 256\u00d7256. The landmark annotations are also scaled accordingly as defined in Equation [<ref>], where (x,y) are the coordinates of any landmark point and (x_new,y_new) are the coordinates of newly mapped location of the landmark and  [h,w] and [h_new,w_new] are the height and width of the original image and resized image respectively.\n\n    x_new   = x*(w_new/w)\n    \n    \t\ty_new   = y*(h_new/h)\n\n\nWe aim to localize the eyes of the user for further gaze tracking operation. Thus we have selected only 12 landmark points that represent both the eyes on the faces. We initialize heatmaps for each landmark position by defining a Gaussian centered at each landmark with standard deviation of 5. Thus for each input image we generate 12 heatmaps, each centered at one of the 12 landmark points.\n\n\n\n\n \u00a7.\u00a7 Data Augmentation\n\nWe have augmented the images by doing different mathematical operations such as horizontal flipping and rotation. We have also performed blurring on the face images to make a robust dataset for training. \nThe landmark points of the horizontally flipped images are redefined in Equation [<ref>].\n\n    x_new   = w_new-x\n    \n    \t\ty_new   = y\n\nThe cropped and resized faces have also been rotated by very small angle and the rotated faces have been added to the augmented dataset. The rotation angle has been selected as -5^\u2218, +5^\u2218, -10^\u2218 and +10^\u2218 and a rotated image has been generated in each category. The landmark points have been rescaled to the new coordinate system as defined in Equation [<ref>]. \n\n    x_new   = x cos\u03b8+y sin\u03b8 +x_offset\n    \n    \t\ty_new   = -x sin\u03b8+y cos\u03b8 +y_offset\n\nWe have used Gaussian filter of dimension 9\u00d79 with standard deviation \u03c3_x=\u03c3_y=1.8 for generating blurred images. The filter is defined as Equation [<ref>].\n\n    f(x,y)=1/\u221a(2\u03c0\u03c3_x\u03c3_y)exp^-(x^2+y^2)/2\u03c3_x\u03c3_y\n\n\nThere are few images, generated after rotation, which are partially cropped, and this leads to the cropping of some landmark points. Thus these images have been manually checked and removed from the dataset. The generated augmented 300W dataset contains 4195 images in total, where the distribution of the images in each category is defined in Table [<ref>].\n\n\n\n\n \u00a7.\u00a7 Evaluation\n\nWe have developed a coarse-to-fine architecture incorporating attention and have evaluated the performance of the architecture in terms of Normalized Mean Error(NME) and Area under Curve(AUC). NME is defined in Equation [<ref>] where iod is the inter-ocular distance that is defined as L2 norm between the outer corners of  eyes. \n\n    NME=1/L\u2211_i=1^Lgt_i-pr_i _2/iod\n\nThe AUC and failure rate (FR) is computed from the cumulative error distribution curve with threshold NME set at 0.05. The performance of the different architectures have been enlisted in Table [<ref>]. The attention heatmaps generated by different frameworks on sample images of 300W and 300VW have been shown in Fig. <ref>. The NME obtained for 300W and 300VW datasets evaluated using different models have been shown in Fig. <ref>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThree different losses, viz., MSE, huber and wing loss have been optimized for determination of optimum parameters for the developed deep learning framework. The performance of the models for 3 loss functions has been enlisted in Table [<ref>]. As we can visualize from the table, for most of the frameworks, including our architecture, the NME is minimum for MSE loss optimization. Huber loss also provides good generalization for Hourglass model, DenseUnet model and  Stacked HG model with CAB<cit.>. NME obtained for wing loss optimization are comparatively quite high for almost all the models. Thus we have selected MSE loss for optimization of the model parameters. \n\n\n\tEvaluation of NME for different loss functions on 300W dataset\n\t\n\t\n\n\t\t\n\t\t\tMethodology     3cMSE loss     3|cHuber loss     3|cWing loss\n\n\t\t\t2-10\n\t\t\t     NME     AUC     FR     NME     AUC     FR     NME     AUC     FR\n\n\t\t\tHourglass model     0.0878     0.1672     0.5983     0.0577\t    0.2786     0.4233     8.8091     0     1\n\n\t\t\tUnet model     0.0167     0.8825     0.0167     0.3047     0.0015     0.9817     9.7909     0     1\n\n\t\t\tDenseUnet model      0.5901     0     1     0.5120     0     1     9.7909     0     1\n\n\t\t\tStacked Unet model      0.0187     0.6371      0.0467      0.0187     0.6370      0.0469     9.7909     0     1\n\n\t\t\tStacked Hourglass model     0.0784     0.1948     0.5533     0.3349\t    0.0004     0.9916     4.6254    0    1\n\n\t\t\tDensely Connected Unet model<cit.>     0.1453     0.0611     0.7750     3.761     0     1     9.7909     0    \t1\n\n\t\t\tStacked Hourglass with CAB<cit.>     0.1331     0.0761     0.7417     0.0777     0.1972    \t0.5533     0.3232     0.0007     0.99\n\n\t\t\tLocalEyenet model     0.0047     0.9082     0     0.0915     0.1573     0.6050     0.1516     0.0542      0.7883\n\n\t\t\t\n\t\n\n\n\n\nThe cumulative mean error is computed over all the samples of 300W dataset. The Cumulative Error Distribution (CED) curve as shown in Fig. <ref>, shows that the DenseUnet model generates the highest NME and our LocalEyenet model displays the lowest NME and covers the maximum AUC.\n\n\n\n\u00a7 ABLATION STUDY\n\nIn order to understand the role of attention block in the eye localization architecture, we have analyzed the model performance for different scenarios of 300VW dataset. We have implemented standard state of the art Stacked Hourglass model and introduced the DLAU unit in place of Residual block in each hourglass. The performance of model is compared by computing the NME, failure rate and AUC for the models. Introduction of the attention block between the Deep hourglass modules improves the localization of the heatmaps in the annotated area. Comparison of the performance for ablation study has been enlisted in Table [<ref>] for 300W dataset.\n\n\nThe videos in 300VW dataset contains three different scenarios. Videos from scenario 1 have almost no illumination variation, but contain head pose variations. Videos from scenario 2 have been recorded in unconstrained conditions with different illuminations and videos from scenario 3 have all variations including illumination, occlusions, make-up and head pose. The performance of the attention model and its ablation for the 3 different scenarios have been assessed in Table [<ref>]. The attention heatmaps generated by standard stacked hourglass model, the updated stacked hourglass model with DLAU and the LocalEyenet model for the 3 different scenarios have been shown in Fig. <ref>. \n\n\n\n\n\n\n\u00a7 TESTING ON REAL-TIME VIDEO STREAM\n\nWe have tested the performance of our model in real-time video stream. The real-time video streaming has been performed using logitech C920 pro webcam which captures frames at 35 fps with resolution of 1920\u00d7 1080. The faces in each frame of the video stream is initialized using dlib face detector. The detected faces are passed through the LocalEyenet model for inferencing. We have tried to impose different conditions such as head pose variation, removal of spectacles and partial occlusion using hands etc. to check the performance of our model. The inference has been performed at the rate of 32 fps which is very close to the frame rate of the videos. The mean error corresponding to the frames are very low and the eye landmarks detected on the faces in different frames have been displayed in Fig.<ref>.\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\nGaze detection systems have the very potential to develop a human-machine interface device. To build a gaze detection system, it is of utmost necessity to develop a machine learning framework using facial landmarks. In this work, we propose a convolutional framework named LocalEyenet. The framework is an attention driven neural network architecture that focuses on localization of region of interests in image. The model detects the facial landmarks corresponding to eye regions using heatmap based regression. We show that our framework performs better than any other state of the art heatmap regression method, even with variations in illumination, pose and occlusion by spectacles and hand. Our model learns to localize the heatmaps at an early stage and thus generates inferences at a very high speed. To summarize, LocalEyenet is a robust facial landmark localizer that can perform very well in real-time eye localization under different varied environment conditions.\n\n\n\n\u00a7 ACKNOWLEDGMENT\n\nThe authors would like to acknowledge Council of Scientific & Industrial Research (CSIR)-Central Electronic Engineering Research Institute (CEERI), Pilani, Rajasthan, India for providing the facilities for conducting the research work. \n\n\nelsarticle-harv \n\n\n"}