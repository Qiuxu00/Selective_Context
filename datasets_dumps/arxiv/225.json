{"entry_id": "http://arxiv.org/abs/2303.07048v1", "published": "20230313121328", "title": "Hybrid Variational Autoencoder for Time Series Forecasting", "authors": ["Borui Cai", "Shuiqiao Yang", "Longxiang Gao", "Yong Xiang"], "primary_category": "cs.LG", "categories": ["cs.LG"], "text": "\n1\n.001\n\n\n\nmode = title]Hybrid Variational Autoencoder for Time Series Forecasting                      \n\n\n\n\n\n\n\n[1]Corresponding author\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1]Borui Cai\n\n\n[1]\n\n\n2]Shuiqiao Yang\n\n\n3]Longxiang Gao\n\n\n1]Yong Xiang\n\n\n\n[1]organization=School of Information Technology, Deakin University,\n    \n    city=Burwood,\n    postcode=3125, \n    state=VIC,\n    country=Australia\n\n\n[2]organization=School of Computer Science and Engineering, University of New South Wales,\n    \n    city=Sydney,\n    \n    postcode=2032, \n    state=NSW,\n    country=Australia\n\n[3]organization=Qilu University of Technology (Shandong Academy of Sciences),\n    city=Jinan,\n    country=China\n\n\n\nEmail addresses: b.cai@deakin.edu.au (B.Cai); shuiqiao.yang@unsw.edu.au (S.Yang); \ngaolx@sdas.org (L.Gao); yong.xiang@deakin.edu.au (Y.Xiang)\n  \n\n\n\nVariational autoencoders (VAE) are powerful generative models that learn the latent representations of input data as random variables. Recent studies show that VAE can flexibly learn the complex temporal dynamics of time series and achieve more promising forecasting results than deterministic models. However, a major limitation of existing works is that they fail to jointly learn the local patterns (e.g., seasonality and trend) and temporal dynamics of time series for forecasting. Accordingly, we propose a novel hybrid variational autoencoder (HyVAE) to integrate the learning of local patterns and temporal dynamics by variational inference for time series forecasting. Experimental results on four real-world datasets show that the proposed HyVAE achieves better forecasting results than various counterpart methods, as well as two HyVAE variants that only learn the local patterns or temporal dynamics of time series, respectively.\n\n\n\n\n\n\n\nTime series forecasting Variational autoencoder Deep learning\n\n\n\n[\n    [\n    Received; accepted\n======================\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nTime series forecasting aims at learning the generation process of time series and uses previously observed samples to predict future values <cit.>. Accurate forecasting is essential and can help with the success of many applications/businesses. For example, an electricity company can design effective energy policies in advance by predicting the future energy consumption <cit.>; a corporation can minimize its investment risk if the future stock prices are accurately predicted <cit.>.\n\nTime series forecasting has been studied in the literature for decades, but to date, it remains a challenging and active research problem due to the complexity of time series. Classical time series forecasting methods, including autoregressive models (AR), moving average models (MA), and autoregressive integrated moving average models (ARIMA) <cit.>, predict future values by assuming they have linear relationships with observed values; however, this simplification normally leads to unsatisfactory results for complex real-world time series. With the booming of deep learning techniques, deep neural networks (DNN) are widely used to tackle time series forecasting problems. Unlike classical models, DNNs are flexible non-linear models that can capture the temporal information of time series for forecasting <cit.>. Convolutional neural networks (CNN) <cit.> and recurrent neural networks (RNN) <cit.> are two types of DNN widely adopted for time series forecasting. CNN captures salient local patterns of short time series subsequences/segments (e.g., seasonality <cit.> and trend <cit.>), while RNN learns long-term or mid-term temporal dynamics/dependencies of the entire time series <cit.>. In fact, many works capture both types of temporal information by proposing hybrid DNN models and obtaining more accurate forecasting results <cit.>. For example, the researchers <cit.> adopts a hybrid neural network, which stacks CNN with RNN, for DNA sequence prediction. Specifically, CNN can capture short and recurring sequence motifs, which represent biological function units in a DNA sequence. RNN, i.e., long short-term memory (LSTM) <cit.>, is stacked with the output of CNN to learn the spatial arrangement of these motifs. \n\nHowever, these DNN-based models cannot capture temporal information from time series with high accuracy since they are sensitive to small perturbations on time series <cit.>. Recent works refer to variational autoencoder (VAE) <cit.>, which is a type of deep generative model, to learn representations of time series as latent random variables and obtain improved results <cit.>. Compared with directly fitting the exact values of time series, the latent random variables learned by VAE represent the generation process of time series and thus can more accurately capture essential temporal information of time series <cit.>.\nBased on this, existing methods learn either local seasonal-trend patterns <cit.> or temporal dynamics <cit.>; but to date, there is no VAE model that can jointly capture both information for time series forecasting.\n\nIn this paper, we bridge this gap by proposing a novel hybrid variational autoencoder (HyVAE) method for time series forecasting. HyVAE follows the variational inference <cit.> to jointly learn local patterns and temporal dynamics of time series. To achieve this goal, HyVAE is designed based on two objectives: 1) capturing local patterns by encoding time series subsequences into latent representations; 2) learning temporal dynamics through the temporal dependencies among latent representations of different time series subsequences. HyVAE integrates the two objectives following the variational inference.\nExtensive experiments conducted on four real-world time series datasets show that HyVAE can improve the time series forecasting accuracy over strong counterpart methods. \nThe contributions of this paper are summarized as follows:\n\n\n\n\n  \u2013 We propose a novel hybrid variational autoencoder (HyVAE) for time series forecasting. HyVAE derives an objective following variational inference to integrate the learning of local patterns and temporal dynamics of time series, thereby improving the accuracy of forecasting.\n\n  \u2013 We conduct comprehensive experiments on four real-world datasets to demonstrate the effectiveness of the proposed HyVAE method, and the results show that HyVAE achieves better forecasting accuracy than strong counterpart methods.\n\n\nThe rest of this paper is organized as follows. The related works are reviewed in Section <ref>. The preliminary knowledge is introduced in Section <ref>. The proposed method is detailed in Section <ref>, and is evaluated in Section <ref>. The paper is summarized in Section <ref>.\n\n\n\n\n\u00a7 RELATED WORK\n\n\nIn this section, we briefly review time series forecasting methods and VAE-related forecasting approaches.\n\n\n\n \u00a7.\u00a7 Time series forecasting\n\nClassical auto-regressive model (AR) predicts by the linear aggregation of past time series values and a stochastic term (e.g., white noise). ARIMA extends AR to non-stationary time series by incorporating moving average (MA) and differencing. Other statistical models, such as linear regression <cit.> and support vector regression <cit.>, enhances the model capacity but still have limited expressiveness. DNNs are flexible non-linear models and are widely used for time series forecasting in recent years. Specifically, RNNs memorize historical information with feedback loops and can conveniently learn the temporal dynamics of time series. Long short-memory network (LSTM) <cit.> is a typical RNN that alleviates gradient vanishing with forget gates, and that enables the learning of long-term temporal dynamics for time series. Other types of RNN, e.g., GRU <cit.>, and Informer <cit.>, which uses the attention mechanism <cit.>, are also used to improve the effectiveness of different forecasting scenarios. In addition, CNNs <cit.> are further adopted to capture local patterns of time series (such as seasonality <cit.> and trends <cit.>). Many works stack CNN and RNN to learn both the local patterns and the temporal dynamics for challenging forecasting problems; for example, combining multi-layer one-dimensional CNNs with bi-directional LSTM for air quality forecasting <cit.> and DNA sequence forecasting <cit.>; integrating a Savitzky\u2013Golay filter (to avoid noise) and a stacked TCN-LSTM for traffic forecasting <cit.>.\n\n\n\n \u00a7.\u00a7 Variational autoencoder-based forecasting\n\nVariational autoencoder (VAE) <cit.> is a powerful deep generative model that encodes the input data as latent random variables, instead of deterministic values. To enhance the flexibility of VAE (learns independent latent random variables), follow-up methods introduce extra dependencies among the latent random variables. For example, ladder variational autoencoder <cit.> specifies a top-down hierarchical dependency among the latent random variables, fully-connected variational autoencoder <cit.> includes all possible dependencies among variables, and graph variational autoencoder <cit.> automatically learns an acyclic dependency graph. Due to the high flexibility, it is introduced to time series forecasting <cit.>. To improve the performance of the vanilla VAE, VRNN <cit.> introduces an RNN as the backbone to capture the long-term temporal dynamics of time series. LaST <cit.> develops disentangled VAE to learn dissociated seasonality and trend patterns of time series for forecasting.\nThe proposed HyVAE is different from existing methods as it integrates the learning of both local patterns and the temporal dynamic for time series forecasting.\n\n\n\n\n\u00a7 PRELIMINARIES\n\n\nIn this section, we first define the problem and then introduce the preliminary knowledge of VAE.\n\n\n\n \u00a7.\u00a7 Notation and problem statement\n\nA scalar is denoted as a lowercase character, a vector is denoted as a bold lowercase character, and a matrix is denoted as an uppercase character. A time series is denoted as s={s_1,s_2,...,s_m,s_m+1,...,s_m+n}, the time series forecasting problem is defined as determining {s_m+1,...,s_m+n} with known {s_1,s_2,...,s_m}, where n is the step of forecasting. For the convenience, we denote y={s_m+1,...,s_m+n}, and the forecasting problem can be formulated as \u0177=f(s_1,s_2,...,s_m), where \u0177 is the predicted values for y. The error of forecasting is measured as follows:\n\n    Err(y,\u0177)=1/n\u2211_i=1^n(y_i-\u0177_i)^2,\n\n\nTime series subsequence is denoted as x_t={s_t,..,s_t+l-1}, where l is the length. Time series subsequence contains contextual information that expresses local patterns <cit.>, and thus we use subsequences in the forecasting task. Following <cit.>, we obtain a series of l length subsequences from time series, using a sliding window. Time series represented by subsequences is denoted as {x^1,...,x^T}, where T=m-l+1 is the number of its subsequences. Thus, the forecasting problem becomes \u0177=f(x^\u2264 T)=f(x^1,...,x^T).\n\n\n\n\n\n\n\n \u00a7.\u00a7 Variational autoencoder\n\nVariational autoencoder (VAE) <cit.> is an unsupervised generative learning model that learns the latent representation of the input data as random variables. Similar to the conventional autoencoder <cit.>, VAE has an encoding process that encodes the input into latent representations, and a decoding process that reconstructs the original input with the learned representations. We show the process of VAE in Figure <ref>. \n\n\n\n\nVAE learns the generative model as p(x,z) = p(x|z)p(z), where x is the input data and z is its latent representations. The prior of z, p(z), is normally defined as a multivariate Gaussian distribution, i.e., z\u223c\ud835\udca9(0,I); we denote that as p(z)= \ud835\udca9(z|0,I) for convenience. The posterior p(z|x) normally can be an arbitrary non-linear non-Gaussian distribution and thus is intractable. To resolve that, VAE approximates the posterior with q(z|x)=\ud835\udca9(z|\u03bc(x),\u03c3(x)), where mean and variance are determined by x. Then, VAE defines the learning problem as the maximum likelihood estimation of logp(x), which can be formulated as:\n\n    logp(x)=KL(q(z|x)||p(z|x))+\u2113,\n\nwhere the first term is the KL divergence between the approximated posterior and the true posterior. Specifically, the KL divergence of two distributions q(x) and p(x) measures their similarity and is defined as:\n\n    KL(q(x)||p(x))=\u2211_xq(x)q(x)/p(x)=\ud835\udd3c_q(x)q(x)/p(x).\n\nIn Eq. (<ref>), since p(z|x) is intractable and KL divergence is non-negative, maximizing logp(x) is achieved by maximizing \u2113, which is the evidence lower bound (ELBO) of logp(x) defined as follows:\n\n    \u2113=E_q(z|x)logp(x|z)-KL(q(z|x)||p(z)),\n\nThe first term in \u2113 maximizes the conditional probability of x given the latent representation z and can be seen as the reconstruction loss, while the second term minimizes the difference between the prior and the approximated posterior.\n\n\n\n\n\u00a7 THE PROPOSED METHOD\n\n\nIn this section, we first provide an overview of the proposed hybrid variational autoencoder (HyVAE) method and then elaborate on its details.   \n\n\n\n \u00a7.\u00a7 Overview of HyVAE\n\nInspired by existing deterministic deep neural models, we propose a novel generative hybrid variational autoencoder (HyVAE) model for time series forecasting. HyVAE jointly learns the local patterns from time series subsequences and the temporal dynamics among time series subsequences. To achieve that, HyVAE is derived based on variational inference to integrate two processes: 1) the encoding of time series subsequences, which captures local patterns; and 2) the encoding of entire time series, which learns temporal dynamics among time series subsequences. In the following content, we separately detail the encoding of time series subsequences and the encoding of the entire time series, respectively, and then explain the integration of these two processes for time series forecasting.\n\n\n\n\n\n \u00a7.\u00a7 Encoding of time series subsequence\n\n\nAs discussed in Section <ref>, many existing models have shown that learning the local patterns can effectively improve time series forecasting <cit.>. To capture the flexible local patterns, we encode time series subsequences as latent random variables, rather than deterministic values.\n\nThe conventional VAE encodes a subsequence (x^t) into latent representations as independent random variables (z^t), which follow a multivariate Gaussian distribution, i.e., p(z^t)=\ud835\udca9(z^t|\u03bc^t,\u03c3^t) and \u03c3^t is the diagonal of a diagonal covariance matrix. However, the values of a time series subsequence are normally not independent and have a causal relationship (e.g., autoregressive); that is, the independent latent random variables cannot properly preserve the meaningful causal information <cit.> within a subsequence. Inspired by ladder variational autoencoder (LVAE) <cit.>, we enforce a hierarchical dependency among the latent random variables to capture the causal information in a subsequence. \n\n\n\n\nWe separate the latent random variables z^t into L groups/ladders (L is the ladder size), i.e, {z^t_1,...,z^t_L}, and the groups have a top-down hierarchical dependency (from 1 to L). We illustrate the encoding and decoding process of subsequence encoding in Fig. <ref>, in which the top row is the encoding process and the bottom row is the decoding process. For the convenience of implementation, we adopt the same top-down dependency among the latent random variables (z^t_1...z^t_L) in the encoding and decoding processes. Based on this, the prior distribution of z^t can be factorized as:\n\n    p(z^t)   =p(z^t_L)\u220f_i=1^L-1p(z^t_i|z^t_i+1),\n    \n    \n    p(z^t_i|z^t_i+1)   =\ud835\udca9(z^t_i|\u03bc_i^t(z_i+1^t),\u03c3_i^t(z_i+1^t)),\n\nwhere {\u03bc(\u22c6),\u03c3(\u22c6)}=\u03c6(\u22c6) and we implement \u03c6(\u22c6) as a multilayer perceptron (MLP). \nBy changing the size of dependency (L, the ladder size), we can regulate how well causal information is preserved, and no causal information when L=1 (i.e., all latent random variables are independent).\nBased on this, the generative model of subsequence encoding can further be factorized as follows:\n\n    p(x^t,z^t)=p(x^t|z_1^t)p(z_L^t)\u220f_i=1^L-1p(z_i^t|z_i+1^t),\n\nwhere p(x^t|z^t_1)=\ud835\udca9(x^t|\u03bc_i^t(z^t_1),\u03c3_i^t(z^t_1)).\nThis hierarchical dependency ensures the latent random variables have sufficient flexibility to model the complex local patterns of subsequences. Since the posterior p(z^t|x^t) is intractable, q(z^t|x^t) is used as an approximation. Meanwhile, to avoid {z_L^t,...,z_1^t} converging to arbitrary variables, they all depend on x^t in the inference model similar to <cit.> as follows:\n\n    q(z^t|x^t)   =q(z_L^t|x^t)\u220f_i=1^L-1q(z_i^t|z_i+1^t,x^t),\n    \n    \n    q(z_L^t|x^t)   =\ud835\udca9(z_L^t|\u03bc_i^t(x^t),\u03c3_i^t(x^t)),\n    \n    \n    q(z_i^t|z_i+1^t,x^t)   =\ud835\udca9(z_i^t|\u03bc_i^t(z_i+1^t,x^t),\u03c3_i^t(z_i+1^t,x^t)),\n\nwhere {\u03bc(\u22c6,\u22c6),\u03c3(\u22c6,\u22c6)}=\u03c6([\u22c6;\u22c6]) and [;] is the concatenation operation.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Encoding of entire time series\n\nFrom the global perspective, we encode all time series subsequences {x^1,...,x^T} as {z^1,...,z^T} to learn the temporal dynamics of entire time series. Since time series subsequences are normally not independent across different time stamps, we first impose a temporal dependency for consecutive subsequences (e.g., p(z^t,z^t-1)=p(z^t|z^t-1)p(z^t-1)). In addition, we capture long-term temporal dependency with other subsequences by hidden states of a recurrent neural network, i.e., gated recurrent unit (GRU) <cit.>. Therefore, we have the following derivation:\np(z^t|z^<t) can be derived as follows:\n\n    p(z^t|z^<t)   =p(z^t|z^t-1,h^t-1),\n    \n    \n    p(z^t|z^t-1,h^t-1)   =\ud835\udca9(z^t|\u03bc_i^t(z^t-1,h^t-1),\u03c3_i^t(z^t-1,h^t-1)),\n\nwhere h is the hidden state and is obtained by:\n\n    h^t=GRU(h^t-1,x^t).\n\nGRU(*) is the calculation of hidden states in a GRU unit. GRU adopts gates and memory cells and alleviates the gradient vanishing problem while being easier to train than LSTM due to fewer gates used. The structure of GRU is formulated as follows:\n\n    r^t   =\u03c3(W_r[h^t-1;x^t]),\n    \u03b6_t   =\u03c3(W_\u03b6[h^t-1;x^t]),\n    h\u0303^t   =tanh(W_h\u0303[r^t\u2218h^t-1;x^t]),\n    h^t   =(1-\u03b6_t)\u2218h^t-1+\u03b6_t\u2218h\u0303^t,\n\nwhere \u2218 is the element-wise product. Specifically, r^t and \u03b6^t are the reset gate vector and update gate vector, which decides how much past information needs to be forgotten/preserved, respectively. Meanwhile, h\u0303^t is the candidate activation vector that memorizes the past information, and h^t is obtained as the balanced sum of the short (h^t-1) memory and the long (h\u0303^t) memory.\n\nFor the generative process p(x^\u2264 T,z^\u2264 T), we explicitly simplify p(x^t|z^\u2264 t) as p(x^t|z^t) to ensure the local pattern of x^t is mainly preserved in z^t; this simplification also can largely reduce the complexity of the reconstruction/decoding process. Based on (<ref>), the generation model can be factorized as follows:\n\n    p(x^\u2264 T,z^\u2264 T)=\u220f_t=1^Tp(x^t|z^t,x^<t)p(z^t|x^<t,z^t-1),\n\np(x^t|z^t,x^<t) also can be denoted as p(x^t|z^t,h^t-1), due to the recursive nature of GRU, which requires h^t-1 being obtained by the recursive calculation with x^<t. \n\n\nSimilarly, we derive the inference model as:\n\n    q(z^t|x^\u2264 t,z^t-1)=   \ud835\udca9(z^t|\u03bc_i^t(x^\u2264 t,z^t-1),\u03c3_i^t(x^\u2264 t,z^t-1))\n    \n    \n    =   \ud835\udca9(z^t|\u03bc_i^t(h^t-1,x^t,z^t-1),\u03c3_i^t(h^t-1,x^t,z^t-1)).\n\nThe above approximated posterior of z^t captures the long-term dynamics carried by x^<t (h^t-1), the neighbouring dependency with z^t-1, and the corresponding subsequence x^t.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Integration and joint learning\n\nBased on the encoding of a subsequence and the encoding of the entire time series (represented as subsequences) discussed above, we now integrate them into a HyVAE model, which can jointly learn the local patterns and temporal dynamics for time series forecasting. The jointly learned latent random variables for both time series subsequences and the entire time series are denoted as {(z_L^1,...,z_1^1),...,(z_L^T,...,z_1^T)}, with respect to time series {x^1,...,x^T}, and the encoding process is illustrated in Fig. <ref>. By combining the prior of subsequence encoding in Eq. (<ref>) and the prior of entire time series encoding in Eq. (<ref>), we obtain the prior of HyVAE, which is factorized as follows:\n\n    p(z^t|z^t-1,h^t-1)   =p(z_L^t|z_1^t-1,h^t-1)\u220f_i=1^Lp(z_i^t|z_i+1^t),\n    \n    \n    p(z_L^t|z_1^t-1,h^t-1)   =\ud835\udca9(z_L^t|\u03bc^t(z_1^t-1,h^t-1),\u03c3^t(z_1^t-1,h^t-1)),\n    \n    \n    p(z_i^t|z_i+1^t)   =\ud835\udca9(x^t|\u03bc_i^t(z^t_1),\u03c3_i^t(z^t_1)).\n\nAs shown in Fig. <ref> (a), the prior of HyVAE integrates the long-term temporal dynamics by affecting the first latent random variable of each subsequence (e.g., z_L^t) with the hidden states (e.g., h^t-1, generated by GRU) of its precedent subsequence. Meanwhile, h^t is obtained by the recurrence process with GRU as shown in Fig. <ref> (b). We then obtain the inference model of HyVAE by integrating Eq. (<ref>) and Eq. (<ref>) as follows (Fig. <ref> (c)):\n\n    q(z^t|x^\u2264 t,z^t-1)   =q(z_L^t|x^\u2264 t,z_1^t-1)\u220f_i=1^L-1q(z_i^t|z_i+1^t,x^t),\n    \n    \n    q(z_L^t|x^\u2264 t,z_1^t-1)   =\ud835\udca9(z_L^t|\u03bc^t(z_1^t-1,x^\u2264 t),\u03c3^t(z_1^t-1,x^\u2264 t)),\n    \n    \n    q(z_i^t|z_i+1^t,x^t)   =\ud835\udca9(z_L^t|\u03bc_i^t(z_i+1^t,x^t),\u03c3_i^t(z_i+1^t,x^t)).\n\nSimilarly, for x^t, the above encoding process also includes the temporal dynamics carried by h^t-1 during the encoding of z_L^t, while the rest latent random variables of z^t only learn from x^t. Then, as shown in Fig. <ref> (d), the generation model of HyVAE is obtained by combining Eq. (<ref>) and Eq. (<ref>) as follows:\n\n    p(x^t|z^t,x^<t)   =p(x^t|z_1^t,h^t-1),\n    \n       =\ud835\udca9(x^t|\u03bc_i^t(z_1^t,h^t-1),\u03c3_i^t(z_1^t,h^t-1)).\n\nFollowing the derivative process of variational inference, with Eq. (<ref>) to (<ref>), HyVAE learns the latent representations by maximizing its ELBO defined as follows:\n\n    \u2113_enc=   \u2211_t=1^T{\ud835\udd3c_q(z_l^t|x^\u2264 t,z_1^t-1)log p(x^t|h^t-1,z_1^t) \n       -\u2211_1^l-1KL(q(z_i^t|z_i+1^t,x^t)||p(z_i^t|z_i+1^t)) \n       -KL(q(z_l^t|x^\u2264 t,z_1^t-1)||p(z_l^t|x^<t,z_1^<t)}.\n\nThe first term in \u2113_enc implies the reconstruction loss of HyVAE for each time series subsequence, i.e., between the input x^t and the x\u0302^t reconstructed with (z^t_1,h^t-1) (see Fig. <ref> (d)). The second and third terms are regularization terms that enforce the encoded latent random variables to jointly capture the local patterns of individual subsequences and learn the temporal dynamics of the entire time series. The expectation of \u2113_enc is approximated by Monte Carlo estimation <cit.> and is estimated with the average of the \u2113_enc of each sample time series.\n\nWe use h^t and z^t for the final time series forecasting, i.e., \u0177=\u03c8(h^t,z^t), where \u03c8(*) is a single-layer fully-connected neural network. The forecasting loss is measured by Eq. (<ref>):\n\n    \u2113_pred=Err(y,\u0177).\n\nThen, the overall loss minimizes the negative ELBO of HyVAE and the forecasting loss as follows:\n\n    \u2113=-\u2113_enc+\u2113_pred.\n\nIn \u2113, \u2113_enc aims at learning representations that capture the latent distribution of time series, while \u2113_pred can be regarded as a regularization term that ensures the latent representations can provide insights for accurate forecasting.\nWe perform ADMA <cit.> for the optimization and use the reparameterization trick <cit.> for the model training. For \u2113_enc, we adopt the warm-up scheme <cit.> during the implementation to avoid inactive latent random variables caused by the variational regularization.\n\n\n\n\n\n\u00a7 EVALUATION\n\n\nIn this section, we first introduce the real-world datasets used to evaluate the proposed method. Then, we explain the accuracy metrics for time series forecasting and briefly describe the counterpart methods. Finally, we analyze the results and compare HyVAE with counterpart methods regarding the effectiveness of time series forecasting. All the experiments are implemented with Python 3.7 and run on a Linux platform with a 2.6G CPU and 132G RAM.\n\n\n\n\n\n\n \u00a7.\u00a7 Datasets\n\nWe choose four datasets widely used for time series forecasting. Parking Birmingham dataset <cit.> is collected from car parks in Birmingham, which regularly records the total occupancy of all available parking spaces between October 4, 2016, and December 19, 2016. We down-sample the recording frequency to every 5 hours and result in 3571 records. Another NASDAQ stock dataset <cit.> consists of stock prices of 104 corporations together with the overall NASDAQ100 index, which is collected from July 26, 2016, to December 22, 2016. We use the NASDAQ100 index for forecasting, and down-sample the records every 30 minutes, which results in 1352 records.\nThe other two datasets[https://research.cs.aalto.fi/aml/datasets.shtml.] record the electricity load values of Poland from the 1990s and monthly Darwin sea level pressures from 1882 to 1998, respectively; both datasets contain 1400 records.\nWe preprocess each dataset with Min-Max normalization by:\n\n    s'_i=s_i-min(s)/max(s)-min(s).\n\nThen, each dataset is split into a training set, a validation set and a test set by {80%,10%,10%}. The number of known time series values used for forecasting is fixed as 50 for all datasets. The statistics of the datasets are shown in Table <ref>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Performance metric\n\nWe use three different metrics widely used for time series forecasting <cit.> in the evaluation, and they are mean square error (MSE), mean absolute error (MAE), and mean absolute percentage error (MAPE).\nMSE and MAE respectively measure the variance and average of the residuals of the forecasting results to ground truth and are respectively defined as follows:\n\n    MSE=1/n\u2211_i=1^n(y_i-\u0177_i)^2,\n    MAE=1/n\u2211_i=1^n|y_i-\u0177_i|.\n\nMAPE measures the proportion of forecasting deviation to the ground truth as follows:\n\n    MAPE=1/n\u2211_i=1^n|y_i-\u0177_i/max(\u03f5,y_i)|,\n\nwhere \u03f5 is an arbitrarily small positive value to ensure the dividing is always legal.\n\n\n\n \u00a7.\u00a7 Counterpart methods\n\nWe select three types of counterpart methods to compare with the proposed method, i.e., the classical statistical models, deterministic DNN-based methods, and VAE-based methods. The classical models include the widely used AR, ARIMA, and SVR. For deterministic DNN-based methods, we choose the LSTM and Informer and implement a stacked CNN and LSTM model (CNN+LSTM) following <cit.> for time series forecasting. For the VAE-based methods, other than the vanilla VAE, we adopt VRNN <cit.> and LaST <cit.>. We brief these methods as follows:\n\n\n\n\n  \u2013 AR forecasts with the weighted sum of past values. ARIMA incorporates moving average and differencing to AR for non-stationary time series. \n\n  \u2013 SVR <cit.> is based on the support vector machine (SVM) and the principle of structural risk minimization.\n\n  \u2013 LSTM <cit.> is an RNN model that can learn the long dynamics with its forget gates. \n\n  \u2013 Informer <cit.> uses multi-head attention with position encoding to learn the latent structure of time series for forecasting. \n\n  \u2013 CNN+LSTM <cit.> stacks CNN and LSTM for accurate air quality forecasting. CNN+LSTM includes three TCN layers and two bi-LSTM layers. \n\n  \u2013 Vanilla VAE <cit.> is the basic variational autoencoder that learns latent representations as independent Gaussian random variables. \n\n  \u2013 VRNN <cit.> extends VAE to be capable of learning temporal dynamics by introducing temporal dependency among the latent representations. \n\n  \u2013 LaST <cit.> adopts disentangled variational autoencoder to capture seasonality and trend, with auxiliary objectives to ensure dissociate representations.\n\n\n\n\n\n \u00a7.\u00a7 Experiment setup\n\n\nIn all the experiments, we use the validation sets to tune optimal parameters and use the test sets for forecasting accuracy measurement. For AR, we search the optimal number of lag (past time series values) from 1 to 10, and use the same strategy to search optimal p (the number of past observations) and q (the size of moving average window) for ARIMA, with the optimal differencing degree searched from 0 to 3. For SVR, we adopt the radial basis function (RBF) kernel for running, with its parameters C (regularization parameter) searched from {1,10,100,1000} and \u03b3 (kernel coefficient) searched from {0.00005,0.0005,0.005,0.05}. \n\nFor LSTM, Informer, CNN+LSTM, vanilla VAE, VRNN, LaST, and HyVAE, we search the optimal batch size from {32,64,128} and set the maximum iteration to be 100 epochs. The learning rate is searched from {0.001,0.01,0.1}.\nThe dimension of the LSTM/GRU hidden states and latent representations are searched from {8,16,32,64,128}, and the number of layers is no more than 3. For HyVAE, the ladder size and the subsequence length are searched from {2,4,6,8,10} and {10,20,30,40}, respectively. We run each method 50 times and report the average accuracy as the final results.\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Main results\n\nIn this experiment, we compare the accuracy of HyVAE with counterpart methods, with respect to single-step forecasting and multi-step (3, 4, and 5 steps) forecasting, respectively, on the four datasets. \n\nAs shown in Table <ref>, HyVAE generally achieves the best performance among all methods on the four datasets. Notably, we observe on the Parking dataset, the MSE achieved by HyVAE (0.133\u00d7 10^-2) is nearly three times smaller than that of the second-best performed LaST (0.366\u00d7 10^-2). The least improvement over all the counterpart methods is shown in the Sealevel dataset, in which HyVAE reduces the MSE, MAE, and MAPE of VRNN (the second best) by 6.3%, 9.1% and 1.6%, respectively. When further considering the type of the counterpart methods, first, we see HyVAE achieves significant improvement over the classical AR, ARIMA, and SVR methods, by achieving nearly one magnitude smaller MSE on the Parking and Stock datasets. Second, compared with the deterministic DNN-based LSTM, Informer, and CNN+LSTM, HyVAE also shows significant improvement; especially on the Stock dataset, HyVAE achieves around two times smaller MSE, MAE, and MAPE than the best-performed deep neural network model (CNN+LSTM). Although CNN+LSTM also considers both local patterns and temporal dynamics of time series and outperforms LSTM and Informer on all the datasets, HyVAE constantly being more effective and thus is better at capturing the complex structure of time series for forecasting. Third, we can see that HyVAE achieves more accurate forecasting results than other VAE-based methods that only learn part of the information of time series. That includes the vanilla VAE, which misses the temporal dynamics, VRNN, which only learns the temporal dynamics, and LaST for seasonality/trend patterns of time series. This observation shows the effectiveness of HyVAE to learn both the local patterns and temporal dynamics for time series forecasting. \n\nFor multi-step forecasting, in Table <ref>, we show the MSE of LSTM, Informer, CNN+LSTM, VRNN, LaST, and HyVAE; AR, ARIMA, SVR, and vanilla VAE are excluded due to low performance. The results show that HyVAE achieves more accurate forecasting results than compared counterpart methods. Although generally, the forecasting accuracy decreases with larger forecasting steps, except for the Parking dataset, the forecasting accuracy of HyVAE decreases much slower than the compared counterpart methods since it captures more informative patterns of time series. For example, from 3-step forecasting to 5-step forecasting in the Electricity dataset, the MSE of HyVAE only decreases by 0.028, while LSTM, Informer, CNN+LSTM, VRNN, and LaST decrease by 0.046, 0.143, 0.034, 0.226 and 0.063, respectively. Meanwhile, CNN+LSTM and HyVAE constantly produce more accurate forecasting results than other deterministic DNN-based methods and VAE-based methods, respectively, and that again supports the effectiveness of learning both local patterns and temporal dynamics for time series forecasting.\n\n\n\n\n\n\n \u00a7.\u00a7 Ablation analysis\n\nWe conduct an ablation analysis to further understand the effectiveness of learning both the local patterns and the temporal dynamics in HyVAE. To do that, we implement two variants of HyVAE by removing the learning of one type of information, respectively; that is, w/o Subseq that excludes the learning of local patterns from subsequences, and w/o Entire that does not learn the temporal dynamics of the entire time series. The parameters of w/o Subseq and w/o Entire are tuned with the validation set the same as HyVAE, and we show the results of time series forecasting measured by MSE in Table <ref>. \n\n\n\n\n\n\n\nIn Table <ref>, HyVAE that learns both information achieves higher forecasting accuracy than the two variants. Specifically, the largest improvement of HyVAE towards the variants is shown in the Stock dataset (0.087), i.e., around six times smaller than that of w/o Subseq (0.513, second best). The smallest improvement appears in the Sealevel dataset, but the MSE of HyVAE is still around two times smaller than that of the second-best performed w/o Subseq (0.623 to 1.088). Meanwhile, it is interesting to observe that w/o Entire, which misses the temporal dynamics, constantly performs worse that w/o Subseq, which misses local patterns, on the four datasets. \n\n\n\n\n\n\n\nWe further show the forecasting results of HyVAE and the variants against the ground truth in Fig. <ref>. For w/o Entire, since no temporal dynamics is learned, it cannot properly capture the global trend of time series; especially for the Stock dataset (Fig. <ref> (b)), it misunderstands the steady curves between the 55 and 75 timestamps as sharp spikes. As for the results of the Electricity dataset and the Sealevel dataset shown in Fig. <ref> (c-d), w/o Entire only emphasizes recurring local patterns but misses their differences at different timestamps. Meanwhile, w/o Subseq can better express the temporal dynamics than w/o Entire, as clearly shown in Fig. <ref> (b); however, it fails to properly capture local details. By combining the strengths of w/o Entire and w/o Subseq, HyVAE achieves the best forecasting results that are quite close to the ground truth.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Parameter analysis\n\nIn this experiment, we analyze the impact of three parameters of HyVAE, i.e., the ladder size, the subsequence length, and the embedding size, on its performance. Specifically, the ladder size determines the causal information during subsequence encoding, and we vary the ladder size from 0 to 10, where 0 means HyVAE learns no causal information of subsequences (see Fig. <ref>). The subsequence length balances the local patterns and the temporal dynamics, i.e., HyVAE is degraded to w/o Subseq or w/o Entire if subsequence length is 0 (no subsequence) or the maximum (50, the subsequence becomes the entire time series), respectively.    \nThe results measured by MSE are shown in Fig. <ref>. For the ladder size shown in Fig. <ref> (a-d), the forecasting accuracy significantly decreases when ladder size is too small or too large. The optimal ladder sizes are relatively small (2 for the Sealevel dataset, 4 for the Parking and Stock datasets, and 6 for the Electricity dataset). Meanwhile, we see that when the ladder size equals 0, HyVAE still outperforms w/o Subseq on all the datasets. The results of subsequence length are shown in Fig. <ref> (c-d), in which we see that HyVAE prefers short subsequences to obtain optimal forecasting results, i.e., subsequence length is 10 (Parking, Electricity, and Sealevel datasets) or 20 (Stock dataset). The reason is that if the subsequences are too long, temporal dynamics can hardly be preserved. Not surprisingly, HyVAE that learns temporal dynamics with different subsequence length achieves better forecasting accuracy than w/o Entire, which does not learn temporal dynamics at all.\n\n\n\n\n\nWe then run HyVAE with varying embedding size {8,16,32,64,128}, which determines the dimension of latent representation and the dimension of hidden states in neural networks, and the results are shown in Fig. <ref>. On all the datasets, accuracy measured by MSE, MAE, and MAPE has similar trends. First, the forecasting accuracy is low with small embedding size, mainly because the small size of latent random variables cannot properly capture the complex non-linear processes of time series. When the embedding size becomes too large (128), the accuracy decreases due to over-fitting. The results show that HyVAE can obtain optimal forecasting results with relatively small embedding size.\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nThis paper proposes a novel hybrid variational autoencoder (HyVAE) model for time series forecasting. HyVAE integrates the learning of local patterns and temporal dynamics into a variational autoencoder. Through comprehensive evaluation on four real-world datasets, we show that HyVAE achieves better time series forecasting accuracy than various counterpart methods, including a deterministic DNN-based method (CNN+LSTM) that also learns both information of time series.\nMoreover, the ablation analyses demonstrate that HyVAE outperforms its two variants which only learn local patterns or temporal dynamics from time series.\n\n\n\nelsarticle-num\n\n\n\n\n\n\n\n"}