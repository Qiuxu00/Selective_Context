{"entry_id": "http://arxiv.org/abs/2303.07170v1", "published": "20230313151303", "title": "Validation of uncertainty quantification metrics: a primer", "authors": ["Pascal Pernot"], "primary_category": "physics.chem-ph", "categories": ["physics.chem-ph", "physics.data-an", "stat.ML"], "text": "\n\n\n\n\n\nInstitut de Chimie Physique, UMR8000 CNRS,\u00a0\n\nUniversit Paris-Saclay, 91405 Orsay, France\npascal.pernot@cnrs.fr\n\n\nThe practice of uncertainty quantification (UQ) validation,\nnotably in machine learning for the physico-chemical sciences, rests\non several graphical methods (scattering plots, calibration curves,\nreliability diagrams and confidence curves) which explore complementary\naspects of calibration, without covering all the desirable ones. For\ninstance, none of these methods deals with the reliability of UQ metrics\nacross the range of input features. Based on three complementary concepts,\ncalibration, consistency and adaptivity, the\ntoolbox of common validation methods for variance- and intervals-\nbased metrics is revisited with the aim to provide a better grasp\non their capabilities. This study is conceived as an introduction\nto UQ validation, and all methods are derived from a few basic rules.\nThe methods are illustrated and tested on synthetic datasets and examples\nextracted from the recent physico-chemical machine learning UQ literature. \n\nValidation of uncertainty quantification metrics: a primer \n    Pascal PERNOT 0000-0001-8586-6222\n    March 30, 2023\n===========================================================\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nThe quest for confidence in the predictions of data-based\nalgorithms<cit.> has led to a profusion\nof uncertainty quantification (UQ) methods in machine learning (ML).<cit.>\nNot all of these UQ methods provide uncertainties that can be relied\non,<cit.> notably if one expects uncertainty\nto inform us on a range of plausible values for a predicted property.<cit.>\nIn pre-ML computational chemistry, UQ metrics consisted essentially\nin standard uncertainty, i.e. the standard deviation of the\ndistribution of plausible values (a variance-based metric),\nor expanded uncertainty, i.e. the half-range of a prediction\ninterval, typically at the 95 % level (an interval-based\nmetric).<cit.> The advent of ML methods provided\nUQ metrics beyond this standard setup, for instance distances in feature\nor latent space<cit.> or the recent\n\u0394-metric,<cit.> which have no direct statistical\nor probabilistic meaning. These metrics might however be converted\nto variance- or interval-based metrics by calibration methods, such\nas conformal inference.<cit.> Still, all\nUQ metrics need to be validated to ensure that they are adapted to\ntheir intended use.\n\nThe validation of UQ metrics is designed around the concept of calibration.\nThe use of uncertainty as a proxy to identify unreliable predictions,\nas done in active learning, does not require the same level\nof calibration as its use for the prediction of properties at the\nmolecule-specific level.<cit.> A handful of validation\nmethods exist that explore more or less complementary aspects of calibration.\nA trio of methods seems to have recently taken the center-stage: the\nreliability diagram (or RMSE vs RMV plot), the calibration\ncurve and the confidence curve. They implement three different\napproaches to calibration which are not necessarily independent, but\nthey do not cover the full spectrum of calibration requirements. In\nparticular, none of these methods addresses the essential reliability\nof predicted uncertainties with respect to the input features. As\nwill be shown below, a UQ metric validated by this trio of methods\nmight still be unreliable for individual predictions. Moreover, it\nappears that some methods are not always used in an appropriate context,\nsuch as the oracle confidence curve for variance-based UQ metrics.<cit.>\n\nThe aim of this article is to design a principled validation framework\nbased on complementary calibration concepts and review the relevant\nmethods within this framework. The choice of methods is based on two\nmain criteria: (1) the nature of the UQ metric to be validated (distribution,\ninterval, variance, other...) and (2) the calibration target (average\ncalibration, consistency or adaptivity, to be defined\nbelow).\n\n\n\n \u00a7.\u00a7 Related studies\n\n\nMost of the validation methods presented in this study derive\nfrom the seminal work of Gneiting et al.<cit.>\non the calibration of probabilistic forecasters. Probabilistic\nforecasters are models that provide for each prediction a distribution\nof plausible values. A major lesson from Gneiting's work is that calibration\nmetrics estimated over a validation dataset (average calibration)\ndo not necessarily lead to useful UQ statements and that additional\nproperties are necessary to design reliable UQ methods. An example\nis sharpness which quantifies the concentration of the prediction\nuncertainties: among a set of average-calibrated methods, one should\nprefer the sharpest one. Pernot<cit.> noted that, in\nthe absence of a target value, sharpness might not be useful for the\nvalidation of individual UQ metrics, so that alternative statistics\nare required for UQ validation.\n\nThe definition of calibration provided by Gneiting et al. was\nextended by Kuleshov<cit.>. The formulation of Kuleshov,\nbased on coverage probabilities, led to use calibration curves\nas a validation tool. But Levi et al.<cit.> demonstrated\nthat calibration curves are not reliable and proposed instead to use\nindividual calibration<cit.>, based on the conditional\nvariance of the errors with respect to the prediction uncertainty.\nImplementation of this conditional calibration equation led\nto reliability diagrams or RMSE vs RMV plots, also called\ncalibration diagrams by Laves et al.<cit.>.\nIn practice, individual calibration is generally unreachable, and\nan alternative is to consider local calibration<cit.>, which\nis reflected in the implementation of conditional statistics through\nbinning schemes, as for instance in reliability diagrams.\n\nPernot<cit.> proposed the concept of tightness\nto differentiate local calibration from average calibration, and implemented\nit in a Local Z-Variance (LZV) analysis in which calibration\nis estimated in subsets of the validation dataset. These subsets can\nbe designed according to any relevant property (predicted value, uncertainty\nor input feature). He also established the link between LZV analysis\nin uncertainty space and reliability diagrams. Both reliability diagrams\nand LZV analysis derive from the concept of conditional calibration,\nbut, for reliability diagrams, the conditioning variable is prediction\nuncertainty, while the LZV analysis opens a larger palette of conditioning\nvariables. Conditional coverage with respect to input features\nwas proposed by Vovk<cit.> to asses the adaptivity<cit.>\nof conformal predictors. The adaptivity concept can therefore be linked\nto conditional calibration in input feature space, a form of tightness. \n\nConfidence curves derive from another validation approach,\nknown as sparsification error curves mainly used in computer\nvision.<cit.> They were not intended to validate\ncalibration, but to estimate the correlation between absolute errors\nand UQ metrics.<cit.> Pernot<cit.> showed\nthat the standard reference curve (the so-called oracle) used\nfor the evaluation of confidence curves is irrelevant for variance-based\nUQ metrics, and he introduced a new probabilistic reference\nthat enables to test conditional calibration in uncertainty space.\n\nTran et al.<cit.> and Scalia et al.<cit.>\npublished motivating overviews of some of the methods presented here\n(reliability diagrams, calibration and confidence curves) applied\nto ML-UQ in materials sciences.\n\n\n\n \u00a7.\u00a7 Contributions \n\n\nBuilding on the the works of Levi et al.<cit.>,\nPernot<cit.> and Angelopoulos et al.<cit.>\nabout conditional calibration, I propose to distinguish two calibration\ntargets besides average calibration, namely \n\n\n  * consistency as the conditional calibration with respect to\nprediction uncertainty, and \n\n  * adaptivity as the conditional calibration with respect to input\nfeatures. \n\nConsistency derives from the metrological consistency of\nmeasurements,<cit.> while adaptivity is borrowed from\nthe conformal inference literature<cit.>. Tightness,\nas introduced by Pernot<cit.>, should then be understood\nas covering both consistency and adaptivity. Unless there is a monotonous\ntransformation between input features and prediction uncertainty,\nconsistency and adaptivity are distinct calibration targets. \n\nThe distinction between consistency and adaptivity is important to\nbetter define the objective(s) of each validation method. In fact,\nadaptivity has not been considered in recent overviews of ML-UQ validation\nmetrics<cit.>, but was practically implemented\n\u2013 albeit unnamed \u2013 through the LZV and Local\nCoverage Probability (LCP) analyses (see for instance Fig. 4 in\nPernot<cit.>). Still, most ML-UQ validation studies\nfocus on calibration and consistency, and I will show that this is\nnot sufficient to ensure the reliability of an UQ method across the\ninput features space. Adaptivity is essential to achieve reliable\nUQ at the molecule-specific level advocated by Reiher<cit.>.\n\n\n\n \u00a7.\u00a7 Structure of the article\n\n\nThe theoretical bases of the calibration-consistency-adaptivity\nframework are presented in the next section (Sect. <ref>)\nfor variance- and interval-based UQ metrics. A comprehensive set of\nvalidation methods is reviewed next (Sect. <ref>)\naccording to their application range and calibration target, in order\nto appreciate their merits and limitations. Examples from the recent\nmaterials and chemistry ML-UQ literature are treated as case studies\nin Sect. <ref>. Recommendations are presented as\nconclusions (Sect. <ref>).\n\n\n\n\u00a7 VALIDATION CONCEPTS AND MODELS\n\n\nA major distinction will be made below between variance-based\nUQ metrics and interval-based UQ metrics. These occur in ML\nas statistical summaries of empirical distributions or ensembles,\nor as parameters of theoretical distributions (typically normal).\nOther UQ metrics, such as distances in feature space or latent space,<cit.>\nor the \u0394-metric,<cit.> which have not the correct\ndimension to be comparable to errors, need first to be converted to\none of those two metrics to be usable for validation.<cit.>\n\n\n\n \u00a7.\u00a7 Validation model for variance-based UQ metrics\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Validation datasets\n\n\nIn order to validate variance-based UQ results, one needs\na set of predicted values V={V_i}_i=1^M, the corresponding\nuncertainties u_V={u_V}_i=1^M, reference data to compare\nwith R={R_i}_i=1^M, and possibly their uncertainties u_R={u_R_i}_i=1^M.\nMost of the validation methods considered below require to transform\nthese to E=R-V and u_E=(u_R^2+u_V^2)^1/2. \n\nThe minimal validation dataset, C={ E,u_E}, enables\nto test calibration and consistency but not adaptivity. It is therefore\nbetter to include a relevant input feature X in the dataset C={ E,u_E,X},\nextending the validation targets to adaptivity in X space. Alternatively,\nif validation is done a posteriori and no relevant input feature\nis available, adaptivity can be tested on C={ E,u_E,V},\nwith some caveats presented in Sect. <ref>.\n\n\n\n  \u00a7.\u00a7.\u00a7 Generative model\n\n\nFor variance-based UQ metrics, validation methods are based\non a probabilistic model linking errors to uncertainties\n\n    E_i\u223c D(0,u_E_i)\n\nwhere D(\u03bc,\u03c3) is an unspecified probability density function\nwith mean \u03bc and standard deviation \u03c3. This model states\nthat errors should be unbiased (\u03bc=0) and that uncertainty describes\ntheir dispersion as a standard deviation, following the metrological\nstandard.<cit.> \n\nIt is essential to note that the relation between E and u_E\nis asymmetric: small uncertainties should be associated with small\nerrors, but small errors might be associated with small or large uncertainties,\nwhile large errors should be associated with large uncertainties.\nThis considerably diminishes the interest of validation tests based\non the ranking of E vs u_E (e.g. correlation coefficients).\n\n\n\n  \u00a7.\u00a7.\u00a7 Validation model\n\n\n\n\n  \nAverage calibration.\n\nAccording to the generative model, the validation of u_E\nshould be based on testing that it correctly describes the dispersion\nof E.<cit.> One can for instance check\nthat\n\n    Var(E)\u2243<u_E^2>\n\nwhere the average is taken over the validation dataset, and which\nis valid only if <E>\u22430.<cit.> However, this\nformula ignores the pairing between errors and uncertainties, and\na more stringent test is based on z-scores (Z=E/u_E)<cit.>,\ni.e.\n\n    Var(Z)\u22431\n\nNote that if the elements of E and u_E are obtained as the\nmeans and standard deviations of small ensembles (say less than 30\nelements) these formulas have to be transformed in order to account\nfor the uncertainty on the statistics.<cit.> Unfortunately,\nhypotheses need then to be made on the distribution of these small\nensembles. For a normal distribution, one should have Var(Z)\u2243(n-1)/(n-3),\nwhere n is the ensemble size.<cit.> \n\nThe satisfaction of one or both of these tests (Eqns. <ref>-<ref>)\nvalidates average calibration, which is a minimal requirement,\nbut does not guarantee the usefulness of individual uncertainties,\nas average calibration can be satisfied by a compensation of under-\nand over-estimated values of u_E.\n\n\n\n  \nConditional calibration: consistency and adaptivity.\n\nBased on Eq. <ref>, one can evaluate the reliability\nof individual uncertainties by conditional calibration<cit.>,\ni.e., \n\n    Var(Z|A=a)\u22431, \u2200 a\u2208\ud835\udc9c\n\nwhere A is a variable with values a in \ud835\udc9c. As detailed\nbelow, A can be any relevant quantity, such as the uncertainty\nu_E or a feature X. The choice of the conditioning variable\ndepends on the question to be answered. Choosing the uncertainty u_E\nwill assess the calibration across the range of uncertainty values,\ni.e. the consistency between E and u_E, while choosing\nX will assess adaptivity. \n\nUsing this terminology, one can see that the individual calibration\nproposed by Levi et al.<cit.> \n\n    Var(E|u_E=\u03c3)\u2243\u03c3^2, \u2200\u03c3>0\n\nderiving from Eq. <ref> validates consistency. Although\nit is a step forward from average calibration, it does not test the\nadaptivity of the UQ metric under scrutiny. Nor does the local\nZ-Variance (LZV) analysis<cit.>\nin u_E space, which can be derived from Eq. <ref>\n\n    Var(Z|u_E=\u03c3)\u22431, \u2200\u03c3>0\n\n\nPractical implementation of conditional calibration tests to variance-\nand interval- based UQ metrics requires to split the validation set\ninto subsets, generally based on the binning of the conditioning variable.\nIn these conditions, one is more testing local than individual\ncalibration<cit.>, and Eq. <ref> leads\nto reliability diagrams<cit.>, and Eq. <ref>\nleads to the local Z-Variance (LZV) analysis in u_E\nspace. Note that the bin size in these methods should be small enough\nto get as close as possible to individual calibration, but also large\nenough to ensure a reasonable power for statistical testing. The LZV\nanalysis is easily applicable to any conditioning variable (see examples\nin Pernot<cit.>), which is not the case of the reliability\ndiagram which would need to superimpose reliability curves for each\nsubset and be difficult to analyze. \n\nAn ideal variance-based UQ metric, i.e., one which provides reliable\nindividual uncertainties, should satisfy calibration, consistency\nand adaptivity. Consistency and adaptivity are therefore two\ncomplementary aspects of tightness, a term introduced in a\nprevious study<cit.> to characterize local calibration. \n\nCalibration is a necessary condition to reach consistency or adaptivity.\nIn fact, consistency/adaptivity expressed as conditional calibration\nshould imply average calibration, but the splitting of the data into\nsubsets makes that the power of individual consistency/adaptivity\ntests is smaller than for the full validation set. It is therefore\nbetter to test average calibration separately, notably for small validation\ndatasets. Note that for homoscedastic datasets (u_E=c^te) consistency\nis implied by calibration, but not adaptivity. As already mentioned,\nas X, V and u_E are not necessarily related by monotonous\ntransformations, one should not expect these complementary conditional\ncalibration tests to provide identical results.\n\n\n\n \u00a7.\u00a7 Validation model for interval-based UQ metrics\n\n\nPrediction intervals can be extracted from predictive distributions,<cit.>\ngenerated by quantile regression or conformal predictors,<cit.>\nor estimated from a variance-based UQ metric and an hypothetical generative\ndistribution<cit.>.\n\n\n\n  \u00a7.\u00a7.\u00a7 Validation datasets\n\n\nIn order to validate interval-based UQ results, one needs\na set of reference data R={R_i}_i=1^M, and possibly their\nuncertainties u_R={u_R_i}_i=1^M, and a series of prediction\nintervals with prescribed coverage P, I_V={I_V,P={I_V_i,P}_i=1^M}_P\u2208\ud835\udcab,\nwhere I_V_i,P=[I_V_i,P^-,I_V_i,P^+] and \ud835\udcab\nis a set of coverage values, typically expressed as percentages in\n]0,100[. \n\nFor convenience and consistency with the variance-based approach,\none might transform these data as errors and error intervals C={V,I_E}={V_i,{I_E_i,P}_P\u2208\ud835\udcab}_i=1^M\nwhere I_E_i,P=[R_i-I_V_i,P^-, R_i-I_V_i,P^+]\nis the P % interval for the prediction error at point i.[Note that this definition differs from the one in my previous studies,\nwhich were based on the existence of a predicted value and an expanded\nuncertainty, leading to zero-centered intervals that should contain\nthe error. In the present case the intervals are error-centered, but\nshould contain 0.] As for variance-based UQ metrics, the uncertainty on the reference\nvalues, if any, should be accounted for, and the inclusion in the\ndataset of pertinent input values or features X={X_i}_i=1^M\nmight be of interest to test adaptivity.\n\n\n\n  \u00a7.\u00a7.\u00a7 Validation model\n\n\n\n\n  \nAverage coverage.\n\nWithin this setup, a method is considered to be calibrated\nif prediction intervals have the correct empirical coverage. One defines\nthe prediction interval coverage probability (PICP) as\n\n    \u03bd_p=\u2119(0\u2208 I_E,P)\n\nwhere \u2119 is the probability function, and I_E,P is\na P=100p % prediction error interval. Using PICPs, a method\nis calibrated in average, or marginally, if\n\n    \u03bd_p=p, \u2200100p\u2208\ud835\udcab\n\nwhich is equivalent to Kuleshov's definition<cit.>\nwhen all probability levels can be estimated (for instance if one\nhas access to the full predictive distribution). For a limited set\nof probabilities, one gets a milder calibration constraint. \n\nApplication of Eq. <ref> for a series of percentages\nprovides a calibration curve, where the values of \u03bd_p\nare plotted against the target probabilities p.<cit.> \n\n\n\n  \nConditional coverage.\n\nAverage calibration based on PICPs can be met by validation sets with\nunsuitable properties<cit.>, and, as for variance-based\nUQ metrics, it is possible to build more stringent calibration tests\nbased on conditional coverage,<cit.> i.e.,\n\n    \u03bd_p=\u2119(0\u2208 I_E,P|A=a),\u2200 a\u2208\ud835\udc9c\n\nwhere A is a property with values a in \ud835\udc9c. To get\nthe analog of consistency, one might consider to use U_P, the\nhalf-range of I_E_i,P, as a conditioning variable in order\nto check that errors are consistent at all uncertainty scales. For\ninstance, Pernot used the expanded uncertainty U_95 to this aim.<cit.>\nFor adaptivity testing, the conditioning variable can be any relevant\nfeature X.<cit.>\n\nIn practice, the PICPs are estimated as frequencies over the validation\nset\n\n    \u03bd_p,M=1/M\u2211_i=1^M1(0\u2208 I_E_i,P)\n\nwhere 1(x) is the indicator function for proposition\nx, taking values 1 when x is true and 0 when x is false.\nEstimating a PICP amounts to count the number of times prediction\nerror intervals contain zero. \n\nImplementation of the conditional coverage tests requites binning\naccording to the conditioning variable, with PICP testing within each\nbin, leading to the Local Coverage Probability (LCP) analysis.<cit.>\n\nNote that, as for reliability diagrams it is also possible to consider\nconditional calibration curves, with the same problem of readability\nand interpretability of overlapping curves.\n\n\n\n\u00a7 VALIDATION METHODS\n\n\nThe principles exposed in the previous section are now developed\ninto practical methods grouped by validation target (average calibration,\nconsistency and adaptivity). All methods are illustrated on synthetic\ndatasets designed to reveal their potential limitations.\n\n\n\n \u00a7.\u00a7 Synthetic datasets\n\n\nTo illustrate the methods presented in this study, six datasets\nof size M=5000 were designed to illustrate common validation problems.\nThey are summarized in Table <ref>. \n\n\nCase A features a quadratic model with additive heteroscedastic\nnoise\n\n    R_i    =a+X_i^2\n    \n    u_E_i    =b\u00d7|R_i|\n    \n    V_i    =R_i+u_E_i\u00d7 N(0,1)\n\nwhere N(0,1) is the standard normal distribution. The errors are\nthus in agreement with the generative model (Eq. <ref>),\nand, by construction, the set C={X,E,u_E} is calibrated,\nconsistent and adaptive. It should pass all the corresponding\ntests. \n\nTo generate a calibrated, non-consistent and non-adaptive dataset\n(Case B), one preserves all the data of Case A, except the uncertainties\nwhich are derived as perturbations of the mean uncertainty of Case\nA\n\n    u_E_i=\u221a(<u_E>_A)\u00d7 N(1,0.1)\n\nThis transformation preserves the calibration, but the errors are\nnow inconsistent with u_E and adaptivity is also lost. \n\nCase C is issued from Case A with a random ordering of u_E. This\npreserves average calibration by Eq. <ref>, but breaks\ncalibration by Eq. <ref>. Consistency and adaptivity\nare also broken.\n\nCase D is derived from Case A by a uniform scaling of u_E by\na factor two. This set should fail all the tests for calibration,\nconsistency and adaptivity. Note that there is not much sense to test\nconsistency and adaptivity when average calibration is not satisfied.\nThis will be considered as illustrative of the expected diagnostics. \n\nCase E is similar to Case A, but the errors have now a Student's-t_\u03bd=4\ndistribution with four degrees of freedom\n\n    V_i=R_i+u_E_i\u00d7 t_4(0,1)\n\nwhere t_4(0,1) is the t distribution scaled to have a unit\nvariance. This dataset is calibrated, consistent and adaptive. However,\ntests involving a normality hypothesis for the generative distribution\nshould fail.\n\nFinally, Case F is an homoscedastic dataset deriving from Case\nA, where the uncertainties are replaced by a constant value (the root\nmean square of Case A uncertainties). It is calibrated, and not adaptive.\nFor a homoscedastic dataset, consistency is identical to calibration.\n\n\n\n \u00a7.\u00a7 Testing average calibration\n\n\nThe simplest way to assess average calibration for variance-based\nUQ metrics is to test the equality in Eq. <ref> or in\nEq. <ref>. Various statistical procedures have been\nproposed, and it is best to avoid any assumption of normality for\nthe distribution of errors or z-scores. See Pernot<cit.>\nfor details.\n\nApplication to the synthetic datasets is presented in Table <ref>.\nFor Cases A, B, E and F, both tests are satisfied. Case D is obviously\nnon calibrated. However, datasets with consistency problems (Case\nC) might still check one of both metrics. \n\n\nFor interval-based UQ metrics, validation is done by testing Eq. <ref>\nfor the probability levels that are available. The statistical procedure\ncomes with the same caveats as for variance-based UQ metrics and has\nbeen detailed by Pernot<cit.>. When a large set of probability\nlevels is available, one can build a calibration curve, as shown next.\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Calibration curve\n\n\nIn a calibration curve<cit.>, one estimates\nthe probability of errors E to fall within a series of intervals\nwith coverage probability varying in [0,1], and one plots the resulting\nprobability against the target one (Eq. <ref>). Calibration\ncurves are applicable to homoscedastic datasets. \n\nThe method is also referred to as confidence- or intervals-based\ncalibration<cit.> and formalized\nas a variant of Eqns. <ref>-<ref>\n\n\n\n    lim_M\u2192\u221e\u03b7_p,M=p, \u2200 p\u2208[0,1]\n\nwhere\n\n    \u03b7_p,M=1/M\u2211_i=1^M1(E_i<q_p,i)\n\nand q_p,i is the quantile for probability p at point i.\n\nThe ideal calibration curve is the identity line (Fig. <ref>,\nCases A and F). For validation, a 95 % confidence interval is plotted<cit.>,\neither around the empirical curve or the identity line (the latter\nis preferred when multiple curves are drawn for the conditional case\npresented later). \n\nAs the synthetic datasets provide only a variance-based UQ metric\n(u_E), the quantiles in Eq. <ref> are calculated\nby assuming a normal generative distribution, as usually done in the\nliterature. In such cases, deviations from the identity line are not\nstraightforward to interpret, as they might have their origin in the\nnon-consistency of uncertainties (Cases B and C), their non-calibration\n(Case D) or a bad choice of reference distribution (Case E). \n\nIt is interesting to contrast these results with the statistics for\naverage calibration obtained above (Table <ref>).\nCase E has satisfying calibration statistics, but presents a slightly\nproblematic calibration curve. This is due to the inadequate choice\nof a normal generative distribution for this dataset based on a Student's\ndistribution. \n\nAlso, Cases B and C present notably deviant calibration curves despite\nthe fact that they have at least one correct calibration metric. The\ncalibration curve obviously contains more information than the average\ncalibration metrics, but it does not provide direct information on\nconsistency or adaptivity. For instance, without additional information,\nit is difficult to differentiate the diagnostics for Cases B and D.\nFor variance-based metrics, its use requires also an hypothesis on\nthe generative distribution to estimate the theoretical quantiles,\nwhich complicates the interpretation of deviant curves. \n\n\n\n \u00a7.\u00a7 Testing consistency\n\n\nThe validation literature for variance- or interval-based\nUQ metrics has mostly focused on consistency tests, and several methods\nare available, which are not fully equivalent. The aim of this section\nis to highlight the main features of these methods. \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 \u201cError vs. uncertainty\u201d plots\n\n\nPlotting the errors vs. the uncertainties can be very informative,\nnotably for datasets missing consistency <cit.>.\nAdditional plotting of guiding lines and running quantiles is a welcomed\ncomplement to facilitate the diagnostic.<cit.> This\nis obviously not applicable to homoscedastic datasets. \n\nExamples for the synthetic datasets are shown in Fig. <ref>.\nThe expected \u201ccorrect behavior\u201d is to have the running quantile\nlines for a 95% confidence interval to lie in the vicinity of and\nparallel to the E=\u00b12\u00d7 u_E lines. Although the generative\ndistribution is not necessarily normal, one does not expect very large\ndeviations from k=2, as seen for instance for the Student's distribution\nof Case E. Moreover, the cloud of points should be symmetrical around\nE=0 (absence of bias).\n\nThe plots enable to sort out Cases B, C and D without ambiguity. For\nCases B and C, the running quantiles are more or less horizontal,\nindicating an absence of link between the dispersion of E and u_E.\nFor Case D, the running quantiles follow closely the k=1 lines,\npointing to a probable overestimation of uncertainties. \n\nFor cases where consistency cannot be frankly rejected on the basis\nof the shape or scale of the data cloud, it is imperative to perform\nmore quantitative tests as presented below. One should not conclude\non good consistency simply based on this kind of plot.\n\n\n\n  \u00a7.\u00a7.\u00a7 Conditional calibration curves in uncertainty space\n\n\nIt is formally possible to construct conditional calibration\ncurves, but, to my knowledge, they have not been proposed in the\nML-UQ literature. \n\nExamples are given in Fig. <ref> using\nu_E as the conditioning variable with 15 equal-counts bins. In\nthe ideal case (Case A), all the conditional curves lie within the\nconfidence interval estimated for a sample size corresponding to a\nsingle bin. Case B shows that the anomaly observed on the average\ncalibration curve is identical at all uncertainty levels, pointing\nto a consistency problem or a wrong choice of generative distribution.\nCase C is interesting, as it shows that the small oscillations of\nthe average calibration curve around the identity line are attenuated\ncompared to the much larger deviations observed for conditional curves.\nThis points to the fact that a seemingly good calibration curve might\nhide a non-consistent case. The large dispersion of conditional curves\nis less likely to be due to a poor choice of generative distribution\n(unless the generative distribution varies strongly with u_E)\nand points to a lack of consistency. Cases D and E offer the same\ndiagnostic as for case B, with an ambiguity between non-consistency\nand generative distribution problem. Using a Student's-t_\u03bd=4\ngenerative distribution for Case E solves the problem. \n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Reliability diagram\n\n\nSo-called reliability diagrams<cit.>\ntest consistency through Eq. <ref> over data subsets,\nwhere the data are ordered by increasing u_E values and split\ninto bins. This method is therefore not usable for homoscedastic datasets.\nReliability diagrams can also be found in the literature as error-based\ncalibration plots<cit.> (not to be confounded with calibration\ncurves, Sect. <ref>), RvE plots<cit.>,\nor RMSE vs. RMV curves<cit.>.\n\nFor convenience, the square roots of the terms of Eq. <ref>\nare used, linking the mean uncertainty in each bin (as the root\nmean squared (RMS) of u_E, also called root mean variance\n(RMV)) with the RMSE or RMSD of E (both are equivalent for unbiased\nerrors). \n\nFor consistent datasets, the reliability curve should lie close to\nthe identity line, up to statistical fluctuations due to finite bin\ncounts. For a conclusive analysis of deviations from the identity\nline, the amplitude of these finite size effects should be estimated,\nfor instance by bootstrapping.<cit.>\n\nThe binning strategy is important<cit.>: some authors\nadvocate for bins with identical counts,<cit.>\nother for bins with identical widths<cit.>. Both choices\nare defensible according to the distribution of uncertainties, notably\nthe absence or presence of heavy tails. It should also be noted that\nthe insensitivity of Eq. <ref> to the pairing between\nE and u_E might still be a hindrance for large bins, but its\neffect should decrease when the bins get smaller. However, small bins\nare affected by large statistical fluctuations. The binning strategy\nshould thus be designed to offer a good compromise. An adaptive binning\nscheme mixing both strategies is proposed in Appendix <ref>.\n\nFor the synthetic datasets, I use a default choice of 15 equal-counts\nbins, leading to about 333 points per bin (Fig. <ref>).\nThe non-consistent cases (B, C, D) are correctly identified, although\nfor case B the deviation from the identity line is not noticeable\nexcept for the two extreme bins.\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Local Z-Variance analysis in uncertainty space\n\n\nLZV analysis was introduced by Pernot<cit.>\nas a method to test local calibration based on Eq. <ref>.\nAs for reliability diagrams, it is based on a binning of the data\naccording to increasing uncertainties. For each bin, one estimates\nVar(Z) and compares it to 1. Here again, error bars on\nthe statistic should be provided to account for finite bin counts.<cit.> \n\nAssuming a nearly constant uncertainty value within a bin, values\nof Var(Z) smaller/larger than 1 indicate over-/under-estimated\nuncertainties (by a factor \u221a(Var(Z))). This suggests\nan alternative plot of the inverse of the standard deviation\nof Z, Var(Z)^-1/2, instead of Var(Z),\nwhich provides a more direct reading and quantification of the local\nuncertainty deviation. This Local Z-scores Inverse Standard Deviation\n(LZISD) analysis is used throughout this study.\n\nApplication of the LZISD analysis to the synthetic datasets confirms\nthe diagnostics of non-tightness for cases B, C and D (Fig. <ref>).\nFor Case B, the deviation from the reference is more legible than\non the corresponding reliability diagram [Fig. <ref>(b)],\nand the largest local deviations can be directly quantified to about\n30 % in default and 60 % in excess (these extreme values are\nprobably underestimated, as they result from a bin averaging). \n\n\n\n  \u00a7.\u00a7.\u00a7 Local Coverage Probability analysis in uncertainty space\n\n\nConditional coverage with respect to an uncertainty metric\nis available through the Local Coverage Probability analysis.<cit.>\nIt can be directly built from prediction intervals if available, or\nestimated from prediction uncertainties and an hypothetical generative\ndistribution. In both cases, the empirical coverages are drawn for\na series of data subsets based on the binning of the conditioning\nvariable and for the available target probabilities. The LCP analysis\nof consistency is not applicable to homoscedastic datasets.\n\nFor the synthetic datasets (Fig. <ref>), one makes the\nhypothesis of a normal generative distribution to build prediction\nintervals from the uncertainties for \ud835\udcab={25,50,75,95}.\nCase A presents perfect conditional coverages over the full uncertainty\nrange and probability levels. For Case B, the empirical coverages\nare too large, except at the P=95 % level. The dependency along\nu_E is weak, except for the two extreme intervals, as was observed\non the LZISD analysis. A contrario, Case C presents a strong coverage\nvariation across u_E, with inadequate mean PICP values. For Case\nD, the overestimated uncertainties produce intervals with excess coverage,\nuniformly across the uncertainty range. Finally, Case E suffers again\nfrom the misidentification of the generative distribution, which is\nsolved by using the correct distribution to generate the intervals\n[Fig. <ref>(f)]. \n\n\nWhen a PICP value reaches 1.0, one gets no information about the mismatch\namplitude with the target probability. The Local Ranges Ratio analysis\n(LRR) has been proposed by Pernot<cit.> to solve this\nproblem. It estimates the ratio between the width of the empirical\ninterval and the width of the theoretical interval. This tool is not\nexploited in the present study, but its availability should be kept\nin mind, \n\n\n\n  \u00a7.\u00a7.\u00a7 Confidence curve\n\n\nA confidence curve is established by estimating an error\nstatistic S on subsets of C iteratively pruned from the points\nwith the largest uncertainties.<cit.> Technically, it\nis a ranking-based method, as (1) it is insensitive to the scale of\nthe uncertainties, and (2) the relative ranking of the errors and\nuncertainties plays a determinant role. It is not applicable to homoscedastic\ndatasets.\n\nIf one defines u_k as the largest uncertainty left after removing\nthe k % largest uncertainties from u_E (k\u2208{0,1,\u2026,99}),\na confidence statistic is defined by\n\n    c_S(k;E,u_E)=S(E | u_E<u_k)\n\nwhere S is an error statistic \u2013 typically the Mean\nAbsolute Error (MAE) or Root Mean Squared Error (RMSE) \u2013\nand S(E | u_E<u_k) denotes that only those errors\nE_i paired with uncertainties u_E_i smaller than u_k\nare selected to compute S. A confidence curve is obtained by plotting\nc_S against k. Both normalized and non-normalized confidence\ncurves are used in the literature, but the RMSE-based non-normalized\nversion should be preferred for the validation of variance-based UQ\nmetrics.<cit.> \n\nA monotonously decreasing confidence curve reveals a desirable association\nbetween the largest errors and the largest uncertainties, an essential\nfeature for active learning. It might also enable to detect\nunreliable predictions. But, in order to test consistency, one needs\nto compare c_S to a reference.\n\n\n\n  \n*Reference curves.\n\nAs already mentioned, only the order of u_E values\nis used to build a confidence curve, and any change of scale of u_E\nleaves c_S unchanged. Without a proper reference, c_S cannot\ninform us on calibration or consistency. \n\nAn oracle curve can be generated by reordering u_E to\nmatch the order of absolute errors. This can be expressed as\n\n    O(k;E)=c_s(k;E,|E|)\n\nIt is evident from the above equation that the oracle is independent\nof u_E and therefore useless for calibration testing. Recast\nin the probabilistic framework introduced above, the oracle would\ncorrespond to a very implausible error distribution D, such that\nE_i=\u00b1 u_E_i. Although it is offered as the default reference\ncurve in some validation libraries, I strongly recommend against its\nuse for variance- or interval-based UQ metrics. \n\nUsing Eq. <ref>, a probabilistic reference curve\nP can be generated by sampling pseudo-errors E_i\nfor each uncertainty u_E_i and calculating a confidence curve\nfor {\u1ebc,u_E}, i.e.,\n\n    P(k;u_E)=\u27e8 c_S(k;\u1ebc,u_E)\u27e9 _\u1ebc\n\nwhere a Monte Carlo average is taken over samples of\n\n    E_i\u223c D(0,u_E_i)\n\nThe sampling is repeated to have converged mean and confidence band\nat the 95 % level, typically 500 times. \n\nIn contrast to the oracle, which depends exclusively on the errors,\nthe probabilistic reference depends on u_E and a choice of distribution\nD, but it does not depend on the actual errors E. Comparison\nof the data confidence curve to P enables to test if E and u_E\nare correctly linked by the probabilistic model, Eq. <ref>,\ni.e. to test consistency. For RMSE-based confidence curves, P does\nnot depend on the choice of generative distribution (only the width\nof the confidence band does). This is not the case for MAE-based confidence\ncurves, which makes them more difficult to interpret.<cit.>\nGiven the choice of a generative distribution D, interval-based\nmetrics can be transformed to variance-based metrics and used to build\na confidence curve and probabilistic reference. \n\nOne can check in Fig. <ref> that consistency\nproblems are well identified by the comparison of a confidence curve\nto the associated probabilistic reference (Cases\u00a0B-D). When small\ndeviations are observed, it is however difficult to discriminate between\na genuine consistency problem and a bad choice of generative distribution\nfor P (Case\u00a0E). The choice of the appropriate distribution for\nCase\u00a0E leaves the confidence curve unchanged but widens the confidence\nband of the probabilistic reference, ensuring its validation. \n\n\n\n\n\n \u00a7.\u00a7 Testing adaptivity\n\n\nAdaptivity is not commonly tested in the ML-UQ literature,\nexcept maybe through a binary approach discerning in-distribution\nand out-of-distribution predictions.<cit.>\n\n\n\n  \u00a7.\u00a7.\u00a7 Z vs Input feature plot\n\n\nThe analog of the \u201cE vs u_E\u201d plot for consistency\nis to plot the z-scores Z as a function of a relevant input\nfeature X to check adaptivity. The distribution of Z should\nbe homogeneous along X and symmetric around Z=0. Here again,\none can use guide lines Z=\u00b12 and compare with running quantiles\nfor a 95% confidence interval of Z values. \n\nConditioning on X is shown in Fig.<ref> for\nthe synthetic datasets. Considering the shape of the data clouds,\nadaptivity can be clearly rejected in cases B and C, while the dispersion\nof the z-scores is insufficient in Case D. The other cases\nrequire a more quantitative validation. \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Conditional calibration curves in input feature space\n\n\nThe same principle as used for consistency testing (Sect. <ref>)\ncan be applied to adaptivity by grouping the data according to the\nbinning of a relevant input feature X. The method can now be applied\nto homoscedastic datasets.\n\nApplication to the synthetic datasets is presented in Fig. <ref>.\nFor Cases A and F, one would conclude to a good adaptivity along X\n. For the other cases, the deviations from the identity line are similar\nto those observed when testing consistency, except for Case B, where\nthe conditional calibration curves are much more dispersed. \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Local Z-Variance analysis in input feature space\n\n\nAs the LZV/LZISD analysis in u_E space was used to\nvalidate consistency, one can perform a LZV of LZISD analysis in X\nspace to validate adaptivity. Here again, the LZISD analysis offers\na more direct quantification of deviations. Both methods can be used\nfor homoscedastic uncertainties.\n\nThe diagnostics provided by the LZISD analysis on the synthetic datasets\nare non-ambiguous (Fig. <ref>): Cases B, C and D stand\nout as having strong deviations from the reference line, while Cases\nA, E and F present a good adaptivity, as observed on the \u201cZ vs\nX\u201d plots. \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Local Coverage Probability analysis in input feature space\n\n\nUsing the same binning for X as used in the other methods,\nthe LCP analysis (Fig. <ref>) provides results conform\nwith those of the conditional calibration curves (Fig. <ref>).\nHere also, the hypothesis of a normal generative distribution used\nto build the probability intervals penalizes Case E, which can be\nsolved by an appropriate choice of distribution (not shown). \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Using latent distances or V as conditioning variables\n\n\nAlthough using one or several input features X as conditioning\nvariable is the most direct way to test for adaptivity, it might not\nalways be practical, for instance when input features are strings,\ngraphs or images. In such cases, one might use instead a latent variable\nor the predicted property value V. \n\nX and V are interchangeable only if the latter is a monotonous\ntransformation of the former, but they might provide different adaptivity\ndiagnostics if this is not the case. Using V answers to the question:\nAre uncertainties reliable over the full range of predictions ? \n\nThe adaptivity analysis of the synthetic datasets using V as a\nconditioning variable is presented in Appendix <ref>.\nIt shows that the non-monotonous shape of the V=f(X) model might\ninduce artifacts in the adaptivity analysis that can be difficult\nto interpret if the functional form of the model is unknown or complex.\n\n\n\n \u00a7.\u00a7 Validation metrics\n\n\nThe present study focuses on graphical validation tools,\nbut validation metrics<cit.> are widely\nused in the ML-UQ literature. For instance, metrics have been designed\nfor calibration curves<cit.>, reliability\ndiagrams<cit.>, and confidence curves<cit.>.\nThese metrics are generally used to rank UQ methods, but they do not\nprovide a validation setup accounting for the statistical fluctuations\ndue to finite-sized datasets or bins. This is essential for small\nvalidation datasets and is not without interest for the conditional\nanalysis of large ML datasets. \n\nPernot<cit.> introduced confidence bands for calibration\ncurves that enables to identify significant deviations from the identity\nline, and also error bars for reliability diagrams, LZV and LCP analyses.\nThis provides a basis to define upper limits for calibration metrics\nthat can be used for validation purpose. In the present state of affairs,\ncalibration metrics such as the expected normalized calibration\nerror (ENCE)<cit.> or the\narea under the confidence-oracle (AUCO)<cit.> cannot\nbe used to validate consistency nor adaptivity and deserve further\nconsideration beyond the purpose of this article. \n\n\n\n\u00a7 APPLICATIONS\n\n\nThe validation methods introduced in the previous sections\nare now applied to \u201creal life\u201d data coming from the materials\nscience and physico-chemical ML-UQ literature. The choice of datasets\nenables to explore various aspects of a posteriori validation.\nIt is not my intent in these reanalyses to criticize the analyzes\nnor the UQ methods presented in the original studies, but simply to\nshow how the augmented set of validation methods proposed above might\nfacilitate and complete the calibration diagnostics.\n\n\n\n \u00a7.\u00a7 Case PAL2022\n\n\nThe data have been gathered from the supplementary information\nof a recent article by Palmer et al.<cit.>. I\nretained 8 sets of errors and uncertainties before and after calibration\nby a bootstrap method, resulting from the combination of two materials\ndatasets (Diffusion, M=2040 and Perovskites, M=3836) and several\nML methods (RF, LR, GPR....). The datasets are tagged by the combination\nof both elements: for instance, Diffusion_RF is the dataset resulting\nfrom the use of the Random Forest method on the Diffusion dataset.\nThe reader is referred to the original article for more details on\nthe methods and datasets. \n\nAs only the errors E and uncertainties u_E are available,\nit is not possible to test adaptivity for this dataset. My aim here\nis mainly to compare the performance of reliability diagrams and LZISD\nanalysis against the binning strategy, and to compare their results\nto confidence curves with the probabilistic reference. \n\n\n\n  \u00a7.\u00a7.\u00a7 Average calibration\n\n\nAs a first step, I estimated for each dataset the mean of\nz-scores and their variance to appreciate the average impact\nof the calibration method. Fig. <ref> summarizes\nthe results. It can be seen that calibration leaves the mean z-score\nunchanged or improves it (the calibrated value is closer to zero).\nAfter calibration, the mean z-scores are not always null, but small\nenough to consider the error sets to be unbiased. The calibration\neffect is more remarkable for Var(Z): all the 95 %\nconfidence intervals for Var(Z) after calibration overlap\nthe target value (1.0), except for Perovskite_LR with a very small\nresidual gap. \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Consistency\n\n\nConsistency can be checked by reliability diagrams, LZV\nanalysis and confidence curves with probabilistic reference. In absence\nof information on the generative distribution(s), it is best to avoid\nconditional calibration curves in this example. Before analyzing the\ndatasets, it is important to point out the impact of the binning strategy\non the conclusions drawn from reliability diagrams and LZV analysis.\nIn their study, Palmer et al. define 15 intervals forming\na regular grid in u_E space, with some adaptations described\nin Appendix <ref>. This is different from\nthe usual strategy consisting in designing bins with identical counts.<cit.>\nBesides, considering the size of the datasets, 15 bins might not be\nenough to reveal local consistency anomalies. The impact of the binning\nstrategy is explored in Appendix <ref>,\nwhere an adaptive binning strategy is designed to combine both approaches.\nWhen using the log-scale for uncertainty binning, this adaptive strategy\nis efficient to reveal local consistency anomalies that do not always\nappear in the original reliability diagrams.\n\n\n\n  \nCalibrated Random Forest.\n\nThe LZISD analysis and reliability diagrams are shown in Fig. <ref>\nfor both datasets. The effect of calibration is well visible on those\ngraphs when comparing the red dots (uncalibrated) to the blue ones\n(calibrated). For both datasets, consistency after calibration is\nrather good, except for the small uncertainty values (below 0.25 eV\nfor Diffusion and 0.15 eV for Perovskite). The legibility is better\non the LZISD analysis, which provides directly an overestimation value\nof 50 % of the uncertainties around 0.2 eV for Diffusion and\nup to 100 % around 0.08 eV for Perovskite. \n\n\nThe confidence curves in Fig. <ref> confirm this\ndiagnostic, as they lie close to the reference for large uncertainties\nand start deviate from it at smaller uncertainties. The non-linear\nscale of threshold values u_k at the top of the plot helps to\nlocate the anomalies in uncertainty space. \n\n\n\n  \nCalibrated Linear Ridge regression.\n\nCalibration and consistency of linear ridge regression results are\nassessed in Fig. <ref>. The effect of calibration\nis noticeable over the whole uncertainty range, except at large uncertainties\nfor the Diffusion dataset, where the situation is worsened by calibration.\nIn this area, the uncertainties are overestimated by a factor about\n2. At smaller uncertainties, the consistency is far from perfect,\nwith areas of underestimated uncertainties around 0.3 eV and 0.5 eV,\ncompensating for the overestimated values at large uncertainty. The\nconfidence curve displays the problem at large uncertainty values,\nbut is less legible at smaller uncertainties. \n\nFor the Perovskite dataset, consistency is not reached either, with\na compensation between overestimated and underestimated uncertainty\nareas. The confidence curve is notably deviating from the reference\ncurve.\n\n\n\n\n\n\n  \nCalibrated Gaussian Process Regression.\n\nOne considers here the calibration results of a Gaussian Process regression\nwith its Bayesian UQ estimate. It is possible to go quickly to the\nconclusion that consistency is not achieved. \n\nFor the Diffusion dataset, all three graphs (Fig. <ref>)\nconcur to reject consistency. The confidence curve even suggests that\nit would not be reasonable to base an active learning strategy on\nthese uncertainties. \n\nIn the case of Perovskite one observes a nugget of data with tiny\nerrors and largely overestimated uncertainties, that was identified\nby the adaptive binning strategy (Appendix <ref>).\nA zoom over the remaining data (not shown) reveals an heterogeneous\nsituations with small areas of underestimation (below 0.2 eV) and\noverestimation (around 0.3 eV and 0.6 eV). The confidence curve\ndeviates considerably from the reference and falls unexpectedly to\nzero at small uncertainties, a consequence of the aforementioned data\nnugget.\n\n\n\n\n\n\n  \nConclusion.\n\nThe reanalysis of these data shows that the reliability diagrams presented\nin the original article with a small number of bins and no error bars\nare often overoptimistic. Using an adaptive binning strategy in log-uncertainty,\nconditional calibration appears heterogeneous for all datasets, meaning\nthat consistency is not achieved despite a rather good average calibration. \n\nThe uncertainties are at worst within a factor two of their ideal\nvalue, a level that has to be contrasted with their intended application.\nThe confidence curves show that not all ML methods provide reliable\nuncertainties for active learning. \n\n\n\n \u00a7.\u00a7 Case BUS2022\n\n\nThe data presented in Busk et al.<cit.>\nhave been kindly collated by Jonas Busk for the present study. In\ntheir article, Busk et al. extend a message passing neural\nnetwork in order to predict properties of molecules and materials\nwith a calibrated probabilistic predictive distribution. An a posteriori\nisotonic regression on data unseen during the training is used to\nensure calibration. For UQ validation, these authors used the more\nor less standard trio of reliability diagrams, calibration curves\n(named quantile-calibration plot) and MAE-based confidence\ncurves with the oracle reference. Note that they duly express a reserve\nabout using the oracle: \u201cHowever, we do not expect a perfect\nranking...\u201d<cit.>. \n\nConsidering the validation results published for the QM9 dataset in\nFig. 2 of the original article, a few questions arise: (1) what\nis causing the imperfect calibration curve ?; (2) if this is a wrong\ndistribution hypothesis, how does it affect the MAE-based confidence\ncurve ?; (3) how is the confidence curve analysis modified if one\nuses the RMSE statistic and the probabilistic reference ?; and, (4)\nwhat is the diagnostic for adaptivity? The present reanalysis aims\nto answers these questions. \n\n\n\n  \u00a7.\u00a7.\u00a7 Reanalysis of the QM9 dataset\n\n\nThe QM9 dataset consists of 13 885 predicted (V,u_V),\nreference (R) atomization energies, and molecular formulas. These\ndata are transformed to C={ X,E,u_E}\naccording to Sect. <ref>, where X\nis the molecular mass generated from the formulas. One should thus\nbe able to test calibration, consistency and adaptivity. Average calibration\nis readily assessed (Var(Z)=0.96(2)).\n\nIt is always instructive to inspect the raw data through \u201cE vs\nu_E\u201d and \u201cZ vs X\u201d plots to get a global appreciation\nof the link between these quantities (Fig. <ref>).\nOne sees in Fig. <ref>(a) that the data points\nare neatly distributed between the E=\u00b13u_E guiding lines, and\nthe 0.025 and 0.975 running quantile lines seem to follow closely\nthe E=\u00b12u_E lines up to uE\u22430.05. Above this value,\nthe uncertainties seem overestimated and the errors are somehow biased\ntoward the positive values. However, this concerns a very small population\n(about 95 points) and filtering them out does not affect significantly\nthe calibration statistics presented below. The problem might be simply\ndue to the sparsity of the data in this uncertainty range. \n\n\nThe \u201cZ vs X\u201d plot in Fig. <ref>(b)\nusing molecular masses enables to check if calibration is homogeneous\nin mass space. The shape of the running quantile lines indicate that\nuncertainties are probably overestimated for masses smaller than the\nmain cluster around 125-130 Da, evolving to a slight underestimation\nabove this peak. Depending on its amplitude, this systematic effect\nmight be problematic, and it hints at a lack of adaptivity. A quantitative\nanalysis of this feature is presented below.\n\nIn a first step, calibration analyses are done to reproduce the Fig. 2\nof the original article: reliability diagram with 10 equal-counts\nbins [Fig. <ref>(a)], calibration curve [Fig. <ref>(b)]\nand MAE-based confidence curve (replacing the oracle by the probabilistic\nreference) [Fig. <ref>(c)]. Then, further analyses\nare performed to complement the information provided by the first\nset: LZISD analysis in uncertainty and V space [Fig. <ref>(d,e)],\nand RMSE-based confidence curve [Fig. <ref>(f)].\n\n\nThe reliability diagram [Fig. <ref>(a)] does not\nenable to see any major problem and would lead us to conclude that\nthe data are consistent. However, the calibration curve [Fig. <ref>(b)]\nis not perfect. Considering the good consistency provided by the reliability\ndiagram, one should conclude that there is probably a distribution\nproblem, i.e., the generative distribution of errors that is used\nto build the probabilistic reference curve should not be normal. Moreover,\nthe confidence curve [Fig. <ref>(c)] is not very\nclose to the probabilistic reference, which, assuming a good consistency,\npoints also to a problem of distribution, as the MAE-based probabilistic\nreference is sensitive to the choice of distribution. This distribution\nproblem is analyzed more specifically below (Sect. <ref>).\n\nA LZISD analysis with an adaptive binning strategy is performed to\ncompare with the original reliability diagram [Fig. <ref>(d)].\nIndeed, one observes some discrepancies: overestimation and underestimation\nof u_E go locally up to 15 %. There is notably an overestimation\ntrend around 0.01 eV. This feature is also present in the reliability\ndiagram, but it is more difficult to visualize small deviations on\na parity plot. \n\nThe LZISD analysis against the molecular mass [Fig. <ref>(e)]\nconfirms and quantifies what has been observed in [Fig. <ref>(b)]:\nan overestimation between 40 and 80 % at masses below 120 Da,\nand a slight underestimation by about 20 % above 130 Da (although\nthere are too few data in this range to conclude decisively). Uncertainties\nare therefore mostly reliable for the main mass peak between 120 and\n130 Da, but not outside of this range. This heterogeneity of calibration\nand lack of adaptivity is not observable in the other validation plots.\n\nFinally, a RMSE-based confidence curve is reported in Fig. <ref>(f).\nIt has been truncated at 0.015 eV, but drops sharply from 0.031 eV\nbecause of the small set of large positive errors corresponding to\nthe largest uncertainties [Fig. <ref>(a)].\nThis confidence curve shows a much better agreement with the probabilistic\nreference than the MAE-based one, albeit there is still a mismatch\ndue to the consistency defects observed in the LZISD analysis and/or\nto the distribution problem mentioned above.\n\n\n\n  \u00a7.\u00a7.\u00a7 Non-normality of the generative probabilistic model\n\n\nThe deviations of the calibration curve observed in Fig. <ref>(b)\ncorrespond neatly to observations made for one of the synthetic datasets\n(Case\u00a0E) when drawing error from a Student's-t distribution\nwith four degrees of freedom and comparing the percentiles with a\nnormal reference [Fig. <ref>(e)]. Indeed,\nif one substitutes the normal reference in the calibration plot by\na t_\u03bd=4 reference, one obtains a perfect calibration curve\n[Fig. <ref>(a)]. \n\nSimilarly, assuming a t_\u03bd=4 generative distribution for the\nprobabilistic reference of confidence curves improves considerably\nthe agreement with the data curve [Fig. <ref>(b,c)],\nboth for the MAE- and RMSE-based approaches. Note the widening of\nthe confidence area of the probabilistic reference in both cases.\nThere remains small deviations indicating that consistency is not\nperfect. \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Conclusions\n\n\nAll diagnostics based on E and u_E conclude to a\ngood calibration and consistency, notably if one accounts for a non-normal\ngenerative model (t_\u03bd=4). The main feature revealed by this\nreanalysis is the lack of adaptivity seen in the LZISD analysis in\nmolecular mass space. It leads to a significant underestimation of\nthe quality of predictions for the smaller molecules in the QM9 dataset.\nBy contrast, the confidence curves indicate that the uncertainties\ncan be used for active learning without a second thought. \n\n\n\n \u00a7.\u00a7 Case HU2022\n\n\nThese data were kindly collated and provided by Y. Hu to\nfacilitate this reanalysis of the results presented in Figs. 3e and\n3f of the paper by Hu et al.<cit.>.\nThese figures present plots of the errors on interatomic potentials\nfor the QM9 dataset vs two distances: a distance in feature space,\nX_F and a distance in latent space<cit.>, X_L.\nBoth distances have been calibrated to interval-based UQ metrics:\nthe half-ranges of prediction intervals, U_F,P and U_L,P\nhave been obtained for P\u2208{68,95}, by multiplication of the\ncorresponding distances by probability-specific scaling factors optimized\nby conformal calibration to ensure average coverage.\nThe percentage of points within the resulting intervals are reported\nin the original figures and show a successful average calibration\nin both cases. However, the distribution of points in these figures\nhints at different conditional calibration statistics for both metrics,\nwhich I propose to quantify in this reanalysis. The reader is referred\nto the original article for details on the dataset and ML-UQ methods.\n\n \n\nA plot of the errors vs the expanded uncertainties at the 95 %\nlevel [Fig. <ref>(a,d)] confirms the features\nobserved in the original figures: in this plot, the running quantiles\nshould follow the y=\u00b1 x guide lines, which is not the case for\nU_F,95, while this seems to be much better for U_L,95, at\nleast below 0.2 eV/sys.\n\nTo quantify this difference, the data are then analyzed with the LCP\nanalysis in feature distance space [Fig. <ref>(b,e)].\nAs the data are conditioned over the feature distance, one is testing\nthe adaptivity of the conformal calibration for individual predictions.\nNote that for U_F,95 the distances and uncertainties are proportional,\nand the adaptivity diagnostic is also a consistency diagnostic! A\nsatisfying average coverage is reached in all cases, in agreement\nwith the conclusions of the original article. However, the local coverage\nof U_L,P is more homogeneous than for U_F,P at both probability\nlevels. The latter presents strong under- and over-coverage areas.\nThe adaptivity of U_L,P is not far from perfect at the 95 %\nlevel, except for the extreme bins, but is not good at the 68 %\nlevel. \n\nIt is also worthwhile to look at confidence curves[Fig. <ref>(c,f)].\nThe original article presents calibration curves which are very close\nto the identity line for both metrics. One can thus confidently transform\nthe expanded uncertainties at the 95 % level to standard uncertainties\nusing a normal distribution hypothesis. The confidence curves display\na much better behavior for U_L than for U_F. For active\nlearning, uncertainty built on latent distance looks therefore much\nmore reliable than built on feature distance. However, consistency\nis not perfect for U_L. \n\nA simple scaling of the distance metrics is thus unable to ensure\nconsistency/adaptivity. A better calibration would require to use\nconditional conformal calibration, which is reputed for its\ndifficulty,<cit.> and, as far\nas I know, has not yet been implemented in the ML-UQ context considered\nhere. \n\n\n\n\u00a7 CONCLUSIONS\n\n\nThis article introduces a principled framework for the validation\nof UQ metrics. The studied examples target mostly machine learning\nin a materials science and chemistry context, but the applicability\nis general. The concept of conditional calibration enables to define\ntwo aspects of local calibration: consistency, which assesses the\nreliability of UQ metrics across the range of uncertainty values,\nand adaptivity, which assesses the reliability of UQ metrics across\nthe range of relevant input features. \n\nThe more or less standard UQ validation methods (calibration curves,\nreliability diagrams and confidence curves) were recast in the proposed\nframework, showing that they are not designed to test adaptivity.\nIn consequence, adaptivity is presently a blind spot in the ML-UQ\nvalidation practice. It was shown on a few examples that consistency\nand adaptivity are scarcely reached by modern ML-UQ methods, and that\na positive consistency diagnostic does not augur of a positive adaptivity\ndiagnostic. Both aspects of local calibration should be tested. \n\n\n\nLet us summarize the main points arising from this study.\n\n\n\n  \nConsistency testing.\n\nBeside the exploratory \u201cE vs u_E\u201d plots, three\nvalidation methods have some pertinence to test consistency of variance-based\nUQ metrics:\n\n\n  * Error vs Uncertainty plots offer a very informative preliminary analysis,\ndevoid of any artifacts due to a binning strategy or the choice of\na generative distribution. However, they do not enable to conclude\npositively on consistency, which requires more quantitative diagnostics.\n\n  * Reliability diagrams and LZV/LZISD analysis are direct implementation\nof the conditional calibration tests. It was shown that the LZV/LZISD\nanalysis offers a visually more discriminant approach, along with\na direct quantification of possible uncertainty misestimations. Both\nmethods are sensitive to the choice of a binning scheme, and an adaptive\nbinning scheme is proposed to compensate for the drawbacks of equal-counts\nor equal-width binning strategies.\n\n  * The RMSE-based confidence curve is not designed to check consistency,\nbut, augmented with a probabilistic reference, it offers an interesting\napproach, combining two diagnostics: the eligibility of the UQ method\nfor a reliable active learning and the consistency of the dataset.\nMoreover, it is less dependent on a binning strategy than the reliability\ndiagrams or LZV analysis. An inconvenience is the dependence of the\nstatistical validation reference curve (more precisely its confidence\nband) on the choice of a generative distribution.\n\nI have also shown that conditional calibration curves could\nbe considered to assess consistency. As for confidence curves, their\ndisadvantage for variance-based UQ metrics is the difficulty to discern\nbetween a lack of consistency or a bad choice of generative distribution\nhypothesis. \n\nI would then recommend the Error vs Uncertainty plot, the LZISD analysis\nand the RMSE-based confidence curve with its probabilistic reference\nas complementary diagnostics for the consistency of variance-based\nUQ metrics.\n\nFor interval-based UQ metrics the LCP analysis seems to be the best\nchoice, as it implements directly the conditional calibration test.\nCalibration curves have been shown to have possibly ambiguous diagnostics,\nwhich can be improved by using conditional calibration curves. In\norder to avoid undue hypotheses about error generative distribution,\ninterval-based validation methods should be reserved to interval-based\nUQ metrics.\n\n\n\n  \nAdaptivity testing.\n\nAdaptation of the standard methods to test conditional calibration\nover some input feature is not ideal but feasible, and one could envision\nconditional reliability diagrams, conditional calibration curves,\nand even conditional confidence curves. This approach requires large\nvalidation datasets and leads to complex graphs that are not easily\nlegible. In addition to the Z vs Input feature plots, I would then\nrecommend the LZISD analysis and the LCP analysis as general tools\nto test adaptivity of variance- and interval-based UQ metrics, respectively. \n\nIt was also noted that using the predicted values V as a conditioning\nvariable to test adaptivity is not without risks, as it might produce\nspurious features that complicate the diagnostic. This should probably\nbe generalized to any property that is susceptible to be strongly\ncorrelated with the errors. Further studies are required to clear\nthis up.\n\n\n\n\u00a7 ACKNOWLEDGMENTS\n\n\nI warmly thank J. Busk for providing me the data for the\nBUS2022 case, and Y. Hu and A. J. Medford for the data in the HU2022\ncase.\n\n\n\n\u00a7 AUTHOR DECLARATIONS\n\n\n\n\n \u00a7.\u00a7 Conflict of Interest\n\n\nThe authors have no conflicts to disclose.\n\n\n\n\u00a7 CODE AND DATA AVAILABILITY\n\n\nThe code and data to reproduce the results of this article\nare available at <https://github.com/ppernot/2023_Primer/releases/tag/v1.0>\nand at Zenodo (<https://doi.org/10.5281/zenodo.6793828>). The\n,<cit.> https://github.com/ppernot/ErrViewLibErrViewLib\npackage implements the validation functions used in the present study,\nunder version  (<https://github.com/ppernot/ErrViewLib/releases/tag/v1.7.0>),\nalso available at Zenodo (<https://doi.org/10.5281/zenodo.7729100>).\nThe  graphical interface to explore the main UQ validation\nmethods provided by  is also freely available (<https://github.com/ppernot/UncVal>).\n\nunsrturlPP\n\n\n\n\n\n\n\n\u00a7 APPENDICES\n\n\n\n\n\n\n\u00a7 USING THE PREDICTED VALUE TO TEST ADAPTIVITY\n\n\nA posteriori validation of adaptivity on datasets that do not contain\ninput features X might still be attempted by using the predicted\nvalues V as proxy. Conditional calibration on V is not necessarily\nidentical to conditional calibration on X, but it might still provide\nuseful diagnostics.\n\nApplication of \u201cZ vs V\u201d plots to the synthetic datasets\nis presented in Fig.<ref>(a-f). Z-score values in\nCase A are globally well distributed (horizontal running quantiles\naround \u00b12, but present an anomaly at small V values. This\ncan be traced back to the correlation of V with E (E=R-V),\nmost visible when R is nearly constant at the bottom of the parabolic\nmodel used to generate the data. This warns us that conditioning on\nV might produce some aberrant features, which are not diagnostic\nof poor adaptivity. The same artifact can be observed in Cases D,\nE and F. For Case E, the excessive dispersion of points, with |Z|\nvalues above 5, points to a non-normal distribution, but this should\nnot be a valid reason to reject adaptivity: a more quantitative analysis\nis mandatory. By contrast, the absence of adaptivity is readily visible\nfor cases B, C, and D : cases B and C present heterogeneous distributions\nalong V, while Case D has Z values mostly contained between\n-1 and 1, pointing to an homogeneous overestimation of uncertainties.\nCase F is similar to Case A. \n\n\nThe translation of these observations to the LZISD analysis\nis presented in Fig. <ref>(g-l). The artifact\nappears as local deviations of the LZISD values, while the intrinsic\nnon-adaptivity of Cases B and C is more global. Similar observations\ncan be made on the conditional calibration curves and LCP analyses\n(not shown). \n\nThis shows that using V as a substitute for X to test adaptivity\nmight lead to ambiguous conclusions when local anomalies are observed.\nHowever, global anomalies are certainly diagnostic of a lack of adaptivity\nof the tested UQ metric.\n\n\n\n\n\n\u00a7 IMPACT OF THE BINNING STRATEGY\n\n\nTo implement the LZV analysis, the default binning strategy\nis based on intervals with (nearly) equal populations.<cit.>\nThis ensures equal testing power in all intervals, but might result\nin intervals with very different ranges. This is also often the strategy\nchosen for reliability diagrams.<cit.> However, some\nauthors prefer to use bins with similar widths,<cit.>\na strategy which faces two problems: bins with insufficient population\nto derive reliable confidence intervals, and bins containing a large\npart of the total sample for uncertainty distributions with large\ntails. \n\nThese problems were addressed by Palmer et al.<cit.>\nby labeling the bins with less than 30 points as unreliable and by\naltering the width of bins in the peak area to ensure that the lowest\n90% of the values were spread across at least five bins (assuming\nthat uncertainty distributions can only have a heavy upper\ntail). The use of this strategy leads to very regular reliability\ndiagrams with deviant points mostly in the upper range of uncertainty. \n\nI present here an alternative binning strategy, considering that for\na positive variable such as uncertainty, a log-transform might be\nan efficient way to reduce the skewness of the distribution causing\nthe accumulation problem in a few bins. Starting from a regular grid\nin log space (n bins), the problem of small counts is solved by\nmerging adjacent bins with insufficient populations, while the problem\nof excessively large counts is solved by splitting bins having more\nthan M/n points (M is the dataset size). Both operations are\niterated until the bins population conforms with the chosen limits\n(the lower limit is set to 30). \n\nTwo examples below, taken from Palmer et al.<cit.>,\nshow that this adaptive strategy is more efficient to reveal tightness\nproblems. The Diffusion/LR and Perovskite/GPR_Bayesian datasets were\nanalyzed by reliability diagrams and LZISD analysis for increasing\nnumber of bins (10, 20 and 40) for the equal-counts bins and adaptive\nstrategies. The reliability diagrams have also to be compared with\nthose of the original article, based on 15 equal-width bins.\n\nLet us consider first the Diffusion/LR case. For both types of analysis,\nthe counts-based binning [Fig. <ref>(a-f)]\nneeds at least 20 bins to reveal an overestimation problem for the\nlargest uncertainties, with a stronger effect for 40 bins. The averaging\neffect in large bins (n=10) would lend us to believe that calibration\nis good in this area, whereas using four times smaller bins reveals\nuncertainty in excess by a factor two around 2 eV. \n\nBy using the new adaptive strategy [Fig. <ref>(g-l)],\nthe problem is apparent for all the specified number of bins. In fact,\nfor a starting point of 10 bins, the merge/split strategy converges\nto 20 bins, for 20 bins, one gets 30 and for 40 bins 47. The first\ncase provides a good binning without the useless details that arise\nfrom the higher bin numbers. \n\nFor this dataset, one clearly has tightness problems with under- and\nover-estimations for uncertainties larger than 0.5. These defects\ncompensate and are not detected when using large bins. This problem\nis also apparent in the original article [Figure 2(d)], albeit\nfor a series of bins with populations below the 30 limit. Aggregating\nthese bins enables to conclude that the effect is statistically significant.\n\n\nThe Perovskite/GPR_Bayesian case is shown in Fig. <ref>.\nAs in the previous example, the counts-based binning needs more than\n20 bins to display a problem at small uncertainties, in both LZV and\nRD analyses [Fig. <ref>(a-f)]. The\nLZV analysis indicates a strong overestimation of the uncertainties\naround 0.01 eV, the reliability diagram showing that the corresponding\nerrors have a standard deviation of about 1E-8, which would certainly\ndeserve a closer inspection. The adaptive analysis needs 20 initial\nbins (30 after adaptation) to reveal the problem [Fig. <ref>(g-l)].\nThis \u201cdata nugget\u201d is undetected by the linear equal-width binning\nof the original study (see Figure 38 of the https://static-content.springer.com/esm/art%3A10.1038%2Fs41524-022-00794-8/MediaObjects/41524_2022_794_MOESM1_ESM.pdfSupplementary Information\nto Palmer et al.<cit.>). \n\n \n\nThe adaptive binning strategy in logarithmic uncertainty seems to\nbe able to detect consistency problems more efficiently than binning\nin linear uncertainty, either with bins of same counts or same width.\nOf course, it does not apply to variables with negative or null values,\nwhere it can be replaced by an adaptive strategy in linear space.\nThe adaptive binning strategy is used for the LZISD, LCP and reliability\ndiagrams in the case studies of the present article with a starting\npoint of n=20 bins.\n"}