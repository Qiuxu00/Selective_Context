{"entry_id": "http://arxiv.org/abs/2303.06907v1", "published": "20230313074846", "title": "ST360IQ: No-Reference Omnidirectional Image Quality Assessment with Spherical Vision Transformers", "authors": ["Nafiseh Jabbari Tofighi", "Mohamed Hedi Elfkir", "Nevrez Imamoglu", "Cagri Ozcinar", "Erkut Erdem", "Aykut Erdem"], "primary_category": "cs.CV", "categories": ["cs.CV", "eess.IV"], "text": "\n\nEffects of Nb Doping on the Charge-Density Wave and Electronic Correlations in the Kagome Metal Cs(V_1-xNb_x)_3Sb_5\n    Hai-Hu\u00a0Wen\n    March 30, 2023\n===================================================================================================================\n\n\n\n\n\n\n\n\n\n\n\nOmnidirectional images, aka 360^\u2218 images, can deliver immersive and interactive visual experiences. As their popularity has increased dramatically in recent years, evaluating the quality of 360^\u2218 images has become a problem of interest since it provides insights for capturing, transmitting, and consuming this new media. However, directly adapting quality assessment methods proposed for standard natural images for omnidirectional data poses certain challenges. These models need to deal with very high-resolution data and implicit distortions due to the spherical form of the images. In this study, we present a  method for no-reference 360^\u2218 image quality assessment. Our proposed ST360IQ model extracts tangent viewports from the salient parts of the input omnidirectional image and employs a vision-transformers based module processing saliency selective patches/tokens that estimates a quality score from each viewport. Then, it aggregates these scores to give a final quality score. Our experiments on two benchmark datasets, namely OIQA and CVIQ datasets, demonstrate that as compared to the state-of-the-art, our approach predicts the quality of an omnidirectional image correlated with the human-perceived image quality. The code has been available on\n<https://github.com/Nafiseh-Tofighi/ST360IQ>\n\n\n\nVision Transformers, 360\u00b0 image quality assessment\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nVirtual reality (VR) data has recently occupied an increasing share of multimedia data as the corresponding hardware, including Head Mounted Displays (HMD) and capture devices, has become more widely accessible to the end-users.\n\nTherefore, developing methods for evaluating the quality of omnidirectional images has also gained much attention from the researchers working in this area.\n\nImage quality assessment (IQA) has been thoroughly studied in the past twenty years\u00a0<cit.>. Generally speaking, IQA algorithms can be classified into full-reference IQA (FR IQA), reduced-reference IQA (RR IQA), and no-reference IQA (NR IQA)\u00a0<cit.>. FR IQA and RR IQA models need full and part reference image information, respectively,  while NR IQA takes only the distorted image as input. In that sense, NR IQA measures subjective, perceptual quality of images.\n\nWith the recent advances in deep learning, deep methods started to dominate the IQA literature. While the early examples employ convolutionals neural networks (CNNs) for predicting the image quality\u00a0<cit.>, the more recent work considers self-attention and vision-transformers (ViT)\u00a0<cit.> based architectures\u00a0<cit.>. In general, the quality assessment literature for 360^\u2218 images follows the IQA literature as most of the existing work either suggest to directly use the same IQA methods proposed for natural images, or make small modification to those methods to deal with the spherical form\u00a0<cit.>. Only very recently, CNN-based models specific to omnidirection images haves been proposed\u00a0<cit.>.\n\n\n\n\n\n\n\nIn our work, we propose a ViT-based framework for omnidirectional image quality assessment called ST360IQ. Up to our knowledge, there are no prior work that explores ViTs for evaluating the perceptual quality of 360^\u2218 images. Our framework estimates the overall image quality through a number of tangent viewports focusing on different parts of the spherical image. In particular, motivated by the relation between visual salience and image quality\u00a0<cit.>, we sample these viewports from the salient parts of the omnidirectional data. We process each viewport independently through a spherical ViT-based module, and extract individual quality scores, which are then aggregated to obtain the final quality score. Using tangent viewports instead of the commonly used projections such as equirectangular projection (ERP) or cube-mapping allows us to deal with  distorted visual data due to the spherical form more effectively. Moreover, our ViT-based formulation specifically designed for the omnidirectional data domain results in more accurate predictions than the state-of-the-art. \n\n\n\n\n \n\n\n\n\n\u00a7 PROPOSED METHOD\n\n\nTo address the challenge of learning to assess the quality of omnidirectional images, we propose a ViT-based method that utilizes a saliency-driven sampling technique to better capture important parts of panoramic content. Figure 1 shows a general overview of our model. In the saliency module, we first predict visually attractive parts of the panoramic image via an off-the-shelf visual saliency prediction model. Our sampling module then extracts tangent viewports from salient regions of the distorted image in ERP format. The main VQA module resembles the one used in <cit.>, but the patch encoding block is re-modeled from the ground up to deal with the special structure of 360^\u2218 images. \n To effectively encode input viewports into a sequence of tokens, we incorporate positional, geometric and source embeddings into the extracted sequence of tokens, and add a learnable classification token (CLS) to capture the global representation for the image. The corresponding 360\u00b0 image quality score is predicted by the output of a fully connected layer on top of the final CLS token representation at the output of the Transformer encoder.\n\nSampling Module. Most 360^\u2218 IQA datasets store images in the ERP format, which is the most popular spherical image representation, but is known to have significant distortions. To improve the model performance in 360^\u2218 IQA task, we utilize tangent image representation <cit.> to reduce the distortion of the viewports extracted from the ERP image. Moreover, to distinguish visually important parts of a panoramic image, we designed a sampling strategy motivated by the human visual attention mechanism. In particular, we employ the 360^\u2218 image saliency prediction model named ATSal <cit.> to predict salient regions of the panorama. In this way, one can assign a saliency score for each patch extracted from the omnidirectional image, which we utilize in our sampling module.\n\nOur main motivation behind our saliency-guided sampling scheme is to combine neural attention (self-attention) mechanism with human visual attention in a simple and intuitive manner. At the first step of the sampling module, an input image in ERP format is fed to the ATSal saliency prediction model to predict a saliency map showing which regions are most likely to attract attention. The mean shift algorithm is then applied to the extracted saliency map to better highlight the salient regions. Finally, we randomly sample\u00a010% of regions wrt the mean saliency scores computed within each overlapping region extracted with a stride value of\u00a0S.  \n\nIn this work, we utilize tangent image representation for 360^\u2218 images, which divide the sphere into several nearly Euclidean zones. Using the center points of each sampled salient region (\u03a6_n, \u03b8_n), we construct a tangent image, which is free from the distortions observed in ERP images. In addition, this enables the model to gain more effective information from the same viewport size thanks to its wider field of view. As our model processes each tangent image separately during training, the quality scores of these tangent images are set to the ground truth quality score assigned to the whole image.\n\n\n\nPatch Encoder and Model Embeddings. After the sampling module, it is necessary to generate transformer input patches from selected viewports. \nInstead of the linear projection as used in the original ViT model\u00a0<cit.>, we utilize ResNet-50 convolutional layers, <cit.> is used for encoding the patches. \nThe embedding information is added at the top of the output of the patch encoder part. \nPosition embeddings are added to the patches to retain positional information, such as standard vision transformer architecture, through each image patch. \nThe model keeps the information about each viewport extracted from which region of the original input image with ERP as a consequence of Geometric embedding.\nThis enables the model to learn better with the help of neighbor viewports correlation information. \nTo this aim, the central data of each tangent viewport (\u03a6_n, \u03b8_n) after normalizing are added to the patches as geometric embedding.\nLast but not least, source embedding is employed since each viewport is fed into the transformer encoder independently, and with this feature, the network has the opportunity of predicting even with a single or limited number of viewports. To achieve this, all tangent viewports extracted from a single ERP input image gain the same source embedding corresponding to their ERP index. This helps the model mark each tangent image comes from which input source. As a result of this architecture, the average score of all tangent images extracted from a single ERP input is reported as the final prediction score for each image.\n\nModel Training and Implementation Details. Input tokens of the vanilla Transformer used in this work have dimensions of D = 384, and we employ a patch size of P = 32. To make the model size equivalent to ResNet-50, we utilize a classic Transformer with six heads, 14 layers, 384 hidden sizes, and 1152 MLP size. For sampling, each input image is divided into regions by stride 16, and then 10% of those salient regions are selected, which gives empirically the best results. The training pipeline uses the mean absolute error (MAE) as the loss function.\n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n\nDatasets. We evaluate the performance of our method using  the commonly used CVIQ\u00a0<cit.> and OIQA\u00a0<cit.> datasets. CVIQ dataset contains 528 compressed images that are obtained by applying three widely used coding standards (JPEG, H.264/AVC, and H.265/HEVC) on 16 lossless images. OIQA dataset respectively consists of 16 raw, and 320 distorted 360^\u2218 images that are obtained from the raw images by applying four common distortion types (Gaussian blur, Gaussian noise, JPEG compression, and JPEG2000 compression) from five different levels. Following\u00a0<cit.>, for our analysis, we randomly decompose the aforementioned datasets into train (\u223c80%) and (\u223c20%) test splits along the lossless/distortion-free image dimension. This results in 13 training and 3 test images for both datasets. By this way, we guarantee that the test images, apart from whether they are distorted or distortion-free, have not been seen during training. \n\n\n\n\n\n\n\nPerformance Comparison. We evaluate the performance of our model with three widely used metrics, namely Spearman's Rank Correlation Coefficient (SRCC), Pearson Correlation Coefficient (PLCC), and Root Mean Squared Error (RMSE). As commonly done, before computing PLCC and RMSE, we apply a five-parameter logistic function to the model's prediction. In Table\u00a0<ref>-<ref>, we present the results of our model along with some traditional and learning-based metrics on the OIQA and CVIQ datasets. Most of these metrics are FR metrics and require an additional reference image. The remaining ones are NR metrics, within which MUSIQ\u00a0<cit.> is the ViT-based model that we build our model on top of, and MC360IQA\u00a0<cit.> and VGCN\u00a0<cit.> are the NR models specifically designed for omnidirectional images. \n\nOur proposed ST360IQ model, in general, outperforms the state of the art models in predicting 360^\u2218 image quality on all tested images. Training and applying MUSIQ on the ERP images give worse results than most of the NR metrics including ours as ERP  images suffers from additional spherical distortions, showing processing omnidirectional images needs special care, as we did in our proposed framework. Moreover, it is important to emphasize that for achieving these performances, our model does not require any pre-training involving IQA datasets that contain natural images. On the other hand, the highly competitive VGCN\u00a0<cit.> does, which can be also partly seen in Table\u00a0<ref> in that its performance decreases without proper pretraining (cf. VGCN* and VGCN+).\n\nEffect of Sampling Module. The performance of ST360IQ strongly relies on the utilized sampling strategies. We sample tangent viewports from a given spherical 360^\u2218 image, and estimate its quality score from these viewports, instead of processing the whole image. Moreover, motivated by the human attention mechanism, we consider a saliency-guided sampling scheme to select the tangent viewports. We evaluate the contribution of these strategies in Table\u00a0<ref>. First, we replace the tangent images with the image regions directly cropped from the salient parts of the corresponding ERP images. This leads to a significant performance loss, validating our claim that inherent distortions in ERP images have a negative influence on the quality predictions. Second, instead of selecting salient viewports, we perform random sampling over the spherical image and extract viewports accordingly regardless of their salience. Again, we observe that such kind of strategy gives poor performance as compared to our full model. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffect of geometric and source embeddings. We assess the effect of using source and geometric embeddings on ST360IQ's performance by excluding them from the patch encodings. The results given in Table\u00a0<ref> shows that adding geometric embeddings introduces significant performance gains. We conjecture that it allows the network to learn viewport-specific biases. Similarly, including source embeddings, in a way, let the model be aware of image-specific characteristics during training over the extracted salient viewports, further improving the prediction accuracies.\n\n\n\n\u00a7 CONCLUSION\n\nWe propose a spherical-ViT based no-reference omnidirectional IQA method called ST360IQ, which predicts the quality score of a 360^\u2218 image by  processing the image by extracting the most salient viewports, and aggregating the local quality scores estimated from them. Using the ViT-architecture allows us to better model the geometry of the spherical structure and the viewport biases. The effectiveness of the suggested approach is demonstrated by the experiments on two common 360^\u2218 IQA datasets, which reveal that our model consistently attains the state-of-the-art performance.\n\n\n\n\n\u00a7 ACKNOWLEDGMENTS\n\nThis work was supported in part by KUIS AI Center Research Award, TUBITAK-1001 Program Award No. 120E501, GEBIP 2018 Award of the Turkish Academy of Sciences to E. Erdem, and BAGEP 2021 Award of the Science Academy to A. Erdem.\n\nIEEEbib\n\n\n\n\n"}