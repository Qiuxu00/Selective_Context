{"entry_id": "http://arxiv.org/abs/2303.07872v1", "published": "20230313030759", "title": "Object-based SLAM utilizing unambiguous pose parameters considering general symmetry types", "authors": ["Taekbeom Lee", "Youngseok Jang", "H. Jin Kim"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n[\n    [\n    March 30, 2023\n==================\n\nempty\nempty\n\n\n\n\n\nExistence of symmetric objects, whose observation at different viewpoints can be identical, can deteriorate the performance of  simultaneous localization and mapping (SLAM). This work proposes a system for robustly optimizing the pose of cameras and objects even in the presence of symmetric objects. We classify objects into three categories depending on their symmetry characteristics, which is efficient and effective in that it allows to deal with general objects and the objects in the same category can be associated with the same type of ambiguity. Then we extract only the unambiguous parameters corresponding to each category and use them in data association and joint optimization of the camera and object pose. The proposed approach provides significant robustness to the SLAM performance by removing the ambiguous parameters and utilizing as much useful geometric information as possible. Comparison with baseline algorithms confirms the superior performance of the proposed system in terms of object tracking and pose estimation, even in challenging scenarios where the baseline fails. \n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nSimultaneous localization and mapping (SLAM), one of the core technologies for autonomous driving, is a technology that reconstructs the environment around the robot and estimates the robot position in the reconstructed map. \nDespite a significant progress in precise localization using geometrical information of the surrounding environment, it is still difficult for a SLAM system to achieve advanced tasks based on human interaction and scene understanding. To overcome the problem, semantic SLAM that uses semantic information in SLAM has been in using deep learning techniques such as recently developed instance segmentation <cit.>, <cit.>, <cit.> and 3d object detection <cit.>, <cit.>, <cit.>. \n\nObject-based SLAM <cit.>, <cit.> is a branch of semantic SLAM that reconstructs an object-based map and provides high-level information. Object-based SLAM simultaneously estimates the location of semantic object inferred from the network while performing feature-based localization used in existing SLAM systems, and enables to estimate camera poses even in featureless environments. In addition, a more expressive map can be constructed by expressing the pose, type, and shape of the semantic objects in the map.\n\nHowever, if the shape of the object itself is symmetric or if the observed shape is symmetric due to occlusion, object-based SLAM can suffer from significant error in the estimation process of ego motion and pose of the object. Since symmetric objects may have the same observation at different viewpoints, data association or motion estimation may fail.\n\n\n\nWe propose a system for robustly optimizing the pose of cameras and objects even in the presence of symmetric objects. The 3d detection module <cit.> is modified to obtain a local observation that predicts multiple poses for each object, similar to <cit.>. We classify object types into asymmetry, discrete, and continuous symmetry based on the local observation of the observed objects. \nBy extracting a  parameter corresponding to the symmetry type from the pose of object, only the robust parameter is used as a constraint to jointly optimize  the camera pose and object pose.\nTherefore, the proposed SLAM system has the advantage of robustness by using as much useful geometric information as possible even when symmetric objects are observed, while directly using 3d detection networks with many existing research and dataset. In summary, the contributions of the paper are as follows:\n\n\n    \n  * We design a symmetry and pose ambiguity aware object SLAM system which fully utilizes information from multiple pose hypotheses of objects to jointly optimize camera pose and globally consistent object pose.\n    \n    \n  * We propose a method to extract reliable information from ambiguous pose of symmetric objects. We categorize symmetry types of general objects and distinguish pose parameters into ambiguous one and unambiguous ones. The extracted unambiguous parameters of each symmetry type are used in the proposed object association and optimization modules.\n    \n    \n  * We use multiple hypotheses 3d detection network as observation module of our system, which can be easily edited from existing networks and changed to better networks in the future.\n\t\n\nThe remainder of this paper is organized as follows. Section <ref> reviews related literature. The overview and core concepts of the system are provided in Section <ref>. Detailed methods for applying core concepts to systems are described in Sections <ref> and <ref>.\nIn Section <ref>, experiments in simulation and public dataset are presented. Finally, conclusions are provided in Section <ref>. \n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\nThis section reviews related studies on object-based SLAM and symmetric-aware object pose estimation.\n\n\n\n \u00a7.\u00a7 Object-based SLAM\n\nObject-based SLAM attempts to build a robust SLAM system by simultaneously optimizing camera poses, feature positions, and poses of objects from semantic information.\nTo express the shape and pose of an object, <cit.>, <cit.>, and <cit.> use prior object model, cuboid, and ellipsoid, respectively. And <cit.>, <cit.> represent the shape and pose of an object using category specific embedding method. Although they showed that joint optimization could increase the robustness of both camera and object pose estimation, they did not consider the presence of ambiguous detection due to symmetric objects or occlusion.\n\n\n\n \u00a7.\u00a7 Symmetry-aware object pose estimation\n\n\n\n  \u00a7.\u00a7.\u00a7 Single view\n\n<cit.> proposes a network that can determine whether a shape has symmetry using an object's CAD model, and <cit.> predicts multiple candidate poses for the detected object and analyzes ambiguity that may be caused by occlusion as well as ambiguity by the shape of an object. However, they estimated object pose only from a single view and did not treat multi-view cases such as SLAM.\n\n\n\n  \u00a7.\u00a7.\u00a7 Multiple view\n\nRecently, studies considering the pose-ambiguity of objects have been proposed in object-based SLAM. PoseRBPF <cit.> estimates the pose distribution of asymmetric and symmetric objects using the rao-blackwellized particle filter. However, it requires a predefined codebook for the pose of the object model. <cit.> is the object-based SLAM system that simultaneously optimizes camera poses and object poses using the projection of reconstructed 3d keypoints as a prior. However, every time they encounter on a new dataset, they have to label the prior. <cit.> expresses geometrical primitives in an unified, decomposed quadric form and explicitly deals with the degenerative case due to a symmetric shape.\nHowever, the degenerative cases considered in <cit.> are limited to a few quadric types, so error can be induced when fitting arbitrary shapes to quadric.\n<cit.> uses a single neural network to predict poses as multi-hypotheses and optimizes them through a max-mixture<cit.> model. Since the symmetric object has ambiguous pose parameters, such as the rotation angle for an axis of symmetry, pose estimation should be performed by considering only the unambiguous elements. However, error can occur because <cit.> directly uses hypotheses containing even ambiguous parameters for pose estimation.\n\nWe propose a robust SLAM system using existing well-developed 3d detection modules while extracting only available geometric elements from symmetric objects and using them as optimization constraints.\n\n\n\n\n\n\n\u00a7 PROPOSED SYSTEM\n\n\n\n\n \u00a7.\u00a7 Symmetry types\n\n\nThe key idea of the proposed system is to propose a criterion that can effectively classify general objects using only three symmetry types and to define different pose representations suitable for each symmetry type in order to fully utilize the geometry information available in the symmetric object.\n\nIn general, objects can be asymmetric or symmetric. The asymmetric object can be inferred to have a consistent object pose in the 3d detection module, no matter which viewpoint it is observed from, but the symmetric object cannot. In addition, we classify symmetry as discrete and continuous types. Discrete symmetric objects refer to reflection symmetric objects that can have a finite number of poses as the observed viewpoints change, and continuous symmetric objects refer to objects with an infinite number of poses based on an axis of symmetry, such as a circular table. Objects which are classified into the following three types are represented in different ways to reflect all possible unambiguous pose parameters:\n\n    \n  * Asymmetry:\n    All detection results of a asymmetric object can be used to optimize a unique pose since the unique pose can be determined when viewpoint changes. Accordingly, the pose can be represented by 6 degrees of freedom (DoF) which is commonly used.\n    \n  * Discrete symmetry:\n    Objects with multiple planes of symmetry have as many poses supporting the same shape as the number of planes of symmetry. We express the pose assuming that the symmetrical planes of most discrete symmetric objects present have one intersection line. The intersection of symmetric planes is defined as the axis of symmetry, and the rotation angle based on the axis is defined as a symmetric angle. Accordingly, the position and the axis of symmetry are shared with the reflected poses of a discrete symmetric object, and only the symmetric angle can be expressed differently. In other words, the pose of a discrete symmetric object is defined as five shared parameters for position and axis of symmetry and unshared parameters for all the symmetry angles.\n    \n  * Continuous symmetry:\n    For objects such as round tables, there is an infinite number of symmetry planes, so there is an infinite number of object poses that support the same shape. For continuous symmetric objects, it is assumed that planes of symmetry have a single intersection, as in the case of discrete symmetry. Therefore, we express the pose of continuous symmetric objects only by position and axis of symmetry after removing the symmetric angle by classifying it as an ambiguous parameter.\n\n\n\n\n \u00a7.\u00a7 Pipeline\n\nThe entire pipeline is described in Fig. <ref>, and the tracking and object mapping modules are designed based on DSP-SLAM <cit.>. When a new keyframe is selected, the multi-hypothesis detection network uses an rgb-d image to detect objects, and the detected objects infer observable multiple pose hypotheses (Section <ref>). The categorization module determines the symmetry type of the detected object based on the distribution of multiple pose hypotheses. We extract the axis of symmetry for symmetric objects and cluster the poses with similar symmetry angles for discrete symmetry objects (Section <ref>). We associate the categorized detection with map objects using the class and pose except for ambiguous parameters. For discrete symmetric objects where a new symmetry angle is observed, we add a non-shared parameter (Section <ref>). The camera pose, map point, and map object are jointly optimized at the backend. The constraint between the camera pose and map object pose is formed differently for each map object type during optimization (Section <ref>).\n\n\n\n\u00a7 CATEGORIZED DETECTION AND OBJECTS\n\n\n\n\n \u00a7.\u00a7 Multi-hypothesis 3d object detection\n\n\nWe modify the 3d object detection module so that it can infer multiple pose hypotheses. Since the distribution of inferred multiple hypotheses influences the determination of the object's symmetry type, it should be modified so that the multi-hypothesis 3d object detection module can cover all the poses that the object can have. <cit.> extends the single-loss single-output system to have multiple outputs, calculating loss <cit.> using only the hypothesis that succeeded in the most accurate inference among multiple hypotheses in the training process. Unlike the method of using the average loss of multiple outputs, this can increase multi-hypothesis diversity because each hypothesis is randomly selected and learned individually. We also modified the 3d object detection module <cit.> in the same way as <cit.>, allowing a source for the object's symmetry type to be inherent in multiple hypotheses.\n\n\n\n \u00a7.\u00a7 Symmetry type categorization of detection\n\n\nAs shown in Fig. <ref>, since the multi-hypothesis detection implies the ambiguity of the object's pose, we can classify the object's symmetry type and extract the type-specific pose parameters only from the detection result without prior information.\n\nUnlike the symmetric object, the multiple pose hypotheses of the asymmetric object are similar to single pose. In other words, the distribution of pose hypotheses of the asymmetric object is very close to unimodal, and the variance is much smaller than that of the symmetrical object.\n\nFor discrete and continuous symmetry types, we perform an additional classification process because the normalized singular values of multiple hypotheses are large in both cases. As mentioned in Section <ref>, both types have certain axis of symmetry, and the axis of symmetry l is obtained as follows:\n\n    l^*=l\u2208\u211d^3argmax[\u03c9_o_1o_2,\u03c9_o_1o_3,\u22ef,\u03c9_o_1o_N]^T\u00b7 l _2\u00a0,\n\n\n    where  \u03c9_o_1o_i = \u03c9_o_1o_i/\u03c9_o_1o_i_2,  \u03c9_o_1o_i = log_so(3)(R_co_1^TR_co_i) \u2208\u211d^3.\n\nN and \u03c9 are the number of hypotheses and rotation axis, respectively. And, symmetry angles are computed by  \u03b8_o_1o_i = \u03c9_o_1o_i_2. After clustering the multiple hypotheses using the DBSCAN algorithm <cit.>, the variance between the representative \u03b8 of each cluster is compared. As illustrated in Fig. <ref>, compared to the continuous symmetric object which corresponds to a continuous set of  \u03b8, the variance among clusters of discrete symmetric object is very large. Representative \u03b8 values of rectangular table form two clusters with about 180 difference, while the round table has many similar representative \u03b8 clusters.\n\nThen, reliable parameters of pose can be extracted for each classified symmetry type. First, the asymmetric object can use 6 DoF from detection results, and the continuous symmetric object can use position and axis of symmetry. In discrete symmetric objects, parameters are position, axis of symmetry, and representative symmetry angles of the clusters.\n\n\n\n\n\n \u00a7.\u00a7 Data association of objects\n\n\nThe categorized detection results are associated with the previously generated map objects.\nFirst, the detection results, which represent relative transformation between the camera and the object, are warped in world coordinate using tracked camera pose T_wc. Then, object matching is performed by comparing the unambiguous parameters of the detected object and map objects considering the symmetry type. Due to inaccurate detection or partial observation, the same object can be categorized by different symmetry types in different views. We address the misclassification by following association strategies. If there is a discrete or continuous symmetry type in the two comparison groups, the position and axis of symmetry (5 DoF) parameters are compared; otherwise, the distance between the 6 DoF parameters is computed. If the distance is small enough and the class is the same, matching between the objects is successful.\n\nAfter object matching is performed, the symmetric angles of the detected discrete symmetric object are associated with the nearest symmetric angle of the matched map object. The unmatched angles are added to the map object as a new symmetric angle.\n\nDiscrete symmetric object can be occasionally classified into asymmetry type at a specific viewpoint, as can be seen from Fig. <ref> (a) and (c) of the the discrete case. Therefore, even if 6 DoF matching between asymmetry objects fails, if 5 DoF matching is successful, we change the map object to the discrete symmetry type to express the pose with position, an axis of symmetry, and symmetric angles. Object association using unambiguous parameters for each symmetry type alleviates mismatching caused by ambiguous parameters. Therefore, the proposed system can track objects for a long time, which acts as an important source in joint optimization.\n\nThe detection result that failed to match is registered as a new map object after the shape reconstruction using deep-sdf <cit.> similar to DSP-SLAM.\n\n\n\n\u00a7 JOINT OPTIMIZATION\n\n\nWe perform joint optimization by modifying the existing SLAM optimization problem using the categorized map objects and associated detection results.\n\n    C^*, O^*, P^* = C,O,Pargmin\u00a0 E_reproj(C, P)+E_obj(C, O)\u00a0,\n\nwhere C, O, and P refer to the camera and object pose and map point included in the optimized window size, respectively. E_reproj denotes the reprojection error, and E_obj denotes the pose error between the map object and the camera. The joint optimization is solved by the Levenberg-Marquardt method through g2o<cit.> solver.\n\nE_obj is designed only with unambiguous parameters in consideration of the symmetry type of map object, which can produce a more robust solution than existing methods that utilize all pose parameters with an ambiguous parameter as a constraint.\n\n\n\n  \u00a7.\u00a7.\u00a7 Asymmetry\n\nThe results of asymmetry map object and associated multiple hypotheses are not linked by multiple edges but by a single edge using a max-mixture model, which selects the hypothesis with the lowest error at each iteration of optimization, similar to <cit.>.\n\n    e_asym(T_wc,T_wo) = min_j\u2208[1,N]log_se(3)(T_co^j\u00b7 T_wo^-1\u00b7 T_wc)\u00a0,\n\nwhere T_wc and T_wo, which mean the pose of the camera and object on the world, are optimization variables, T_co^j represents the j-th hypothesis, and N is the total number of hypotheses.\n\n\n\n  \u00a7.\u00a7.\u00a7 Discrete symmetry\n\nDiscrete symmetric map objects have position and axis of symmetry as shared parameters and M symmetric angles as non-shared parameters. Each hypothesis is associated with one of symmetry angles of the matched map object, and edges are formed as many as the number of associated symmetric angles, m. Each edge is modeled with the max-mixture like the asymmetry case.\n\n    e_disc(T_wc,T_wo_i) = min_j_i\u2208[1,N_i]log_SE(3)(T_co^j_i\u00b7 T_wo_i^-1\u00b7 T_wc)\u00a0,\n\n\n    where\u00a0 T_wo_i = [ exp(\u03b8_wo_i\u00b7\u03c9_wo)                              t_wo;            0_1\u00d73                1 ], \u00a0 i \u2208[1,m]\u00a0.\n\nN_i refers to the number of hypotheses associated with the i-th symmetric angle, and T_wo_i means the i-th symmetric pose constructed by position and axis of symmetry which are shared parameters and the i-th symmetric angle. Furthermore, for unconstrained parameterization in optimization, the axis of symmetry is used by the following expression: \n\u03c9_wo = f(\u03d5_wo, \u03c8_wo), where \u03d5_wo, \u03c8_wo and f(\u00b7) are polar angle, azimuth in spherical coordinates, and the transformation function from (\u03d5, \u03c8) to \u03c9, respectively.\n\n\n\n  \u00a7.\u00a7.\u00a7 Continuous symmetry\n\nThe continuous symmetric map object has only position and axis of symmetry as unambiguous parameters. The axis of symmetry (\u03c9_co) extracted from the rotation part and individual position parts of multiple hypotheses are used to formulate the following:\n\n    e_cts(T_wc,T_wo) = e_trans + \u03b3\u00b7 e_axis\u00a0,\n\n\n    where\u00a0\n        e_trans   = min_j\u2208[1,N] t_co^j - R_wc^T(t_wc-t_wo) _2\u00a0,\n    \n        e_axis   = f^-1(R_wc\u00b7\u03c9_co) - [\u03d5_wo, \u03c8_wo]^T_2\u00a0.\n\ne_trans and e_axis are max-mixture based translation error and error related to axis of symmetry, respectively. \u03b3 is the constant weight for balancing two error terms.\n\n\n\n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n\n\nThis section presents the performance of the proposed symmetry-aware object SLAM system compared with the baseline. Simulation and public datasets are used to evaluate the proposed system.\n\n\n\n \u00a7.\u00a7 Setup\n\nWhen a camera looks at the feature-rich scene, ego-motion can be estimated accurately by the feature-based SLAM backbone <cit.> in object-based SLAM systems. On the other hand, object-camera constraints are dominant estimation sources if the camera observes a featureless scene.\nTherefore, in order to  effectively evaluate the proposed object-based SLAM system, we construct a simulation environment such that one side of the environment has rich feature points and the other side has a few feature points, as shown in Fig. <ref>. Then we obtain the simulation dataset using Unreal Engine and AirSim <cit.>. The camera moves surrounding a center object in the environment so that camera observes feature-rich and featureless scenes alternately for each specific region, as shown in Fig. <ref> (a) and (b).\nAs shown in Fig. <ref>, the center object can be replaced with discrete and continuous symmetric objects in the simulation environment. These cases are called sim(disc) and sim(cts). In addition, we evaluate the proposed system in the general indoor scenes using the popular scanNet dataset <cit.>.\n\nFor training of the 3D detection network used in our system, pre-training is performed using SUN RGB-D dataset <cit.>. After that, to reduce the domain gap between actual and simulation data, additional data are acquired in the simulation, and fine-tuning is performed. We only fine-tune the detection network for simulation dataset since ScanNet dataset only provide axis-aligned bounding box. Both pre-training and fine-tuning is done in the same way as <cit.>. The SUN RGB-D dataset assumes that the input point clouds are represented in the coordinates aligned with the direction of gravity, and expresses the object's orientation using the rotation angle with respect to the axis of gravity (i.e. yaw). \nHowever, ScanNet dataset has no gravity direction, so we calculated in advance the rotation matrix that aligns the y-axis of camera with the normal vector of the ground plane for each frame. An additional sensor such as an inertial measurement unit or ground detection module may help to change the preprocessing for online implementation. \nWe used the number of hypotheses as 30, which was selected empirically to categorize symmetry types of detection well.\n\nWe test using  Intel i7-10700 (2.9GHz) and NVIDIA RTX 2060 GPU. DSP-SLAM using single hypothesis detection results is used as the first baseline system (SH), and the second baseline (MH) is the modified DSP-SLAM to integrate with key idea on <cit.> which uses multi-hypothesis detection with no consideration of symmetry types.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Case study\n\nTo understand how the proposed system enhances the overall performance, we test object tracking and pose estimation under the presence of symmetric objects with pose ambiguity, using the environment in Fig. <ref>. For fair comparison by isolating the other sources that affect the performance except the pose ambiguity, we construct a pose graph using edges from the camera to objects obtained from the proposed categorization and association module and true camera nodes and estimated the object pose by optimization. The same setup is employed  to test the baseline algorithms. \n\nFig. <ref> shows the result of object tracking and the reconstructed map object using the sim(disc) dataset. The proposed method continuously tracks the center object, and a single map object is reconstructed accordingly. On the other hand, the baseline algorithms (MH, SH) fail to track, and incorrectly recognize a single object as two or more map objects. \n\nThe translation error of the estimated object is reported in Fig. <ref>. MH and SH show large error when establishing the object again after the tracking failure, whereas the proposed method maintains small error through successful object tracking. \n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 System evaluation\n\nThe performance of the entire system was compared and evaluated with the baseline system using the simulation environment and ScanNet.\nFirst, Fig. <ref> shows the quantitative results compared with the baseline system using the simulation environment sim(disc). The baseline and camera trajectory are plotted on the map built by the proposed system. In the beginning, a feature-rich scene is observed as shown in Fig. <ref> (a), so all three systems are good at estimating pose. Such trend changes as the features that can be used for localization disappear, since the camera location must be estimated using only the tracked object. Based on the good object tracking performance, the proposed system also demonstrates good performance even in the featureless region using an unambiguous pose parameter that fits the symmetry type. However, both MH and SH systems fail to estimate the pose in the featureless region. The baseline algorithms associate the detection with the map object on the map using all 6 DoF parameters, and there are cases when none of the multiple hypotheses fits the previous detection result. The data association may fail due to such mismatch in yaw angle. Quantitative results for the simulation environment are shown in Table <ref>. For sim(cts), the baseline algorithms also had no failure in pose estimation, but we can see that the proposed algorithm has the highest performance.\n\nFig. <ref> shows qualitative results on a ScanNet Scene0022_00. This sequence includes not only the symmetrical objects but also the objects that do not fully enter the camera field of view, so the uncertainty of detection is high.\nThis is revealed in the result of SH. As seen in simulation setting, the baseline system fails data association and many objects are registered in same position of the map. However, the proposed system robustly recognizes as a single object and optimizes the pose  even in these cases. The quantitative results are shown in table <ref>. We evaluate the system performance using root mean squared error (RMSE) error of each keyframe position. The proposed system exhibits similar or better path estimation performance in most sequences.\n\n\n\n\u00a7 CONCLUSIONS\n\n\nSymmetric objects present in the scene  can cause the performance degradation or even failure of SLAM, since their observation at different viewpoints can be identical and cause obscurity. We proposed a method for robustly optimizing the pose of cameras and objects even in the presence of symmetric objects. \n\nThe proposed classification of objects into three categories depending on their symmetry characteristics was successfully applied to various objects. Under the proposed method, the objects in the same category can be associated with the same type of ambiguity, which contributes to the efficiency in data association.\nBy extracting only the unambiguous parameters corresponding to each category and using them in data association and joint optimization of the camera and object pose, the proposed approach provides significant robustness to the SLAM performance. Proposed system showed better performance than baseline systems in environments with many symmetric objects. \n\n-8cm\n\n\n\n\n\n\n\n\n\n\n./bibtex/IEEEtran\n\n\n\n\n"}