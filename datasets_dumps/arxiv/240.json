{"entry_id": "http://arxiv.org/abs/2303.07026v1", "published": "20230313114238", "title": "Visual-Policy Learning through Multi-Camera View to Single-Camera View Knowledge Distillation for Robot Manipulation Tasks", "authors": ["Cihan Acar", "Kuluhan Binici", "Alp Tekirda\u011f", "Wu Yan"], "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI"], "text": "\n\n\n\nAdaptive Dereverberation, Noise and Interferer Reduction Using Sparse Weighted Linearly Constrained Minimum Power Beamforming\n\n\n\nThis work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) \u2013 Project ID 390895286 \u2013 EXC 2177/1.\n\n    Henri Gode and Simon Doclo\nDepartment of Medical Physics and Acoustics and Cluster of Excellence Hearing4all,\nUniversity of Oldenburg, Germany \n\n{henri.gode, simon.doclo}@uni-oldenburg.de\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    March 30, 2023\n====================================================================================================================================================================================================================================================================\n\nempty\nempty\n\n\n\n\nThe use of multi-camera views simultaneously has been shown to improve the generalization capabilities and performance of visual policies. \nHowever, the hardware cost and design constraints in real-world scenarios can potentially make it challenging to use multiple cameras. \nIn this study, we present a novel approach to enhance the generalization performance of vision-based Reinforcement Learning (RL) algorithms for robotic manipulation tasks. \nOur proposed method involves utilizing a technique known as knowledge distillation, in which a pre-trained \u201cteacher\u201d policy trained with multiple camera viewpoints guides a \u201cstudent\u201d policy in learning from a single camera viewpoint. \nTo enhance the student policy's robustness against camera location perturbations, it is trained using data augmentation and extreme viewpoint changes.\nAs a result, the student policy learns robust visual features that allow it to locate the object of interest accurately and consistently,  regardless of the camera viewpoint.\nThe efficacy and efficiency of the proposed method were evaluated both in simulation and real-world environments. \nThe results demonstrate that the single-view visual student policy can successfully learn to grasp and lift a challenging object, which was not possible with a single-view policy alone.\nFurthermore, the student policy demonstrates zero-shot transfer capability, where it can successfully grasp and lift objects in real-world scenarios for unseen visual configurations.\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nIn recent years, the field of deep reinforcement learning (DRL) has seen rapid progress in robot control and manipulation applications, with vision-based RL algorithms being particularly noteworthy due to their ability to learn directly from high-dimensional visual inputs such as images, point clouds or videos <cit.>-levine2018learning<cit.>. \nHowever, one of the most significant challenges for these algorithms remains the generalization of learned skills to new and diverse situations. \nTo address this issue, researchers have employed various data randomization<cit.> and augmentation methods, including changes to background color, texture, and other parameters\u00a0<cit.>laskin2020reinforcement-hansen2021generalization<cit.>. \nWhile these techniques have shown effectiveness in learning representations of high-dimensional data, their generalization capabilities are limited to the features acquired from a fixed viewpoint perspective.\n\n\n\nTo overcome this limitation, researchers have explored various techniques to enhance the generalization capabilities of vision-based RL algorithms. \nOne approach involves training the algorithm using multiple viewpoints to recognize objects from different perspectives. \nFor example, to develop visuomotor manipulation skills, researchers often employ a third-person camera view that captures the robot and environment from the outside, as well as a first-person (egocentric) camera that is linked to the robot's end-effector.\nThe third-person camera allows visuomotor policy to view the environment and robot from a wide static perspective and results in a better understanding of the environment. \nIn contrast, the egocentric camera provides a closer yet restricted observation due to its inability to view the robot arm and environment in full scope. \nThe perspective of the egocentric camera changes according to the position of the end-effector, which can lead to important visual features becoming obscured.\nIt is also feasible to use both egocentric and third-person cameras simultaneously, which improves the generalization capability and performance of visual policy <cit.><cit.>.   \nIn a simulation environment, it is possible to use multiple third-person and egocentric cameras together to obtain better results without having any limitations. \nOn the other hand, in real-world scenarios, increasing the number of cameras incurs higher hardware costs and design constraints.\n\nIn this paper, we propose a method for learning a robust manipulation policy that can generalize to unseen visual configurations, using a single camera.  \nOur approach leverages knowledge distillation to transfer privileged multiple-view information from a teacher policy to a student policy that observes the task using information only from a single camera. \nBy learning from the teacher policy's rich multi-view information, the student policy can acquire robust features that can consistently locate the object of interest, regardless of the single-camera viewpoint.\nFirst, a teacher policy is trained with multiple views gathered from fixed viewpoints to learn more generalizable representations. \nThen, a student with a single-camera viewpoint is trained to imitate the action of the teacher policy. \nTo enhance robustness against camera location, the camera locations and parameters are constantly changed, in addition to data augmentation techniques such as random cropping and color jiggering. \nConsequently, the single-camera student can learn the same actions as the teacher from heavily augmented and perturbed camera positions. \nThis enables the student policy to perform manipulation tasks using a single-camera observation and as a result,  makes it more practical and efficient for real-world applications.\nTo the best of our knowledge, this is the first study that explores the transfer of privileged multiple-view teacher information to a randomized single-view student for learning robot manipulation tasks. \nTo this end, the main contributions in this work can be summarized as follows:\n\n    \n  * A novel and robust visual-policy learning by transferring privileged multiple-view information of the teacher to the single-camera view student.\n    \n  * The use of randomized camera views during training, allowing the student policy to learn to adapt to different camera angles and positions.\n    \n  * Investigation on the effect of the number of third-person camera views for knowledge distillation from the multiple-view teacher to the single-view student.\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n \u00a7.\u00a7 Vision-based Reinforcement Learning\n\nThe overall goal of vision-based reinforcement learning is to enable agents to learn from visual inputs, such as images or videos, in order to perform tasks in complex environments. \nTo extract high-level features and improve the sample efficiency of vision-based RL algorithms, CURL method is proposed\u00a0<cit.>. This method employs contrastive loss to ensure consistency between an image and its augmented version. Many works also focus on data augmentation methods such as random shift, random crop, color jitter, and applying Gaussian noise, and random convolutions for learning robust representations from the raw input image\u00a0<cit.>-hansen2021generalization<cit.>.\nData augmentation and CURL are also successfully implemented for real-world robotic tasks\u00a0<cit.>.\nOne limitation of these methods is that they may not be adequate to handle more intricate forms of variation, such as variations in viewpoint or camera parameters.\n\n\n\n \u00a7.\u00a7 Knowledge Distillation\n\nThe procedure of using the knowledge of a pre-trained neural network model to enhance the capabilities of another one is referred as knowledge distillation (KD) <cit.>. \nMost commonly the pre-trained model, called the \u201cteacher\", is used to provide additional annotation for the data samples to be used along with the ground truth labels while training another model from scratch.  \nThis enriches the limited information conveyed by the ground truth labels. As a result, the model trained with such enhanced information, called the \u201cstudent\", can achieve performance beyond its original capacity and even outperform the teacher.\n\nIn the context of reinforcement learning, KD has been employed for various tasks, such as policy distillation <cit.> which proposes distilling the knowledge of a large and accurate policy network, called the teacher, into a smaller and faster policy network, called the student.\n\n\n\n \u00a7.\u00a7 Privileged Provision\n\nRecently, KD is utilized in learning vision-based urban driving <cit.>, in-hand object reorientation <cit.>, and  quadrupedal locomotion over challenging environments <cit.> <cit.> where the teacher policy that has access to privileged information in a simulation environment is used to guide the student policy that has only access to sensory information available in the real world.\nThe student policy is then deployed and successfully tested in a variety of challenging real-world environments.\nSimilary, KD is also applied for zero-shot generalization of visual policies <cit.>.\nAt first, weak image augmentation is used to train an expert policy.\nThen, using imitation learning with strong augmentations, a student network learns to mimic the expert policy.\nEven though it has been demonstrated that strong augmentations can offer robust representation learning for a range of visual policy tasks in diverse simulation contexts, generalization capabilities are restricted to features acquired through a single fixed viewpoint only.\n\n\n\n\n\n\u00a7 PRELIMINARIES\n\nThe reinforcement learning environment is defined as a Markov Decision Process (MDP) tuple \u27e8 S,A,p,r,\u03b3\u27e9: S is the continuous state space, A is continuous action space, p(s_t+1|s_t,a_t) is the state transition model where probability density of the next state  s_t+1\u2208 S given s_t \u2208 S and a_t \u2208 A, r is the reward function, and \u03b3\u2208 (0, 1] is the discount factor. \nEpisode length is denoted by T.\nThe objective of an RL algorithm is to find an optimal policy \u03c0: S \u2192 A to maximize the expected return. \n\n  \nIn order to determine the policy distribution \u03c0(a_t|s_t), we employ an actor-critic model.\nAs our algorithm of choice, we use the Soft Actor-Critic (SAC) <cit.>, a state-of-the-art off-policy maximum entropy deep reinforcement learning algorithm due to its good performance in sparse reward setups.\nThe optimal policy equation of SAC is shown below:\n\n    \u03c0^* = max_\u03c0\u2211_t\ud835\udd3c_(s_t, a_t) \u223c\u03c1_\u03c0[ r(s_t, a_t) + \u03b1 H(\u03c0(\u00b7 | s_t)) ]\n\nwhere the temperature parameter, denoted as \u03b1, controls the stochasticity of the optimal policy by determining the relative importance of the entropy term compared to the reward.\n\n\n\u00a7 APPROACH\n\n\n\n \u00a7.\u00a7 Problem Formulation\n\nIn visual policy learning, the agent perceives the environment through cameras and therefore, the environment is only partially observable. \nThis makes the problem formulation a Partial Observability Markov Decision Process (POMDP) instead of a standard MDP. \nAs a result, the state vectors s_t in (<ref>) are replaced with observations, denoted as o. \nThese observations are obtained by encoding RGB images collected from the environment [I^1,...I^N] via a function f before being provided to the  policy module:\n\n    o := f([I^1,...I^N])   where    I\u2208\u211d^C \u00d7 H \u00d7 W\n\nTypically f is parameterized by a neural network (NN) \u03d5 and learned using gradient-based optimization techniques. \nSince neural networks fail to generalize when training data lacks diversity, increasing the number of unique views N by using more cameras can improve the performance. \nAs a result, there is a natural trade-off between hardware cost and performance since utilizing many camera devices to process diverse views of the environment typically provides robust policies against domain variations at the expense of increased hardware demand. \nTo break such a trade-off, we propose training a privileged teacher policy \u03c0_t(a|o_t) using multiple camera views from the simulation environment and later distilling the learned policy to train a non-privileged student policy \u03c0_s(a|o_s) that works with only single-view observations. \nIn this way, while the hardware cost of deploying \u03c0_s is limited to a single camera, it can provide similar performance to \u03c0_t. \n\nOur objective is to find the optimal student policy \u03c0_s^* that minimizes the knowledge distillation (KD) loss. \nThe KD loss measures the discrepancy between the student policy and the teacher policy:\n\n    \u03c0_s^* := \u03c0_sargmin  L_KD(\u03c0_s,\u03c0_t)\n\n\n\n\n\n\n\nLet I_e and [I_t^1...I_t^N] denote RGB images captured from the egocentric and the N third-view cameras.  \n\n\n\nSince the teacher policy has access to more privileged information, it can learn better generalizable visual representations. \nWe can formally describe the teacher and student policies as,\n\n    z\n\n\nEven though the camera position of the single-view student is perturbed at each episode and strong augmentation is applied at each frame, the multi-view teacher has fixed camera observations so that the student can learn suitable actions to take for different camera viewpoints.    \n\n\n\n\n\n \u00a7.\u00a7 Implementation Details\n \nThe teacher operates with images collected from a single egocentric camera (I_e) and N stationary third-person view cameras ([I_t^\u03b8_1,...I_t^\u03b8_N]) while the student uses the view of a single third-person view camera (I_t^\u03b8_k).\n\n    o_t   := f_\u03d5 t(I_e, [I_t^\u03b8_1,...I_t^\u03b8_N]) \n    \n        o_s   := f_\u03d5 s(I_t^\u03b8_k)   where  \u03b8_k \u223c U(0,\u03b8_max)\n\nTo make the student robust against environmental perturbations, we apply domain randomization and sample the camera angle \u03b8_k randomly from a uniform distribution that ranges from 0 to \u03b8_max. \nBoth the teacher and the student have the same network architecture comprised of a 12-layer convolutional encoder with a single-layer self-attention module and a SAC policy.\nWe use the PyTorch\u00a0<cit.> implementation of SAC as in <cit.>.\nKnowledge Distillation:  \nOur use of KD stems from a similar principle that motivated the DAgger <cit.> which reduces the distribution shift from the learned policy  with data from an expert policy. \nDifferent than DAgger, our expert is not a human, instead we use a pre-trained teacher policy.\nTo train the student, we use two sources of information: the actions and encoded features provided by the teacher. \nTo distill the first information into the student, the mean squared error (MSE) between the student \u03c0_s and teacher \u03c0_t policies is minimized: \n\n    L_KD :=\u03c0_s(o_s) - \u03c0_t(o_t)\n\nThe student policy \u03c0_s is updated by minimizing this loss using the transition batch sampled from experiences replay buffer \ud835\udc9f.\nThe updated student policy is then used to collect more data, which is added to \ud835\udc9f and used for the next iteration of the policy update.\n\nDuring the training of the teacher, only the loss of the critic is used to update the weights of the encoder network for learning the representation space.   \nFor training the student encoder, instead of using the gradients of critic to update the student encoder we implement two methods.\n\nThe first one is inspired by <cit.> where we align the pairwise similarity matrix of teacher observations C_t  with that of the student observations C_s by minimizing the following loss:\n\n    L_sim := \u2211_i\u2211_jC_T^ij - C_S^ij_F^2\n\nDifferent to <cit.>, We use the cosine distance to quantify the pairwise similarities between different observations:\n\n    C := C/o_[i,:]o_[i,:]^T;   C := oo^T;   o\u2208\u211d^b\u00d7(cwh)\n\nHere o is the flattened version of the observation tensor o \u2208\u211d^b \u00d7 c \u00d7 w \u00d7 h. \n\nNotice that the i^th row and j^th column of C represents the following value:\n\n    C^ij := o^(i)o^(j)/o^(i)o^(j)_2\n\n\n\nIn the second one, the distillation objective is to minimize the Euclidean distance between the features of the teacher and student. \nThus, the second method tries to minimize the sum of all the squared differences between the teacher encoder output and the predicted student output:\n\n    L_MSE := o_t - o_s_2\n\n\nOur results, to be presented in the next section, show that minimizing the Euclidean distance between features yields superior performance for the student policy when compared to the pairwise similarity method. \nThis finding suggests that minimizing the Euclidean distance provides better distillation for learning high-level visual features.\nCurriculum Learning: To make the student robust against environmental perturbations we apply domain randomization and permute the camera angle \u03b8_k and camera parameters.\nWe utilize a simple curriculum training that gradually increases the range of camera parameters of the student as the training  develops to ensure continues policy learning.\nOur intuition is that gradually increasing the camera parameters increases the difficulty level of the task.\n\nIt is also possible to utilize more efficient automatic curriculum learning methods like ALP-GMM <cit.>, Automatic Curriculum Learning through\nValue Disagreement <cit.>, where the complexity of the task is determined by the performance of student policy.\nData Augmentation: To improve the generalization ability of the policies, we employ two techniques for data augmentation during training: pixel shifting and color augmentation. \nThese techniques help to expose the models to a more diverse range of visual inputs, which in turn can improve their ability to perform the task in a wider variety of scenarios. \nSpecifically, we randomly shift the pixels of the images in two dimensions and apply color augmentation techniques such as changing the brightness, contrast, and saturation. \nBy applying these techniques, we aim to make the policies more robust to changes in lighting, and other environmental factors that may be present in real-world scenarios. \nSetup Details:  \nMultiple cameras can be employed to capture various viewpoints and enhance the generalization capability of visual-policy learning. \nOur study focuses on the utilization of two-camera and three-camera setups to train an expert visual policy. \nThe two-camera setup comprises a first-person (egocentric) camera attached to the robot's end-effector and a third-person camera positioned in front of the robot to capture a bird's eye view of the table and robot. \nThe three-camera setup consists of a first-person camera and two third-person cameras positioned in front and on the side of the robot, respectively.\nDuring the training of the student policy, we employ only one of the third-person cameras to capture the visual information. \nMore precisely, we use a single third-person camera placed in front of the robot. \nWhen training a student policy, only one of the third-person cameras is used, but the camera parameters and view are randomly changed. \nBy constantly changing the camera view and parameters, the training data becomes more diverse, and the student policy can learn to adapt to different camera angles and positions. \nThis allows the student policy to generalize better to new, unseen camera views, which is crucial for applications where the camera position and orientation may not be known or fixed. \n\n\n\n\n\n\nIn our context, the visual policy is responsible for generating control actions that enable the robot to perform a given task. \nThe action space includes control over the 3-DoF end-effector position along the X, Y, and Z axes, 1-DoF rotation control on the yaw-axis, and 1-DoF gripper control. \nTo train the teacher and student policies, we use 84x84 RGB camera images as observations. \n\nWe train the teacher policies using a sparse reward scheme with a fixed time horizon, in which the agent gets rewarded only if the task is completed successfully.\nAchieving a task successfully results in reward at each subsequent time step, and accumulates until the end of the episode, which encourages the agent to complete the task as quickly as possible.\nTask Definition: The tasks in our experiments require the robot to grasp either a cube or a mug and lift it to a predetermined height from its initial position. \nWhile cubes and spheres have a symmetrical shape that makes it easier for visual policies to learn grasping strategies, the lack of symmetry in the mug presents a greater challenge.\nThe task is further complicated by the visual localization and the sparse reward signal, which require the agent to infer object positions solely from high-dimensional RGB camera pixel data. \nTo increase the diversity of training data, objects are randomly spawned at different positions on the table for each episode. \nThis ensures that the agent learns to adapt to different object positions, enhancing its ability to perform the task in novel scenarios.\n\n\n\n\n\n\n\u00a7 RESULTS\n\n\n\n\n\n \u00a7.\u00a7 Simulation Experiments\n\nIn order to evaluate the effectiveness of our proposed approach, we conducted experiments using a 7 degree-of-freedom Franka Panda Emika manipulator equipped with a two-fingered parallel gripper in a PyBullet simulation environment <cit.>. \nTo train the teacher policies \u03c0_t, we used a combination of first-person, front, and side camera views, as illustrated in Figure <ref>(a). \n\n \nWe introduced a significant level of randomization to the camera position and parameters, such as field of view (fov) and aspect ratio to increase the robustness of the student policies \u03c0_s. \nPyBullet facilitated this by employing a synthetic camera specified by two 4 by 4 matrices: the view matrix and the projection matrix, allowing for the rendering of images from arbitrary camera positions for each episode. \nWe provide a few examples in Figure <ref>(b) to showcase the different camera perspectives used during the training of the student policies in the simulation environment.\nFor all teachers and students trained, we use the same data argumentation methods described in section <ref>.\nIn the simulation environment, for each task we used a 3D-mouse to collect 50 human demonstrations (total of 5K transitions), which were added to the replay buffer to aid in training the multi-camera view expert teachers in a sparse reward setting. \nThis method of using demonstrations has been shown to be effective in improving the learning process and accelerating the convergence of the RL algorithms <cit.>.\nDuring the training process, we keep only the most recent 50K transitions in the replay buffer.\n\nThe training results of teacher policies \u03c0_t using different numbers of camera views and the corresponding results of the student policies distilled from them to lift the mug and cube objects are presented in Figure <ref>.\nFor the cube object, all teacher policies have successfully learned the task, but the single-camera view teacher policy \u03c0_t1cam had a lower performance than the multiple camera-view teacher policies \u03c0_t2cam and \u03c0_t3cam. \nMore importantly, for the mug object, the single-camera view teacher policy \u03c0_t1cam was unable to effectively learn the task.\nOverall, these results demonstrate the importance of utilizing multiple camera views for learning complex tasks more such as lifting asymmetric objects (eg. mugs) effectively. \n\nTable <ref> presents the results of evaluating the performance of teacher and student policies on lifting the cube and mug objects across 100 random front-camera views. \nAll results are averaged across three different runs.\nThe performances of six different policies are compared, including single-camera teacher policy \u03c0_t1cam, two-camera multi-view teacher policy \u03c0_t2cam, three-camera multi-view teacher policy \u03c0_t3cam, and three student policies \u03c0_s_t1cam, \u03c0_s_t2cam, and \u03c0_s_t3cam, distilled from the corresponding teacher policies. \nThe results indicate that the students distilled from multi-view teachers, \u03c0_s_t2cam and \u03c0_s_t3cam, outperform the students distilled from single-camera teachers, \u03c0_s_t1cam, as well as all the multi-view teacher policies \u03c0_t2cam and \u03c0_t3cam, in both cube and mug lifting tasks. \nNotably, the multi-camera view to single-camera view distillation approach enables the single-camera view student policy to learn and perform the task of lifting the mug object, despite the single front-view camera teacher was unable to accomplish this. \nThis highlights the effectiveness of the multi-camera view to single-camera view distillation in enhancing the capabilities of the single-camera view visual policy.\n\nAblation Studies: In addition to our main experiments, we conducted ablation studies to investigate the effect of feature alignment between the teacher and student observations. \nAs shown in Figure <ref>, we compared two different alignment methods: minimizing the euclidean distance between the features of the teacher and student (eq. <ref>) and aligning the pairwise similarity matrix of teacher observations with that of the student observations (eq. <ref>). \nOur results indicate that the former method provides better results, suggesting that direct feature alignment is more effective for knowledge distillation in our framework. \nThis analysis provides insights into the importance of feature alignment in knowledge distillation and its impact on the performance of the student policy.\n\n\n\n\n \u00a7.\u00a7 Real Robot Experiments\n\nTo validate the zero-shot policy transfer capability of the proposed framework, experiments were conducted in a real-world environment using the Panda robot with a Panda Hand gripper and an intel RealSense D435i RGBD camera as shown in Figure <ref>.  \nPolymetis robot control framework <cit.> was used to control the robot and the gripper position. \n\nThe visual policies trained for each of the aforementioned lifting tasks, i.e. mug and cube, in the simulation were directly transferred to the real-world setting without any further training.  \nFigure <ref> shows snapshots of the robot successfully executing these tasks with a random camera viewpoint.\n\nIn order to evaluate the performance of student policies, we tested them under random camera views for cube and mug tasks as shown in Figure <ref> and Figure <ref> respectively. \nAs \u03c0_s_t3cam had demonstrated overall better performance than \u03c0_s_t2cam in simulation, we selected it for evaluation in real-world experiments.\nTable <ref> and Table <ref> show the average success rates of cube and mug lifting tasks for different random views in these figures, respectively.\n The success rate is defined as the percentage of successful lifts out of the total number of trials.  \n\nThe results show that the student policy \u03c0_s_t3cam outperforms the student policy trained with a single camera view  \u03c0_s_t1cam in all camera viewpoints.  \nAs for the mug lifting task, there is no corresponding \u03c0_s_t1cam policy available as the single-camera view teacher was unable to learn the task in the simulation environment. \nNotably, the average success rates were lower than those obtained in simulation. This can be attributed to the domain gap between simulation and the real-world.\nThe single-view teacher policy, on the other hand,  was unable to lift the object in any of these random camera views, despite being trained in simulation with the data augmentation method as well.\n\n\nMoreover, we conducted an analysis to evaluate the impact of the robot state on the performance of the proposed approach. \n\nThe joint states of the robot, denoted by q\u2208\u211d^7 (representing seven degrees of freedom joint angles), were augmented with visual features to train both the teacher and student policies.\nThe results presented in Table <ref> demonstrate that incorporating robot joint angles leads to performance improvements for zero-shot policy transfer in a real-world setup.\nThese findings highlight the need for further research to bridge the gap between simulation and reality and improve the transferability of policies learned in simulation to real-world scenarios.\n\n\nThe video attachment[<https://youtu.be/CnDQK9ly5eg>]  demonstrates the successful lifting of a cube and mug objects by the robot from various random angles and positions.\nThe video showcases the robot performing the task from a front view and successfully lifting the objects consecutively, demonstrating the ability of the student policy to learn robust visual features and accurately and handle camera location perturbations in real-time.\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION AND FUTURE WORK\n\n\nIn conclusion, we proposed a multi-camera view to single-camera view distillation approach for enhancing the performance of a single-view camera robot policy. \nThe proposed method enables a single-view policy to learn from a pre-trained teacher policy, which has been trained with multiple camera viewpoints,  enabling the teacher policy to transfer accurate and robust features to the student policy.\nTo make the student policy robust against unseen visual camera configurations, we utilized a combination of data augmentation techniques and variations in camera location and parameters. \n\nThe experimental results demonstrated that our approach can enhance the performance of a single-view camera policy, and successfully lift the challenging object even when the single-camera view teacher policies fail.\nTherefore, this approach has the potential to make visual policy learning more practical and cost-effective in settings where using multiple cameras may not be feasible in the first place or where camera calibration is not required.\n\n\nFuture work will focus on investigating the use of point-cloud or deep image-based representations to further enhance the robustness and generalization capabilities of visual policy learning, potentially leading to improved performance in zero-shot sim-to-real transfer scenarios.\nBy incorporating more diverse set of objects and longer horizon complex tasks in our experiments, we seek to enhance the practicality and versatility of the proposed approach, making it suitable for a wide range of robotic manipulation applications in real-world scenarios. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-2cm   \n                                  \n                                  \n                                  \n                                  \n                                  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 ACKNOWLEDGMENT\n\n\nThis research was supported by grant no. A19E4a0101 from the Singapore Government\u2019s Research, Innovation and Enterprise 2020 plan (Advanced Manufacturing and Engineering domain) and administered by the Agency for Science, Technology, and Research.\nThis work is also supported by the A*STAR Computational Resource Centre through the use of its high performance computing facilities.\n\nieeetr\n\n\n"}