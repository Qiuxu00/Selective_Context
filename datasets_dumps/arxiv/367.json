{"entry_id": "http://arxiv.org/abs/2303.06822v1", "published": "20230313024908", "title": "Automatic Identification and Extraction of Self-Claimed Assumptions on GitHub", "authors": ["Chen Yang", "Zinan Ma", "Peng Liang", "Xiaohua Liu"], "primary_category": "cs.SE", "categories": ["cs.SE"], "text": "\n\n\n\n\n\n\n\n\n\n\nAutomatic Identification and Extraction of Self-Claimed Assumptions on GitHub\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChen Yang23, Zinan Ma2, Peng Liang4, Xiaohua Liu2\n    \n    2School of Artificial Intelligence, Shenzhen Polytechnic, Shenzhen, China\n\n    3State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China\n\n    4School of Computer Science, Wuhan University, Wuhan, China\n\n    {yangchen, 21680260, lxh}szpt.edu.cn, liangp@whu.edu.cn\n    \n\n    \n================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\nIn software development, due to the lack of knowledge or information, time pressure, complex context, and many other factors, various uncertainties emerge during the development process, leading to assumptions scattered in projects. Being unaware of certain assumptions can result in critical problems (e.g., system vulnerability and failures). The prerequisite of analyzing and understanding assumptions in software development is to identify and extract those assumptions with acceptable effort. \nIn this paper, we proposed a tool (i.e., SCAMiner) to automatically identify and extract self-claimed assumptions (SCAs) on GitHub projects. To evaluate the applicability of SCAMiner, we first presented an example of using the tool to mine SCAs from one large and popular deep learning framework project: the TensorFlow project on GitHub. We then conducted an evaluation of the tool. The results show that SCAMiner can effectively identify and extract SCAs from the repositories on GitHub.\n\n\n\nSelf-Claimed Assumption, GitHub, Mining Software Repositories\n\n\n\n\n\u00a7 INTRODUCTION\n \nAssumptions in the field of software development is a broad topic: different types of assumptions (e.g., requirement assumptions <cit.>, design assumptions <cit.>, and construction assumptions <cit.>) have been extensively discussed. For instance, in the early phases of software development, there could be many uncertain things. However, in order to meet the project business goals (e.g., schedule and deadlines), stakeholders have to work in the presence of such uncertainties; these uncertainties can lead to assumptions. In this paper, we advocate treating uncertainty and assumption as two different but related concepts: one way to deal with uncertainties is to make implicit or explicit assumptions (i.e., a thing that is uncertain, but accepted as true)\u00a0<cit.>. \n\nThe importance of assumptions and their management in software development has been highlighted in many studies and industrial cases. For example, Corbat\u00f3 <cit.> mentioned in his ACM Turing Award lecture that \u201cdesign bugs are often subtle and occur by evolution with early assumptions being forgotten as new features or uses are added to systems.\u201d Garlan et al. pointed out that incompatible assumptions in software architecture can cause architectural mismatch <cit.>. Lewis et al. also mentioned similar results in machine learning systems: since there are different types of stakeholders (e.g., data scientist, software engineer, and system user) of a machine learning system, they could make different but incompatible or invalid assumptions, leading to system misunderstanding, mismatch, etc <cit.>.\nIn October 2018, Lion Air Flight 610 crashed 13 minutes after takeoff and killed all 189 people on board; In March 2019, Ethiopian Airlines Flight 302 crashed and ended another 157 lives. According to the reports from the government, one critical reason of the 737 MAX crashes is regarding not-well managed assumptions <cit.><cit.>. In the report, they mentioned that the aircraft company made invalid assumptions about the critical system components. Specifically, the invalid assumptions regarding MCAS (Maneuvering Characteristics Augmentation System) are the root cause of the crashes. The report also pointed out the need of identifying and re-evaluating important assumptions in the system. \n\nAs mentioned by many researchers and practitioners, stakeholders constantly make assumptions in their work. For example, in the release of TensorFlow v2.3.0, there are 1,460 explicitly claimed assumptions (also called as self-claimed assumptions, SCAs) made by the developers in the code comments, and TensorFlow has more than 100 releases <cit.>. For example, in the sentence: \u201c[tf/xla] fixup numbering of xla parameters used for aliasing previously, the xla argument parameter was incorrectly assumed to be corresponding to the index in the vector of `xlacompiler::argument'\u201d, it includes an SCA of assuming that the xla argument parameter is corresponding to the index in the vector of xlacompiler::argument and the developer claimed that it was an invalid assumption.\nSCAs are related to many types of software artifacts, such as decisions, technical debt, and source code <cit.>. For example, in TensorFlow, there is an SCA: \u201cTODO: Looks like there is an assumption that weight has only one user. We should add a check here\u201d, which induces a technical debt.\nExisting research on assumptions and their management in software development usually use experiments, surveys, and case studies to manually identify and extract assumptions through observation, questionnaires, interviews, focus groups, and documentation analysis\u00a0<cit.>. Since such approaches have high costs (e.g., time and resources), the number of identified and extracted assumptions in those studies is often limited, leading to various problems of developing new theories, approaches, and methods of assumptions and their management in software development.\n\nIn this work, to overcome the issues of manually identifying and extracting assumptions in software development, we proposed a tool: SCAMiner, which can be used to automatically identify and extract SCAs on GitHub repositories. \n\nBesides SCAs, the tool can also be easily extended to other research fields (e.g., identifying and extracting technical debt\u00a0<cit.>). \n\nHow to access SCAMiner. SCAMiner is available at [<http://39.108.224.140>]. Users can register or use a guest account to login the tool. We also provided a deployment package for users who want to try the tool on their local environment <cit.>. Users can read and follow the instructions in the description for the deployment of the tool.\n\nThe remainder of the paper is organized as follows. Section <ref> provides related work, Section <ref> describes the details of SCAMiner, Section <ref> presents an example of using the tool, Section <ref> describes an  evaluation of SCAMiner, and Section <ref> concludes the paper with future directions.\n\n\n\n\u00a7 RELATED WORK\n \nIn the field of assumptions and their management in software development, most assumptions are manually identified and extracted by researchers and practitioners. Landuyt and Joosen focused on assumptions made during the application of a threat modeling framework (i.e., LINDDUN), which allows the identification of privacy-related design flaws in the architecting phase <cit.>. They conducted a descriptive study with 122 master students, and the students identified and extracted 845 assumptions from the models created by the students. \nYang et al. conducted an exploratory study of assumptions made in the development of nine popular deep learning frameworks (e.g., TensorFlow, Keras, and PyTorch) on GitHub <cit.>. They identified and extracted 3,084 assumptions from the code comments in over 50,000 files of the deep learning frameworks.\nXiong et al. studied assumptions in the Hibernate developer mailing list, including their expression, classification, trend over time, and related software artifacts <cit.>. In their study, they identified and extracted 832 assumptions. \nLi et al. developed a machine learning approach <cit.> to identify and classify assumptions based on the dataset constructed by Xiong et al. <cit.>, which can read the data (i.e., sentences) from the dataset (i.e., a .csv file), preprocess the data (e.g., using NLTK and Word2Vec), train classifiers (e.g., Perception, Logistic Regression, and Support Vector Machines), and evaluate the trained classifiers (e.g., precision, recall, and F1-score). However, their approach is not specifically developed for SCAs and cannot mine assumptions from other sources (e.g., GitHub repositories).\n\nCompared to the related work above, the tool (i.e., SCAMiner) proposed in this work focuses on GitHub repositories, and can automatically collect data (e.g., issues, pull requests, and commits) and identify and extract SCAs. SCAMiner can also be easily extended to work with other repositories (e.g., Stack Overflow) or other types of software artifacts (e.g., technical debt).\n\n\n\n\u00a7 SCAMINER\n \nSCAMiner is composed of four modules: Repository Management, Data Collection, Data Extraction, and System Management, as shown in Fig. <ref>.\n\nRepository Management includes (1) getting information of the repositories and their releases from the GitHub server, (2) searching and showing details of the repositories, (3) downloading source code of each release of the repositories, and (4) deleting all the data of specific repositories. \nAdding a repository using SCAMiner requires users to enter the owner (e.g., tensorflow) and name (e.g., tensorflow) of the repository. When adding a repository, SCAMiner first checks whether the repository exists in the MySQL database and the GitHub server. If all the checks pass, SCAMiner gets information (e.g., URL, releases, and tags) of the repository from the GitHub server and insert the data into the MySQL database. \n\nData Collection aims to (1) show the data models of Repository, Release, Tag, Pull Request (PR), Commit, and Issue, (2) search and show data collection information of repositories and collect issues, PRs, and commits based on the data models, (3) monitor data collection processes, and (4) show data collection history. \nThe data models are predefined and currently cannot be changed by users. The basic information of each repository (e.g., its releases and tags) and the information of the data collection processes are stored in a MySQL database, while the data of issues, PRs, and commits are stored in a MongoDB database.\nWhen collecting data from the GitHub server using SCAMiner, users can also set a time (default: 10 seconds) for automatically refreshing the data collection status (i.e., collecting, finished, and error). \nSCAMiner uses cursors (i.e., issue cursor, PR cursor, and commit cursor) to record each batch of the data downloaded from the GitHub server, and therefore SCAMiner supports continuing data collection after it has been stopped due to errors and exceptions (e.g., over the limits by GitHub).\n\nData Identification and Extraction is composed of three submodules: Data Search, Assumption Extraction, and Knowledge Graph. \nData Search aims to search and show specific issues, PRs, and commits based on keywords. \nIn data search, SCAMiner requires users to specify which repository (e.g., TensorFlow), data type (i.e., issue, pr, and commit), search scope (e.g., title), and keywords (e.g., assume) to search. For example, for issues of the TensorFlow project, a search scope can be title body comments.body, which means that SCAMiner will search data within the scope of the title of the issues, the body of the issues, and the body of the comments of the issues in the TensorFlow project. \nFor keywords, SCAMiner supports AND (i.e., using double quotation marks, e.g., \u201cassume\" \u201csoftware\") and OR (i.e., without quotation marks, e.g., assume software).\nThe search terms are highlighted in the search results. If the description of a data item is too long, users can click on the \u201cdetail\" button to see the full information of the item.\nAssumption Extraction contains four functions: SCA Identification, SCA Extraction, PA (Potential Assumption) Identification, and PA Extraction. \nIn the SCA Identification function, we used a keyword-based search approach for identifying SCAs (i.e., word level), based on the assumption related search terms (i.e., assumption, assumptions, assume, assumes, assumed, assuming, assumable, and assumably) and the following search scope: (1) title, body, body of comments of issues, (2) title, body, body of comments of PRs, and (3) message of commits. \n\nSince the results from using the SCA Identification function are at the word level (i.e., highlighting the matched terms), in the SCA Extraction function, SCAMiner provides support for locating, matching, and extracting the related sentences that include the matched terms (i.e., at the sentence level). As most of the description (e.g., description of an issue) is created and edited by GitHub users, there could be punctuation problems existing in the description (e.g., a sentence does not have a \u201c.\" or \u201c.\" is used in the source code, such as \u201ca.b\"), which may lead to errors in the separation of the sentences. Therefore, we manually identified the patterns of SCA description from the projects on GitHub, and implemented the patterns in the SCA Extraction function to extract SCA sentences. \n\nMoreover, there are sentences without using assumption related keywords, but could act as assumptions (we call them potential assumptions, PAs). Though PAs are not SCAs, they can be further reviewed by stakeholders and transformed to SCAs (as the inputs for SCA extraction). Therefore, SCAMiner also provides support for identifying and extracting such assumptions through the PA Identification and PA Extraction function, which complement SCA identification and extraction. We followed the guidelines of assumption identification proposed in <cit.>, and manually collected and labeled 35,855 sentences from the issues, PRs, and commits of multiple repositories (e.g., Keras and Theano), constructed a dataset for PAs, fine-tuned a deep learning model based on ALBERT (a lite BERT, which architecture is based on BERT), and trained and adjusted a classification model for PA identification <cit.>. The reason of choosing ALBERT is because ALBERT is one of the most powerful language models, which can achieve good performance with fewer parameters (compared to BERT) in many tasks, such as the binary single-sentence classification task <cit.>. \nSince we are using deep learning models and identifying PAs at the sentence level, the identification process could be rather slow on CPUs (e.g., it may take hours/days on our server to identify PAs, depending on the amount of data to be processed). Therefore, we used a queue (i.e., a waiting list with a first-in first-out strategy) on the server to manage the tasks of identifying PAs. In the PA Extraction function, SCAMiner organizes the PAs (sentences) identified from the PA Identification function into a file, and users can download the file for further review. \nKnowledge Graph supports both traditional knowledge graph and dynamic knowledge graph. SCAMiner provides three dimensions (i.e., release, month, and day) to construct the timeline of the data. For each repository, SCAMiner creates and connects entities according to the timeline and their states. For example, a pr can be published, merged, and closed, and therefore SCAMiner creates three connected entities if they are within a timeline. \n\nSystem Management supports user registration, login, and logoff, and provides access control and system logs.\n\n \n\nThe architecture of SCAMiner is shown in Fig. <ref>. When a user clicks on a menu or button, the Web component organizes the data, generates a request, and sends it to the Controller component through the Python Interface component. The Controller component analyzes the request: \n(1) If the request is regarding getting data from the GitHub server, the Controller component organizes the data and calls the functions in the GitHub Service component. The GitHub Service component reads the GitHub configuration (stored in the system) and organizes queries based on the predefined data models. Then the GitHub Service component sends requests to the GitHub server, gets responses from the GitHub server, analyzes the responses, and sends back the data to the Controller component. \n(2) If the request is regarding interacting with the MySQL or the MongoDB database, the Controller component organizes the data and calls the functions in the Data Service component. The Data Service component further organizes the data and calls the functions in the DAO (Data Access Object) component, which implements the interaction with the MySQL or the MongoDB database. The DAO component reads the database configuration (stored in the system), communicates with the databases, gets the results from the databases, and sends them back to the Data Service component. The Data Service component sends the data getting from the databases back to the Controller component. \n(3) If the request is regarding using the trained model (based on ALBERT) to identify PAs, the Controller component organizes the data and calls the functions in the Data Service component. The Data Service component preprocesses the data, loads the trained model, and uses the model to identify PAs. The results are then sent back to the Controller component. \nFinally, the Controller component organizes the data getting from GitHub, the MySQL database, the MongoDB database or the deep learning model, sends the data back to the Web component through the Python Interface component, and then the Web component shows the results to the user.\n\n \n\n\n\n\u00a7 USING SCAMINER\n \n\n\nIn this section, we walk the usage of SCAMiner through an example: the TensorFlow project on GitHub. TensorFlow is one of the most popular deep learning frameworks, which is widely used in many deep learning systems and application domains. \n\nUsers need to register an account and set a personal access token of GitHub\n\n\nwhen using SCAMiner. The token is used to access the GitHub Application Programming Interface (API), since SCAMiner needs to communicate with the GitHub API to get data (e.g., issues, PRs, and commits). We also provide a default token for SCAMiner users. However, since GitHub has limitations in place to protect against excessive or abusive calls to GitHub servers (e.g., the rate limit is 5,000 points per hour and individual calls cannot request more than 500,000 total nodes)[<https://docs.github.com/en/graphql/overview/resource-limitations>], using the default token may lead to errors in data collection because of these limitations. \nAfter registration of the SCAMiner account, users can login SCAMiner with the account. \nBelow is the process of using SCAMiner to identify and extract SCAs from the TensorFlow project on GitHub.\n\nCreate the TensorFlow repository.\nUsers need to click on the Repository Management module, then click on the \u201cAdd\" button, enter the owner as \u201ctensorflow\" and the name as \u201ctensorflow\", and click on the \u201cSave\" button to create the TensorFlow repository on SCAMiner. For each release of a repository, SCAMiner provides users a link to download the source code in the Repository Management module (this is an optional step). Then users can use tools such as Visual Studio Code and PyCharm to further browse the code and search SCAs in the code. \n\nCollect issues, PRs, and commits on TensorFlow.\nAfter the TensorFlow repository is created on SCAMiner, users can further use the Data Collection module to collect issues, PRs, and commits of the TensorFlow repository. \nUsers can start multiple tasks simultaneously, but this could cause errors because of the limitation by GitHub.\n\nIdentify and Extract SCAs on TensorFlow.\n\nIn the Assumption Extraction submodule of the Data Extraction module, users need to select the TensorFlow repository and a data type, and click on the \u201cSCA Identification\" button. SCAMiner will show the results and highlight all the SCAs. \nUsers can further extract the data to a csv file to construct a dataset by clicking on the \u201cSCA Extraction\" button. The first line in the csv file is the title, indicating the repository and type (i.e., issue, PR, and commit) of the extracted data.\n\n\nIdentify and Extract PAs on TensorFlow.\nIn the Assumption Extraction submodule of the Data Extraction module, users need to select the TensorFlow repository and a data type, and click on the \u201cPA Identification\" button. SCAMiner will show the results and highlight all the sentences that may include a PA. After PA identification, users can click on the \u201cPA Extraction\" button to download the results of PA identification for further review.\n\nGenerate a knowledge graph of the SCAs on TensorFlow.\nThis step is optional. Users need to select the TensorFlow repository and a dimension (i.e., release, month, or day) to construct a knowledge graph of SCAs based on the chosen dimension. \n\nThe aforementioned results can be further used in various context. For example, through identifying assumptions, users may better understand what was assuming in a certain project and deal with such uncertainty in their future work.\n\n\n\n\u00a7 EVALUATION OF SCAMINER\n \nWe conducted an evaluation on data collection, SCA identification, SCA extraction, PA identification, and PA extraction, as shown in Fig. <ref>. \n\nThe output of data collection is the input of SCA identification and PA identification, the output of SCA identification is the input of SCA extraction, and the output of PA identification is the input of PA extraction.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Evaluation of Data Collection\n \nWe conducted an evaluation of using SCAMiner to collect issues, pull requests (PRs), and commits on seven GitHub repositories: Caffe, CNTK, Theano, DeepLearning4J (DL4J), MXNet, Keras, and TensorFlow, regarding the completeness and performance of SCAMiner on data collection. We first created an issue data model (including repository name, title, ID, author, URL, labels, state, body, and comments), a PR data model (including repository name, owner, title, ID, author, URL, labels, state, body, comments, and reviews), and a commit data model (including repository name, owner, OID, author name, author email, committed date, URL, and message). The data items of each data model were selected from the GraphQL API on GitHub[<https://docs.github.com/en/graphql/reference/objects>]. \n\nThe configuration of the server we used for the evaluation of the data collection is: (1) CPU: Intel(R) Xeon(R) Platinum 8255C CPU @ 2.50GHz, 2 cores, (2) Memory: 2GB, (3) \nHard Disk: 40GB SSD, (4) Operation System: Linux VM-20-4-centos 3.10.0-1160.45.1.el7.x86_64 #1 SMP. The results of using SCAMiner to collect data are shown in Table <ref>.\n\n\n\nThe results show that SCAMiner can effectively collect issues, PRs, and commits of the repositories on GitHub. Certain repositories (e.g., TensorFlow) may be frequently updated, and there is a need to construct a mechanism for continuously collecting data.\n\n\n\n \u00a7.\u00a7 Evaluation of SCA Identification\n \nAfter data collection (as mentioned in Section <ref>), we used the collected data (i.e., issues, PRs, and commits) of the Keras and TensorFlow repository to conduct an evaluation on identifying SCAs using SCAMiner (i.e., the SCA Identification function).   \nThe first author manually checked the identified results to classify them as SCAs or non-SCAs. The evaluation results of SCA identification are shown in Table <ref>. \n\nThe count of the identified SCAs could be larger than the search results (e.g., count of messages in the commits of the Keras repository), since each issue, PR, or commit may include multiple SCAs. For example, an issue \nof Keras mentions: \u201cAssume we are trying to learn a sequence to sequence map. For this we can use Recurrent and TimeDistributedDense layers. Now assume that the sequences have different lengths. We should pad both input and desired sequences with zeros, right? But how will the objective function handle the padded values? There is no choice to pass a mask to the objective function. Won't this bias the cost function?\", which includes two SCAs: \u201cassume we are trying to learn a sequence to sequence map\" and \u201cassume that the sequences have different lengths\".\n\nSince SCAMiner used a keyword-based (i.e., the assumption related terms) search approach for SCA identification, it could go wrong in certain context (e.g., a variable in a code snippet named assume). \nMoreover, we also found that certain SCAs lack details. For example, an issue \nof Keras mentioned: \u201cstrict enforcement of user input assumptions, and raising of helpful error messages.\" However, we cannot understand what exactly the user input assumptions are. These SCAs need to be further processed by SCAMiner (e.g., add warnings in the results).\n\n\n\nThe results show that SCAMiner can correctly identify 94.92% SCAs (1,961 SCAs out of 2,066 SCAs) from the issues, PRs, and commits of the Keras and TensorFlow repository. Certain variables and functions named as for example assume exist in issues, PRs, and commits, which needs to be further processed by SCAMiner.\n\n\n\n \u00a7.\u00a7 Evaluation of SCA Extraction\n\nWe further evaluated whether SCAMiner (i.e., the SCA Extraction function) can correctly extract SCAs (i.e., at the sentence level) using the identification results (i.e., 1961 identified SCAs) of the Keras and TensorFlow repository from <ref>. The first author manually checked the extracted results to classify them as correct extraction and missed extraction. The results are shown in Table <ref>. As an example, there are 298 SCAs in the body of Keras issues. SCAMiner correctly extracted 290 of the SCAs, but missed 8 SCAs.\n\n\n\nThe results show that SCAMiner can correctly extract 97.55% SCAs (1,913 SCAs out of 1,961 SCAs) from the issues, PRs, and commits of the Keras and TensorFlow repository. Certain structures of the issues, PRs, and commits (e.g., \u201cassume\" and \u201cassumption\" exist in one sentence) may lead to errors in SCA extraction, which needs further investigation and improvements.\n\n\n\n \u00a7.\u00a7 Evaluation of PA Identification\n\nFor PAs, we manually labeled 35,855 sentences from the issues, PRs, and commits of multiple repositories (e.g., Keras and Theano), and constructed a dataset, containing a training set and a test set with a data proportion of 8:2 from the labeled sentences. We created a vocabulary and tokenized the data from the dataset based on the vocabulary. Then we constructed the deep learning model (based on ALBERT), trained the model for 50,000 epochs with a batch size of 32 and a learning rate of 2e-5. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe results show that the best accuracy of identifying PAs using SCAMiner on the test set is 0.9451 on Epoch 22,000. Considering repositories may have various context (e.g., different development policies), there is a need to further extend the dataset to include more data from the repositories, which will help to improve the generalization ability of SCAMiner in identifying PAs.\n\n\n\n\u00a7 CONCLUSIONS\n \nAssumptions and their management are important in software development. The prerequisite of analyzing and understanding assumptions in software development is to identify and extract those assumptions with acceptable effort. To this end, we proposed SCAMiner to automatically identify and extract SCAs on GitHub projects. Besides providing a running example of using SCAMiner on the TensorFlow project, we also evaluated the performance of SCAMiner, and the results show that SCAMiner can effectively identify and extract SCAs from the repositories on GitHub. \nSCAMiner can be potentially used for the research topics regarding assumptions and their management in software development, such as assumption making, evolution, evaluation, and reasoning.\n\nFor future work, the following aspects of SCAMiner can be further optimized: (1) There is a need to construct a mechanism for continuously collecting data using SCAMiner. (2) The identification of SCAs and PAs can be further optimized (e.g., develop new deep learning models, construct a larger dataset and train deep learning models based on the dataset). (3) Certain patterns of the issues, PRs, and commits (e.g., a variable named \u201cassume\") may lead to incorrect SCA identification and extraction, which can be further addressed in SCAMiner. (4) Assumptions are related to various types of software artifacts (e.g., requirements, design decisions, and technical debt). Automatically recovering the relationships between assumptions and such artifacts in SCAMiner is a promising future direction.\n\n\nIEEEtran\n\n\n"}