{"entry_id": "http://arxiv.org/abs/2303.06689v1", "published": "20230312153603", "title": "Self-planning Code Generation with Large Language Model", "authors": ["Xue Jiang", "Yihong Dong", "Lecheng Wang", "Qiwei Shang", "Ge Li"], "primary_category": "cs.SE", "categories": ["cs.SE"], "text": "\n\n\nProjectability\n  disentanglement for accurate and automated electronic-structure Hamiltonians\n    Nicola Marzari\n    March 30, 2023\n=============================================================================================\n\n\n\n\n\nAlthough large language models have demonstrated impressive ability in code generation, they are still struggling to address the complicated intent provided by humans. \nIt is widely acknowledged that humans typically employ planning to decompose complex problems and schedule the solution steps prior to implementation.\nThus we introduce planning into code generation to help the model understand complex intent and reduce the difficulty of problem solving.\nThis paper proposes a self-planning code generation method with large language model, which consists of two phases, namely planning phase and implementation phase. Specifically, in the planning phase, the language model plans out the solution steps from the intent combined with in-context learning. Then it enters the implementation phase, where the model generates code step by step, guided by the solution steps. \nThe effectiveness of self-planning code generation has been rigorously evaluated on multiple code generation datasets and the results have demonstrated a marked superiority over naive direct generation approaches with language model. The improvement in performance is substantial, highlighting the significance of self-planning in code generation tasks.\n\n\n\n\n\u201cThe art of programming is the art of organizing complexity.\"Edsger W.Dijkstra\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nProgramming is a pervasive and powerful tool for problem-solving. As one of the most central problems in programming theory, code generation allows machine to automatically programming to satisfy human intent expressed in the form of some specification. \n\nIn recent years, code generation has achieved great progress in both academia and industry <cit.>. In particular, large language models (LLMs) <cit.> demonstrates impressive code generation abilities, attracting attention from various fields such as artificial intelligence, natural language processing (NLP), and software engineering.\n\nIn code generation, the human-provided intent is usually a natural language description of \"what to do\" problem, while the model solves the problem by generating \"how to do\" code. When the intent is straightforward, it is easy to map to the solution, which can be well handled by state-of-the-art code generation models. However, as the problem becomes complicated and scaled, directly generating complex code satisfying intent is a challenge for both people and models, even LLMs. The human experience is that outlining a plan in advance and then completing the entire code step by step following the plan will make things easier. Inspired by this, we desire to incorporate planning into code generation. Plan-aided code generation has the following two benefits. 1) It can abstract the problem and provide instructions for solving it, which helps the model understand how to generate code. 2) It breaks down the complex problem into several easy-to-solve subproblems, which reduces the difficulty of problem solving. Therefore, the earlier plan can guide the generation of the correct code for complex problems. \n\n\nHowever, plan-aided code generation presupposes the existence of an approach for converting intent into plan. If we build such an approach from scratch, it requires a large amount of resources to label intent-plan pairs for training. In-context learning provides an important way of using LLMs without training. Through in-context learning, we can achieve planning for code generation by simply prompting the LLMs with a few examples demonstrating the task, rather than fine-tuning individual model checkpoints.\n\n\n\n\n\n\nCode generation can be viewed as a complex reasoning task. In NLP landscape, Chain of thought (CoT) prompting <cit.> achieves a significant improvement that transcends the scaling laws by simulating human thought processes to address language reasoning tasks, such as mathematical <cit.>, commonsense <cit.>, and symbolic reasoning <cit.>.\nNevertheless, CoT is quite different from the plan and the example is attached in Appendix <ref>.\nThe chain of thought in CoT is defined as concrete reasoning steps toward the goal, which is almost at the same level as code that is considered as an abstraction of human thought,\n\nso using CoT cannot reduce the difficulty of generating code. Moreover, CoT usually fails to handle complex problems, since complex problems will lead to a surge of reasoning steps. In contrast, planning is consistent with the methodology of dealing with problem complexity in requirements engineering, i.e. abstraction and decomposition <cit.>.\n\n\nIn this paper, we propose a self-planning code generation method with LLMs that exploits the planning capabilities of LLMs themselves to facilitate code generation. Self-planning code generation consists of two phases:\n1) Planning phase, LLMs generate plans for problems at test time by providing only a few intent-to-plan demonstrations as examples in prompting;\n2) Implementation phase, LLMs generate code that satisfies the intent step by step, guided by the plan.\nSelf-planning code generation leverages in-context learning to generate plans autonomously without annotating plan corpus and extra training. Empirical evaluations have provided evidence that the self-planning approach yields accurate sub-problem decomposition and well-considered steps that enable the model to solve complex problems.\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 SELF-PLANNING\n\nIn self-planning code generation, we propose to perform self-planning before models generate code, and the process can be divided into two phases.\n\n\n    \n  * Planning phase   We take advantage of the ability of LLMs to perform self-planning by in-context learning.\n    Given a prompt C that consists of k tuples concatenated together \u27e8 x_1 \u00b7 y_1\u27e9\u2225\u27e8 x_1 \u00b7 y_1\u27e9\u2225...\u2225\u27e8 x_k \u00b7 y_k\u27e9, where x denotes the human intent, y denotes the plan, \u00b7 indicate the concatenation of both. Plan is a scheduling of subproblems that abstract and decompose from intent, which is set to y = {q_1,q_2,...,q_n}. During inference, the test-time intent will be concatenated after the prompt, and C\u2225 x_test will be fed into the LLMs \u2133, which will attempt to do planning for the new intent. Thus we can obtain the test-time plan y_test.\n    \n    Note that k is a fairly low number, which means that we can achieve self-planning by annotating only a few examples demonstrating planning.\n    \n    \n  * Implementation phase   In this phase, we append the plan y_test to the intent x_test as input for the model \u2133 to obtain the final code z_test.\n\n\nThe above two phases can be formalized as the following equation.\n\n    \ud835\udcab(z|x,C)    = \u2211_y \ud835\udcab(z|y,x,C)\u00b7\ud835\udcab(y|x,C),\n       \u221d\ud835\udcab(z|y,x,C)\u00b7\ud835\udcab(y|x,C),\n\n\nWe further simplify \ud835\udcab(z|y,x,C) = \ud835\udcab(z|y,x) via conditional independence assumptions, thus:\n\n\n    \ud835\udcab(z|x,C) \u225c\ud835\udcab(z|y,x)_Implementation phase\u00b7  \ud835\udcab(y|x,C)_Planning phase,\n\n\nExample  An example of self-planning code generation is shown in Fig.<ref>. \nIn the planning phase, human provides an intent to find the n-th number that is a Fibonacci number and it's also prime. LLM abstracts two subproblems from the intent, i.e., generating a Fibonacci sequence and determining if a number is prime, and plans four steps to solve the subproblems combinatorially.\nThen entering the implementation phase, we append the plan to the intent and feed it to LLM. LLM generates code under the navigation of the steps, and surprisingly, it wraps \"determine if a number is prime\" into a subfunction and calls it.\n\n\nIn direct code generation, in contrast, LLM cannot understand that the intent is a combination of multiple problems; it knows to write something about \"prime\" and \"Fibonacci\", but actually, \nit generates a confusing code, i.e. it enumerates the first five correct samples [This is related to the fact that we provided five public test cases at test time and the model copied them.] and then calculating the Fibonacci numbers, completely losing the requirement to determine whether it is a prime number.\n\nCrafting prompts for self-planning  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn our approach, the design of prompts follows specific principles to facilitate the efficient and rapid development of the user-defined plan for code generation. To aid in this process, we present a guideline for crafting prompts.\n\n\n\n  * Only essential steps are necessary for prompt. Detailed explanations in the prompt can obstruct the model's ability to decompose the task and should be avoided. The model can independently perform the necessary details to fulfill the intent.\n\n\n  * Clearly specify a set of rules. Ensure that all examples in the prompt exhibit a consistent pattern, such as beginning with a verb or using consistent language constructs, such as \"if,\" \"check,\" \"return,\" \"create,\" etc. \n\n\n  * Select examples that represent the majority of cases. It is important not to include examples that are atypical or marginal as this may negatively impact the performance of the model. The selected examples should be of average length and representative of typical tasks and solutions. \n\n\n  * Incorporate code features. The desired functionality, such as the definition of helper functions and the importation of libraries, can be introduced into the prompt through the use of directive statements.\n\n\n\n\n\n\u00a7 EXPERIMENT SETUP\n\n\n\n \u00a7.\u00a7 Benchmarks\n\nWe conduct a broad evaluation on code generation benchmarks to demonstrate the effectiveness of self-planning. \n\n\nMBPP <cit.>  This benchmark, referred to as \"Mostly Basic Programming Problems\", contains nearly 1000 crowd-sourced python programming problems, covering programming fundamentals, standard library functionality, and more. Each problem in the benchmark consists of a NL description, a code solution, and 3 automated test cases. A portion of the manually verified data is extracted as \"MBPP-sanitized\". For MBPP, which does not include function signatures, only the NL description is provided as input.\n\nHumanEval <cit.>  This benchmark is a set of 164 handwritten programming problems, proposed by OpenAI. Each problem includes a function signature, NL description, function body, and several unit tests, with an average of 7.7 tests per problem. For HumanEval, function signature, NL description, and public test cases are provided as input.\n\n\n\nFurthermore, we utilize an expanded version of MBPP and HumanEval\n\n, which includes over 100 additional test cases per task, to reinforce the validity of code evaluation <cit.>. This extended version is referred to as MBPP-ET and HumanEval-ET.\n\n\n\n\n \u00a7.\u00a7 Prompting\n\nOur approach and prompt-based baselines adopt 8-shot prompting as the default setting. The examples selected from the dataset for prompting will be excluded from the evaluation. Further details of all prompts can be found in Appendix. \n\n\nIn this paper, we adopt code-davinci-002 as our base model.\nThree baselines were compared: Direct, CoT, and Ground-truth Planning.\nThe direct means generating code directly using LLMs, without prompting or fine-tuning. For comparison with CoT approach, we manually create detailed chain steps for CoT on the code generation tasks. To investigate the advantages of the planning approach, we constructed pseudo ground-truth plan by prompting LLMs to produce plan based on ground-truth code. \n\n\n\n \u00a7.\u00a7 Metrics\n\nTo assess the accuracy of the generated code, we employ two metrics: an execution-based metric, namely Pass@k, and a match-based metric, namely CodeBLEU.\n\nPass@k  Pass@k measures the functional correctness of the generated code through executing test cases. We use the unbiased version <cit.> of Pass@k, where n>=k samples are generated for each problem, count the number of correct samples c<=n which pass test cases and calculate the following estimator,\n\n\n\n    Pass@k = \ud835\udd3c_Problems[ 1-[ n-c;   k ]/[ n; k ] ].\n\n\n\n\nCodeBLEU  CodeBLEU <cit.> is a variant of BLEU that injects code features for code evaluation. CodeBLEU considers abstract syntax tree and dataflow matching in addition to n-gram co-currence (BLEU),\n\n    CodeBLEU     = \u03b1\u00b7BLEU + \u03b2\u00b7BLEU_w e i g h t\n        + \u03b4\u00b7Match_ast + \u03b6\u00b7Match_df.\n\n\n\n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n\n\n\n \u00a7.\u00a7 Main Results\n\nThe main results are summarized in Table <ref>, which demonstrate a significant effect of self-planning code generation. The experimental results suggest that obtaining the plan or CoT from the intent can provide a noteworthy advantage in code generation compared to the direct generation of code from the intent. As anticipated, our proposed self-planning method demonstrates state-of-the-art performance, showing a significant improvement in Pass@1 over the CoT method across all datasets. \n\nWe find that the improvement of the CoT approach over direct code generation is marginal, as the challenge of generating an accurate and sufficiently detailed CoT is comparable to that of direct code generation.\nMoreover, we evaluated the impact of utilizing the ground truth code to plan (i.e., ground truth planning) in facilitating code generation. \n\nThis approach simulates to some extent the ground truth planning provided by developers and provides an understanding of the approximate upper bound (which is actually low) of the self-planning approach. \nThe results in Table <ref> indicate a substantial improvement in the use of ground truth planning, as evidenced by a relative improvement of over 30% on the MBPP-sanitized dataset. The results of our study provide strong evidence of the effectiveness of the proposed method in achieving superior performance.\n\n\n\n\n\n\n \u00a7.\u00a7 Variants of Self-planning\n\nIn table <ref>, we present an in-depth analysis of the effects of several variations in self-planning strategies. First, we investigate the utilization of multi-turn as opposed to single-turn self-planning. The multi-turn approach involves the iterative use of solution steps of plan to generate the corresponding code, while the single-turn approach involves the employment of all steps (i.e., plan) to generate the entire code in a single iteration. We find that the multi-turn usually fails to generate correct codes. This can be ascribed to two possible causes: 1) there is a lack of one-to-one correspondence between the code snippets and steps, with the implementation of a solution step possibly interspersed with multiple code snippets; 2) LLM faces challenges in determining the termination point for code generation with a sub-plan. Since the final goal or intent is specified at the outset, this often results in the model persistently generating the entire code without considering the indicated end of a step, making it difficult to determine where to truncate the code. Second, the self-planning approach, when implemented as a one-phase process, has been shown to yield improved performance compared to the two-phase way. However, this improvement is achieved at the cost of increased complexity in the prompt requirements. Specifically, the two-phase way only requires the provision of intent and plan examples in the prompt, whereas the one-phase way necessitates the additional specification of correct code examples. As such, it can be argued that the one-phase self-planning approach constitutes a trade-off between improved performance and heightened complexity in construction, relative to the two-phase way.\n\n\n\n\n\n \u00a7.\u00a7 Robustness of Self-planning\n\nIn this section, we present an analysis of the robustness of self-planning by examining its effect under different quantitative and stylistic configurations of prompts.\nIn Table <ref>, we can observe that the performance of self-planning with n-shot improves as the value of n increases. However, it is crucial to consider the input length limit of LLMs (typically 2048 or 4096). As a result, it is not feasible to indefinitely increase the value of n without exceeding the input length limit. Based on this consideration, we generally recommend using either 8-shot or 4-shot for self-planning in LLMs. Furthermore, we explore the effect of self-planning with extremely concise plan. Although its effect is improved compared to directly generating code, it still has a noticeable gap in comparison to the standard plan. This discrepancy may stem from the challenges faced by LLMs in understanding extremely concise plan, which usually deviates from common human cognitive patterns (composed of only a few phrases or verbs). Therefore, we argue that a proper plan should conform to the expected output of LLMs and align with human writing conventions.\nOur findings provide evidence of the stability and dependability of the self-planning method.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Case Study\n\nTo validate the effectiveness of our approach, we conducted case studies with qualitative evaluations. As depicted in Fig.<ref>, we demonstrate the performance of both self-planning and direct code generation through three case studies. In these cases, the direct code generation approach only addresses a limited aspect of the intent, which often results in the generation of incorrect code. In contrast, the self-planning code generation method first converts the intent into plan, and then systematically resolves each solution step of plan, which is relatively straightforward to resolve. This methodology reduces the complexity of code generation and effectively minimizes the risk of overlooking critical elements. \n\n\nIn case 1, the task of LLMs is \u201cGiven an array of integers nums, find the minimum sum of any non-empty sub-array of nums\u201d. The code generated directly by LLM only considers a subset of the sub-arrays, whereas our approach ensures that none of them are overlooked.\nIn case 2, the task of LLMs is \u201cReceive an integer as input and return the special factorial of this integer.\u201d The direct code generation simply implements the standard factorial in a recursive manner, neglecting the definition of the special factorial. In contrast, our method implements the standard factorial through the use of sub-functions and subsequently utilizes these sub-functions to construct the special factorial.\nIn case 3, the task of LLMs is \u201cGiven the lengths of the three sides of a triangle, returns whether the triangle is a right triangle.\u201d Our approach is more comprehensive as it calculates the different sides as hypotenuses by applying the Pythagorean theorem, whereas direct code generation only considers a single case. Additionally, our method also evaluates whether the input sides form a triangle.\nOverall, our approach offers a more thorough and nuanced approach to solving the tasks assigned to LLMs, whereas direct code generation provides a more straightforward and limited solution.\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n \u00a7.\u00a7 Code Generation\n\nTraditional code generation approaches are based on supervised learning, which initially treats code as equivalent to natural language and then gradually incorporates more code-specific features.\n<cit.> first propose to employ sequence-to-sequence models for transformation between natural language to code snippets. <cit.> propose TranX, which maps natural language descriptions to an abstract syntax tree of code based on a series of tree construction actions. <cit.> presents a generative modeling approach for source code that uses a static-analysis tool. <cit.> devise a PDA-based methodology to guarantee grammatical correctness for code generation. \n\nDeveloping a planning-based code generation approach be challenging due to the requirement of a large amount of training data to enable the model to learn planning, and obtaining plans is a difficult task.\n\nWith the advent of CodeBERT <cit.>, the boom in pre-training with code corpus using large language models began. Codex <cit.>, InCoder <cit.>, Codegen <cit.>, and AlphaCode <cit.> have successively emerged, the number of parameters of the models has rapidly increased, and the performance of code generation has improved tremendously. The advent of large models has brought us a new way of tackling planning for code generation.\n\n\n\n\n \u00a7.\u00a7 Prompting Techniques\n\nFew-shot prompting is a technique that emerged as the number of model parameters exploded. Instead of fine-tuning a separate language model checkpoint for each new task, few-shot prompting can be utilized by simply providing the model with a limited number of input-output examples that illustrate the task.\nPrompting techniques like chain-of-thought prompting (COT) <cit.> have further boosted the performance of LLMs and inspired some further improvements. \nTo address natural language reasoning tasks, <cit.> propose least-to-most prompting which reduces a complex problem into a list of sub-problems and then solves these sub-problems in order. <cit.> propose to accompany generating code alongside the intermediate reasoning steps and subproblems in the original CoT and least-to-most prompting, delegating solving to the compiler, thus improving solution accuracy.\n\nHowever, the tasks faced by those approaches are simple and have fixed solving patterns, making them unusable for code generation. \nOur self-planning approach is designed for code generation that can abstract sub-problems from complex human intent and then plan solving steps that may not be sequential but intertwined. \n\n\n\n\u00a7 DISCUSSION AND FUTURE WORK\n\n\nDuring the practice of self-planning, we discovered some problems that need to be further addressed. Our plan-based approach is still bound by the lack of information in the provided intent. Adding more information, such as usage examples, or detailed descriptions, can improve the performance of our approach. Allowing the model to determine where are missing or ambiguous in intent, and interactively asking users for additional information, can make complex requirements easier to solve.\n\n\n\n\n\nAnother problem is the unstable performance of self-planning, as sensitivity to examples has a significant impact on the prompting approach, making it difficult to control what the model learns from the prompt. A common situation is that the model tends to replicate irrelevant steps in the prompt if it does not have enough confidence to generate a plan. How to make the model induct the patterns which need to be inherited from the prompt and placed in the right place, avoiding blind copying, is a worthy problem to investigate.\n\nFinally, in our work, code generation is used to address the intent proposed by humans. Taken further, lots of real-world problems can be solved by building solutions through programs. Programs have many desirable features over natural languages, such as logic, non-ambiguity, long-distance dependencies, symbolic mapping, and arithmetic ability. Recently, some studies \n\npoint out that language models trained based on programs have stronger language reasoning abilities than those trained based on purely natural language. An intuitive way is to solve real-world problems through a method similar to our paper, such as planning the NL steps based on code, so that the users can get more logical and well-thought-out solutions. Solving real-world problems with programs should be a promising direction to explore.\n\nThis paper attempts to reduce the difficulty of code generation by analyzing and processing human intent, which is consistent with the thought of requirement engineering. The current LLMs are capable of generating code that address simple human requirements, however, it is still a long way from producing a fully functional piece of software. The requirements of software development are significantly more complex and intricate. It may be worthwhile to explore beyond code writing to the realm of requirements analysis, incorporating the methodology of requirements engineering with LLMs. \n\n\n\n\n\u00a7 CONCLUSION\n\nWe explore plan-based code generation methods and propose a simple method to perform self-planning and generate code with LLMs. Self-planning code generation outperforms na\u00efve direct generation on multiple code generation datasets by a large margin. Beyond that, we discuss several promising future works that we hope will move code generation, and even software engineering one step forward. \n\nunsrtnat\n\n\n\n\n\n\u00a7 APPENDIX\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Self-planning Prompt for HumanEval\n \ndef encrypt(s):\n \n        \u201d'\n\n        Create a function encrypt that takes a string as an argument and returns a string encrypted with the alphabet being rotated. The alphabet should be rotated in a manner such that the letters shift down by two multiplied to two places.\n\n        For example:\n\n        encrypt('hi') returns 'lm'\n\n        encrypt('asdfghjkl') returns 'ewhjklnop'\n\n        encrypt('gf') returns 'kj'\n\n        encrypt('et') returns 'ix'\n\n        Let's think step by step.\n\n        1. Create a alphabet, bias two places multiplied by two.\n\n        2. Loop the input, find the latter bias letter in alphabet.\n\n        3. Return result.\n\n        \u201d'\n\n    \n\n\ndef check_if_last_char_is_a_letter(txt):\n\n        \u201d'\n\n        Create a function that returns True if the last character of a given string is an alphabetical character and is not a part of a word, and False otherwise. Note: \"word\" is a group of characters separated by space.\n\n        Examples:\n\n        check_if_last_char_is_a_letter(\"apple pie\") \u2192 False\n\n        check_if_last_char_is_a_letter(\"apple pi e\") \u2192 True\n\n        check_if_last_char_is_a_letter(\"apple pi e \") \u2192 False\n\n        check_if_last_char_is_a_letter(\"\") \u2192 False \n\n        Let's think step by step.\n\n        1. If the string is empty, return False.\n\n        2. If the string does not end with a alphabetical character, return False.\n\n        3. Split the given string into a list of words.\n\n        4. Check if the length of the last word is equal to 1.\n\n        \u201d'\n\n  \n\n\ndef file_name_check(file_name):\n\n        \u201d'\n\n        Create a function which takes a string representing a file's name, and returns 'Yes' if the the file's name is valid, and returns 'No' otherwise. A file's name is considered to be valid if and only if all the following conditions are met: - There should not be more than three digits ('0'-'9') in the file's name. - The file's name contains exactly one dot '.' - The substring before the dot should not be empty, and it starts with a letter from the latin alphapet ('a'-'z' and 'A'-'Z'). - The substring after the dot should be one of these: ['txt', 'exe', 'dll']\n\n        Examples:\n\n        file_name_check(\"example.txt\")  => 'Yes'\n\n        file_name_check(\"1example.dll\")  => 'No' (the name should start with a latin alphapet letter)\n\n        Let's think step by step.\n\n        1. Check if the file name is valid according to the conditions.\n\n        2. Return \"Yes\" if valid, otherwise return \"NO\".\n\n        \u201d'\n\n  \n\n\ndef fruit_distribution(s,n): \n\n        \u201d' \n\n        In this task, you will be given a string that represents a number of apples and oranges that are distributed in a basket of fruit this basket contains apples, oranges, and mango fruits. Given the string that represents the total number of the oranges and apples and an integer that represent the total number of the fruits in the basket return the number of the mango fruits in the basket. \n\n        for examble: \n\n        fruit_distribution(\"5 apples and 6 oranges\", 19) ->19 - 5 - 6 = 8\n \n        fruit_distribution(\"0 apples and 1 oranges\",3) -> 3 - 0 - 1 = 2 \n\n        fruit_distribution(\"2 apples and 3 oranges\", 100) -> 100 - 2 - 3 = 95 \n\n        fruit_distribution(\"100 apples and 1 oranges\",120) -> 120 - 100 - 1 = 19\n\n        Let's think step by step.\n\n        1. Extract the numbers of oranges and apples from given string.\n\n        2. Calculate the sum of oranges and apples.\n\n        3. Deduct the sum from the total number of fruits.\n\n        4. Return the number of mangoes.\n\n        \u201d'\n\n\ndef prime_fib(n: int):  \n\n        \u201d'  \n\n        prime_fib returns n-th number that is a Fibonacci number and it's also prime. \n\n        Examples: \n\n        >>> prime_fib(1) 2  \n\n        >>> prime_fib(2) 3  \n\n        >>> prime_fib(3) 5  \n\n        >>> prime_fib(4) 13  \n\n        >>> prime_fib(5) 89 \n\n        Let's think step by step.  \n\n        1. Create a function to check if a number is prime. \n\n        2. Generate a Fibonacci sequence. \n\n        3. Check if each number in the Fibonacci sequence is prime, decrement the counter. \n\n        4. If the counter is 0, return the Fibonacci number. \n\n        \u201d'   \n\ndef compare_one(a, b): \n\n        \u201d \n\n        Create a function that takes integers, floats, or strings representing real numbers, and returns the larger variable in its given variable type. Return None if the values are equal. Note: If a real number is represented as a string, the floating point might be . or , \n\n        Examples: \n\n        compare_one(1, 2.5) \u2192 2.5 \n\n        compare_one(1, \"2,3\") \u2192 \"2,3\" \n\n        compare_one(\"5,1\", \"6\") \u2192 \"6\" \n\n        compare_one(\"1\", 1) \u2192 None \n\n        Let's think step by step. \n\n        1. Store the original inputs. \n\n        2. Check if inputs are strings and convert to floats. \n\n        3. Compare the two inputs and return the larger one in its original data type. \n\n        \u201d'    \n\ndef sort_even(l: list): \n\n        \u201d' \n\n        This function takes a list l and returns a list l' such that l' is identical to l in the odd indicies, while its values at the even indicies are equal to the values of the even indicies of l, but sorted. \n\n        Examples: \n\n        >>> sort_even([1, 2, 3]) \n\n        [1, 2, 3] \n\n        >>> sort_even([5, 6, 3, 4]) \n\n        [3, 6, 5, 4] \n\n        Let's think step by step. \n\n        1. Create a list of all the even indices of the given list. \n\n        2. Sort the list of even indices. \n\n        3. Return a new list that is identical to the original list in the odd indicies, and equal to the sorted even indices in the even indicies. \n\n        \u201d' \n\ndef search(lst): \n\n        \u201d' \n\n        You are given a non-empty list of positive integers. Return the greatest integer that is greater than zero, and has a frequency greater than or equal to the value of the integer itself. The frequency of an integer is the number of times it appears in the list. If no such a value exist, return -1. \n\n        Examples: \n\n        search([4, 1, 2, 2, 3, 1]) == 2 \n\n        search([1, 2, 2, 3, 3, 3, 4, 4, 4]) == 3 \n\n        search([5, 5, 4, 4, 4]) == -1 \n\n        Let's think step by step. \n\n        1. Create a frequency dict. \n\n        2. Sort the input list. \n\n        3. Loop the input list, if frequency no lesser than the integer, set result. \n\n        4. Return the result. \n\n        \u201d' \n\n    \n\n\n \u00a7.\u00a7 Self-planning Prompt for MBPP-sanitied\n\n\nWrite a function to sum the length of the names of a given list of names after removing the names that start with a lowercase letter. \n\nLet's think step by step. \n\n1. Loop the input list. \n\n2. If the name not start with lowercase letter, add the length of the name to result. \n\n3. Return the result. \n\nWrite a function to increment the numeric values in the given strings by k. \n\nLet's think step by step. \n\n1. Loop the input list. \n\n2. If a string is a number, increment it. \n\n3. Return modified list. \n\nWrite a python function to find sum of all prime divisors of a given number. \n\nLet's think step by step. \n\n1. Create a inner function to check if a number is prime. \n\n2. Loop all number less than the input that is prime. \n\n3. Check if the input is divisible by that. \n\n4. Return the result. \n\nWrite a function to find the lateral surface area of a cone. \n\nLet's think step by step. \n\n1. Calculate the generatrix of the cone. \n\n2. Return the result. \n\n3. Please import inside the function. \n\nWrite a function to remove all tuples with all none values in the given tuple list. \n\nLet's think step by step. \n\n1. Loop the given tuple list. \n\n2. Check if all elements in the tuple are None. \n\n3. If not, append the tuple to the result list. \n\n4. Return the result. \n\nWrite a python function to find the last two digits in factorial of a given number. \n\nLet's think step by step. \n\n1. Calculate the factorial of the input number. \n\n2. Return the last two digits of it. \n\nWrite a python function to replace multiple occurence of character by single. \n\nLet's think step by step. \n\n1. Create a pattern that the input character repeats mulitiple times. \n\n2. Replace the pattern in input string with input character. \n\n3. Please import inside the function. \n\nWrite a python function to move all zeroes to the end of the given list. \n\nLet's think step by step. \n\n1. Count the number of zeros. \n\n2. Remove the zeros from the list. \n\n3. Append the zeros to the end of the list. \n\n4. Return the list. \n\n\n\n\n \u00a7.\u00a7 Prompt Example of Baseline\n \n\u00a0\n\nExample of Chain-of-Thought Prompting\n\u00a0\n\ndef encrypt(s):\n \n        \u201d'\n\n        Create a function encrypt that takes a string as an argument and returns a string encrypted with the alphabet being rotated. The alphabet should be rotated in a manner such that the letters shift down by two multiplied to two places.\n\n        For example:\n\n        encrypt('hi') returns 'lm'\n\n        encrypt('asdfghjkl') returns 'ewhjklnop'\n\n        encrypt('gf') returns 'kj'\n\n        encrypt('et') returns 'ix'\n\n        Let's think step by step.\n\n        1. Create a string \"alphabet\" with all letters of the alphabet.\n\n        2. Assign the number of places to shift the letters to a variable \"bias\".\n\n        3. Initialize a string \"result\" with an empty string.\n\n        4. Iterate over the characters of the string \"s\".\n\n        5. Find the index of the character in the string \"alphabet\".\n\n        6. Add the number of places to shift the letters to the index.\n\n        7. If the index is larger than 25, subtract 26 from the index.\n\n        8. Add the character at the index to the string \"result\".\n\n        9. Return the string \"result\".\n\n        \u201d'\n\n    \n\u00a0\n\nExample of Extremely Concise Style Self-planning Prompt\n\u00a0\n\ndef encrypt(s):\n \n        \u201d'\n\n        Create a function encrypt that takes a string as an argument and returns a string encrypted with the alphabet being rotated. The alphabet should be rotated in a manner such that the letters shift down by two multiplied to two places.\n\n        For example:\n\n        encrypt('hi') returns 'lm'\n\n        encrypt('asdfghjkl') returns 'ewhjklnop'\n\n        encrypt('gf') returns 'kj'\n\n        encrypt('et') returns 'ix'\n\n        Let's think step by step.\n\n        1. Alphabet, bias 4.\n\n        2. Latter bias, append.\n\n        \u201d'\n\n\n\u00a0\n\nExample of Ground-truth Planning Prompt\n\u00a0\n\ndef encrypt(s):\n\n        \u201d'\n\n        Create a function encrypt that takes a string as an argument and returns a string encrypted with the alphabet being rotated. The alphabet should be rotated in a manner such that the letters shift down by two multiplied to two places.\n\n        \u201d'\n\n        alphabet = 'abcdefghijklmnopqrstuvwxyz'\n\n        bias = 2 * 2\n\n        result = \u201d\n\n        for char in s:\n\n            index = alphabet.find(char) + bias\n\n            if index > 25:\n\n                index = index - 26\n\n            result += alphabet[index]\n\n        return result\n\nWrite steps according to the code.\n\n        1. Create a alphabet, bias two places multiplied by two.\n\n        2. Loop the input, find the latter bias letter in alphabet.\n\n        3. Return result.\n\n    \n\n\n\u00a0\n\nExample of Self-planning Prompt (One-phase)\n\u00a0\n    \n\ndef encrypt(s):\n \n        \u201d'\n\n        Create a function encrypt that takes a string as an argument and returns a string encrypted with the alphabet being rotated. The alphabet should be rotated in a manner such that the letters shift down by two multiplied to two places.\n\n        For example:\n\n        encrypt('hi') returns 'lm'\n\n        encrypt('asdfghjkl') returns 'ewhjklnop'\n\n        encrypt('gf') returns 'kj'\n\n        encrypt('et') returns 'ix'\n\n        Let's think step by step.\n\n        1. Create a alphabet, bias two places multiplied by two.\n\n        2. Loop the input, find the latter bias letter in alphabet.\n\n        3. Return result.\n\n        \u201d'\n\n        # Write your code here.\n\n        alphabet = 'abcdefghijklmnopqrstuvwxyz'\n\n        bias = 2 * 2\n\n        result = \u201d\n\n        for char in s:\n\n            index = alphabet.find(char) + bias\n\n            if index > 25:\n\n                index = index - 26\n\n            result += alphabet[index]\n\n        return result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}