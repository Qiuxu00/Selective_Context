{"entry_id": "http://arxiv.org/abs/2303.07016v1", "published": "20230313112532", "title": "HOOV: Hand Out-Of-View Tracking for Proprioceptive Interaction using Inertial Sensing", "authors": ["Paul Streli", "Rayan Armani", "Yi Fei Cheng", "Christian Holz"], "primary_category": "cs.HC", "categories": ["cs.HC", "cs.CV", "I.2; I.5; H.5"], "text": "\n\n\n\n\nHOOV: Hand Out-Of-View Tracking for Proprioceptive Interaction using Inertial Sensing]HOOV: Hand Out-Of-View Tracking for Proprioceptive Interaction using Inertial Sensing\n\n\n\n Department of Computer Science \n ETH Z\u00fcrich\n    Switzerland\n\n\n\n\n\n Department of Computer Science \n ETH Z\u00fcrich\n    Switzerland\n\n\n\n\n\n\n Department of Computer Science \n ETH Z\u00fcrich\n    Switzerland\n\n\n\n\n0000-0001-9655-9519\n\n Department of Computer Science \n ETH Z\u00fcrich\n    Switzerland\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurrent Virtual Reality systems are designed for interaction under visual control.\nUsing built-in cameras, headsets track the user's hands or hand-held controllers while they are inside the field of view.\nCurrent systems thus ignore the user's interaction with off-screen content\u2014virtual objects that the user could quickly access through proprioception without requiring laborious head motions to bring them into focus.\nIn this paper, we present HOOV, a wrist-worn sensing method that allows VR users to interact with objects outside their field of view.\nBased on the signals of a single wrist-worn inertial sensor, HOOV continuously estimates the user's hand position in 3-space to complement the headset's tracking as the hands leave the tracking range.\nOur novel data-driven method predicts hand positions and trajectories from just the continuous estimation of hand orientation, which by itself is stable based solely on inertial observations.\nOur inertial sensing simultaneously detects finger pinching to register off-screen selection events, confirms them using a haptic actuator inside our wrist device, and thus allows users to select, grab, and drop virtual content.\nWe compared HOOV's performance with a camera-based optical motion capture system in two folds.\nIn the first evaluation, participants interacted based on tracking information from the motion capture system to assess the accuracy of their proprioceptive input, whereas in the second, they interacted based on HOOV's real-time estimations.\nWe found that HOOV's target-agnostic estimations had a mean tracking error of 7.7 cm, which allowed participants to reliably access virtual objects around their body without first bringing them into focus. \nWe demonstrate several applications that leverage the larger input space HOOV opens up for quick proprioceptive interaction, and conclude by discussing the potential of our technique.\n\n\n\n\n\n\n\n\n\n<ccs2012>\n<concept>\n<concept_id>10003120.10003121.10003124.10010866</concept_id>\n<concept_desc>Human-centered computing\u00a0Virtual reality</concept_desc>\n<concept_significance>500</concept_significance>\n</concept>\n<concept>\n<concept_id>10003120.10003121.10003125</concept_id>\n<concept_desc>Human-centered computing\u00a0Interaction devices</concept_desc>\n<concept_significance>300</concept_significance>\n</concept>\n<concept>\n<concept_id>10010147.10010257.10010293.10010294</concept_id>\n<concept_desc>Computing methodologies\u00a0Neural networks</concept_desc>\n<concept_significance>100</concept_significance>\n</concept>\n<concept>\n<concept_id>10010147.10010178.10010224.10010245.10010253</concept_id>\n<concept_desc>Computing methodologies\u00a0Tracking</concept_desc>\n<concept_significance>500</concept_significance>\n</concept>\n<concept>\n<concept_id>10010147.10010178.10010224.10010226.10010238</concept_id>\n<concept_desc>Computing methodologies\u00a0Motion capture</concept_desc>\n<concept_significance>100</concept_significance>\n</concept>\n</ccs2012>\n\n\n[500]Human-centered computing\u00a0Virtual reality\n[300]Human-centered computing\u00a0Interaction devices\n[100]Computing methodologies\u00a0Neural networks\n[500]Computing methodologies\u00a0Tracking\n[100]Computing methodologies\u00a0Motion capture\n\n\n\n\n\n\n\n    \n    < g r a p h i c s >\n\n    \n is a wireless sensing method that complements existing virtual and augmented reality headsets to support hand tracking outside the field of view of the headset's cameras.\nIn this example app of our system, a participant is building a structure composed of blocks with different sizes.\nWhen the user places an object onto the structure, his hand is tracked by the headset's cameras (left).\nAs the user reaches for a block just to his right (right), his hand leaves the field of view of the cameras.\nNow,  continuously estimates the 6D position and orientation of the wrist using the 6-axis inertial measuring unit attached to the wrist during the time the hand stays outside the field of view.\n    \n    The figure consists of two pictures of a user wearing an Oculus Quest 2 headset with HOOV attached to his right hand's wrist. In the left picture, the user interacts with building blocks in a tower structure within the field of view of the headset's cameras. In the right picture, the user grabs for another building block outside the cameras' field of view.\n    \n\n\n\n[\n    Christian Holz\n    March 30, 2023\n==================\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nMixed Reality systems are increasingly designed to be standalone, both for Augmented Reality (AR) as well as Virtual Reality (VR) scenarios.\nThey typically integrate all necessary components for an immersive virtual experience in a user-worn headset.\nThese include a stereoscopic display, stereo sound, processors for rendering, as well as sensors such as inertial measurement units (IMUs) and cameras for inside-out head-pose tracking in world space.\n\n\nStandalone systems are highly portable, allowing them to operate in mobile scenarios, but requiring them to make several trade-offs in their design.\nBecause their provided comfort strongly affects the user experience, size, form factor, and weight are the limiting factors in the implementation.\nThese considerations ultimately limit the available computing power, battery capacity, and display size, which confines the visible field of view (FOV) for the wearer.\nThey also affect the number, type, and placement of integrated sensors, in particular the cameras that track the user's surroundings and, thus, the effective tracking FOV.\nIn addition to scene perception, these embedded cameras increasingly deliver the signal for detecting user input through hand gestures and interaction with virtual and physical objects.\nTherefore, the effective tracking FOV also determines the operational range for hand input on these devices.\n\nIn practice, today's headsets typically cover an operational range that slightly exceeds the user's visible FOV, thereby requiring all hand-object interaction to happen under visual control.\nWhile the operational range could be increased with additional headset cameras, which would incur additional needs for compute, power, and physical space inside the headset, certain areas around the body might still be outside the line of sight due to occlusion caused by clothing, hair, or other parts of the user's body.\n\n\n\nAs a result, today's systems implicitly require all interaction to occur in the visual FOV, which neglects most of the available space around the user for input.\nWe argue that this is a missed opportunity, as humans perceive more than 210\u00a0<cit.>, allowing us to perform hand-object interactions even at the edge of our periphery.\nEven without visual control, we routinely rely on proprioception to quickly place and retrieve physical objects in our vicinity\u00a0<cit.>\u2014without requiring a turn of the head, which would slow down such motions.\nSince such short-term use cases do not justify the cost of additional cameras, off-screen interaction is not part of the interaction vocabulary on today's headsets.\nAnd with mixed reality devices' ever-decreasing form factor, we do not expect future devices to substantially widen the tracking FOV.\n\nIn this paper, we introduce , a method to track interaction outside a headset's tracking field-of-view in immersive environments.\nUsing our novel data-driven inertial estimation pipeline,  complements current headsets by leveraging the continuous signals from a 6-axis IMU inside a wrist-worn band to estimate the user's current hand position outside the headset's tracking FOV.\n\n\n\n\n \u00a7.\u00a7 Outside field-of-view interaction\n\n\n<ref> shows an example application of our method.\nHere, a user is building a structure composed of blocks with different sizes, placed by his side for easy access.\nThe user can pick up a building block by pinching it, dragging it to the tower, and dropping it at the desired location and with the hand-held orientation.\nThe building blocks by his side replenish, such that grabbing one immediately produces another one at the same location.\n\n\nHaving built up the tower so far, the user has internalized the location of building blocks by his side by now.\nNaturally, as time progresses, he has to rely less and less on visual operation and, instead, simply reaches for one of the three locations to grab the corresponding piece.\nThis aptly supports his construction process as the complexity of the structure advances, allowing him to keep his visual attention fixated on the tower and the spot where he plans to place the next block, while reaching out to grab it.\n\n\nThe underlying implementation of this is powered by our method .\nWhen the user's hand leaves the headset's tracking FOV,  takes over the tracking and continuously provides the hand's current 3D position to the application.\nFor this,  processes the signals from the 3-axis accelerometer and 3-axis gyroscope sensor integrated inside the wrist-worn band as input.\nFrom these,  estimates the wrist's current 3D orientation and feeds it into our novel temporal machine learning model, which estimates the current hand position.\n\n\n\n\n\n\nTo register input commands from the user,  leverages the same IMU to detect hand gestures.\nIn <ref>, the user pinches to grab the intended building block, which  recognizes as a performed gesture in the signal stream.\n's pinch detection complements the interaction paradigm on today's VR platforms, in which pinching is commonly used either for direct or remote selection through a ray\u00a0<cit.>.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Performance Evaluation\n\n\nWe conducted two studies to evaluate 's performance during outside field-of-view interaction in stationary scenarios.\nBoth studies consisted of three tasks each. \nIn the first task, participants grabbed objects outside their FOV while facing forward, resembling our examples in <ref> but scaled up to 17\u00a0potential targets.\nIn the second task, participants placed virtual spheres into 1 of 17 discrete drop zones by their side.\nIn a final compound task, participants repeatedly switched between retrieving and placing objects outside their FOV, which constituted a compound and, thus, longer task.\nIn , participants' wrists were tracked by an 8-camera OptiTrack Prime 13 system, permitting an analysis of the human ability to interact based on proprioception only, under nearly optimal tracking conditions.\nParticipants achieved a success rate of \u223c91% when grabbing and dropping objects off-screen.\nIn , participants were operating exclusively using 's tracking when their hand left the headset's view.\nThey were successful in 86.18%, 87.65% and 86.18% of cases for the grab, placement, and compound task respectively.\n\nWhen considered as a target-agnostic 3D tracking system, 's simulated position estimates form a 3D trajectory that has a mean absolute error of 7.77 cm from the OptiTrack's reference path in .\nWe observed that 's error was lowest within the first three seconds, covering the duration of most interactions outside the user's field of view.\nIn our simulations of lower tracking FOV in the headset (i.e., earlier moments at which the headset hands off tracking to ), we found that 's tracking error slightly increased, with a mean absolute error of 9.28 cm in a simulated 120 tracking FOV and a 10.16 cm error in a simulated 90 tracking FOV.\nIn , the average error remained below 16 cm during real-time interactive use over longer off-screen trajectories.\nIn its current implementation, 's inference pipeline runs in real-time on a desktop machine with an NVIDIA GeForce RTX 3090 GPU, requiring 4 ms to predict a 3D hand pose, which amounts to a maximum update frequency of 250 Hz.\n\n\nIn both studies, participants also completed a separate condition of the same tasks under visual control.\nIn this condition, the hand was only tracked by the headset, in our case an Oculus Quest\u00a02, requiring participants to turn their head to keep the hand inside the headset's field of view.\nWhile the average success rate rose to 95% across all tasks in , the average task completion time also increased, most notably by the compound task.\nDue to the monotony and repetitiveness of the task, most participants pointed out the unfavorable need for turning their heads in this condition and the strain this puts on their neck over time, preferring interaction through  over interaction under visual control.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Applications\n\n\n enables several scenarios that are directly applicable and useful for immersive systems.\nWe show this at the example of three demo applications that benefit from the knowledge of the current hand position beyond the trackable FOV of the headset's cameras.\nIn the first application, users can interact with building blocks outside their FOV as illustrated in <ref> and <ref>.\nIn the second scenario,  improves a Beat Saber gaming experience by tracking quick arm movements that cross hardly tracked areas around the user's body.\nFinally,  facilitates tracked arm movements of extended range as demonstrated in our third application, an archery game.\n\nWe conclude this paper with a discussion of our applications and the implications of our method for future immersive systems.\n\n\n\n\n \u00a7.\u00a7 Contributions\n\n\nIn this paper, we contribute\n\n\n    \n  * a tracking method that enables short-term interaction beyond the FOV of the cameras inside AR/VR headsets.\n    Our method merely relies on the signals from a 6-axis IMU, integrated into our custom inertial sensing device placed on the user's wrist, which captures the information for outside-field-of-view 6D hand tracking and pinch detection and provides haptic feedback during interaction events.\n    \n    \n  * a novel temporal deep learning architecture that comprises a Transformer for the estimation of the current hand position from the IMU signals and the latest available output of the headset's visual hand tracking pipeline.\n    \n    \n  * a user study with 12 participants to investigate human proprioceptive abilities and 's tracking performance, where participants retrieved and placed spheres at 17 different target locations outside the visual and the camera's field of view.\n    Using an 8-camera OptiTrack system, participants grabbed and placed the correct object \u223c91% of the time.\n    In an offline simulation, they would have grabbed and placed the correct object in \u223c85% of all cases using .\n    Compared to a baseline with a commodity headset where participants grabbed and placed objects under visual control, their success rate increased to \u223c94% at the expense of 20% slower task completion and repeated neck movement.\n    \n    \n  * a user study with 10 participants where participants completed the selection and placement tasks using  in real-time, achieving success rates of more than 86%.\n\n\n\n\n\u00a7 RELATED WORK\n\n\n is related to body capture using worn sensors, inertial tracking, and interactions driven by spatial memory, proprioception, and kinesthesia. \n\n\n\n \u00a7.\u00a7 Body capture using worn sensors\n\n\nInformation about the user's body pose, especially the posture of the arm, enables VR and AR systems to correctly embody the user and to detect input through gestures.\n\nIn contrast to external body pose capture systems that include high-end commercial marker-based camera systems\u00a0<cit.>, depth and RGB\u00a0<cit.> cameras as well as non-optical systems\u00a0<cit.>, body-worn systems enable tracking in mobile settings, and do not require the user to remain within a constraint area.\nTo receive a wider view of the user's body for tracking, cameras with wide angle-lenses were attached to various parts of the body including the wrist\u00a0<cit.>, the feet\u00a0<cit.>, and the chest\u00a0<cit.> or alternatively integrated into controllers\u00a0<cit.> or using a suspension on the headset\u00a0<cit.> further away from the user's body.\nHowever, these solutions tend to suffer from occlusion and are often obtrusive to wear.\nAlternative approaches directly estimate the full-body pose based solely on the available temporal motion information of the user's head and hands\u00a0<cit.>.\nMoreover, various specialized mobile hand-held or body-worn systems have been built for tracking that make use of sensing modalities such as magnetic\u00a0<cit.>, mechanical\u00a0<cit.>, or acoustic\u00a0<cit.> sensing.\n\n\n\n\n\n\n\nPrior research has also shown that body-attached IMUs can be used in a standalone system for tracking human pose.\nDepending on the number of body locations that are instrumented, the problem varies in difficulty.\nSparse Inertial Poser\u00a0<cit.> estimates the 3D human pose based on 6 IMU sensors attached to the wrists, lower legs, the back and the head using a joint optimization framework incorporating anthropometric constraints.\nDeep Inertial poser\u00a0<cit.> trains a recurrent neural network (RNN) for this task.\nTranspose\u00a0<cit.> and PIP\u00a0<cit.> further improve on the predicted output by incorporating joint-specific loss terms and physic-aware motion optimizers.\n\n\n\n\n \u00a7.\u00a7 IMU-based arm and wrist tracking\n\nInstead of tracking the full body,  attached three IMU sensors to the hand, forearm and upperarm to track a user's arm pose through solving inverse kinematics\u00a0<cit.>.\nClosely related to our work,  presented an online and an offline tracking method to estimate the posture of the arm based on a single wrist-worn 9-axis IMU integrated within a smartwatch\u00a0<cit.>. \nThe method finds an optimal path through a set of potential candidate arm postures that are retrieved from a dictionary based on the IMU-estimated wrist orientation at each time step.\nThe offline version uses Viterbi decoding to estimate an optimal tracking path, and achieves a medium tracking error of 9.2 cm for the wrist.\nHowever, since it occurs a complexity of \ud835\udcaa(N^3T), it is not suitable for real time-tracking.\nA simpler online version that estimates the arm position from the candidate postures at a single step using a frequency-weighted average achieves a median tracking error of 13.3 cm.\n\u00a0<cit.> propose to improve the computation based on Hidden Markov Model state reorganization, achieving an accuracy of 12.94 cm.\nWith an updated algorithm including a Particle Filter, a median error of 8.8 cm was reported in a static setting\u00a0<cit.>.\n\nSimilarly,  propose an RNN architecture to track the arm posture from a 6-axis IMU assuming a fixed shoulder position\u00a0<cit.>.\nThey report a median error of 15.4 cm and a MAE of 16.4 cm for the wrist in a leave-one-subject-out evaluation scheme.\nLimbMotion uses an additional edge device for acoustic ranging to estimate the arm posture with a median wrist error of 8.9 cm\u00a0<cit.>.\n\nCompared to the previous approaches, our method does not aim to support stand-alone arm tracking based on a 6-axis IMU but to support a visual tracking system for short periods of time where the hand moves outside the cameras field of view.\nThis allows us to correct for drift, especially in the yaw direction which is more pronounced due to the missing magnetometer, as soon as the hand is visible by the headset's cameras.\nMoreover, our method does not assume a fixed shoulder position.\n\n\n\n\n\n \u00a7.\u00a7 Spatial Memory, Proprioception, and Kinesthesia\n\nThe human ability to operate without visual control is mainly supported by the following factors: spatial memory <cit.>, proprioception <cit.>, and kinesthesia <cit.>. \nSpatial memory refers to the part of memory responsible for recording the position and spatial relations between objects <cit.>. \nProprioception refers to the sense of position and orientation of one's body parts with respect to each other <cit.>.\nKinesthesia, which is often used interchangeably with proprioception, refers to the perception of one's body movements and motions <cit.>. In prior research, there is substantial work focused on characterizing the aforementioned cognitive and perceptual abilities of people <cit.>. \nGutwin et al. <cit.> and Cockburn and McKenzie <cit.>, for instance, studied people's capabilities of building spatial memory in 2D and 3D spaces. \nHocherman <cit.>, Soechting and Flanders <cit.>, and Medendrop et al. <cit.> examined the extent to which people can rely on their proprioception to perform target selections. \nAndrade and Meudell <cit.> and Postma and De Haan <cit.> showed that people are capable of learning spatial locations without paying particular attention to them. \n\nWithin the human-computer interaction community, it is generally acknowledged that spatial memory, proprioception, and kinesthesia can be exploited to support rich and efficient interactions <cit.>.\nLi et al.'s Virtual Shelves technique <cit.> leveraged users' kinesthetic memory for triggering programmable shortcuts.\nUsing Virtual Shelves, users can select shortcuts by pointing a spatially-aware device at 28 different locations in front of themselves.\nYan et al. <cit.> build on top of this work and experimentally studied eye-free target acquisition in VR including the additional aspect of user comfort. \nCockburn et al. <cit.> presented a design space exploration for the interaction termed \"air pointing\". \nGustafson et al.'s Imaginary Interfaces <cit.> sought to enable bi-manual empty-handed interactions without visual feedback by relying on users' short-term memory.\nImaginary phone <cit.> demonstrated the potential of transferring users' spatial memory from a familiar physical device to operating an \"imaginary\" equivalence on their palm. \nAdditional works explored proprioception-driven interactions for menu usage\u00a0<cit.>, allowing information access via a hip-attached touch device\u00a0<cit.>, supporting mobile phone access for visually impaired users\u00a0<cit.>, and enabling interaction with the back of a headset <cit.>.\n\nAlso closely related to , many works support users in proprioceptively performing interactions through haptics. \n\u00a0<cit.> guided visually-impaired users in target selection on a large wall-mounted display with vibrotactile feedback. \nBarham et al.'s CAVIAR device\u00a0<cit.> similarly used vibrotactile actuators to guide users' hands with continuous stimuli. \nVo and Brewster \u00a0<cit.> studied ultrasonic haptic feedback to support spatial localization.\n\nHOOV ultimately aims to enable the aforementioned spatial memory- and proprioception-driven interactions by expanding the tracking space of current VR devices for hand-object interactions. \nWe focus on addressing the technical challenge of performing the sort of eye-free interactions explored by Yan et al. <cit.> when the available headset tracking field-of-view is limited. \nWe further support users in performing eye-free interactions with haptic feedback via an actuator integrated into our wrist-worn device. \n\n\n\n\n\n\n\u00a7  METHOD: ESTIMATING PINCH POSITIONS FROM OBSERVATIONS OF WRIST ORIENTATIONS\n\n\n\n\nWe now introduce our method that enables the tracking of the wrist position, \ud835\udc29^w\u2208\u211d^3, and orientation, \ud835\udc11^w\u2208 SO(3), based on the captured acceleration, \ud835\udc1a^w\u2208\u211d^3, and angular velocity signals, \u03c9^w\u2208\u211d^3, of a 6-axis IMU placed at the user's wrist.\nBesides the wrist-worn band's motion signals,  also receives as input the headset's current position, \ud835\udc29^h\u2208\u211d^3, and orientation, \ud835\udc11^h\u2208 SO(3), as well as the sequence of \u03c4_t last available head and hand poses {\ud835\udc29^w, \ud835\udc11^w \ud835\udc29^h, \ud835\udc11^h}_-(\u03c4_t-1):0 before the hand has left the tracking FOV of the headset.\nWe describe this mapping f by\n\n    {\ud835\udc29^w, \ud835\udc11^w}_1:\u03c4 = f({\ud835\udc1a^w,\u03c9^w,\ud835\udc29^h, \ud835\udc11^h}_1:\u03c4_i, {\ud835\udc29^w, \ud835\udc11^w, \ud835\udc29^h, \ud835\udc11^h}_-(\u03c4_t-1):0),\n\n\nwhere \u03c4 matches the number of considered time steps predicted since the hand has left the tracking FOV, and \u03c4_i is the number of inputs sampled in the same period.\n\nThis is a challenging problem since we only receive the noisy observations of the relative motions of the wrist whose position lies in a 5-DoF space given a static shoulder position (3 DoF through shoulder joint and 2 DoF through lower arm)\u00a0<cit.> and a 7-DoF space given a constant neck position (additional 2 DoF from clavicle) as input.\nHowever, the set of possible wrist poses is severely constrained given the knowledge of the forearm's orientation and by incorporating the anatomical constraints of the individual human joints\u00a0<cit.>.\nWhile orientation estimation from a 6-axis IMU suffers from yaw drift due to the lack of a magnetometer, we demonstrate that a learning-based method can correct for this drift for short-term out-of-view interactions in static settings.\nOur method does this by estimating the most likely arm trajectory from an implicitly acquired approximated distribution of previously seen trajectory observations.\n\nIn addition to tracking the 3D positions of the wrist,  detects input commands from the wearer.\nWe continuously process the signals of the accelerometer for characteristic patterns to detect gestures, such as pinching or fist clenching.\nUsing the build-in haptic actuator, our prototype is capable of rendering haptic feedback to the user in response via a knocking sensation.\n\n\n\n \u00a7.\u00a7 Inertial 6D hand pose estimation\n\n\n<ref> shows an overview of our inertial 6D hand pose estimation pipeline.\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Input and output representation\n\nThe input consists of the accelerometer and gyroscope signals from the IMU within the wrist-worn band as well as the current head position and orientation estimated by the inside-out tracking of the VR headset. \nThe spacing between the samples of tracked head positions is matched to the sampling interval of the IMU through cubic interpolation.\nWe further estimate an initial orientation of the wrist by directly applying an extended Kalman Filter\u00a0<cit.> to the gyroscope and accelerometer signals\u00a0<cit.>.\nWe convert all orientations to their 6D representation to ensure continuity\u00a0<cit.>.\nFor each time step, we concatenate the acceleration, angular velocity, head position and orientation, and the output of the extended Kalman Filter \ud835\udc11^k. \nThis input sequence, \ud835\udc17\u2208\u211d^\u03c4_i\u00d7 21, starts from the moment when the hand leaves the trackable FOV of the headset.\nWe obtain another input sequence \ud835\udc12\u2208\u211d^\u03c4_t\u00d7 18 of the last \u03c4_t head and hand poses that were tracked by the headset before the hand left the tracking FOV.\n\nBased on this input, our estimator directly predicts wrist position \ud835\udc29^w\u2208\u211d^3 and rotation \ud835\udc11^w\u2208\u211d^6 within the world.\nUsing the orientation of the wrist, we produce a more refined estimate of the pinch position, \ud835\udc29^p\u2208\u211d^3, by applying an offset of 15 cm to the wrist position,\n\n    \ud835\udc29^p = \ud835\udc11^w(0, -0.15, 0)^T + \ud835\udc29^w,\n\nwhere \ud835\udc11^w is the rotation matrix corresponding to \ud835\udc11^w and the y-axis of the right-handed local coordinate system centered at the wrist points along the forearm to the shoulder with the z-axis pointing downwards through the palm.\nWe took this option because we expected position estimates to be better than representations that encode the rotations for each joint along the kinematic tree from the head to the wrist.\nSince the interaction happens outside the user's visible FOV, visualization artifacts due to varying bone lengths are of limited concern.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Network architecture\n\n\n relies on a neural network to approximate the mapping f.\nThe architecture of the network consists of a downsampling module that receives the input sequence \ud835\udc17 containing the inertial motion information as input and reduces the sequence along the temporal axis by a factor of 8.\nThe module consists of three blocks that each reduce the signal's temporal resolution by a factor of 2 and consist of two convolutional layers with a kernel of size 3 followed by a max pooling layer.\nThe samples of the remaining features are converted to linear embeddings that are fed to a Transformer consisting of two encoder layers.\nAt the start of the sequence, we add an initial token that embeds the information from the headset-tracked hand and head poses \ud835\udc12, extracted through a 2-layer Elman RNN.\nWe apply a sinusoidal positional encoding to the input of the Transformer\u00a0<cit.>, and avoid that future samples are attended to for the estimation of any output samples by applying a corresponding mask to the sequence.\n\nFor each output sample of the Transformer corresponding to the down-sampled features from the inertial input, we predict the position and orientation of the wrist using a series of fully-connected layers.\nThis design supports sequences of variable length.\n\n\n\n  \u00a7.\u00a7.\u00a7 Loss function\n\n\nOur loss function, \n\n    \u2112 = \u2112_p + \u2112_R = \u2211_t=1^\u03c4 (|\ud835\udc29\u0302^w_t-\ud835\udc29^w_t| + |\ud835\udc11\u0302^w_t-\ud835\udc11^w_t|),\n\nconsists of two terms, where \u2112_p penalizes the positional and \u2112_R the rotational offset using the L1 loss function, and \ud835\udc29^w_t and \ud835\udc11^w_t are the ground-truth and \ud835\udc29\u0302^w_t and \ud835\udc11\u0302^w_t the predicted position and orientation of the wrist at time t respectively.\n\n\n\n \u00a7.\u00a7 Pinch detection\n\n\nTo detect pinch events, we threshold the running rate-of-change score c, which accumulates the absolute change in the acceleration signals \ud835\udc1a^w captured by the IMU at the wrist across time\u00a0<cit.>,\n\n\n    c_t = 1Dc_t-1 + |\u2016\ud835\udc1a^w_t\u2016_2-\u2016\ud835\udc1a_t-1^w\u2016_2|.\n\n\nPinch events cause sudden changes in a^w_t that lead to a strong increase c_t, and thus, can be detected through a threshold.\nThe exponential reduction factor D attenuates past accumulations.\n\n\n\n\n\n\n\u00a7 IMPLEMENTATION\n\n\n is implemented to run as a real-time interactive tracking system on an 8-core\nIntel Core i7-9700K CPU at 3.60 GHz with an NVIDIA GeForce RTX 3090 GPU.\n consists of three components: 1) a wrist-worn sensing platform, 2) a virtual reality interface, and 3) a central control and sensor fusion unit that handles the communication to the wristband hardware and the virtual reality headset as well as the estimation of the hand pose outside the FOV.\n\n\n\n \u00a7.\u00a7 Hardware\n\n\n\n\nWe custom-designed an embedded platform for  for sensing and actuation during interaction in mid-air.\nAs shown in <ref>, our electronics platform centers around a System-on-a-Chip\n(NRF52840, Nordic Semiconductors) that samples a 6-axis IMU (LSM6DSOX, STMircroelectronics) at 427 Hz. \nOur prototype streams the inertial data to a PC, either wirelessly over BLE or through a wired serial interface.\nThe prototype integrates a separate board that embeds an audio amplifier (MAX98357A, Maxim Integrated) to drive a Lofelt\u00a0L5 actuator to produce haptic feedback. \nAll components of our prototype, including a battery, are housed in a 3D-printed case, which attaches to the wrist using a strap. \n\n\n\n\n\n\n\n \u00a7.\u00a7 Virtual Reality\n\n\nWe implemented our VR application in Unity\u00a02021 for the Oculus Quest\u00a02 VR headset. \nOur application communicates with our outside-field-of-view tracking pipeline via a web socket. \nAs tracking source, our VR environment receives the hand pose computed by the headset, sampled at \u223c70 Hz when the user's hand is tracked, and the estimates produced by  otherwise.\nInput events are triggered through 's pinch detection algorithm in both states.\n\n\n\n\n \u00a7.\u00a7 Sensor fusion and outside FOV tracking\n\n\nThe main control unit of our implementation is a state machine that moves between states depending on the current tracking status of the VR headset.\nDuring the outside FOV state, the headset forwards the received head poses and the inertial input data together with the last 5 available hand and head poses from the VR headset to the deep learning-based hand pose estimation pipeline.\n\nWe implemented the main control unit in Python\u00a03.8.\nThe program runs across multiple cores to handle the communication with the sensing hardware and the processing of the corresponding signals, and to set the current hand pose for the VR interface.\n\nUpon detecting a pinch gesture using an exponential reduction factor D of 1.07 and a threshold of 4.9 m/s^2, our system triggers an event inside the VR environment and sends a command to the motor driver to activate the haptic feedback once.\n\n\n's network is implemented in PyTorch and has 4,408,199 trainable parameters.\nWe use the Kaiming initialization\u00a0<cit.>, the Adam optimizer\u00a0<cit.> with a learning rate of 10^-4, and a batch size of 16 where we randomly group sequences of equal length.\n\n\n\n\n\u00a7 : EVALUATION WHERE PARTICIPANTS INTERACTED BASED ON  TRACKING\n\n\nTo evaluate 's potential, we conducted a user study in which participants placed and retrieved objects outside their FOV.\n served two purposes: (1) It allowed us to quantify 's accuracy of tracking wrist positions outside the headset's tracking FOV. (2) We could evaluate how quickly and accurately participants selected and placed objects outside their field of view compared to performing the same task under visual control by turning their heads.\nThis quantified the effect of operating under proprioceptive control.\nTo evaluate the upper bound of our approach, we conducted this first evaluation while participants interacted based on OptiTrack tracking to exclude the impact of tracking errors on participants' behavior.\n(See Section\u00a0<ref> for our real-time evaluation of  for the same tasks ().)\n\n\n\n\n\n\n\n \u00a7.\u00a7 Study design\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Apparatus\n\nOur experiment apparatus consisted of an Oculus Quest\u00a02 VR headset, a  wristband as described above, a computer for logging, and an optical motion capture system.\n\n\n\n  \nVR headset\nThroughout the study, participants wore a Quest\u00a02 VR headset with integrated visual hand tracking.\nFor the study setup, we implemented a VR environment featuring objects, placement zones, and task protocols in Unity\u00a02021.\nParticipants saw a visualization of their right hand when it was within their FOV.\n\n\n\n  \n's IMU streams and haptic feedback\nParticipants wore a  wristband on their right arm, which continuously streamed the data from the IMU to a PC for processing.\nFrom the continuous stream of accelerations, our apparatus detected pinch events as described in Section\u00a0<ref>.\nTo eliminate the impact of wireless connectivity onto participants' performance, we connected the wristband through thin and flexible magnet wires to the PC for power and reliable serial communication in this study.\n\n\n\n  \nMotion capture for 3D wrist and head trajectories\nTo compare the accuracy of 's estimated wrist position to a ground-truth baseline, we tracked rigid-body markers attached to the participant's headset, shoulder, elbow, and wristband using an 8-camera OptiTrack system with sub-mm accuracy to obtain positions and rotations at a sampling frequency of 240 Hz.\nThe rigid bodies are made out of cardboard that we attached to worn elbow and shoulder pads, and the HOOV wristband (<ref>).\nTo track the headset, we directly attached four 9-mm tracking markers to the device.\n\nAn experimenter ensured a consistent placement of the rigid bodies across participants ahead of the experiment and verified throughout that the markers did not slip.\nWe calibrated the OptiTrack to the coordinate system of the VR headset through two pre-defined calibration points in the physical space that we marked with a controller in the virtual environment.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Participants\n\n\nWe recruited 12 participants from our local university (3 female, 9 male, ages 22\u201335, mean=25.7 years). \nUsing the OptiTrack, we measured the distance between the worn headset and the floor as well as the distance between the neck and the wrist while participants stood in a T-pose.\nThe average headset distance from the ground was 173 cm (SD=12 cm, min=149 cm, max=188 cm), and arm lengths ranged between 62 cm and 80 cm (mean=72 cm, SD=6 cm). \nParticipants self-reported their prior experience with VR technology on a 5-point Likert scale (from 1\u2013never to 5\u2013more than 20 times).\nParticipants' ratings ranged between 1 and 5 and their median prior experience was 3.\nEach participant received a small gratuity for their time.\n\n\n\n  \u00a7.\u00a7.\u00a7 Task\n\nThe study consisted of three grab-and-place tasks.\nThroughout the experiment, participants stood at a fixed point that was highlighted on the floor using tape, such that the experimenter could verify their position.\n\nIn the first task  (<ref>), participants stood next to a set of 17 spherical target objects outside their FOV in the virtual environment, grabbed an intended sphere, and placed it into the blue dropzone in front of them.\nParticipants saw a down-sized illustration of the task environment in front of them, which highlighted the sphere to pick next.\nParticipants grabbed a sphere using a pinch gesture, selecting the sphere that was closest to their right hand when in range.\nParticipants then moved it towards the dropzone and released it using a second pinch.\n\nIn the second task , participants picked up a spherical target in front of them, and placed it within one of 17 drop zones to their right outside their FOV.\nAgain, a set of three matching illustrations highlighted the task's target drop zone next to them.\nParticipants first grabbed the sphere using a pinch gesture, before moving it to the target drop zone.\nAfter pinching again, the apparatus placed the sphere in the drop zone that was closest to the hand's position at the time of the second pinch.\n\nThe third task  combined Tasks\u00a02 and\u00a01.\nFirst, participants grabbed the sphere within their field of view in front of them and dropped it into one of the 17 drop zones outside their field of view as instructed\u00a0(<ref>b).\nThen, they were instructed to grab one of the 17 spheres from outside their field of view and release it within the drop zone in front of them\u00a0(<ref>a).\n\n\nAs shown in <ref>, the spheres and drop zones were arranged in a grid of 3\u00a0rows, 3\u00a0columns, and 2\u00a0layers in the lateral direction of the participant.\nWe excluded the closer center location behind the participant, because reaching it would require contorting one's arm in an uncomfortable pose without turning one's upper body.\nAll other targets outside the FOV were convenient to reach.\n\nThe spacing of the targets and drop zones remained static throughout the study, and was adjusted at the beginning of the experiment to each participant's eye level and arm length to ensure all targets could be reached.\nThe spacing in the lateral direction between the two layers was equal to half the length of the participant's arm.\nThe spacing between objects and drop zones in the sagittal and ventral plane was equal to half the distance between the neck and the wrist of the participant when performing a T-pose, corresponding to an angular spacing of around 30 between the drop zones and targets further away from the participant.\nThe target and drop zone grid was centered at the height of the participant's shoulder, and placed at a distance so that the participant's hand would reach all elements of the second layer when elongated.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Conditions\n\nParticipants performed the three tasks in two conditions.\n\nIn the  condition, the participant's hand was solely tracked by the Quest\u00a02 headset.\nThus, participants needed to turn their heads during the task and follow their hands to ensure that the headset tracked the hand's position when performing the task.\n\nIn the  condition, participants were instructed not to turn their head and to look straight forward.\nAn experimenter ensured this throughout by monitoring the head-mounted display through a secondary screen.\nDuring this condition, the wrist position and orientation was provided to the VR environment by the OptiTrack motion capture system.\n\nIn both conditions, the apparatus detected pinch gestures for grabbing and releasing the spheres using the IMU inside the  wristband as in Section\u00a0<ref>.\nParticipants received haptic feedback for detected pinch events and were notified of erroneous selections and placements with audio feedback.\n\n\n\n  \u00a7.\u00a7.\u00a7 Procedure\n\n\nThe study started with a brief introduction of the tasks.\nThe experimenter then noted down the participant's age and gender.\nParticipants performed a T-pose to calibrate the system to their body size.\nThe study started with a training session of all conditions and tasks, followed by the experiment.\n\nDuring training, participants performed 17 trials, one for each target location, for each task.\nWhen using the  condition, participants received visual guidance for the  and  task through the down-sized visualization to allow them to refine their internal model of their hand position in 3-space.\nThe visualization highlighted the corresponding sphere or drop zone as soon as the participant's hand came into contact with it.\nThis online highlighting was exclusive to the training session and was not available during the actual experiment.\n\nAfter training, participants performed all tasks in each of the conditions without guidance.\nWe counterbalanced the order of the task and conditions across participants.\nParticipants performed 34\u00a0trials for  and , and 68\u00a0trials for the  task, where a single trial consisted of placing or retrieving a single target object.\nFor  and , each drop zone and sphere was used as target twice with the order across targets randomized. The same drop zone or target sphere was not used in direct succession.\nFor the  task, we fixed the order of locations between directly successive  and  subtasks but randomized the order across combinations.\nWe ensured that each valid position was used twice as drop zone and target object location.\nIn total, participants completed 2 conditions \u00d7 (34\u00a0 trials + 34\u00a0 + 68\u00a0 trials) = 272\u00a0trials in under 45 minutes.\nParticipants were instructed to complete the tasks as fast as possible while avoiding mistakes.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Measures\n\nAs dependent variables, our apparatus measured the time it took participants to complete each trial and the percentage of successfully completed trials.\nA trial was successful when the participant placed the correct sphere in the correct drop zone.\n\n\n\n\n \u00a7.\u00a7 Comparing  with high-accuracy tracking from a motion capture system\n\n\nOur study apparatus logged all signals and, thus, recorded a labeled dataset of IMU signals and ground-truth wrist and head positions from the motion capture system.\n\nWe used the recordings to train and test 's estimation network for an offline evaluation.\nThis allowed us to not only quantify 's performance given the specific configuration of the headset, but we could also simulate a variety of FOV configurations and, thus, moments at which  took over tracking from the headset due to our knowledge of the hand positions relative to the head pose at every time step.\nThe  condition thereby acts as the upper bound for participants' overall performance given the tracking system's high accuracy.\nUsing the OptiTrack data, we could evaluate participants' ability to interact with objects outside their FOV under near-perfect tracking conditions.\nThis enables us to decompose 's error into contributions from participants' inherent limitation to perform proprioceptive interactions and limitations associated with our method itself.\n\n\n\n  \nTraining and evaluation\nWe evaluated our network in a 6-fold cross-validation where each fold used the data of two randomly selected participants for testing and the other 10\u00a0participants for training.\nWe further augmented our training data with the recordings from three additional participants that we acquired while piloting our study setup.\nFor training, we use all data from the  condition, including the trajectories recorded during participants' initial training sessions.\nTo further augment our dataset, we trained our network on sequences that simulate a horizontal FOV between 40 and 120 in steps of 5.\nWe also generated training input based on the logged tracking information from the Quest\u00a02.\n\nWe trained our network until convergence on validation data from a participant that was extracted and excluded from each training dataset.\nWe trained for at least 250,0000\u00a0iterations on an NVIDIA GeForce RTX 3090, which takes approximately 12\u00a0hours.\nFor the test set, we only use the trials of the  condition from the actual experiment.\n\nWe simulated and evaluated three headset tracking FOVs, including 90 (i.e., the visible FOV of the Quest\u00a02), 120 (i.e., the approximate FOV of human binocular vision), and the actual tracking FOV of the Quest\u00a02.\nThe latter exceeds 120 as we empirically determined through the headset's reported tracking state.\n\nWe evaluated 's output in terms of mean absolute distance error (MAE) and mean angle of the difference rotation (MAD)\u00a0<cit.>, \u03b8=2arccos(\ud835\udc2a\u0302^w\u00b7\ud835\udc2a^w), to the measurements reported by the motion capture system, averaged over the whole out-of-field-of-view tracking path.\nHere, \ud835\udc2a\u0302^w and \ud835\udc2a^w are the unit quaternions corresponding to the ground-truth orientation \ud835\udc11\u0302^w and predicted orientation \ud835\udc11^w respectively, and \u00b7 denotes the inner (or dot) product of vectors.\nWe also compare 's success rates for the simulated FOV ranges to  and . \n\n\n\n\n\n\n \u00a7.\u00a7 Results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Success rate\n\n\n<ref> (left) shows an overview of participants' performance success during this study.\nUsing , participants were careful not to make mistakes when performing the tasks under visual control.\nParticipants reached an average success rate of 94.12% for  (max=100.00, min=88.24, SE=1.30), 94.61% for  (max=100.00, min=79.41, SE=1.69), and 94.12% for  (max=100.00, min=88.24, SE=0.93).\nUsing , participants' mean success rate was 90.93% for  (max=100.00, min=76.47, SE=1.91), 90.69% for  (max=97.06, min=82.35, SE=1.50), and 90.56% for  (max=100.00, min=83.82, SE=1.26).  \n\nWhen simulating the tracking of participants' wrist motions outside the FOV with , participants' overall success rate was 87.50% for  (SE=1.63, max=97.06, min=76.47), 84.56% for  (SE=2.02, max=97.06, min=67.65) and 83.82% for the  task (SE=1.83, max=97.06, min=72.06).\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Trial Duration\n\n\nAs shown in <ref> (left), participants' mean completion time was shorter for the three tasks when not looking at the outside FOV regions, i.e., during , , and  in the  condition.\nOn average, participants took 2.37 s to complete a  trial (max=3.27, min=1.66, SE=0.14), 2.65 s for a  trial (max=3.50, min=1.96, SE=0.15), and 2.79 s for a  trial (max=3.59, min=2.07, SE=0.17).\nAs shown in <ref>, participants' mean completion time increased across all tasks in the  condition.\nHere, the average mean trial completion time was 2.93 s for  (max=5.36, min=2.29, SE=0.22), 3.12 s for  (max=4.58, min=2.33, SE=0.17), and 3.29 s for the  task (max=4.36, min=2.57, SE=0.16).\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Absolute tracking accuracy of \n\n\nComparing the wrist poses estimated by  to the poses captured by the OptiTrack system when participants' hands were outside the tracking FOV of the Quest\u00a02, we found an MAE of 7.77 cm across participants.\nThe median position offset averaged across participants was 7.00 cm.\n<ref> illustrates the development of the average tracking error over time from the point when a participant's hand left the FOV of the headset.\n\nWhen dropping an object outside the FOV, 's position estimates had an average MAE to the actual hand position of 6.95 cm.\nThe average MAE when selecting objects outside the FOV was 7.72 cm.\nThe orientation error as measured by the mean angle of the difference rotation is 6.50 (see\u00a0<ref>). \n \n\nAs mentioned above, we simulated a series of tracking FOV to assess how 's tracking capabilities may deteriorate for larger areas outside the FOV and, thus, increased of -based tracking.\nFor a simulated horizontal FOV of 120, the average positional MAE across participants increased to 9.28 cm, and the average MAD increased to 7.67.\nThe offset to the actual release position and grab position while performing the  and  stayed within 10 cm, amounting to 8.92 cm and 9.86 cm, respectively, on average.\nFor a simulated horizontal FOV of only 90,  estimated positions with an average MAE of 10.16 cm and estimated orientations with an MAD of 8.12.\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Discussion\n\n\nThe results of  underline the potential of our method  to complement camera-based headset tracking for motions and interactions outside their tracking FOV.\nMost obviously was participants' speed increase when they could directly interact with targets outside their FOV without turning their heads to first bring them into the view.\nOn average, mean completion times were 19% lower for the  and 15% lower for the  and  task when participants were able to interact outside their FOV.\nBecause participants were able to directly compare the conditions during their experience in the study, several mentioned, unprompted, that turning their heads felt like a burden during this task.\n\nThe results are also promising, as our study task simulated a common scenario in real life\u2014performing an action for hand-object interaction while focusing on another object or action somewhere else.\nReal-world examples include grabbing a water bottle or shifting gears while driving, or switching brushes while painting.\n\nHowever, the gain in speed came at the cost of accuracy as expected.\nUnder visual control, participants achieved higher success rates for grabbing and dropping targets than operating outside their FOV without vision, which has been quantified in previous experiments\u00a0<cit.>.\nNote that our targets were world-anchored and static; while the experimenter ensured that participants did not move from the position on the floor, shifts in upper-body pose may already result in a considerable effect.\n\nUpon closer inspection of where the outside FOV input events were least correct, we saw a disproportional amount of errors for the spheres and drop zones behind the user.\nIn these cases, participants found it difficult to locate their hand outside their field of view correctly, possibly because these locations were outside participants' proprioceptive range and thus the area they would naturally interact with outside their FOV in other use cases.\nRelated, in erroneous cases,  failed to detect the correct object especially when participants placed their hand just between two targets.\nThen, even small deviations in estimated positions led to binary changes of target outcomes.\nWe note that future systems may take this into account and potentially increase the separation between targets to correct for the tracking inaccuracies, introducing a dead band between targets where input events have no effect.\n\nIn terms of 's potential to complement the tracking capabilities of headsets with limited FOVs, we first highlight its small median error of <\u00a08 for all three simulated FOV conditions in\u00a0<ref>.\nSeveral applications in VR benefit from stable orientation estimation, such as games (e.g., Beat Saber) or 3D interactive environments (e.g., for ray-casting).\nNext, we underline 's capability to estimate absolute positions outside the headset's tracking FOV.\nGiven a suitable interface design for virtual content outside the user's FOV, the median error of 7 cm\u2014about the length of an index finger\u2014offers sufficient tracking accuracy to navigate through a range of targets in the hemisphere defined by the arm's length.\nFor this purpose, elements would need sufficient size and spacing that would allow users to traverse them in conjunction with the haptic feedback rendered by our wrist device.\n\nWhile 's tracking estimates are closest to the ground-truth within the first three seconds after the hand leaves the headset's FOV,  showed promise in that its tracking error deteriorated only minimally under our simulated 90 tracking FOV condition.\nThe mean position error of \u223c10 cm and the mean angle of the difference rotation of \u223c8offer encouraging benefits for  to work in conjunction with future headsets optimizing on power and form factor.\n\n\nRegarding participants' qualitative feedback (<ref>), \ntheir overall reception of performing the tasks in the  condition was positive.\n8 out of 12 participants reported that they preferred the  condition over , as their necks felt strained from repetitively turning around.\nSeveral participants also mentioned that this condition felt more efficient, even if we saw during the analysis that they tended to make more mistakes.\nIn contrast, 4 of the 12 participants said that they would rather look at the object while performing the task, because it felt `safer.'\n\n\n\n\n\n  \nComparison to related approaches\n\n\n\n\n is not directly comparable to other methods that estimate the wrist position from a single IMU, because we evaluated our system on a different dataset and  receives an initial position of the hand as input.\nHowever, 's relatively small median position error of 7 cm demonstrates the strength of our estimation pipeline to handle the accurate estimation of the wrist pose over short periods.\nComparing the results to the values reported in the literature,  performs 15.5% better than ArmTrak's offline algorithm\u00a0<cit.>, 11.7% better than the Particle Filter-based approach by \u00a0<cit.>, 12.7% better than LimbMotion\u00a0<cit.>, which uses an additional remote edge device, and 49.5% better than the RNN proposed by\u00a0\u00a0<cit.> (see\u00a0<ref>).\n\nIn terms of human performance, the results from our study generally align with the conclusions drawn by\u00a0\u00a0<cit.>.\nHowever, we also showed that participants were able to differentiate between objects in the lateral direction outside their field of view.\n\nIn order to understand 's estimation performance during real-time use, we conducted another evaluation where participants' interaction was based solely on 's tracking.\n\n\n\n\n\n\u00a7 : EVALUATION WHERE PARTICIPANTS INTERACTED BASED ON 'S REAL-TIME ESTIMATES\n\n\n\n\nIn , we conducted another evaluation where participants interacted based on 's real-time predictions, such that any potential tracking error of our method affected their behavior and thus performance.\n\n\n\n\n \u00a7.\u00a7 Study design\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Apparatus\n\n\nWe built on the experiment apparatus from our previous study (Section\u00a0<ref>).\n\nTo additionally evaluate the accuracy of our pinch detection and position, we attached two motion capture markers to participants' thumb and index fingernails before the study.\nTracking them, we simultaneously assessed the detection accuracy of pinch gestures as well as the accurate position of the fingers relative to the tracked wristband.\nOur apparatus detected pinch events when the two markers converged to a distance below 3 cm in a sudden motion.\n\nIn contrast to , the OptiTrack system was merely present in our apparatus to record ground-truth wrist poses for later analysis and had no influence on participants' behavior, the processed input events, or the virtual scene during the study.\nLikewise, the apparatus continued to detect pinch events based on the IMU signals (and not based on the attached OptiTrack markers).\n\nThe VR front-end of our study apparatus received the wrist position and orientation from 's real-time estimations.\nFor this, a PC (32 GB RAM, 3.60 GHz CPU) with an NVIDIA GeForce RTX 3090 received the headset's pose, hand positions from the VR headset when visible, and the stream of IMU signals from 's wristband and processed them in real-time using the model trained on all data from  (<ref>).\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Participants\n\n\nWe recruited a fresh set of 10 participants (4 female, 6 male, ages 22\u201328, mean=25.3 years).\nHeadset distances from the ground were 157\u2014190 cm (mean=170 cm, SD=9 cm), and arm lengths ranged between 57\u201476 cm (mean=69 cm, SD=6 cm).\nParticipants' prior experience with VR technology was lower than in our first study, ranging between 1\u20134 on a 5-point Likert scale (median=2).\nParticipants received a small gratuity for their time.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Task\n \n\nParticipants completed the same three grab-and-place tasks (,   & ) as in  while standing at an indicated stationary position.\nThe placement of the 17\u00a0virtual spheres and drop zones was also identical to our first study.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Conditions\n\n\nParticipants again completed each task in two different conditions.\n\nThe first condition was identical to the  condition in  (Section\u00a0<ref>), where participants visually followed their hands by turning their heads to interact with out-of-view targets.\n\nIn the second condition , participants faced straightforward and completed the tasks without turning their heads.\nUnlike in , our apparatus tracked participants' interaction, rendered positions, and executed input events based on 's predictions.\nParticipants again received haptic feedback at grab and release events and audio feedback depending on the outcome of a trial.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Procedure\n\n\nAt the start of the study, participants received an introduction to the tasks, provided information about their age and gender, and performed the T-pose calibration procedure to adjust the virtual environment to their body size.\nParticipants received training for each condition and task that included visual feedback for the  condition.\n\nOverall, participants completed 272 trials (= 2 conditions \u00d7 (34  trials + 34\u00a0 trials + 68\u00a0 trials)) as fast as they could while avoiding errors.\nEach drop zone and sphere appeared as target twice for each combination of task and condition.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Measures\n\n\nUsing our study apparatus, we measured the duration of each trial as well as participants' success rates.\nWe evaluated the tracking performance of  compared to the ground-truth tracking from the OptiTrack's marker-based tracking system.\nUsing the additional optical markers, we also analyzed the accuracy of our pinch gesture detection.\n\n\n\n\n \u00a7.\u00a7 Results\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Success rate\n\n\nUsing the , participants correctly performed the task in 97.65% (max=100.00, min=91.18, SE=0.83) of all trials for , 95.88% (max=100.00, min=85.29, SE=1.43) for , and 97.94% (max=100.00, min=92.65, SE=0.72) for .\n\nUsing 's real-time tracking, participants correctly completed the task in 87.65% (max=97.06, min=70.59, SE=2.04) of trials for , 86.18% (max=100.00, min=73.53, SE=2.28) for , and 86.18% (max=91.18, min=76.47, SE=1.23) for .\n\nAnalyzing the sub-mm accuracy measurements from the motion capture system for comparison, participants' hands were in the correct position 92.35% (max=97.06, min=76.47, SE=1.83) of the trials for , 95.59% (max=100.00, min=79.41, SE=1.75) for  and 92.21% (max=100.00, min=72.06, SE=2.09) during the  task.\nNote that this analysis of human performance during interaction based on proprioception is hypothetical, as any real-time tracking error incurred by  had an impact on participants' behavior during this study.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Trial duration\n\n\nUsing , participants on average completed trials in\n3.17 s (max=4.06, min=2.10, SE=0.20) for ,\n3.74 s (max=5.26, min=2.48, SE=0.22) for ,\nand 3.90 s (max=6.41, min=2.49, SE=0.32) for .\nUsing 's real-time tracking and without participants turning their heads, mean trial duration was\n2.99 s (max=3.63, min=2.44, SE=0.11) for ,\n3.54 s (max=4.07, min=2.93, SE=0.10) for ,\nand 3.38 s (max=4.11, min=2.87, SE=0.13) for .\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Pinch accuracy\n\n\nCompared to the detected pinches using the two additional optical markers, 's pinch detection achieved a recall of 94.85% and a precision of 98.05% across all three tasks.\nAny false-negative detection in  during the study led participants to pinch again, since  tracking had no impact on the behavior of the visual environment and study procedure.\n\n\n\n  \u00a7.\u00a7.\u00a7 Absolute tracking accuracy of HOOV\n\n\nWhen considering  as a continuous tracker of wrist position, 's median position error was 15.11 cm (MAE=16.97 cm) averaged across all trials and participants.\nIn terms of accuracy during input events outside the FOV, the position error of 's predicted release and grab events was\n14.43 (\u00b14.16) cm for ,\n16.41 (\u00b14.21) cm for ,\nand 14.85 (\u00b13.19) cm for .\n\nAnalyzing 's refined estimates of fingertip positions (15 cm offset to the wrist) using the markers attached to participants' fingernails, \nthe average error was 4.9 cm when using the 's wrist position as the basis\nand 14.0 cm when using 's estimated wrist orientation and position as the basis for refinement (which encompasses 's error of predicting wrist positions).\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Qualitative feedback\n\n\nAs shown in <ref> (bottom), participants again preferred performing the study using  overall, that is without requiring them to turn their heads to bring off-screen targets into view first.\n7 out of the 10 participants reported a preference for the  condition over .\nParticipants again mentioned the efficiency of not having to look for close-by off-screen targets before selecting them, although our analysis again showed that they had a slightly decreased success rate.\n\n\n\n\n \u00a7.\u00a7 Discussion\n\n\nOverall and compared to , participants were more careful in performing tasks in , both in the  and the  condition.\nThis is evident as the success rates for  and  both exceeded participants' results in the first study using the OptiTrack system (see\u00a0<ref>).\nThis also explains the longer average trial completion time in  as shown in <ref>. \n\n\nSpecific to the  condition, participants achieved a similar performance in terms of success rate.\nThis demonstrates the capability of our learning-based method to generalize to and support interaction for users unseen during training without the need for calibration or fine-tuning.\nIt also shows our method's suitability for proprioceptive interaction when possibly imperfect tracking influences user behavior and thus alters their interaction.\nFurther supporting our method was participants' qualitative feedback, which highlighted that 's tracking capabilities that extend and complement those of a regular headset were well-received.\n\nIn terms of tracking accuracy,  produced a higher position error than in .\nAgain, this likely resulted from the longer time that participants spent completing trials and, thus, the increased amount of time they interacted outside the field of view.\nThis result is also commensurate with our assessment of tracking error over time as shown in <ref>.\nNevertheless, the absolute error in position remained relatively stable over time and still enabled participants to complete the intended tasks successfully.\n\nThe results from  also highlight the suitability of our pinch detection.\nOur system is tuned towards a more conservative prediction of pinch events, resulting in a low number of wrongly detected peaks.\nThis rarely required participants to pinch more than once when our online detection missed an event.\n\nFinally, this evaluation also validated our previous assumption of a constant offset between 's predicted wrist position and the participant's fingertip.\nThe offset we found from the pinch position reported by the optical tracker was moderate and well below the error introduced by interacting based on proprioception outside the FOV in the first place.\n\n\n\n  \nComparison to related approaches. \nThe wrist position error for the interactive tracking in  is comparable to the results reported for 's RNN-based method\u00a0<cit.>.\nHowever, their results were obtained through an offline evaluation (similar to ) using a Kinect camera with limited accuracy, and the displayed user motions had a limited range in the yaw direction.\nWhile our performance falls short of the reported median tracking error of 8.8 cm for MUSE\u00a0<cit.> in a static setting with unsupervised user motions, the error becomes comparable in a medical rehabilitation (MR) application involving reach-and-grasp tasks (see\u00a0<ref>). In addition, MUSE relies on a 9-axis IMU with a magnetometer prone to interference and performs the tracking in a 5-DoF space with respect to the torso.\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 APPLICATIONS\n\n\nBeing complementary to the optical tracking abilities of commercial headsets, a multitude of applications can benefit from  during operation to support outside FOV interaction. \nA large number of game titles and immersive experiences rely on external tracking to obtain the position of the user's hand-held controllers around the body.\nSuch external tracking also supports comfortable interaction, where users can let their arms hang loosely by their sides, but still engage in an interactive experience thanks to controller input.\nWith , we aim to enable similar levels of interactivity while supporting the portability-optimized form factor of current VR headsets, which therefore track all input inside-out as opposed to relying on additional sensing infrastructure.\nTo that end, we prototyped three demonstrations that illustrate the benefits of  in various application areas.\n\n\n\n\n\n\n \u00a7.\u00a7 Beat Saber\n\nTo first illustrate the use of our system's extended tracking capabilities, we extended the Beat Saber gameplay to include omnidirectional targets. \nRather than presenting targets to users from a single direction, our game scene contains four tracks. \nSince users do not have a full visual understanding of the arriving targets, they have to rely more greatly on cues from the music to execute movements. \nFor instance, they may be encouraged to swing backward on a regular beat. \nWith inside-out tracking capabilities of HMDs, these movements would be difficult to track, precluding such interactions from being implemented in current applications. \n resolves this issue and extends opportunities for gameplay to take advantage of a more dynamic range of movements.\n\n\n\n \u00a7.\u00a7 Archery\n\nSimilar to Beat Saber, firing a bow requires movements that sometimes involve moving one's hand out of the tracking field-of-view of the head-mounted display. \nUnlike Beat Saber, archery further requires the user to steadily maintain an out-of-view hand position. \n nonetheless supports this and further enables the release interaction with the on-wrist IMU. \n\n\n\n \u00a7.\u00a7 Block Builder\n\nLastly, we implemented a block builder application to demonstrate the applicability of  in areas beyond games and play. \nThe Block Builder application consists of a workbench that is instrumented with containers for blocks around the edges.\nUsers can grab specific blocks from each container and piece them together on the workbench. \nAs in a variety of other everyday activities, such as typing, we do not necessarily interact with the world with visual control, but rather sometimes rely on our sense of proprioception for control.\nAs users gradually gain familiarity with the environment, we can expect users to rely more so on their spatial memory to grab items as opposed to visual control. \n enables this interaction.\n\n\n\u00a7 LIMITATIONS AND FUTURE WORK\n\n\nWhile the results of the method we introduced in this paper are promising, several limitations exist that deserve further investigation in future work.\n\n\n\n  \n as a method to compensate for IMU drift\nCombating the drift arising from estimating positions based on observations from inertial motions is a challenging problem.\nWith , we so far address this for the limited space of a person's reach, only for a short period of time, and only for stationary and standing scenarios.\n<ref> shows the development of tracking error over time, highlighting the challenge and remaining work needed to improve dead reckoning based on IMU signals.\n can therefore not be considered a general-purpose hand position estimator using inertial sensing.\nThe results we presented in this paper also assume reaching and grabbing tasks and may therefore not translate to other tasks users may perform in their proprioceptive reach, such as gesturing, drawing, or other kinds of spatial navigation.\nThe assumptions about the task, space, and scenarios we made to develop  specifically for stationary VR scenarios considerably set apart the problem we addressed from more general inertial-based odometry, which attempts dead reckoning over much larger areas in mobile and moving settings.\n\n\n\n  \nNon-stationary environments\nDuring our evaluation, participants stood still at a fixed position in the experimental space while performing all tasks as instructed.\nThough calibrated to each participant's body dimensions once at the beginning of the experiment, all targets were static and remained world-anchored.\nAlthough the experimenter verified throughout that participants did not step away from this position, the nature of the task did not allow us to control for a completely steady posture.\nTherefore, shifts in upper-body posture or slight shoulder rotations may have contributed to the errors we observed, as participants obtained no feedback about their position in the virtual space relative to the targets.\n\nFuture evaluations could extend our evaluation to include non-stationary environments where participants can move around.\nThis would additionally evaluate the robustness of the method to motion artifacts.\nBecause 's algorithm currently needs a PC with a good GPU, such an evaluation would require a mobile reimplementation of our method, using model-size reduction techniques and deployment on the embedded platform to support such operation.\n\n\n\n  \nComputational complexity\nDue to the self-attention layers, 's neural network incurs a complexity that grows quadratically with the length of the input sequence, limiting the maximum outside-the-field-of-view tracking duration.\nFuture work should consider architectures with recurrent elements and memory\u00a0<cit.> to avoid the repeated processing of the whole input sequence.\n\n\n\n  \nTracking accuracy\nWhile the tracking accuracy of  is state-of-the-art for the given input modality, running from IMU-based orientation estimates only given the last hand observation from the headset, the existing error leaves room for improvement, especially compared to the tracking capabilities of an optical tracking system or a head-mounted display.\nWe believe that a part of this challenge can be addressed by acquiring a much larger data set, which would allow our method to be refined for personalization and to adapt to varying body sizes.\n\n\n\n  \nInstrumentation of the wrist\n requires the user to wear a wristband with an embedded IMU to be tracked outside the FOV and to submit input commands.\nWhile this is a common sensor inside every smartwatch today, it still requires a separate device to operate (parts of) VR experiences, which is counter to the intuition of manufacturers to integrate all VR components into just the headset.\nGiven the existing ecosystem of watches and their various programming interfaces, we are excited about the possibility of transferring and adapting our method to run on commodity devices and to support out-of-the-box use.\nThis will necessarily entail optimization strategies on the performance level as mentioned above in order to reduce the computational cost of our model or outsource its estimations to the neural computing unit of a headset.\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nWe have presented , a wrist-worn sensing and position estimation method that complements existing AR and VR headsets in tracking the user's hand interactions outside the tracking field of view of their built-in cameras.\n obtains its input from a 6-axis IMU that is embedded in a wrist-worn band, which captures 3-axis acceleration and 3-axis rotational velocity.\nFrom these observations, 's learning-based inference pipeline estimates the user's current wrist orientation and position as soon as the hand leaves the tracking range of the headset.\n\nOur user studies showed the promise of our method, allowing participants to leverage their proprioception and interact with one out of 17 targets outside their FOV with an overall mean success rate of around 85% () and 87% ().\nThe outside FOV interaction in our study supported speed improvements over what would be required on today's standalone headsets\u2014turning one's head to bring the virtual content into view first and exclusively interact with it under visual control.\nWhile under visual control, participants successfully interacted with the correct targets in 19 of 20 cases, we quantified the drop in success rates under outside FOV operation:\nMonitored with an external motion capture system, participants' success rate dropped to 18 of 20 trials when interacting purely based on their proprioception (and with targets that remained static and world-anchored).\nOf these 18 successful trials, \u223c17\u00a0interactions were correctly detected using \u2014without any external infrastructure or cameras.\n\nOur study also uncovered 's potential as a tracking complement for raw 3D positions outside the headset's tracking FOV.\n's median tracking error of 7 cm for short-term outside-field-of-view interactions outperforms related techniques while requiring a commodity sensor that is commonly found in smartwatches and current hand-held controllers (<ref>).\n\n opens up an exciting opportunity to broaden the interactive space for AR and VR headsets that may now be able to better leverage users' proprioception within the large outside FOV space around the user that can accommodate convenient reach.\n\nCollectively, we believe that  will support the development of a future generation of AR and VR headsets that optimize for form factor and power consumption by outsourcing some of the computation to other devices.\nWe see an opportunity for adaptive interaction techniques\u2014as commonly used for selection tasks where input is ambiguous (e.g., touchscreen typing)\u2014to operate in conjunction with 's estimates to support users' proprioceptive interactions.\nEven more, we see the future potential for  to support hand tracking and interaction inside the headset's FOV, thereby alleviating the computational cost of performing computer vision-based camera processing in real-time with a hand position estimation that may run on an embedded wrist-worn device.\n\n\n\n\n\n\n\n\nWe sincerely thank Manuel Meier for helpful discussions and comments.\nWe are grateful to NVIDIA for the provision of computing resources through the NVIDIA Academic Grant.\nWe thank the anonymous reviewers and all participants of our user studies.\n\n\n\n\nACM-Reference-Format\n\n\n\n\n\n\n\n"}