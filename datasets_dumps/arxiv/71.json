{"entry_id": "http://arxiv.org/abs/2303.07283v1", "published": "20230313170055", "title": "CHESS: A Framework for Evaluation of Self-adaptive Systems based on Chaos Engineering", "authors": ["Sehrish Malik", "Moeen Ali Naqvi", "Leon Moonen"], "primary_category": "cs.SE", "categories": ["cs.SE", "cs.NE"], "text": "\n\nAlign and Attend: Multimodal Summarization with Dual Contrastive Losses\n    Bo He^1Part of this work was done when Bo was an intern at Adobe Research., Jun Wang^1, Jielin Qiu^2, Trung Bui^3, Abhinav Shrivastava^1, Zhaowen Wang^3\n\n^1University of Maryland, College Park     ^2Carnegie Mellon University     ^3Adobe Research\n\n{bohe,abhinav}@cs.umd.edu, junwong@umd.edu, jielinq@andrew.cmu.edu, {bui,zhawang}@adobe.com\n\n    March 30, 2023\n========================================================================================================================================================================================================================================================================================================================================================\n\nplain\n\n\n\n  There is an increasing need to assess the correct behavior of self-adaptive and self-healing systems due to their adoption in critical and highly dynamic environments.\n  However, there is a lack of systematic evaluation methods for self-adaptive and self-healing systems. \n  We proposed CHESS, a novel approach to address this gap by evaluating self-adaptive and self-healing systems through fault injection based on chaos engineering (CE). \n  \n  \n  The artifact presented in this paper provides an extensive overview of the use of CHESS through two microservice-based case studies: a smart office case study and an existing demo application called Yelb. \n  It comes with a managing system service, a self-monitoring service, as well as five fault injection scenarios covering infrastructure faults and functional faults. \n  Each of these components can be easily extended or replaced to adopt the CHESS approach to a new case study, help explore its promises and limitations, and identify directions for future research.\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n \n\n\n\n\u00a7 INTRODUCTION\n\n\n\nSelf-adaptive systems (SAS) and self-healing systems (SHS) are becoming increasingly important in fields such as the Internet of Things, Industry 4.0, and smart cities\u00a0<cit.>.\nThese systems are designed to operate in highly dynamic environments and are expected to handle uncertainty and unanticipated behavior, as well as provide fault tolerance and resilience. \nHowever, due to the complexity and dynamic nature of these systems, it is challenging to anticipate all possible scenarios that these systems will encounter.\nThis is particularly important for the evaluation of these systems, which requires assessing the correct behavior of these systems. \n\nThere has been a growing concern among researchers about the lack of systematic evaluation for SAS and SHS\u00a0<cit.>.\nA recent systematic mapping study found that only a small percentage of studies in this field \nfocus on evaluating previously developed applications\u00a0<cit.>. \nIn addition, there are limited tools available to support evaluations based on runtime measures, \nwith most studies focusing on evaluating the models used to design the system\u00a0<cit.>. \nThese models, however, may not take into account all potential scenarios \nthat a system may encounter during operation. \nWhile runtime models offer a solution to these limitations, \nthey also come with their own challenges, such as maintenance and the need for model creation\u00a0<cit.>.\nHence, there is a need for mechanisms to evaluate SAS and SHS based on runtime measures \nthat consider potential scenarios a system may encounter during operation. \n\nTo fill this gap, our earlier work proposed CHESS, \nan approach for the systematic evaluation of self-adaptive \nand self-healing systems that build on chaos engineering principles\u00a0<cit.>. \nCHESS systematically perturbates the system-under-evaluation \nand records how the system responds to those perturbations.\nThe artifact[\u00a0CHESS artifact on Zenodo: <https://doi.org/10.5281/zenodo.6817763>] presented in this \npaper \nprovides an extensive overview of the use of \nCHESS through two microservice-based case studies: \na smart office case study and an existing demo application called Yelb. \nConcretely, the artifact consists of \n(i) predefined functional and infrastructural level fault injection scenarios,\n(ii) a self-monitoring service that presents extensive logs \nfor the deployed services' normal and abnormal behaviors,\n(iii) the managing system service that reacts to the system's abnormal behavior traces and brings the system back to a stable condition, \nand (iv) a comparison of the service failure and cascading effects with and without deployment \nof the managing system service. \n\nThe remainder of this paper is organized as follows.\nIn <ref>, we summarize self-adaptive and self-healing systems for the microservice architecture, evaluation of these systems based on CE, and position CHESS in the landscape of self-adaptive system artifacts. \n<ref> presents the design and architecture of CHESS, along with the microservice-based case studies and test scenarios considered for the fault injection.\n<ref> describes how to use the artifact with both of the case studies and presents the results of the fault injection scenarios. \nWe conclude in <ref>, including discussions of the artifact's applicability and directions for future work. \n\n \n\n\n\n\u00a7 BACKGROUND AND POSITIONING OF THE ARTIFACT\n\n\n\nWe briefly introduce the basics of SAS and SHS, and their evaluation in the context of microservices architecture, and highlight how CHESS complements \nthe existing artifacts for engineering self-adaptive systems. \n\n\n\n \u00a7.\u00a7 Self-Adaptive System for Microservices Architecture\n\n\nSelf-adaptive systems are a class of software systems that have the ability to automatically adapt to changes in their environment\u00a0<cit.>.\nAt a high level, these systems can be seen as comprising a managed system that is controlled by a managing system, generally realized through a MAPE-K feedback loop\u00a0<cit.>. \n\nSelf-adaptive systems for microservices architecture can bring multiple benefits, \nsuch as increased scalability, improved reliability, and reduced maintenance costs.\nStudies have suggested that there is an overall improvement in the management of microservices-based applications,\nallocation of microservices among the available servers,\nquality attributes such as performance, scalability, and resilience \nthrough the introduction of self-healing, self-management, \nand self-optimization properties\u00a0<cit.>.\nFurthermore, an architecture-based self-adaptation framework with a MAPE-K feedback loop \nfor a microservice as a managed system shows a reduced cost of ownership \nand faster self-adaptation\u00a0<cit.>. \nOn the other hand, the introduction of microservices architecture, as a managing system, can improve the self-adaptation \ncapabilities of systems for various measures such as run-time data analysis\u00a0<cit.>.\nSome challenges in developing a self-adaptive system based on microservices include\ndeveloping monitoring and adaptation mechanisms for ensuring quality attributes;\ndetermining the level of distribution, observability, \nand granularity for deploying control components; \nand determining mechanisms for evaluation of the given quality attributes\u00a0<cit.>.\n\n\n\n \u00a7.\u00a7 Evaluation of SAS and SHS based on Chaos Engineering\n\n\nThe evaluation of self-adaptive and self-healing systems is an important aspect of their design and deployment.\nIn our previous work\u00a0<cit.>, we defined the evaluation of self-adaptive and self-healing systems as\n\"an approach to determine if a system meets objectives under operation, identify areas in which the system performs as well as desired or predicted, and provide evidence to the value and applicability of the system.\" \n\nSeveral approaches to the evaluation of SAS and SHS include model-based evaluation\u00a0<cit.>, \nmetric-based evaluation\u00a0<cit.>, model checking\u00a0<cit.>, \nand runtime testing and verification\u00a0<cit.> each with its benefits and limitations. \nHowever, none of these evaluation approaches are viable for evaluation \ncovering the execution of the system under real-life failure scenarios.\nTherefore, to address this gap, we introduce a mechanism that builds on chaos engineering principles.\nChaos engineering (CE) is the practice of intentionally causing and studying controlled chaos \nwithin software systems operating in realistic environments, \nwith the goal of increasing the systems' resilience and ability to handle unforeseen circumstances\u00a0<cit.>.\nThe core tenets of CE can be outlined as four main principles, \nwhich include formulating hypotheses based on the steady-state behavior of systems, \nintroducing variations to real-world events, \nconducting experiments in a production environment, and automating these experiments for continuous execution.\nOur approach evaluates the systems through a systematic process that involves exposing the system to faults and testing its ability to recover from such perturbations.\n\n\n\n \u00a7.\u00a7 Positioning of the Artifact\n\n\nArtifacts play a vital role in advancing the field of self-adaptation. \nThey serve as tangible examples of the algorithms and techniques developed by researchers, \nallowing for their evaluation and assessment. \nIn addition, artifacts provide problematic scenarios and solutions that can inspire further research, \nas well as facilitate the comparison of results among different studies.\nThe self-adaptive exemplars website[\u00a0<http://self-adaptive.org/exemplars/>] gives an overview of re-usable artifacts produced by researchers and engineers in the self-healing and self-adaptive community.\n\nVarious existing artifacts focus on web-based systems, microservices architecture, or cloud environments that assist in the evaluation of the managed systems. \nZnn.com\u00a0<cit.> is a web-based information system that mimics \nreal-world systems and provides an experimental environment to facilitate the evaluation. \nThe exemplar applies a self-adaptive framework, Rainbow and presented an \nevaluation of the self-adaptive system based on a benchmark.\nHogna\u00a0<cit.> is a platform for deploying self-managing \nweb applications on the cloud. \nIt automates operations, monitors the health of the applications, extracts metrics, \nand analyzes performance data based on a model to create and execute an action plan. \nK8-Scalar\u00a0<cit.> is an exemplar that allows the evaluation of different self-adaptive approaches to autoscaling container-orchestrated services.\nIt is based on Docker, and Kubernetes, and extends a generic testbed for scalability evaluation of large-scale systems called Scalar.\nSEAByTE\u00a0<cit.> enhances the automation of continuous A/B testing of a \nmicroservice-based system. \nFurthermore, exemplars such as SWIM\u00a0<cit.>, DARTSim\u00a0<cit.>, \nand RDMSim\u00a0<cit.> consist of managed systems \nwhich can assist in the evaluation of external adaptation managers.\n\nThe present artifact represents a departure from existing artifacts in the field, \nas it prioritizes the evaluation of managing systems \nby inducing faults within the managed system. \nThus, as opposed to primarily focusing on the creation of new self-adaptive approaches, \nit supports one of the critical tasks of software engineering research, i.e., the systematic evaluation of novel approaches.\nThis aligns with the growing desire to produce artifacts in self-adaptation\nthat support industry-relevant research\u00a0<cit.>. \n\n  \n\n \n\n\n\n\n\n\u00a7 CHESS\n\n\n\nIn this section, we present the architecture of CHESS, the demo applications used for testing, and the test scenarios for fault injection. \n\n\n\n \u00a7.\u00a7 Design and Architecture\n\n\nFigure\u00a0<ref> presents a detailed architecture for the CHESS approach. It consists of four main modules: containerized deployment cluster (CDC), system monitor, system manager, and fault injection. These modules are presented below.\n\nContainerized Deployment Cluster (CDC)\nThis artifact discusses the process of introducing faults into microservice-based applications deployed in a containerized environment using Kubernetes (K8s). \nThe environment details are shown in Table\u00a0<ref>.\nThe K8s cluster starts with a total of 20 GB memory and 4 CPUs to ensure that it has sufficient resources to run the demo applications.\nTo make the demo applications accessible from outside the cluster, we have configured the K8s cluster with MetalLB, which is an addon that enables external IP services in K8s. \nIn addition, the Istio service mesh is installed in the cluster to monitor the microservices traffic flow and to provide an easy way to manage the traffic between microservices.\nIn order to visualize the data traffic and services' status, we have installed Grafana, Kiali, and Prometheus monitoring tools. \nGrafana is used to visualize the data traffic, while Kiali is used to visualize the services' status. \nPrometheus is used to query data metrics from its database and to store the data for later analysis. \nThe combination of these tools provides a complete solution for monitoring microservices-based applications and for evaluating the fault tolerance of these applications.\n\n\n\nSystem Monitor\nThe system monitor module monitors the status of the running services and performs necessary checks to ensure their validity. \nIts purpose is to observe the behavior of the system under various conditions, distinguish the system's normal behavior patterns from abnormal ones, and alert the system manager in case of any abnormal behavior.\n\nSystem Manager\nThe system manager module plays a crucial role in ensuring the continuous operation of services deployed in the CDC. \nThe primary responsibility of the system manager module is to receive abnormal system behavior alerts from the system monitor module and handle the recovery phase of the services that encounter faults.\nIn this artifact, the system manager adopts a rule-based approach for fault recovery. \nThe recovery process is automated and follows a set of predefined rules, which have been configured for the selected demo applications.\nThe use of the system manager module also enables us to compare the impact of faults, known as the blast radius, for different fault injection scenarios, both with and without the system manager. \nThis helps us to evaluate the effectiveness of the system manager in mitigating the effects of faults in the running services.\n\nFault Injection\nThe fault injection module follows the principles of Chaos Engineering (CE) to induce faults in the deployed application services. \nA set of chaos experiments is carefully designed and scripted, for each fault injection scenario, in order to inject the desired faults into the services. \nA chaos experiment template is shown in Table\u00a0<ref>. A chaos experiment first defines the steady state with a probe function and a check against a tolerance. \nOnce the steady state is met, the chaos action method is called to inject the respective fault, using the service_name and namespace arguments to identify the service.\nA pause before or after the fault injection can also be added.\nA virtual environment in Python is prepared with \"chaostoolkit\" and \"chaostoolkit-Kubernetes\" libraries to execute the chaos injection scripts. Each chaos injection generates chaos logs, which can be used for system observation purposes.\n\n\n  \n\n\n \u00a7.\u00a7 Demo Applications for Testing\n\n \nThe artifact uses the smart office case study from our earlier paper\u00a0<cit.> and an open source example application named Yelb[\u00a0The Yelb application was reused from <https://github.com/mreferre/yelb>]\nas the demo applications for chaos injection and testing.\n\nSmart Office Case Study\nThe smart office case study consists of nine services, including three input services (temperature sensor, motion sensor, and external weather), two control services (heating control, light control), two actuator services (heating actuator, light actuator), an MQTT broker, and a user interface. \nThe control services retrieve periodic sensing data and weather data, then use rules to control heating and lighting actuators. \nThe service graph for the smart office case study is shown in Figure\u00a0<ref>.\n\n\n\n Yelb Application\nThe Yelb application consists of four services: a user interface service, an application server (appserver) service, a redis server service, and a database service. \nIt allows users to vote for their preferred restaurant among given options, and updates a pie chart based on the number of votes received for each option. \nThe Istio view of the service graph for the Yelb application is shown in Figure\u00a0<ref>.\n\n\n\n\n\n \u00a7.\u00a7 Test Scenarios for Fault Injection\n \n\nChaos Engineering allows the injection of two types of faults in a running application: infrastructure level and functional level.\nIn the context of microservices-based applications deployed in a CDC cluster, the infrastructure level faults include the faults that occur due to issues with CDC configurations and resources.\nThese faults can be mostly injected using predefined functions available within various chaos toolkits.\nThe functional level faults are unique to the application and require some additional knowledge of the application's backend logic and data flow.\nChaos Engineering provides support for functional level faults, which can be injected by writing custom probes and action methods for a specific scenario and calling them in a chaos experiment script. \n\nThe artifact presents 5 fault injection scenarios, 4 using the smart office case study and 1 using the Yelb app.\n\n\n  * a deployed sensor is down unexpectedly.\n\t\n  * a deployed sensor sends erroneous readings.\n\t\n  * a running service is down abruptly.\n\t\n  * a running service is delayed.\n    \n  * a running service is loaded with a high service request rate (SRR).\n\nTable\u00a0<ref> presents an overview of faults injected with respect to the fault level and the target application. \nThe infrastructure level fault injection is executed on the system service in a black-box manner.\nThe infrastructure & functional level fault execution is performed in a grey-box manner where some additional access to the service's functional status is required.\n\n\n\n \n\n\n\n\u00a7 EXPERIMENTS\n\n\n\nThe artifact is available in a virtual machine.\nThe directory structure for artifact implementation is shown in Figure\u00a0<ref>.\n\n\n\n\n\n \u00a7.\u00a7 How to use the artifact: Smart-Office Scenarios\n\n\nThere are four main steps, along with two optional steps for online data visualization, to run the artifact for the smart-office scenarios (refer to Table\u00a0<ref>).\n\nThe first step is to create or start the K8s cluster by running the script cluster.py. \nThis will create a new minikube-based K8s cluster with 4 CPUs and 2000 MB of memory. \nThe newly created cluster will be configured with metalLB load balancer and have istio mesh installed, with istio-injection enabled and monitoring tools installed.\nNext, you can deploy the demo application's services for a specified scenario by running the script system_monitor.py X Y. \nThe value for X represents the scenario number and can be 1, 2, 3, or 4 to run FS-1, FS-2, FS-3, or FS-4, respectively. \nThe value for Y is either 0 if you want to run the system monitor alone, or 1 if you want to run the system monitor with the system manager to recover the services from failures.\nTo view the running deployments and services in the k8s cluster, run the command: kubectl get all. \nStep three involves activating the virtual chaos environment for chaos injection. \nThen, run the chaos command chaos run scenario-X.yml to inject a failure corresponding to the running scenario. \nWhere, scenario-X represents the chaos experiment to be injected, and X can be 1, 2, 3, or 4 for FS-1, FS-2, FS-3, or FS-4, respectively.\nFinally, you can use online monitoring tools to examine the online monitoring data for service statuses and traffic flow. \nIf desired, the Kiali dashboard and Grafana dashboard can be launched in separate terminals to observe the service data metrics.\n\n\n\n\n\n\n\n \u00a7.\u00a7 How to use the artifact: Yelb-App Scenario\n\n\nThe execution process for the Yelb application scenario involves four main steps, with two additional optional steps for online data visualization, as described in Table\u00a0<ref>.\n\nThe first step is to create the K8s cluster, followed by the deployment of demo application services. \nThe app-server service can be enabled for auto-scaling using the horizontal pod auto-scaling (HPA) functionality of K8s by running the following command: kubectl autoscale deployment yelb-appserver \u2013cpu-percent=10 \u2013min=1 \u2013max=20. \nThe parameters cpu-percent, min, and max represent the dedicated CPU percentage, the minimum number of replicas to be running, and the maximum number of replicas that can run under heavy load, respectively. These values can be adjusted based on the available resources in the cluster.\nIn the third step, the pods' logger is executed by running the shell script numpods.sh. \nThis script records changes in the number of replicas over time and writes the results to a log file, which can later be analyzed to understand the relationship between the number of active users and the usage of CPU resources, and the increase or decrease in replicas.\nThe fourth step is to run the scripted chaos for the fast voting onto the yelb-appserver. \nFinally, for real-time data observation, the Kiali dashboard and Grafana dashboard can be launched in separate terminals, as an optional step.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Results\n\n\nSmart-Office Scenarios\nThe first four scenarios (FS1, FS2, FS3, and FS4) are designed to evaluate the effect of chaos injection and failures on a running system, both with and without the deployment of a system management service. \nThey provide a comparison of the system's behavior when it fails to recover from an injected fault and when it successfully recovers to evaluate the system's behavior and recovery capabilities.\n Figure\u00a0<ref> depicts the system monitor output for FS-1 when run without a management service (i.e., running python3 system_monitor 1 0). \n In this scenario, an injected fault in the form of data corruption leads to a cascading failure that the system is unable to recover. \n On the other hand, Figure\u00a0<ref> shows the system monitor output for FS-1 when run with a management service (i.e., running python3 system_monitor 1 1). \n In this case, the system manager is able to quickly identify and recover from the failed service, resulting in a much smoother and more efficient recovery process.\n\n\n\n\n\n\n\n\n\n\n\nYelb-App Scenario\nThe FS5 evaluates the scalability of services deployed in a Kubernetes cluster by imposing different loads on the service.\nThe resulting log file captures two types of data:  information about all running pods (obtained through kubectl get pods) and data on HPA deployment (obtained through kubectl get hpa).\nThe log is updated every 30 seconds and records the name, status, number of restarts, and age of each running pod (as shown in Figure\u00a0<ref>), and the name, CPU targets, minimum and maximum number of pods, number of replicas, and age of the HPA deployment (as shown in Figure\u00a0<ref>).\nThis log can be analyzed to uncover patterns in the service's performance and resource utilization over time. \nFor example, Figure\u00a0<ref> presents sample data extracted from observing generated log files.\nThe extracted data depicts a dramatic increase in CPU usage and the number of replicas over a 22-minute period.\n\n\n\n\n\n\n\n \n\n\n\n\u00a7 CONCLUDING REMARKS\n\n\n\nContributions\nThis paper presents an artifact that provides a detailed overview of \nhow the CHESS approach can be used to evaluate a system's \nresilience and ability to recover from various types of faults. \nThe implemented modules highlight the effectiveness of the system monitoring and system managing services in detecting and mitigating failures in the system. \nThe artifact consists of (i) predefined\nfunctional and infrastructural level fault injection scenarios,\n(ii) a self-monitoring service that presents extensive logs\nfor the deployed services\u2019 normal and abnormal behaviors,\n(iii) the managing system service that reacts to the system\nabnormal behavior traces and brings the system back to the\nstable condition, and (iv) a comparison of the service failure\nand cascading effects with and without deployment of the\nmanaging system service.\nThe artifact is available on Zenodo,^<ref> and a demo video is on YouTube.[\u00a0Artifact demo video: <https://youtu.be/CBcaPJgpi-o>]\n\nApplicability\nThe artifact provides the SEAMS community with support for one of the critical tasks of software engineering research, i.e., the systematic evaluation of novel approaches.\nIt aligns with the growing desire to produce self-adaptation artifacts that support industry-relevant research\u00a0<cit.>\nby using chaos engineering to systematically observe containerized applications in Docker\u00a0<cit.>.\nArtifact components are reusable, extendable, and modifiable for new case studies. \nExisting fault scenarios can be combined and expanded to create complex ones. \nThe custom fault injection scripts can inspire exploration of grey-box level fault injection and testing.\n\nFuture work\nDirections of interest include exploring the use of observability data and system logs for chaos engineering-controlled data synthesis. \nIn addition, techniques for the automated selection of regions for chaos experiments \nbased on the health and performance status of services are also of interest. \nFinally, the inclusion of contextual information, such as ontologies or knowledge graphs, \ncan also be considered to enhance fault injection targeting.\n\n\n\nAcknowledgments\nThis research is supported by the Research Council of Norway \nthrough the \ncureIT project (#300461)\nand used resources provided by \nthe Experimental Infrastructure for Exploration of Exascale Computing (eX3), \nsupported by the Research Council of Norway (#270053)\n.\n\n \n\n\n-1.7cm \n\n\n"}