{"entry_id": "http://arxiv.org/abs/2303.06705v1", "published": "20230312165408", "title": "Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement", "authors": ["Yuanhao Cai", "Hao Bian", "Jing Lin", "Haoqian Wang", "Radu Timofte", "Yulun Zhang"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\n\nRetinexformer: One-stage Retinex-based \n Transformer for  Low-light Image Enhancement\n    \n\tYuanhao Cai ^1,2, Hao Bian ^1,2,, Jing Lin ^1,2, \n Haoqian Wang ^1,2,, Radu Timofte ^3,4, Yulun Zhang ^4\n\n\t^1 Shenzhen International Graduate School, Tsinghua University, \n ^2  Shenzhen Institute of Future Media Technology, ^3 University of W\u00fcrzburg, ^4 ETH Z\u00fcrich\n\n    March 30, 2023\n===============================================================================================================================================================================================================================================================================\n\n\nempty\n\n\n\n\n\n\nWhen enhancing low-light images, many deep learning algorithms are based on the Retinex theory. However, the Retinex model does not consider the corruptions hidden in the dark or introduced by the light-up process. Besides, these methods usually require a tedious multi-stage training pipeline and rely on convolutional neural networks, showing limitations in capturing long-range dependencies. In this paper, we formulate a simple yet principled One-stage Retinex-based Framework (ORF). ORF first estimates the illumination information to light up the low-light image and then restores the corruption to produce the enhanced image. We design an Illumination-Guided Transformer (IGT) that utilizes illumination representations to direct the modeling of non-local interactions of regions with different lighting conditions. By plugging IGT into ORF, we obtain our algorithm, Retinexformer. Comprehensive quantitative and qualitative experiments demonstrate that our Retinexformer significantly outperforms state-of-the-art methods on seven benchmarks. The user study and application on low-light object detection also reveal the latent practical values of our method. Codes and pre-trained models will be released.\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\nLow-light image enhancement is an important yet challenging task in computer vision. It aims to improve the poor visibility and low contrast of low-light images and restore the corruptions (e.g., noise, artifact, color distortion, etc.) hidden in the dark or introduced by the light-up process. These issues challenge not only human visual perception but also other vision tasks like nighttime object detection.\n\n\n\nHence, a large number of algorithms have been proposed for low-light image enhancement. \nPlain methods like histogram equalization and gamma correction tend to produce undesired artifacts because they barely consider the  illumination factors. Traditional cognition methods rely on the Retinex theory\u00a0<cit.> that assumes the color image can be decomposed into two components, i.e., reflectance  and illumination. Different from plain methods, traditional methods  focus on illumination estimation but usually introduce severe noise or distort color locally because these methods assume that the images are noise- and color distortion-free. This is inconsistent with real under-exposed scenes. \n\nWith the development of deep learning, convolutional neural networks (CNNs) have been applied in low-light image enhancement. These CNN-based methods are mainly divided into two categories. The first category directly employs a CNN to learn a brute-force mapping function from the low-light image to its normal-light counterpart, thereby ignoring human color perception. This kind of methods lack interpretability and theoretically proven properties. The second category is inspired by the Retinex theory. These methods\u00a0<cit.> usually suffer from a multi-stage training pipeline. They employ different CNNs to decompose the color image, denoise the reflectance, and adjust the illumination, respectively. These CNNs are first trained independently and then connected together to be finetuned end-to-end. The training process is tedious and time-consuming. \n\n\n\nIn addition, these CNN-based methods show limitations in capturing long-range dependencies and non-local self-similarity, which are critical for image restoration. The recently rising deep learning model, Transformer, may provide a possibility to address this drawback of CNN-based methods. However, directly applying original vision Transformers for low-light image enhancement may encounter an issue. The computational complexity is quadratic to the input spatial size. This  computational cost may be unaffordable. \nDue to this limitation, some CNN-Transformer hybrid algorithms like SNR-Net\u00a0<cit.> only employ a single global Transformer layer at the lowest spatial resolution of a U-shaped CNN. Thus, the potential of Transformer for low-light image enhancement still remains under-explored. \n\nTo cope with the above problems, we propose a novel method,  Retinexformer, for low-light image enhancement. Firstly, we formulate a simple yet principled One-stage Retinex-based Framework (ORF). We revise the original Retinex model  by introducing perturbation terms to the reflectance and illumination for modeling the corruptions. Our ORF estimates the illumination information and uses it to light up the low-light images. Then ORF employs a corruption restorer to suppress noise, artifacts, under-/over-exposure, and color distortion. Different from  previous Retinex-based deep learning frameworks that suffer from a tedious multi-stage training pipeline, our ORF is trained end-to-end in a one-stage manner. Secondly, we propose an Illumination-Guided Transformer (IGT) to model the long-range dependencies. The key component of IGT is Illumination-Guided Multi-head Self-Attention (IG-MSA). IG-MSA  exploits the illumination representations to direct the computation of self-attention and enhance the interactions between regions of  different exposure levels. Finally, we plug IGT into ORF as the corruption restorer to derive our method, Retinexformer. As shown in Fig.\u00a0<ref>, our Retinexformer surpasses state-of-the-art (SOTA) Retinex-based deep learning methods by large margins on various datasets. Especially on SID\u00a0<cit.>, SDSD\u00a0<cit.>-indoor, and LOL-v2\u00a0<cit.>-synthetic, the improvements are over 6 dB.\n\nOur contributions can be summarized as follows:\n\n\t\n\n\t\n  * We propose a novel Transformer-based algorithm, Retinexformer, for  low-light image enhancement. \n\t\n\n\t\n  * We formulate a one-stage Retinex-based low-light enhancement framework, ORF, that enjoys an easy one-stage training process and models the corruptions well.\n\t\n\n\t\n  * We design a new self-attention mechanism, IG-MSA, that utilizes the illumination information as a key clue to guide the modeling of long-range dependences.\n\t\n\n\t\n  * Quantitative and qualitative experiments show that our Retinexformer outperforms SOTA algorithms on seven datasets. The results of user study and low-light detection also suggest the practical values of our method.\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\t\n\n\n\n\n\n \u00a7.\u00a7 Low-light Image Enhancement\n\n\n\n\n\nPlain Methods. Plain methods like histogram equalization\u00a0<cit.> and Gama Correction (GC)\u00a0<cit.> directly amplify the low visibility and contrast of under-exposed images. Yet, these methods barely consider the illumination factors, making the enhanced images perceptually inconsistent with the real normal-light scenes.\n\n\n\nTraditional Cognition Methods. Different from plain algorithms, conventional  methods\u00a0<cit.> bear the illumination factors into consideration. They rely on the Retinex theory and treat the reflectance component of the low-light image as a plausible solution of the enhanced result. For example, Guo et al.\u00a0<cit.> propose to refine the initial estimated illumination map by imposing a structure prior on it. Wang et al.\u00a0<cit.> design an algorithm NPE to preserve the naturalness of details during the enhancement of non-uniform illumination images. However, these methods naively assume that the low-light images are corruption-free, leading to severe noise and color distortion issues during the enhancement process. Besides, these methods rely on hand-crafted priors, usually requiring careful parameter tweaking and suffering from poor generalization ability.\n\n\n\n\n\nDeep Learning Methods. With the rapid progress of deep learning, CNN\u00a0<cit.> has been widely used in low-light image enhancement. For instance, Lore et al.\u00a0<cit.> propose a stacked sparse denoising auto-encoder LLNet for joint low-light enhancement and noise suppression. Wei et al.\u00a0<cit.> and follow-up works\u00a0<cit.> combine the Retinex decomposition with deep learning. However, these methods usually suffer from a tedious multi-stage training pipeline. Several CNNs are employed to learn or adjust different components of the Retinex model, respectively. Wang et al.\u00a0<cit.> propose a one-stage Retinex-based  CNN, dubbed DeepUPE, to directly predict the illumination map. Nonetheless, DeepUPE does not consider the corruption factors, leading to amplified noise and color distortion when lighting up  under-exposed photos. In addition, these CNN-based methods also show limitations in capturing long-range dependencies.\n\n\n\n\n\n \u00a7.\u00a7 Vision Transformer\n\n\n\nThe natural language processing model, Transformer, is proposed in \u00a0<cit.> for machine translation. In recent years, Transformer and its variants have been applied in many computer vision tasks and achieved impressive results in high-level vision (e.g., image classification\u00a0<cit.>, semantic segmentation\u00a0<cit.>, object detection\u00a0<cit.>, etc.) and low-level vision (e.g., image restoration\u00a0<cit.>, image synthesis\u00a0<cit.>, etc.). For example, \nXu et al.\u00a0<cit.> propose an SNR-aware CNN-Transformer hybrid network, SNR-Net, for low-light image enhancement. However, SNR-Net only employs a single  global Transformer layer at the lowest resolution of a U-shaped CNN due to the enormous computational costs of the vanilla global Transformer. The potential of Transformer has not been fully explored for low-light image enhancement.\n\n\n\n\n\n\u00a7 METHOD\n\n\n\nFig.\u00a0<ref> illustrates the overall architecture of our method. As shown in Fig.\u00a0<ref> (a), our Retinexformer is based on our formulated One-stage Retinex-based Framework (ORF). ORF consists of an illumination estimator (i) and a corruption restorer (ii). We design an Illumination-Guided Transformer (IGT) to play the role of the corruption restorer. As depicted in Fig.\u00a0<ref> (b), the basic unit of IGT is Illumination-Guided Attention Block (IGAB), which is composed of two layer normalization (LN), an Illumination-Guided Multi-head Self-Attention (IG-MSA) module, and a feed-forward network (FFN). Fig.\u00a0<ref> (c) shows the details of IG-MSA.\n\n\n\n\n\n \u00a7.\u00a7 One-stage Retinex-based Framework\n\n\n\n\nAccording to  the Retinex theory. A low-light image \ud835\udc08\u2208\u211d^H\u00d7 W\u00d7 3 can be decomposed into a reflectance image  \ud835\udc11\u2208\u211d^H\u00d7 W\u00d7 3 and an illumination map  \ud835\udc0b\u2208\u211d^H\u00d7 W as\n\n\n\n    \ud835\udc08 = \ud835\udc11\u2299\ud835\udc0b,\n\nwhere \u2299 denotes the element-wise multiplication. This Retinex model assumes \ud835\udc08 is corruption-free, which is inconsistent with the real under-exposed scenes. We analyze that the corruptions mainly steam from two factors. Firstly, the high-ISO and long-exposure imaging settings of dark scenes inevitably introduce noise and artifacts. Secondly, the light-up process may amplify the noise and artifacts and also cause under-/over-exposure  and color distortion, as illustrated in the zoomed-in patch  i and ii of Fig.\u00a0<ref> (a).\n\nTo model the corruptions, we reformulate Eq.\u00a0(<ref>) by introducing a perturbation term for \ud835\udc11 and \ud835\udc0b respectively, as\n\n\n\n    \ud835\udc08   = (\ud835\udc11 + \ud835\udc11\u0302) \u2299 (\ud835\udc0b + \ud835\udc0b\u0302) \n       = \ud835\udc11\u2299\ud835\udc0b + \ud835\udc11\u2299\ud835\udc0b\u0302 + \ud835\udc11\u0302\u2299 (\ud835\udc0b + \ud835\udc0b\u0302),\n\nwhere \ud835\udc11\u0302\u2208\u211d^H\u00d7 W\u00d7 3 and \ud835\udc0b\u0302\u2208\u211d^H\u00d7 W denote the perturbations. Similar to <cit.>, we regard \ud835\udc11 as a well-exposed image. To light up \ud835\udc08, we element-wisely multiply the two sides of Eq.\u00a0(<ref>) by a light-up map \ud835\udc0b\u0305 such that \ud835\udc0b\u0305\u2299\ud835\udc0b = 1 as\n\n\n\n    \ud835\udc08\u2299\ud835\udc0b\u0305 = \ud835\udc11 + \ud835\udc11\u2299 (\ud835\udc0b\u0302\u2299\ud835\udc0b\u0305) + (\ud835\udc11\u0302\u2299 (\ud835\udc0b + \ud835\udc0b\u0302)) \u2299\ud835\udc0b\u0305,\n\nwhere \ud835\udc11\u0302\u2299 (\ud835\udc0b + \ud835\udc0b\u0302) represents the noise and artifacts hidden in the dark scenes and are amplified by \ud835\udc0b\u0305. \ud835\udc11\u2299 (\ud835\udc0b\u0302\u2299\ud835\udc0b\u0305) indicates the under-/over-exposure and color distortion caused by the light-up process. We simplify Eq.\u00a0(<ref>) as\n\n\n\n    \ud835\udc08_lu= \ud835\udc08\u2299\ud835\udc0b\u0305 = \ud835\udc11 + \ud835\udc02,\n\nwhere \ud835\udc08_lu\u2208\u211d^H\u00d7 W\u00d7 3 represents the lit-up image and \ud835\udc02\u2208\u211d^H\u00d7 W\u00d7 3 indicates the overall corruption term. Subsequently, we formulate our ORF as\n\n\n\n    (\ud835\udc08_lu, \ud835\udc05_lu) = \u2130(\ud835\udc08, \ud835\udc0b_p), \u00a0\u00a0\u00a0\u00a0\ud835\udc08_en = \u211b(\ud835\udc08_lu, \ud835\udc05_lu),\n\nwhere \u2130 denotes the illumination estimator and \u211b represents the corruption restorer. \u2130 takes \ud835\udc08 and its  illumination prior map \ud835\udc0b_p \u2208\u211d^H\u00d7 W as inputs. \ud835\udc0b_p= mean_c(\ud835\udc08) where mean_c indicates  the operation that calculates the mean values for each pixel along the channel dimension. \u2130 outputs the lit-up image \ud835\udc08_lu and light-up feature \ud835\udc05_lu\u2208\u211d^H\u00d7 W\u00d7 C. Then \ud835\udc08_lu and \ud835\udc05_lu are  fed into \u211b to restore the corruptions and produce the enhanced image \ud835\udc08_en\u2208\u211d^H\u00d7 W\u00d7 3.\n\nThe architecture of \u2130 is shown in Fig.\u00a0<ref> (a) (i).  \u2130 firstly uses a conv1\u00d71 (convolution with kernel size = 1) to fuse the concatenation of \ud835\udc08 and \ud835\udc0b_p. We notice that the well-exposed regions can provide semantic contextual information for under-exposed regions. Thus, a depth-wise separable conv9\u00d79 is adopted to model the interactions of regions with different lighting conditions to generate the light-up feature  \ud835\udc05_lu.  Then \u2130 uses a conv1\u00d71 to aggregate \ud835\udc05_lu to produce the light-up map \ud835\udc0b\u0305\u2208\u211d^H\u00d7 W\u00d7 3. We set \ud835\udc0b\u0305 as a three-channel RGB tensor instead of a single-channel one like \u00a0<cit.> to improve its representation capacity in simulating the nonlinearity across RGB channels for color enhancement. Then \ud835\udc0b\u0305 is used to light up  \ud835\udc08 in Eq.\u00a0(<ref>). \n\n\n\n\nDiscussion. (i) Different from previous Retinex-based deep learning methods\u00a0<cit.>, our ORF estimates \ud835\udc0b\u0305 instead of the illumination map \ud835\udc0b because if ORF estimates \ud835\udc0b, then the lit-up image will be obtained by an element-wise division (\ud835\udc08 ./ \ud835\udc0b). Computers are vulnerable to this operation. The values of tensors can be very small (sometimes even equal to 0). The division may easily cause the data overflow issue. Besides, small  errors randomly generated by the computer will be amplified by this operation and lead to inaccurate estimation. Hence, modeling \ud835\udc0b\u0305 is more robust.\n\n(ii) Previous Retinex-based deep learning methods mainly focus on suppressing the corruptions like noise on the reflectance image, i.e., \ud835\udc11\u0302 in Eq.\u00a0(<ref>). They overlook the estimation error on the illumination map, i.e., \ud835\udc0b\u0302 in Eq.\u00a0(<ref>), thus easily leading to under-/over-exposure and color distortion during the light up process. In contrast, our ORF considers all these corruptions and employs \u211b to restore them all.\n\n\n\n\n\n \u00a7.\u00a7 Illumination-Guided Transformer\n\n\n\nPrevious deep learning methods mainly rely on CNNs, showing limitations in capturing long-range dependencies. Some CNN-Transformer hybrid works like SNR-Net\u00a0<cit.> only employ a global Transformer layer at the lowest resolution of a U-shaped CNN due to the enormous computational complexity of global multi-head self-attention (MSA). The potential of Transformer has not been fully explored. To fill this gap, we design an Illumination-Guided Transformer (IGT) to play the role of the corruption restorer \u211b in Eq.\u00a0(<ref>).\n\n\n\nNetwork Structure. As illustrated in Fig.\u00a0<ref> (a) (ii), IGT adopts a three-scale U-shaped architecture\u00a0<cit.>. The input of IGT is the lit-up image \ud835\udc08_lu. In the downsampling branch, \ud835\udc08_lu  undergoes a conv3\u00d73, an IGAB, a strided conv4\u00d74 (for downscaling the features), two IGABs, and a strided conv4\u00d74 to generate hierarchical features \ud835\udc05_i\u2208\u211d^H/2^i\u00d7W/2^i\u00d7 2^iC where i = 0, 1, 2. Then \ud835\udc05_2 passes through two IGABs. Subsequently, a symmetrical structure is designed as the upsampling branch. The deconv2\u00d72 with stride = 2 is exploited to upscale the features. Skip connections are used to alleviate the information loss caused by the downsampling branch. The upsampling branch outputs a residual image \ud835\udc08_re\u2208\u211d^H\u00d7 W\u00d7 3. Then the enhanced image \ud835\udc08_en is derived by the sum of \ud835\udc08_lu and \ud835\udc08_re, i.e., \ud835\udc08_en = \ud835\udc08_lu + \ud835\udc08_re.\n\n\n\nIG-MSA. As illustrated in Fig.\u00a0<ref> (c), the light-up feature \ud835\udc05_lu\u2208\u211d^H\u00d7 W\u00d7 C estimated by \u2130 is fed into each IG-MSA of IGT. Please note that Fig.\u00a0<ref> (c) depicts IG-MSA for the largest scale. For smaller scales, conv4\u00d74 layers with stride = 2  are used to downscale \ud835\udc05_lu to match the spatial size, which is omitted in this figure. As aforementioned, the non-trivial computational cost of global MSA limits the application of Transformer in low-light image enhancement. To tackle this issue, IG-MSA treats a single-channel feature map as a token and then  computes the self-attention.\n\n\n\nFirstly, the input feature \ud835\udc05_in\u2208\u211d^H\u00d7 W\u00d7 C is reshaped into tokens \ud835\udc17\u2208\u211d^HW\u00d7 C. Then \ud835\udc17 is split into k heads:\n\n\n\n    \ud835\udc17 = [\ud835\udc17_1,\u00a0\ud835\udc17_2,\u00a0\u22ef,\u00a0\ud835\udc17_k],\n\nwhere \ud835\udc17_i \u2208\u211d^HW\u00d7 d_k, d_k = C/k, and i = 1, 2, \u22ef, k. Note that Fig.\u00a0<ref> (c) shows the situation with k = 1 and omits some details for simplification. For each head_i, three fully connected (fc) layers without bias are used to linearly project \ud835\udc17_i into query elements \ud835\udc10_i\u2208\u211d^HW \u00d7 d_k, key elements \ud835\udc0a_i \u2208\u211d^HW \u00d7 d_k, and value elements \ud835\udc15_i \u2208\u211d^HW \u00d7 d_k as \n\n\n\n    \ud835\udc10_i = \ud835\udc17_i\ud835\udc16_\ud835\udc10_i^T,\u00a0\u00a0\ud835\udc0a_i = \ud835\udc17_i\ud835\udc16_\ud835\udc0a_i^T,\u00a0\u00a0\ud835\udc15_i = \ud835\udc17_i\ud835\udc16_\ud835\udc15_i^T,\n\nwhere \ud835\udc16_\ud835\udc10_i, \ud835\udc16_\ud835\udc0a_i, and \ud835\udc16_\ud835\udc15_i\u2208\u211d^d_k \u00d7 d_k represent the learnable parameters of the fc layers and T denotes the matrix transpose. We notice that different regions of the same image may have different lighting conditions. Dark regions usually have severer corruptions and are more difficult to restore. Regions with better lighting conditions can provide semantic contextual representations to help enhance the dark regions. Thus, we use the light-up feature \ud835\udc05_lu encoding illumination information and interactions of regions with different lighting conditions to direct the computation of self-attention. To align with the shape of  \ud835\udc17, we also reshape \ud835\udc05_lu into \ud835\udc18\u2208\u211d^HW\u00d7 C and split it into k heads:\n\n\n\n    \ud835\udc18 = [\ud835\udc18_1,\u00a0\ud835\udc18_2,\u00a0\u22ef,\u00a0\ud835\udc18_k],\n\nwhere \ud835\udc18_i \u2208\u211d^HW\u00d7 d_k, i = 1, 2, \u22ef, k. Then the self-attention for each head_i is formulated as \n\n\n\n    Attention(\ud835\udc10_i, \ud835\udc0a_i, \ud835\udc15_i, \ud835\udc18_i) = (\ud835\udc18_i \u2299\ud835\udc15_i)softmax(\ud835\udc0a_i^T\ud835\udc10_i/\u03b1_i),\n\nwhere \u03b1_i \u2208\u211d^1 is a learnable parameter that adaptively scales the matrix multiplication. Subsequently, k heads are concatenated to pass through an fc layer and then plus a positional encoding \ud835\udc0f\u2208\u211d^HW\u00d7 C (learnable parameters) to produce the output tokens \ud835\udc17_out\u2208\u211d^HW\u00d7 C. Finally, we reshape \ud835\udc17_out to derive the output feature \ud835\udc05_out\u2208\u211d^H\u00d7 W\u00d7 C.\n\n\n\nComplexity Analysis. We analyze that the computational complexity of our IG-MSA mainly comes from the k computations of the two matrix multiplication in Eq.\u00a0(<ref>), i.e., \u211d^d_k\u00d7 HW\u00d7\u211d^HW\u00d7 d_k and \u211d^HW\u00d7 d_k\u00d7\u211d^d_k \u00d7 d_k. Therefore, the complexity \ud835\udcaa(IG-MSA) can be formulated as \n\n\n\n    \ud835\udcaa(IG-MSA)    = k\u00b7[d_k\u00b7(d_k\u00b7 HW) + HW\u00b7(d_k\u00b7 d_k)], \n       = 2HWkd_k^2 = 2HWk(C/k)^2 = 2HWC^2/k.\n\nWhile the complexity of the global MSA (G-MSA) used by some previous CNN-Transformer methods like SNR-Net is\n\n\n\n    \ud835\udcaa(G-MSA) = 2(HW)^2C.\n\nCompare Eq.\u00a0(<ref>) with Eq.\u00a0(<ref>). \ud835\udcaa(G-MSA) is quadratic to the input spatial size (HW). This burden is expensive and limits the application of Transformer for low-light image enhancement. Therefore, previous CNN-Transformer hybrid algorithms only employ a G-MSA layer at the lowest spatial resolution of a U-shaped CNN to save the computational costs. In contrast, \ud835\udcaa(IG-MSA) is linear to the spatial size. This much lower computational complexity enables our IG-MSA to be plugged into each basic unit IGAB of the network. By this means, the potential of Transformer for low-light image enhancement can be further explored.\n\n\n\n\n\n\u00a7 EXPERIMENT\n\n\n\n \u00a7.\u00a7 Datasets and Implementation Details\n\nWe eveluate our method on LOL (v1\u00a0<cit.> and v2\u00a0<cit.>), SID\u00a0<cit.>, SMID\u00a0<cit.>, and SDSD\u00a0<cit.> (indoor and outdoor). \n\n\n\n\nLOL. The LOL dataset has two versions: v1 and v2. LOL-v1 provides 485 low-/normal-light image pairs for training and 15 image pairs for testing. Each pair contains a low-light image and its corresponding well-exposed reference image. LOL-v2 is divided into two subsets, i.e., LOL-v2-real and LOL-v2-synthetic. The train set of LOL-v2-real includes 689 low-/normal-light image pairs and the test set contains 100 pairs. Most low-light images are captured from a variety of scenes by changing the ISO and exposure time, while other parameters are fixed. LOL-v2-synthetic is created by analyzing the illumination distribution of low-light images and then synthesizing from RAW images.\n\n\n\nSID. The subset of SID dataset captured by the Sony \u03b17S II  camera is adopted for evaluation. There are 2697 short-/long-exposure RAW image pairs. The low-/normal-light RGB images are obtained by using the same in-camera signal processing of SID\u00a0<cit.> to transfer the RAW images to RGB domain. 2099 low-/normal-light image pairs are used for training and 598 image pairs are selected for testing.\n\n\nSMID. The SMID benchmark collects 20809 short-/long-exposure RAW image pairs. We transfer the RAW data to low-/normal-light RGB image pairs. 15763 pairs are used for training and the left pairs are adopted for testing.\n\n\nSDSD. The static version of the SDSD dataset is adopted for evaluation. It is captured by a Canon EOS 6D Mark II camera with an ND filter. The SDSD dataset contains two subsets, i.e., SDSD-indoor and SDSD-outdoor. SDSD-indoor includes 62 low-/normal-light video pairs for training and 6 pairs for testing. SDSD-outdoor contains 116 low-/normal-light video pairs for training and 10 video pairs for testing.\n\n\nImplementation Details. We implement Retinexformer by PyTorch\u00a0<cit.>. The model is trained with the Adam\u00a0<cit.> optimizer (\u03b2_1 = 0.9 and \u03b2_2 = 0.999) for 2.5 \u00d7 10^5 iterations. The learning rate is initially set to 2\u00d7 10^-4 and then steadily decreased to 1\u00d7 10^-6 by the cosine annealing scheme\u00a0<cit.> during the training process. Patches at the size of 128\u00d7128 are randomly cropped from the low-/normal-light image pairs as training samples. The batch size is 8. The training data is augmented with random rotation and flipping. The training objective is to minimize the mean absolute error (MAE) between the lit-up image and enhanced image. We adopt the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM)\u00a0<cit.> as the evaluation metrics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Low-light Image Enhancement\n\n\nQuantitative Results. We compare our method with a wide range of SOTA enhancement algorithms. The PSNR and SSIM results are reported in Tab.\u00a0<ref>. Our Retinexformer significantly outperforms SOTA methods on the seven datasets while requiring moderate computational and memory costs. \n\nWhen compared with the recent best method SNR-Net, our Retinexformer achieves 0.55, 1.32, 1.53, 1.57, 0.66, 0.33, and 1.18 dB improvements on LOL-v1, LOL-v2-real, LOL-v2-synthetic, SID, SMID, SDSD-indoor, and SDSD-outdoor datasets. However, our Retinexformer only costs 40% (1.61 / 4.01) Parmas and 59% (15.57 / 26.35) FLOPS.\n\nWhen compared with SOTA Retinex-based deep learning methods (including DeepUPE\u00a0<cit.>, RetinexNet\u00a0<cit.>, RUAS\u00a0<cit.>, and KinD\u00a0<cit.>), our Retinexformer yields 4.30, 4.43, 8.54, 6.00, 3.27, 6.60, and 6.00 dB improvements on the seven benchmarks. Especially on SID and SDSD datasets that are severely corrupted by noise and artifacts, the improvements are over 6 dB, as plotted in Fig.\u00a0<ref>. \n\nWhen compared with SOTA Transformer-based image restoration algorithms (including IPT\u00a0<cit.>, Uformer\u00a0<cit.>, and Restormer\u00a0<cit.>), our Retinexformer gains by 2.73, 2.86, 4.26, 2.17, 1.95, 3.66, and 2.29 dB on the seven datasets. Nonetheless, Retinexformer requires only 1.4% and 6.2% Params, 0.2% and 10.9% FLOPS of IPT and Restormer.\n\nAll these results clearly suggest the outstanding effectiveness and efficiency advantage of our Retinexformer.\n\n\n\nQualitative Results. The visual results of Retinexformer and SOTA  algorithms on the seven benchmarks  are shown in Fig.\u00a0<ref>,\u00a0<ref>, and\u00a0<ref>. Please zoom in for a better view. Previous methods either cause color distortion like RUAS in Fig.\u00a0<ref>, or contain over-/under-exposed regions and fail to suppress the noise like RetinexNet and DeepUPE in Fig.\u00a0<ref>, or generate blurry images like Restormer and SNR-Net in Fig.\u00a0<ref>, or introduce black spots and unnatural artifacts like DRBN\u00a0<cit.> and SNR-Net in Fig.\u00a0<ref>. In contrast, our Retinexformer can effectively enhance the poor visibility and low contrast, reliably remove the noise without introducing  spots and artifacts, and robustly preserve the color.\n\n\n\nUser Study Score. We conduct a user study to quantify the human subjective visual perception quality of the enhanced low-light images from the seven datasets. 23 human subjects are invited to score the visual quality of the enhanced results, independently. These testers are told to observe the results from: (i) whether the results contain under-/over-exposed regions, (ii) whether the results contain color distortion, and (iii) whether the results are corrupted by noise or artifacts. The scores range from 1 (worst) to 5 (best). For each low-light image, we display it and the results enhanced by various algorithms but without their names to the human testers. There are 156 testing images in total. The user study scores are reported in Tab.\u00a0<ref>. Our Retinexformer achieves the highest score on average. Besides, our results are  most favored by the human subjects on LOL-v2-real (L-v2-R), LOL-v2-synthetic (L-v2-S), SID, SMID, and SDSD-outdoor (SD-out) datasets and second most favored on LOL-v1 (L-v1) and SDSD-indoor (SD-in) benchmarks.\n\n\n\n\n\n \u00a7.\u00a7 Low-light Object Detection\n\n\n\nExperiment Settings. We conduct low-light object detection experiments on the ExDark\u00a0<cit.> dataset to compare the preprocessing  effects of different enhancement algorithms for high-level vision understanding. The ExDark dataset consists of 7363 under-exposed images annotated with 12 object category bounding boxes. 5890 images are selected for training while the left 1473 images are used for testing. YOLO-v3\u00a0<cit.> is employed as the detector and trained from scratch. Different low-light enhancement methods serve as the preprocessing modules with fixed parameters. \n\n\n\nQuantitative Results. The average precision (AP) scores are listed  in Tab.\u00a0<ref>. Our Retinexformer achieves the highest  result on average, 66.1 AP, which is 0.5 AP higher than the recent best self-supervised method SCI\u00a0<cit.> and  0.8 AP higher than the recent best fully-supervised method SNR-Net\u00a0<cit.>. Besides, Retinexformer yields the best results on five object categories: bicycle, boat, bottle, cat, and dog.  \n\n\n\nQualitative Results. Fig.\u00a0<ref> depicts the visual comparisons of detection results. The detector easily misses some target  objects or predicts inaccurate locations on the images enhanced by previous algorithms. In contrast, the detector can reliably predict well-placed  bounding boxes to cover all the desired objects on the image enhanced by our Retinexformer. These results demonstrate the superiority of the proposed method in benefiting high-level vision understanding.\n\n\n\n\n\n\n \u00a7.\u00a7 Ablation Study\n\n\nWe conduct ablation study on the SDSD-outdoor dataset for the good convergence and stable performance of Retinexformer on it. The results are reported in Tab.\u00a0<ref>.\n\n\n\nBreak-down Ablation. We conduct a break-down ablation to study the effect of each component towards higher performance, as shown in Tab.\u00a0<ref>.  Baseline-1 is derived by removing ORF and IG-MSA from  Retinexformer. When we respectively apply ORF and IG-MSA, baseline-1 achieves 1.45 and 2.39 dB improvements. When jointly exploiting the two techniques, baseline-1 gains by 3.37 dB. This evidence suggests the effectiveness of our ORF and IG-MSA.\n\n\n\nOne-stage Retinex-based Framework. We conduct an ablation to study ORF. The results are listed in Tab.\u00a0<ref>. We first remove ORF from Retinexformer and set the input of \u211b as \ud835\udc08_lu = \ud835\udc08. The model yields 28.86 dB. Then we apply ORF but set \u2130 to estimate the illumination map \ud835\udc0b. The input of \u211b is \ud835\udc08./\ud835\udc0b where ./ indicates the element-wise division. To avoid exceptions thrown by computer, we add \ud835\udc0b with a small constant \u03f5 = 1\u00d710^-4. Yet, as analyzed in Sec.\u00a0<ref>, the computer is vulnerable to the division of small values. Thus, the model obtains a limited improvement of 0.11 dB. To tackle this issue, we estimate the light-up map \ud835\udc0b\u0305 and set the input of \u211b as \ud835\udc08_lu = \ud835\udc08\u2299\ud835\udc0b\u0305. The model gains by 0.40 dB. After using  \ud835\udc05_lu to direct \u211b, the model continues to achieve an improvement of 0.58 dB in PSNR and 0.007 in SSIM.\n\n\n\nSelf-Attention Scheme. We conduct an ablation to study the effect of the self-attention scheme. The results are reported in Tab.\u00a0<ref>. Baseline-2 is obtained by removing IG-MSA from Retinexformer. For fair comparison, we plug the global MSA (G-MSA) used by previous CNN-Transformer hybrid methods into each basic unit of \u211b. The input feature maps of G-MSA are downscaled into 1/4 size to avoid out of memory. We also compare our IG-MSA with local window-based MSA (W-MSA) proposed by Swin Transformer\u00a0<cit.>. As listed in Tab.\u00a0<ref>, our IG-MSA surpasses G-MSA and W-MSA by 1.41 and 1.34 dB while costing 2.08G and 0.86G FLOPS less. These results demonstrate the cost-effectiveness advantage of the proposed IG-MSA.\n\n\n\n\n\u00a7 CONCLUSION\n\n\nIn this paper, we propose a novel Transformer-based method, Retinexformer, for low-light image enhancement. We start from the Retinex theory. By analyzing the corruptions hidden in the under-exposed scenes and caused by the light-up process, we introduce perturbation terms into the original Retinex model and formulate a new Retinex-based framework, ORF.  Then we design an IGT that utilizes the illumination information captured by ORF to direct the modeling of long-range dependences and interactions of regions with different lighting conditions. Finally, our Retinexformer is derived by plugging IGT into ORF. Extensive quantitative and qualitative experiments show that our Retinexformer dramatically outperforms SOTA methods on seven datasets. The results of user study and low-light detection also demonstrate the practical values of our method.\n\nieee_fullname\n\n\n\n"}