{"entry_id": "http://arxiv.org/abs/2303.06821v1", "published": "20230313024854", "title": "SDF-3DGAN: A 3D Object Generative Method Based on Implicit Signed Distance Function", "authors": ["Lutao Jiang", "Ruyi Ji", "Libo Zhang"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\t\n\n\n\nJournal of  Class Files,\u00a0Vol.\u00a014, No.\u00a08, August\u00a02021\nShell et al.: A Sample Article Using IEEEtran.cls for IEEE Journals\n\n\n\n\n\nSDF-3DGAN: A 3D Object Generative Method Based on Implicit Signed Distance Function\n    Lutao Jiang^*,\n\tRuyi Ji^*,\n\tLibo Zhang^\u2020 \n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t*The two authors contributed equally to this work \u2020Corresponding author: Libo Zhang Lutao Jiang and Ruyi Ji is with the State Key Laboratory of Computer\n\t\tScience, Institute of Software Chinese Academy of Sciences, Beijing 100190,\n\t\tChina, and also with the University of Chinese Academy of Sciences, Beijing\n\t\t101400, China (e-mail: lutao2021@iscas.ac.cn; ruyi2017@iscas.ac.cn).Libo Zhang is with the State Key Laboratory of Computer\n\t\tScience, Institute of Software Chinese Academy of Sciences, Beijing 100190,\n\t\tChina (e-mail: libo@iscas.ac.cn).\n\t\n\t\n        \nThis paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.\nManuscript received April 19, 2021; revised August 16, 2021.\n    \n============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\nIn this paper, we develop a new method, termed SDF-3DGAN, for 3D object generation and 3D-Aware image synthesis tasks, which introduce implicit Signed Distance Function (SDF) as the 3D object representation method in the generative field. We apply SDF for higher quality representation of 3D object in space and design a new SDF neural renderer, which has higher efficiency and higher accuracy. To train only on 2D images, we first generate the objects, which are represented by SDF, from Gaussian distribution. Then we render them to 2D images and use them to apply GAN training method together with 2D images in the dataset. In the new rendering method, we relieve all the potential of SDF mathematical property to alleviate computation pressure in the previous SDF neural renderer. In specific, our new SDF neural renderer can solve the problem of sampling ambiguity when the number of sampling point is not enough, use the less points to finish higher quality sampling task in the rendering pipeline. And in this rendering pipeline, we can locate the surface easily. Therefore, we apply normal loss on it to control the smoothness of generated object surface, which can make our method enjoy the much higher generation quality. Quantitative and qualitative experiments conducted on public benchmarks demonstrate favorable performance against the state-of-the-art methods in 3D object generation task and 3D-Aware image synthesis task. Our codes will be released at https://github.com/lutao2021/SDF-3DGAN.\n\n\n\n3D object generation, Mesh generation, 3D-aware image synthesis, Generative adversarial networks (GAN), Signed distance function (SDF), Implicit neural representation, Neural rendering\n\n\n\n\n\u00a7 INRODUCTION\n\n\n3D object generation has arguably emerged as one of the most intriguing research hotspots. This task is different from the reconstruction task. It requires the creation of an object that do not exist in the world from a random vector. With the advent of GAN <cit.>, diverse follow-up architectures have popularized a variety of 2D image generation tasks <cit.>. GAN endows neural networks with the capability of creation, , generating the fake samples resembling real samples. However, its potential in 3D object generation task has yet to be further explored. As such, we observe that all existing methods for this task remain a extremely low level due to the limitations such as computation overhead, representation method, .\n\n3D-GAN <cit.> is one representative of early methods. It generates a voxel model directly, but its spatial resolution is heavily restricted by the expensive memory consumption and low accuracy of voxel representation method. Recently, NeRF <cit.> has gained momentum in 3D representation domain. The main idea of NeRF is to utilize a simple Multi-Layer Perception (MLP) network to encode the implicit representation of a scene. To be specific, given the position coordinate and viewing direction of any point in space, MLP network predicts the volume density and RGB color of this point. After that, the color of ray can be accumulated by the volume rendering <cit.> algorithm. With the advance of NeRF, numerous NeRF-based generators have been proposed, such as GRAF <cit.>, PI-GAN <cit.> and GOF <cit.>. Theoretically, these methods enjoy unlimited spatial resolution and can generate information of arbitrary point. Nevertheless, the existing NeRF-based generators individually predict the volume density or opacity  of each point in space, neglecting the rich relationship between points. Such a schema easily incurs discontinuity defects on the real 3D model, and even some chaotic performance, as shown in Fig.\u00a0<ref> (the 1^st row).\n\n\n\nInspired by success of implicit Signed Distance Function (SDF)<cit.>, we propose SDF-3DGAN, a method that introduce implicit Signed Distance Function (SDF) as the 3D object representation method for the 3D object generation and 3D-Aware image synthesis task. Different from the previous methods, which generates the voxel or the NeRF representation, we present to utilize SDF to represent the shape of an object. In general, SDF-3DGAN leverages an implicit SDF to represent an object and can render 2D RGB images from arbitrary angles. It is worth noting that the implicit SDF representation can be converted into other formats of 3D models, such as mesh. Compared to the voxel representation, the SDF representation can overcome its low representation accuracy due to its discrete nature in space. And the most important reason why our results are significantly better than previous NeRF-based methods (Fig.\u00a0<ref>) is that the implicit SDF allows every point in space to perceive the presence of the surface and guarantees a stronger correlation between spatial points.\n\nSDF is a function of the shortest distance from a point to the surface in space. The signed distance of a point outside the surface is defined as a positive value, while a point inside the surface is defined as a negative value. The zero-level set of a signed distance function is the surface of an object. In our design, we adopt a simple MLP network as the implicit signed distance function. Its input is the coordinate of a point and the output is signed shortest distance to the object surface. And we explicitly constrain its gradient's norm at every point to approach 1, which turns this simple MLP network to be a implicit SDF. \n\nIn the training stage, our overall training pipeline consists of two steps. Using the 2D car dataset as an example, we generate a 3D car model by giving the neural network a restricted random vector. To supervise its ability to convert arbitrary random vectors into different cars, we need to render them as 2D images at various angles by the rendering methods we have designed. The discriminator of GAN is then used to determine whether the generated image is similar to the 2D car images in the dataset. By achieving the goal of making the generated SDF look like a car at all angles, our generator can convert arbitrary random vectors into different 3D models of cars.\n\n\nHowever, to render a 2D image with the resolution of H \u00d7 W, common neural volume rendering algorithms require to sample H \u00d7 W rays, along with N points on each ray. Considering a 2D image with H=W=N=128, we need to query the MLP network 2,097,152 times to render such an image, resulting in the prohibitive calculation cost. To alleviate the computation burden, we design a new SDF rendering pipeline which takes full advantage of excellent mathematical properties of SDF to improve the sampling efficiency. And this rendering method is also can be applied in other SDF tasks.\n\nSurprisingly, we notice concurrent method StyleSDF <cit.> shares the similar idea with us. Even though both leverage SDF as the representation method of an object, the focus of our two papers is significantly different. StyleSDF focuses on high-resolution (1024 \u00d7 1024) face generation by training an overly complicated model similar to StyleGAN <cit.>, which requires more computation resource to upsample the face resolution in the second stage, while its first stage is just amenable for low-resolution (64 \u00d7 64) feature map generation. And two stages are optimized separately, which breaks the multi-view consistency in 3D-aware image synthesis. In contrast, we pay more attention to generate more realistic 3D objects and high-quality 3D-aware images simultaneously. Moreover, we propose to utilize the mathematical properties of SDF to reduce computation burden when rendering an image. And we can readily find the points and normal vectors of the surface. Therefore, we introduce normal loss to regularize the smoothness of surface, which allows our method for more realistic representation. In addition, with our improved rendering algorithms, which solves the problem of sampling error when the number of sampling points is too small (in the section IV.B.3), we can support the training of 360\u00b0 datasets, not just forward-facing small-angle face datasets (StyleSDF only use these datasets). We validate the effectiveness of our method on a full angle dataset CARLA <cit.>, beyond the face domain. Moreover, our method produces higher quality and higher resolution feature maps than the first stage of StyleSDF. I believe that replacing its first stage generator with ours would yield better results if computational resources were available.\n\nTo sum up, the main contributions of this work can be summarized into five folds.\n\n\t\n  * We present a novel method for 3D object generation, named SDF-3DGAN, which introduce implicit SDF as the representation method of 3D object in the generative field.\n\t\n  * We design a new SDF neural renderer for implicit SDF representation method, which can render the generated object to 2D RGB images more efficiently and accurately than previous SDF neural renderer. And this rendering method is also can be applied in other SDF tasks.\n\t\n  * The proposed rendering pipeline takes full advantage of mathematical properties of SDF to reduce computation cost and significantly improves the sampling efficiency. Besides, we avoid the dilemma that it is prone to locate the wrong surface position even cannot find surface when the sampling points are insufficient. \n\t\n  * Thanks to the fact that we can readily find the desirable surface, we can constrain the variation of the surface normal vector, , the surface SDF gradient, which greatly smooths the surface of the generated object.\n\t\n  * Quantitative and qualitative experiments conducted on the challenging benchmarks, including CARLA <cit.>, CelebA <cit.>, BFM <cit.>, show favorable performance against the state-of-the-art methods, validating effectiveness of our proposed pipeline and optimization algorithms.\n\n\nThe remainder of this paper is organized as follows. Section <ref> briefly reviews significant works relevant to our method. In Section <ref>, we have briefly introduced some of the key concepts. In Section <ref>, we describe the proposed pipeline and method in detail. In Section <ref>, the extensive experimental results are reported and analyzed, including the comparison between the proposed method and existing cutting-edge methods, validation of generation ability, and the comprehensive analysis of the ablation study. Finally, Section <ref> draws the conclusions of the proposed method and summarizes several potential directions for the future research.\n\n\n\n\u00a7 RELATED WORK\n\nIn this section, we briefly review three major research directions closely related to our method, , 3D generative model, implicit signed distance function, and generative adversarial networks.\n\n\n\n \u00a7.\u00a7 3D Generative Model\n\nAs one of seminal methods, 3D-GAN <cit.> proposes to use GAN for the task of 3D object generation, where it relies on the generator to produce a voxel of a 3D object. VON <cit.> proposes to disentangle object representation. Concretely, it first generates a 3D model by 3D-GAN, then computes a perspective-specific 2.5D sketch by differentiable projection, and finally generates a 2D image by texture network.\nHowever, both 3D-GAN and VON require 3D supervision, and the memory-sensitive drawback dramatically limits their spatial resolution. HoloGAN <cit.> proposes a method only requiring 2D unlabeled images to generate 3D-aware images. Unfortunately, it cannot explicitly generate a 3D model in space. The arrival of NeRF <cit.> has revolutionized the paradigm of retrieving the volume density and RGB color of any point in space via a neural network. For example, GRAF <cit.> is the first method to introduce NeRF into the 3D object generation task. It leverages a NeRF-like MLP structure to generate 3D object. PI-GAN <cit.> designs a mapping network similar to StyleGAN <cit.>, and introduces FiLM <cit.> and SIREN <cit.> activation functions. The follow-up method GOF <cit.> changes the volume density of PI-GAN to occupancy, and proposes an algorithm to allow model aware of the surface of the object. ShadeGAN <cit.> attaches the illumination model to PI-GAN and designs a surface tracking CNN-based network to improve sampling efficiency. Likewise, there exist some two-stage methods <cit.> that utilize volume rendering algorithms to produce low-resolution features which are then upsampled to a high resolution image. It is worthy to note that all these implicit representation methods can be converted to other 3D representation methods such like mesh.\n\n\n\n \u00a7.\u00a7 Implicit Signed Distance Function\n\nWith the development of implicit neural representations (INR), ever-increasing research attention is shifted to adopt deep neural networks as implicit signed distance functions. Considering that a deep neural network can serve as a continuous function, DeepSDF <cit.> pioneers to introduce deep neural network into 3D model representation and reconstruction. To get free from 3D supervision in single-view or multi-view 3D reconstruction task, SDFDiff <cit.> proposes to use a differentiable method to render 3D shapes represented by signed distance functions. SAL <cit.> designs a novel method based on implicit shape representation for surface reconstruction tasks from an un-oriented point cloud. IDR <cit.> introduces a neural network architecture to disentangle geometry and appearance, allowing model to encode more complex lighting conditions and materials. NeuS <cit.> proposes a new SDF-based volume rendering method, which maps signed distance to its proposed s-density and generalize to the volume rendering algorithm. \n\n\n\n \u00a7.\u00a7 Generative Adversarial Networks\n\nIan GoodFellow <cit.> propose generative adversarial networks, which consist of a generator and a discriminator.\nGiven a random noise, the generator network can convert it to fake data which tries to resemble real data as far as possible. The discriminator network is tasked with distinguishing whether the input data comes from real data or fake data. The two parts evolve together in an adversarial manner, ending up with a generator network capable to produce high quality data. GAN has been applied to a variety of tasks, particularly in the field of 2D images, such as image generation <cit.>, inpainting <cit.>, dehazing <cit.>, . Specifically, progressive GAN <cit.> proposes a gradual transition training scheme from low-resolution to high-resolution. Following a coarse-to-fine paradigm, the training process starts by learning coarse-grained features and then pays more attention to the fine-grained features. The StyleGAN series <cit.> not only significantly improve the quality of generated images, but also spark significant improvements in image reconstruction and editability. INR-GAN <cit.> proposes to leverage a continuous function which can generate an image by querying the coordinates of each pixel.\n\n\n\n\u00a7 PRELIMINARIES\n\nTo better understand the proposed method, we first briefly introduce a few important concepts, , the representative neural rendering method, NeRF, the conventional sampling strategy on the ray, hierarchical sampling, and Signed Distance Function (SDF). If you are already familiar with these concepts, we recommend skipping this section.\n\n\n\n\n\n \u00a7.\u00a7 NeRF (Neural Radiance Field)\n\nNeRF readily represents a 3D object or a scene through a simple neural network. It predicts the volume density and RGB color values for each point in a scene, then the volume rendering algorithm is used to render the observed image at the given viewpoint. Given the coordinate x of a point and the observation direction d(\u03b8,\u03d5) (\u03b8 and \u03d5 are the pitch and yaw respectively), NeRF queries the volume density \u03c3 and the RGB color c of a point through MLP. To render an RGB image, we first set view-dependent rays, then sample points along these rays and query their volume density \u03c3 and RGB color c respectively. Finally, the classic volume rendering technique <cit.> is utilized to calculate the color of each ray. Each ray corresponds to a pixel in the 2D image, as shown in Fig. <ref>. The specific calculation of each ray is mathematically reformulated as follows:\n\n    \u0108(r)=\u2211_i=1^N T_i(1-exp(-\u03c3(x_i) \u03b4_i)) c(x_i, d), \n     where  T_i=exp(-\u2211_j=1^i-1\u03c3(x_j) \u03b4_j)\n\nwhere \u03c3(x_i) stands for the volume density of the point x_i, c(x_i, d) is the RGB color of point x_i conditioned on observing direction d, \u03b4_i is the distance between the two points of x_i and x_i+1, and r is the ray which is represented r(t)=o+td, x_i=r(t_i).\n\n\n\n \u00a7.\u00a7 Hierarchical Sample\n\nTo calculate the color of each ray via the classical rendering algorithms, there is a requirement to sample points along the ray. NeRF offers a hierarchical sampling strategy. That is, in the first round, the sampling points along a ray are drawn from a uniform distribution. After that, we calculate the weight of i-th sampling point through the formula T_i(1-exp(-\u03c3(x_i) \u03b4_i)) on each ray. Formulating above-acquired weights as the probability density, the second round sampling is performed along this ray. Finally, all points sampled in the above two rounds contribute to the calculation of color belonging to this ray in Equ.\u00a0(<ref>).\n\n\n\n \u00a7.\u00a7 SDF (Signed Distance Function)\n\nSDF refers to a function of the shortest distance from a point to the surface in space. Following the common practice, the signed distance of points outside the surface are positive and negative for those inside the surface. The zero-level set of a signed distance function is the surface of an object. Theoretically, if a function is with gradient's norm 1 at every point in space, then it can serve as the SDF of an object. Likewise, if we explicitly constrain the gradient's norm of network's output to be 1 with respect to the input points, such a neural network turns to be an implicit SDF. Obviously, the gradient of SDF of a point on the surface is its normal vector. Taking a ball of radius 1 whose center is located at the origin of coordinate system as an example, its SDF can be mathematically formulated as f(x,y,z) = \u221a(x^2+y^2+z^2)-1. Its surface is the zero-level set of SDF, which is written as {(x,y,z)|f(x,y,z) = \u221a(x^2+y^2+z^2)-1=0}.\n\n\n\n\u00a7 METHOD\n\n\n\n\nAs in Fig.\u00a0<ref>, our overall training process is divided into two stages. First, given shape code z_s, which is a  random vector, we can generate an object represented by SDF(x;z_s). And this SDF is a conditioned network. Second, given color code z_c, which is also a random vector, we can determine the surface color of this object. Then, we can render this object to a 2D image at any angle by our new designed rendering algorithm. Through training, we leave the discriminator without the ability to distinguish the difference between the generated arbitrary-angle images and the images in the dataset, the generated images are close enough to the images of the given dataset. The object generated in space thus satisfies the shape we want to generate. Considering there are large number of symbols in the next paper, we have therefore summarised a symbol table in Table <ref>.\n\n\n \n\n\n \u00a7.\u00a7 Generate Object\n\n\n\n\nWe first generate an object in space, which is represented by the SDF neural network. As shown in Fig.\u00a0<ref>, we use a simple MLP network as the SDF. This neural network can be expressed as the following equation\n\n    V   =\u2131(x;z_s) \n    \n    \t\ts   =\u2131_1(V) \n    c   =\u2131_2(V,d;z_c)\n\n\nwhere \u2131, \u2131_1 and \u2131_2 are some linear layers in the Fig.\u00a0<ref>, x\u2208\u211d^3 is the 3D space coordinate, z_s\u2208\u211d^128 is the shape code, V\u2208\u211d^256 is the intermediate feature vector, s is the signed distance, d\u2208\u211d^3 is the observation direction of the point, z_c\u2208\u211d^128 is the color code and c\u2208\u211d^3 is the RGB color of point. And in the training stage, both z_s and z_c obey standard Gaussian distribution.\n\nThrough fixing the input z_s to the neural network \u2131_1(\u2131(\u00b7)), this neural network can be seen as the function SDF(x;z_s), where x is the argument of this function. Different z_s represent different objects. Taking the car dataset as an example, after training is completed, changing different z_s will change the shape of the car. For training, we need to render the object to 2D RGB images at arbitrary angle. Therefore, we attach a color branch to the network to achieve this goal. We input the position feature vector, observation direction and color code to this branch to obtain the RGB value of the point. Similarly, different z_c can change the color of the object.\n\n\n\n \u00a7.\u00a7 Render the Generated Object to 2D RGB Image\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 The Proposed SDF Neural Renderer\n\n\n\n\nThe overall flow-chart of the proposed SDF neural renderer is illustrated in Fig.\u00a0<ref>. To render an image, we first sample the camera position in space. For convenience, we sample the camera position on a sphere of radius 1 which is located at the origin of the coordinate system. Similar to PI-GAN <cit.>, we then turn the camera direction towards the origin of coordinates and emit H \u00d7 W rays (each ray corresponds to one pixel). After that, we sample points along the ray for subsequent integration calculation. Next, the position coordinate together with observation direction are fed into the well-designed SDF neural network to compute the according signed distance and RGB color. To adapt to the classical volume rendering formula, we project the signed distance to opacity. Finally, we leverage the volume rendering formula to calculate the color of each ray, which is equivalent to rendering the entire picture. Next, we will introduce the color calculation method and point sampling method in detail.\n\n\n\n  \u00a7.\u00a7.\u00a7 Map Signed Distance to Opacity and Calculate Color\n\nAs discussed above, to adapt to the classical volume rendering formula, we explicitly map signed distance to opacity \u03b1\u2208(0,1] and calculate the color of each ray via Equ.\u00a0(<ref>).\n\n    \u0108(r; z_s, z_c)=\u2211_i=1^N\u03b1(x_i; z_s) \u220f_j<i(1-\u03b1(x_j; z_s)) c(x_i, d; z_s, z_c)\n\nwhere r is a ray, and N x_i are the points on the ray, which are arranged from near to far from the position of the ray origin. \u03b1(x_i;z_s) means the opacity of point x_i conditioned on shape code z_s, c(x_i, d;z_s,z_c) is the color of point x_i with observation direction d conditioned on color code z_c.\n\nIntuitively, the opacity of a point should be larger when it approaches to the surface of an object. When far from the surface, its opacity should approach to 0. To satisfy the above characteristics, we design a mapping function \u2133(\u00b7), which is tasked with converting signed distance of a point to its opacity. \nFormally, a desirable mapping function should be with following three characteristics: \n\n\t\n  * \u2133(s)=\u2133(-s)\n\t\n  * Monotonically increasing at (-\u221e,0) and monotonically decreasing at (0,+\u221e)\n\t\n  * Approach to 0 when the signed distance is far from 0 and equal to 1 when the signed distance is 0.\n\n\nEmpirically, we observe that the derivative function of sigmoid satisfies the above conditions well. Since such a function varies in the range of (0,0.25], we regularize it by a scale factor of four. Thereby, the mapping function can be given as Equ.\u00a0(<ref>), where \u03b2 is a learnable parameter increasing with the progress of training, which ensures the function \u2133(s) to shrink towards s=0 , as shown in Fig. <ref>.\n\n\n\n\n    \u2133(s ; \u03b2)=4 \u00b7sigmoid(\u03b2\u00b7s) \u00b7 (1-sigmoid(\u03b2\u00b7s))\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Sampling Method and Accurate Sampling Strategy\n\nAssuming a ray r(t)=o+td, where o refers to source point, d represents the ray direction and t is a ray depth scalar, we sample points along the ray to proceed volume rendering via Equ.\u00a0(<ref>). More specifically, we need to sample N points, \u00a0ray depth scalar t, within an interval for subsequent calculation on the ray. Upon the shortest distance to surface acquired by SDF, we leverage ray marching algorithm to search the approximate position of the surface. Mathematically, we apply the Equ.\u00a0(<ref>) to iteratively search the surface location.\n\n    t_i+1 = t_i + SDF(o+t_id)\n\n\nObviously, for a point, marching forward with the shortest distance to surface on the ray will never cross the surface. It reaches the surface only if the direction of the ray coincides with that of the shortest distance. Fig. <ref> presents a typical example for better understanding. After numerous rounds of computation, we figure out the approximate ray depth t_d is within the acceptable error range. And we set this depth value t_d to be the center of sampling interval. Then, we uniformly sample points in the [t_d-\u03b4,t_d+\u03b4]. \u03b4 refers to a hyper-parameter, which decreases as training proceeds. \n\n\n\n\n\nAs \u03b2 in Equ.\u00a0(<ref>) increases in the training process, the \u03b1 curve near the surface gets narrower gradually (refer to Fig.\u00a0<ref>). As a result, only points very close to the surface have an opacity far from 0. Therefore, when the sampling points are insufficient in the first round, it would be troublesome to find a correct surface or cannot find any surface, as illustrated in Fig.\u00a0<ref>. As a result, when we calculate colors through Equ.\u00a0(<ref>), there exists two possible problems, (1) all points tend to be with a weight of 0, (2) points close to the second surface would have larger weights. As a result, the generated image would be with discontinuous points, which in turn poses challenges for the stable training (please refer to the first two rows of Fig.\u00a0<ref>).\n\nTo solve the above issues, we propose an accurate sampling strategy to ensure the sampling efficiency and accuracy. Concretely, on a ray r(t)=o+td, we first locate the two points which are the first pair of signed distance values varied from positive to negative in the previous uniform sampling procedure after ray marching. We denote such two points as (t_1,s_1), (t_2,s_2) respectively, where s_1 and s_2 indicate the signed distance values at positions t_1 and t_2, s_1>0, s_2<0. We project these two points in the coordinate system to compute the primary root of the line through Equ.\u00a0(<ref>).\n\n    t_s=-t_2-t_1/s_2-s_1 s_1 + t_1\n\nwhere t_s is the accurate surface that we locate. We provide an example for better understanding in Fig.\u00a0<ref>. Empirically, such simple strategy not only solves the above two issues, but also serves as an alternative high-efficiency choice in hierarchical sampling (refer to section <ref>-B). The ablation study in section <ref> demonstrates the superiority over its existing counterparts.\n\n\n\n\n\n\n \u00a7.\u00a7 Training Strategy\n\nWhen we render the object to 2D images, we can change the number of sampling rays, , the number of pixels. One ray can be seen as one pixel. Therefore, we can render images at any resolutions theoretically. But more pixels means that we need to sample more rays and points to calculate the RGB values of each ray. Restricted by hardware, we only render the 2D images at 32 \u00d7 32 to 128 \u00d7 128 resolution.\n\nEnjoying ability to render an image at any resolution, we borrow an off-the-shelf progressive discriminator used by PI-GAN <cit.> to first learn the coarse-grained content at a low resolution, and then gradually increase the resolution to learn more details. The training procedure is divided into three stages, which consist of 32 \u00d7 32, 64 \u00d7 64 and 128 \u00d7 128 resolutions. And as the rendered image resolution gradually increases, we shrink the range of sampling interval and reduce the number of sampling points to relieve the computation cost.\n\n\n\n \u00a7.\u00a7 Loss Function\n\n\n\n\n  \nGAN Loss\nLet the generator be G_\u03b8_1(z_s,z_c, \u03be), where \u03b8_1 is the learnable parameter and \u03be is the camera pose. As such, the discriminator is D_\u03b8_2(\u00b7), where \u03b8_2 refers to the learnable parameter.\nThe input of discriminator is either a real image \u2110 from dataset or a generated image from G_\u03b8_1(z_s,z_c, \u03be). We adopt non-saturating GAN loss with R1 regularization <cit.> as follows:\n\n    \u2112_GAN=\ud835\udd3c_z_s\u223c p_z_s, z_c\u223c p_z_c, \u03be\u223c p_\u03be[f(D_\u03b8_2(G_\u03b8_1(z_s,z_c, \u03be)))] \n    \n    \t\t+\ud835\udd3c_I \u223c p_\ud835\udc9f[f(-D_\u03b8_2(\u2110))+\u03bb|\u2207 D_\u03b8_2(\u2110)|^2] \n     where    f(u)=-log (1+exp (-u))\n\n\n\n\n  \nEikonal Loss\nThe Eikonal Loss in Equ.\u00a0(<ref>) is utilized to ensure network learns a meaningful SDF. In another word, we make efforts to ensure that the norm of the gradient vector at each point in space converge to 1.\n\n\n    \u2112_Eikonal=\ud835\udd3c_x(\u2207_x f(x ; z_s)-1)^2\n\n\n\n\n  \nNormal Loss\nFurther, we leverage the \u2113_2 Normal Loss in Equ.\u00a0(<ref>) to smooth generated 3D model. \n\n    \u2112_Normal=1/N\u2211_x_s\u2208\ud835\udcae\u2207_x_s(x_s, z_s)-\u2207_x_s(x_s+\u03f5, z_s)\n\nwhere N is the number of points involved in the calculation, \ud835\udcae denotes the set of points on the surface and \u03f5 indicates the micro-disturbances. Concretely, the \u2113_2 normal loss constraint is imposed on the normal vector variation of the surface. Exactly, the gradient of SDF at a point is the normal vector of this point.\n\nIn summary, the final loss function is:\n\n    \u2112=\u2112_GAN + \u03bb_Eikonal\u2112_Eikonal + \u03bb_Normal\u2112_Normal\n\nwhere \u03bb_Eikonal and \u03bb_Normal are two hyper-parameters to balance the above loss items.\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\nIn this section, we conduct extensive experiments to verify the effectiveness of our proposed method. Specifically, we first describe the three challenging datasets involved in this paper. Then, we introduce the implementation details of our experiments. Next, the comparisons between the proposed method and existing works are reported and analyzed qualitatively. Finally, comprehensive ablation studies are performed to help understand the proposed method better.\n\n\n\n \u00a7.\u00a7 Datasets\n\nTo evaluate the effectiveness and generation ability of our method and for fair comparison to other methods, we conduct extensive experiments on three widely used and challenging datasets, including CARLA <cit.>, CelebA <cit.>, BFM <cit.>. These three datasets are used widely by most SOTA methods. The CARLA dataset is derived from GRAF <cit.> where the Carla Driving simulator <cit.> is used to render 10,000 images of 18 car models and each image has a diverse appearance. The cars are all in the center of image and the background is white. The camera position of the rendered image is sampled on an upper hemisphere uniformly, which ranges from 0\u00b0-360\u00b0 and 0\u00b0-85\u00b0 for azimuth and polar angle, respectively. CelebA contains over 200,000 real-world images of faces without camera parameters. It contains images of face at various forward angles, but does not contain any pitch and yaw angle information. Following the common practice in prior works, we only use the head part patches cropped from original images in our experiments. With regard to BFM dataset, it is a collection of 200,000 face images generated by unsup3d <cit.> using the Basel Face Model <cit.>. Different form CelebA, it is a simulation dataset, therefore, it lacks a lot of information on facial detail. Notably, during the training, we only use the original RGB color information and do not rely on any extra information of the above datasets.\n\n\n\n \u00a7.\u00a7 Implementation Details\n\n\n\nWe leverage in PyTorch <cit.> to implement the proposed method. All the experiments are trained for 60-80 hours on a server with four NVIDIA RTX3090 GPUs. We utilize the Adam <cit.> optimizer to train the model. In Table <ref>, we list the specific experiment details of the different training stages. For CARLA and CelebA datasets, the training process is consisted of three phases, a 32 \u00d7 32 resolution phase, a 64\u00d7 64 resolution phase and a 128 \u00d7 128 resolution phase. In terms of the BFM dataset, training process only include the first two phases in the above datasets. Our sampling strategy of camera positions during training is adaptive to the datasets. For CARLA, we use the real distribution as described above. Since CelebA is a collection dataset of real camera data without camera parameters, it is impossible to obtain the real distribution of camera poses. For a fair comparison, we therefore use an estimated normal distribution used in PI-GAN, with a vertical standard deviation of 0.155 radians and a horizontal standard deviation of 0.3 radians. And for BFM, we use the same camera pose distribution as CelebA.\n\n\n\n \u00a7.\u00a7 Main Results and Qualitative Analysis\n\n\n\n\n\n\n\n\n\n\n\nWe choose to use the visual results of the generated 3D model to finish the comparison of 3D object generation quality. It is worth noting that we have not carefully selected the generated 3D models. The 3D models generated by our method are all the smooth and realistic high quality models we present. And the 3D models generated by the other methods suffer from spatial clutter or unevenness of the model surface. As for the quality of 3D-Aware image synthesis, we use the Fr\u00e9chet Inception Distance (FID) <cit.> and Kernel Inception Distance (KID) <cit.> metrics, which are widely used in the field of image synthesis, to evaluate. The FID and KID measure the distance between the distribution of the generated data and the training data. And we use frames per second (FPS) to measure the speed of generating 3D-Aware images.\n\nWe compared our method with three mainstream approaches, including GRAF <cit.>, PI-GAN <cit.> and GOF <cit.>. In Fig. <ref>, we show a visual comparison between our method and the state-of-the-art in terms of 2D image, depth map and 3D mesh on the CARLA dataset. As can be seen from Fig.\u00a0<ref>, for the depth map and 3D mesh, there is much mess in the bottom of car yielded by these three methods. Besides, the bodywork of car is hardly generated smoothly and shows a severe ripple-like effect (zoom in for a better view). We believe it is because previous methods directly output the volume density or opacity of a point, making it difficult to obtain a effective supervision of chaotic area. By contrast, our method allows for the direct representation of a continuous surface of an object in space by the zero-level set of the SDF, which is equal to place a numerical constraint on the position away from the surface of the object and enhances the relationships between points and surface in space. Therefore, we avoid the mess around the car effectively. To further validate the effectiveness of the proposed method, in the top four rows of Fig.\u00a0<ref>, we demonstrate the full angles rotation of one of the generated mesh models on the CARLA dataset. It can be seen that our method shows obvious advantages over existing methods in generation quality. \n\nIn Fig.\u00a0<ref>, we visually compare our method with existing cutting-edge methods on the CelebA dataset. One can see that although the previous methods can generate continuous 3D-aware images, the corresponding spatial accuracy is inferior to our method. In terms of mesh performance GRAF exported, there are many small lumpy patches and incorrect depressions, resulting in undesirable quality. Although the mesh results yielded by PI-GAN are significantly improved, there are still many abnormal stripes. GOF proposes the Opacity Regularization Loss, which encourages all output of neural network to be zero or one. It is advisable to forces only points very close to the surface to be with non-zero weights when calculating the color. However, such a constraint makes network output near the surface vary drastically with the micro-variation of the input, posing more challenges on fitting the relationships in the space.  Unlike GOF, our proposed method fits the SDF of an object, which enables output of neural network to vary in a smooth form as the input changes, while preserving the principal advantage of GOF. As evidenced in the bottom four rows of Fig.\u00a0<ref>, we demonstrate the full angles rotation of the generated mesh model on the CelebA dataset. We can see that the mesh generated by our model is more realistic and natural, while enjoying smooth and spotless merits. The other methods, however, are challenged by clutters spread the space due to the lack of explicit constraints in space. Fig. <ref> presents a similar visual trend on BFM dataset. \n\n\n\n\nTable <ref> summarizes quantitative comparisons between the proposed method and current SOTA methods in terms of Fr\u00e9chet Inception Distance (FID) <cit.> and Kernel Inception Distance (KID) <cit.> on the datasets (CARLA, CelebA and BFM). For a fair comparison, we generate images at 128 \u00d7 128 resolution to calculate the metrics of FID and KID. Without bells and whistles, the quantitative performance shows that our method significantly performs better than the previous method. And performance in terms of frames per second (FPS) further proves the computation efficiency of our proposed new rendering pipeline.\n\nIn Fig. <ref>, we also show the effect of changing z_c for a given fixed z_s, , the 3D model is the same, but the appearance color of the model is constantly changed.\n\n\n\n\n\n \u00a7.\u00a7 Ablation Studies\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Accurate Sampling\n\n\n\n\n\n\nTo verify the effect of our proposed sampling strategy,  we perform experiments to make comparisons with the sampling method adopted by the original NeRF and PI-GAN. We choose a model, which is trained for the first stage on CARLA dataset at 32 \u00d7 32 resolution using coarse-to-fine sampling, as the baseline. The explanation for this is that a rudiment of SDF is just responsible for shape. And we observe that as the \u03b2 in Equ.\u00a0(<ref>) rises, the problems depicted in Fig. <ref> start to emerge. In Fig. <ref>, we show the comparisons of \u201cOnly Coarse\u201d sampling, \u201cCoarse+Fine\u201d sampling and our \u201cCoarse+Accurate\u201d sampling. As can be seen, the manners using coarse sampling and two rounds of \u201cCoarse+Fine\u201d sampling (first two lines in Fig. <ref>) show numerous small dots spread the surface, resulting in the discontinuous surface of an object. Differently, our method (last line in Fig. <ref>) not only solves the sampling ambiguity problem (shown in Fig. <ref>), but also significantly improves the sampling efficiency with less computation. Assuming that coarse sampling requires N points and fine sampling requires M points, our method succeeds to reduce the sampling number from N+M to N+1, which is ascribed to the fact that SDF serves as intermediate representations in 3D space and significantly reduce computational burden.\n\n\n\n  \u00a7.\u00a7.\u00a7 Speed Comparison of Different Renderer\n\n\n\n\nTo verify the effectiveness of our proposed rendering pipeline, we use an already trained SDF generator to compare the speed of rendering a 128 \u00d7 128 image when two methods achieving similar generated image quality. Therein, one is the volume rendering method that most of the traditional methods used. And another is our method, which is optimized by the SDF mathematical property. In Table <ref>, the results show that our designed rendering method can significantly reduce the required rendering time, which in turn saves training time. This improvement is due to the fact that our renderer can use fewer points to locate the surface of objects more accurately, while the previous methods must use the enough points.\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Eikonal Loss Weight\n\nEikonal Loss is the key to ensuring that our network can be seen as an implicit SDF. It is obviously that the gradient of the SDF function with respect to the input coordinates should be 1 everywhere. Therefore, if the Eikonal Loss is too large, the network cannot be seen as an implicit SDF. We conduct some experiments on CARLA dataset with different \u03bb_Eikonal. In Table <ref>, too small the \u03bb_Eikonal will result in too large the Eikonal Loss, which prevents the neural network from acting as an implicit SDF. When \u03bb_Eikonal is too large, it will make the network pay too little attention to the quality of the generated image, leading to a decrease in the quality of the generated 3D-aware image. Therefore, we choose 0.5 as the weight for Eikonal Loss.\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Normal Loss Weight\n\nAs we described earlier, we can manipulate the smoothness of the generated 3D model by \u2113_2 Normal Loss, thanks to the fact that our model explicitly perceives surface of the generated 3D model. Different datasets require different coefficients, but the general trend is the same. We have therefore chosen the CARLA dataset as an example. Fig. <ref> illustrates the smoothness of SDF after conversion to mesh conditioned on different Normal Loss coefficients. It is quite obvious that the larger factor is, the smoother the surface is. And when this loss item is removed,  we can see that the surface of the model degrades dramatically. In the Table <ref>, we find the generation quality of 3D-Aware images decreases as \u03bb_Normal rises. Considering the visual results are good enough with a setting of 1.0, so we set it to 1.0 on balance.\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\nTo improve 3D object generation task, we deliver a new method termed SDF-3DGAN, which introduce implicit Signed Distance Function (SDF) as the representation method of 3D object in the generative field. We apply SDF for higher quality representation of a 3D object in space and design a new SDF neural renderer. In the training process, we first generate the objects represented by implicit SDF. Then, we use our new rendering pipeline to render them to 2D images at arbitrary angles to apply GAN training method with 2D images in the dataset. We utilize the mathematical property of SDF solves the problems of sampling ambiguity when the number of sampling points is too small. Therefore, we can use the less points to finish higher quality rendering, which saves our training time. At the same time, our renderer can locate the surface point easily, which make us can apply normal loss to control the normal vector variation to make our generated object surface smoother and more realistic. Finally, quantitative and qualitative experiments conducted on public benchmarks demonstrate favorable performance against the state-of-the-art methods in 3D object generation task and 3D-Aware image synthesis task.\n\nThe limitation of our method is that in training stage, the resolution of rendered 3D-aware images is still not high enough. This will affect the generation quality of more surface details. Therefore, in the future research, we will shift focus on exploring the implicit representation generator on the higher resolution.\n\n\n\n\n\n\nIEEEtran\n\n\n\n\n\n\n\n\n\n\n\n\n"}