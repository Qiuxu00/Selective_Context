{"entry_id": "http://arxiv.org/abs/2303.06628v1", "published": "20230312102807", "title": "Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models", "authors": ["Zangwei Zheng", "Mingyuan Ma", "Kai Wang", "Ziheng Qin", "Xiangyu Yue", "Yang You"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.LG"], "text": "\n\n\n\n\n\n\nPreventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models\n    \nZangwei Zheng^1  Mingyuan Ma^2  Kai Wang^1  Ziheng Qin^1  Xiangyu Yue^3  Yang You^1\n\n\n\n\n^1National University of Singapore  ^2UC Berkeley  ^3The Chinese University of Hong Kong\n\n\n\n \n1{zangwei, kai.wang, zihengq, youy}@comp.nus.edu.sg\n2mamingyuan2001@berkeley.edu\n3xyyue@ie.cuhk.edu.hk\n\n    \n==================================================================================================================================================================================================================================================================================================\n\n\n\nempty\n\n\n\n   Continual learning (CL) can help pre-trained vision-language models efficiently adapt to new or under-trained data distributions without re-training. Nevertheless, during the continual training of the Contrastive Language-Image Pre-training (CLIP) model, we observe that the model's zero-shot transfer ability significantly degrades due to catastrophic forgetting.\n    Existing CL methods can mitigate forgetting by replaying previous data. However, since the CLIP dataset is private, replay methods cannot access the pre-training dataset. In addition, replaying data of previously learned downstream tasks can enhance their performance but comes at the cost of sacrificing zero-shot performance.\n    To address this challenge, we propose a novel method ZSCL to prevent zero-shot transfer degradation in the continual learning of vision-language models in both feature and parameter space. \n    In the feature space, a reference dataset is introduced for distillation between the current and initial models. The reference dataset should have semantic diversity but no need to be labeled, seen in pre-training, or matched image-text pairs. \n    In parameter space, we prevent a large parameter shift by averaging weights during the training.\n    We propose a more challenging Multi-domain Task Incremental Learning (MTIL) benchmark to evaluate different methods, where tasks are from various domains instead of class-separated in a single dataset. Our method outperforms other methods in the traditional class-incremental learning setting and the MTIL by 9.7% average score. Our code locates at <https://github.com/Thunderbeee/ZSCL>.\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\n\nMost deep learning models can access all the data during training\u00a0<cit.>.\nIf we want to expand a model's knowledge, such as learning a newly found animal species\u00a0<cit.>, we can re-train the model by adding new classes to the training dataset. However, re-training a large model is costly.\nIn contrast, continual learning (CL)\u00a0<cit.> incrementally learns task one after another.\nIt can reduce this cost by only learning the new data.\nNonetheless, a model tends to forget previous information catastrophically when learning new tasks\u00a0<cit.>. \nThe \u201ccatastrophic forgetting\u201d phenomenon is a great challenge for CL.\n\n\nRecently, vision-language models have shown powerful zero-shot transfer ability\u00a0<cit.>. They can give zero-shot predictions without any training examples of a task.\nHowever, the performance on some tasks is poor due to insufficient relevant image-text pairs in the pre-training datasets. For example, it is difficult for CLIP\u00a0<cit.> to distinguish among digital numbers, with an accuracy on MNIST\u00a0<cit.> below 60%\nmuch lower than a naively trained CNN\u00a0<cit.>. If we want to widen the knowledge in the vision-language model by re-training, the computational cost is too large (e.g., CLIP is pretrained on 400 million image-text pairs). Fine-tuning downstream tasks achieves high performance, but one model for a task takes much memory, and the model is not reusable.\nPrompt learning\u00a0<cit.> keeps the backbone parameters unchanged. However, it is only effective with limited training data due to a limited prompt length\u00a0<cit.>.\nIn contrast, continual learning makes learning new knowledge a lifelong process for the vision-language model.\nThe continually learned model can handle any image-text input and can be further used for downstream tasks\u00a0<cit.>. \n\n\nWe find that existing CL methods hardly prevent the forgetting phenomenon for zero-shot transfer ability in continual learning of a pre-trained vision-language model.\nAs shown in <ref> (a), the CL with a pre-trained vision-language model differs from the traditional one. Besides forgetting previously learned task knowledge, the CLIP-based CL suffers from forgetting pre-training knowledge, namely a degradation of zero-shot transfer ability. \nFor the replay-based CL methods\u00a0<cit.>, the dataset during pre-training may be private and inaccessible during fine-tuning. For distillation-based CL methods\u00a0<cit.>, they do not lay enough emphasis on the pre-trained model.\nOn the one hand, a large model state change hinders tasks thereafter from using high-quality feature representations. On the other hand, it significantly degrades zero-shot performance on unseen datasets.\n\n\nOur method ZSCL protects the Zero-Shot transfer ability during Continual Learning.\nWe view the knowledge stored in the pre-training model from two perspectives: a well-learned feature space and a good value in the parameter space. \nIn feature space, we re-design previous distillation loss\u00a0<cit.> with different loss styles, teacher models, and data sources. We find the original CLIP model, as opposed to the newly acquired model, is the best option for the teacher model. Instead of using data collected from previous tasks\u00a0<cit.> or current task\u00a0<cit.>, we find a reference dataset with diverse semantics (e.g., images sampled from ImageNet) is a good option for distillation loss. The reference images need not be labeled or matched with the text. Preserving the relative similarity between reference images and texts makes the feature space deviate little from the original.\nIn the parameter space, \nWiSE-FT\u00a0<cit.> proposes interpolating the initial and fine-tuned model for better performance. Inspired by this, we ensemble the weights throughout continuous training to prevent a significant shift from the initial CLIP, which can be seen as interpolating models of different zero-shot transfer and downstream task performance tradeoffs. The weight ensemble method is more stable and not sensitive to hyper-parameters.\n\n\nTo better evaluate our method, we propose a new benchmark Multi-domain Task Incremental Learning (MTIL). Previous CL tasks are crafted by separating classes in one dataset\u00a0<cit.>, where the images and classes are within a single domain. In contrast, MTIL consists of data from different sources requiring different expert knowledge. It comprises 11 tasks ranging from animal species to aircraft series recognition. As displayed in <ref> (b), when sequentially training CLIP on 11 datasets, the drop in the performance of task i after training task i is the traditional forgetting phenomenon. The degradation in the accuracy compared to the original zero-shot one before training task i represents the forgetting in zero-shot transfer ability. Our method better protects the zero-shot transfer ability and preserves the learned knowledge. We outperform previous methods in both conventional class-incremental learning and MTIL settings.\nIn <ref> (b), \n\n\nTo summarize, our contributions are as follows:\n\n    \n  * We investigate continual training with the vision-language model and demonstrate the importance of preserving zero-shot transfer ability. A more challenging benchmark MTIL is proposed to evaluate CL methods where the tasks come from distinct domains.\n    \n  * We propose a novel method ZSCL to mitigate the catastrophic forgetting problem in continual learning of the vision-language model by distillation in the feature space and weight ensemble in the parameter space. \n    \n  * The proposed ZSCL outperforms all state-of-the-art methods across multiple benchmark datasets. On 10 steps CL of CIFAR100 and TinyImageNet, our method outperforms the best of previous ones by 7.7% and 6.0% for the Last accuracy. On MTCL, ZSCL outperforms others by 10.9% on Transfer and 9.7% on Avg. scores.\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n\n  \nVision-Language Models.\nInspired by the success of language foundation models such as GPT-3\u00a0<cit.> and T5\u00a0<cit.>, a series of work pre-train vision-language models on large-scale image-text datasets\u00a0<cit.>. Among them, Contrastive Language-Vision Pre-training\u00a0<cit.> achieves remarkable performance on various downstream tasks. It concentrates on aligning images and texts to acquire a joint embedding space.\nThe CLIP model contains an image encoder\u00a0<cit.> and a text encoder\u00a0<cit.>. During pre-training, contrastive learning is performed in which a paired image-text is a positive pair while image and text from different image-text pairs form a negative pair. \nFor inference, the closest text embedding for the image is chosen as the prediction. Vision-language models can give zero-shot predictions on unseen tasks with a robust zero-shot transfer ability on various downstream tasks.\n\n\n\n  \nContinual Learning Methods. Most existing continual learning methods can be categorized into four groups: parameter expansion, memory replay, distillation loss, and parameter regularization. Parameter expansion methods such as DyTox\u00a0<cit.> and DEN\u00a0<cit.> \nintroduce new parameters for new tasks. As we want to achieve a more powerful CLIP model at the end of CL, we do not change the architecture of the CLIP model. Memory replay methods\u00a0<cit.> including iCaRL\u00a0<cit.> and SER\u00a0<cit.> keep a memory for exemplars from previously learned tasks. However, pre-training datasets are too large for choosing exemplars or may not be available at downstream training, and downstream data are not good exemplars for preserving the pre-training knowledge. Distillation loss such as LwF\u00a0<cit.>, LwM\u00a0<cit.>, LwF-VR\u00a0<cit.>, and PodNet\u00a0<cit.> aligns current output space with previous ones, whereas distillation based on current tasks are not strong enough to maintain foundational knowledge. For the CLIP model, we find that general images, even if never seen by the model, plus sentences with enough semantics, can be a good \u201creplay\u201d for the distillation loss. The parameter regularization loss restricts the flexibility of model parameters by training loss\u00a0<cit.> or weight averaging\u00a0<cit.>. Although this type of strategy performs badly compared to other ways in previous research\u00a0<cit.>, we found it helpful for CLIP continual learning. The limited parameter space prevents the model from diverging significantly from its original state.\n\n\n\n  \nVision-Language Models for Downstream Tasks.\nMany works propose different training strategies of vision-language models for better performance on downstream tasks, such as CoOp\u00a0<cit.>, CLIP-Adapter\u00a0<cit.> and WiSE-FT\u00a0<cit.>. However, very few attempts at continual learning exist. Recently, Thengane\u00a0\u00a0<cit.> shows CLIP zero-shot prediction achieves state-of-the-art performance in CL settings even without any training. LwF-VR\u00a0<cit.> is a modified LwF method for the CLIP model where random-generated sentences are used for distillation loss. However, it only considers the feature space, and the distillation with random sentences cannot protect the vision backbone. Differently, we re-examine what should be used for distillation in the feature space and combine the parameter space weight ensemble to provide better performance for the vision-language model continual learning.\n\n\n\u00a7 APPROACH\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Preliminaries\n\n\n\n\n  \nContinual Learning. Given n tasks [\ud835\udcaf^1, \ud835\udcaf^2, \u22ef, \ud835\udcaf^n], continual training is conducted in sequence on each task \ud835\udcaf^i=(\ud835\udc9f^i, C^i), i=1,\u2026,n. Here, \ud835\udc9f^i represents the task dataset {(x^i_j, y^i_j}_j=1^N_i, where x^i_j is an image, y^i_j is a one-hot vector indicating the ground truth, and N_i is the number of images in the dataset. Class names C^i={c^i_j}_j=1^m_i maps the label of an image to an object name, where m_i is the number of classes for task \ud835\udcaf^i. The objective of continual learning is to achieve good performance on all tasks. \n\nTwo continual learning settings are focused on in this study\u00a0<cit.>. In task-incremental learning, at inference, the image x to be predicted is given with its task identity t, so the model only needs to distinguish between different classes in C^t. In class-incremental learning, the task identity t is not given. Thus we need to predict with the combined class set C=\u22c3_i=1^nC^i.\n\n\n\n  \nCLIP model. The CLIP model contains an image encoder f_i and a text encoder f_t. The inference process of the CLIP model for image classification is as follows. First, for task \ud835\udcaf^i, each class c is transformed into a sentence by a template like \u201ca photo of {c}\u201d. Then f_t encodes the classes into text embeddings {t^i_j}_j=1^m_i. An image encoder encodes input image x_k. The similarity score between the image embedding and text embeddings are calculated as s^i_k,j=f_i(x_k),t^i_j, where \u00b7,\u00b7 denotes the cosine similarity. The class with the largest similarity score is the prediction for the image. \n\nTo fine-tune the CLIP model for downstream tasks, cross-entropy loss CE is applied to the similarity score with a temperature scaling:\n\n    \u2112_CE = 1/N\u2211_i=1^NCE(\u03c4\u00b7s_i, y_i),\n\nwhere \u03c4 is a parameter learned during the pre-training.\n\n\n\n \u00a7.\u00a7 Distillation in Feature Space\n\n\nWell-learned feature space for aligned images and texts enables vision-language models' strong zero-shot transfer ability. It also facilitates the learning of downstream tasks. Compared with the pre-training dataset, downstream datasets lie in a small scope in the feature space (shown in <ref>(b)). Direct fine-tuning of downstream tasks greatly distorts the feature distribution of out-of-distribution data, which leads to a significant drop in zero-shot prediction performance.\n\nWhile the cross-entropy improves the performance by altering fine-tuned feature subspace, we need a new regularization to preserve the potential out-of-distribution feature space. The relative similarity between one image and different texts is:\n\n    p=Softmax(s_1, \u22ef, s_m).\n\nWe hope the above similarity distribution is stable during fine-tuning for all potential images and texts. Given a teacher model f, distillation loss can be applied to penalize changes from the original distribution:\n\n    \u2112_dist_img=CE(p,p)=-\u2211_j=1^mp_j\u00b7logp_j,\n\nwhere p is the distribution given by the teacher model.\n\nAlthough the distillation form has been widely used in previous methods\u00a0<cit.>, they are applied to continual learning from scratch. We investigate different components of the distillation loss for enhancing pre-trained vision-language models.\n\nThree components are discussed in this paper in detail: the data source, the teacher model, and the loss design. <ref> shows the performance for the different choices. First, for the data source, LwF\u00a0<cit.> uses data from the current task, while iCarl\u00a0<cit.> carefully selects data from previous tasks. However, data from downstream tasks span a small subspace and are not good enough to preserve the whole feature space. An ideal choice is the pre-training dataset. However, the CLIP pre-training dataset is private, and the size is too large to load. To solve this challenge, we introduce the reference dataset. A reference dataset is a publicly available image dataset with enough semantics. Enough semantics can be seen as random sampling in the whole feature space. The texts can be related class names, unrelated sentences, or even random tokens. This is because text semantics are easier to sample, and we need not ground truth to calculate the distance between one image to sufficient text samples spread among feature space.\n\nFor the teacher model, <cit.> use the model after learning task i-1 and before learning task i as the teacher model. During the continual training of the vision-language model, the feature space deviates gradually from the initial model. Using fine-tuned models as teacher models enlarges this change. In contrast, we find using the pre-trained model as a teacher model not only preserves the zero-shot transfer ability but also takes advantage of well-learned feature space for better downstream performance.\n\nFinally, previous distillation loss is applied on traditional backbones, where a classification head gives the probability for different labels. For the vision-language model, the probability is calculated based on the relative distance between images and texts. Thus, in addition to <ref>, we impose regularization \u2112_dist_txt on the distances from a text to a batch of images. The whole framework is shown in <ref> (a) with the following training loss: \n    \u2112 = \u2112_ce+\u03bb\u00b7 (\u2112_lwf_img + \u2112_lwf_txt).\n\n\n\n\n \u00a7.\u00a7 Weight Ensemble in Parameter Space\n\n\n\n\n\nMachine learning models integrate learned knowledge in their parameters. To mitigate the forgetting problem, a series of works\u00a0<cit.> impose regularization losses on the changes of parameters. Weight consolidation (WC)\u00a0<cit.> imposes the following loss: \n\n    \u2112_WC=\u2211_i(\u03b8_i - \u03b8_i)^2.\n\nwhere \u03b8 is the parameters of the current model, and \u03b8 is the reference ones. Although this regularization prevents forgetting, it hinders learning new tasks in a challenging CL setting.\n\nApart from regularization losses, another method in parameter space is to ensemble different model weights. Model soup\u00a0<cit.> averages weights of multiple fine-tuned models to improve the model's robustness but introduces additional training costs. WiSE-FT\u00a0<cit.> propose a weighted average between fine-tuned model and the original model to improve the out-of-distribution prediction robustness:\n\n    f(x; (1-\u03b1)\u00b7\u03b8_0 + \u03b1\u00b7\u03b8_1),\n\nwhere \u03b8_0 is the original model and \u03b8_1 is the fine-tuned one. However, this method is hyper-parameter-sensitive where different \u03b1 gives different tradeoffs between zero-shot transfer ability and downstream task performance (blue line in <ref>.\n\nInspired by this, we extend the weighted average to the CL setting. The motivation for the weighted average is to prevent fine-tuning from losing too much knowledge in the original model. As training goes by (green line in <ref>), the model performs better on new tasks while losing accuracy on previous ones. The models among training compose a sequence of different learning-forgetting tradeoffs. Instead of interpolating only between the initial and the final model, our method weight ensemble (WE) averages the weights in the sequence during the training time:\n\n    \u03b8\u0302_t = \u03b8_0     t=0 \n    1/t+1\u03b8_t + t/t+1\u00b7\u03b8\u0302_t-1   every I iterations.\n\nwhere model weight sampling happens every I iteration. \nThe method is related to Stochastic Weight Averaging (SWA)\u00a0<cit.>, but we do not use a modified learning rate schedule here because instead of getting better generalization ability, WE aim to give an improved learning-forgetting tradeoff. \nAs shown in <ref>, WE achieves better performance on downstream tasks than WiSE-FT. In addition, while WiSE-FT is sensitive to different values of \u03b1, our method is much more robust under different hyper-parameter (I) choices. \n\n\n\u00a7 MULTI-DOMAIN TASK INCREMENTAL LEARNING\n\n\n\n\n\n\n\n\n\n  \nConventional Continual Learning Benchmark. A benchmark consisting of several tasks is needed to evaluate different methods for continual learning. Most previous benchmarks are built by separating classes in a single dataset, such as MNIST\u00a0<cit.>, CIFAR100\u00a0<cit.>, TinyImageNet\u00a0<cit.>, and ImageNet\u00a0<cit.>. We also evaluate our method with traditional benchmarks. In CIFAR100\u00a0<cit.>, classes are separated into groups to build tasks. Suppose the dataset has m classes, a k-step setting means we learn m/k classes in each new task. The CIFAR100 contains 100 classes, and the setting of 10, 20, and 50 steps are visited. For TinyImageNet with m=200, the first step learns 100 classes, and the rest is learned with 5, 10, and 20 steps. As for ImageNet-100, we have two settings: ImageNet-100-B0, which includes the same amount of classes for each step, and ImageNet-100-B50, which has 50 classes for the first step, and the remaining 50 classes are observed progressively over the next 10 stages. For ImageNet\u00a0<cit.>, we investigate a 10-step setting, which learns 100 new classes per task.\n\n\n\n  \nMTIL Benchmark. Different classes from one dataset share the common image source and a similar style\u00a0<cit.>. Thus, we propose Multi-domain Task Incremental Learning (MTIL), a cross-domain version of task incremental learning. Different tasks are collected from different domains, requiring different domain knowledge for humans to achieve high accuracy. Our MTIL benchmark consists of 11 tasks (detailed in supplementary materials), as some of the tasks illustrated in <ref> (a). The MTIL benchmark is very challenging with a total number of 1,201 classes. We propose two training orders detailed in the appendix, and by default, the results are given under training order-I.\n\n\n\n\n\n\n\n  \nEvaluation Metrics. The metrics of MTIL are illustrated in <ref>(b), where rows represent training steps and a column shows performance for one dataset.\nFor conventional continual learning, only scores under the diagonal are meaningful, since they cannot give zero-shot predictions on unseen tasks. In contrast, the zeros-hot transfer ability enables a vision-language model to provide predictions for all datasets. The average accuracy on all datasets among all timestamps is the \u201cAvg\u201d metric. The \u201cLast\u201d metric is the average performance of all tasks after CL. The \u201cTransfer\u201d metric is the average task performance in the upper-right triangle of the matrix. Every task's performance is first averaged to equal the weight of each dataset. It measures to what extent the zero-shot transfer ability is preserved. Before learning task i, tasks not earlier than i are not fine-tuned. Thus, their performance is an indicator of zero-shot transfer ability. \n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n\n\n\n\n\n\n\n  \nImplementation. We use CLIP\u00a0<cit.> model with image encoder ViT-B/16\u00a0<cit.>. We conduct training with AdamW\u00a0<cit.> optimizer and use a label smoothing\u00a0<cit.> technique for a better baseline result. For multi-domain task continual learning, we train 1K iterations for each task, while for class-incremental learning, we follow the same evaluation setting in\u00a0<cit.>. More implementation details can be found in the supplementary material.\n\n\n\n \u00a7.\u00a7 Main Properties\n\n\n\nWe ablate our method in feature-space in <ref>, and different choices for parameter-space regularization in <ref>. Several interesting characteristics are noted.\n\n\n\n  \nContinual learning loss. In <ref>, several types of loss for feature space are tested. The Feature Distance penalizing absolute distances achieves a low accuracy. Distillation loss on probability distribution regularizes relative distance in the feature space. With image-only or text-only distillation, the Transfer, Avg., and Last accuracy all improve. A further boost in performance occurs with both of the distillation losses.\n\n\n\n  \nData source for replay. <ref> seek the standard for a good reference dataset. As shown in <ref>, distillation on images of current tasks achieves a good Transfer score. However, it hinders the learning on new tasks and results in a low Avg. and Last score. Images in a specific domain (, Flowers) are also not good choices. General images in ImageNet and Conceptual Caption (CC) datasets are examples of good reference datasets. These images are easily available by a web crawler\u00a0<cit.>. Text with more semantics can improve performance (shown in <ref>). When using sentences from the Conceptual Caption dataset\u00a0<cit.>, or even sentences randomly generated from the CLIP vocabulary, there is no ground truth target among the texts for the image from the ImageNet dataset. However, they all achieve an improvement due to more semantics. The reference image dataset does not need to be labeled, matched with the text, or seen by the CLIP model. \n\nEnough semantics in the reference image dataset boosts the distillation performance. In <ref>, a smaller number of images and classes all lead to a degradation in the performance. Fewer classes of images in the reference dataset have a worse impact on the performance compared with the image numbers. To keep a reasonable memory buffer, we randomly sample 100k images from ImageNet for MTIL and use texts from CC dataset. For class-incremental learning, conceptual caption dataset's validation set (28k images) is used to avoid information leakage.\n\n\n\n  \nTeacher model. <ref> shows the performance of distillation loss with different teacher models. Unlike conventional continual learning, the teacher model should not be the one trained on the previous task; otherwise, the deviation from the initial CLIP in the previous task may be amplified. In contrast, with the initial CLIP as the teacher model, not only is the zero-shot performance improved but the Mean and Last scores are also boosted, indicating that preserving high-quality feature space facilitates continual learning.\n\n\n\n\n\n\n  \nParameter-space regularization. In\u00a0<ref>, we experiment with three different parameter-space regularizations. \nWe experiment two variants of WiSE-FT\u00a0<cit.>: the weighted average between the current model with the initial CLIP or the one at the previous task. The result shows the latter one is a better choice because keeping weight averaging with initial CLIP loses the newly learned knowledge. \nWe experiment with different \u03b1 choices for WiSE, and the best result is reported. While distillation loss improves the whole performance, the parameter-space regularization further protects the zero-shot transfer ability with a higher Transfer. Among the three parameter-space regularization, only WE achieves a better Last score.\n\nWC greatly improves the Transfer scores with a lower Last score. A combination of the weight consolidation loss with weight ensemble achieves a better tradeoff between Transfer and Last value. While ZSCL^*, a variant without the WC loss, obtains the highest Last score, the ZSCL with WC loss outperforms it with 2.8% Avg. and 5.9% Transfer scores.\n\n\n\n \u00a7.\u00a7 Multi-domain Task Incremental Learning\n\n\n\n<ref> displays the performance of different methods on the MTIL benchmark, and <ref> presents the detailed Transfer, Avg, and Last metrics on each dataset. Zero-shot denotes the zero-shot prediction performance of the initial CLIP model, and Fine-tune means the direct fine-tuning accuracy on each dataset, which can be seen as an upper-bound where no forgetting phenomenon happens. Continual learning uses cross-entropy loss to learn each dataset sequentially, where there is a significant forgetting issue on both zero-shot predictions (Transfer drops by 24.8%) and newly learned knowledge (Last drops by 9.4%). Previous methods improve the Last performance slightly and cannot maintain a high zero-shot prediction performance. Without WC, ZSCL^* achieves the best Last scores, outperform previous best one by 4.4%. Our method ZSCL improves 9.1% on Transfer accuracy, with only 1.3% drops compared with the initial CLIP model, and achieves a 10.1% gain in the Avg. accuracy.\n\n\n\n\n\n \u00a7.\u00a7 Class Incremental Learning\n\n\n\nWe evaluate our methods on conventional class incremental learning, which shows our method's ability in learning new tasks and preventing previously learned knowledge. <ref> and <ref> display the results on CIFAR100 and TinyImageNet, respectively. \nWe re-implement some previous methods with a CLIP backbone (after CLIP in the table), while others using a special network design cannot be easily adapted. Although zero-shot CLIP prediction achieves a good result on these benchmarks, continual learning with direct fine-tuning or LwF\u00a0<cit.> degrades the performance greatly, especially under the setting of a large step number. This demonstrates a severe catastrophic forgetting phenomenon in fine-tuning the CLIP model. Our method consistently improves the performance of the CLIP model on both Avg. and Last scores with a large gap towards previous ones. \n\n\n\n\u00a7 CONCLUSION\n\n\n\nIn this paper, we investigate continual learning with the vision-language model. We propose a better continual learning algorithm to protect the zero-shot transfer ability in the vision-language model learned in the pre-training stage. Our algorithm mitigates the catastrophic forgetting in both feature space and parameter space. In feature space, distilling the initial model on a reference dataset significantly boosts the model's performance. In parameter space, weight ensemble among different training stages alleviates the forgetting issue. We propose a more challenging Multi-domain Task Incremental Learning (MTIL) benchmark to evaluate the continual learning methods better. On both conventional and new benchmarks, our method achieves state-of-the-art performance.\n\n\nieee_fullname\n\n\n\n\nAppendix\n\n\n\u00a7 ADDITIONAL BENCHMARK DESCRIPTION\n\n\n\n<ref> displays the detailed information for different datasets in our benchmark. Two orders are used for the evaluation; the first one is alphabet order (Order-I): Aircraft, Caltech101, CIFAR100, DTD, EuroSAT, Flowers, Food, MNIST, OxfordPet, StanfordCars, SUN397. And the second one is a random order (Order-II): StanfordCars, Food, MNIST, OxfordPet, Flowers, SUN397, Aircraft, Caltech101, DTD, EuroSAT, CIFAR100.\n\n\n\n\u00a7 ADDITIONAL IMPLEMENTATION DETAILS\n\n\n\nOur implementation is based on PyTorch\u00a0<cit.>. We use batch size 64 for the MTIL benchmark and 128 for the class-incremental learning benchmark. The learning rates are searched among {10^-5,10^-6,10^-7}. Label smoothing\u00a0<cit.> can substitute the regularization of weight decay and achieve better performance. the label smoothing strength is searched among {0.1,0.2,0.3}. In general, for MTIL, CIFAR100, and TinyImageNet, weight decay 0 and label smoothing 0.2 are good choices. For ImageNet, weight decay 0.1 and label smoothing 0 are used. We experiment with I\u2208{1,10,100} and find slight changes in performance and thus fix I to 100. A large \u03bb hinders learning new knowledge, and \u03bb=1 is a good choice.\n\n\n\n\u00a7 ADDITIONAL MTIL RESULTS\n\n\nThe complete result of the MTIL benchmark with t datasets is a matrix of t\u00d7 t. It is difficult to compare different matrices between different methods, so we summarize the performance by three indicators in the main text. Here, we show the complete matrix of ZSCL in <ref> and ZSCL^* in <ref>.\n\nThe result of the MTIL method in Order-II is presented in <ref>. Our method surpasses previous methods in another order setting of the MTIL benchmark. A similar conclusion holds from the results of MTIL Order-II compared with MTIL Order-I. Our method ZSCL outperforms others by 9.2% on the Avg. score and 18.1% on the Last score with only a 1.2% performance loss on the Transfer score. This shows our approach can work for different orders of the multi-domain task incremental learning. \n\nIn addition, compared with Order-I, previous methods achieve a much lower Last score (e.g., for Continual-Learning, Order-I has 77.3%, while Order-II has 65.3%). With ZSCL, the Last score is similar (83.6% compared with 83.4%). This shows our method is more robust towards different training orders.\n\n\n\n\n\n\n\n\n\u00a7 ADDITIONAL CONVENTIONAL CLASS INCREMENTAL LEARNING RESULTS\n\n\nWe re-implement previous methods based on the CLIP backbone for continual learning. For LwF-based\u00a0<cit.> methods, we experiment with two choices for the teacher model, the previous one and the initial one. We find the initial one gives out better performance and report this result. For the WiSE-FT\u00a0<cit.> method, we take the average of models after learning each task. We experiment with the average between the previous and current one and the initial one and the current one. Better results are reported.\n\nThe result of class-continual learning on ImageNet benchmark is presented in <ref>. On IN100-B10, our method outperforms others by 1.97% for the Avg. score and 1.30% for the Last score. On IN100-B50, our method outperforms others by 3.54% for the Avg. score and 6.62 for the Last score.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 LIMITATION AND FUTURE WORK\n\n\n\nOur proposed method has a limitation that a reference dataset is needed. A promising direction of the work is to preserve the zero-shot transfer ability without the need for an outside dataset. For example, we may generate a synthetic image dataset as the reference dataset. Methods like\u00a0<cit.> can synthesize datasets from a network.\n\nThere is a trend in the deep learning community to build large models with a huge dataset\u00a0<cit.>, including vision-language models\u00a0<cit.>. These models serve as foundation models\u00a0<cit.> for downstream tasks and require millions of dollars for training. As re-training cost upsurges, continual learning is an efficient approach for updating these models with custom usage.\n\n\n\nIn some cases, we want to correct wrong information in the pre-training dataset or update old information with the latest one. How to conduct this task with a reference dataset is left as future work.\n\n\n\n"}