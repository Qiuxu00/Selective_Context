{"entry_id": "http://arxiv.org/abs/2303.07005v1", "published": "20230313110134", "title": "Real-Time Audio-Visual End-to-End Speech Enhancement", "authors": ["Zirun Zhu", "Hemin Yang", "Min Tang", "Ziyi Yang", "Sefik Emre Eskimez", "Huaming Wang"], "primary_category": "eess.AS", "categories": ["eess.AS"], "text": "\n\n\nAppendix\n    Mengchao He\n    March 30, 2023\n==================\n\n\n\n\nAudio-visual speech enhancement (AV-SE) methods utilize auxiliary visual cues to enhance speakers' voices. Therefore, technically they should be able to outperform the audio-only speech enhancement (SE) methods. However, there are few works in the literature on an AV-SE system that can work in real time on a CPU. In this paper, we propose a low-latency real-time audio-visual end-to-end enhancement (AV-E3Net) model based on the recently proposed end-to-end enhancement network (E3Net). Our main contribution includes two aspects: 1) We employ a dense connection module to solve the performance degradation caused by the deep model structure. This module significantly improves the model's performance on the AV-SE task. 2) We propose a multi-stage gating-and-summation (GS) fusion module to merge audio and visual cues. Our results show that the proposed model provides better perceptual quality and intelligibility than the baseline E3net model with a negligible computational cost increase.\n\n\n\n\nspeech enhancement, audio-visual, real-time, low-latency, dense connection\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nVideo communications have been universally applied for both business and personal connections due to the COVID-19 pandemic. Since numerous people have shifted to online communication, the request for better perceptual quality and intelligibility of audio has become increasingly crucial. However, some factors, such as background noise, reverberation, and interfering (background) speakers, can degrade the quality of the call. The leakage of interfering speakers can significantly degrade the intelligibility of the main speaker and potentially cause privacy issues. Unfortunately, unconditional audio-only speech enhancement models cannot remove interfering speakers since they are usually trained to preserve all human speech\u00a0<cit.>. \n\n\n\n\nPersonalized speech enhancement (PSE) models were proposed to enhance target speakers by adding target speakers' voice profiles\u00a0<cit.> for suppressing the interfering speakers and environmental noises. They utilized the speaker embeddings extracted by a pre-trained speaker encoder on enrollment audios\u00a0<cit.>. Alternatively, video can assist in speech enhancement from different aspects without the need for enrollment. First, the face of the speaker reveals the speaker's identity\u00a0<cit.>. Second, lip motion is highly correlated with the phonetic information of the target speech\u00a0<cit.>.\n\n\n\nIt is essential to limit the latency and computational complexity of the model to make audio-visual speech enhancement (AV-SE) models suitable for real-time communication. Although many works employed causal designs, only a few reported the computational complexity of their methods\u00a0<cit.>. This paper focuses on optimizing intelligibility and perceptual quality for real-time processing on the CPU (i.e., the inference time on the CPU is shorter than the audio time with low latency). Specifically, we propose a low-latency real-time AV-SE system, namely AV-E3Net, based on the recently proposed end-to-end enhancement network (E3Net)\u00a0<cit.>. AV-E3Net takes pixels of the mouth region of interest (ROI) and the noisy audio signal as inputs and produces the enhanced audio signal. We employ a dense connection module in AV-E3Net, which helps with better gradient flow for deeper networks. We propose a novel multi-stage gating-and-summation (GS) fusion module for merging audio and video features. We evaluate our proposed models in different application scenarios. Our results suggest that AV-E3Net yields significantly better results for the AV-SE task than the baselines.    \n\n\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\nRecently, multiple personalized speech enhancement (PSE) systems were proposed and proven computationally efficient to work in real time. Personalized PercepNet\u00a0<cit.> utilized the target speaker's voice embeddings to improve the speech enhancement capabilities of original PercepNet <cit.>. Thakker et al.\u00a0<cit.> proposed a real-time causal PSE model named end-to-end enhancement network (E3Net). Compared with other bigger PSE models such as pDCCRN\u00a0<cit.>, E3Net provided better perceptual and transcription quality with much smaller computational complexity. PSE models are conditional models that utilize speaker embeddings, and their success can be extended to other conditional models, such as AV-SE models.\n\nTraditionally, an AV-SE network comprises four components: audio encoder, video encoder, enhancement network, and audio decoder\u00a0<cit.>. The audio/video encoder extracts audio/video features, and the enhancement network combines two features to produce enhanced audio embedding. The audio decoder decodes enhanced audio embedding to recover the audio. The video encoder can be pre-trained or jointly trained with the other modules. In\u00a0<cit.>, the video encoder was trained jointly with the rest of the network, whereas\u00a0<cit.> and\u00a0<cit.> employed a pre-trained video encoder. Joint training of the video encoder with the rest of the network is somewhat challenging because of the deeper model architecture. However, there are some techniques to alleviate this problem. ResNet\u00a0<cit.> and DenseNet\u00a0<cit.> proposed dense shortcuts to address the training issue caused by the deep structures. Zhang et al.\u00a0<cit.> also proposed a unified perspective of the dense shortcut in ResNet and DenseNet. Motivated by these works, this paper employs a dense connection module to tackle the performance issue caused by the deep architecture of AV-E3Net. \n\nAudio and video fusion is an important research direction for AV-SE \u00a0<cit.> and multimodal learning\u00a0<cit.>. The most common fusion method is concatenation, which is easy to implement, but one modality often tends to dominate the other\u00a0<cit.>. Xu et al.\u00a0<cit.> proposed an attention-based fusion method. However, this method considerably increased the computational cost. Joze et al.\u00a0<cit.>, and Iuzzolino and Koishida\u00a0<cit.> proposed a squeeze-excite (SE) fusion that employs a gating module to recalibrate the modality. Additionally, their work integrated slow fusion\u00a0<cit.> with the gating module and proved that slow fusion is more effective. Wang et al.<cit.> also employed a gating network to perform a product-based fusion, ensuring the performance of the model lower-bounded by an audio-based system. Inspired by\u00a0<cit.>, we propose a multi-stage gating-and-summation (GS) module, which integrates slow fusion and provides a lightweight and efficient approach to fuse audio and video features. \n\nAn essential requirement for AV-SE systems to be adopted in practical scenarios is to make them work in real time with low computational costs. Unfortunately, only a few existing works focused on this scenario. Gu et al.\u00a0<cit.> proposed a real-time audio-visual speech separation and provided the real-time factor (RTF) measured on a GPU as the metric for computational costs. Gogate et al.<cit.> also proposed a real-time audio-visual speech enhancement model, whereas no computational complexity metric was provided. In contrast, we propose an AV-SE system that can work in real time on the CPU by utilizing computationally efficient E3Net as our backbone, using lightweight ShufflNetV2 as the video encoder, and using only mouth ROI as the visual input. \n\n\n\n\u00a7 METHODOLOGY\n\n\n\nThe overview of AV-E3Net architecture is shown in Figure\u00a0<ref>. In this section, we introduce each module of the proposed model.\n\n\n\n \u00a7.\u00a7 Audio network\n\n\nThe audio network comprises an audio encoder, a masking network, and an audio decoder. The audio encoder processes the input audio to generate audio features; subsequently, they are fed into the masking network to produce a mask, which is applied to the audio features. Within the masking network, the audio features are fused with video features generated by the video encoder. At last, the audio decoder reconstructs the audio from suppressed audio features. We follow the design of E3Net\u00a0<cit.>. The audio encoder and audio decoders are 1D convolution and 1D transposed convolution layers, respectively. The masking network linearly stacks a ReLU activation, a layer normalization, a projection block, a fusion module, multiple LSTM blocks, and a mask prediction module. Please refer to\u00a0<cit.> for the detailed description of the LSTM block and the mask prediction. In addition, we employ the dense connection module to tackle performance issues caused by a deep model structure. A dense connection replaces the original skip connection of E3Net in each LSTM block. The dense connection also exists in the fusion module. Generally, a module with a skip connection can be expressed as:\n\n    Y_n=\u03b8(f_n(x_n)+x_n)\n\nwhere n is the index of the module, \u03b8 is a layer normalization, x_n is the input of the  module f_n, and Y_n is the output. As an alternative, the dense connection is defined as:\n\n    X_n=\u03a3_i=0^nx_i,\n\n\n    Y_n=\u03b8(f_n(x_n)+X_n)\n\nor\n\n    X_n=X_n-1+x_n,\n\n\n    Y_n=\u03b8(f_n(x_n)+X_n)\n\nTherefore, X_n is a dense summation variable that is updated from X_n-1 to X_n through all dense connection blocks. Note that a dense connection requires each x_n to have the same shape. In this way, the dense connection provides shortcuts across all LSTM blocks and fusion modules.\n\n\n\n \u00a7.\u00a7 Video encoder\n\n\nIn a recent lipreading study, Ma et al.\u00a0<cit.> employed a lightweight ShuffleNetV2\u00a0<cit.> as the video encoder to extract visual features. This work verified that ShuffleNetV2 could provide satisfactory performance with much efficient computational complexity in lipreading tasks. Motivated by its success in lipreading, we also employ ShuffleNetV2, followed by a projection block, as the video encoder. The projection block changes the dimension of video features. It comprises a fully connected layer, a PReLU activation, and a layer normalization. Afterward, we optionally employ video LSTM blocks to capture the speaker's lip motions. Video LSTM blocks share the same structure as those in the audio network.\n\n\n\n \u00a7.\u00a7 Audio-Visual fusion\n\n\n\n\nNext, we describe our proposed multi-stage gating-and-summation (GS) fusion module. For the multi-stage fusion, the video LSTM blocks are forced to be paired with the audio LSTM blocks in the masking network. The fusion blocks are placed at the beginning of each pair of LSTM blocks and after the last pair of LSTM blocks, as shown in Figure\u00a0<ref>.\n\nFigure\u00a0<ref> presents the detailed architecture of the GS fusion block.\nThe block takes audio features F_a,n\u2208 R^d_a and video features F_v,n\u2208 R^d_v from the n^th pair of LSTM blocks as inputs. Two operations are included in this fusion block. First, a gating module is employed to calculate the importance of channels and recalibrate the original audio features by the importance,\n\n    G_n = \u03c3(g(\u03b4(h([F_a,n;F_v,n])))),\n\n\n    H_a,n = G_n \u2299 F_a,n\n\nwhere [F_a,n;F_v,n] concatenates audio features and video features, G_n \u2208 R^d_a, \u03c3 is the sigmoid activation, h and g are fully connected layers, and \u03b4 is a ReLU activation. Second, a dense summation module is defined as,\n\n    X_a,n=X_n+F_a,n\n\n\n    Y_a,n = \u03b8(proj(H_a,n)+X_a,n)\n\nwhere proj is a projection block, \u03b8 is the layer normalization, X_n is the dense summation variable from the n^th audio LSTM block. Updated by adding F_a,n, the new dense summation variable becomes X_a,n.\u00a0<cit.> observed serious performance degradation of other AV-SE models\u00a0<cit.> in less noisy scenarios. Therefore, they suggested that AV-SE systems should be mainly audio-based, and video cues should provide only additional contributions. The design of GS fusion follows this idea: the gating module and the summation module make fused features closer to the space of audio features. The dense summation also provides shortcuts across all LSTM blocks and fusion modules and helps with better gradient flow for AV-E3Net.\n\n\n\nNote that, due to the mismatch of frame rates for the video and audio, we up-sample the video frames to match them with the audio frames by replicating them. \n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n\n\n\n\n\n \u00a7.\u00a7 Training and validation data\n\n\nThis work followed the data simulation pipeline of\u00a0<cit.>. We utilized clean speech samples from AVSpeech\u00a0<cit.>, VoxCeleb2\u00a0<cit.>, and LRS3\u00a0<cit.> datasets. By filtering the samples according to the Mean Opinion Score (MOS) of the audio quality, we selected 214, 170, and 30 hours of video samples from these datasets, respectively. Furthermore, we used a face detector for each frame of the video samples, and if the number of frames with a face detected divided by the total number of frames was less than 0.95, we discarded that video sample. \n\nFor creating the noisy mixtures, we convolved clean speech samples with simulated room impulse response (RIR) using the image method. We employed noise clips from Audioset\u00a0<cit.> and Freesound\u00a0<cit.>, which were also convolved with RIRs. The clean speech, noise clips, and RIR files were split exclusively to simulate train, validation, and test sets. 20% of simulated samples contained only the target speaker, and 80% of them contained both the target and interference speakers. In\u00a0<cit.>, there was a restriction that the target speaker should be closer to the microphone than the interference speaker. However, in this work, we relieved this restriction. Our system assumes that the target speaker's face is the only face captured by the camera. We simulated 20,000 hours of training data and 10 hours of validation data. The average length of simulated mixture samples was around 10 seconds. We used the video frames unaltered along with simulated noisy audio samples. For video frames where the face detector did not capture the target speaker's face, we filled in the video frame input with zero tensors. \n\n\n\n\n\n\n \u00a7.\u00a7 Test sets and evaluation metrics\n\n\nTest sets followed the same simulation approach as train/validation sets, in which the source data were mutually exclusive. Only LRS3\u00a0<cit.> data was used in test sets, and the average length of simulated mixture samples was 6 seconds. We simulated test sets for two target scenarios: 1) the mixture comprises the target speaker, the interference speaker, and noise. 2) the mixture comprises the target speaker and noise. Following\u00a0<cit.>, we named these two scenarios TS1 and TS2, respectively. TS1  and TS2 included 10 hours and 1 hour of data, respectively. To measure the perceptual quality and the intelligibility of processed audio, we utilized word error rate (WER), perceptual evaluation of speech quality (PESQ), and Signal-to-distortion ratio (SDR). We also measured the real-time factor (RTF) on an Intel(R) Xeon(R) W-2133 CPU @ 3.60GHz. Since the inference speed of the model changed from time to time on a CPU, we ran the same model 100 times on a 3 seconds input to reduce the variance of observations.\n\n\n\n \u00a7.\u00a7 Implementation Details\n\n\nAudio and video samples were re-sampled in 16KHz, 25 fps, and 360p, respectively. We followed video pre-processing of lipreading\u00a0<cit.>. Each video frame was processed by 1) face and landmarks detection, 2) similarity transformation based on landmarks, and 3) cropping in a size of 50x50 on the mouth ROI. Small sizes for cropping can further reduce the video encoder's computational cost, which contributed most of the computational cost in AV-E3Net. We used Microsoft's internal face detection tool for face and landmarks detection. During training, the noisy mixtures were chunked into 3 seconds of audio batches aligned with 75 video frames. We used the power-law compressed phase-aware (PLCPA) loss function\u00a0<cit.>.\n\nRegarding the audio encoder and decoder, we set window and hop sizes to 320 (20 ms) and 160 (10 ms), respectively. The theoretical latency of AV-E3Net was 20ms. The number of features used in the audio encoder was 2048. Within the masking network, the projection block projected features from R^2048 to R^512. The number of audio LSTM blocks was 4. Within the LSTM block, the input and output dimensions of the fully connected block were 512, and the intermediate dimension of the fully connected block was 1024. The input and output dimensions of LSTM were 512. Regarding the video encoder, we used ShuffleNetV2 0.5x\u00a0<cit.> to encode video frames to 1024-dimension features. Then a projection block was employed to project video features to 512-dimension. Afterward, video LSTM blocks for lip motion capture shared the same configuration as audio LSTM blocks. GS fusion's audio and video input dimensions were 512, and the first fully connected layer projected concatenated features from R^1024 to R^512. We set the optimizer as AdamW\u00a0<cit.> and the learning rate scheduler to be a step decay scheduler with a gradual warm-up mechanism. The peak learning rate was 0.001.\n\n\n\n \u00a7.\u00a7 Baseline Systems\n\nWe employed the following baseline models for comparison with our proposed models: \n\nAO-E3Net: An audio-only E3Net model. It is an unconditional model and not capable of removing the interfering speaker.\n\nAV-DCATTUNET: A variant of pDCATTUNET, introduced in\u00a0<cit.>, in which the speaker embedding was replaced by the video frame embedding extracted by a pre-trained face recognition model (ShuffleNetV2 0.5x). The face recognition model takes the whole face of each video frame as the input. We set the number of encoder/decoder blocks to 6 and the number of bottleneck blocks to 4. The STFT window and hop sizes were 512 and 256 samples, respectively.\n\nNaive AV-E3Net: The AV-E3Net without dense connection and multi-stage GS fusion. It combined the video encoder (ShuffleNetV2 0.5x) with E3Net and employed a single concatenation block to merge late video features from the video encoder with intermediate audio features before the first audio LSTM block. A single concatenation block comprises a concatenation layer,  a projection block, a dense connection or a skip connection, and a layer normalization. Particularly, only skip connection was used in Naive AV-E3Net's LSTM block and single concatenation block.\n\n\n\n \u00a7.\u00a7 Results\n\n\n\n\n\n\n\nTable\u00a0<ref> shows the computational complexity of different model configurations and the corresponding perceptual quality and intelligibility results on TS1 and TS2 test sets. According to the results, AO-E3Net performs poorly on TS1 since it cannot remove the interfering speaker. In contrast, AV-DCATTUNET provides much better results on both TS1 and TS2 than the AO-E3Net in terms of speech and transcription quality. Naive AV-E3Net without video LSTM yields worse WER results than AV-DCATTUNET but provides better SDR. Adding a single video LSTM to the Naive AV-E3Net yields similar results; However, increasing it to 4 LSTM blocks degrades speech and transcription quality, indicating training difficulty. By adding the dense connection to AV-E3Net, we observe significant speech and transcription quality improvement with a negligible computational cost increase. With the dense connection, adding more video LSTM layers improves the transcription quality while yielding similar speech quality. AV-E3Net models with dense connection outperform AV-DCATTUNET on TS1 and achieve comparable performance on TS2 with a much lower computational cost. Next, the results show that AV-E3Net with multi-stage training further improves speech and transcription quality. The best AV-E3Net results are obtained using multi-stage fusion with the GS fusion. The GS fusion helps with substantially better performance on TS2, which is for the less noisy scenario. The computational cost increase for multi-stage GS fusion is minor.\n\nIt should be noted that the AV-DCATTUNET model cannot be used for real-time processing because of the model's dependency on a pre-trained video encoder. Although the face recognition model employs the efficient ShuffleNetV2, it takes the whole face rather than the mouth ROI as the input. Therefore, the RTF on the video encoder reaches 1.442. However, since AV-E3Net uses only mouth ROI as the input, it can work in real time. These results suggest that AV-E3Net performs better than the bigger model (AV-DCATTUNET) with a lower computational cost. \n\n\n\n\u00a7 CONCLUSIONS\n\n\n\nWe proposed a low-latency real-time audio-visual end-to-end speech enhancement model AV-E3Net. We employed a dense connection module, which significantly improved both perceptual quality and intelligibility with minimal increase in computational costs. Furthermore, we proposed a novel multi-stage gating-and-summation (GS) fusion module that dynamically and effectively fuses speech and vision modalities. We showed that our proposed model performed much better than the baseline systems. Furthermore, the ablation study showed the impact of adding dense connection and multi-stage GS modules. The computational cost of our system is much lower than the baseline AV-SE system and can work in real time on the CPU. Therefore, the proposed AV-E3Net has excellent potential in real-world video communication applications as a low-latency and real-time model.[Samples available at <https://github.com/zzrdwj/AVSE>]\n\n\n\n\n\n\n\n\n\nIEEEtran\n\n\n"}