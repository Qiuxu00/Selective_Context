{"entry_id": "http://arxiv.org/abs/2303.06895v1", "published": "20230313065748", "title": "An Improved Sample Complexity for Rank-1 Matrix Sensing", "authors": ["Yichuan Deng", "Zhihang Li", "Zhao Song"], "primary_category": "cs.IT", "categories": ["cs.IT", "math.IT"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nIntern Project\n    \nYichuan Deng. University of Science and Technology of China.\nZhihang Li. Huazhong Agriculture University. \nZhao Song. Adobe Research\n\n    \n==========================================================================================================================================\n\n????\n\n\n[\n\n???\n\n\n\n\n\n\n\n\n\n\n\n\n\nequal*\n\n\nAeiau Zzzzequal,to\nBauiu C.\u00a0Yyyyequal,to,goo\nCieua Vvvvvgoo\nIaesut Saoeued\nFiuea Rrrrto\nTateu H.\u00a0Yaseheed,to,goo\nAaoeu Iasohgoo\nBuiui Eueued\nAeuia Zzzzed\nBieea C.\u00a0Yyyyto,goo\nTeoau Xxxxed\nEee Pppped\n\n\ntoDepartment of Computation, University of Torontoland, Torontoland, Canada\ngooGoogol ShallowMind, New London, Michigan, USA\nedSchool of Computation, University of Edenborrow, Edenborrow, United Kingdom\n\nCieua Vvvvvc.vvvvv@googol.com\nEee Ppppep@eden.co.uk\n\n\n\n\nMachine Learning, ICML\n\n0.3in\n]\n\n \n\n\n\n\n\n\n\n  Intern Project\n    \nYichuan Deng. University of Science and Technology of China.\nZhihang Li. Huazhong Agriculture University. \nZhao Song. Adobe Research\n\n    \n==========================================================================================================================================\n\n\nMatrix sensing is a problem in signal processing and machine learning that involves recovering a low-rank matrix from a set of linear measurements. The goal is to reconstruct the original matrix as accurately as possible, given only a set of linear measurements obtained by sensing the matrix <cit.>. In this work, we focus on a particular direction of matrix sensing, which is called rank-1 matrix sensing <cit.>. \nWe present an improvement over the original algorithm in <cit.>. \nIt is based on a novel analysis and sketching technique that enables faster convergence rates and better accuracy in recovering low-rank matrices. The algorithm focuses on developing a theoretical understanding of the matrix sensing problem and establishing its advantages over previous methods. The proposed sketching technique allows for efficiently extracting relevant information from the linear measurements, making the algorithm computationally efficient and scalable. \n\nOur novel matrix sensing algorithm improves former result <cit.> on in two senses,\n\n    \n  * We improve the sample complexity from O(\u03f5^-2 dk^2) to O(\u03f5^-2 (d+k^2)).\n    \n  * We improve the running time from O(md^2 k^2) to O(m d^2 k).   \n\nThe proposed algorithm has theoretical guarantees and is analyzed to provide insights into the underlying structure of low-rank matrices and the nature of the linear measurements used in the recovery process. \nIt advances the theoretical understanding of matrix sensing and provides a new approach for solving this important problem. \n\n  \n  empty\n\n\n\n\n\n\n\n\n\n\nMatrix sensing is a problem in signal processing and machine learning that involves recovering a low-rank matrix from a set of linear measurements. The goal is to reconstruct the original matrix as accurately as possible, given only a set of linear measurements obtained by sensing the matrix <cit.>. In this work, we focus on a particular direction of matrix sensing, which is called rank-1 matrix sensing <cit.>. \nWe present an improvement over the original algorithm in <cit.>. \nIt is based on a novel analysis and sketching technique that enables faster convergence rates and better accuracy in recovering low-rank matrices. The algorithm focuses on developing a theoretical understanding of the matrix sensing problem and establishing its advantages over previous methods. The proposed sketching technique allows for efficiently extracting relevant information from the linear measurements, making the algorithm computationally efficient and scalable. \n\nOur novel matrix sensing algorithm improves former result <cit.> on in two senses,\n\n    \n  * We improve the sample complexity from O(\u03f5^-2 dk^2) to O(\u03f5^-2 (d+k^2)).\n    \n  * We improve the running time from O(md^2 k^2) to O(m d^2 k).   \n\nThe proposed algorithm has theoretical guarantees and is analyzed to provide insights into the underlying structure of low-rank matrices and the nature of the linear measurements used in the recovery process. \nIt advances the theoretical understanding of matrix sensing and provides a new approach for solving this important problem. \n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\nThe matrix sensing problem is a fundamental problem in signal processing and machine learning that involves recovering a low-rank matrix from a set of linear measurement. This problem arises in various applications such as image and video processing <cit.> and sensor networks <cit.>. \nMathematically, matrix sensing can be formulated as a matrix view of compressive sensing problem <cit.>. The rank-1 matrix sensing problem was formally raised in <cit.>. \n\nThe matrix sensing problem has attracted significant attention in recent years, and several algorithms have been proposed to solve it efficiently. In this paper, we provide a novel improvement over the origin algorithm in <cit.>, with improvement both on running time and sample complexity. \n \n\nMatrix sensing is a fundamental problem in signal processing and machine learning that involves recovering a low-rank matrix from a set of linear measurements. Specifically, given a matrix W_*\u2208\u211d^d \u00d7 d of rank k that is not directly accessible, we aim to recover W_* from a set of linear measurements b \u2208^n applied to the ground truth matrix W^* where\n\n    b_i=[A_i^\u22a4 W_*],\u00a0\u00a0\u00a0\u2200 i=1, \u2026, m,\n\nwhere A_i are known linear operators. The measurements b_i are obtained by sensing the matrix W_* using a set of linear measurements, and the goal is to reconstruct the original matrix W_* as accurately as possible. This problem arises in various applications such as image and video processing, sensor networks, and recommendation systems.\n\nThe matrix sensing problem is ill-posed since there may exist multiple low-rank matrices that satisfy the given linear measurements. However, the problem becomes well-posed under some assumptions on the underlying matrix, such as incoherence and restricted isometry property (RIP) <cit.>\n, which ensure unique and stable recovery of the matrix. A well-used method to solve this problem is to use convex optimization techniques that minimize a certain loss function subject to the linear constraints. Specifically, one can solve the following convex optimization problem:\n\n    min_W_*\u00a0(W_*)\n    s.t.   \u00a0[A_i^\u22a4 W_*] = b_i, \u2200 i=1,\u2026,m.\n\nHowever, this problem is NP-hard <cit.> and intractable in general, and hence, various relaxation methods \nhave been proposed, such as nuclear norm minimization and its variants, which provide computationally efficient solutions with theoretical guarantees. In this work, we focus on the rank-one independent measurements. Under this setting, the linear operators A_i can be decomposed into the form of A_i = x_iy_i^\u22a4, where x_i \u2208^d, y_i \u2208^d are all sampled from zero-mean multivariate Gaussian distribution N(0, I_d). \n\nOur work on improving the matrix sensing algorithm is based on a novel analysis and sketching technique \nthat enables faster convergence rates and better accuracy in recovering low-rank matrices. We focus on developing a theoretical understanding of the proposed algorithm and establishing its advantages over previous methods. Our analysis provides insights into the underlying structure of the low-rank matrices and the nature of the linear measurements used in the recovery process. The proposed sketching technique allows us to efficiently extract relevant information from the linear measurements, making our algorithm computationally efficient and scalable. Overall, our contribution advances the theoretical understanding of matrix sensing and provides a new approach for solving this important problem. \n\n\n\n\n \u00a7.\u00a7 Our Result\n\nTo summarize, we improve both the running time of original algorithm <cit.> from O(md^2k^2) to O(md^2k), and the sample complexity from O(\u03f5^-2dk^2) to O(\u03f5^-2(d + k^2)). Formally, we get the following result, \n \n\n\n\nLet \u03f5_0 \u2208 (0,0.1) denote the final accuracy of the algorithm. Let \u03b4\u2208 (0,0.1) denote the failure probability of the algorithm. Let \u03c3_1^* denote the largest singular value of ground-truth matrix W_* \u2208^d \u00d7 d. Let \u03ba denote the condition number of ground-truth matrix W_* \u2208^d \u00d7 d. \nLet \u03f5\u2208 (0, 0.001/(k^1.5\u03ba)) denote the RIP parameter. Let m = \u0398 (\u03f5^-2 (d+k^2)log(d/\u03b4)). Let T = \u0398(log(k \u03ba\u03c3_1^* /\u03f5_0)) . There is a matrix sensing algorithm (Algorithm\u00a0<ref>) that takes O(m T) samples, runs in T iterations, and each iteration takes O(md^2 k) time, finally outputs a matrix W \u2208^d \u00d7 d such that\n\n    (1-\u03f5_0) W_* \u227c W \u227c (1+\u03f5_0) W_*\n\nholds with probability at least 1-\u03b4.\n\n\n \n\n\n\n\n\n \u00a7.\u00a7 Related Work\n\n\n\n\n  \nMatrix Sensing\n\n\nThe matrix sensing problem has attracted significant attention in recent years, and several algorithms have been proposed to solve it efficiently. One of the earliest approaches is the convex optimization-based algorithm proposed by Cand\u00e8s and Recht in 2009 <cit.>, which minimizes the nuclear norm of the matrix subject to the linear constraints. This approach has been shown to achieve optimal recovery guarantees under certain conditions on the linear operators, such as incoherence and RIP.\nSince then, various algorithms have been proposed that improve upon the original approach in terms of computational efficiency and theoretical guarantees. For instance, the iterative hard\nthresholding algorithm (IHT) proposed by Blumensath and Davies in 2009 <cit.>, and its variants, such\nas the iterative soft thresholding algorithm (IST), provide computationally efficient solutions with\nimproved recovery guarantees. \nIn the work by Recht, Fazel, and Parrilo \n\n<cit.>, they gave some measurement operators satisfying the RIP and proved that, with O(k d log d) measurements, a rank-k matrix W_* \u2208^d \u00d7 d can be recovered. \nMoreover, later works have proposed new approaches that exploit additional structure in the low-rank matrix, such as sparsity or group sparsity, to further improve recovery guarantees and efficiency. For instance, the sparse plus low-rank (S + L) approach proposed by \n<cit.>, and its variants, such as the robust principal component analysis (RPCA) and the sparse subspace clustering (SSC), provide efficient solutions with improved robustness to outliers and noise. More recently, <cit.> considers the non-square matrix sensing under RIP assumptions, and show that matrix factorization \ndoes not introduce any spurious\nlocal minima \nunder RIP. <cit.> studies the technique of discrete-time mirror descent utilized to address the unregularized empirical risk in matrix sensing.\n\n\n\n\n  \nCompressive Sensing\n \nCompressive sensing has been a widely studied topic in signal processing and theoretical computer science field <cit.>. <cit.> gave a fast algorithm (runs in time O(klog n log( n/k)) for generall in puts and O(klog n log(n/k)) for at most k non-zero Fourier coefficients input) for k-sparse approximation to the discrete Fourier transform of an n-dimensional signal. <cit.> provided an algorithm such that it uses O_d(k log N loglog N) samples of signal and runs in time O_d(klog^d+3 N) for k-sparse approximation to the Fourier transform of a length of N signal. Later work <cit.> proposed a new technique for analysing noisy hashing schemes that arise in Sparse FFT, which is called isolation on average, and applying it, it achieves sample-optimal results in klog^O(1)n time for estimating the values of a list of frequencies using few samples and computing Sparse FFT itself. \n<cit.> gave the first sublinear-time \u2113_2/\u2113_2 compressed sensing which achieves the optimal number of measurements without iterating. After that, <cit.> provided an algorithm which uses O(k log k log n) samples to compute a k-sparse approximation to the d-dimensional Fourier transform of a length n signal. \nLater by <cit.> provided an efficient Fourier Interpolation algorithm that improves the previous best algorithm <cit.> on sample complexity, time complexity and output sparsity. And in <cit.> they presented a unified framework for the problem of band-limited signal reconstruction and achieves high-dimensional Fourier sparse recovery and high-accuracy Fourier interpolation. \nRecent work <cit.> designed robust algorithms for super-resolution imaging that are efficient in terms of both running time and sample complexity for any constant dimension under the same noise model as <cit.>, based on new techniques in Sparse Fourier transform. \n\n\n\n\n\n\n\n  \nFaster Iterative Algorithm via Sketching\n\nLow rank matrix completion is a well-known problem in machine learning with various applications in practical fields such as recommender systems, computer vision, and signal processing. Some notable surveys of this problem are provided in <cit.>. While Candes and Recht <cit.> first proved the sample complexity for low rank matrix completion, other works such as <cit.> and <cit.> have provided improvements and guarantees on convergence for heuristics. In recent years, sketching has been applied to various machine learning problems such as linear regression <cit.>, low-rank approximation <cit.>, weighted low rank approximation, matrix CUR decomposition <cit.>, and tensor regression <cit.>, leading to improved efficiency of optimization algorithms in many problems. For examples, linear programming <cit.>, matrix completion <cit.>, empirical risk minimization <cit.>, training over-parameterized neural network <cit.>, discrepancy algorithm <cit.>, frank-wolfe method <cit.>, and reinforcement learning <cit.>.\n\n \n\n\n\n  \nRoadmap.\nWe organize the following paper as follows. In Section\u00a0<ref> we provide the technique overview for our paper. In Section\u00a0<ref> we provide some tools and existing results for our work. In Section\u00a0<ref> we provide the detailed analysis for our algorithm. In Section\u00a0<ref> we argue that our measurements are good. In Section\u00a0<ref> we provide analysis for a shrinking step. \nIn Section\u00a0<ref> we provide the analysis for our techniques used to solve the optimization problem at each iteration. \n\n \n\n\n\n\u00a7 TECHNIQUE OVERVIEW\n\n\n \n\nIn this section, we provide a detailed overview of the techniques used to prove our results. Our approach is based on a combination of matrix sketching and low-rank matrix recovery techniques. Specifically, we use a sketching technique that allows us to efficiently extract relevant information from linear measurements of the low-rank matrix. We then use this information to recover the low-rank matrix using a convex optimization algorithm. With these techniques, we are able to improve previous results in both sample complexity and running time. From the two perspective, we give the overview of our techniques here. \n\n\n\n \u00a7.\u00a7 Tighter Analysis Implies Reduction to Sample Complexity\n\nOur approach achieves this improvement by using a new sketching technique that compresses the original matrix into a smaller one while preserving its low-rank structure. This compressed version can then be used to efficiently extract relevant information from linear measurements of the original matrix.\n\nTo analyze the performance of our approach, we use tools from random matrix theory and concentration inequalities. Specifically, we use the Bernstein's inequality for matrices to establish bounds on the error of our recovery algorithm. \nWe first define our measurements and operators, for each i \u2208 [m], let x_i, y_i denotes samples from (0, I_d). We define\n\n    \n  * A_i := x_i y_i^\u22a4;\n    \n  * b_i := x_i^\u22a4 W_* y_i;\n    \n  * W_0 := 1/m\u2211_i = 1^m b_i A_i;\n    \n  * B_x := 1/m\u2211_i = 1^m (y_i^\u22a4 v)^2 x_ix_i^\u22a4;\n    \n  * B_y := 1/m\u2211_i = 1^m (x_i^\u22a4 v)^2 y_iy_i^\u22a4;\n    \n  * G_x := 1/m\u2211_i=1^m (y_i^\u22a4 v)(y_i^\u22a4 v_)x_ix_i^\u22a4;\n    \n  * G_x := 1/m\u2211_i=1^m (x_i^\u22a4 v)(x_i^\u22a4 v_)y_iy_i^\u22a4.\n\nWe need to argue that our measurements are good under our choices of m, here the word \u201cgood\u201d means that \n\n    \n  * W_0 - W_*\u2264\u03f5\u00b7W_*;\n    \n  * B_x- I\u2264\u03f5 and B_y - I\u2264\u03f5;\n    \n  * G_x\u2264\u03f5 and G_y\u2264\u03f5.\n\nIn our analysis we need to first bound Z_i and [Z_iZ_i^\u22a4], where Z_i := x_ix_i^\u22a4 U_*\u03a3_*Y_*^\u22a4 y_iy_i^\u22a4. With an analysis, we are able to show that (Lemma\u00a0<ref> and Lemma\u00a0<ref>)\n\n    [Z_i\u2264 C^2 k^2 log^2(d/\u03b4)\u03c3^4 \u00b7\u03c3_1^*]    \u00a0\u2265 1 - \u03b4/(d) \n    [Z_iZ_i^\u22a4]   \u00a0\u2264 C^2k^2\u03c3^4(\u03c3_1^*)^2.\n\nNow, applying these two results and by Bernstein's inequality, we are able to show that our operators are all \u201cgood\u201d (Theorem\u00a0<ref>). \n\n\n\n\n \u00a7.\u00a7 Induction Implies Correctness\n\n\nTo get the final error bounded, we use an inductive strategy to analyze. Here we let U_* and V_* be the decomposition of ground truth W_*, i.e., W_* = U_* V_*. We show that, when iteratively applying our alternating minimization method, if U_t and V_t are closed to U_* and V_* respectively, then the output of next iteration t+1 is close to U_* and V_*. Specifically, we show that, if (U_t, U_*) \u22641/4\u00b7(V_t, V_*), then it yields\n\n    (V_t+1, V_*) \u22641/4\u00b7(U_t, U_*).\n\nSimilarly, from the other side, if (V_t+1, V_*) \u22641/4\u00b7(U_t, U_*), we have\n\n    (U_t+1, U_*) \u22641/4\u00b7(V_t+1, V_*).\n\nThis two recurrence relations together give the guarantee that, if the starting error U_0 - U_* and V_0 - V_*, the distance from V_t and U_t to V_* and U_*, respectively.\n\nTo prove the result, we first define the value of \u03f5_d as /10. Then, by the algorithm, we have the following relationship between V_t+1 and V_t+1 R^-1,\n\n    V_t+1 = V_t+1 R^-1 = (W_*^\u22a4 U_t - F)R^-1,\n\nwhere the second step follows from the definition of V and defining F as Definition\u00a0<ref>. Now we show that, F and R^-1 can be bound respectively,\n\n    F   \u00a0\u2264 2\u03f5 k^1.5\u00b7\u03c3_1^* \u00b7(U_t, U_*)   Lemma\u00a0<ref>\n    R^-1   \u00a0\u2264 10/\u03c3_k^*   \u00a0Lemma\u00a0<ref>\n\nNote that the bound of ^-1 need (U_t, U_*) \u22641/4\u00b7(V_t, V_*))\n\nWith these bounds, we are able to show the bound for (V_t+1, V_*). We first notice that, (V_t+1, V_*) can be represented as (V_*,)^\u22a4 V_t+1, where V_*,\u2208^d \u00d7 (d-k) is a fixed orthonormal basis of the subspace orthogonal to span(V_*). Then we show that (Claim\u00a0<ref>)\n\n    (V_*,)^\u22a4 V_t+1 = -(V_*, )^\u22a4 FR^-1.\n\nNow, by turning (V_t+1, V_*) to the term of F and R, and using the bound for F and R^-1, we are finally able to reach the bound \n\n    (V_t+1, V_*) \n        =    \u00a0FR^-1\n    \u2264   \u00a0F\u00b7R^-1\n    \u2264   \u00a0 2\u03f5 k^1.5\u00b7\u03c3_1^* \u00b7(U_t, U_*) \u00b7R^-1\n    \u2264   \u00a0 2\u03f5 k^1.5\u00b7\u03c3_1^* \u00b7(U_t, U_*) \u00b7 10/\u03c3_k^* \n    \u2264   \u00a0 0.01 \u00b7(U_t, U_*).\n\nBy a similar analysis, we can show Eq.(<ref>). \n\nNow applying them and with a detailed analysis, we have the claimed proved. Finally, when we prove that the initialization of the parameters are good, we can show that, the final output W_T satisfies\n\n    W_T - W_*\u2264\u03f5_0.\n\n\n\n\n \u00a7.\u00a7 Speeding up with Sketching Technique\n\nNow we consider the running time at each iteration. \nAt each iteration of our algorithm, we need to solve the following optimization problem: \n\n    min_V \u2208^d \u00d7 k\u2211_i = 1^m ([A_i^\u22a4 UV^\u22a4] - b)^2.\n\nWhen this problem is straightforwardly solved, it costs O(md^2k^2) time, which is very expensive. So from another new direction, we give an analysis such that, this problem can be converted to a minimization problem where the target variable is a vector. To be specific, we show that, above optimization question (<ref>) is equivalent to the following (Lemma\u00a0<ref>),\n\n    min_v \u2208^dkMv - b_2^2,\n\nwhere the matrix M \u2208^m \u00d7 dk\nis defined to be the reformed matrix of U^\u22a4 A_i's, i.e.,\n\n    M_i,* := (U^\u22a4 A_i), \u00a0\u00a0\u2200 i \u2208 [m].\n\nWhen working on this form of optimization problem, inspired by a recent work <cit.>, we apply the fast sketch-to-solve low-rank matrix completion method. With this technique, we are able to reduce the running time to O(md^2k) (Theorem\u00a0<ref>), which is much more acceptable. \n\n\n\n\n\u00a7 PRELIMINARY\n\n\nIn this section, we provide preliminaries to be used in our paper.  In Section\u00a0<ref> we introduce notations we use. In Section\u00a0<ref> and Section\u00a0<ref> we provide some randomness facts and algebra facts respectively. In Section\u00a0<ref> we introduce the important definition of restricted isometry property. In Section\u00a0<ref> we provide results fro rank-one estimation. In Section\u00a0<ref> we introduce the rank-one independent Gaussian operator. In Section\u00a0<ref> we state our notations for angles and distances. In Section\u00a0<ref> we provide some matrix concentration results. \n\n\n\n \u00a7.\u00a7 Notations\n\n\n\nLet x \u2208^n and w \u2208_\u2265 0^n, we define the norm x_w := (\u2211_i=1^n w_i x_i^2)^1/2.  \n\nFor n > k, for any matrix A \u2208^n \u00d7 k, we denote the spectral norm of A by A, i.e., A  := sup_x\u2208^k A x _2 /  x _2.\n\n\nWe denote the Frobenius norm of A by A _F, i.e., A _F : = (\u2211_i=1^n \u2211_j=1^k A_i,j^2 )^1/2.\n\nFor any square matrix A \u2208^n \u00d7 n, we denote its trace by [A], i.e., [A] := \u2211_i=1^n A_i,i.\n\nFor any A \u2208^n \u00d7 d and B \u2208^n \u00d7 d, we denote \u27e8 A , B \u27e9 = [A^\u22a4 B].\n\nLet A \u2208^n \u00d7 d and x \u2208^d be any matrix and vector, we have that\n\n    A x _2^2 = \u27e8 A x, A x \u27e9 = \u27e8 x , A^\u22a4 A x \u27e9 = x^\u22a4 A^\u22a4 A x.\n\n\nLet the SVD of A \u2208^n \u00d7 k to be U\u03a3 B^\u22a4, where U \u2208^n \u00d7 k and V \u2208^k \u00d7 k have orthonormal columns and \u03a3\u2208^k \u00d7 k be diagonal matrix. We say the columns of U are the singular vectors of A. We denote the Moore-Penrose pseudoinverse matrix of A as A^\u2020\u2208k \u00d7 n, i.e., A^\u2020 := V\u03a3^-1U^\u22a4. We call the diagonal entries \u03c3_1, \u03c3_2, \u2026, \u03c3_k of \u03a3 to be the eigenvalues of A. We assume they are sorted from largest to lowest, so \u03c3_i denotes its i-th largest eigenvalue, and we can write it as \u03c3_i(A). \n\n\n\n\nFor A \u2208^n_1 \u00d7 d_1, B \u2208^n_2 \u00d7 d_2. We define kronecker product \u2297 as (A \u2297 B)_i_1+(i_2-1)n_1, j_1 + (j_2-1)n_2\n \nfor all i_1 \u2208 [n_1], j_1 \u2208 [d_1], i_2 \u2208 [n_2] and j_2 \u2208 [d_2].\n\nFor any non-singular matrix A \u2208^n \u00d7 n, we define A=QR its QR-decomposition, where Q \u2208^n \u00d7 n is an orthogonal matrix and R \u2208^n \u00d7 n is an non-singular lower triangular matrix. For any full-rank matrix A \u2208^n \u00d7 m, we define A=QR its QR-decomposition, where Q \u2208^m \u00d7 n is an orthogonal matrix and R \u2208^n \u00d7 n is an non-singular lower triangular matrix. We use R=QR(A) \u2208^n \u00d7 n to denote the lower triangular matrix obtained by the QR-decomposition of A \u2208^m \u00d7 n. \n\nLet A \u2208^k\u00d7 k be a symmetric matrix. The eigenvalue decomposition of A is A = U\u039b U^\u22a4, where \u039b is a diagonal matrix. \n\n\n\nIf a matrix A is positive semidefinite (PSD) matrix, we denote it as A \u227d 0, which means x^\u22a4 A x \u2265 0 for all x. \n\nSimilarly, we say A \u227d B if x^\u22a4  Ax \u2265 x^\u22a4 B x for all vector x. \n \n\nFor any matrix U \u2208^n \u00d7 k, we say U is an orthonormal basis if U_i=1 for all i \u2208 [k] and for any i\u2260 j, we have \u27e8 U_i, U_j \u27e9 = 0. Here for each i \u2208 [k], we use U_i to denote the i-th column of matrix U.\n\nFor any U \u2208^n \u00d7 k (suppose n > k)which is an orthonormal basis, \nwe define U_\u2208^n \u00d7 (n-k) to be another orthonormial basis that, \n\n    U U^\u22a4 + U_ U_^\u22a4 = I_n\n\nand\n\n    U^\u22a4 U_ =  0^k \u00d7 (n-k)\n\nwhere we use 0^k \u00d7 (n-k) to denote a k \u00d7 (n-k) all-zero matrix. \n\nWe say a vector x lies in the span of U, if there exists a vector y such that x = U y.\n\nWe say a vector z lies in the complement of span of U, if there exists a vector w such that z = U_ w. Then it is obvious that \u27e8 x,z \u27e9 = x^\u22a4 z =z^\u22a4 x =0.\n\nFor a matrix A, we define \u03c3_min(A) := min_x A x _2 /  x _2. Equivalently,  \u03c3_min(A) := min_x:  x _2=1 A x _2.\n\nSimilarly, we define \u03c3_max(A) := max_x  A x _2 /  x _2. Equivalently,  \u03c3_max(A) := max_x:  x _2=1 A x _2 \n\nLet A_1, \u22ef, A_n denote a list of square matrices. Let S denote a block diagonal matrix S = [ A_1            ;     A_2        ;           \u22f1    ;             A_n ]. Then S  = max_i\u2208 [n] A_i. \n\nWe use [] to denote probability. We use [] to denote expectation.\n\n\n\nLet a and b denote two random variables. Let f(a) denote some event that depends on a (for example f(a) can be a=0 or a \u2265 10.). Let g(b) denote some event that depends on b. We say a and b are independent if [f(a) \u00a0and\u00a0 g(b)] = [f(a)] \u00b7[g(b)]. We say a and b are not independent if [ f(a) \u00a0and\u00a0 g(b)] \u2260[f(a)] \u00b7[g(b)]. Usually if a and b are independent, then we also have [ab] = [a] \u00b7[b]. \n\nWe say a random variable x is symmetric if [x = u] = [x=-u].\n \n\nFor any random variable x \u223c N(\u03bc,\u03c3^2). This means [x ] = \u03bc and [x^2] = \u03c3^2.\n\nWe use O(f) to denote f \u00b7(log f).\n\n\n\n\nWe use (a,b,c) to denote the time of multiplying an a \u00d7 b matrix with another b \u00d7 c matrix.\n \n  \nWe use \u03c9 to denote the exponent of matrix multiplication, i.e., n^\u03c9 =(n,n,n).\n\n\n\n \u00a7.\u00a7 Randomness Facts\n\n\n\n\nWe have\n\n    \n  * Part 1. Expectation has linearity, i.e., [ \u2211_i=1^n x_i ] = \u2211_i=1^n [x_i].\n    \n  * Part 2. For any random vectors x and y, if x and y are independent, then for any fixed function f, we have _x,y[f(x) f(y)] = _x[f(x) ] \u00b7_y[ f(y)].\n    \n  * Part 3. Let A\u2208^d \u00d7 d denote a fixed matrix. For any fixed function f : ^d \u2192^d \u00d7 d, we have _x[f(x) \u00b7 A ] = _x [f(x)] \u00b7 A.\n    \n  * Part 4. Given n events A_1, A_2, \u22ef A_n. For each i \u2208 [n], if [ A_i ] \u2265 1-\u03b4_i. Then taking a union bound over all the n events, we have [ A_1 \u00a0and\u00a0 A_2 \u22ef A_n] \u2265 1- \u2211_i=1^n \u03b4_i.\n\n\n\n\n\n \u00a7.\u00a7 Algebra Facts\n\n\nWe state some standard facts and omit their proofs, since they're very standard.\n\n\nWe have\n\n\n    \n  * For any orthonormal basis U \u2208^n \u00d7 k, we have U x _2 =  x _2.\n    \n  * For any orthonornal basis U \u2208^n \u00d7 k, we have U _F \u2264\u221a(k).\n    \n  * For any diagonal matrix \u03a3\u2208^k \u00d7 k and any vector x \u2208^k, we have \u03a3 x _2 \u2265\u03c3_min(\u03a3)  x _2. \n    \n  * For symmetric matrix A, we have \u03c3_min(A) = min_z :  z _2=1 z^\u22a4 A z.\n    \n  * For symmetric matrix A, we have \u03c3_min(A)  z_2^2 \u2264 z^\u22a4 A z for all vectors z.\n    \n  * For symmetric matrix A, we have \u03c3_max(A)  z_2^2 \u2265 z^\u22a4 A z for all vectors z.\n    \n  * For any matrix A, we have A\u2264 A _F.\n    \n  * For any square matrix A \u2208^k \u00d7 k and vector x \u2208^k, we have x^\u22a4 A x = \u2211_i=1^k \u2211_j=1^k x_i A_i,j x_j = \u2211_i=1^k x_i A_i,i x_i + \u2211_i\u2260 j x_i A_i,j x_j.\n    \n  * For any square and invertible matrix R, we have R^-1 = \u03c3_min(R)^-1\n    \n  * For any matrix A and for any unit vector x, we have A \u2265 A x _2.\n    \n  * For any matrix A, A A^\u22a4 =  A^\u22a4 A.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Restricted Isometry Property\n\n\n\n\n    A linear operator \ud835\udc9c: ^d\u00d7 d\u2192^m satisfies RIP iff, for \u2200 W \u2208^d \u00d7 d  \n    s.t. (W)\u2264 k, the following holds:\n    \n    (1-\u03f5_k) \u00b7W_F^2\u2264 A(W)_F^2\u2264(1+\u03f5_k) \u00b7W_F^2\n\n    where \u03f5_k > 0 is a constant dependent only on k.\n\n\n\n\n \u00a7.\u00a7 Rank-one Estimation\n\n\nThe goal of matrix sensing is to design a linear operator \ud835\udc9c:^d \u00d7 d\u2192^m and a recovery algorithm so that a low-rank matrix W_*\u2208^d \u00d7 d can be recovered exactly using \ud835\udc9c(W_*). \n\n\nGiven a ground-truth matrix W_* \u2208^d \u00d7 d. Let (x_1, y_1) , \u22ef,  (x_m, y_m) \u2208^d\u00d7^d denote m pair of feature vectors. Let b \u2208^m be defined\n\n    b_i = x_i^\u22a4 W_* y_i, \u00a0\u00a0\u00a0\u2200 i \u2208 [m].\n\nThe goal is to use b \u2208^m and { (x_i,y_i)}_i \u2208 [m]\u2282^d \u00d7^d to recover W_* \u2208^d \u00d7 d.\n\n\n\n\nWe propose two different kinds of rank-one measurement operators based on Gaussian distribution.\n\n\n\n \u00a7.\u00a7 Rank-one Independent Gaussian Operator\n\n\n\nWe formally define Gaussian independent operator, here.\n\nLet (x_1, y_1) , \u22ef, (x_m, y_m) \u2282^d \u00d7^d denote i.i.d. samples from  Gaussian distribution.\n\nFor each i \u2208 [m], we define A_i \u2208^d \u00d7 d as follows\n\n    A_i := x_i y_i^\u22a4 .\n \n\nWe define A_GI\u2208^d \u00d7 m d as follows: \n\n    \ud835\udc9c_GI := [ A_1 A_2   \u22ef A_m ] .\n\nHere GI denotes Gaussian Independent. \n \n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Matrix Angle and Distance\n\n\n\nWe list several basic definitions and tools in literature, e.g., see <cit.>.\n\nLet X, Y \u2208^n \u00d7 k denote two matrices.\n\nFor any matrix X, and for orthonormal matrix Y (Y^\u22a4 Y = I_k) we define\n\n    \n  * tan\u03b8(Y,X) :=  Y_^\u22a4 X ( Y^\u22a4 X )^-1\n\nFor orthonormal matrices Y and X (Y^\u22a4 Y = I_k and X^\u22a4 X = I_k), we define\n\n    \n  * cos\u03b8 (Y,X) := \u03c3_min (Y^\u22a4 X). \n     \n        \n  * It is obvious that cos (Y,X) = 1/  (Y^\u22a4 X)^-1 and cos(Y,X) \u2264 1.\n    \n    \n  * sin\u03b8(Y,X) :=  (I - Y Y^\u22a4) X.\n     \n        \n  * It is obvious that sin\u03b8(Y,X) =  Y_ Y_^\u22a4 X  =  Y_^\u22a4 X and sin\u03b8(Y,X) \u2264 1.\n        \n  * From Lemma\u00a0<ref>, we know that sin^2\u03b8(Y,X) + cos^2\u03b8(Y,X) = 1. \n    \n    \n  * (Y,X) := sin\u03b8(Y,X)\n  \n\n\n\n\n\n\n\nLet X, Y\u2208^n\u00d7 k be orthogonal matrices, then \n\n    tan\u03b8(Y,X) = sin\u03b8(Y,X)/cos\u03b8(Y,X).\n\n\n\n\n\n\n\nLet X, Y\u2208^n\u00d7 k be orthogonal matrices, then \n\n    sin^2\u03b8(Y, X) + cos^2\u03b8(Y,X) =1.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Matrix Concentration\n\n\n\n\n  Given a finite sequence { X_1, \u22ef X_m }\u2282^n_1 \u00d7 n_2 of independent, random matrices all with the dimension of n_1 \u00d7 n_2.\n\n    Let Z = \u2211_i=1^m X_i.\n\n  Assume that\n  \n    [X_i] = 0, \u2200 i \u2208 [m],  X_i \u2264 M, \u2200 i \u2208 [m]\n\n\nLet [Z] be the matrix variances statistic of sum\n\n    [Z] = max{\u2211_i=1^m [X_iX_i^\u22a4]  , \u2211_i=1^m [X_i^\u22a4 X_i] }\n\nThen it holds that\n\n    [ Z ] \u2264 (2 [Z] \u00b7log(n_1+n_2))^1/2 + M log(n_1 + n_3) /3\n\nFurther, for all t>0\n\n    [  Z \u2265 t ] \u2264 (n_1 + n_2) \u00b7exp( -t^2/2/[Z] + M t/3 )\n\n\n\n\n\n\n\n\u00a7 ANALYSIS\n\n\nHere in this section, we provide analysis for our proposed algorithm. In Section\u00a0<ref>, we provide definitions in our algorithm analysis. In Section\u00a0<ref> we define the operators to be used. In Section\u00a0<ref> we provide our main theorem together with its proof. In Section\u00a0<ref> we introduce our main induction hypothesis. \n\n\n\n \u00a7.\u00a7 Definitions\n\n\n\n\n\n\nWe define W_* \u2208^d \u00d7 d as follows\n\n    W_* = U_* \u03a3_* V_*^\u22a4\n\nwhere U_* \u2208^n \u00d7 k are orthonormal columns,  \nand V_* \u2208^n \u00d7 k are orthonormal columns.\nLet \u03c3_1^*, \u03c3_2^*, \u22ef\u03c3_k^* denote the diagonal entries of diagonal  matrix \u03a3_* \u2208^d \u00d7 d.\n\n\n\nLet W_* be defined as Definition\u00a0<ref>. We define \u03ba to the condition number of W_*, i.e.,\n\n    \u03ba : = \u03c3_1/\u03c3_k.\n\nIt is obvious that \u03ba\u2265 1.\n\n\n\nFor each i \u2208 [m], let x_i,y_i denote samples from N(0,I_d).\n\nFor each i \u2208 [m], we define\n\n    A_i = x_i y_i^\u22a4\n\nand\n\n    b_i = x_i^\u22a4 W_* y_i.\n\n\n\n\n\n \u00a7.\u00a7 Operators\n\n\n\n\nFor each i \u2208 [m], let A_i and b_i be defined as Definition\u00a0<ref>. \n\n We define W_0 := 1/m\u2211_i=1^m b_i A_i.\n\nWe say initialization matrix W_0 \u2208^d \u00d7 d is an \u03f5-good operator if \n\n    W_0 - W_* \u2264 W_* \u00b7\u03f5.\n\n\n\n\n\n \nFor any vectors u,v, we define  \n\n    \n  * B_x:=1/m\u2211_l=1^m(y_l^\u22a4 v)^2x_lx_l^\u22a4\n    \n  * B_y:=1/m\u2211_l=1^m(x_l^\u22a4 u)^2 y_ly_l^\u22a4\n  \n We say B = (B_x,B_y) is \u03f5-operator if the following holds: \n\n \n \n  * B_x-I\u2264\u03f5 \n \n  * B_y-I\u2264\u03f5\n \n\n\n\nFor any vectors u,v \u2208^d. \nWe define  \n\n    \n  * G_x:=1/m\u2211_l=1^m(y_l^\u22a4 v)(y_l^\u22a4 v_)x_lx_l^\u22a4 \n    \n  * G_y:=1/m\u2211_l=1^m(x_l^\u22a4 u)(x_l^\u22a4 u_ ) y_ly_l^\u22a4\n\n u,u_\u2208^d,v,v_\u2208^d are unit vectors, s.t., u^\u22a4 u_=0 and v^\u22a4 v_=0. \n We say G = (G_x,G_y) is \u03f5-operator if the following holds\n\n \n \n  * G_x\u2264\u03f5,\n \n  * G_y\u2264\u03f5.\n \n\n\n\n\n \u00a7.\u00a7 Main Result\n\n\n\nWe prove our main convergence result as follows:  \n\nLet W_* \u2208^d \u00d7 d be defined as Definition\u00a0<ref>. \n\n\n\nAlso, let \ud835\udc9c:^d \u00d7 d\u2192^m be a linear measurement operator parameterized by m matrices, i.e., \ud835\udc9c={A_1,A_2,\u22ef,A_m} where A_l=x_l y_l^\u22a4. Let \ud835\udc9c(W) be as given by\n\n    b=\ud835\udc9c(W)=\n        [ [ A_1^\u22a4 W] [ A_2^\u22a4 W]          \u22ef  [A_m^\u22a4 W] ]^\u22a4\n\n\n\nIf the following conditions hold \n\n    \n  * \u03f5= 0.001 / (k^1.5\u03ba ) \n    \n  * T =  100log( \u03ba k / \u03f5_0)\n    \n  * Let {(b_i,A_i)}_i\u2208 [m] be an \u03f5-init operator (Definition\u00a0<ref>).\n    \n  * Let B be an \u03f5-operator (Definition\u00a0<ref>).  \n    \n  * Let G be an \u03f5-operator(Definition\u00a0<ref>).\n\nThen, after T-iterations of the alternating minimization method (Algorithm <ref>), we obtain W_T=U_T V_T^\u22a4 s.t., \n\n    W_T-W_*\u2264\u03f5_0.\n\n\n\n \n\n \n\nWe first present the update equation for V\u0302_t+1\u2208^d \u00d7 k. \n\n\n\n \n\nAlso, note that using the initialization property (first property mentioned in Theorem\u00a0<ref>), we get, \n\n    W_0 -W_*\u2264\u03f5\u03c3_1^* \u2264\u03c3_k^*/100 .\n\n\nNow, using the standard sin theta theorem for singular vector perturbation <cit.>, we get: \n\n    (U_0,U_*) \u2264   \u00a01/100\n    (V_0,V_*) \u2264   \u00a01/100\n\n\nAfter T iteration (via Lemma\u00a0<ref>), we obtain\n\n    (U_T,U_*) \u2264   \u00a0 (1/4)^T \n    (V_T,V_*) \u2264   \u00a0 (1/4)^T\n\nwhich implies that\n\n    W_T - W_* \u2264\u03f5_0\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Main Induction Hypothesis\n\n\n\n\n    We define \u03f5_d: = 1/10.\n    We assume that \u03f5= 0.001 / (k^1.5\u03ba ).\n    For all t \u2208 [T], we have the following results.\n    \n    \n        \n  * Part 1. If (U_t,U_*) \u22641/4(V_t, V_*) \u2264\u03f5_d, then we have\n        \n            \n  * (V_t+1, V_*) \u22641/4(U_t,U_*) \u2264\u03f5_d\n        \n        \n  * Part 2. If (V_t+1, V_*) \u22641/4(U_t,U_*) \u2264\u03f5_d, then we have\n        \n            \n  * (U_t+1,U_*) \u22641/4(V_t+1, V_*) \u2264\u03f5_d\n        \n    \n\n\n\nProof of Part 1.\n\nRecall that for each i \u2208 [n], we have\n\n    b_i = x_i^\u22a4 W_* y_i = \u27e8 x_i y_i^\u22a4 , W_* \u27e9  = \u27e8 A_i, W_* \u27e9 = [A_i^\u22a4 W_*].\n\n\n \n\nRecall that  \n\n    V\u0302_t+1=   \u00a0min_V\u2208^d\u00d7 k\u2211_i=1^m(b_i-x_i^\u22a4 U_t V^\u22a4 y_i)^2\n    \n    =   \u00a0min_V\u2208^d\u00d7 k\u2211_i=1^m(x_i^\u22a4 W_* y_i-x_i^\u22a4 U_t V^\u22a4 y_i)^2\n\n\n \n\nHence, by setting gradient of this objective function to zero. Let F \u2208^d \u00d7 k be defined as Definition\u00a0<ref>.\n\nWe have V_t+1\u2208^d \u00d7 k can be written as follows: \n\n    V\u0302_t+1 = W_*^\u22a4 U_t - F\n\nwhere F \u2208^d \u00d7 k is the error matrix\n\n    F = [ F_1 F_2   \u22ef F_k ]\n\nwhere F_i \u2208^d for each i \u2208 [k].  \n \n\n\n\nThen, using the definitions of F \u2208^d \u00d7 k and Definition\u00a0<ref>,  \nwe get:\n\n\n    [\n        [ F_1;   \u22ee; F_k ]]\n        =B^-1(BD-C)S\u00b7(V_*)\n\nwhere (V_*) \u2208^dk is the vectorization of matrix V_* \u2208^d \u00d7 k.\n \n \n\nNow, recall that in the t+1-th iteration of Algorithm\u00a0<ref>, V_t+1\u2208^d \u00d7 k is obtained by QR decomposition of V\u0302_t+1\u2208^d \u00d7 k. Using notation mentioned above,\n\n    V\u0302_t+1=V_t+1R\n\n\nwhere R \u2208^k \u00d7 k denotes the lower triangular matrix R_t+1\u2208^k \u00d7 k obtained by the QR decomposition of V_t+1\u2208^d \u00d7 k.\n\n \n\nWe can rewrite V_t+1\u2208^d \u00d7 k as follows\n\n    V_t+1 =    \u00a0V\u0302_t+1 R^-1\n    \n          =    \u00a0 (W_*^\u22a4 U_t-F)R^-1\n\nwhere the first step follows from  Eq.\u00a0(<ref>) , and the last step follows from Eq.\u00a0(<ref>).\n\nMultiplying both the sides by V_*,\u2208^d \u00d7 (d-k), where V_*,\u2208^d \u00d7 (d-k) is a fixed orthonormal basis of the subspace orthogonal to span(V_*), using Claim\u00a0<ref>\n\n    (V_*, )^\u22a4 V_t+1 = -(V_*, )^\u22a4 FR^-1\n\n\nThus, we get:\n\n    (V_t+1, V_*) =   \u00a0(V_*, )^\u22a4 V_t+1\n    \n        =    \u00a0 (V_*, )^\u22a4 F R^-1\n        =    \u00a0 F R^-1\n    \u2264   \u00a0F\u00b7R^-1\n    \u2264   \u00a0 0.001 \u03c3_k^* (U_t, U_*) \u00b7 R^-1\n    \u2264   \u00a0 0.001 \u03c3_k^* (U_t, U_*) \u00b7 2 (\u03c3_k^*)^-1\n    \u2264   \u00a0 0.01 \u00b7(U_t,U_*)\n\n\nwhere the first step follows from definition of  (see Definition\u00a0<ref>), the second step follows from Eq.\u00a0(<ref>), the third step follows from V_*, is an orthonormal basis, \n and the forth step follows from  Fact\u00a0<ref>, the fifth step follows from Lemma.\u00a0<ref>, the sixth step follows from Lemma\u00a0<ref> (In order to run this lemma, we need to the condition of Part 1 statement to be holding), the last step follows from simple algebra. \n\n\nProof of Part 2.\n\nSimilarly, we can prove this as Part 1.\n\n\n\n \n\n\n\n\u00a7 MEASUREMENTS ARE GOOD OPERATOR\n\n\nIn this section, we provide detailed analysis for our operators. First Section\u00a0<ref> we introduce some standard results for truncated Gaussian. In Section\u00a0<ref> and Section\u00a0<ref> we bound the term Z_i and [Z_iZ_i^\u22a4] respectively. In Section\u00a0<ref> we state our main lemma. In Section\u00a0<ref> we show that out initialization is good. In Section\u00a0<ref> we show our two operators are good. \n \n\n\n\n \u00a7.\u00a7 Tools for Gaussian\n\n\n\nWe state a standard tool from literature,\n\n    Let X \u223c\ud835\udcb3_k^2 be a chi-squared distributed random variable with k degrees of freedom. Each one has zero means and \u03c3^2 variance. \n    \n    Then it holds that\n    \n    [X - k\u03c3^2 \u2265 (2\u221a(kt) + 2t) \u03c3^2]\n            \u2264   \u00a0exp(-t)\n    [k\u03c3^2 - X \u2265 2\u221a(kt)\u03c3^2]\n            \u2264   \u00a0exp(-t)\n\n    Further if k \u2265\u03a9(\u03f5^-2 t) and t \u2265\u03a9(log(1/\u03b4)), then we have\n    \n    [ | X - k \u03c3^2 | \u2264\u03f5 k \u03c3^2 ] \u2264\u03b4.\n\n\n\nWe state a standard fact for the 4-th moment of Gaussian distribution.\n\nLet x \u223c N(0,\u03c3^2), then it holds that _x \u223c N(0,\u03c3^2)[x^4] = 3 \u03c3^2.\n\n\n\nLet x \u223c N(0, \u03c3^2 I_d) denote a random Gaussian vector. Then we have\n\n\n  * Part 1 \n\n    [x x^\u22a4 x x^\u22a4] = (d+2) \u03c3^4\n\n\n  * Part 2\n\n    [ x x^\u22a4 x x^\u22a4 ] = (d+2) \u03c3^4\n\n\n\n\nWe define A:=xx^\u22a4 xx^\u22a4. Then we have\n\n    A_i,j = x_i \u2211_l=1^d x_l x_l x_j\n\nFor i=j, we have\n\n    [A_i,i] =    \u00a0[ x_i \u2211_l=1^d x_l x_l x_i ] \n    \n    =    \u00a0[x_i(\u2211_l=1^i-1x_l x_l + x_i x_i + \u2211_l=i+1^d x_l x_l) x_i] \n    \n    =    \u00a0[x_i^4] + \u2211_l \u2208 [d] \\ i[x_l^2 x_i^2] \n    \n    =    \u00a0[x_i^4] + \u2211_l \u2208 [d] \\ i[x_l^2] [x_i^2] \n    \n    =    \u00a0[x_i^4] + (d-1) \u03c3^4 \n    \n    =    \u00a0 3 \u03c3^4 + (d-1) \u03c3^4 \n    \n    =    \u00a0 (d + 2) \u03c3^4\n\n\nwhere the third step follows from linearity of expectation (Fact\u00a0<ref>), the forth step follows from x_l and x_i are independent, the fifth step follows _z \u223c N(0,\u03c3^2)[z^4] =3 \u03c3^4.\n \n\nFor i\u2260 j, we have\n\n    [A_i,j] =    \u00a0[ x_i \u2211_l=1^d x_l x_l x_j ] \n    \n    =    \u00a0[x_i x_j^3] + [x_i^3 x_j] + \u2211_l \u2208 [d] \\ i,j[x_i x_l^2 x_j] \n    \n    =    \u00a0 0\n\nwhere the second step follows from linearity of expectation (Fact\u00a0<ref>).\n\n\n\n[Rotation invariance property of Gaussian]\n\n    Let A^\u22a4\u2208^d \u00d7 k with k < d denote an orthonormal basis (i.e., AA^\u22a4 = I_k). Then for a Gaussian x \u223c(0, \u03c3^2 I_d), we have\n    \n    Ax \u223c(0, \u03c3^2 I_k).\n\n\n\n\n    Let y := Ax \u2208^k, then\n    \n    y_i = \u2211_j = 1^dA_ijx_j, \u00a0\u00a0\u2200 i \u2208 [k].\n\n    By definition of Gaussian distribution\n \n    \n    y_i \u223c(0, \u03c3^2\u2211_j = 1^dA_ij^2).\n\n    Recall that A^\u22a4 is an orthonormal basis.\n\n    \n    We have\n    \n    A_ij^2 = 1.\n\n    Thus we have\n    \n    y \u223c(0, \u03c3^2 I_k),\n\n\n\n\n\n \u00a7.\u00a7 Bounding \n\n\n\n\nLet x_i denote a random Gaussian vector samples from N(0, \u03c3^2 I_d). Let y_i denote a random Gaussian vector samples from N(0, \u03c3^2 I_d).\n\nLet U_*, V_* \u2208^d \u00d7 k.\n\nWe define\n\n    Z_i := x_i x_i^\u22a4 U_* \u03a3_* V_*^\u22a4 y_i y_i^\u22a4, \u00a0\u00a0\u00a0\u2200 i \u2208 [m]\n\n\n\n  * Part 1. We have\n\n    [  Z_i \u2264 C^2 k^2 log^2(d/\u03b4) \u03c3^4 \u00b7\u03c3_1^* ] \u2265 1-\u03b4/(d).\n\n\n  * Part 2. If k \u2265\u03a9(log(d/\u03b4))  We have\n\n    [  Z_i \u2264 C^2 k^2 \u03c3^4 \u00b7\u03c3_1^* ] \u2265 1-\u03b4/(d).\n\n\n\n\n\nProof of Part 1.\n\nWe define \n\n    a_i := U_*^\u22a4 x_i \u2208^k \n    \n    b_i := V_*^\u22a4 y_i \u2208^k\n\nSince U_* and V_* are orthornormal basis, due to rotation invariance property of Gaussian (Fact\u00a0<ref>) \n, we know that a_i \u223c N(0,\u03c3^2 I_k) and b_i \u223c N(0, \u03c3^2 I_k).\n\nWe also know that \n\n\n    x_i = (U_*^\u22a4)^\u2020 a_i = U_* a_i \n    \n    y_i = (V_*^\u22a4)^\u2020 b_i = V_* b_i\n\n\nThus, by replacing x_i,y_i with a_i,b_i, we have\n\n    Z_i  \n    =    \u00a0 x_i x_i^\u22a4 U_* \u03a3_* V_*^\u22a4 y_i y_i^\u22a4\n    \n    =    \u00a0 U_* a_i a_i^\u22a4 U_*^\u22a4 U_* \u03a3_* V_*^\u22a4 V_* b_i b_i^\u22a4 V_*^\u22a4\n    \n    =    \u00a0 U_* a_i a_i^\u22a4\u03a3_* b_i b_i^\u22a4 V_*^\u22a4\n    \u2264   \u00a0 U_* \u00b7 a_i a_i^\u22a4\u00b7\u03a3_* \u00b7 b_i b_i^\u22a4\u00b7 V_*^\u22a4\n    \u2264   \u00a0\u03c3_1^* \u00b7 a_i _2^2 \u00b7 b_i _2^2\n\nwhere the second step follows from replacing x,y by a,b, the third step follows from U_*^\u22a4 U_* = I and V_*^\u22a4 V_* = I, the forth step follows from Fact\u00a0<ref>. \n\n \n\nDue to property of Gaussian, we know that\n\n    [ |a_i,j| > \u221a(Clog(d/\u03b4))\u03c3 ] \u2264\u03b4/(d)\n\n\nTaking a union bound over k coordinates, we know that\n\n    [  a_i _2^2 \u2264 C k log(d/\u03b4) \u03c3^2 ] \u2265 1-\u03b4 /(d)\n\nSimilarly, we can prove it for b_i _2^2.\n\n\nProof of Part 2.\nSince k \u2265\u03a9(log(d/\u03b4)), then we can use Lemma\u00a0<ref> to obtain a better bound.\n \n\n\n\n\n\n\n\n \u00a7.\u00a7 Bounding \n\n\nWe can show that\n\n    [ Z_i Z_i^\u22a4] \u2264 C^2 k^2 \u03c3^4 (\u03c3_1^*)^2.\n\n\n\n\n\n\n\n \n\nUsing Lemma\u00a0<ref>\n\n\n    _a \u223c N(0, \u03c3^2 I_k )[ a_i a_i^\u22a4 a_i a_i^\u22a4 ] \u2264 C k \u03c3^2.\n\nThus, we have\n\n    [ a_i a_i^\u22a4 a_i a_i^\u22a4] \u227c Ck \u03c3^2 \u00b7 I_k\n\n\nThen, we have\n\n    [Z_i Z_i^\u22a4] \n             =    \u00a0_x,y[ x_i x_i^\u22a4 U_* \u03a3_* V_*^\u22a4 y_i y_i^\u22a4 y_i y_i^\u22a4 V_* \u03a3_* U_*^\u22a4 x_i x_i^\u22a4 ] \n    \n             =    \u00a0_a,b[ U_* a_i a_i^\u22a4 U_*^\u22a4 U_* \u03a3_* V_*^\u22a4 V_* b_i b_i^\u22a4 V_*^\u22a4 V_* b_i b_i^\u22a4 V_*^\u22a4 V_* \u03a3_* U_*^\u22a4 U_* a_i a_i^\u22a4 U_*^\u22a4 ]\n    \n             =    \u00a0_a,b[ U_* a_i a_i^\u22a4\u03a3_*  b_i b_i^\u22a4  V_*^\u22a4 V_* b_i b_i^\u22a4\u03a3_*  a_i a_i^\u22a4 U_*^\u22a4 ] \n     \n             =    \u00a0_a,b[ U_* a_i a_i^\u22a4\u03a3_*  b_i b_i^\u22a4 b_i b_i^\u22a4\u03a3_*  a_i a_i^\u22a4 U_*^\u22a4 ] \n    \u2264   \u00a0_a,b[ a_i a_i^\u22a4\u03a3_*  b_i b_i^\u22a4 b_i b_i^\u22a4\u03a3_*  a_i a_i^\u22a4  ]  \n    \u2264   \u00a0_a[ a_i a_i^\u22a4\u03a3_* _b[ b_i b_i^\u22a4 b_i b_i^\u22a4 ] \u03a3_*  a_i a_i^\u22a4  ]  \n    \u2264   \u00a0 C^2 k^2 \u03c3^4 (\u03c3_1^*)^2\n\n     where the first step follows from the definition of Z_i, the second step follows from replacing x_i,y_i with a_i,b_i, the third step follows from U_*,V_* are orthonormal columns, the fourth step follows from V_* are orthonormal columns, the fifth step follows from\n    U_* \u2264 1\n    , the sixth step follows from\n    using Lemma\u00a0<ref> twice.\n\n\n\n\n\n \u00a7.\u00a7 Main Results\n\n\n\nWe prove our main result for measurements.  \n\n\n\n     Let {A_i,b_i}_i\u2208 [m] denote measurements be defined as Definition\u00a0<ref>.\n\n     Assuming the following conditions are holding\n     \n        \n  * k = \u03a9(log(d/\u03b4))\n        \n  * m = \u03a9(\u03f5^-2 (d+k^2) log(d/\u03b4))\n      \n     \n     Then, \n     \n        \n  * The property in Definition\u00a0<ref>, initialization is a \u03f5-operator\n        \n  * The property in Definition\u00a0<ref>, B are \u03f5-operator.\n        \n  * The property in Definition\u00a0<ref>, G are \u03f5-operator.\n     \n     holds with probability at least 1-\u03b4/(d).\n\n\n\nUsing Lemma\u00a0<ref> and Lemma\u00a0<ref>, we complete the proof.\n\n\n\n\n\n \u00a7.\u00a7 Initialization Is a Good Operator\n\n\n\nWe define matrix S \u2208^d \u00d7 d as follows\n\n    S: = 1/m\u2211_i=1^m b_i A_i.\n\n\nIf the following two condition holds\n \n\n    \n  * Condition 1. k = \u03a9(log(d/\u03b4)),\n    \n  * Condition 2. m = \u03a9( \u03f5^-2 k^2 log(d/\u03b4) ).\n\n\nThen we have\n\n    [  S  - W_* \u2264\u03f5\u00b7 W_*  ] \u2265 1-\u03b4.\n\n\n\n\n\n\n\n    \n \n    \n    (Initialization in Definition\u00a0<ref>) Now, we have: \n\n    \n    S =    \u00a01/m\u2211_i=1^m b_i A_i \n    \n              =    \u00a01/m\u2211_i=1^m b_i x_i y_i^\u22a4\n    \n              =    \u00a01/m\u2211_i=1^m  x_i b_i y_i^\u22a4\n    \n              =    \u00a01/m\u2211_i=1^m x_i x_i^\u22a4 W_* y_i  y_i^\u22a4\n     \n              =    \u00a01/m\u2211_i=1^m x_i x_i^\u22a4 U_* \u03a3_* V_*^\u22a4 y_i y_i^\u22a4,\n\n     where the first step follows from Definition\u00a0<ref>, the second step follows from A_i = x_i y_i^\u22a4, the third step follows from b_i is a scalar, the forth step follows from b_i = x_i^\u22a4 W_* y_i, the fifth step follows from W_* = U_* \u03a3_* V_*^\u22a4. \n\nFor each i \u2208 [m], we define matrix Z_i \u2208^d \u00d7 d as follows:\n\n    Z_i := x_i x_i^\u22a4 U_* \u03a3_* V_*^\u22a4 y_i y_i^\u22a4,\n\nthen we can rewrite S \u2208^d \u00d7 d in the following sense,\n     \n    S = 1/m\u2211_i=1^m Z_i\n\n     \n     Note that, we can compute [Z_i] \u2208^d \u00d7 d\n     \n    _x_i,y_i[Z_i]\n            =    \u00a0_x_i, y_i[  x_ix_i^\u22a4_d \u00d7 d U_* \u03a3_* V_*^\u22a4_ d \u00d7 d y_i y_i^\u22a4_d \u00d7 d ] \n    \n            =    \u00a0_x_i[  x_ix_i^\u22a4_d \u00d7 d U_* \u03a3_* V_*^\u22a4_ d \u00d7 d ]  \u00b7_y_i [ y_i y_i^\u22a4_d \u00d7 d ] \n    \n            =    \u00a0_x_i [x_i x_i^\u22a4 ] \u00b7 U_* \u03a3_* V_*^\u22a4\u00b7_y_i[ y_i y_i^\u22a4] \n     \n            =    \u00a0 U_* \u03a3_* V_*^\u22a4\n\n    where the first step follows definition of Z_i, the second step follows from x_i and y_i are independent and Fact\u00a0<ref>, the third step follows from Fact\u00a0<ref> the forth step follows from [x_ix_i^\u22a4] = I_d and [y_i y_i^\u22a4] = I_d.\n\n \n\n     As S \u2208^d \u00d7 d is a sum of m random matrices, the goal is to apply Theorem\u00a0<ref>\n    to show that S is close to\n    \n    [S] =    \u00a0  W_* \n    \n        =    \u00a0 U_* \u03a3_* V_*^\u22a4\n\n    for large enough m. \n    \nUsing Lemma\u00a0<ref> (Part 2) with choosing Gaussian variance \u03c3^2=1, we have\n \n\n    [  Z_i \u2264 C^2 k^2  \u03c3_1^*, \u2200 i \u2208 [m] ] \u2265 1-\u03b4/(d)\n\n\n\n\n    \n     Using Lemma\u00a0<ref> with choosing Gaussian variance \u03c3^2= 1, we can bound [Z_i Z_i^\u22a4] as follows  \n     \n     \n    [Z_i Z_i^\u22a4] \u2264   \u00a0 C^2 k^2 (\u03c3_1^*)^2\n\n\n\n    \n Let Z = \u2211_i=1^m (Z_i - W_*).\n \n Applying Theorem\u00a0<ref> we get \n     \n    [  Z \u2265 t ] \u2264 2d \u00b7exp( -t^2/2/[Z] + M t/3 )\n\n     where\n\n \n    Z =    \u00a0 m S - m W_* \n    [Z] =    \u00a0 m \u00b7 C^2 k^2  (\u03c3_1^*)^2,    \u00a0by\u00a0Eq.\u00a0(<ref>)\n    \n     M=    \u00a0 C^2 k^2 \u03c3_1^*    \u00a0by\u00a0Eq.\u00a0(<ref>)\n\n\nReplacing t= \u03f5\u03c3_1^* m and Z = mS - mW_* inside [] in Eq.\u00a0(<ref>), we have \n\n\n    [  S - W^* \u2265   \u00a0\u03f5\u03c3_1^* ] \u2264 2d \u00b7exp( -t^2 /2/[Z] + M t /3)\n\nOur goal is to choose m sufficiently large such that the above quantity is upper bounded by 2d \u00b7exp ( - \u03a9( log(d/\u03b4) )).\n\nFirst, we need \n\n\n    t^2/[Z]\n    =    \u00a0\u03f5^2 m^2 (\u03c3_1^*)^2 / m \u00b7 C^2 k^2  (\u03c3_1^*)^2  \n    \n    =    \u00a0\u03f5^2 m / C^2 k^2  \n    \u2265   \u00a0log(d/\u03b4)\n\nwhere the first step follows from choice of t and bound for [Z].\n\nThis requires\n\n    m \u2265 C^2 \u03f5^-2 k^2 log(d/\u03b4)\n\n\nSecond, we need \n\n    t^2 /  M t   =    \u00a0\u03f5 m \u03c3_1^* / M \n    \n    =    \u00a0\u03f5 m \u03c3_1^* /C^2 k^2   \u03c3_1^*\n    \n    =    \u00a0\u03f5 m /C^2 k^2  \n    \u2265   \u00a0log(d/\u03b4)\n\nwhere the first step follows from choice of t and the second step follows from bound on M.\n\nThis requires\n\n    m \u2265 C^2 \u03f5^-2 k^2 log(d/\u03b4)\n\n    \n  Finally, we should choose\n     \n    m \u2265 10C^2 \u03f5^-2 k^2 log(d/\u03b4) ,\n\n\n    Which implies that \n     \n    [  S - W_* \u2264\u03f5\u00b7\u03c3_1^* ] \u2265 1- \u03b4/(d).\n\n     \n   Taking the union bound with all Z_i are upper bounded, then we complete the proof.\n\n\n \n\n\n\n \u00a7.\u00a7 Operator  and  is good\n\n\n\n\nIf the following two conditions hold\n\n    \n  * Condition 1. d = \u03a9(log(d/\u03b4))\n    \n  * Condition 2. m = \u03a9(\u03f5^-2 d log(d/\u03b4))\n\nThen operator B (see Definition\u00a0<ref>) is \u03f5 good, i.e.,\n\n    [  B_x - I_d\u2264\u03f5 ] \u2265   \u00a0 1-\u03b4/(d)  \n    [  B_y - I_d \u2264\u03f5 ] \u2265   \u00a0 1-\u03b4/(d)\n\nSimilar results hold for operator G (see Definition\u00a0<ref>).\n\n\n\n\n\n\n \n\n \n     \n     Recall that\n     B_x:=1/m\u2211_l=1^m(y_l^\u22a4 v)^2x_lx_l^\u22a4.\n\n     Recall that B_y:=1/m\u2211_l=1^m(x_l^\u22a4 u)^2 y_ly_l^\u22a4.\n     \n     Now, as x_i,y_i are rotationally invariant random variables \n     , wlog, we can assume u=e_1.\n     \n     We use x_i,1\u2208 to denote the first entry of x_i \u2208^d. \n\n     \n     Thus,  \n     \n    (x_i^\u22a4 u u^\u22a4 x_i)=x_i,1^2\n\n     Then \n     \n    [ (x_i^\u22a4 u u^\u22a4 x_i)^2 ] = [x_i,1^4 ] = 3\n\n\n    We define\n    \n    Z_i = (x_i^\u22a4 u)^2 y_i y_i^\u22a4\n\n    then\n    \n    [Z_i] = I_d\n    \n    \n   Using similar idea in Lemma\u00a0<ref>, we have\n    \n    [  Z_i \u2264 C d , \u2200 i \u2208 [m] ] \u2265 1- \u03b4/(d)\n\n    \n    We can bound\n    \n    [ Z_i Z_i^\u22a4 ] \n        =    \u00a0_x,y[ (x_i^\u22a4 u)^2 y_i y_i^\u22a4  y_i y_i^\u22a4  (x_i^\u22a4 u)^2 ] \n    \n        =    \u00a0_x[ (x_i^\u22a4 u)^2 _y[y_i y_i^\u22a4  y_i y_i^\u22a4 ]  (x_i^\u22a4 u)^2 ] \n    \n        =    \u00a0 (d+2) \u00b7 | _x[ (x_i^\u22a4 u)^2  (x_i^\u22a4 u)^2 ] | \n    \n        =    \u00a0 (d+2) \u00b7 3 \n    \u2264   \u00a0 C d\n\nwhere the fourth step follows from C \u2265 1 is a sufficiently large constant.\n\n\nLet Z = \u2211_i=1^m (Z_i - I_d).\n\n    Applying Theorem\u00a0<ref> we get\n    \n    [ Z\u2265 t] \u2264 2d \u00b7exp(-t^2/2/[Z] + Mt/3),\n\nwhere\n\n    Z =    \u00a0 m \u00b7 B - m \u00b7 I \n    [Z] =    \u00a0 C m d \n    \n        M =    \u00a0 C d\n\n\nUsing t = m \u03f5 and Z = \u2211_i=1^m (Z_i - I_d), and B = 1/m\u2211_i=1^m Z_i, we have\n\n    [  Z \u2265 t] \n    =    \u00a0[ \u2211_i=1^m (Z_i - I_d) \u2265 m \u03f5 ] \n    \n    =    \u00a0[ 1/m\u2211_i=1^m Z_i - I_d \u2265\u03f5 ] \n    \n    =    \u00a0[  B - I_d \u2265\u03f5 ]\n\n    \nBy choosing t = m \u03f5 and m = \u03a9(\u03f5^-2 d log(d/\u03b4)) we have\n\n    \n    [  B - I_d \u2265\u03f5 ] \u2264\u03b4/(d).\n\nwhere B can be either B_x or B_y.\n\n\n    Similarly, we can prove \n    \n    [G_x\u2264\u03f5] \u2265 1 - \u03b4, \n    [G_y\u2264\u03f5] \u2265 1 - \u03b4.\n\n\n\n\n\n\n\n\n\n\u00a7 ONE SHRINKING STEP\n\n\nIn this section, we provide a shirking step for our result. In Section\u00a0<ref> we define the matrices B, C, D ,S to be used in analysis. In Section\u00a0<ref> we upper bound the norm of BD- C. In Section\u00a0<ref> we show the update term V_t + 1 can be written in a different way. In Section\u00a0<ref> and Section\u00a0<ref> we upper bounded F and R^-1 respectively. \n\n\n\n \u00a7.\u00a7 Definitions of \n\n\n\n \n\nFor each p \u2208 [k], let u_*,p\u2208^n denotes the p-th column of matrix U_* \u2208^n \u00d7 k. \n\nFor each p \u2208 [k], let u_t,p denote the p-th column of matrix U_t \u2208^n \u00d7 k.\n\nWe define block matrices B, C, D, S \u2208^kd \u00d7 kd as follows:\nFor each (p,q) \u2208 [k] \u00d7 [k]\n\n    \n  * Let B_p,q\u2208^d \u00d7 d denote the (p,q)-th block of B \n    \n    B_p,q= \u2211_i=1^m  y_i y_i^\u22a4_d \u00d7 d \u00a0matrix\u00b7 (x_i^\u22a4 u_t,p)_scalar\u00b7 (x_i^\u22a4 u_t,q) _scalar\n\n    \n  * Let C_p,q\u2208^d \u00d7 d denote the (p,q)-th block of C, \n    \n    C_p,q= \u2211_i=1^m  y_i y_i^\u22a4_ d \u00d7 d \u00a0matrix\u00b7 (x_i^\u22a4 u_t,p ) _scalar\u00b7 (x_i^\u22a4 u_*q) _scalar\n\n    \n  * Let D_p,q\u2208^d \u00d7 d denote the (p,q)-th block of D, \n    \n    D_p,q= u_t,p^\u22a4 u_*q I\n\n    \n  * Let S_p,q\u2208^d \u00d7 d denote the (p,q)-th block of S, \n    \n    S_p,q= \u03c3_p^* I ,    if\u00a0 p=q; \n    \u00a0 0,    if\u00a0 p  q.\n\n    Here \u03c3_1^*, \u22ef\u03c3_k^* are singular values of W_* \u2208^d \u00d7 d.\n   \n  * We define F \u2208^d \u00d7 k as follows\n   \n    (F) _d \u00d7 1 :=  B^-1_d \u00d7 d (BD-C) _d \u00d7 d S _d \u00d7 d\u00b7(V_*) _d \u00d7 1.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Upper Bound on \n\n\n\nLet B, C and D be defined as Definition\u00a0<ref>. Then we have\n\n    BD-C\u2264\u03f5\u00b7(U,U_*) \u00b7  k\n\n\n\n\n\nLet z_1, \u22ef, z_k \u2208^d denote k vectors. Let z = [ z_1;   \u22ee; z_k ]. \n\nWe define f(z):=z^\u22a4 (BD-  C)z\n\nWe define f(z,p,q) = z_p^\u22a4 (BD-C)_p,q z_q.\n\nThen we can rewrite\n\n    z^\u22a4 (BD - C) z\n    =    \u00a0\u2211_p=1^k \u2211_q=1^k z_p^\u22a4 (BD-C)_p,q z_q \n    \n    =    \u00a0\u2211_p=1^k \u2211_q=1^k z_p^\u22a4 ( B_p,: D_:,q - C_p,q ) z_q  \n    \n    =    \u00a0\u2211_p=1^k \u2211_q=1^k z_p^\u22a4 ( \u2211_l=1^k B_p,l D_l,q - C_p,q ) z_q\n\nBy definition, we know\n\n    B_p,l =    \u00a0\u2211_i=1^m y_i y_i^\u22a4 (x_i^\u22a4 u_t,p) \u00b7 (  u_t,l^\u22a4 x_i ) \n    \n    D_l,q =    \u00a0 (u_*,q^\u22a4 u_t,l ) I_d \n    \n    C_p,q =    \u00a0\u2211_i=1^m y_i y_i^\u22a4 (x_i^\u22a4 u_t,p) \u00b7 (  u_*,q^\u22a4 x_i )\n\n\nWe can rewrite C_p,q as follows\n\n    C_p,q = \u2211_i=1^m y_i y_i^\u22a4\u00b7 (x_i^\u22a4 u_t,p) \u00b7 (  u_*,q^\u22a4 I_d x_i )\n\n\nLet us compute \n\n    B_p,l D_l,q \n    =    \u00a0\u2211_i=1^m y_i y_i^\u22a4 (x_i^\u22a4 u_t,p) \u00b7 ( u_t,l^\u22a4 x_i  ) \u00b7 ( u_*,q^\u22a4 u_t,l  )  \n    \n    =    \u00a0\u2211_i=1^m y_i y_i^\u22a4 (x_i^\u22a4 u_t,p) \u00b7  ( u_*,q^\u22a4 u_t,l  ) \u00b7 ( u_t,l^\u22a4 x_i  )\n\nwhere the second step follows from a \u00b7 b = b \u00b7 a for any two scalars.\n\n \n\nTaking the summation over all l \u2208 [k], we have\n\n    \u2211_l=1^k B_p,l D_l,q \n    =    \u00a0\u2211_l=1^k \u2211_i=1^m y_i y_i^\u22a4 (x_i^\u22a4 u_t,p) \u00b7  ( u_*,q^\u22a4 u_t,l  ) \u00b7 ( u_t,l^\u22a4 x_i  ) \n    \n    =    \u00a0\u2211_i=1^m y_i y_i^\u22a4 (x_i^\u22a4 u_t,p) \u00b7   u_*,q^\u22a4\u2211_l=1^k (u_t,l\u00b7 u_t,l^\u22a4 ) x_i   \n    \n    =    \u00a0\u2211_i=1^m  y_i y_i^\u22a4_matrix\u00b7 (x_i^\u22a4 u_t,p) _scalar\u00b7  u_*,q^\u22a4  U_t U_t^\u22a4 x_i _scalar\n\nwhere first step follows from definition of B and D.\n\nThen, we have\n\n    \u2211_l=1^k B_p,l D_l,q - C_p,q\n    =    \u00a0 (\u2211_i=1^m  y_i y_i^\u22a4_matrix\u00b7 (x_i^\u22a4 u_t,p) _scalar\u00b7  u_*,q^\u22a4  U_t U_t^\u22a4 x_i _scalar) - C_p,q\n    \n    =    \u00a0 (\u2211_i=1^m  y_i y_i^\u22a4_matrix\u00b7 (x_i^\u22a4 u_t,p) _scalar\u00b7  u_*,q^\u22a4  U_t U_t^\u22a4 x_i _scalar) - (\u2211_i=1^m y_i y_i^\u22a4\u00b7 (x_i^\u22a4 u_t,p) \u00b7 (  u_*,q^\u22a4 I_d x_i )) \n    \n    =    \u00a0\u2211_i=1^m  y_i y_i^\u22a4_matrix\u00b7 (x_i^\u22a4 u_t,p) _scalar\u00b7  u_*,q^\u22a4 ( U_t U_t^\u22a4 - I_d) x_i _scalar\n\nwhere the first step follows from Eq.\u00a0(<ref>), the second step follows from Eq.\u00a0(<ref>), the last step follows from merging the terms to obtain (U_t U_t^\u22a4 - I_d).\n\n\nThus,\n\n    f(z,p,q)\n    =    \u00a0z_p^\u22a4 ( \u2211_l=1^k B_p,l D_l,q - C_p,q ) z_q \n    \n    =    \u00a0\u2211_i=1^m  ( z_p^\u22a4 y_i ) _scalar ( y_i^\u22a4 z_q ) _scalar\u00b7 (x_i^\u22a4 u_t,p) _scalar\u00b7   u_*,q^\u22a4  ( U_t U_t^\u22a4 - I_d) x_i _scalar\n\n\n \nFor easy of analysis, we define v_t:= u_*,q^\u22a4  ( U_t U_t^\u22a4 - I_d). This means v_t lies in the complement of span of U_t. \n\nThen \n\n    v_t _2 \n    =    \u00a0 u_*,q^\u22a4  ( U_t U_t^\u22a4 - I_d) _2 \n    \n    =    \u00a0 e_q^\u22a4 U_*^\u22a4 (U_t U_t^\u22a4 - I_d) \n    \u2264   \u00a0 U_*^\u22a4 (U_t U_t^\u22a4 - I_d) \n    \n    =    \u00a0(U_*,U_t).\n\nwhere the second step follows from u_*,q^\u22a4 = e_q^\u22a4 U_*^\u22a4 (e_q \u2208^k is the vector q-th location is 1 and all other locations are 0s), \nthird step follows from Fact\u00a0<ref>.\n\nWe want to apply Definition\u00a0<ref>, but the issue is z_p, z_q and v_t are not unit vectors. So normalize them. Let z_p = z_p / z_p _2 , z_q = z_q / z_q _2 and v_t = v_t/  v_t _2.\n\nIn order to apply for Definition\u00a0<ref>, we also need v_t^\u22a4 u_t,p=0. \n\nThis is obvious true, since v_t lies in the complement of span of U_t and u_t,p in the span of U_t. \n\nWe define \n\n    G := \u2211_i=1^m  (x_i^\u22a4 u_t,p) _scalar\u00b7 (x_i^\u22a4v_t) _scalar\u00b7 y_i y_i^\u22a4_matrix\n\n\nBy  Definition\u00a0<ref>, we know that \n\n    G \u2264\u03f5.\n\nBy definition of spectral norm, we have for any unit vector z_p and z_q, we know that\n\n    |z_p^\u22a4 G z_q | \u2264 G \u2264\u03f5.\n\nwhere the first step follows from definition of spectral norm (Fact\u00a0<ref>), and the last step follows from Definition\u00a0<ref>.\n \n\nNote that\n\n    f(p,q,z) =    \u00a0\u2211_i=1^m  (x_i^\u22a4 u_t,p) \u00b7 (x_i^\u22a4v_t) _scalar\u00b7 (z_p^\u22a4 y_i) \u00b7 (y_i^\u22a4z_q) _scalar\u00b7 z_p _2 \u00b7 z_q _2 \u00b7 v_t _2 _scalar\n    \n    =    \u00a0z_p^\u22a4_ 1 \u00d7 d\u00b7( \u2211_i=1^m  (x_i^\u22a4 u_t,p) \u00b7 (x_i^\u22a4v_t) _scalar\u00b7 y_i y_i^\u22a4_ d \u00d7 d ) \u00b7z_q _d \u00d7 1\u00b7 z_p _2 \u00b7 z_q _2 \u00b7 v_t _2 _scalar\n    \n    =    \u00a0z_p^\u22a4_1 \u00d7 d\u00b7 G _d \u00d7 d\u00b7z_q _d \u00d7 1\u00b7 z_p _2 \u00b7 z_q _2 \u00b7 v_t _2 _scalar\n\nwhere the second step follows from rewrite the second scalar (z_p^\u22a4 y_i) (y_i^\u22a4z_q) = z_p^\u22a4 (y_i y_i^\u22a4) z_q, the last step follows from definition of G.\n\nThen,\n\n    |f(z,p,q)|\n    =    \u00a0 | \u2211_i=1^m z_p^\u22a4 G z_q | \u00b7 z_p _2  z_q _2  v_t _2 \n    \u2264   \u00a0\u03f5 z_p _2  z_q _2 \u00b7 v_t _2 \n    \u2264   \u00a0\u03f5 z_p _2  z_q _2 \u00b7(U_t,U_*)\n\nwhere the last step follows from Eq.\u00a0(<ref>).\n\n\n\nFinally, we have\n\n    BD-C\n        =    \u00a0max_z,z_2=1|z^\u22a4(BD-C)z| \n    \n        =    \u00a0max_z,z_2=1|\u2211_ p \u2208 [ k ],q \u2208 [k] f(z,p,q)| \n    \u2264   \u00a0max_z,z_2=1\u2211_ p \u2208 [ k ],q \u2208 [k] | f(z,p,q)| \n    \u2264   \u00a0\u03f5\u00b7(U_t,U_*) max_z,z_2=1\u2211_p\u2208 [k] , q \u2208 [k] z_p_2z_q_2 \n    \u2264   \u00a0\u03f5\u00b7(U,U_*) \u00b7  k\n\nwhere the first step follows from Fact\u00a0<ref>, the last step step follows from \u2211_p=1^k  z_p _2 \u2264\u221a(k) (\u2211_p=1^k  z_p _2^2)^1/2 = \u221a(k).\n\n  \n\n\n\n\n\n \u00a7.\u00a7 Rewrite \n\n\n\n\nIf \n\n    V_t+1 = (W_*^\u22a4 U_t-F)R^-1\n\nthen, \n\n    (V_*, )^\u22a4 V_t+1 = -(V_*, )^\u22a4 FR^-1\n\n\n\n\n\n \n    Multiplying both sides by V_*,\u2208^d \u00d7 (d-k):\n    \n    V_t+1=   \u00a0 (W_*^\u22a4 U_t-F)R^-1\n    \n            (V_*, )^\u22a4 V_t+1=   \u00a0(V_*, )^\u22a4(W_*^\u22a4 U_t-F)R^-1\n    \n            (V_*, )^\u22a4 V_t+1=   \u00a0(V_*, )^\u22a4 W_*^\u22a4 R^-1-(V_*, )^\u22a4 FR^-1\n\n    We just need to show (V_*, )^\u22a4 W_*^\u22a4 R^-1=0.\n\nBy definition of V_*,, we know:\n\n    V_*,^\u22a4 V_*= 0_k \u00d7 (n-k)\n\n\nThus, we have:\n\n    (V_*, )^\u22a4 W_*^\u22a4 =   \u00a0 V_*,^\u22a4 V_* \u03a3_* U_*^\u22a4\n    \n        =   \u00a0 0\n\n \n\n\n\n\n\n\n \u00a7.\u00a7 Upper bound on \n\n\n\nLet \ud835\udc9c be a rank-one measurement operator where A_i = x_i u_i^\u22a4. Let \u03ba be defined as Definition\u00a0<ref>.  \n\n \n Then, we have\n \n    F \u2264 2 \u03f5 k^1.5\u00b7\u03c3_1^* \u00b7(U_t,U_*)\n\n\n Further, if \u03f5\u2264 0.001 / ( k^1.5\u03ba )\n\n    F \u2264 0.01 \u00b7\u03c3_k^* \u00b7(U_t,U_*).\n\n\n\nRecall that \n\n    (F) = B^-1(BD-C)S \u00b7(V_*).\n\n\nHere, we can upper bound F as follows\n\n    F\u2264   \u00a0F_F \n    \n        =    \u00a0(F) _2 \n    \u2264   \u00a0B^-1\u00b7BD-C\u00b7S\u00b7(V_*)_2 \n    \n        =    \u00a0B^-1\u00b7(BD-C)\u00b7 S \u00b7\u221a(k)\n    \u2264   \u00a0B^-1\u00b7(BD-C)\u00b7\u03c3_1^* \u00b7\u221a(k)\n\nwhere the first step follows from \u00b7\u2264\u00b7_F (Fact\u00a0<ref>), the second step follows vectorization of F is a vector, the third step follows from A x _2 \u2264A \u00b7 x _2, the forth step follows from (V_*) _2 =  V_* _F \u2264\u221a(k)  \n(Fact\u00a0<ref>) and the last step follows from S \u2264\u03c3_1^* (see Definition\u00a0<ref>). \n\nNow, we first bound B^-1=1/(\u03c3_min(B)). \n\nAlso, let Z=[ z_1 z_2   \u22ef z_k ] and let z=(Z). \n\n\nNote that B_p,q denotes the (p,q)-th block of B.\n\nWe define \n\n    B := { x \u2208^kd\u00a0|\u00a0 x _2 = 1 }.\n\n\nThen  \n\n\n\n    \u03c3_min(B)\n        =    \u00a0min_z \u2208 Bz^\u22a4 B z \n    \n        =    \u00a0min_z \u2208 B\u2211_ p \u2208 [k], q \u2208 [k] z_p^\u22a4 B_pqz_q \n    \n        =    \u00a0min_z \u2208 B\u2211_p=1^k z_p^\u22a4 B_p,pz_p+\u2211_p\u2260 qz_p^\u22a4 B_p,qz_q.\n\nwhere the first step follows from Fact\u00a0<ref>, \nthe second step follows from simple algebra, the last step follows from  \n(Fact\u00a0<ref>).\n\n\n\n \n\nWe can lower bound z_p^\u22a4 B_p,pz_p as follows\n\n    z_p^\u22a4 B_p,p z_p\n        \u2265   \u00a0\u03c3_min(B_p,p) \u00b7 z_p _2^2 \n    \u2265   \u00a0 (1-\u03f5) \u00b7 z_p _2^2\n\nwhere the first step follows from Fact\u00a0<ref>  \n, the last step follows from Definition\u00a0<ref> .\n\n\nWe can upper bound | z^\u22a4 B_p,q z_q | as follows,\n\n    |z_p^\u22a4 B_p,q z_q|\n        \u2264   \u00a0 z_p _2 \u00b7 B_p,q\u00b7 z_q _2 \n    \u2264   \u00a0\u03f5\u00b7 z_p _2 \u00b7 z_q _2\n\nwhere the first step follows from Fact\u00a0<ref>, the last step follows from Definition\u00a0<ref> . \n\nWe have\n\n    \u03c3_min(B)\n        =    \u00a0min_z,z_2=1\u2211_p=1^k z_p^\u22a4 B_p,pz_p+\u2211_p\u2260 qz_p^\u22a4 B_p,qz_q \n    \u2265   \u00a0min_z,z_2=1 (1-\u03f5)\u2211_p=1^k  z_p _2^2 +\u2211_p\u2260 qz_p^\u22a4 B_p,qz_q \n    \u2265   \u00a0min_z,z_2=1(1-\u03f5)\u2211_p=1^kz_p_2^2-\u03f5\u2211_p \u2260 qz_p_2z_q_2 \n    \n        =    \u00a0min_z,z_2=1 (1-\u03f5) -\u03f5\u2211_p \u2260 qz_p_2z_q_2  \n    \n        =    \u00a0min_z,z_2=1 (1-\u03f5) - k \u03f5\n    \u2265   \u00a0 1- 2 k\u03f5\n    \u2265   \u00a0 1/2\n\n\n\nwhere the first step follows from Eq.\u00a0(<ref>),\nthe second step follows from Eq.\u00a0(<ref>), the third step follows from Eq.\u00a0(<ref>), the forth step follows from \u2211_p=1^k  z_p _2^2 = 1(which derived from the z_2=1 constraint and the definition of z_2), the fifth step follows from \u2211_p \u2260 q z_p _2  z_q _2 \u2264 k, \nand the last step follows from \u03f5\u2264 0.1/k. \n\n We can show that\n \n    B^-1 = \u03c3_min(B) \u2264 2.\n\n where the first step follows from Fact\u00a0<ref>, the second step follows from Eq.\u00a0(<ref>).\n\nNow, consider BD-C, using Claim\u00a0<ref>, we have\n\n    BD-C\u2264  k\u00b7\u03f5\u00b7(U_t,U_*)\n\n\n\nNow, we have\n\n    F \u2264   \u00a0 B^-1\u00b7 (BD - C) \u00b7\u03c3_1^* \u00b7\u221a(k)\n    \u2264   \u00a0 2 \u00b7 (BD - C) \u00b7\u03c3_1^* \u00b7\u221a(k)\n    \u2264   \u00a0 2 \u00b7 k \u00b7\u03f5\u00b7(U_t,U_*) \u00b7\u03c3_1^* \u00b7\u221a(k)\n \nwhere the first step follows from Eq\u00a0.(<ref>), the second step follows from Eq.\u00a0(<ref>),  \nand the third step follows from  Eq.\u00a0(<ref>).\n\n\n\n\n \u00a7.\u00a7 Upper bound on \n\n\n\nLet \ud835\udc9c be a rank-one measurement operator matrix where A_i=x_i y_i^\u22a4. Also, let \ud835\udc9c satisfy three properties mentioned in Theorem <ref>.\n\nIf the following condition holds\n\n    \n  * (U_t, U_*) \u22641/4\u2264\u03f5_d = 1/10 (The condition of Part 1 of Lemma\u00a0<ref>)\n\n\n\nThen, \n\n    R^-1\u2264   \u00a0 10 /\u03c3_k^*\n\n\n\nFor simplicity, in the following proof, we use V to denote V_t+1. We use U to denote U_t.\n\nUsing Fact\u00a0<ref>\n\n    R^-1 = \u03c3_min(R)^-1\n\n\nWe can lower bound \u03c3_min(R) as follows:\n\n    \u03c3_min(R)\n        =    \u00a0min_z,z_2=1Rz_2 \n    \n        =    \u00a0min_z,z_2=1VRz_2 \n    \n        =    \u00a0min_z,z_2=1V_*\u03a3_*U_*^\u22a4 Uz-Fz_2 \n    \u2265   \u00a0min_z,z_2=1V_*\u03a3_*U_*^\u22a4 Uz_2-Fz_2 \n    \u2265   \u00a0min_z,z_2=1V_*\u03a3_*U_*^\u22a4 Uz_2-F\n\nwhere the first step follows from definition of \u03c3_min,\nthe second step follows from Fact\u00a0<ref>,  \nthe third step follows from V = (W_*^\u22a4 U-F)R^-1 =  (V_* \u03a3_* U_*^\u22a4 U - F) R^-1 (due to Eq.\u00a0(<ref>) and Definition\u00a0<ref>)  \n, the forth step follows from triangle inequality,  \nthe fifth step follows from A x _2 \u2264 A for all x _2=1.\n\nNext, we can show that\n\n    min_z,z_2=1V_*\u03a3_*U_*^\u22a4 Uz_2\n    =    \u00a0min_z,z_2=1\u03a3_*U_*^\u22a4 Uz_2 \n    \u2265   \u00a0min_z,z_2=1\u03c3_k^* \u00b7 U_*^\u22a4 Uz_2 \n    \n    =    \u00a0\u03c3_k^* \u00b7\u03c3_min(U^\u22a4 U_*)\n \nwhere the first step follows from Fact\u00a0<ref>,  \n the second step follows from Fact\u00a0<ref>, the third step follows from definition of \u03c3_min,  \n \nNext, we have\n\n    \u03c3_min(U^\u22a4 U_*) \n    =    \u00a0cos\u03b8(U_*, U) \n    \n    =    \u00a0\u221a(1-sin^2 \u03b8(U_*,U))\n    \u2265   \u00a0\u221a(1- (U_*,U)^2)\n \nwhere the first step follows definition of cos, the second step follows from sin^2 \u03b8 + cos^2 \u03b8  =1 (Lemma\u00a0<ref>), the third step follows from sin\u2264 (see Definition\u00a0<ref>).\n\n\nPutting it all together, we have\n\n    \u03c3_min(R) \u2265   \u00a0\u03c3_k^* \u221a(1-(U_*,U)^2) -  F \n    \u2265   \u00a0\u03c3_k^* \u221a(1-(U_*,U)^2) - 0.001 \u03c3_k^* (U_*,U) \n    \n    =    \u00a0\u03c3_k^* ( \u221a(1-(U_*,U)^2) - 0.001 (U_*,U)  ) \n    \u2265   \u00a0 0.2 \u03c3_k^*\n\nwhere the second step follows from Lemma\u00a0<ref>, the last step follows from (U_*,U) < 1/10.\n \n\n\n\n\n\n\u00a7 MATRIX SENSING REGRESSION\n\n\n\nOur algorithm has O(log(1/\u03f5_0)) iterations, in previous section we have proved why is that number of iterations sufficient. In order to show the final running time, we still need to provide a bound for the time we spend in each iteration. In this section, we prove a bound for cost per iteration. \nIn Section\u00a0<ref> we provide a basic claim that, our sensing problem is equivalent to some regression problem. In Section\u00a0<ref> we show the different running time of the two implementation of each iteration. In Section\u00a0<ref> we provide the time analysis for each of the iteration of our solver. In Section\u00a0<ref> shows the complexity for the straightforward solver. Finally in Section\u00a0<ref> we show the bound for the condition number. \n\n\n\n \u00a7.\u00a7 Definition and Equivalence\n\n\nIn matrix sensing, we need to solve the following problem per iteration:\n\n\nLet A_1,\u2026,A_m \u2208^d\u00d7 d, U\u2208^d\u00d7 k and b\u2208^m be given. The goal is to solve the following minimization problem\n\n    min_V\u2208^d\u00d7 k\u2211_i=1^m ([A_i^\u22a4 U V^\u22a4]-b_i)^2,\n\n\n\nWe define another regression problem \n\nLet A_1,\u2026,A_m \u2208^d\u00d7 d, U\u2208^d\u00d7 k and b\u2208^m be given.\n\nWe define matrix M\u2208^m\u00d7 dk as follows\n\n    M_i,* :=    \u00a0(U^\u22a4 A_i), \u00a0\u00a0\u00a0\u2200 i \u2208 [m].\n\n\nThe goal is to solve the following minimization problem.\n\n    min_v\u2208^d kMv-b _2^2,\n\n\n\nWe can prove the following equivalence result\n\n\nLet A_1,\u2026,A_m \u2208^d\u00d7 d, U\u2208^d\u00d7 k and b\u2208^m be given.\n\nIf the following conditions hold\n\n    \n  * M_i,* :=  (U^\u22a4 A_i), \u00a0\u00a0\u00a0\u2200 i \u2208 [m].\n\n  * The solution matrix V \u2208^d \u00d7 k can be reshaped through vector v \u2208^dk, i.e., v = (V^\u22a4).\n\n\n\nThen, the problem (defined in Definition\u00a0<ref>) is equivalent to problem (defined in Definition\u00a0<ref>) .\n \n\n\n\nLet X, Y\u2208^d\u00d7 d, we want to show that \n\n    [X^\u22a4 Y] =    \u00a0(X)^\u22a4(Y).\n\nNote that the RHS is essentially\n\u2211_i \u2208 [d]\u2211_j \u2208 [d] X_i,jY_i,j, for the LHS, note that \n\n    (X^\u22a4 Y)_j,j =    \u00a0\u2211_i\u2208 [d] X_i,j Y_i,j,\n\nthe trace is then sum over j. \n\nThus, we have Eq.\u00a0(<ref>). This means that for each i\u2208 [d], \n\n    [A_i^\u22a4 UV^\u22a4]=(U^\u22a4 A_i)^\u22a4(V^\u22a4).\n\n\n\nSet M\u2208^m \u00d7 dk be the matrix where each row is (U^\u22a4 A_i), we see Definition\u00a0<ref> is equivalent to solve the regression problem as in the statement. This completes the proof.\n\n\n\n\n \u00a7.\u00a7 From Sensing Matrix to Regression Matrix\n\n\n\n\nLet A_1,\u2026,A_m \u2208^d\u00d7 d, U\u2208^d\u00d7 k . \n We define matrix M\u2208^m\u00d7 dk as follows\n\n    M_i,* :=    \u00a0(U^\u22a4 A_i), \u00a0\u00a0\u00a0\u2200 i \u2208 [m].\n\n\n\nThe naive implementation of computing M \u2208^m \u00d7 dk\ntakes m \u00b7(k,d,d) time.\nWithout using fast matrix multiplication, it is O(md^2k) time.\n\n\nFor each i \u2208 [m], computing matrix U^\u22a4\u2208^k \u00d7 d times A_i \u2208^d \u00d7 d takes (k,d,d) time. Thus, we complete the proof.\n\n\n\nThe batch implementation takes (k,dm,d) time. \nWithout using fast matrix multiplication, it takes O(md^2 k) time.\n\n\nWe can stack all the A_i together, then we matrix multiplication. For example, we construct matrix A \u2208^d \u00d7 dm. Then computing U^\u22a4 A takes (k,d,dm) time.\n\nThe above two approach only has difference when we use fast matrix multiplication.\n\n\n\n \u00a7.\u00a7 Our Fast Regression Solver\n\nIn this section, we provide the results of our fast regression solver. Our approach is basically as in <cit.>. For detailed analysis, we refer the readers to the Section\u00a05 in <cit.>. \n\n\nAssume m = \u03a9(dk).  \nThere is an algorithm that runs in time\n\n    O( m d^2 k + d^3 k^3 )\n\nand outputs a v' such that \n\n    M v' - b _2 \u2264 (1+\u03f5) min_v \u2208^dk M v - b_2\n\n\n\n\nFrom Claim\u00a0<ref>, writing down M \u2208^m \u00d7 dk takes O(md^2 k) time.\n\n\n \n\nUsing Fast regression resolver as <cit.>, the fast regression solver takes\n \n\n    O( ( m\u00b7 dk  + (dk)^3 ) \u00b7log(\u03ba(M)/\u03f5) \u00b7log^2(n/\u03b4) )\n\n\n\n\nIn each iteration, our requires takes O( m d^2 k) time.\n\n\nFinally, in order to run Lemma\u00a0<ref>, we need to argue that \u03ba(M) \u2264(k,d,\u03ba(W_*)).\n\n\n This is true because \u03ba(U) \u2264 O(\u03ba(W_*)) and condition number of random Gaussian matrices is bounded by (k,d).\n\n\n\nThen applying Lemma\u00a0<ref>, we can bound \u03ba(M) in each iteration.\n\n\n\nEventually, we just run standard error analysis in <cit.>. Thus, we should get the desired speedup.\n\nThe reason we can drop the (dk)^3 is m \u2265 dk^2.\n\n\n\n\n\n \u00a7.\u00a7 Straightforward Solver\n\n\nNote that from sample complexity analysis, we know that m = \u03a9(dk).\n\nAssume m = \u03a9(dk). \nThe straightforward implementation of the regression problem (Defintion\u00a0<ref>) takes \n\n    O(md^2 k^2)\n\ntime.\n\n\nThe algorithm has two steps. From Claim\u00a0<ref>, writing down M \u2208^m \u00d7 dk takes O(md^2 k) time.\n\nThe first step is writing down the matrix M \u2208^m \u00d7 dk.\n\nThe second step is solving regression, it needs to compute M^\u2020 b (where M^\u2020\u2208^d k \u00d7 m )\n\n    M^\u2020 b = ( M^\u22a4 M )^-1 M b\n\n\nthis will take time\n\n    (dk,m,dk) + (dk,dk,dk) \n    =    \u00a0 m d^2k^2 + (dk)^3 \n    \n    =    \u00a0 m d^2 k^2\n\nthe second step follows from m =\u03a9(dk)\n.\n\nThus, the total time is\n\n    m d^2 k + md^2 k^2 = O(m d^2k^2)\n\n\n\n\n\n\n \u00a7.\u00a7 Condition Number\n\n\n\n\n\nWe define B \u2208^m \u00d7 k as follows B := X U and X \u2208^m \u00d7 d and U \u2208^d \u00d7 k.\n\nThen, we can rewrite M \u2208^m \u00d7 dk\n \n\n    M_m \u00d7 dk = B_m \u00d7 k\u2297Y_m \u00d7 d\n\n\nThen, we know that \u03ba(M) = \u03ba(B) \u00b7\u03ba(Y) \u2264\u03ba(U) \u03ba(X) \u03ba(Y).\n \n\n\n\n\nRecall U \u2208^d \u00d7 k. Then we define b_i = U^\u22a4 x_i for each i \u2208 [m].\n\nThen we have\n\n    M_i,* = ( U^\u22a4 x_i y_i^\u22a4 ) = (b_i y_i^\u22a4 ).\n\n\nThus, it implies \n\n    M = B \u2297 Y\n \n\n \n\n\n\n\n\nalpha\n\n\n\nalpha\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}