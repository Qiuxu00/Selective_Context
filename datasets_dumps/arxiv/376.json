{"entry_id": "http://arxiv.org/abs/2303.06811v2", "published": "20230313015914", "title": "The NPU-Elevoc Personalized Speech Enhancement System for ICASSP2023 DNS Challenge", "authors": ["Xiaopeng Yan", "Yindi Yang", "Zhihao Guo", "Liangliang Peng", "Lei Xie"], "primary_category": "eess.AS", "categories": ["eess.AS"], "text": "\n\nWeighted Euclidean balancing for a matrix exposure in estimating causal effect\n    [\n    \n==============================================================================\n\n\n\n\n\n\n\n*Corresponding Author\nThis paper describes our NPU-Elevoc personalized speech enhancement system (NAPSE) for the 5th Deep Noise Suppression Challenge<cit.> at ICASSP 2023. Based on the superior two-stage model TEA-PSE 2.0\u00a0<cit.>, our system particularly explores better strategy for speaker embedding fusion, optimizes the model training pipeline, and leverages adversarial training and multi-scale loss. According to the results[https://aka.ms/5th-dns-challenge][https://github.com/microsoft/DNS-Challenge], our system is tied for the 1st place in the headset track (track 1) and ranked 2nd in the speakerphone track (track 2).\n\n\n\n\n\n\npersonalized speech enhancement, real-time, generative adversarial network, deep learning.\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\nTaking a speaker's short enrollment as prior, personalized speech enhancement (PSE) aims to extract the target speaker's speech from a mixture signal that may contain noise, reverberation, and interfering speaker. The 5th edition of the deep noise suppression (DNS) challenge, held in ICASSP 2023, particularly focuses on PSE for full-band signal collected from headset microphone (track 1) and speaker-phone (track 2). In this challenge, our model is based on TEA-PSE 2.0\u00a0<cit.> \u2013 an upgraded two-stage model from the previous DNS challenge championship\u00a0<cit.>. Still keeping the two-stage strategy of decomposing a difficult learning task into easier sub-processes, TEA-PSE 2.0 adopts subband operations and a time-frequency convolution module to reduce computational complexity and further improve performance. Compared to the oracle TEA-PSE 2.0, we have made substantial improvements in several aspects. First, we leverage a stronger ResNet34-based speaker embedding model which has recently achieved state-of-the-art performance on VoxCeleb\u00a0<cit.>. Moreover, inspired by the recent observation\u00a0<cit.> that a simple filterbank feature can preserve the integrity of speaker information with good generalization, we explore the complementarity between such acoustic representation and commonly-used neural speaker embedding through fusion experiments. Second, we study the training strategy for the two-stage model with the conclusion that the best performance is achieved when we first train the stage-one model, then freeze the stage-one model to train the stage-two model, and finally, jointly train both stage models. Ultimately, we improve model optimization with multi-scale loss\u00a0<cit.> and GAN loss\u00a0<cit.>. Particularly, we adopt MetricGAN\u00a0<cit.> to predict PESQ and DNSMOS because it can learn these non-differentiable metrics to optimize the model.\n\n\n\n\n\n\n\u00a7 PROPOSED METHOD\n\n\n\n\n\n\n \u00a7.\u00a7 Overview\n\n\n\nOur system is mainly composed of three parts: speaker encoder, speech enhancement model, and MetricGAN discriminator. A RestNet34-based speaker encoder\u00a0<cit.> extracts speaker embedding from the enrollment speech. The speaker embedding, together with utterance-level mean and standard deviation of the Fbank feature extracted from the enrollment speech, goes through a learnable fusion module. The fused embedding is thus fed into the speech enhancement model as the prior information for speaker extraction.\n\nThe speech enhancement model follows that in TEA-PSE 2.0\u00a0<cit.>, consisting of a two-stage model and subband operations. The two-stage model includes MAG-Net (stage one) and COM-Net (stage two) to process magnitude and complex features respectively. The encoder of MAG-Net is composed of three frequency down-sampling (FD) layers, the decoder of MAG-Net is composed of three frequency up-sampling (FU) layers, and the middle layer is composed of four stacked gated temporal convolutional modules (S-GTCM). Likewise, COM-Net has a similar network topology as MAGNet, while its dual-decoder architecture is designed to estimate the real and imaginary spectrum separately. The MetricGAN discriminator is introduced to help the speech enhancement model to better optimize perceptual metrics including PESQ and DNSMOS.\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 GAN\n\n\n\nAdversarial learning has shown superior performance recently. Perceptual metrics that are closer to human auditory impression on speech quality, have been recently considered as optimization targets\u00a0<cit.>. Thus, we adopt GAN to learn these non-differentiable metrics. Specifically, we use MetricGAN+\u00a0<cit.> to predict PESQ and MetricGAN-U\u00a0<cit.> to predict DNSMOS OVRL respectively. \n\n\n\n\n\n\n\n    m_PESQ(\u015d,s)    = (PESQ(\u015d,s)+0.5)/5\n    \n    \n    m_DNSMOS(\u015d,s)    = (DNSMOS(\u015d)-1.0)/4\n    \n    \u2112_D   = | D(\u015d) - m(\u015d,s) |^2\n    \n    \u2112_G   = | D(\u015d) - 1.0 |^2\n\n\nwhere s and \u015d refer to the original clean and estimated sources. m(\u015d,s) is the function to get metric, and values are normalized to [0,1]. \u2112_D is used to train the MetricGAN discriminator network and \u2112_G is used to train the speech enhancement network. \n\n\n\n\n\n \u00a7.\u00a7 Loss function\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the stage-one network, we use the scale-invariant signal-to-noise ratio (SI-SNR) loss \u2112_si-snr, the asymmetric loss \u2112_asym and the magnitude spectral mean square loss \u2112_mag. For the stage-two network, we add the complex spectral mean square loss \u2112_RI. In addition to the above losses in TEA-PSE 2.0, we add multi-scale loss\u00a0<cit.> and GAN loss as well, which are formulated as\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \u2112_1   =\u2112_si-snr + 1/M\u2211_m=1^M(\u2112_asym + \u2112_mag) + \u2112_G\n    \u2112_2   =\u2112_si-snr + 1/M\u2211_m=1^M(\u2112_asym + \u2112_mag + \u2112_RI) + \u2112_G\n\n\n\n\nwhere \u2112_1 and \u2112_2 are the total loss of stage one and two models respectively, and m indicates the scale corresponding to different STFT configurations. In this paper, we set M=3 with number of FFT bins \u2208 {512, 1024, 2048}, hop sizes \u2208 {240, 480, 960}, and window lengths \u2208 {480, 960, 1920} respectively.\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n\n\n \u00a7.\u00a7 Dataset\n\n\n\nSpeech and noise data are all taken from DNS4 track2 personalized dataset and DNS5 dataset. We simulate 150,000 RIR clips by image method[https://github.com/phecda-xu/RIR-Generator] and the RT60 of RIRs ranges from 0.1s to 1.2s. Totally we simulate about 2,000 hours of data for the model training of the two tracks while 170 hours are used as a subset for quick ablation study on dev_test set. The model structures for the two tracks are the same but trained using different sets on the official Github.\n\n\n\n\n\n\n \u00a7.\u00a7 Training Setup\n\n\n\nDuring the model training, the parameters of Resnet34 speaker encoder are always frozen. The learnable fusion module is a 256-dimensional Dense layer to combine 256-dimensional Resnet34 embedding and 160-dimensional Fbank (80 means and 80 standard deviations). The configuration of the speech enhancement model is exactly the same as that of TEA-PSE 2.0\u00a0<cit.>. The subband split and merge is based on PQMF and the number of subbands is set to 4. Window length and hop size are 20ms and 10ms respectively. The final submitted model has 12.49M parameters in total, where the parameters of the Resnet34 part is 6.63M. The parameters of the MetricGAN discriminator are not included here as it is discarded during inference. The multiply-accumulate operations (MACs) per second are 8.5G. The average real-time factor (RTF) per frame is 0.48, tested on Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz using a single thread for ONNX inference testing.\n \n\n\n\n\n\n \u00a7.\u00a7 Results and analysis\n\n\n\nWe first perform a quick ablation study based on the models trained using the 170-hour subset. Stage training test results on the oracle TEA-PSE2.0 model is shown in Table\u00a0<ref>. We can see that the best performance is achieved when we first train the stage-one model, then freeze the stage-one model to train the stage-two model, and finally, jointly train the both stage models. We then discard the stage-one model and use the stage-two model only for a quick investigation, as shown in Table\u00a0<ref>. We can see that the use of Fbank for speaker embedding fusion, multi-loss, and perceptual metric-based adversarial learning are also beneficial to the performance. Specifically, for the use of perceptual metrics, using MetricGAN-U to estimate DNSMOS SIG and BAK is better than estimating DNSMOS OVRL and PESQ. Here DNSMOS SIG and BAK are estimated each using a separate MetricGAN-U. Table\u00a0<ref> shows the final subjective listening results of the challenge. Our system is tied for 1st place in Track 1 and ranked 2nd in Track 2 according to the final score.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIEEEbib\n\n\n\n\n"}