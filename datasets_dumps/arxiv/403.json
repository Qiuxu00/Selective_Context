{"entry_id": "http://arxiv.org/abs/2303.06768v2", "published": "20230312222509", "title": "The Planner Optimization Problem: Formulations and Frameworks", "authors": ["Yiyuan Lee", "Katie Lee", "Panpan Cai", "David Hsu", "Lydia E. Kavraki"], "primary_category": "cs.AI", "categories": ["cs.AI", "cs.RO"], "text": "\n\n\nWavelet Galerkin Method for an Electromagnetic Scattering Problem\n    Michelle Michelle\n    \n=================================================================\n\n\n\n\n\n  Identifying internal parameters for planning is crucial to maximizing the performance of a planner. However, automatically tuning internal parameters which are conditioned on the problem instance is especially challenging. A recent line of work focuses on learning planning parameter generators, but lack a consistent problem definition and software framework. This work proposes the unified planner optimization problem (POP) formulation, along with the Open Planner Optimization Framework (OPOF), a highly extensible software framework to specify and to solve these problems in a reusable manner.\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nPlanning is a cornerstone of decision making in artificial intelligence. In planning, a planner is given an instance of a problem and utilizes some form of computation to realize a certain goal. Often, the performance of the planner is heavily influenced by a set of internal planning parameters. In order to maximize the performance of the planner, it is crucial to select the best parameters. \nIn this work, we assume the general case where high-quality parameters are hard to derive analytically, especially for highly complex planners. Instead, we focus on how to tune the parameters using a selected set of training problem instances. This essentially treats the planner as a black-box function, and we try out different parameters until we are satisfied with the observed performance of the planner. Methods developed for such a paradigm are extremely general, since they make no assumption about the internals of the planner.\n\nTo automatically tune a given planner, prevailing methods in what is known as algorithm configuration (AC) <cit.> or hyperparameter optimization (HPO) <cit.> typically find a fixed set of parameters which work best on average across the training set of problem instances. However, we argue that the parameters should be conditioned on the problem instance to achieve maximal performance. In particular, we need to identify a generator which maps a given problem instance to high-quality planning parameters. This is extremely challenging because the space of such possible mappings is much larger and even less understood than the space of planning parameters. An emerging trend of work focuses on some form of this problem \u2013 for example by learning belief-dependent macro-actions for online POMDP planners <cit.>; learning workspace- and task-dependent sampling distributions for sampling-based motion planners (SBMPs) <cit.>; and learning belief-dependent attention for autonomous driving <cit.>. While the key ideas and techniques behind these works are almost exactly the same, they lack a consistent problem formulation and have vastly different codebases.\n\nIn this work, we introduce the general planner optimization problem (POP) formulation which unifies the key ideas behind such recent contributions in tuning planning parameters that are conditioned on the problem instance. Additionally, we provide a highly extensible software framework, the Open Planner Optimization Framework (OPOF), to specify and to solve these problems in a reusable manner. We hope that this will allow the community to approach the problem in a coherent and accelerated manner. OPOF is available open-source at https://opof.kavrakilab.orghttps://opof.kavrakilab.org.\n\n\n\n\u00a7 BACKGROUND\n\n\n\n\n \u00a7.\u00a7 Planner Optimization Problem (POP)\n\n\nA planner optimization problem (POP) is a 4-tuple (\ud835\udc9e, \ud835\udcb3, f, \ud835\udc9f), where \ud835\udc9e is the problem instance space, \ud835\udcb3 is the planning parameter space, f is the planning objective, and \ud835\udc9f is the problem instance distribution. A planner takes as input a problem instance c \u2208\ud835\udc9e and planning parameters x \u2208\ud835\udcb3, and performs some computation on c using x. It returns a numeric value indicating the quality of the computation, which inherently follows the distribution f(x; c), the planning objective. \n\nThe goal is to find a generator G_\u03b8(c), mapping problem instances in \ud835\udc9e to planning parameters in \ud835\udcb3, such that the expected planning performance is maximized:\n\n    \u03b8max\u00a0\ud835\udd3c_c \u223c\ud835\udc9f[\ud835\udd3c_x \u223c G_\u03b8(c)[\ud835\udd3c[f(x; c)]]].\n\nThe generator may be stochastic, in which case samples x \u223c G_\u03b8(c) are produced whenever the generator is called. On the other hand, a deterministic generator always returns the same value x = G_\u03b8(c) for the same problem instance c. An unconditional generator is one whose output is unaffected by its input c.\n\nThe problem is challenging because the generator G_\u03b8(c) needs to be learned solely through interaction with the planner. In particular, we only have access to the planner which produces samples of f(x; c). We do not assume to have an analytical, much less differentiable, form for f(x; c). \n\n\n\n\n \u00a7.\u00a7 Related work\n\n\nBlack-box optimization (BBO). In black-box optimization (BBO) <cit.>, we are tasked with finding inputs that maximize a closed objective function. In particular, we have no knowledge about the analytical expression or derivatives of the function, and can only evaluate the objective function at different inputs. There are 2 broad classes of approaches to solve BBOs. (i) Evolutionary algorithms (ES) <cit.> track a population of points across the input space. The population is modified based on some notion of mutation, crossover, and culling across iterative evaluations against the objective function. (ii) Bayesian optimization (BO) methods <cit.> build a suitable model of the objective function, based on already observed evaluations, and uses the model to acquire subsequent evaluation points in a manner that trades off exploration and exploitation. \n\n\nAlgorithm configuration (AC). Algorithm configuration (AC) <cit.>, also known as hyperparameter optimization (HPO) <cit.>, is a form of BBO where we want inputs that work well on average over a set of problem instances. Additionally, the importance of generalization is considered \u2013 the inputs optimized for the training set must also work well on a validation set of unseen problem instances. A typical approach is to treat the averaged objective function as the objective function and to solve the derived BBO problem, for which BBO techniques can be applied <cit.>.\n\nPlanner optimization problem (POP). Our POP formulation (<ref>) is effectively an extension of AC. In a POP, we seek to find a generator mapping problem instances to high-quality inputs for the objective function. This is in contrast to AC, which seeks only a single input that works well on average across the training set. Such conditional solutions are important in AI planning problems, where the optimal planning parameters depend critically on the problem instance. Different forms of POPs have only been explored recently by works such as <cit.>, which apply the Generator-Critic algorithm introduced in <cit.>. A deep neural network is used for the generator, which is trained with the help of a critic, another deep neural network. The critic is trained to model the response of the planner via supervision loss. It serves as a differentiable surrogate objective for the planning objective, enabling training gradients to flow into the generator for gradient-based updates. Through the POP formulation, we hope to unify the key ideas motivating these works.\n\nSoftware Frameworks. POPs require specialized components to support the notion of a generator and to integrate gradient-based deep learning techniques, such as those used in the aforementioned Generator-Critic algorithm. While software frameworks for BBO such as PyPop7 (ES) <cit.> and SMAC3 (BO) <cit.> exist and are actively maintained, rewiring them for POPs would require awkward hacks that would make usage unnatural. Thus, through OPOF, we hope to provide a software framework specially for POPs with intuitive and reusable interfaces, while harnessing the power of deep learning methods exposed via PyTorch <cit.>.\n\n\n\n\n\n\n\u00a7 OPOF: OPEN-SOURCE PLANNER OPTIMIZATION FRAMEWORK\n\n\n\n\n \u00a7.\u00a7 Domains: abstracting the planner optimization problem\n\n\nAt the core of OPOF lies the domain abstraction for specifying a planner optimization problem. The Domain interface consists of a minimal set of functions to specify the details of a given planner optimization problem \u2013 such as the training distribution, the parameter space, and the planner.\n\n\n\n\n\n \u00a7.\u00a7 Available domains\n\n\n\n\n\n\nDomains are implemented in external Pip packages, which are installed alongside OPOF. We provide a collection of domains (<ref>), which we expect to grow over time. At the time of OPOF's beta release, the following domains are available:\n\n\n  \n  * 2D grid world (<ref>). Small-scale toy domains with easy to understand behavior from classic, discrete planning. Serves as a sanity check against developing algorithms.\n  \n  * SBMP (<ref>). Domains from <cit.> to optimize hyperparameters, samplers, and projections for sampling-based motion planning. The motion planner is given a tight time budget (1 - 3 s), and requires effective parameters to plan reasonably well.\n  \n  * Online POMDP planning (<ref>). Domains from <cit.> to learn macro-actions for improving online POMDP planning. The POMDP planner is given a very tight time budget (100 ms), and discovering effective macro-actions is critical to plan reasonably well.\n\n\n\n\n\n \u00a7.\u00a7 Built-in algorithms\n\n\n\n\nOPOF contains built-in stable implementations of algorithms that solve the planner optimization problem, integrated directly into the core opof package. We expect this list to grow with time. \n\n\n\n\n\nGC is our implementation of the Generator-Critic algorithm introduced across the recent works of <cit.>. Two neural networks are learned simultaneously \u2013 a generator network representing the generator G_\u03b8(c) and a critic network. The generator is stochastic, and maps a problem instance c \u2208\ud835\udc9e to a sample of planning parameters x \u2208\ud835\udcb3. The critic maps c and x into a real distribution modeling f(x; c). During training, the stochastic generator takes a random problem instance c \u2208\ud835\udc9e and produces a sample x \u2208\ud835\udcb3, which is used to probe the planner. The planner's response, along with c and x, are used to update the critic via supervision loss. The critic then acts as a differentiable surrogate objective for f(x; c), passing gradients to the generator via the chain rule.\n\nSMAC is a wrapper around the SMAC3 package <cit.>, an actively maintained tool for algorithm configuration using the latest Bayesian optimization techniques. We use the HPOFacade strategy provided by SMAC3. In the context of the planner optimization problem, SMAC learns only a generator G_\u03b8(c) = \u03b8 that is unconditional (i.e. does not change with the problem instance) and deterministic (i.e., always returns the same planning parameters). While SMAC does not exploit information specific to each problem instance, it provides an approach that has strong theoretical grounding and serves as a reasonable baseline and sanity check.\n\n\n\n\u00a7 DESIGN DECISIONS\n\n\n\n\n \u00a7.\u00a7 Many domains and many algorithms\n\nOPOF is domain-centric \u2013 we only impose what a domain should look like, not how it should be solved. Such a choice maximizes the convenience of developing both domains and algorithms separately, by implicitly enforcing components to be reusable. One can easily reuse existing algorithms on a new domain of interest; similarly, one can also reuse existing domains to benchmark a new algorithm of interest.\n\n\n\n\n\n\n \u00a7.\u00a7 Portable and convenient packaging\n\n\nOPOF makes extensive use of Pip, the Python package manager, to allow external domains and algorithms to be packaged and distributed in a portable and convenient format. As such, users may, for example, easily install OPOF along with all available domain packages (at the time of OPOF's beta release) with: $ pip install opof opof-grid2d opof-sbmp opof-pomdp.\n\n\n\n\u00a7 FUTURE DIRECTIONS\n\n\nWe hope to apply/extend the POP formulation and the OPOF software framework in several ways.\n\n\n  \n  * Interesting domain applications. POP is a very general formulation that can be applied to a vast range of planners. With OPOF, we hope that this will ease such efforts for planning researchers interested in optimizing planners.\n  \n  * Developing general algorithms. Right now, there is only one practical algorithm, the Generator-Critic (GC) algorithm, which is designed specifically for planner optimization. We hope that OPOF will aid research efforts to produce better and more efficient algorithms.\n  \n  * Harder parameter classes. Existing domains have parameter spaces whose dimensions are reasonably small (<1000). We would like to explore how this framework scales to larger parameter spaces, and the domains for which this would be useful.\n  \n  * Applications outside of optimizing speed. The definition of the planning objective is general, and allows us to apply POP outside of improving planning speed. For example, can we use it to maximize some form of safety or exploration? Or, in an entirely different field such as HRI, can we instead seek to find internal parameters maximizing the observed behavior of some non-differentiable agent?  \n\n\n\n\n\u00a7 ACKNOWLEDGMENT\n\nThis research is supported in part by NSF RI 2008720 and Rice University funds; by Shanghai Jiao Tong University Startup Funding No. WH220403042; and by the National Research Foundation (NRF), Singapore and DSO National Laboratories under the AI Singapore Program (AISG Award No: AISG2-RP-2020-016).\n\n\n\n\n\n\n\n\n\n\u00a7 DETAILED DOMAIN DESCRIPTIONS\n\n\n\n\n \u00a7.\u00a7 Grid world\n\n\n\n\nOverview. Through the opof-grid2d package, we provide two relatively simple grid world domains (<ref>): RandomWalk2D[size] and Maze2D[size]. We emphasize that we want to discover high-quality planning parameters solely by interacting with the planner, where we treat the planner as a closed black-box function.  Specifically, we do not attempt to handcraft parameters specialized to the planner's operation. The goal is to be able to develop general algorithms for the broader planner optimization problem, using the grid world domains merely to test these general algorithms. These general algorithms would apply to harder domains for which the complexity of the planner prevents us from handcrafting specialized parameters. \n\nRandom walk. In the RandomWalk2D[size] domain (<ref>), an agent starts at a location (green) and moves in random directions according to some fixed probabilities until it reaches the goal (magenta). When attempting to move \u201cinto\u201d an obstacle (black) or the borders of the grid, a step is spent but the position of the agent does not change. The probability of moving in each direction is fixed across all steps. The planner optimization problem is to find a generator G_\u03b8(c) that maps a problem instance c (in this case, the combination of board layout and start and goal positions) into direction probabilities (in this case, vectors \u2208\u211d^4 with non-negative entries summing to 1), such that the number of steps taken during the random walk is minimized. The training set and testing set each contain 1000 problem instances, where the obstacle, start, and goal positions are uniformly sampled.\n\nMaze A* search. In the Maze2D[size] domain (<ref>), A* search <cit.> is run against a heuristic function h(n) to find a path from the start (green) to the goal (green). The heuristic function determines the priority in which nodes are expanded (cells that are in darker red have a lower g(n) + h(n) value, and have higher priority). The maze is assumed to be perfect, i.e., there is exactly one path between any two cells. The planner optimization problem is to find a generator G_\u03b8(c) that maps a problem instance c (in this case, the combination of board layout and start and goal positions) to h(n) (in this case, assignments of values \u2208 [0, size^2] to each of the size^2 cells) such that the number of nodes expanded in the A* search is minimized. The training set and testing set each contain 1000 problem instances, where the maze is generated using Wilson's algorithm <cit.>, and the start and goal positions are uniformly sampled.\n\n\n\n\n\n \u00a7.\u00a7 Sampling-based motion planning (SBMP)\n\n\n\n\n\nOverview. Through the opof-sbmp package, we provide the SBMPHyp[env,planner] domain (<ref>), which explores doing sampling-based motion planning in a specified environment using a specified planner. Unlike the grid world domains, these planners are much more complex and the relationship between the choice of planning parameters and planner performance is unclear. This makes it particularly suitable for OPOF, since we treat the planner as closed black-box function and specifically assume no knowledge of the planner's internals.\n\nThe planner optimization problem. The robot is tasked with moving its arm(s) from a start configuration to a goal configuration by running a sampling-based motion planner. The planner optimization problem is to find a generator G_\u03b8(c) that maps a problem instance c (in this case, the combination of obstacle poses in the environment and the start and goal robot configurations) to a set of planner hyperparameters (which depend on the planner used), such that the time taken for the motion planner to find a path is minimized. The training set and testing set contain 1000 and 100 problem instances respectively. These problem instances are adapted from MotionBenchMaker <cit.>. Obstacle positions are sampled according to a predefined distribution, while start and goal configurations are sampled using inverse kinematics <cit.> for some environment-specific task.\n\nEnvironments. We provide 3 environments at the time of writing. \nIn Cage, a 6-dof UR5 robot is tasked to pick up a block (green) in a cage (<ref>), starting from a random robot configuration (<ref>). The position and orientation of the cage, as well as the position of the block in the cage, are randomized. \nIn Bookshelf, a 8-dof Fetch robot is tasked to reach for a cylinder in a bookshelf (<ref>), starting from a random robot configuration (<ref>). The position and orientation of the bookshelf and cylinders, as well as the choice of cylinder to reach for, are randomized. \nIn Table, a 14-dof dual-arm Baxter robot must fold its arms crossed in a constricted space underneath a table and in between two vertical bars (<ref>), starting from a random robot configuration (<ref>). The lateral positions of the vertical bars are randomized.\n\nPlanners. We provide 2 planners at the time of writing. \nFor RRTConnect <cit.>, we grow two random search trees from the start and the goal configurations toward randomly sampled target points in the free space, until the two trees connect. We tune the following parameters: a range \u2208 [0.01, 5.00] parameter, which limits how much to extend the trees at each step; and a weight vector \u2208\u211d^50 with non-negative entries summing to 1, which controls the sampling of target points using the experience-based sampling scheme adapted from <cit.>. For LBKPIECE1 <cit.>, two random search trees are similarly grown from the start and the goal configurations, but controls the exploration of the configuration space using grid-based projections. We tune the following parameters: range  \u2208 [0.01, 5.00], border_fraction \u2208 [0.001, 1], and min_valid_path_fraction \u2208 [0.001, 1], which respectively determine how much to extend the trees at each step, how much to focus exploration on unexplored cells, and the threshold for which partially valid extensions are allowed; and a projection vector [0, 1]^2 \u00d7 d\u2282\u211d^2 \u00d7 d which corresponds to the linear projection function used to induce the 2-dimensional exploration grid, where d is the robot's number of degrees of freedom. \n\n\n\n\n \u00a7.\u00a7 Online POMDP planning\n\n\n\n\nOverview. Through the opof-pomdp package, we provide the POMDPMacro[task,length] domain, which explores doing online POMDP planning for a specified task using the DESPOT <cit.> online POMDP planner. The robot operates in a partially observable world, and tracks a belief over the world's state across actions that it has taken. Given the current belief at each step, the robot must determine a good action (which corresponds to moving a fixed distance toward some heading) to execute. \nIt does so by running the DESPOT online POMDP planner. DESPOT runs some form of anytime Monte-Carlo tree search over possible action and observation sequences, rooted at the current belief, and returns a lower bound for the computed partial policy. \n\nThe planner optimization problem. Since the tree search is exponential in search depth, POMDPMacro[task,length] explores using open-loop macro-actions to improve the planning efficiency. Here, DESPOT is parameterized with a set of 8 macro-actions, which are 2D cubic Bezier curves stretched and discretized into length number of line segments that determine the heading of each corresponding action in the macro-action. Each Bezier curve is controlled by a control vector \u2208\u211d^2 \u00d7 3, which determine the control points of the curve. Since the shape of a Bezier curve is invariant up to a fixed constant across the control points, we constrain the control vector to lie on the unit sphere. The planner optimization problem is to find a generator G_\u03b8(c) that maps a problem instance (in this case, the combination of the current belief, represented as a particle filter, and the current task parameters, whose representation depends on the task) to a joint control vector \u2208\u211d^8 \u00d7 2 \u00d7 3 (which determines the shape of the 8 macro-actions), such that the lower bound value reported by DESPOT is maximized. POMDPMacro[task,length] comes with 2 tasks at the time of writing.\n\nLight-Dark. In the LightDark task (<ref>), the robot (blue circle) wants to move to and stop exactly at a goal location (green cross). However, it cannot observe its own position in the dark region (gray background), but can do so only in the light region (white vertical strip). It starts with uncertainty over its position (yellow particles) and should discover, through planning, that localizing against the light before attempting to stop at the goal will lead to a higher success rate, despite taking a longer route. The task is parameterized by the goal position and the position of the light strip, which are uniformly selected.\n\nPuck-Push. In the PuckPush task (<ref>), a circular robot (blue) pushes a circular puck (yellow circle) toward a goal (green circle). The world has two vertical strips (yellow) which have the same color as the puck, preventing observations of the puck from being made when on top. The robot starts with little uncertainty (red particles) over its position and the puck's position corresponding to sensor noise, which grows as the puck moves across the vertical strips. Furthermore, since both robot and puck are circular, the puck slides across the surface of the robot whenever it is pushed. The robot must discover, through planning, an extremely long-horizon plan that can (i) recover localization of the puck, and can (ii) recover from the sliding effect by retracing to re-push the puck. The task is parameterized by position of the goal region, which is uniformly selected within the white area on the right.\n\nPOMDPMacro[task,length] differs from the other domain in that the distribution of problem instances \ud835\udc9f is dynamic. It is hard to prescribe a \u201cdataset of beliefs\u201d in online POMDP planning to construct a problem instance distribution. The space of reachable beliefs is too hard to determine beforehand, and too small relative to the entire belief space to sample at random. Instead, POMDPMacro[task,length] loops through episodes of planning and execution, returning the current task parameters and belief at the current step whenever samples from \ud835\udc9f are requested. Such a dynamic distribution adds noise to the training procedure, causing it to slow down. We believe that in the future, accounting for this noise will likely speed up training.\n\n\n\n\u00a7 EXPERIMENTAL CONFIGURATION\n\n\nWe run GC and SMAC against across ranges of domains. All experiments were done on a machine with an AMD 5900x (24 threads @ 3.70 GHz), an RTX 3060, and 32GB of RAM. The choice of planning objective and range of training domain configurations are detailed as follows:\n\n  \n  * RandomWalk2D[size]: The planning objective f(x; c) is given as - steps / (4 \u00d7 size^2), where steps is the number of steps taken to reach the goal. A maximum of 4 \u00d7 size^2 steps are allowed. We vary size = 5, 11, 21 and allow up to 200K planner calls for training. We use 8 worker threads for training.\n  \n  \n  * Maze2D[size]: The planning objective f(x; c) is given as - steps / n_empty, where steps is the number of nodes expanded before finding the goal and n_empty is the number of obstacle-free cells. We vary size = 5, 11, 21 and allow up to 150K planner calls for training.  We use 8 worker threads for training.\n  \n  \n  * SBMPHyp[env,planner]: The planning objective f(x; c) is given as -time, where time is the time taken for the motion planner to find a collision-free path from the start to goal robot configuration. We vary env = Cage, Bookshelf, Table with maximum allowed planning times of 1, 2, 3 seconds respectively, and vary planner = RRTConnect, LBKPIECE1. We allow up to 100K planner calls for training, and use 16 worker threads.\n  \n  \n  * POMDPMacro[env,planner]: The planning objective f(x; c) is given as the lower bound value reported by DESPOT, under a timeout of 100 ms. When evaluating, we instead run the planner across 50 episodes, at each step calling the generator, and compute the average sum of rewards (as opposed to considering the lower bound value for a single belief during training). We vary env = LightDark, PuckPush, using length = 8, 5 respectively, which are suggested to be the optimal macro-action lengths in <cit.>. We allow up to 500K and 1M planner calls respectively for training, and use 16 worker threads.\n\n\nWhen running GC, we update the generator for every sample of problem instance, planning parameters, and planner call. For SMAC, which does not condition the generator on the problem instance, we instead sample a set of planning parameters, test its average performance across 50 randomly sampled problem instances, and use the average result reported by the planner to update the generator. This is because SMAC's runtime grows cubically in the number of updates \u2014 this strategy allows us to use the same number of planner calls but limit the number of actual updates. We instead attempt to make each update more efficient by averaging the results of the planner calls to reduce noise.\n\n\n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n\n\nTraining results are shown in <ref>. After training, we evaluate the trained generators against the training problem distribution, and show the results in <ref>. We also show the key properties of each domain, including the dimensionalities of the space of problem instance and the space of planning parameters. \n\nConditioning improves performance. We note from <ref> and <ref> that in general, GC converges with better performance than SMAC. This is expected, since GC learns a conditional generator which can produce better parameters by exploiting information about the specific problem instance. On the other hand, SMAC maintains the AC formulation of optimization by looking for a single set of parameters that works well on average across the entire training set. It generally performs poorly, since it does not exploit information specific to each problem instance. In rare cases, SMAC is able to perform as well as GC (e.g. SBMPHyp[Bookshelf,LBKPIECE1] and POMDPMacro[PuckPush,5]), indicating that the additional information of individual problem instances do no provide any benefit to GC. SMAC occasionally outperforms GC (e.g. SBMPHyp[Cage,LBKPIECE1]), suggesting that constraining the solution space to be simple (i.e., to the set of unconditional generators) may make it easier to find a good solution.\n\nNot conditioning converges faster. From <ref>, SMAC generally converges faster (albeit with poorer performance). This is most noticeable in cases such as SBMPHyp[Table,RRTConnect], SBMPHyp[Bookshelf,LBKPIECE1], and POMDPMacro[PuckPush,5]. This is again expected, since the space of generators considered by GC is strictly larger than that of SMAC, increasing the exploration time needed for GC before converging. \n\nFundamental optimization limits. On the toy domains RandomWalk[size] and Maze2D[size], GC almost completely fails to improve performance once the domain size becomes large enough. This hints that GC has inherent limits with respect to the structure of the problem. Does it fail, because of sample complexity issues, or because of structural priors over the parameter space, or something else? Since these toy domains are relatively well-behaved and easy to understand, there is much room for potential theoretical analysis. \n\n\n1, 2 Terminated early after 58K and 500K planner calls respectively to keep training times reasonable. Due to the cubic \nnature of SMAC, the full duration would have required approx. 2.6 and 8.6 days respectively.\n\nlll\n    Training performance. We evaluate the generator at regular intervals across checkpoints, and plot the measured planning performance.\n    \n\n\n    \n    \n    < g r a p h i c s >\n\n       \n    \n    < g r a p h i c s >\n\n       \n    \n    < g r a p h i c s >\n\n    \n\n\n    1cRandomWalk2D[5]\n       \n    1cRandomWalk2D[11]    \n       \n    1cRandomWalk2D[21]\n    \n\n\n\n    \n    < g r a p h i c s >\n\n       \n    \n    < g r a p h i c s >\n\n       \n    \n    < g r a p h i c s >\n\n    \n\n\n\n    1cMaze2D[5]\n       \n    1cMaze2D[11]\n       \n    1cMaze2D[21]\n    \n\n\n    \n    \n    < g r a p h i c s >\n\n       \n    \n    < g r a p h i c s >\n\n       \n    \n    < g r a p h i c s >\n\n    \n\n    \n    1cSBMPHyp[Cage,RRTConnect]\n       \n    1cSBMPHyp[Bookshelf,RRTConnect]\n       \n    1cSBMPHyp[Table,RRTConnect]\n    \n\n\n\n    \n    < g r a p h i c s >\n\n       \n    \n    < g r a p h i c s >\n\n       \n    \n    < g r a p h i c s >\n\n    \n\n\n    \n    1cSBMPHyp[Cage,LBKPIECE1]\n       \n    1cSBMPHyp[Bookshelf,LBKPIECE1]\n       \n    1cSBMPHyp[Table,LBKPIECE1]\n    \n\n\n\n    \n    \n    < g r a p h i c s >\n\n       \n    \n    < g r a p h i c s >\n\n       \n    1c1*[55pt]\n    < g r a p h i c s >\n\n    \n\n\n    1cPOMDPMacro[LightDark,8]\n       \n    1cPOMDPMacro[PuckPush,5]\n       \n  \n\n  \n\n\n\n"}