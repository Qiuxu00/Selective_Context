{"entry_id": "http://arxiv.org/abs/2303.06944v1", "published": "20230313092248", "title": "A Human Subject Study of Named Entity Recognition (NER) in Conversational Music Recommendation Queries", "authors": ["Elena V. Epure", "Romain Hennequin"], "primary_category": "cs.CL", "categories": ["cs.CL"], "text": "\n\nContext-Aware Selective Label Smoothing for Calibrating Sequence Recognition Model\n    Shuangping Huang^1,2,  Yu Luo^1,  Zhenzhou Zhuang^1,  Jin-Gang Yu^1,2*,  Mengchao He^3,  Yongpan Wang^4\n    \n===========================================================================================================\n\n\nWe conducted a human subject study of named entity recognition on a noisy corpus of conversational music recommendation queries, with many irregular and novel named entities.\nWe evaluated the human NER linguistic behaviour in these challenging conditions and compared it with the most common NER systems nowadays, fine-tuned transformers.\nOur goal was to learn about the task to guide the design of better evaluation methods and NER algorithms.\nThe results showed that NER in our context was quite hard for both human and algorithms under a strict evaluation schema;\nhumans had higher precision, while the model higher recall because of entity exposure especially during pre-training; and entity types had different error patterns (e.g. frequent typing errors for artists).\nThe released corpus goes beyond predefined frames of interaction and can support future work in conversational music recommendation.\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nMusic recommendation systems (RSs), fundamental to streaming services nowadays, learn from user listening history or music content which artists or tracks to suggest next <cit.>.\nMost of these algorithms provide personalized music content to the users when logging in the streaming apps or websites, or when triggered with pre-defined utterances via voice assistants <cit.>. \nMore recent conversational RSs aim to help users to express their recommendation needs by supporting interactions via queries in natural language <cit.>.\nHowever, despite existing in the scientific literature, such conversational RSs are not widely deployed because of multiple issues, one being NER.\n\nThe processing of recommendation queries entails the extraction of named entity mentions <cit.>. \nThis sub-task faces multiple challenges, even when queries are framed as pre-defined utterances.\nThe transcriptions of the voice queries results in lower-case noisy text, often with misspellings <cit.>.\nThe lack of capitalisation in entities and misspelled words are often present in text-based queries too <cit.>.\nMusic entities, or those coming from the creative content domains, are highly irregular:\nthey do not follow inherent patterns as it is the case with people's names, and there is little to no separation between the vocabularies of entity and context words, especially for creative works <cit.> (e.g. common words like \"I\" or \"love\" in track titles).\nAlso, new music entities appear all the time.\nMajor music streaming services ingest one new track almost every second <cit.>.\n \n\nPrevious works have already shown that NER systems struggle with the aforementioned challenges <cit.>.\nThus, multiple approaches have been proposed to address them, either focused 1) on collecting more and relevant data for training / fine-tuning standard NER sequential models <cit.>; or \n2) on model's design choices that favour generalisation <cit.>.\nMost solutions focused on the latter objective have been motivated by the human NER linguistic behaviour, e.g. make the model rely more on context cues than on named entity mentions or learn from a few examples only, as humans do.\nHowever, apart from some scarce, partially related works <cit.>, there is no systematic investigation of how humans actually perform NER on noisy text with many new and irregular named entities.\nMoreover, in the case of music recommendation, we are not aware of any existing dataset of queries in natural language, annotated with named entities.\n\nThus, our goal is to investigate the human NER linguistic behavior when confronted with these challenging conditions.\nFor that, we create MusicRecoNER, a new corpus of noisy natural language queries for music recommendation in English that simulates human-music assistant interactions.\nWe then conduct a human subject research study to establish a human baseline and learn from it.\nFinally, we perform a detailed comparison of humans and the most popular NER systems nowadays, fine-tuned transformers, that covers multiple evaluation schemes (strict named entity segmentation and typing, exact segmentation only, or partial segmentation with strict named entity typing) and scenarios including entities previously seen or unseen by the model or humans.\n\nThe results showed that the task was challenging for humans.\nGiven an aggregated metric such as F1 score, human and algorithmic performances were on par.\nHowever, the detailed evaluation revealed that humans struggled more with recall while the best model with precision.\nThe high recall obtained by the model was partially a result of entity exposure during pre-training or fine-tuning.\nAlso, music entities had different error patterns and, in some queries, had ambiguous context that made their segmentation and typing quite hard.\n\nTo sum up, our research contribution[Code and data are available at <https://github.com/deezer/music-ner-eacl2023>.] are:\n\n    \n  * MusicRecoNER, a corpus of noisy complex natural language queries for music recommendation collected from human-human conversations in English, but which simulates human-music assistant interactions, annotated with Artist and WoA (work of art) entities. This dataset is not limited to pre-defined utterances as it would be the case if collected from interactions with conversational or voice assistants. Thus, it contains entities in diverse context, being also a useful resource for future work on conversational music recommendation. \n    \n  * A human subject study design for NER in noisy text with many new and irregular named entities. The proposed method is transferable to other creative content domains that face similar challenges to music such as books, movies, videos, but also to any other domain with scarce data, which wants to learn more about the NER task before building a system.\n    \n    \n  * An extensive music NER benchmark on noisy text which compares the performance of human versus automatic baselines under multiple evaluation schemes, scenarios and by controlling for the novelty of named entities.\n\n\n\n\n\u00a7 RELATED WORK\n\n\nAnalysing human and algorithmic performance was done for multiple NLP tasks in the past.\n<cit.> ran an annotation campaign on the GLUE benchmark with the goal to estimate the effort needed by existing models to catch up with the humans under limited-data regimes.  \n<cit.> conducted a large-scale human study on topic shift identification in order to discover patterns of disagreements and consolidate the evaluation metrics.\n<cit.> analysed the human behaviour for understanding ambiguous text-based or spoken sentences to guide the development of a machine learning system.\nMultiple machine translation works challenged the human parity claim <cit.> and proposed a secondary evaluation method to reveal detailed differences between humans or algorithms <cit.>.\n\nCompared to these, we benchmark humans and models on a different task\u2014named entity recognition, \nbut we share similar goals\u2014to estimate the human-algorithmic performance gap and to identify patterns that could support the design of better evaluation methods or automatic solutions.\nHuman annotation is frequent in NER especially when targeting a new domain such as archaeology <cit.>, or a new language such as Indonesian <cit.>.\nHowever, we are not aware of any annotated corpus of conversational queries for recommendation in the music domain.\nSome other related works propose corpora of noisy social media text containing new entities including irregular ones <cit.>, a noisy dataset of movie-related queries <cit.>, a dataset of music artist biographies annotated for entity linking <cit.>, or a corpus of tweets associated with a classical music radio channel <cit.>.\n\nPrevious works have showed that transformers fine-tuned for NER are strong baselines, especially when training data is scarce <cit.>.\nA more recent line of research employs these pre-trained models as few-short learners <cit.>.\nHowever, the results are still below those obtained with a fine-tuning approach.\nIn order to improve the bare-bone fine-tuned transformers, other works adopted distant supervision <cit.>, and the inclusion of gazetteers <cit.> or contextual triggers <cit.>.\nThough these solutions are interesting and relevant to our problem and context, in the current research, we want to rely on the results of this study before making any design choices for an advanced NER system in the music domain.\n\nWhen conducting human subject studies, the quality of annotations (inter-rater agreement or reliability) is often assessed with Kappa statistic or its variations <cit.>.\nYet, for NER, or more generally for labelling phrases, this statistic is less applicable as the number of negative cases on which it relies is ill-defined <cit.>.\nTo address this issue, multiple imperfect solutions have been proposed such as to compute the Kappa statistic at the token level <cit.>\u2014however, this does not reflect the task well as each token is not tagged individually; or to estimate the negative cases by enumerating all n-grams or noun phrases from a text\u2014however, this lacks accuracy <cit.>.\n<cit.> show that when the number of negative cases gets very large, the Kappa statistic approaches the F1 score.\nThus, F1 is considered a better metric, which we also adopt to measure the performance of humans and compare the NER human and algorithmic baselines.\n\n\n\n\n\u00a7 HUMAN SUBJECT NER STUDY\n\n\n\n\n \u00a7.\u00a7 Data Collection\n\n\nFor data collection we have chosen the music suggestions subreddit[<www.reddit.com/r/musicsuggestions/>] as a relevant data source.\nReddit is a discussion website where members can submit questions, share content and interact with other members. \nIt is organised in subreddits built around dedicated topics.\nEach discussion starts with an initial post that has a title and description. \nFrom this post, threads of conversations develop.\nWe were interested only in posts triggered by a music information seeking or recommendation need.\nWe  crawled the full subreddit with 8615 initial posts.\nThis number corresponds to the posts in the beginning of 2020. \nWe did not consider posts' comments. \n\nThese humans-to-humans posts asking for music recommendations are particularly relevant to study as they go beyond pre-defined frames of interaction with a text or voice-based assistant.\nHence, they exhibit a realistic human use of language, which although more challenging, could help with the development of the next generation of music assistants. \nFor NER, the existence of queries in natural language translates in a more diverse context surrounding named entities, thus in a higher query generalisation for music recommendation.\nBy manually checking this data, we noticed that many mentioned artists or music titles were not popular.\nThus, we expected most named entities to be new to the annotators, an aspect we wanted to control for, as mentioned in Section <ref>.\n  \n\n\n \u00a7.\u00a7 Data Cleaning and Pre-processing\n\n\nAs we aimed at creating a corpus of music recommendation queries simulating human-assistant interactions, we made multiple decisions to pre-process the collected posts.\nWe performed a manual cleaning of this data by removing those posts which directly shared music with the community; were aimed at promoting music or other music-related entities; contained explicit words; or contained only links to external music resources.\n\nThen, we focused on titles only as the post content was rather long, specific to asynchronous communication; \nas human-assistant interactions happen synchronously, the written or spoken queries are expected to be short, composed of a few short sentences at most <cit.>.\nWe removed all references to specific music-related services in order to obtain generic queries (e.g. we removed \"Youtube\" from the request \"music similar to my Youtube playlist\"). \nWe also removed words which were explicit markers of human-human interaction in order to ensure compatibility with human-assistant interaction. For instance, we removed phrases such as \"hello guys\" or \"could anybody\".\n\n\n\nWe performed the rest of the pre-processing steps to ensure that the queries contained, to some extent, the kind of noise that could be found in transcribed voice queries too, such as those obtained when interacting with a voice assistant.\nFor this, we transformed the text in lowercase and removed punctuation marks and emoticons (with some exceptions when the symbol was part of the named entity's pronunciation such as \"&\"). We kept content from parentheses when found at the end of a post title, otherwise we removed it. \nAlthough very common in automatic transcriptions, we did not introduce any artificial noise regarding the spelling of named entities.\nStill some noise was present as Reddit authors sometimes made misspelling errors.\nThese steps were done automatically. \nWe release both the original and pre-processed data.\nAll keywords used in the described steps are in Appendix <ref>.\nWe show multiple query examples in Table <ref>.\n\n\n\n \u00a7.\u00a7 Annotation Guidelines and Procedure\n\n\nWe sampled multiple subsets of 600 queries each from the cleaned and pre-processed corpus.\nThis number was established by estimating the required time for the experiment to be maximum 2 hours per annotator, based on an initial trial on 751 queries. \nThe annotation guidelines were also tested in the trial experiment and refined after.\nThe subjects were informed that the goal was to identify names of artists (e.g. bands, singers, composers) and titles of works of art (e.g. albums, tracks, playlists, soundtracks) in unformatted music-related queries. \nWe requested the annotators not to consult the Internet as we wanted them to rely on the query content only and on their own previous knowledge.\n\nWe then introduced the labels: Artist_known, Artist_deduced, WoA_known, WoA_deduced, and Artist_or_WoA_deduced with examples.\nThe last one was for ambiguous cases of named entity typing, but allowed the annotators to segment.\nSegmentation is still very relevant when parsing natural language queries for music recommendation as the type could be eventually disambiguated with the help of a search engine, for instance.\nThe other labels corresponded to Artist and WoA types, completed by whether the annotator knew the entity from before or deduced it from query's content, as we wanted to keep track of entity's novelty.\n\nThen, we introduced challenging annotation cases with guidelines on how to proceed.\nWe instructed the annotators to include Artist and WoA named entities from other domains too such as movies or video games, but to ignore all the other entity types such as countries or music genres; \nto consider the innermost entities in case of nested entities; \nto ignore implicit entities such as \"this singer\";\nto always include the \"'s\" from the possessive case as part of the named entity;\nand to consider a named entity with misspelled, translated and transliterated words as correct.\nThe final form of the guidelines is shown in Appendix <ref>.\n\nTen annotators (1 for the trial, and 9 for the main study) were recruited from our organisation with the condition to be fluent in English.\nEach set of 600 queries (DS1, DS2, and DS3) was given to three annotators.\nThe annotation campaign was performed using Doccano <cit.>.\nThe guidelines and the annotation tool were presented in a 30-minute workshop where annotators could ask questions. \nThey could consult the guidelines and  contact the researchers if they needed any clarification during the experiment too.\nAfter, one week was set aside for each annotator to complete the annotations individually.\n\n\n\n\n\n \u00a7.\u00a7 Ground-truth MusicRecoNER Corpus\n\n\nOften in related works, a ground-truth corpus is obtained by using full agreement or majority voting <cit.> (e.g. tag named entities on which at least two out of three human annotators agreed).\nHowever, here, because we wanted to establish a human baseline and have a corpus exhaustively annotated, we labelled the ground-truth corpus ourselves from scratch.\n\nCompared to the settings of the human subject study, we had access to the original Reddit post titles including capitalised text and punctuation.\nDuring the annotation, we used web and music streaming search engines to check if certain entities were Artist or WoA.\nThe full Reddit post was also used to disambiguate cases when a name could be both an Artist or a WoA.\nThe most challenging examples were discussed among us.\nThe ground-truth preparation together with the adjudication discussions happened over several weeks, as the process to disambiguate entities was more complex.\n\nStatistics about each dataset are presented in Part I of Table <ref>.\nArtist mentions are more common than WoA mentions.\nRegardless of the type, we could notice that a large majority of entity mentions are unique in each dataset.\nThe mean number of entity mentions per query is around 2, with the maximum varying between 6 and 10.\nFrom these, the proportion of queries with no entity is on average 56%.\n\n\n\n\u00a7 EVALUATION PROTOCOL\n\n\n\n\n\n \u00a7.\u00a7 Fine-tuned Transformer Baselines\n\n\nThe goals of the human subject NER study are to establish a human baseline on this challenging dataset of noisy queries for music recommendation and to learn from the human linguistic behavior in comparison to the most common NER systems nowadays, the fine-tuned transformers.\nWe consider three language models proven to have good results in various natural language tasks including language understanding, sequence labeling or text classification: BERT <cit.>, RoBERTa <cit.> and MPNet <cit.>.\n\nBERT <cit.> is a multi-layer bidirectional encoder based on the original Transformer architecture <cit.>.\nIt is pre-trained on: 1) the cloze task, i.e. to predict a masked token from the left and right context; and 2) next sentence prediction, i.e. to predict the next sentence from a given one.\nRoBERTa <cit.> has the same architecture as BERT, but incorporates multiple training steps proven to lead to an increased performance than the original model: the training of the model using more data, with larger batches, on longer sequences and for a longer time; \nand keeping only the cloze task as a pre-training objective while applying a dynamic masking schema to the input training data.\n\n\n\nMPNet <cit.> proposes a new pre-training objective by integrating the masked language modeling objective of BERT and the permuted language modeling objective introduced in XLNet <cit.>.\nThat is, it models the dependency among the masked tokens at prediction (i.e. takes into account the already predicted masked tokens to generate the current one), while providing visibility on the position information of the full sentence (i.e. the positions of the masked token and the next ones to be predicted). \n\nWe fine-tune the pre-trained versions of these models released in the huggingface transformers library <cit.> for token classification / sequence labeling.\nWe took the largest available version for each of them: bert-large-uncased, roberta-large, \nand mpnet-base.\nFrom all, only BERT is pre-trained on uncased text.\n\nDuring experiments, we noticed that the model initialisation had a large impact on the results.\nThis instability is well-documented in the past work, especially when the corpus for fine-tuning was small <cit.>. \nThus, to overcome bad initialisation and have more coherent results over different runs, we re-initialized the last layer of each pre-trained model.\nThis also led to faster convergence and more efficient fine-tuning.\nWe also tried to increase the number of the re-initialized layers to 2, but the results were similar or sometimes worse.\n\n\n\n \u00a7.\u00a7 Evaluation Metrics and Schemes\n\n\n\n\n\nPrecision (P), recall (R) and F1 are commonly used to evaluate automatic NER systems <cit.>.\n\nIn our evaluation, we extend these metrics to support a more detailed benchmark and understanding of the kind of errors a NER system makes.\nNamely, we also allow for a relaxed system's evaluation, when either segmentation or typing is correct, but not necessarily both.\n\nA NER system can produce various types of outcomes\n<cit.>. \nInspired by this and <cit.>, all NER outcomes, which we denote O, can be:\n\n\n    \n  * Correct outcomes (O_c): predicted and ground-truth entities match.\n   \n  * Missing outcomes (O_m): system entirely fails to spot a ground-truth entity.\n    \n  * Spurious outcomes (O_s): false entities are produced by the system. \n    \n  * Incorrect outcomes (O_i): predicted and ground-truth entities do not match because of either typing or segmentation errors.\n    \n\n\nTo classify the predictions of a NER system in these categories, we first need to fix an evaluation schema.\nThe most common one in the literature is the Strict match <cit.> when both segmentation and typing are correct.\nUnder the Strict schema, a prediction is incorrect when its boundaries were correct but not its type, or when its type was correct but not its boundaries.\nAll other cases (e.g. partial segmentation with incorrect type) are classified as spurious.\n\nThe Exact schema classifies a prediction as correct when its boundaries match those of the ground-truth, regardless of its type.\nIn contrast, the Entity schema classifies a prediction as correct when its type matches that of the ground-truth, regardless of its boundaries.\nFor these latter schemes, incorrect is adapted from its definition in Strict;\nmissed and spurious are the same too.\n\nWe use another class of outcomes, partial (O_p), only when computing the human performance.\nAs described in Section <ref>, humans could annotate a text as Artist_or_WoA_deduced.\nThus, whenever a human prediction had this label and matched exactly the boundaries of the ground-truth entity, partial was incremented and contributed to the final scores with a factor of 0.5 <cit.>, as follows:\n\n\n    R = (|O_c| + 0.5 * |O_p|) / (|O| - |O_s|) \n    \n         P = (|O_c| + 0.5 * |O_p|) / (|O| - |O_m|)\n\n\nWe exemplify the different outcomes under the mentioned schemes in Table <ref>.\n\nOne practical detail regarding the calculation of the evaluation metrics is that we had to apply some segmentation corrections before, to cover the situations when human annotations started or finished in the middle of a word.\nThis could appear because Doccano did not force automatically an alignment to a desired tokenization (entire words).\nThus, we corrected the start or end index of the concerned span by moving them to the left or right, based on a simple heuristic with regard to the closest found separating character (space or newline) to the concerned word.\nWe did not intervene when an entity was composed of multiple words and only a part of them were annotated, but we captured this type of errors with the used evaluation schemes.\nNo correction was needed in the case of model annotations as, during fine-tuning, we propagated the label of the first word token to the rest;\nhence, the labels were always consistent for all word tokens.\n\n\n\n \u00a7.\u00a7 Evaluation Scenarios\n\n\nWe explicitly consider the novelty of entities.\nIn the case of humans, this was encoded in the annotation process as we introduced the labels suffixed with _known. \nFine-tuned models could have seen music entities from the test set during pre-training, when they were exposed to a large amount of unlabelled data\n\nor during fine-tuning, if the train and test sets had common entities.\nWhile this latter exposure could be easily checked, the pre-training exposure is more challenging to assess as it requires access to the pre-training data or to find other ways to test exposure based on the model only <cit.>.\n\n\nThe solution we adopted targeted BERT, which performed on par with the other models as revealed in Section <ref>.\nBERT is pre-trained on Wikipedia and BookCorpus <cit.>.\nThus, music entities could be found more likely in the Wikipedia content.\nHowever, some music entities could be quite rarely mentioned in Wikipedia compared to others.\nTo quantify BERT's exposure to an entity e we used the following method. \nFirst, we tried to link each entity to Wikipedia by querying the Wikidata knowledge base <cit.>.\nWe re-ranked the returned results to give priority to music entities and returned the first entity whose type was in a pre-defined type list (see Appendix <ref>). \nSecond, we computed exposure by adapting the metric proposed by <cit.>:\n\n\n    expo(e) = {[ log|\ud835\udcae|-logrank(e)         e \u2208 Wiki.;                 0         e \u2209 Wiki.;                   ].\n\n\nwhere \ud835\udcae represents all Wikipedia named entities and the function rank considers entity popularity (higher the popularity, lower the rank).\nWe retrieve \ud835\udcae and entity counts from Wikipedia2Vec <cit.>. \nWe manually checked the linking for 300 random entities.\n82% were correct, either linked or not found on Wikipedia correctly.\n14% were linked to music-related entities but not the right ones and the rest were errors or missed entities.\nExamples of entities with high exposure values are: the beatles, elvis, pink floyd, metallica, drake, johnny cash, eminem, nirvana, and coldplay.\nWe could notice that all are of type Artist.\n\n\n\n\n\u00a7 RESULTS AND DISCUSSION\n\n\n\n\n\n\n\n\n\n        \n\n\n\n    \n\n\n\n        \n\n\n\n\n\n\n\n\nWe report scores using 4-fold cross-validation on the datasets presented in Table <ref>.\nMeans and standard deviations (std.) are computed over different folds, different initialisation seeds for the model, and different human annotators.\nIn most cases, this was over 12 data points as, for each model, the results were aggregated over each dataset as a test and 3 different initialisation seeds[All the models were trained and tested on the ground-truth datasets, and did not consider annotator-specific sets.] and \nfor the human evaluation, over each dataset as a test and 3 human predictors per dataset.\n\n\n\nWhen comparing BERT and the other models in Table <ref>, BERT and human baselines in Tables <ref> and <ref>, and results on Seen versus Unseen entities obtained either by humans or BERT in Table <ref>, scores in bold are statistically larger (p-value=0.05).\nWe test statistical significance with the Mann-Whitney U Test (Wilcoxon Rank Sum Test, ), which assesses under the null hypothesis that two randomly selected observations X and Y come from the same distribution.\n\n\n\n\n \u00a7.\u00a7 Fine-tuned Transformer Baselines\n\n\nTable <ref> shows that the fine-tuned BERT, pre-trained on uncased text, and MPNet yield the largest F1 scores for each entity type or overall.\nRoBERTa is statistically comparable and only marginally lower than the other models.\nAlthough MPNet and RoBERTa share the same pre-training corpus and the Transformer architecture, the addition of the permuting language objective to the cloze task gives a slight advantage to MPNet.\nWe use BERT for the rest of the experiments.\n\n\n\n \u00a7.\u00a7 Humans vs. Fine-tuned BERT\n\n\n\n\nTable <ref> shows that the performance of BERT is comparable to that of the human baseline in terms of F1 score.\nHowever, Table <ref> shows that humans and BERT perform differently in terms of precision and recall.\nHumans have a higher precision, for both Artist and WoA, whilst BERT has a marginal or significantly larger recall than humans, especially for Artist.\nWe confirmed that this phenomenon was not due to a particular precision / recall compromise by testing various precision / recall value and optimizing on F1.\nAlso, BERT has a lower precision than the recall, but we see the opposite for humans.\nConsidering Equations 1 and 2, the model appears to hypothesize spurious entities more often, while humans tend to miss entities more often.\n\nTable <ref> also shows that the F1 scores under Exact and Entity schemes are larger than under Strict as some of the errors produced are because of segmentation or typing.\nHowever, we can notice a different behaviour for the two entity types for both BERT and humans.\nIn the case of WoA, the Entity F1 scores are slightly larger than those obtained under the Exact schema, showing that boundary errors happen more frequently.\nOn the contrary, for Artist entities, the segmentation is more often correct, but the typing is wrong.\n\n\n\n \u00a7.\u00a7 Error Analysis\n\n\n\nFigure <ref>, showing a detailed error analysis, confirms that indeed BERT has more often spurious outcomes than humans, for both entity types.\nAlso, humans miss to annotate ground-truth entities more often than BERT.\nWe can equally observe that BERT is highly superior in identifying correct named entities.\nPrevious works on NER <cit.> have discussed that a system should learn to exploit the context  (i.e. the non-entity words) rather than entity memorisation to generalise.\nHowever, the high number of correctly recognised entities as well as the frequent spurious entities suggest that this may not be the case here; \nand BERT's behaviour may be linked to entity exposure. \nAs shown at the end of Section <ref>, the entities with the highest exposure score were of type Artist.\nWe could see in Figure <ref> that there are a lot more correct Artist entities, and the number of missed and spurious outcomes for Artist is lower, which seems to be aligned with our hypothesis related to entity exposure.\n\n\n\n \u00a7.\u00a7 Impact of Entity Exposure\n\nIn Table <ref>, Part II, we show the percentage of entities known by at least one annotator among the three in each dataset.\nThis varies between 24% and 30%.\nIn practice, each annotator has known at most this number of entities, which confirms that most entities from the collected corpus were new to our subjects.\nThe entity exposure is much larger for BERT.\nWhile the train and test sets share only maximum 15% of the entities, BERT has seen up to a half of corpus' entities during pre-training.\n\n\nTo check the model's performance on seen versus unseen entities, we show Recall scores for these groups in Table <ref>.\nSeen entities are those present in the train set or with expo(e)>1. \nUnseen entities have expo(e)=0 and are not known to humans.\nThe rest of the entities are discarded from the evaluation.\n\nBERT's recall on Seen is much larger than on Unseen, which confirms our hypothesis that memorisation plays a role.\nHowever, the model seems to rely significantly on context too given that the results on Unseen are still quite high.\n\n\n\nWe also report the results of humans in Table <ref> and see a similar pattern.\nAlthough the split is made considering the model's exposure, humans are also very likely to know entities from Seen.\nThe lower humans' scores on Unseen show that the recognition of these entities is quite challenging, possibly because of insufficient context.\nFor example, \"songs bands similar to sales getting it on off and on and porches mood\" contains an enumeration that is difficult to segment and type (Artist: \"sales\", \"porches\"; WoA: \"getting it on\", \"off and on\", \"mood\").\nAlso, entity typing is ambiguous in \"anything similar to some people say\" (WoA).\nFor these imperfectly recognised entities, including external resources such as gazetteers or search engines might be an option to explore.\n\n\n\n\u00a7 CONCLUSION\n\n\nIn this work, we investigated the human linguistic behavior when performing NER in the music domain.\nWe created MusicRecoNER, a new corpus of complex noisy queries for recommendations annotated with Artist and WoA entities.\nWe then designed and conducted a human subject research study to establish a human baseline and learn from its comparison with the most popular systems nowadays, fine-tuned transformers.\nWe performed a thorough evaluation covering multiple metrics, schemes and scenarios, including a careful analysis of the impact of entity exposure on results.\n\nThe results obtained by the algorithmic baselines were comparable to the human ones. \nYet, the detailed evaluation showed that humans yielded a better precision while the model had a better recall, linked also to entity exposure during pre-training and fine-tuning.\nThus, when evaluating fine-tuned pre-trained models, checking their performance on new entities shows their real generalisation ability.\nRegarding the NER evaluation protocol, human performances were much better under a more relaxed schema focused on segmentation or typing only.\nSuch a schema could prove a more realistic setup to aim to when training models too. \nAlso, we noticed that the relevant schema depended on the entity type as Artist was better segmented, while WoA better typed. \n\nContrary to previous claims, we show that, in our domain, NER in challenging conditions such as noisy text, and irregular or novel entities is rather hard for humans even when provided with complex instructions and multiple examples.\nThus, although we could learn from the human linguistic behaviour, we should not, by default, assume their results to be a target for any NLP problem. \nFor some tasks, it is common when establishing a human baseline to consider it as an upper bound for the model.\nThis is not necessarily a desirable outcome in our case as it would imply mislabelling 1/3 WoA entities.\nMore generally, as we also showed by studying the impact of entity exposure, algorithms can store a lot more knowledge than humans and one may want to leverage this as much as possible.\n\nAs for proposing a better system to perform music NER, one next step would be to continue the model's pre-training on more related data, in our case music, to get even more exposure, or to integrate gazetteers.\nStill, given the rate of new entities in our domain, forcing the model to rely more on context, when context is not confusing, is another desirable future direction.\nIn case of context ambiguity, asking questions to clarify the request and supporting user interaction in  natural language could be ultimately the answer towards a more suitable, but still very challenging solution.\nWe plan to explore these ideas as future work.\n\n\n\n\u00a7 LIMITATIONS\n\n\nWe further discuss the limitations of our work.\nThe corpus of noisy complex queries in natural language we use in the human subject study and we release is built based on a single source, Reddit.\nThe demographics of the users using Reddit are relatively narrow, with a majority being male, young, and educated[<https://foundationinc.co/lab/reddit-statistics/>]. \nMoreover, users seeking music recommendations on this type of forums may be rather \"music enthusiasts\" and may not represent regular music listeners.\nThe implications are that the language employed in these queries could be specific to this category of population.\nAlso, the mentioned entities could reflect the music taste of this type of profiles only.  \nThis latter implication turned to be an advantage for us as we ended up with many novel entities, unknown by the annotators who participated in the study.\nAs for the first implication, we manually checked the queries, and found them quite diverse, not necessarily using a specific vocabulary but more general language expressions. \nAn alternative to creating such a corpus could have been a Wizard of Oz experimental setup <cit.>.\nHowever, this would require significantly more costs and would highly depend on the type of profiles interested in participating in such a music discovery experiment.\n\nSecond, we pre-processed the corpus in order to simulate written or transcribed speech-based human-computer interactions.\nHowever, the steps we took may be largely insufficient to simulate the kind of noise found in transcriptions.\nAs we also discussed in Section <ref>, we did not inject any artificial noise for named entities, while spelling errors when automatically transcribing them are a common problem. \nAnother limitation regarding named entities is the computation of the model's exposure by leveraging Wikipedia.\nOur linking was quite rudimentary and imperfect, as we reported in Section <ref>.\nMoreover, for retrieving entity ranks, we used Wikipedia2Vec <cit.>, which is built on a slightly older Wikipedia version than the one BERT was trained on.\nTherefore, the results obtained by the model on the Unseen dataset may be slightly larger, as the model could have been exposed to some of these entities.\nHowever, our goal was to prove a phenomenon\u2014the impact of named entity exposure on the results, even if this impact may be marginally underestimated.\n\nFinally, the annotators recruited from our organisation have similar age and demographics.\nAlso they likely have a richer musical background compared to regular human subjects.\nThis signifies that, in reality, the number of novel entities could be higher, which could also impact the overall results obtained with the human baseline.\nNevertheless, this hypothesis could be tested only by running subsequent studies including more subjects.\n\n\n\n\u00a7 ETHICAL CONSIDERATIONS\n\n\nWe have provided most of the details about data collection, data cleaning and pre-processing, and the annotation procedure and guidelines in Section <ref> and Appendices.\nWe discuss further various ethics-related aspects not covered yet in the paper.\n\nThe dataset was gathered from the music suggestion subreddit via the Reddit API.\nAccording to the privacy policy of Reddit[<https://www.reddit.com/policies/privacy-policy>], third parties can freely access public content via the API.\nWe have not gathered any other information besides the public posts\u2014their titles and descriptions.\n\nAs previously mentioned, the annotators were recruited from our organisation.\nThey performed the annotation tasks during their regular paid hours.\nMoreover, the participation was fully on voluntarily basis, following a public call for participation by the authors within the organisation.\n\n\n\n\n\n\nacl_natbib\n\n\n\n\n\n\n\n\u00a7 DATA FILTERING KEYWORDS\n\n\nThe list of keywords used in the data cleaning and pre-processing steps are presented in Table <ref>. \nThese keywords were used to filter out posts, which were manually verified after. The outcome of the verification was either to exclude these posts from the data, or to keep the posts as they were or after having removed specific words (as described in Section <ref>). \nWe have considered both lower and upper case variations of each keyword. \n\n\n\n\n\n\u00a7 ANNOTATION GUIDELINES\n\n\n\n\n\n \u00a7.\u00a7 Introduction\n\nThe goal of this annotation experiment is to identify names of artists (e.g. bands, singers, composers) and names of works of art (e.g. albums, tracks, playlists, soundtracks, movies, video games) in music-related requests. \nThe requests could be single- or multi-line. Also, they are unformatted, meaning that they contain no capitalized letters or punctuation marks. Also contractions such as \"Artist\u2019s first album\", \"don\u2019t\" are written as if pronounced, specifically \"artists first album\" and \"dont\".\n\nThrough this experiment, we study how well humans can identify named entities (artists and works of art) in unformatted text by relying on the request content only, and on one's own knowledge.\nFor this reason, it is important that during annotation you do not consult the Internet to verify if some parts of text are named entities, but rely on your intuition after reading the text. \n\n\n\n \u00a7.\u00a7 Named Entity Categories\n\nThere are two categories referring to the entity type Artist; two categories referring to the entity type Work of Art (WoA); and one category for dealing with ambiguous cases as follows:\n\n\n\n  \n1. Artist_known. \nThis category should be used for sequences of words denoting an artist that is previously known to the annotator.\n\nIn the next request I recognize \"queen\" and \"the clash\" to be Artist entities because I knew them from the past:\ni like queen and the clash what else should i listen to.\n\nNote that when \"the\" is part of the name (e.g. \"the clash\"), it should be annotated likewise.\n\n\n\n  \n2. Artist_deduced This category should be used for sequences of words denoting an artist that is not known to the annotator, but deduced from the text. \n\nIn the next request I recognize \"stephan fort\u00e9\" to be an Artist: looking for something like the first album of stephan fort\u00e9.\n\nI have never heard of this artist before, but I deduced it from the request\u2019s content.\n\n\n\n  \n3. WoA_known\nThis category should be used for sequences of words denoting a work of art that is previously known to the annotator.\n\nIn the next request I recognize \"karma police\" to be a WoA because I knew it before: im a very picky music listener but i love karma police any other suggestions.\n\n\n\n  \n4. WoA_deduced\nThis category should be used for sequences of words denoting a work of art that is not known to the annotator, but deduced from text.\n\nIn the next request I recognize \"special affair\" to be a WoA: songs like special affair.\n\nI have never heard of this work of art before, but I deduced it from the request\u2019s content.\n\t\n\n\n  \n5. Artist_or_WoA_deduced\nThis category is used for sequences of words recognised to denote an artist or a work of art, but choosing between the two entity types is challenging.\n\t\nIn the next request I recognize \"superunloader\" to be either an Artist or a WoA: music like superunloader \n\nI have never heard of this before and it is difficult for me to distinguish between the two options.\n\n\n\n \u00a7.\u00a7 (Challenging) Examples\n\nPlease read the following examples carefully and re-consult them during the experiment whenever needed.\n\n\n\n  \nRelevant named entities not related to music.\nA text could contain other types of works of art such as movies or video games. Annotate these names using the category WoA. \nSimilarly, annotate with the Artist category movie directors, filmmakers, music composers and so on. \nAll the other types of named entities not related to Artist and WoA must be ignored (e.g. names of countries, music genres etc.).\n\nIn the example below, \"gemini\" is annotated as WoA and \"ang lee\" as Artist:\ni recently watched this film gemini made by ang lee and liked the soundtrack any similar recommendations of this.\n\n\n\n  \nMultiple named entities clearly delimited.\nA text could contain multiple entities which are clearly delimited by other words such as \"by\", \"from\", \"and\" etc. Please annotate all of them individually.\n\nIn the example below, \"heartbeat\" is a WoA and \"annie\" is an Artist: songs with similar vibe and structure as heartbeat by annie.\n\nIn the example below, \"hallelujah\" is a WoA and \"jeff buckley\" is Artist:\nother beautiful songs by jeff buckley apart from hallelujah.\n\n\n\n  \nMultiple named entities with no delimitation. A text could contain multiple entities which are not clearly delimited. Try to annotate each segment of text individually with its corresponding named entity category.\n\nIn the example below, if the annotator recognizes the entities, then 3 separate Artist entities should be selected, namely \"imagine dragons\", \"bastille\", and \"daya\": singers bands like imagine dragons bastille and daya.\n\nHowever, if not all entities are known from the past, then the unknown span of text could be annotated either as Artist_or_WoA_deduced, Artist_deduced or WoA_deduced depending on the content and the annotator's intuition. \nFor instance, if the annotator recognizes only \"imagine dragons\" but not the rest, then \"bastille and daya\" could be considered either 1 entity (\"bastille and daya\") or 2 entities (\"bastille\", \"daya\") and further annotated with any of the 3 categories mentioned above.\n\n\n\n  \nNamed entities collated with 's from the possessive case. \nIn this case, include the \"s\" as part of the named entity. \n\nIn the example below, \"toni braxton\" is the real name of the artist, but \"toni braxtons\" (coming from \"toni braxton's\") is actually annotated as an Artist entity:\nnewer 2005+ ballads in the style of toni braxtons un break my heart and stevie wonders all in love is fair.\nSimilarly for \"stevie wonders\" (coming from \"stevie wonder's\"). \n\n\n\n  \nNested named entities. \nA text could contain nested entities. \nThis means that there is a larger text segment that could be considered as an entity and a smaller text segment inside the larger one that could be also considered as an entity.\n\nIn this case, always favor the innermost text segment with an exception which is described below. Multiple examples are given further.\n\nIn the example below, \"treasure planet\" is annotated as WoA and not \"treasure planet soundtrack\" (which is also a WoA, but the innermost one is chosen):\nlooking for calm violin music very similar to the first 34 seconds of 12 years later from the treasure planet soundtrack.\n\nIn the example below, \"ezra collective\" and \"ty\" are annotated as 2 separate Artist entities and not as one: \"ezra collective feat ty\":\nrecommend me some good jazz hip hop songs with rap like chapter 7 by ezra collective feat ty.\nThere is also a third entity, \"chapter 7\", annotated as WoA):\n\nIn the example below, although \"i took a pill in ibiza seeb remix\" could be considered as a WoA, the innermost entities are annotated instead, namely \"i took a pill in ibiza\" as WoA and \"seeb\" as Artist: songs similar to i took a pill in ibiza seeb remix.\n\nException: if the name of a well-known band that you recognize is composed of 2 or more individual artist names, then annotate the band name using the category Artist_known. In the example below, I recognized that \"emerson lake and palmer\" is the name of a band despite the fact that it refers to three individual artists (\"emerson\", \"lake\", \"palmer\"): \nother artists similar to emerson lake and palmer.\n\n\n\n  \nExplicit versus implicit named entities. \nThere are cases when an Artist or a WoA are mentioned in text, but these entities are not explicitly named. For instance, neither \"the last album\", nor \"this singer\" are explicit named entities in the request below; hence they must not be annotated:\nshow me something similar to the last album of this singer.\n\n\n\n\n  \n(Incorrect) Variations of the original named entities. \nThe text may contain variations of the original names of the entities (including misspelled, missing, translated or transliterated words). \nNormally, in order to recognize an incorrectly written named entity, the named entity must be already known to the annotator. \nIn these cases, even if the named entity does not match exactly the real name, the annotator is required to annotate the corresponding span of text using the named entity categories ending with the \"_known\" suffix.\n \nIn the example below, the annotator recognizes \"hey ponchuto\" to be mistakenly written:\nfast dancey blues or songs like hey ponchuto from the mask.\nThe original named entity which the annotator knows from the past is \"hey pachuco\". \nThus, \"hey ponchuto\" is annotated as WoA_known. \nNote that \"the mask\" is a WoA too (a movie).\n\n\n\n\u00a7 PRE-DEFINED ENTITY TYPES FOR WIKIDATA LINKING\n\n\nIn order to ensure that the entity linking gives priority to music-related entities, we re-rank the returned results.\nSpecifically, we return the first entity whose type matches any of the criteria below:\n\n    \n  * Artist:  type matches exactly one of the following types\u2014musical group, rock group, supergroup, musical ensemble, girl group, or it contains one of the following strings\u2014band, duo, musician, singer.\n    \n    WoA:  type matches exactly one of the following types\u2014album, musical work/composition, song, single, extended play, or it contains one of the following strings\u2014album, song.\n\n\nIf the previous matching fails, the fallback is the first entity of type human for an Artist entity, or of type video or film for a WoA entity.\nIf none of these type criteria is met, then an empty string, corresponding to failed linking is returned.\n\n\n\n\u00a7 COMPUTATIONAL INFORMATION\n\n\nFor training and evaluation, we had a 32-core Intel Xeon Gold 6134 CPU @ 3.20GHz CPU with 128GB RAM, equipped with 4 GTX 1080 GPUs, each with 11GB RAM.\nFine-tuning a single model on three datasets from the four we annotated during 3 epochs and testing it on the hold-out dataset on a single GPU took about 2 minutes.\n\n"}