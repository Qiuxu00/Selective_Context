{"entry_id": "http://arxiv.org/abs/2303.06860v1", "published": "20230313050825", "title": "View Adaptive Light Field Deblurring Networks with Depth Perception", "authors": ["Zeqi Shen", "Shuo Zhang", "Zhuhao Zhang", "Qihua Chen", "Xueyao Dong", "Youfang Lin"], "primary_category": "cs.CV", "categories": ["cs.CV", "eess.IV"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n^1School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China\n^2Beijing Key Laboratory of Traffic Data Analysis and Mining, Beijing, China\n^3CAAC Key Laboratory of Intelligent Passenger Service of Civil Aviation, Beijing, China\n\n\nshenzeqi,zhangshuo,19281058,19281062,19281063,yflin@bjtu.edu.cn\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Light Field (LF) deblurring task is a challenging problem as the blur images are caused by different reasons like the camera shake and the object motion. The single image deblurring method is a possible way to solve this problem. However, since it deals with each view independently and cannot effectively utilize and maintain the LF structure, the restoration effect is usually not ideal. Besides, the LF blur is more complex because the degree is affected by the views and depth. Therefore, we carefully designed a novel LF deblurring network based on the LF blur characteristics. On one hand, since the blur degree varies a lot in different views, we design a novel view adaptive spatial convolution to deblur blurred LFs, which calculates the exclusive convolution kernel for each view. On the other hand, because the blur degree also varies with the depth of the object, a depth perception view attention is designed to deblur different depth areas by selectively integrating information from different views. Besides, we introduce an angular position embedding to maintain the LF structure better, which ensures the model correctly restores the view information. Quantitative and qualitative experimental results on synthetic and real images show that the deblurring effect of our method is better than other state-of-the-art methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView Adaptive Light Field Deblurring Networks with Depth Perception\n    \n Zeqi Shen^1,2, Shuo Zhang^1,2,3, Zhuhao Zhang^1, Qihua Chen^1,Xueyao Dong^1, Youfang Lin^1,2,3\n    Received: date / Accepted: date\n====================================================================================================\n\n\n\n\u00a7 INTRODUCTION\n\n\nThere are many reasons for image blur, such as camera shake and object motion. Image blur seriously affects the visual quality of images and the performance of some common computer vision tasks such as tracking\u00a0<cit.>, SLAM\u00a0<cit.>, and object detection\u00a0<cit.>.\nTherefore, how to remove the image blur effectively and efficiently has attracted extensive attention from researchers. In recent years, with the rapid development of deep learning, some well-designed single image deblurring algorithms\u00a0<cit.> have made remarkable progress. \nThe single image blur can be modeled as:\n\n    I_B(x,y) =\u222b_P^t_x,yI^t_S(x,y)  dt,\n\nwhere I_B(x,y) denotes the blurred image, I^t_S(x,y) denotes the sharp image, P^t_x,y denotes the motion path of the pixel (x,y) within the exposure time t. The goal of the deblurring task is to recover I_S from I_B.\n\nWith the maturity of commercial Light Field (LF) cameras, which can obtain images from different views through one shot, researchers and enterprises have paid more attention to Light Field images (LFs). \nLimited by hardware equipment, it is easy to cause image blur when shooting LFs, which limits the application scene of LFs. Therefore, the progress of LF deblurring task can promote the development of various LF applications such as virtual reality\u00a0<cit.>, salient object detection\u00a0<cit.>, 3D reconstruction\u00a0<cit.> and depth estimation\u00a0<cit.>. \n\n\n\n\nIn this paper, we use 4D function L(u,v,x,y)\u2208\u211d^U\u00d7 V\u00d7 X\u00d7 Y \nto represent LFs, where (u,v) and (x,y) are the angular and spatial coordinates, respectively. If we fixed (u,v), the Sub-Aperture Image (SAI) I_u,v can be obtained. If we fixed (x,y), the Micro-lens Images M_x,y can be obtained. \n\n\nDifferent from the single image blur, the LF blur can be modeled as:\n\n    L_B(u,v,x,y) =\u222b_P^t_u,v,x,yL^t_S(u,v,x,y)  dt,\n\nwhere L_B(u,v,x,y) denotes the blurred LFs, L^t_S(u,v,x,y) denotes the sharp LFs, P^t_u,v,x,y denotes the motion path of the pixel (x,y) in view (u,v) within the exposure time t. \nCompared with the single image, which has only spatial dimension (x,y), the LFs have extra angular dimension (u,v). Therefore, the blur phenomenon in LFs is more complex than the single image. Fig.\u00a0<ref> is an example of LF deblurring task. On one hand, when the light field camera moves, the motion path of each view relative to the object is different, so the blur degree of each view is also different. On the other hand, similar to a single image, objects with different depths produce different degrees of blur, i.e., blur is more serious in pixels with small depth and less in pixels with large depth.\n\nSingle image methods\u00a0<cit.> are possible ways to deal with LF deblurring task.\nHowever, for the LF blur is affected by the views and depth, if the single image methods are directly applied to the LF deblurring task, i.e., dealing with each view independently, their results cannot maintain the LF structure. Since the LFs are collected regularly in the angular dimension, the LFs have the consistent structure, i.e., the correspondence between different views.\nFurthermore, since the single image methods cannot effectively utilize the LF angular and depth information, their results are still blurred in some blur areas. Although the LFs with abundant angular information has greater potential to deal with this problem, LF deblurring methods\u00a0<cit.> are still in their infancy. Traditional methods\u00a0<cit.> are often time-consuming. Although the LF method\u00a0<cit.> based on deep learning process images quickly, they destroy the LF consistent structure, because the LFs are divided into multiple groups in their network.\nBy analyzing the current research status of the single image and LF methods, we summarize three urgent problems in the LF deblurring task:\n1) How to deal with the problem of different blur degrees in different views. 2) How to solve the different blur degrees caused by different depths in the same view. 3) How to maintain the LF consistency.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this paper, based on the above problems we find and summarize, we propose a novel end-to-end learning-based framework. For the different blur degrees in different views, our method calculates the exclusive convolution kernels for different views to perceive and deal with the corresponding blur. For the pixels with different blur degrees from different depths, the mixed range between the sharp pixel we want to recover and the other view pixels is different in the corresponding micro-lens image. Therefore, we designed a depth perception view attention for micro-lens images to pick out the sharp pixel information, which is mixed in other view pixels. Then the sharp images are restored pixel by pixel based on implicit depth information. Besides, an angular position embedding is applied to depth perception view attention to maintain the LF consistency.\n\n\n\n\n\nOur main contributions can be summarized:\n\n    \n  * We propose a novel view adaptive spatial convolution to deblur the different views, which automatically adapts to the blur degree of different views.\n    \n  * We design a novel depth perception view attention to restore sharp images pixel by pixel, which deals with the different blurs caused by different depths. Besides, the angular position embedding is applied to maintain the LF consistency. \n    \n  * Quantitative and qualitative experimental results on synthetic and real images show that our method is superior to other state-of-art methods.\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\nThis section will introduce the related works from single image and LF methods. Since the application scenarios of the blind methods are more widely than unblinded methods, \n\nwe only analyze the blind deblurring method that the blur kernel is unknown.\n\n\n\n \u00a7.\u00a7 Single Image Deblurring Method\n\nIn the past few decades, single image deblurring task has been studied extensively and remarkable achievements have been achieved. Traditional methods\nremove image blur by various prior knowledge such as color\u00a0<cit.>, gradient \u00a0<cit.>, dark channel \u00a0<cit.>,and L0 regularization\u00a0<cit.>. \nHowever, these methods are often computationally complex and cannot achieve satisfactory results on complex real datasets.\n\n\u00a0<cit.> are the  pioneers to solve the deblurring problem based on deep learning, and made remarkable progress. Following these pioneers, multi-scale convolution\u00a0<cit.> and Recurrent Neural Networks\u00a0<cit.> are applied to model the spatial variation blur kernel in dynamic scenes. Inspired by the success of Generative Adversarial Networks, \u00a0<cit.> generated the sharp images more in line with human visual perception.\n\nAlthough the single image methods are very effective for processing single image deblurring, the LF blur is different from single image. The single image methods have some limitations in processing LFs: 1) The single image methods do not use the abundant angular information in LFs, so that the depth information is missing during the deblurring processing. By contrast, our LF method makes full use of depth information by combining complementary information from all views in LFs, so that the blur boundaries in different depths can be better handled. 2) Using the single image methods to deal with each view separately cannot maintain the LF angular structure very well, which will break the corresponding relationships between views in LFs. By contrast, we process all views at one time, which maintains the LF structure well.\n\n\n\n\n\n \u00a7.\u00a7 LF Deblurring Method\n\nThe LFs belong to multi-images, but it is different from other multi-image types such as video and stereo images. Although these methods\u00a0<cit.> are excellent in dealing with blur in their field, we do not describe them in detail for their image types are different with us.\n\n\nWith the maturity of the commercialization of light field cameras, a variety of applications based on light fields have been brought. \nAlthough LFs have been widely studied in the fields of super-resolution\u00a0<cit.> and depth estimation\u00a0<cit.>, there is little research on LF deblurring.\n\nDansereau et al.\u00a0<cit.> first proposed the non-blind LF deblurring method based on the known camera motion. Srinivasan et al.\u00a0<cit.> are the first to model the 3-DOF camera motion and proposed a blind LF deblurring method on a 3-DOF dataset. \nDifferent from 3-DOF camera motion, 6-DOF methods \u00a0<cit.> deal with the LF deblurring task on the 6-DOF motion. However, the computational complexity of these traditional methods is too high, and the deblurring effect is seriously limited by the estimation of camera motion. \n\n\n\n\nThe existing LF deblurring method \u00a0<cit.> based on deep learning does not make full use of the LF spatial and angular structure. Since the LFs are divided into multiple groups, the LF structure is destroyed. Different from them, the LFs are input as a whole in our method. Therefore, the restoration results of all views are obtained only by one calculation, which effectively maintains the LF structure.\n\n\n\n\n\n\n\n\n\n\n\u00a7 MOTIVATION\n\n\n\nInspired by stereo deblurring scenes\u00a0<cit.>, we found there are two characteristics of blurred LFs, which are useful for the LF deblurring task. On one hand, the blur degree of the same object is different in different views. On the other hand, the blur degree of different objects with various depths is also different. Fig.\u00a0<ref> is a schematic diagram of blurred LFs to explain the two characteristics. For convenience, we only show two cameras in Fig.\u00a0<ref>, which can be easily extended to the case of LF.\n\nIn Fig.\u00a0<ref> (a), the building is on the same vertical line as one of the cameras and the light emitted from the building reaches the image plane after passing through the camera lens. We suppose the camera moves in a simple straight line along the Y-axis. As the two cameras move along the Y-axis, the position that the light of the building reaches on the left image plane x1 does not change, but on the right image plane, the position change from x3 to x2. For more complex rotational motion, this phenomenon will be more obvious. \nTherefore, the blur degree of the building in different views is different since the different relative motion between the different view cameras and the object.  \nIf we share the parameters of the convolution kernel for all views when designing the model, the model uses the same convolution kernel to deal with different blur degrees, which is unreasonable. \nNoting this, we design a novel view adaptive spatial convolution, which dynamically calculates the exclusive convolution kernel for each view without significantly increasing the parameters. The detail is introduced in Sec.\u00a0<ref>.\n\nIn Fig.\u00a0<ref> (b), the person and the building have different depths. As the camera move along the X-axis, the position that the light of the building reaches on the image changes from x1 to x5, while the position that the light of the person reaches on the image changes from x4 to x1. As we can see from Fig.\u00a0<ref> (b), x5-x1 is less than x1-x4, which means that when the camera moves, the blur degree of the objects with different depths is different. Generally, the blur degree of objects with a small depth is large, and the blur degree of objects with a large depth is small. \n\nTo make full use of the LF angular information, we introduce the micro-lens image. In the micro-lens image, the sharp pixel we want to recover is mixed with other view pixels. For the different blur degrees brought by depth, the mixed range between the sharp pixel and other pixels is also different. If the blur degree of the sharp pixel is serious, the mixed range is large, otherwise, it is small. When restoring a sharp pixel, we should focus on the positions where the sharp pixel is mixed.\nTherefore, we design a depth perception view attention to find the area where the sharp pixels are mixed. Then the sharp images are restored pixel by pixel by fusing the corresponding weighted micro-lens image features. The detail is introduced in Sec.\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 OUR METHOD\n\n\nThe overview architecture of our proposed method is shown in Fig.\u00a0<ref>. The blurred LFs are first fed into the View Adaptive Spatial Convolution (VASC) module, which dynamically calculates the exclusive convolution kernel for each view.\nAfter that, the sharp LFs can be reconstructed pixel by pixel through the Depth Perception View Attention (DPVA) module, which calculates different attention weights for objects with different depths to pick out the sharp pixel information in the corresponding micro-lens images.\nBesides, to maintain the consistency of LFs, the Angular Position Embedding (APE) module is introduced into DPVA module.\n\n\n\n\n\n \u00a7.\u00a7 View Adaptive Spatial Convolution\n\n\nAs mentioned in Sec.\u00a0<ref>, the blur degrees of the same object are different in different views. \nThere are two intuitive ways to deal with it.\n1) The model ignores the blur degrees difference of different views and shares the same convolution kernel for all views. 2) The model simply assigns a convolution kernel to each view.\nFor the first way, if all views share the same convolution kernel, it is difficult for a model to perceive different blur degrees. The lack of the ability to perceive the blur degrees will lead the model to deal with the average blur degree of all views. For the second way, if simply assign a convolution kernel to each view when building the model, the number of model parameters will increase exponentially. For example, assuming that the parameter required to process one view is C_in\u00d7 C_out\u00d7 k \u00d7 k, where C_in, C_out, and k denotes the number of input channels, the number of output channels, and the convolution kernel size, respectively.\nTherefore, the parameter required to process N_view views according to the above method is N_view\u00d7 C_in\u00d7 C_out\u00d7 k \u00d7 k.\nIn order to enable the model to perceive the difference of blur degrees between different views without significantly increasing the number of parameters,\nVASC module is cleverly designed, which is shown in Fig.\u00a0<ref>. \n\n\n\n\n\n\n\n\n\nFirstly, the blurred LFs L_B(u,v,x,y) are input into the VASC module in the form of SAIs. Then the exclusive convolution kernel for each view are generated.\nTake one view as an example, the SAI features are fed into one average pooling and two full connections to generate the corresponding view adaptive convolution kernel W_u,v\u2208\u211d^k\u00d7 k.\nThen our method extracts the view adaptive spatial features based on spatial convolution by W_u,v.\nFinally, the all view adaptive features F_VA\u2208\u211d^U\u00d7 V\u00d7 X\u00d7 Y\u00d7 C are fused through angular convolution.\n\n    F_VA(u,v,x,y) =H_VASC(L_B(u,v,x,y)).\n\nThe number of VASC module is C_in\u00d7 C_K + C_K\u00d7 C_K +C_K\u00d7 C_in\u00d7 C_out\u00d7 k \u00d7 k, where C_K denotes the number of nodes in the two full connection layers. In this paper, C_K =4 < N_view=25, which denotes that our model has fewer parameters. For more details, please refer the codes in the supplementary materials.\n\nBy using the plug and play VASC module iteratively, our model not only effectively perceives the difference of blur degrees in different views but also does not significantly increase the number of parameters.\n\n\n\n\n\n\n \u00a7.\u00a7 Depth Perception View Attention\n\n\nAs mentioned in Sec.\u00a0<ref>, the blur degree of objects with different depths in the same view is different. To make full use of the LF angular information, we introduce the micro-lens image, where the the sharp pixel we want to recover is mixed with other pixels from other views.\nSince the different blur degrees, the mixed range in micro-lens images is also different.\nTherefore, we designed the DPVA module to find the mixed area for the different blur degrees pixels and then pick out the sharp pixel information based on implicit depth information, which is shown in Fig.\u00a0<ref>.\n\n\n\n\n\n\n\n\n\nSpecifically, the view adaptive features F_VA from the VASC module are fed into two branches. \nIn the lower branch, our method generates the view exclusive LF features F^ve_u,v\u2208\u211d^ X\u00d7 Y\u00d7 UVC for all views, through a 2D convolution layer that the output channel number is U\u00d7 V times larger than the input channel number.\n\n\n\n\nIn the upper branch, the view adaptive features F_VA are fed into another 2D convolution layer to change channel dimension from C to UV, then U\u00d7 V depth perception features F^dp_u,v\u2208\u211d^X\u00d7 Y\u00d7 UV can be obtained. \nTo maintain interaction between all views,\nwe reorganize all depth perception features as F^ndp_u,v\u2208\u211d^X\u00d7 Y\u00d7 UV.\nTaking the (u,v) view as an example, the uv-th channel dimension of all depth perception features F^dp_u,v is connected as the feature F^ndp_u,v. \nTo further maintain the consistency of LFs, the APE module is applied to the DPVA module to generate the depth perception features with angular position embedding F^adp_u,v, which is introduced in Sec.\u00a0<ref>.\nAfter that, the F^adp_u,v are fed into several full connection layers to get Depth Perception View Attention Weight W^dp_u,v\u2208\u211d^ X\u00d7 Y\u00d7 UVC, which denotes the whether the position in the micro-lens image has information that needs to be restored.\n\n\nThen, the sharp features F^sharp_u,v\u2208\u211d^ X\u00d7 Y\u00d7 C can be obtained by the view exclusive LF features F^ve_u,v and the View Attention Weight W^dp_u,v:\n\n    F^sharp_u,v(x,y,c) =\u2211_\u00fbv\u0302F^ve_u,v(x,y,\u00fb*v\u0302*c)* W^dp_u,v(x,y,\u00fb*v\u0302*c).\n\n\nFinally, the sharp LFs L_S can be generated from the sharp features F^sharp through a 2D convolution. \n\n    L_S(u,v,x,y) =H_DPVA(F_VA(u,v,x,y)).\n\n\nOur method takes the whole LFs as the input and uses and maintains the LF spatial and angular structure as much as possible. On the contrary, the past deep learning LF methods only input part of the LFs into the network at a time. They not only cannot make full use of the LF structure but also need multiple calculations to obtain the results of all views.\n\n\n\n\n \u00a7.\u00a7 Angular Position Embedding\n\n\nWithout the angular position embedding, the model cannot know which view is being processed. The lack of positional embedding causes the model to tend to handle the average of all views, which destroys the LF structure.\n\nTherefore, the APE module is necessary for the DPVA module, which can be seen in Fig.\u00a0<ref>. Since the angular dimension of the LFs is fixed when acquiring the LFs, we directly use the absolute position as position embedding. Taking the (u,v) view as an example, the angular coordinates (u,v) is concatenated with F^ndp_u,v to get F^adp_u,v\u2208\u211d^X\u00d7 Y\u00d7 (UV+2), which changes the channel dimension from U\u00d7 V to U\u00d7 V + 2.\n \n\n    F^adp_u,v(x,y)=H_APE(F^ndp_u,v(x,y),(u,v)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\nIn this section, we first introduce the dataset and training details. Then the proposed method is compared with other single image methods and LF methods on synthetic and real images quantitatively and qualitatively. Next, the ablation study is taken to prove the effectiveness of the designed modules. Finally, the limitation of our method is discussed.\n\n\n\n \u00a7.\u00a7 Training Details and Datasets\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Dataset.\n\nThere are two main kinds of datasets (3-DOF and 6-DOF) in the LF deblurring task. In the 3-DOF dataset, the camera shifts in X, Y, and Z directions. Besides the 3-DOF motions, the camera rotates in X, Y, and Z directions in the 6-DOF dataset. In this paper, we trained models on 3-DOF and 6-DOF datasets separately, since the camera motions are different.\nWe picked 200 LFs from Stanford Lytro \u00a0<cit.> as a 3-DOF training dataset and 40 LFs as a 3-DOF test dataset. The simulation method of blurred images is consistent with that of \u00a0<cit.>. Since\u00a0<cit.> only published 40 images of 6-DOF datasets. We selected the 3 images used in\u00a0<cit.> as the 6-DOF test dataset and others as the training dataset.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Training Details.\n\nIn this paper, we crop the training LFs into 5\u00d75\u00d764\u00d764\u00d73 patches. Image flipping and image rotation are applied as data augmentation. We use RGB images as input and output. The angular size of input and output are both 5\u00d75. The number of VASC modules is 8.\nThe proposed model is optimized using the Adam\u00a0<cit.> algorithm with a batch size of 4. The initial learning rate is set as 1e-3 in the first 200 epochs and is then divided by 10 every 100 epochs. We implement the model with the PyTorch framework and the training process roughly takes 2 days for the 3-DOF dataset and 1 days for the 6-DOF dataset with 4 Intel(R) Xeon(R) CPU E5-2683 v3 @ 2.00GHz with a Titan XP GPU.\n\n\n\n  \u00a7.\u00a7.\u00a7 Loss Function.\n\nOur method uses the L1 Loss as supervision:\n\n\n    Loss_L1= L_S-L_gt_1,\n \nwhere L_gt denotes the ground truth.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Quantitative Comparison\n\nWe compared two single image methods DMPHN\u00a0<cit.> and MPRNet\u00a0<cit.> with excellent results in the field of single image deblurring.\nAlthough there are some deblurring methods based on LFs, most of them do not publish codes and datasets\u00a0<cit.>. Fortunately, LFBMD\u00a0<cit.> opened their codes so that we can easily compare with them.\nSince the LF super-resolution methods also explore the LF spatial and angular information, which is conducive to deblurring, we modify three LF super-resolution methods SAS\u00a0<cit.>, InterNet\u00a0<cit.>, and MEGNet\u00a0<cit.> for LF deblurring task.\n\n\nTab.\u00a0<ref> is the quantitative results of 3-DOF and 6-DOF test datasets. The PSNR, SSIM, NCC, and LMSE are used for the evaluation index. The higher the values of PSNR, SSIM, and NCC denote the better results. LMSE is the opposite. Tab.\u00a0<ref> shows our method is much better than other single image and LF methods in all evaluation indexes on 3-DOF test datasets.\nOn the 6DOF test dataset, our method also achieved the best results on PSNR and LMSE, and other evaluation indexes also achieved comparable results.\nBecause the single image methods DMPHN\u00a0<cit.> and MPRNet\u00a0<cit.> process each view alone, they cannot effectively use the angular information of the LFs, i.e., they cannot obtain complementary information from other views. On the contrary, our LF method takes the whole LFs as the input, which fully explores and makes use of the angular and spatial information, so it achieves a better restoration effect.\nFor other LF methods LFBMD\u00a0<cit.>, SAS\u00a0<cit.>,    InterNet\u00a0<cit.>, and MEGNet\u00a0<cit.>, they also use the angular information of LFs, but they do not consider that the blur degree is affected by the views and depth, so they cannot perceive the difference of views and depth well. Different from them, our method designs the VASC module and DPVA module to deal with the different blur degrees in different views and depths, which makes our method more effective.\n\n\n\nTab.\u00a0<ref> shows the average time of processing a 5 \u00d7 5 \u00d7 500 \u00d7 336 LFs by various methods and the parameters of various models.\nFor the single image methods DMPHN\u00a0<cit.> and MPRNet\u00a0<cit.>, time is the sum of all views.\nLFBMD\u00a0<cit.> is a traditional method that the parameter is not displayed. LFBMD\u00a0<cit.> needs to predict the motion trajectory of the camera first and needs a complex optimization process, so it is very time-consuming to process an LF, which greatly limits the application scenarios. \nThe time consumption of LF methods based on deep learning is similar to that of single image methods.\nHowever, the parameters of the LF methods are much smaller than those based on single image. Specifically, the parameters of our method are 30 times less than MPRNet\u00a0<cit.> and 33 times less than DMPHN\u00a0<cit.>.\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Qualitative Comparison\n\nFig.\u00a0<ref> is the qualitative comparison with single image methods. We show the result of the central view and enlarge some areas to make the comparison more obvious. In Fig.\u00a0<ref>, our results are more clear than the results of the single image methods, and the details are better preserved. To better show the performance of various methods for maintaining the LF structure, we introduce the Epipolar Plane Images (EPIs). In EPIs, the slope of the line segment represents the disparity between two adjacent views. If the LF structure is maintained well, the line segment will not bend or break. \nSince the single image methods process each view separately, there are bending and fracture in the EPIs of the single image methods in Fig.\u00a0<ref>. On the contrary, our method takes the LFs as a whole and introduce the APE module to embed angular position, the line segment in our EPIs is a straight line, which fully proves that our method can maintain the structural consistency of LFs.\n\nFig.\u00a0<ref> is the qualitative comparison with LF methods on the test dataset.\nFig.\u00a0<ref> shows that the result of LFBMD is still blurred and is easy to cause color deviation. This is because LFBMD needs to manually set multiple hyperparameters, which limits the application scenario. When the scene becomes complex, the setting of hyperparameters will not be accurate enough, which will easily lead to the deterioration. \nDifferent from it, the LF methods based on deep learning do not need to set complex hyperparameters, since they learn automatically from a large amount of data.\nBecause these LF methods SAS\u00a0<cit.>, InterNet\u00a0<cit.>, and MEGNet\u00a0<cit.> are not specially designed for deblurring tasks, they cannot perceive the different blur degrees in different views and depths. Compared with our methods, the results of these methods have some obvious artifacts, which can be seen in the enlarged image.\n\nFig.\u00a0<ref> is the qualitative comparison of real\nscenes. Since there is no published real LF blur dataset, we use a Lytro Illum LF camera to obtain the real blur scene for visual comparison. Our results are sharper than the single image methods, which shows our method is more robust in real scenes.\n\n\n\n\n\n \u00a7.\u00a7 Ablation Study\n\n\n\n\n\nTo verify the effectiveness of our method, we did some ablation studies on the 3-DOF test dataset.\nTo ensure the fairness of the ablation study, we keep the parameters of the models almost the same. The experimental results are shown in Tab\u00a0<ref>. \nFirst, we replace the VASC module with the ordinary convolution (w/o VASC) which is shared by all views. The significant decline in the evaluation indexes shows that ordinary convolution cannot learn the prior knowledge that the blur degree of different views is different. On the contrary, the VASC module calculates an exclusive convolution kernel for each view effectively.\nThen the DPVA module is removed completely (w/o DPVA) to verify it can generate the different view attention weights for regions with different depths. The experimental results in Tab.\u00a0<ref> show that the DPVA module improves the performance by fusing the angular information in the micro-lens image.\nWithout the APE module (w/o APE), the performance decreases significantly. This is because the APE module helps to maintain the LF structural consistency. Besides, if there is no APE module, the DPVA module cannot realize what information needs to obtain from the micro-lens image for the specific view, which explains why the performance without APE module is worse than that without DPVA module.\nFig.\u00a0<ref> shows the error map of ablation study. We can see that no matter VASC module, DPVA module, or APE module is missing, there will be more error areas, which shows the effectiveness of the structure we designed again. Our method fully explores the LF blur characteristics by the VASC and DPVA module. Therefore, our results are sharper and the details are restored more perfectly.\n\n\n \u00a7.\u00a7 Limitation\n\n\nFig.\u00a0<ref> is a failure case of all single image and LF methods. In this scene, blur is very serious in all views. Our LF method cannot effectively obtain useful information from other views to restore the sharp image. Therefore, the ability of deblurring degenerates to the single image deblurring methods. Nevertheless, the detail of our result is still better than the single image methods, especially the restoration of trunk texture.\n\n\n\n\n\u00a7 CONCLUSIONS\n\nIn this paper, we proposed a novel LF deblurring method based on deep learning. \nFirst, the VASC module is designed to effectively deal with different blur degrees in different views by calculating an exclusive convolution kernel for each view. Besides, we designed a DPVA module to deal with the different blur degrees in different depths. \nThrough this module, the sharp results are recovered by the weighted micro-lens image based on implicit depth information.\nFurthermore, the APE module is also applied to maintain the overall consistency of the LFs. \nOur LF method takes the whole LFs as a whole and utilizes the complementary angular information, which makes it to better maintain the structural consistency of the LFs than the single image methods.\nQuantitative and qualitative experimental results show that our method is much better than other single image and LF deblurring methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nACM-Reference-Format\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}