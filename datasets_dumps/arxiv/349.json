{"entry_id": "http://arxiv.org/abs/2303.06841v1", "published": "20230313041533", "title": "Learning Transductions and Alignments with RNN Seq2seq Models", "authors": ["Zhengxiang Wang"], "primary_category": "cs.CL", "categories": ["cs.CL"], "text": "\n\n\n\n\n\n\n\n\nLearning Transductions and Alignments \n with RNN Seq2seq Models\n    Zhengxiang Wang zhengxiang.wang@stonybrook.edu \n\n       Department of Linguistics\n\n       Stony Brook University\n\n       Stony Brook, NY 11794, USA\n    Received: date / Accepted: date\n=======================================================================================================================================================\n\n\n\n\nThe paper studies the capabilities of Recurrent-Neural-Network sequence to sequence (RNN seq2seq) models in learning four string-to-string transduction tasks: identity, reversal, total reduplication, and input-specified reduplication. These transductions are traditionally well studied under finite state transducers and attributed with varying complexity. We find that RNN seq2seq models are only able to approximate a mapping that fits the training or in-distribution data. Attention helps significantly, but does not solve the out-of-distribution generalization limitation. Task complexity and RNN variants also play a role in the results. Our results are best understood in terms of the complexity hierarchy of formal languages as opposed to that of string transductions. \n\n\n\n\n\nRecurrent Neural Networks, sequence to sequence models, alignments, string transductions, attention, generalization abilities, formal language theory\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nGiven the black-box nature of neural networks, learning formal languages has emerged as ideal proxy tasks for evaluating the expressive power and generalization capacity of neural networks <cit.>. Unlike real-world learning tasks, the underlying function generating a given formal language is typically known in advance. This makes possible a more flexible and complete control over data and, as a result, more fine-grained analyses of results obtained in experiments. Moreover, the rich tradition of studying formal languages offers critical insights into interpreting the learning results of neural networks, such as, from automata-theoretic perspectives <cit.>.\n\nThe current paper examines the learning capabilities of Recurrent-Neural-Network sequence to sequence (RNN seq2seq) models, in the context of string transductions. It focuses on the abilities of various types of RNN seq2seq models to learn a series of four transduction tasks of varying complexity and generalize to unseen in-distribution and out-of-distribution examples. The tasks under analysis are four string-to-string functions, including, identity, reversal, total reduplication, and input-specified reduplication (see <ref> or <cit.> for a review). Previous works on RNN seq2seq models have only investigated identity <cit.>, reversal <cit.>, and total reduplication <cit.>, but not input-specified reduplication. To the best of our knowledge, this paper also represents the very first study that analyzes the learning behaviors of three major RNN seq2seq model variants with and without attention on these four transduction tasks altogether. Both in-distribution and out-of-distribution generalization abilities are studied to better understand models' generalization capacity, which is central to a more rigorous and interpretable science of machine learning and its reliable application in real world <cit.>. The concept of distribution is defined mostly relating to the input sequence lengths, as detailed in <ref>.\n\nOur results show that RNN seq2seq models tend to only learn a mapping that fits the training or in-distribution data. The attention mechanism that facilitates the internal information flow makes RNN seq2seq models a nearly perfect in-distribution learner with greatly improved out-of-distribution generalization abilities for all tasks. However, attentional models are not learning the underlying transduction functions either. Through unified training/evaluation conditions, we further analyze the complexity of learning the four tasks and the effect of RNN variants in the seq2seq models. \n\nThe major contributions of our study are twofold. First, we present the very first comparative results of various types of RNN seq2seq models learning the four well-studied transduction tasks under highly controlled and reproducible experiments. Second, we provide novel characterizations of the four learning tasks based on the architectural consideration of RNN seq2seq models. We show that the complexity hierarchy of formal languages, as opposed to that of string transductions, better predict our results. \n\n\n\n\nThe paper proceeds as follows. We review technical preliminaries about RNN seq2seq models in <ref> and introduce the four learning tasks in <ref>. Experimental setups, including data, model training details, and evaluation methods, are described in <ref>. We present the results in <ref>, which are summarized and further discussed in <ref>. The paper ends with a delineation of limitations and future works in <ref>.\n\nThe source code, data, model training logs, trained models, and experimental results are open-sourced at <https://github.com/jaaack-wang/rnn-seq2seq-learning>.\n\n\n\n\n\u00a7 PRELIMINARIES\n\n\n\n\n \u00a7.\u00a7 RNNs\n\n\nRNNs represent a neural network architecture that utilizes repeated application of a recurrent unit to process a variable-length sequence x \n = (x_1,...,x_T). At each time step t, the recurrent unit computes a vector h_t \u2208\u211d^D \u00d7 1 by taking as inputs the embedding e_t of the current input symbol x_t (via an embedding layer E) and the previous hidden state h_t-1\n\n\n    h_t = f(h_t-1, e_t)\n\n\nwhere f(.) is a non-linear state transition function and varies among different variants of RNNs. The hidden state is commonly initialized as a zero vector. A non-initial hidden state h_t may be passed to an output layer to compute the probability distribution of the next symbol x\u0302_t+1 over an output alphabet \u0393 of size N, using a softmax activation function \n\n\n    p(x\u0302_t+1, i=1   |   x_t,...,x_1) = exp(w_ih_t)/\u2211_i'=1^N exp(w_i'h_t)\n\n\nwhere x\u0302_t+1, i=1 denotes x\u0302_t+1 being the i_th symbol \u2208\u0393 using one-hot encoding and w_i \u2208\u211d^1 \u00d7 D is a weight vector associated with that symbol. For the purpose of sequence generation, the embedding \u00ea_t+1 for x\u0302_t+1 along with h_t can be passed as inputs to the recurrent unit to compute the subsequent hidden states and output symbols via the iterative (or auto-regressive) application of Eq.(<ref>) and Eq.(<ref>).\n\n\n\nThis study uses three common variants of RNNs <cit.>: Simple RNN (SRNN, ), Long Short-term Memory (LSTM, ), and Gated Recurrent Units (GRU, ). The main difference among these three types of RNNs lie in the construction of the recurrent unit, where LSTM and GRU come with additional gating mechanisms to control information flow across time steps, and LSTM has a cell state besides the hidden state. The mathematical details for the state transition functions of the three types of RNNs are provided in Appendix [app:rnnfs]A. For simplicity and interpretability, all RNNs in this study are single-layered and unidirectional.\n\n\n\n\n \u00a7.\u00a7 RNN seq2seq models\n\n\nA RNN seq2seq model is an encoder-decoder structure where both the encoder and decoder are RNNs <cit.>. Given a pair of variable-length sequences x = (x_1,...,x_T) and y = (y_1,...,y_T'), the encoder is a processor that consumes the input sequence x sequentially until the final hidden state h_T^enc is produced. The decoder is a generator that takes as initial inputs h_T^enc and a preset start symbol <s> and is trained to auto-regressively generate an output sequence \u0177 = (\u0177_1,...,\u0177_T') to approximate y as much as possible. A preset end symbol </s> is also used to signal the termination of generation. Both the start and end symbols may be appended to x to help h_T^enc learn some useful information about sequence boundaries of the input sequence.  \n\n\n\nIn this study, we train three types of RNN seq2seq models, i.e., SRNN seq2seq, LSTM seq2seq, and GRU seq2seq, where the encoders and decoders are RNNs of same variant and with same hidden size. All the models are trained end-to-end by minimizing the cross-entropy loss between \u0177 and y through mini-batch gradient descent.\n\n\n\n\n \u00a7.\u00a7 Attention\n\n\nAttention[Here we only consider the so-called \u201cglobal attention\" where the encoder's hidden states are all accessible.] is a mechanism that allows the decoder in a seq2seq model to access information from all hidden states H^enc\u2208\u211d^D \u00d7 T of the encoder. It is first proposed to improve the performance of neural machine translation <cit.> and has later on been found to be a critical component of the Transformer architecture <cit.>. Attention has been hypothesized as external memory resources <cit.> or a \u201cweighted skip connection\u201d <cit.> to account for the success of seq2seq models augmented with attention.\n\nFormally, attention typically works as follows. At each decoding time step t, an attentional weight vector a_t\u2208\u211d^T \u00d7 1 can be computed by \n\n\n    a_t,i = exp(score(h_t^dec, h_i^enc))/\u2211_i'=1^T exp(score(h_t^dec, h_i'^enc))\n\n\nwhere a_t,i is a scalar weight that corresponds to the i_th hidden state h_i^enc of the encoder, and score a function that measures how well h_t^dec aligns with h_i^enc for i \u2208{1,...,T}. A context vector c_t\u2208\u211d^D \u00d7 1 can be computed by weighing H^enc with a_t through matrix multiplication, then concatenated with the embedding for \u0177_t, and together consumed by the decoder to generate an output. There are many variants of the score functions as in Eq.(<ref>) <cit.>. This study uses a simple one as follows\n\n\n    score(h_t^dec, h_i^enc) = v_a  tanh(W_a[h_t^dec;h_i^enc])\n\n\nwhere W_a\u2208\u211d^D\u00d7 2D and v_a\u2208\u211d^1 \u00d7 D are learnt weights to reduce the concatenated hidden states [h_t^dec;h_i^enc] \u2208\u211d^2D \u00d7 1 to an alignment score. tanh is a hyperbolic tangent function.\n\n\n\n\n\u00a7 LEARNING TASKS\n\n\n\n\n \u00a7.\u00a7 Task description and FST characterizations\n\n\n\nWe are interested in the following four learning tasks, representable by four deterministic string-to-string functions with an input alphabet \u03a3 and an output alphabet \u0393: (A) identity; (B) reversal; (C) total reduplication; (D) input-specified reduplication. For a given string w \u2208\u03a3^*, f_A(w) = w, f_B(w) = w^R, f_C(w) = ww, and f_D(w, @^n) = ww^n, where w^R denotes the reverse of w and @ a special instruction symbol whose number of occurrence (i.e., n) signals the number of copies to make for w. For example, if w = abc and n=3, then f_A(abc) = abc, f_B(abc) = cba, f_C(abc) = abcabc, and f_D(abc, @@@) = abcabcabcabc. It is evident that f_A and f_C are two special cases of f_D as f_D(w, \u03f5) = w and f_D(w, @) = ww, where \u03f5 means zero instruction symbol @. For all the functions, \u03a3 = \u0393, except f_D, where \u03a3\u2229\u0393 = \u0393 and \u03a3 - \u0393 = {@}. Please note that, for the ease of discussion, we do not count @ when talking about \u03a3 or input sequence lengths in the following sections.\n\nTraditionally, the four tasks are modelled with finite state transducers (FSTs) <cit.>. More concretely, f_A can be easily modelled by a 1-way FST where each input symbol is simply mapped to itself, whereas a 2-way FST that can read input sequences back and forth is required for modelling f_B and f_C. To model f_D, a 2-way FST enhanced with the capability of counting the number of instruction symbols is needed. As these four tasks require FSTs of increasing expressive capacity, they are characterized accordingly <cit.>, with f_A being a rational function, f_B and f_C regular functions, and f_D a polyregular function <cit.>. Under the FST-theoretic characterizations, f_D > f_B/f_C > f_A, where / means \u201cunordered\" and > is a \u201cmore complex than\u201d relation for learning these tasks. \n\n\n\n \u00a7.\u00a7 In-distribution and out-of-distribution\n\n\nGiven the deterministic nature of the four functions above, we define the concept of in-distribution and out-of-distribution in terms of the input sequences. For a model trained on input sequences of lengths \u2112 (for all the functions) or with numbers \ud835\udca9 of instruction symbol (only for f_D), in-distribution input sequences are those whose lengths \u2112'\u2286\u2112 and, where possible, whose numbers of instruction symbol \ud835\udca9'\u2286\ud835\udca9. Input sequences are out-of-distribution if either \u2112'\u2229\u2112 = \u00d8 or \ud835\udca9'\u2229\ud835\udca9 = \u00d8. Distinguishing in-distribution and out-of-distribution input sequences allows us to examine a trained model's abilities to generalize to examples that are independent and identically distributed and those that are beyond, in relation to the distribution of the training examples. Furthermore, a trained model's out-of-distribution generalization ability reveals whether the model learns the underlying function or approximates the in-distribution data. \n\n\n\n\n\n\n\n \u00a7.\u00a7 Complexity hypothesis\n\n\nAs discussed in <ref>, RNN seq2seq models take an encoder-decoder structure, where the decoder only \u201cwrites\" after the encoder \u201creads\" all the input symbols, unlike the read-and-write operation seen in FSTs. Moreover, since the domain of the four underlying functions is simply \u03a3^*, the input sequences see no dependency among their symbols, nor do the target sequences. It follows that, for a RNN seq2seq model to learn these tasks, the decoder must be able to store information about all the input symbols from the encoder and retrieve the output symbols in correct alignments with the input symbols. In this sense, the four learning tasks can be described as learning alignments or dependencies between the input and target sequences for RNN seq2seq models. Fig\u00a0<ref> illustrates the conjectured mechanism for learning identity and reversal functions. Total reduplication and input-specified reduplication functions should be learnt in a similar process, as the outputs of these two functions can be seen as the concatenation of multiple identity functions applied in a sequence <cit.>. To learn input-specified reduplication, the decoder should additionally be able to count the number of instruction symbols. \n\nAccordingly, we propose the following task complexity hierarchy for RNN seq2seq models: input-specified reduplication (f_D) > total reduplication (f_C) > identity (f_A) > reversal (f_B). On the one hand, RNNs are notorious for the long-term dependency learning issue that comes with gradient based learning <cit.>. This makes retrieving information about the early input symbols more difficult than the recent ones for the decoder, resulting in more learning complexity for f_A than f_B. On the other hand, longer target sequences require more memory resources than the shorter ones for a RNN seq2seq model to learn the input-target alignments. As f_C produces target sequence strictly twice longer than that of f_A for any given string w, f_C > f_A. f_D is most complex to learn since its target sequence length may grow unboundly, as the number of instruction symbols increases unboundly. Finally, it is possible that f_A is no less complex than (\u2265) f_B for attentional RNN seq2seq models, since attention allows all the encoder's hidden states to be accessible at any decoding time step, potentially alleviating the long-term dependency learning issue identified in the RNNs without attention.  \n\n\nCompared to the hierarchy established in <ref> with the FST-theoretic characterizations, a major difference here is the relative ranking between f_A and f_B. However, if we combine the input and target sequences together and take the transduction tasks as language recognition tasks, the hypothesis that f_A > f_B may be understood in terms of the complexity hierarchy of formal languages. For example, if we replace the arrow in Fig\u00a0<ref> with # and concatenate the input and target sequences, we have a copy language w#w for f_A and a palindrome language w#w^R for f_B. According to the Chomsky\u2019s hierarchy <cit.>, these two languages belong to context sensitive language (CSL) and context free language (CFL), respectively, where CSL is more complex than CFL. Similarly, the two respective languages corresponding to f_C and f_D are w#ww (CSL) and w#ww^n (at least CSL). Obviously, under the classic complexity hierarchy of formal languages, the languages associated with the four functions also imply that f_D > f_C > f_A > f_B, supplying a good insight into the complexity hypothesis proposed in this section. \n\n\n\n\n\n\n\n\u00a7 EXPERIMENTAL SETUPS\n\n\n\n\nTo ensure a fair evaluation of the learnability of the four tasks by RNN seq2seq models in a finite setting, we equipped all models with decently large parameter size such that the lack of sufficient capacity to fit the training data is not a bottleneck. For the same consideration, we also utilized various training techniques to improve the success rate of convergence for all the models. To make the results comparable across models of varying configurations and across different tasks, the input sequences and the training and evaluation conditions were deliberately set identical for every model trained and evaluated. \n\n\n\n\n \u00a7.\u00a7 Data\n\n\nTo simulate a more realistic learning setting, we set \u03a3 to be the 26 lowercase English letters. We sampled from \u03a3^* random strings of lengths 1-30 as the input sequences with the target sequences obtained by applying the four deterministic functions that represent the tasks. In-distribution strings are those of input lengths 6-15, available in the train, dev (development), and test sets. Out-of-distribution strings are those of input lengths 1-5 and 16-30, available only in the gen (generalization) set. For input-specified reduplication in particular, the in-distribution input sequences are of lengths 6-15, followed by 1-3 instruction symbols. The out-of-distribution input sequences can be of lengths 1-30 and with 1-6 instruction symbols, as long as either the length or the number of instruction symbols is unseen during training. \n\nFor the train/dev sets, there are 1,000 input-target pairs per input length and where applicable, per instruction symbol number, so the amount of data for input-specified reduplication is three times larger than other tasks in terms of training. For test/gen sets, the number of input-target pairs is 5,000 on the same levels. The test and gen sets were made much larger than the train/dev sets for the sake of more reliable evaluations. The four datasets are mutually disjoint. More details about the data can be found in Appendix [app:data]B.1.\n\n\n\n\n \u00a7.\u00a7 Training details\n\n\nModels were constructed in PyTorch <cit.> and run on standard GPU from Google Colaboratory via Colab Pro^+ subscription. Each model consists of two embedding layers with embedding size 128, before the respective single-layered RNN encoder and decoder, both of hidden size 512. We initialized all trainable weights using Xavier initialization <cit.> to reduce the vanishing gradient problem associated with RNNs, as the input sequences get longer. We applied Adam optimizer <cit.> with 5e-4 learning rate and 1e-5 L2 weight decay rate and normalized gradients with maximum norm clipped at 1 <cit.> to alleviate or circumvent the exploding gradient problem common of training gradient based RNNs <cit.>. Details about the model size are provided in [app:model_size]B.2. \n\nTo speed up convergence at the training time, we employed a technique called teacher forcing <cit.> to permit the decoders to access the real next symbols from the target sequences, instead of using the predicted next symbols as inputs. All models were trained up to 500 epochs with the train/dev sets performances evaluated every 10 epochs. The batch size is 1,000 and every batch only contained input sequences of same lengths to avoid padding, which changes the mapping of the input-target sequences. Training only stopped if one of the following conditions was met: (1) models run through all the epochs; (2) the full-sequence accuracy (see <ref>) on the dev set reaches 1.0; (3) the full-sequence accuracy on the train set and dev set exceeds 0.9999 and 0.995, respectively, at the same time. Models with the highest full-sequence accuracy on the dev set were saved and deployed to the test and gen sets and were trained and evaluated for three runs. The effectiveness of such training methods and procedures is demonstrated in Appendix [app:effectiveness_of_training]B.3.\n\n\n\n\n \u00a7.\u00a7 Evaluation methods\n\n\nWe used the following three metrics to evaluate how well RNN seq2seq models learn the input-target alignments for the four tasks: full-sequence accuracy, first n-symbol accuracy, and overlap rate. All these metrics are measured from the initial output symbol to the end symbol </s> against the target sequences. Full-sequence accuracy measures the percentage of the target sequences being 100% correctly generated, whereas first n-symbol accuracy only measures the average proportion of the first n symbols being correctly generated for the target sequences. Overlap rate is the ratio of each generated output sequence \u0177 = (\u0177_1,...,\u0177_n) overlapping with the corresponding target sequence y = (y_1,...,y_n), when compared in a pair-wise manner, on average for the target sequences. \n\nThese three metrics provide well-rounded measurements of alignments between two sequences. When a more restrictive metric shows a low score and thus becomes less discriminative, there can be a more fine-grained alternative. In this study, we used the full-sequence accuracy as the main metric and reported the last two metrics only when necessary. \n\n\n\n\n\n\n\n\n\n\n\n\u00a7 RESULTS\n\n\nThe main results can be found in Table\u00a0<ref> and Fig\u00a0<ref>, which present full-sequence accuracy on aggregate and per-input-length levels, respectively. Of the three runs, we selected the one with best overall performance on the test and gen sets, weighted 40% and 60%, respectively. Related results measured in first n-symbol accuracy and overlap rate, are in Appendix [app:results]C for references. Since the results across these three metrics share similar patterns, unless particularly mentioned, findings based on full-sequence accuracy remain valid in terms of the other two metrics. This section also reports results obtained from follow-up experiments. \n\n\n\n\n \u00a7.\u00a7 Generalization behaviors\n\n\nAs can be observed in Table\u00a0<ref>, RNN seq2seq models, with or without attention, consistently achieve better aggregate full-sequence accuracy in the test sets than in the gen sets with a large average margin (i.e., at least 33%) for all the tasks. Even on the per-input-length level, depicted in Fig\u00a0<ref>, the gen set full-sequence accuracy decreases as the difference between the unseen lengths and the nearest seen lengths increases most of the time. These strongly indicate that it is generally much more difficult for RNN seq2seq models to generalize to the out-of-distribution examples than the in-distribution ones. Empirically, RNN seq2seq models' out-of-distribution generalization abilities are rather restricted if longer unseen and untested input sequence lengths are also taken into account. It implies that RNN seq2seq models tend to learn to approximate only the distribution or a plausible mapping of the training data for given input sequence lengths, instead of the underlying data generation functions, if these functions are learnable at all.   \n\n\n\n\n \u00a7.\u00a7 Attentional versus attention-less models\n\n\nThe main results show straightforwardly that attention greatly helps models fit the train sets and generalize to the test/gen sets. Attentional models can all achieve (nearly) 100% aggregate full-sequence accuracy on both the train and test sets, whereas attention-less models cannot and almost always show a big train-test variance. Moreover, attentional models also outperform the attention-less counterparts in generalizing to the out-of-distribution examples. In other words, attentional RNN seq2seq models are stronger in-distribution learners with better out-of-distribution generalization abilities, compared to the attention-less ones. \n\nBesides, attention significantly improves learning efficiency. It is observed (see Appendix [app:effectiveness_of_training]B.3) that during training attentional models could converge within 160 epochs on average, whereas attention-less models often did not even with 500 epochs for all the learning tasks. Furthermore, Fig\u00a0<ref> shows that the test set performance of the attention-less models goes down nearly as a function of the input length, which indicates the need of greater sample complexity for training. To further contrast the learning efficiency between attentional and attention-less models, we conducted a follow-up experiment in total reduplication, which appears to be the hardest task for the attention-less models to learn according to Table\u00a0<ref>. We adopted the same training methods and procedures for this experiment, except that the attentional models used 1/4 training examples and 1/4 hidden size, and the attention-less models used 3 times more training examples and 3 times more training epochs, compared to their original setups. The results in Table\u00a0<ref> show that by using only 1/12 training examples, 1/9 parameter size (see Appendix [app:model_size]B.2), and 1/3 training epochs, the attentional models still learn total reduplication better than the attention-less models across all the datasets. \n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Task complexity\n\n\nThe task complexity hypothesis formulated in <ref> is mostly borne out for attention-less models. Table\u00a0<ref> and Fig\u00a0<ref> show clear evidence that for each type of attention-less models, total reduplication is apparently more complex than identity, which is apparently more complex than reversal, on both aggregate and per-input-length levels in terms of full-sequence accuracy. Given the identical training and evaluation conditions for these three tasks indicated in <ref>, we argue that the complexity of learning these tasks is the major, if not only, attributable reason for the observed performance difference. \n\nHowever, it appears that input-specified reduplication does not turn out to be more complex than total reduplication, which goes against the hypothesis. We argue that this result is due to two reasons. First, the number of instruction symbols for training is only 1-3, which is far from representative of the generative capacity of input-specified reduplication. Second, the training data size for input-specified reduplication is three times larger than that for total reduplication in the main experiments. Our follow-up experiment in <ref> shows that when given same level sample complexity, attention-less models can learn total reduplication better than input-specified reduplication, as far as the train/test set performance is concerned. Nevertheless, further experiments are needed in order to establish the relative complexity of learning input-specified reduplication. \n\n\nGiven the experimental setups of the study, the results for the four tasks are less informative of their complexity for attentional models. This is because attentional models can learn these tasks (nearly) perfectly for in-distribution examples (even with much less resources as shown in <ref>) and the out-of-distribution performance is less interpretable. \n\n\n\n\n\n \u00a7.\u00a7 RNN variants\n\n\nOverall, attentional SRNN models appear to have better out-of-distribution generalization abilities than the GRU/LSTM counterparts, whereas attention-less GRU/LSTM models are more expressive and can learn nearly all the tasks with apparent advantages over the related SRNN models. The only two exceptions are attentional GRU/LSTM models consistently generalizing better to the gen sets than the SRNN counterparts for identity, and attention-less SRNN models outperforming the attention-less GRU/LSTM models in both test/gen sets for reversal. Generally, LSTM seems to be better than GRU for most cases, regardless of the use of attention, but the difference is small and inconsistent across runs. \n\nPrevious research <cit.> has shown that LSTM can learn counting. The results for input-specified reduplication, detailed in Fig\u00a0<ref>, also show that on a per-instruction-symbol-number level and regardless of attention, LSTM models are more capable of generalizing to input sequences with unseen numbers of instruction symbols, whereas SRNN basically fail and GRU can generalize to a highly limited extent. Moreover, although attention helps models to generalize better as shown in the last section, both attention-less GRU and LSTM models exhibit overall better and more stable out-of-distribution generalization abilities when only the number of instruction symbols is unseen. However, they both perform poorly on input sequences of unseen lengths without attention. \n\nFor unclear reasons, attentional SRNN models trained in this study can all reliably generalize to input sequences of longer unseen lengths with seen numbers of instruction symbols to an extent close to the respective train set performance, which, however, varies greatly across different runs. For example, our follow-up evaluation (see Appendix [app:follow_up_eval]C.4) shows that the best attentional SRNN model can generate input sequences with seen instruction symbol numbers and of up to 50 symbols with at least 46% full-sequence accuracy. Moreover, despite the low full-sequence accuracy of the best attentional SRNN model for input sequences with unseen instruction symbol numbers depicted in Fig\u00a0<ref>, its first n-symbol accuracy and overlap rate are both overall better than that of the LSTM counterpart (see Appendix [app:input_spec_red_res]C.3). \n\n\n\n\n\n\n\n\n\u00a7 DISCUSSION AND CONCLUSION\n\n\nThis study investigated how well the three major types of RNN seq2seq models, with and without attention, learn four transduction tasks that can be described as learning alignments between the input and target sequences. Through highly unified training/evaluation conditions, we compared the experimental results across tasks, models of varying configurations, and test/gen set performance, among other things. Unlike previous research, the input alphabet \u03a3 for our experiments contains 26 unique symbols, instead of binary, making our results more meaningful to real-world tasks that concern, say, morpho-phonological transduction. The major findings are summarized below. \n\n\nGeneralization abilities. RNN seq2seq models, regardless of the use of attention, are prone to learning a function that fits the training or in-distribution data. Their out-of-distribution generalization abilities are highly limited and only restricted to data that is more similar to the training data, for example, in terms of input sequence lengths. <cit.> shows similar results when a much wider range of input sequence lengths (with a binary \u03a3) is used for training and evaluating RNN seq2seq models learning identity and reversal functions. \n\nAttention. Attention makes learning alignments between input and target sequences much more efficient and robust. For all tasks, attentional models can almost always fit both train and test sets to (nearly) 100% full-sequence accuracy, even with greatly reduced training data size and model size, whereas attention-less models cannot and easily suffer from a big train-test variance due to a need of large sample complexity. Attentional models also show significantly stronger out-of-distribution generalization abilities than their attention-less counterparts. The impressive learning efficiency accelerated by attention echoes its original motivation; that is, \u201clearning to align\" <cit.>. \n\nTask complexity. We established the following task complexity for attention-less RNN seq2seq models: total reduplication > identity > reversal. This is different from the traditional FST-theoretic viewpoint based on string transductions, which treats reversal and total reduplication as a function class that is strictly more complex than identity. In contrast, this result can be better understood under the complexity hierarchy of formal languages, if the four transduction tasks are re-framed as language recognition tasks, as demonstrated in <ref>. We implied that input-specified reduplication should in principle be more complex than total reduplication for the greater need of memory resources to learn input-target alignments. However, this is not verified in this study, constrained by the lack of computational resources to set out the related experiment at a more proper scale. Since all the tasks are easy for attentional models to learn in an in-distributional setting, it is not attested whether a similar hierachy also applies for attentional models. \n\n\nRNN variants. The effect of RNN variants is a complicated one and appears to interact with other factors, e.g., the use of attention, and the task to learn. When attention is not used, LSTM/GRU models are expectedly and significantly more expressive than SRNN models for all tasks other than reversal, probably thanks to the additional built-in gating mechanisms that improve long-term memory <cit.> and thus reduce the sample complexity to learn input-target alignments. However, attentional SRNN model appears to have consistently greater out-of-distribution generalization performance for all tasks except identity, compared to attentional GRU/LSTM models. LSTM, in particular, exhibits a stronger counting ability for learning input-specified reduplication, regardless of the use of attention. \n\nThe results presented above showcase the fruitfulness of utilizing formal language tasks to probe the learning capabilities of neural networks. Although alignments in the context of this study are \u201chard\" alignments <cit.> or only refer to alignments of identical segments from input and target sequences, some fundamental conclusions drawn here should also stand for more complicated input-target alignment learning problem, such as machine translation. For example, <cit.> finds that reversing input sequences improves the performance of attention-less LSTM seq2seq models for machine translation. Our results on identity and reversal and the hypothesized task complexity hierarchy show exactly that. \n\nBecause this is an empirical study, we want to bring to attention in the end the importance of not making hasty conclusions about negative learnability of certain tasks. Obviously, many factors play a role in the final performance of trained models, such as sample complexity, model complexity, training methods, etc. For instance, in our main experiments, attention-less SRNN models barely learn total reduplication, in line with <cit.>, but the follow-up experiment shows that what may appear as a learnability problem is just a training problem, constrained by both training data size and time. In fact, in the absence of a sound theoretical proof, it is not possible to make a claim about what is unlearnable for neural networks, because we cannot experiment exhaustively to validate such a universal claim. Nevertheless, we hope our study and some interesting puzzles brought up but left unaccounted for here may encourage more future works at the intersection of machine learning and formal language theory. \n\n\n\n\n\u00a7 LIMITATIONS AND FUTURE WORKS\n\n\nConstrained by available resources, we are unable to conduct our research at a larger scale, such as using a wider range of input sequences lengths during training and evaluation for all tasks. This study also does not provide any systematic and quantitative analysis as to why the trained RNN Seq2seq models fail to generalize to out-of-distribution examples, which is an important step toward understanding how those models generalize. Furthermore, it is worth testing whether the observations made in this study still hold with bidirectional and/or multi-layered RNN seq2seq models, which are more commonly used in practice, as well as with different variants of attention. These, however, are not quite possible for this study to experiment out. We hope these limitations can be addressed in future studies.\n\nFor future studies that continue the line of the current research, we suggest using a special case of input-specified reduplication, where the target sequence is simply as many copies of the input sequence as the input length. This makes possible unifying the input vocabulary and sample complexity for all the four tasks, but requires expensive computational resources, as the target length grows quadratically as the input length. In general, it is important to establish the learning complexity of RNN seq2seq models for some carefully-selected tasks, such as the four tasks in the study, which have well-established characterizations under FST and logic, among others <cit.>. Knowing what is more complex and less complex for a model class to learn is essential for understanding the learning capabilities of that model class.\n\n\n\n\n\u00a7 ACKNOWLEDGEMENTS\n\n\nThe current research literally would not have been made possible without the continuous guidance and inspirations from Jeffrey Heinz. I am also grateful for Jordan Kodner, William Oliver, Sarah Payne, Nicholas Behrje for reading through the manuscript and providing helpful comments. I also thank the inspirations from Imane Machouahi Rifi.\n\n\n\n\n\n\n\n\u00a7 A. MATHEMATICAL DETAILS OF THE RNN VARIANTS\n\n\n\nFor convenience, the state transition function for RNNs from Eq.(<ref>) is reproduced below\n\n\n    h_t = f(h_t-1, e_t)\n\n\nFor SRNN, f is typically a sigmoid or hyperbolic tangent function. We used the latter. \n\nFor LSTM, f takes what is known as an cell state c_t in addition to the hidden state h_t. At each time t, both states are updated as follows\n\n\n\n\n    h_t     = o_t * tanh(c_t) \n    c_t     = f_t * c_t-1 + i_t * c\u0303_t \n    c\u0303_t     = tanh(W_c[h_t-1; e_t] + b_c) \n    \n    o_t    = sigmoid(W_o[h_t-1; e_t] + b_o) \n    \n    f_t    = sigmoid(W_f[h_t-1; e_t] + b_f) \n    \n    i_t    = sigmoid(W_i[h_t-1; e_t] + b_i)\n\n\n\nwhere [  .  ;   .  ] denotes vector concatenation along the last axis. Eq.(<ref>-<ref>) are the equations for the output gate o_t, forget gate f_t, and input gate i_t, respectively, all of which are scalar values \u2208 (0,1), thanks to the sigmoid activation function. Because of this numerical property, they are said to act as a filter to control the information flow inside the recurrent unit. The update of h_t is determined by o_t and c_t, whose update is determined by weighing c_t-1 and the temporarily updated cell state c\u0303_t with f_t and i_t, respectively.\n\nInspired by LSTM, GRU takes a simplified state transition function as follows\n\n\n\n    h_t     = z_t * h_t-1 + (1 - z_t) * h\u0303_t \n    h\u0303_t     = tanh(W_h[r_t * h_t-1; e_t] + b_h) \n    \n    z_t     = sigmoid(W_z[h_t-1; e_t] + b_z) \n    \n    r_t    = sigmoid(W_r[h_t-1; e_t] + b_r)\n\n\n\nwhere z_t is the update gate, r_t the reset gate, and h\u0303_t the temporarily updated h_t. Here, h_t takes the place of both h_t and c_t as in LSTM, and z_t replaces LSTM's f_t and i_t. h_t is updated by weighing h_t-1 and h\u0303_t with z_t and (1-z_t), respectively. \n\n\n\n\n\n\u00a7 B. EXPERIMENTAL DETAILS\n\n\n\n\n \u00a7.\u00a7 B.1. Data\n\n\n\nTable\u00a0<ref> summarizes the data size for the following tasks: identity, reversal, and total reduplication. For input-specified reduplication, the in-distribution input sequences (in the train/dev/test sets) are of lengths 6-15, followed by 1-3 instruction symbols. Per instruction symbol number, there are as many examples as the other tasks per input length, so the data size for input-specified reduplication is three times larger than that for the other tasks in terms of the respective train/dev/test sets. The out-of-distribution input sequences (in the gen set), on the other hand, can be of lengths 1-30 and with 1-6 instruction symbols. More concretely, the gen set for input-specified reduplication has a total of 750,000 input-target pairs and consists of two parts. One has input sequences of unseen lengths (1-5 & 16-30) followed by 1-6 instruction symbols, and the other has input sequences of seen lengths (6-15) but followed by 4-6 instruction symbols. The second part of the input sequences are from the test set to ensure the input sequences are identical across different numbers of instruction symbols. This is for evaluating if models learn to count the instruction symbols.\n\nPlease note that, duplicates are mostly disallowed in each type of dataset where possible and each dataset is disjoint from other datasets for each given task. In gen sets, which contain input sequences of lengths 1 and 2, we randomly sampled 5,000 strings from \u03a3^|1| and \u03a3^|2| with duplicates to make the later evaluation unified and simpler. \n\nFor the follow-up experiment in <ref>, the attentional models were trained on 1/4 of data from the original train/dev sets and reused the original test/gen sets. The attention-less models were trained on train/dev sets that are three times larger than the original ones, with all the four datasets re-generated from scratch to ensure each dataset remains disjoint from one another. For the follow-up evaluation mentioned in <ref>, we re-generated data for input sequences of lengths 31-50 and with 1-6 instruction symbol numbers following the original data generation protocol described above. \n\n\n\n\n\n \u00a7.\u00a7 B.2. Model size\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 B.3. Effectiveness of training\n\n\n\nAs reported in Table\u00a0<ref>, most of the best models fitted the train sets to nearly 100%. This is also true for models in other runs, as the average standard deviation for the loss and full-sequence accuracy in the train sets for all the models is only around 0.04 and 0.12, respectively. Table\u00a0<ref> provides details about the average number of epochs used over the three runs for each type of model from the main experiments. It is clear that attentional models can easily converge under the training methods and procedures of this study. \n\n\nWhen attention is not in use, only the SRNN models appear unable to fit total reduplication and input-specified reduplication. The problem is highly likely to do with insufficient amounts of training data and time, provided that we keep other hyperparameters unchanged. The evidence is the follow-up experiment in total reduplication in <ref>, which used more training data with extended number of epochs. Fig\u00a0<ref> is the training log of one attenton-less SRNN model from the follow-up experiment with the other two runs showing similar training pattern. Despite the fluctuations in loss and other metrics, our training methods clearly led the models to convergence when enough training resources are given. Similar training patterns can also be found for attention-less GRU/LSTM models from the main experiments. The train-test variance is apparently only due to insufficient training data size, but not our training methods. \n\n\n\n\n\n\n\u00a7 C. RESULTS\n\n\n\n\n\n \u00a7.\u00a7 C.1. Aggregate main results measured in other metrics\n\n\n\nTable\u00a0<ref> and Table\u00a0<ref> show the aggregate main results measured in first n-symbol accuracy and overlap rate, respectively. The results were selected from the same runs consistent with Table\u00a0<ref>. Best results are in bold for the test and gen sets.\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 C.2. Per-input-length main results measured in other metrics\n\n\n\nFig\u00a0<ref> and Fig\u00a0<ref> show the main results measured in first n-symbol accuracy and overlap rate on a per-input-length level, respectively. \n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 C.3. More fine grained results on input-specified reduplication\n\n\n\nThe results for input-specified reduplication per input length per instruction symbol number are presented in Fig\u00a0<ref> and Fig\u00a0<ref>, respectively. \n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 C.4. Follow-up evaluation\n\n\n\nFig\u00a0<ref> shows the result of the follow-up evaluation of the best attentional SRNN model generalizing to input sequences of longer unseen lengths with seen numbers of instruction symbols, as mentioned in <ref>.\n\n\n\n\n\n\n\n\n0.2in\n\n\n"}