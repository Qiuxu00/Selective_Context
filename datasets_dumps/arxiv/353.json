{"entry_id": "http://arxiv.org/abs/2303.06837v1", "published": "20230313034650", "title": "Adversarial Attacks to Direct Data-driven Control for Destabilization", "authors": ["Hampei Sasahara"], "primary_category": "eess.SY", "categories": ["eess.SY", "cs.SY"], "text": "\n\n\n\nDDFM: Denoising Diffusion Model for Multi-Modality Image Fusion\n    Zixiang Zhao^1,2  Haowen Bai^1  Yuanzhi Zhu^2  Jiangshe Zhang^1  Shuang Xu^3\n\n            Yulun Zhang^2  Kai Zhang^2  Deyu Meng^1  Radu Timofte^2,4  Luc Van Gool^2\n\n\t\t^1Xi\u2019an Jiaotong University   ^2Computer Vision Lab, ETH Z\u00fcrich\n\n            ^3Northwestern Polytechnical University   ^4University of Wurzburg\n\n\n    Received: date / Accepted: date\n============================================================================================================================================================================================================================================================================================================================\n\nempty\nempty\n\n\n\n\nThis study investigates the vulnerability of direct data-driven control to adversarial attacks in the form of a small but sophisticated perturbation added to the original data.\nThe directed gradient sign method (DGSM) is developed as a specific attack method, based on the fast gradient sign method (FGSM), which has originally been considered in image classification.\nDGSM uses the gradient of the eigenvalues of the resulting closed-loop system and crafts a perturbation in the direction where the system becomes less stable.\nIt is demonstrated that the system can be destabilized by the attack, even if the original closed-loop system with the clean data has a large margin of stability.\nTo increase the robustness against the attack, regularization methods that have been developed to deal with random disturbances are considered.\nTheir effectiveness is evaluated by numerical experiments using an inverted pendulum model.\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nAdvances in computing power and an increase in available data have led to the success of data-driven methods in various applications, such as autonomous driving\u00a0<cit.>, communications\u00a0<cit.>, and games\u00a0<cit.>.\nIn the field of control theory, this success has sparked a trend towards direct data-driven control methods, which aim at designing controllers directly from data without the need for a system identification process\u00a0<cit.>.\nOne prominent scheme is the Willems' fundamental lemma-based approach\u00a0<cit.>, which provides explicit control formulations and requires low computational complexity\u00a0<cit.>.\n\n\nMeanwhile, in the context of image classification, it has been reported that a data-driven method using neural networks is susceptible to adversarial attacks\u00a0<cit.>.\nSpecifically, adding small perturbations to images that remain imperceptible to human vision system can change the prediction of the trained neural network classifier.\nThis type of vulnerability has also been observed in different domains such as speech recognition\u00a0<cit.> and reinforcement learning\u00a0<cit.>.\nInfluenced by those results, adversarial attacks and defenses have become a critical area of research on data-driven techniques.\n\n\n\n\nMost work on control system security focuses on vulnerabilities of control systems themselves and defense techniques with explicit model knowledge against attacks exploiting these vulnerabilities, such as zero-dynamics attack analysis\u00a0<cit.>, observer-based attack detection\u00a0<cit.>, and moving target defense\u00a0<cit.>.\nIn addition, there have also been recent studies on data-driven approaches, such as data-driven stealthy attack design\u00a0<cit.> and data-driven attack detection\u00a0<cit.>.\nHowever, the vulnerability of data-driven control algorithm has received less attention, and there is a need for dedicated techniques to address this issue.\n\n\nThe main objective of this study is to evaluate the robustness of direct data-driven control methods against adversarial attacks, and to provide insights on how to design secure and reliable data-driven controller design algorithms.\n\nThe aim of the attacker is to disrupt the stability of the closed-loop system by making small modifications to the data.\nAs the worst-case scenario, we first consider a powerful attacker who has complete knowledge of the system, the controller design algorithm, and the clean input and output data.\n\n\n\nSubsequently, we consider gray box attacks where we assume that the adversary has access to the model and the algorithm but not the data, and additionally may not know design parameters in the algorithm.\nEffectiveness of crafted perturbations without partial knowledge is known as the transferability property, which has been confirmed in the domain of computer vision\u00a0<cit.> and reinforcement learning\u00a0<cit.>.\nWe observe that the data and parameter transferability property holds in direct data-driven control as well.\n\nOur first contribution is to demonstrate the vulnerability of direct data-driven control.\nWe introduce a specific attack, which we refer to as the directed gradient sign method (DGSM), based on the fast gradient sign method (FGSM), which has originally been developed for efficient computation of a severe adversarial perturbation in image classification\u00a0<cit.>.\nThe idea behind FGSM is to calculate the perturbation vector in the direction of the gradient of the cost function while limiting each element's absolute value to a specified small constant.\nDGSM is an adaptation of this method, designed to destabilize the targeted control system.\nDGSM calculates the gradient of the eigenvalues of the resulting closed-loop system and determines the perturbation in the direction that makes the system less stable.\nFig.\u00a0<ref> illustrates a demonstration of DGSM applied to a discrete-time linear system.\nIt is shown that while the system can be stabilized by using clean data where the resulting eigenvalues are far from the unit circle it can be made unstable by a small but sophisticated perturbation.\n\n\n\nSecond, we investigate defense methods using regularization.\n\nWe consider two regularization approaches: the first is the certainty-equivalence regularization that links the direct data-driven control with the indirect one via system identification using the ordinary least-square estimation\u00a0<cit.>.\nThe second is the robustness-inducing regularization that ensures robustness against noise\u00a0<cit.>.\nWe demonstrate that both approaches can improve robustness against adversarial attacks and compare their effectiveness.\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Organization and Notation\n\nThe paper is organized as follows.\nSec.\u00a0<ref> reviews key concepts of direct data-driven control based on the fundamental lemma and discusses a technique for generating adversarial perturbations used in image classification with neural networks.\nIn Sec.\u00a0<ref>, we outline the attack scenario and present the adversarial method adapted for direct data-driven control that leads to destabilization.\nSec.\u00a0<ref> provides experimental evaluation to discuss the vulnerabilities of interest and the improvement in robustness through regularization.\nFinally, Sec.\u00a0<ref> concludes and summarizes the paper.\n\n\n\n\nWe denote the transpose of a matrix M by M^ T,\nthe trace and the spectrum of a square matrix M by (M) and \u03c3(M), respectively,\nthe maximum and minimum singular values of a matrix M by \u03c3_ max(M) and \u03c3_ min(M), respectively,\nthe max norm of a matrix M by M_ max,\nthe right inverse of a right-invertible matrix M by M^\u2020,\nthe positive and negative (semi)definiteness of a Hermetian matrix M by M\u227b (\u227d) 0 and M\u227a (\u227c) 0, respectively,\nand the component-wise sign function by sign(\u00b7).\n\n\n\n\u00a7 PRELIMINARY\n\n\n\n\n\n \u00a7.\u00a7 Data-Driven Control based on Fundamental Lemma\n\n\nWe first review the direct data-driven control based on the Willems' fundamental lemma\u00a0<cit.>.\n\nConsider a discrete-time linear time-invariant system\nx(t+1)=Ax(t)+Bu(t)+d(t) for t\u2208\u2115\n\n\n\nwhere x(t)\u2208\u211d^n is the state, u(t)\u2208\u211d^m is the control input, and d(t)\u2208\u211d^n is the exogenous disturbance.\nAssume that the pair (A,B) is unknown to the controller designer but it is stabilizable.\nWe consider the linear quadratic regulator (LQR) problem\u00a0<cit.>, which has widely been studied as a benchmark problem.\nSpecifically, design a static state-feedback control u(t)=Kx(t) that minimizes the cost function J(K)=\u2211_i=1^n\u2211_t=0^\u221e{x(t)^ TQx(t) + u(t)^ TRu(t)}|_x(0)=e_i\n\n\n\nwith Q\u227d 0 and R\u227b 0 where e_i is the ith canonical basis vector.\nIt is known that the cost function can be rewritten as\nJ(K)=(QP)+(K^ TRKP)\n\n\n\nwhere P\u227d I is the controllability Gramian of the closed-loop system when A+BK is Schur.\n\nThe objective of direct data-driven control is to design the optimal feedback gain using data of input and output signals without explicit system identification.\nAssume that the time series\nU_0 :=[u(0) u(1) \u22ef u(T-1)]\u2208\u211d^m\u00d7 T\nand\nX :=[x(0) x(1) \u22ef x(T-1) x(T)] \u2208\u211d^n\u00d7 (T+1)\n\n\n\n\n\n\nare available.\nThe first and last T-long time series of X are denoted by X_0\u2208\u211d^m\u00d7 T and X_1\u2208\u211d^m\u00d7 T, respectively.\nLetting D_0:=[d(0) d(1) \u22ef d(T-1)]\u2208\u211d^m\u00d7 T,\n\n\n\nwe have the relationship\n\n\n    X_1-D_0 = [B A]W_0,\n\nwhere\nW_0:=[U_0^ T X_0^ T]^ T.\n\n\n\n\n\n\n\n\n\n\nWe here assume that rank  W_0 = n+m\n\n\n\nholds.\nThis rank condition, which is generally necessary for data-driven LQR design\u00a0<cit.>, is satisfied if the input signal is persistently exciting in the noiseless case as shown by the Willems' fundamental lemma\u00a0<cit.>.\n\nThe key idea of the approach laid out in\u00a0<cit.> is to parameterize the controller using the available data by introducing a new variable G\u2208\u211d^T\u00d7 n with the relationship\n\n    [K^ T I]^ T\n    =W_0G.\n\n\nThen the closed-loop matrix can be parameterized directly by data matrices as\nA+BK = [B A]W_0G = (X_1-D_0)G.\n\n\n\n\n\n\nThe LQR controller design can be formulated as\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    [              min_P,K,G         (QP)+(K^ TRKP);                   s.t. X_1GPG^ TX_1^ T-P+I\u227c 0;                            P \u227d I  and (<ref>);                                               ]\n\nby disregarding the noise term.\n\n\nHowever, it has been revealed that the formulation\u00a0(<ref>) is not robust to disturbance\u00a0<cit.>.\nTo enhance robustness against disturbance, a regularized formulation has been proposed:\n\n    [              min_P,K,G    (QP)+(K^ TRKP)+\u03b3\u03a0 G;                   s.t. X_1GPG^ TX_1^ T-P+I\u227c 0;                            P \u227d I  and (<ref>);                        ]\n\nwith a constant \u03b3\u22650 where \u03a0:=I-W_0^\u2020W_0\n\n\n\nand \u00b7 is any matrix norm.\nThe regularizer \u03b3\u03a0 G is referred to as certainty-equivalence regularization because it leads to the controller equivalent to the certainty-equivalence indirect data-driven LQR with least-square estimation of the system model when \u03b3 is sufficiently large\u00a0<cit.>.\nMeanwhile, another regularization that can guarantee robustness has been proposed:\n\n    [                 min_P,K,G (QP)+(K^ TRKP)+\u03c1 (GPG^ T);                      s.t.    X_1GPG^ TX_1^ T-P+I\u227c 0;                                  P \u227d I  and (<ref>);                           ]\n\nwith a constant \u03c1\u22650.\nThe regularizer \u03c1 (GPG^ T) plays the role to reduce the size of the matrix GPG^ T to achieve the actual stability requirement (X_1-D_0)GPG^ T(X_1-D_0)^ T-P+I\u227c 0 using the constraint X_1GPG^ TX_1^ T-P+I\u227c 0.\nWe refer to the latter one as robustness-inducing regularization.\nFor reformulation of\u00a0(<ref>),\u00a0(<ref>), and\u00a0(<ref>) into convex programs, see\u00a0<cit.>.\n\n\n\n \u00a7.\u00a7 Fast Gradient Sign Method\n\n\nThe fast gradient sign method (FGSM) is a method to efficiently compute an adversarial perturbation for a given image\u00a0<cit.>.\nLet L(X,Y;\u03b8) be the loss function of the neural network where X\u2208\ud835\udcb3 is the input image, Y\u2208\ud835\udcb4 is the label, and \u03b8 is the trained parameter, and let f:\ud835\udcb3\u2192\ud835\udcb4 be the trained classification model. \nThe objective of the adversary is to cause misclassification by adding a small perturbation \u0394\u2208\ud835\udcb3 such that f(X+\u0394)\u2260 f(X).\nSpecifically, the max norm of the perturbation is restricted, i.e., \u0394_ max\u2264\u03f5\n\nwith a small constant \u03f5>0.\n\nThe core idea of FGSM is to choose a perturbation that locally maximizes the loss function.\nThe linear approximation of the loss function with respect to \u0394 is given by\n\n    L(X+\u0394,Y;\u03b8)\u2243 L(X,Y;\u03b8)+\u2211_k,\u2113(\u2207_XL(X,Y;\u03b8))_k\u2113\u0394_k\u2113\n\nwhere the subscript (\u00b7)_k\u2113 denotes the (k,\u2113) component.\nThe right-hand side of\u00a0(<ref>) is maximized by choosing \u0394_k\u2113=\u03f5  sign(\u2207_XL(X,Y;\u03b8))_k\u2113, whose matrix form is given by\n\n    \u0394 = \u03f5  sign(\u2207_XL(X,Y;\u03b8)).\n\n\nFGSM creates a series of perturbations in the form increasing \u03f5 until misclassification occurs.\nIn the next section, we apply this idea to adversarial attacks on direct data-driven control for destabilization.\n\n\n\n\u00a7 ADVERSARIAL ATTACKS TO DIRECT DATA-DRIVEN CONTROL\n\n\n\n\n\n \u00a7.\u00a7 Threat Model\n\nThis study considers the following threat model:\nThe adversary can add a perturbation (\u0394 U, \u0394 X) to the input and output data (U_0,X).\nAdditionally, the adversary knows the system model (A,B), the data (U_0,D_0,X), and the controller design algorithm.\nThis scenario is depicted in Fig.\u00a0<ref>.\nThe controller K\u0302 is designed using the perturbed data (\u00db,X\u0302):=(U_0+\u0394 U,X+\u0394 X), which results in the closed-loop matrix A+BK\u0302.\nThe attack objective is to destabilize the system by crafting a small perturbation such that the closed-loop matrix has an eigenvalue outside the unit circle.\n\n\n\nAdditionally, we consider gray-box attacks where the adversary has access to the system model (A,B) and the controller design algorithm but not the data (U_0,D_0,X), and additionally may not know the design parameters \u03b3 and \u03c1.\nIn this case, a reasonable attack strategy is to use hypothetical input \u00db and disturbance D\u0302_0 and calculate the corresponding state trajectory X\u0302.\nWe refer to effectiveness of the attack without knowledge of the data as the transferability across data.\nAdditionally, when the design parameters are unknown, hypothetical design parameters \u03b3\u0302 or \u03c1\u0302 are also used.\nWe refer to the effectiveness in this scenario as the transferability across parameters.\nWe numerically evaluate the transferability properties in Sec.\u00a0<ref>.\n\n\n\n \u00a7.\u00a7 Directed Gradient Sign Method\n\nWe develop the directed gradient sign method (DGSM) to design a severe perturbation\n\u0394:=(\u0394 U, \u0394 X) that satisfies \u0394_ max\u2264\u03f5 with a small constant \u03f5>0.\n\n\n\n\n\n\n\n\n\n\nLet\n\n\n    \u039b(U_0,X,\u0394):=\u03c3(A+BK\u0302)\n\ndenote the eigenvalues of the closed-loop system with the direct data-driven control\u00a0(<ref>) or\u00a0(<ref>) using the perturbed data (\u00db,X\u0302).\n\nThe aim of the attack is to place some element of \u039b(U_0,X,\u0394) outside the unit circle.\n\nThe core idea of DGSM is to choose a perturbation that locally shifts an eigenvalue in the less stable direction.\n\nWe temporarily fix the eigenvalue of interest, denoted by \u03bb_i(U_0,X,\u0394),\nand denote its gradient with respect to \u0394 by \u2207_\u0394\u03bb_i(U_0,X,\u0394).\n\n\n\n\nThe linear approximation of the eigenvalue with respect to \u0394 is given by\n\n    \u03bb_i(U_0,X,\u0394)\u2243\u03bb_i(U_0,X,0)+\u2211_k,\u2113\u2207_\u0394\u03bb_i(U_0,X,\u0394)) \u0394_k\u2113.\n\nWe choose \u0394_k \u2113 such that the right-hand side of\u00a0(<ref>) moves closer to the unit circle.\nSpecifically, DGSM crafts the perturbation\n\n    \u0394 = \u03f5  sign(\u03a0_\u03bb_i(\u2207_\u0394\u03bb_i(U_0,X,\u0394)))\n\nwhere \u03a0_\u03bb_i:\u2102^(m+n)\u00d7(2T+1)\u2192\u211d^(m+n)\u00d7(2T+1) is defined by\n\n    \u03a0_\u03bb_i(Z):= \u03bb_iZ+\u03bb_iZ\n\nwith\n\n\n    Z:=\u2207_\u0394\u03bb_i(U_0,X,\u0394).\n\n\nThe role of the function \u03a0_\u03bb_i is illustrated in Fig.\u00a0<ref>.\nSuppose that Z_k\u2113 faces the direction of \u03bb_i.\n\n\nMore precisely, the angle between \u03bb_i and Z_k\u2113, denoted by \u03d5, is less than \u03c0/2, which leads to \u03a0_\u03bb_i(Z_k\u2113)>0.\nWe now suppose that the angle between \u03bb_i and another element Z_k\u0303\u2113\u0303, denoted by \u03d5\u0303, is greater than \u03c0/2.\nThen we have \u03a0_\u03bb_i(Z_k\u0303\u2113\u0303)<0.\n\nIn both cases, owing to the function \u03a0_\u03bb_i, the perturbed eigenvalue moves closer to the unit circle as depicted in the figure.\nBy aggregating all components, the linear approximation of the perturbed eigenvalue \u03bb\u0302_i is given  by\n\n\n    \u03bb\u0302_i\u2243\u03bb_i+\u03f5 \u2211_k,\u2113 sign(\u03a0_\u03bb_i(Z_k\u2113))Z_k\u2113,\n\nwhich is expected to be placed outside the unit circle by increasing \u03f5.\n\n\n\nDGSM performs the procedure above for every \u03bb_i for i=1,\u2026,n increasing \u03f5 until the resulting system is destabilized.\nIts algorithm is summarized in Algorithm\u00a01, where {\u03f5_k} denotes possible candidates of the constant \u03f5 in the ascending order.\nAlgorithm\u00a01 finds a perturbation \u0394 with the smallest \u03f5 in {\u03f5_k} such that the resulting closed-loop system becomes unstable.\n\n\n\n0\n\n\n \u00a7.\u00a7 Scaled-DGSM\n\nWe also consider the scaled-DGSM, a minor extension of DGSM.\nFor multi-input systems, the scales of each input can differ, i.e., u_i_\u221e\u2260u_j_\u221e for i\u2260 j where u_i denotes the ith row of U_0.\nTo account for those differences, the magnitude of the perturbation is scaled based on the corresponding signal.\nSpecifically, the scaled-DGSM\n\n\n\n\n\ngenerates the scaled input perturbation\n\n    \u03b4_u_i = u_i_\u221e\u03b4^ ori_u_i\n\nwhere \u03b4_u_i and \u03b4^ ori_u_i denotes the ith row of the scaled input perturbation and  that of the original input perturbation created by Algorithm\u00a01, respectively.\nThe state perturbation is also scaled similarly in the scaled-DGSM.\n\n\n\n\n\u00a7 NUMERICAL EXPERIMENTS\n\n\n\n\n\n \u00a7.\u00a7 Experimental Setup\n\n\nWe evaluate our adversarial attacks through numerical experiments.\nWe consider the inverted pendulum\u00a0<cit.> with sampling period 0.01 whose system matrices are given by\n\n    A=[\n     [ 0.9844 0.0466 0.0347; 0.0397 1.0009 0.0007; 0.0004 0.0200 1.0000 ]],   B=[\n     [ 0.25;    0;    0 ]].\n\n0\nWe consider two systems:\nan artificial one of a marginally unstable Laplacian system\n\n    A=[\n     [ 1.01 0.01    0; 0.01 1.01 0.01;    0 0.01 1.01 ]],   B=I\n\nconsidered in\u00a0<cit.>, and a practical one of a three-tank chemical process\n=0.33\n    A=[\n     [  0.96     0     0;  0.04  0.97     0; -0.04     0  0.90 ]],  \n     B=[\n     [  8.80 -2.30     0     0;  0.20  2.20  4.90     0; -0.21  -2.2  1.90    21 ]]\n\nconsidered in\u00a0<cit.>.\n=3\nWe set the weight matrices to Q=I and R=10^-5I.\nThe input signal is randomly and independently generated by u(t)\u223c\ud835\udca9(0,1).\nWe consider the disturbance-free case, i.e., d(t)=0.\n\n\nThe time horizon is set to T=10.\nThe 2-induced norm is taken as the matrix norm in\u00a0(<ref>).\nThe gradient \u2207_\u0394(\u03bb_i(U_0,X,\u0394)) is computed by the central difference approximation\u00a0<cit.>.\n\n\n0\n\n\n \u00a7.\u00a7 Vulnerable Instance\n\n\n\nWe first demonstrate that the direct data-driven control can be vulnerable.\nConsider the Laplacian system\u00a0(<ref>) with the disturbance parameter d=0.05.\nThe magnitude of the adversarial perturbation is set to \u03f5=0.1.\nThe controller is designed using the certainty-equivalence regularization\u00a0(<ref>) with a regularization parameter \u03bb=10^-3.\n\nThe results are shown in Fig.\u00a0<ref>.\nDespite the perturbed data appearing similar to the original one, the resulting closed-loop system becomes unstable due to the adversarial attack.\nIndeed, the eigenvalues of the closed-loop system with the original input signal are {0.0011,-0.0076,0.0315}, while those with the perturbed signal are {0.0037, 0.1730,1.0179}.\n\nThe max norm and the maximum singular value of the random disturbance and the adversarial perturbation are described in TABLE\u00a0<ref>.\nThere is a significant difference between the maximum singular values, as opposed to the max norm.\nThis result supports the hypothesis in Sec.\u00a0<ref> that DGSM creates a perturbation with small individual elements but a large maximum singular value.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Robustness Improvement by Regularization\n\n\nWe examine the improvement in robustness through regularization by comparing DGSM with a random attack where each element of \u0394 takes \u03f5 or -\u03f5 with equal probability.\nLet N_ all and N_ unstable denote the total number of samples and the number of the samples where the resulting closed-loop system is unstable, respectively.\nIn addition, let \u03f5\u0305 denote the minimum \u03f5 such that N_ unstable/N_ all\u2265\u03c4 for a given threshold \u03c4\u2208[0,1].\nWe set N_ all=50 and \u03c4=0.8.\n\nFig.\u00a0<ref> depicts the curves of \u03f5\u0305 with varying \u03b3 for DGSM and the random attack when using the certainty-equivalence regularization\u00a0(<ref>).\nFirst, it is observed that the magnitude of the adversarial perturbation necessary for destabilization increases as the regularization parameter \u03b3 increases.\nThis result implies that the regularization method originally proposed for coping with disturbance is also effective in improving robustness against adversarial attacks.\nSecond, the necessary magnitude in DGSM is approximately 10% of that in the random attack, which illustrates the significant impact of DGSM.\n\n\n\nFig.\u00a0<ref> depicts the curves of \u03f5\u0305 with varying \u03c1 when using the robustness-inducing regularization\u00a0(<ref>).\nThis figure shows results similar to Fig.\u00a0<ref>.\n\nConsequently, both regularization methods are effective for adversarial attacks.\n\n\n\nNext, we compare the effectiveness of the two regularization methods.\nWe take \u03b3=0.1 and \u03c1=10^-5 such that the resulting closed-loop performances J(K) are almost equal.\nFig.\u00a0<ref> depicts N_ unstable/N_ all with the two regularized controller design methods\u00a0(<ref>) and\u00a0(<ref>) for varying \u03f5.\nIt can be observed that the robustness-inducing regularization\u00a0(<ref>) always outperforms the certainty-equivalence regularization\u00a0(<ref>).\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Transferability\n\n\nWe consider transferability across data where the data (U_0,D_0,X) is unknown and DGSM uses a hypothetical input \u00db_0 whose elements are also randomly and independently generated by \ud835\udca9(0,1) and D\u0302_0=0.\nFig.\u00a0<ref> depicts the curves of \u03f5\u0305 with varying \u03b3 for DGSM without knowledge of data and that with full knowledge when using the certainty-equivalence regularization\u00a0(<ref>).\nThis figure shows that DGSM exhibits the transferability property across data.\n\n\n\n\n\nSubsequently, we examine transferability across design parameters where the regularization parameter \u03b3 in addition to the data (U_0,D_0,X) is unknown.\nWe use \u03b3=0.1 as a hypothetical parameter.\nFig.\u00a0<ref> depicts the corresponding curves\nAs in the transferability across data, the results confirm the transferability property across parameters.\n\n\n\n\n\n\n \u00a7.\u00a7 Discussion\n\n\nThe regularization methods described by\u00a0(<ref>) and\u00a0(<ref>) provide a quantitative condition to ensure stability:\nThe resulting closed-loop system with the certainty-equivalence regularization is stable when \u03b3 and the signal-to-noise ratio (SNR) defined by\nSNR:=\u03c3_ min(W_0)/\u03c3_max(D_0)\n\n\n\nare sufficiently large\u00a0<cit.>.\nThat with the robustness-inducing regularization is stable when \u03c1 is sufficiently large and \u03c3_ max(D_0) is sufficiently small\u00a0<cit.>.\nOne may expect that DGSM crafts a severe input perturbation such that its maximum singular value is large but its elements are small.\nHowever, for the single-input system, \u03c3_ max(\u0394 U)=\u03f5\u221a(T) for any \u0394 U whose elements take \u03f5 or -\u03f5.\nThis means that the input perturbations made by DGSM and the random attack have the same maximum singular value.\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\n\nThis study has investigated the vulnerability of direct data-driven control, specifically focusing on the Willems' fundamental lemma-based approach with two regularization methods, namely certainty-equivalence regularization and robustness-inducing regularization.\nTo this end, a new method called DGSM, based on FGSM which has been originally been proposed for neural networks, has been introduced.\nIt has been demonstrated that direct data-driven control can be vulnerable, i.e., the resulting closed-loop system can be destabilized by a small but sophisticated perturbation.\nNumerical experiments have indicated that strengthening regularization enhances robustness against adversarial attacks.\n\n\n\nFuture research should include further tests of the vulnerability with various types of data and systems under different operating conditions, a theoretical analysis of DGSM, and exploration of novel defense techniques for reliable direct data-driven control.\nFor example, detection of adversarial perturbations\u00a0<cit.> is a promising direction.\nFinally, for a more comprehensive understanding of the vulnerability, more sophisticated attacks should be considered.\n\n\nThe parameters in the simulation in Fig.\u00a0<ref> are as follows.\nThe system is a marginally unstable Laplacian system\n0\n\n    A=[\n     [ 1.01 0.01    0; 0.01 1.01 0.01;    0 0.01 1.01 ]],   B=I\n\nconsidered in\u00a0<cit.>.\nEach element of the disturbance D is randomly generated from \ud835\udca9(0,d^2) with d=0.05.\nThe weight matrices are Q=I and R=10^-3I.\nThe time horizon is set to T=15.\nThe magnitude of the adversarial perturbation is set to \u03f5=0.16.\nThe controller is designed using the certainty-equivalence regularization\u00a0(<ref>) with a regularization parameter \u03b3=10^-3.\n\n\n\n\nIEEEtran\n\n\n"}