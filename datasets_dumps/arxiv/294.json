{"entry_id": "http://arxiv.org/abs/2303.06931v1", "published": "20230313085510", "title": "DeepVigor: Vulnerability Value Ranges and Factors for DNNs' Reliability Assessment", "authors": ["Mohammad Hasan Ahmadilivani", "Mahdi Taheri", "Jaan Raik", "Masoud Daneshtalab", "Maksim Jenihhin"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AR", "B.8.1; I.4.9"], "text": "\n\n\n\n\n\n1]Mohammad Hasan Ahmadilivani\n1]Mahdi Taheri\n1]Jaan Raik\n1,2]Masoud Daneshtalab\n1]Maksim Jenihhin\n[1]Tallinn University of Technology, Tallinn, Estonia\n[2]M\u00e4lardalen University, V\u00e4ster\u00e5s, Sweden\n[1]{mohammad.ahmadilivani, mahdi.taheri, jaan.raik, maksim.jenihhin}@taltech.ee\n[2]masoud.daneshtalab@mdu.se\n\nDeepVigor: VulnerabIlity Value RanGes and FactORs for DNNs' Reliability Assessment\n    [\n    March 30, 2023\n==================================================================================\n\n\n\nfirstpage\n\n\n\nDeep Neural Networks (DNNs) and their accelerators are being deployed ever more frequently in safety-critical applications leading to increasing reliability concerns. A traditional and accurate method for assessing DNNs' reliability has been resorting to fault injection, which, however, suffers from prohibitive time complexity. While analytical and hybrid fault injection-/analytical-based methods have been proposed, they are either inaccurate or specific to particular accelerator architectures. \n\nIn this work, we propose a novel accurate, fine-grain, metric-oriented, and accelerator-agnostic method called DeepVigor that provides vulnerability value ranges for DNN neurons' outputs. An outcome of DeepVigor is an analytical model representing vulnerable and non-vulnerable ranges for each neuron that can be exploited to develop different techniques for improving DNNs' reliability. Moreover, DeepVigor provides reliability assessment metrics based on vulnerability factors for bits, neurons, and layers using the vulnerability ranges.\n\nThe proposed method is not only faster than fault injection but also provides extensive and accurate information about the reliability of DNNs, independent from the accelerator. The experimental evaluations in the paper indicate that the proposed vulnerability ranges are 99.9% to 100% accurate even when evaluated on previously unseen test data. Also, it is shown that the obtained vulnerability factors represent the criticality of bits, neurons, and layers proficiently. DeepVigor is implemented in the PyTorch framework and validated on complex DNN benchmarks.\n\n\n\n\nThe work is supported in part by the European Union through European Social Fund in the frames of the \u201cInformation and Communication Technologies (ICT) programme\u201d (\u201cITA-IoIT\u201d topic), by the Estonian Research Council grant PUT PRG1467 \u201cCRASHLES\u201d and by Estonian-French PARROT project \u201cEnTrustED\u201d.\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nDeep Neural Networks (DNNs) have recently emerged to be exploited in a wide range of applications. DNN accelerators have also penetrated into safety-critical applications e.g., autonomous vehicles <cit.>. Therefore, several concerns are raised regarding developing and utilizing DNN accelerators in the realm of safety-critical applications, one of them being the reliability. \n\nReliability of DNNs concerns their accelerators' ability to perform correctly in the presence of faults <cit.> originating from either the environment (e.g., soft errors, electromagnetic effects, temperature variations) or inside of the chip (e.g., manufacturing defects, process variations, aging effects) <cit.>. As shown in Fig. <ref>, faults may occur in different locations of accelerators either in memory or logic components and they influence the parameters (e.g., weights and bias) and intermediate results (layers' activations) of neural networks that can decrease their accuracy drastically <cit.>. By technology miniaturization, the effect of Single Event Transient (SET) and Single Event Upset (SEU) faults in devices is increasing thereby jeopardizing the reliability of modern digital systems <cit.>.\n\n\n\nRecently, several works have been published on the assessment and improvement of the reliability of a variety of DNNs as well as on  different levels of system hierarchy <cit.>. Reliability assessment is the process of modeling \nthe target DNN accelerator and measuring its reliability with respect to the corresponding quantitative evaluation metrics. Reliability assessment is the underlying procedure for improving reliability since it presents how the system could be influenced by threats as well as which locations of the system are more vulnerable to them. Therefore, it is the very first and principal phase of a reliable design process. \n\nThroughout the literature, reliability assessment methods for DNNs are mainly categorized into two major classes: fault injection (FI) and resilience analysis. The majority of the works assess the reliability of DNNs relying on FI, which provides realistic results on the impact of different fault models on the system's execution and is performed directly on the target platform (accelerator's software <cit.> or RTL model <cit.>, FPGA <cit.>, GPU <cit.>). FI outputs different evaluations for DNNs' reliability by accuracy loss, vulnerability factors, or fault classification <cit.>. Moreover, fine-grain evaluations for finding critical bits can be performed by exhaustive FI or an optimized method in <cit.>.\n\nNevertheless, FI methods are prohibitively time-consuming and carry a high complexity due to the need to inject an enormous amount of faults into a huge number of DNN parameters as well as time instances to reach an acceptable confidence level <cit.>. The more fine-grain evaluation is required the more sophisticated experiments should be performed. In addition, most faults in a FI experiment on DNNs are masked <cit.> and are thus unnecessarily examined. Furthermore, the outcome of such assessment is application/platform specific which can not be generalized for other platforms <cit.>.\n\nResilience analysis methods cope with the drawbacks of FI. They analyze the function of DNNs mathematically and have the potential to evaluate their reliability with arbitrary metrics. Therefore, resilience analysis methods can provide a deeper insight into the reliability evaluations of DNNs with lower complexity. Moreover, they can be conducted in different fault-tolerant designs on various platforms <cit.>. \n\nLayer-wise Relevance Propagation (LRP) algorithm is leveraged in <cit.> to obtain the contribution of neurons to the output to express their criticality and apply protections to improve the reliability of DNN accelerators. The sensitivity of DNN's filters is obtained by Taylor expansion with given error rates in <cit.> for designing an error-resilient and energy-efficient accelerator. \n\nThe conducted resilience analyses in these works are not able to provide reliability measurement metrics and detailed vulnerability evaluations. Moreover, they combine the criticality scores of neurons over individual outputs of the DNNs, thus resulting in missing important information about the resilience of DNNs as a whole. Mahmoud et al. <cit.> proposed different heuristics for vulnerability estimation of feature maps without FI. These estimations which are more coarse grain than the LRP-based methods, lead to hardening the accelerators, however, the accuracy of the vulnerability estimation methods is remarkably lower than that of fault-injection methods.\n\n\nThe aforementioned papers on resilience analysis methods have focused mainly on finding the most critical neurons/weights in a DNN to protect them against faults in a fault-tolerant design. In addition, they do not explain sufficiently how a fault propagates through the network and influence its outputs. Fidelity framework <cit.> is proposed to take advantage of both FI and analyzing DNN accelerators to provide reliability metrics. However, it requires detailed information of the accelerator architecture/implementation. To the best of our knowledge, there is no accelerator-agnostic resilience analysis method for DNNs that can compete with FI in terms of reliability evaluation to be less time-consuming, and accurate with fine-grain metrics enabling different reliability improvement techniques. \n\nIn this research work, we introduce the concept of neurons' vulnerability ranges expressing whether or not a fault at the output of neurons would misclassify the network. Thus, it enables a comprehensive reliability study with a novel resilience analysis method called DeepVigor where the vulnerability factors of layers, neurons, and bits in a DNN are obtained. The contributions in this work are:\n\n\n    \n  * Proposing DeepVigor, a novel accurate, metric-oriented, and accelerator-agnostic resilience analysis method for DNNs reliability assessment faster than fault injection;\n    \n  * Introducing and acquiring vulnerability ranges for all neurons in DNNs, assisted by a fault propagation analysis, providing accurate categorization of critical/non-critical faults;\n    \n  * Providing fine-grain vulnerability factors as reliability evaluation metrics for layers, neurons, and bits in DNNs, compared with and validated by fault injection.\n\n\nThe remainder of the paper is organized as follows: the resilience analysis method is presented in Section <ref>, and the experimental setup and results are provided in Section <ref>. The applicability of the method is discussed in Section <ref>, and the work is concluded in Section <ref>.\n\n\n\n\n\u00a7 DNN RELIABILITY ASSESSMENT WITH DEEPVIGOR\n \n\n\n\n \u00a7.\u00a7 Fault Model\n\n\nIn this work, the fault propagation analysis is performed at the outputs of DNN neurons. However, they will cover a vast majority of internal faults of the neurons occurring inside the MAC units and also a large portion of faults in the weights and neurons' input activations. It is assumed that only one neuron has an erroneous output per execution due to faults which is a common assumption in the literature <cit.>.\n\nFor validation by FI, the single-bit fault model has been applied. While the multiple-bit fault model is more accurate, it requires a prohibitively large number of fault combinations to be considered (3^n - 1 combinations, where n is the number of bits). Fortunately, it has been shown that high fault coverage obtained using the single-bit model results in a high fault coverage of multiple-bit faults <cit.>. Therefore, a vast majority of practical FI and test methods are based on the single-bit fault assumption. Single bitflip faults are injected randomly at neurons' outputs and once per execution. \n\n\n\n \u00a7.\u00a7 Fault Propagation Analysis\n\n\nFig.\u00a0<ref> depicts an overview of the rationale behind the DeepVigor method. A tiny neural network with few layers and neurons with given inputs, golden (fault-free) activation values (inside of neurons), and weights (on the arrows) is shown. The golden classification output is class1. A fault changes the neuron's output by \u03b4 which is the difference between the golden and faulty activation values. This \u03b4 that can have either a negative or a positive value will be propagated to the output layer and may change the classification result. The fault propagation will make a difference on each output class as \u0394_1 and \u0394_2. Misclassification happens when the value of the output neuron class2 gets higher than that of neuron class1. \n\n\n\nThus, the propagation of the fault can be traced from the neuron to the output and a problem for misclassification can be expressed as shown in Fig.\u00a0<ref>. By solving the problem of misclassification condition in the output, the value for \u03b4 is obtained as a vulnerability threshold that expresses how much a fault should influence the neuron to misclassify the network. Therefore, a vulnerability value range for the neuron is acquired. In this example, the range (-\u221e, -5.39) is a vulnerable range and [-5.39, +\u221e) is non-vulnerable range. This idea is generalized for a DNN including multiple output classes and other corresponding functions in this paper. \n\n\n\n\n \u00a7.\u00a7 The DeepVigor Method\n \n\nThe steps of the proposed DNNs' resilience analysis method (DeepVigor) and its validation are illustrated in Fig.\u00a0<ref>. As shown, an analysis is performed on a set of data (i.e., set1, training set) and outputs the vulnerability value ranges as well as the vulnerability factors. Furthermore, FI is performed on the same and different data (i.e., set2, test set) to validate the outcomes of the analysis. The steps of DeepVigor are as follows: \n\n\n\n\nStep1 - Gradient-based Initialization:\nIn the first step, a neuron is examined whether or not to be processed for the vulnerability analysis. For this purpose, assuming a neural network consisting of L layers with N output classes in C = {c_1, c_2, ..., c_N}. Neuron k at layer l is selected to be examined. The neuron's output is corrupted by adding a sample positive or negative value as \u03f5_k^l to its output and the feed-forward of the network is executed over a batch of input data. A loss function \u2112 is defined in Equation (<ref>) as: \n\n    \u2112 = sigmoid(\u2211_j=0^N(\u2130_c_t - \u2130_c_i))\n\nwhere c_t is the golden top class and \u2130_c_t and \u2130_c_i are the erroneous output values corresponding to the respective classes. The loss function computes the summation of differences between the value of the golden top class and the other outputs in the corrupted network and applies a sigmoid function. The golden top class is what the fault-free DNN outputs as its classification whether or not it is correctly classified.\n\n\u2112 represents the impact of the neuron's erroneous output on the golden top class of the network. When the gradient of \u2112 w.r.t. the corrupted neuron's output for one input is zero, it means that any error at this neuron's output does not change the output classification. Considering a batch of inputs, if the gradients are zero for a portion of inputs larger than a threshold, the neuron is disregarded for the vulnerability analysis. In case most of the gradients are not zero, a range for searching the vulnerability value is initialized.\n\nConsidering \u03f5_k^l is a positive value for one input, in case the gradient is positive, there is a minimum value 0 < \u03b4_k^l < \u03f5_k^l for the neuron that if error \u03b4_k^l is added to its output (by a fault at its inputs or the output value itself) the network's golden classification would change. But if the gradient is negative, then \u03b4_k^l should be searched through the values larger than \u03f5_k^l. A similar scenario is valid for negative values of \u03f5_k^l.\n\n\n\nStep2 - Neurons' Vulnerability Analysis: \nIn this step, the vulnerability ranges of neurons under analysis are obtained. \nLet R_NV(l,k,x) = [r_lower,r_upper] be a Range of Non-vulnerable Values for a k-th neuron at layer l with input data x. The bounds of range R for x are calculated as follows:\n    \n\n    r_upper = min(\u03b4_k^l), \u03b4_k^l>0 , \u2130_c_t <  \u2130_c_i, i \u2260 t \n    \n                r_lower = max(\u03b4_k^l), \u03b4_k^l<0 , \u2130_c_t <  \u2130_c_i, i \u2260 t\n\nwhere c_t and c_i are the golden top class and any other output class, respectively, and \u2130_c_t and \u2130_c_i are the erroneous output values corresponding to the respective classes.\n\nEquation (<ref>) finds the maximum negative and minimum positive values induced at the corresponding neuron that do not lead to misclassifying the input data from the golden classification. Further, a Range of Vulnerable Values R_VV(l,k,x) for a k-th neuron at layer l with input data x is equal to R_VV = (-\u221e,r_lower) \u222a (r_upper,\u221e).\n\nNote, the equation is applied for a single input data. In the case of a data set X containing T input data x_j the R_NV and R_VV will get refined and will be equal to intersections of their respective ranges over all inputs x_j as follows:\n\n    R_NV(l,k) = \u22c2^T_j=1R_NV(l,k,x_j) \n    \n                R_VV(l,k) = \u22c2^T_j=1R_VV(l,k,x_j)\n\n\n\n\nThe outcome of solving the equations for each neuron and merging the results over all inputs will be the vulnerability value ranges for each class separately, each range specifies the impact of a fault on changing the neuron value whether it influences the network classification result or not. Fig. <ref> depicts different cases for vulnerability ranges over all numbers. Three vulnerability ranges are identified as follows:\n\n\n    \n  * Non-vulnerable range: If a fault lay an effect on the neuron output in this range, no misclassification happens (hachured-green sections in Fig. <ref>);\n    \n  * Vulnerable range: If a fault makes a difference at the output of the neuron in this range, the output will be misclassified (cross hachured-red sections in Fig. <ref>);\n    \n  * Semi-vulnerable range: If a fault causes the neuron value to move as an amount in this range, this fault may cause a misclassification (dashed-grey sections in Fig. <ref>). Cases d-f in Fig. <ref> happen when the portion of zero gradients in step1 is less than the threshold and more than 1-threshold.\n    \n\n\n\n\n\nStep3 - Bitflip Mapping:\nIn this step, DeepVigor maps the neurons' bitflipped values over input data on the vulnerability value ranges to indicate fine-grain vulnerability factors as metrics for the DNNs' reliability. For this purpose, the inputs used in step2 and obtained vulnerability value ranges are fed to the network and in each bit of each neuron, bitflips are performed. In each bitflip, the difference in the new value of the target neuron is calculated and compared with the corresponding vulnerability range. \n\nBased on the range of what the bitflip maps, the bit is considered vulnerable or non-vulnerable, respectively. By this analysis, the number of vulnerable bits of the neurons is obtained over the inputs. Hence, vulnerability factors of each layer (LVF), neuron (NVF), or bit (BVF) of the DNN can be defined as equations (<ref>), (<ref>), and (<ref>), respectively. Vulnerability factors express the probability of misclassifying the network in case of the occurrence of a bitflip at the target element.\n\n\n    LVF  =                                         \n    #vulnerable bits in layer/#inputs \u00d7#layer's  neurons \u00d7 word length\u00d7 100\n\n\n\n    NVF = #vulnerable bits in neuron/#inputs \u00d7 word length\u00d7 100\n\n\n\n    BVF = #vulnerable times for bit/#inputs  \u00d7 100\n \n\n\n\n \u00a7.\u00a7 Validating DeepVigor By Fault Injection\n\nAs illustrated in Fig.\u00a0<ref>, DeepVigor results are validated by means of FI over the input data and categorizing faults based on the vulnerability value ranges. The steps of the validation process of DeepVigor are as follows:\n\nStep1 - Random Fault Injection: \nAccording to the adopted fault model, when one input is fed to the network, a random single bitflip is injected into a random neuron in a layer. This process is repeated several times for one input depending on the number of neurons and word length of data to reach a 95% confidence level and 1% error margin based on <cit.>. The required number of faults is obtained by Equation (<ref>) where N = word   length \u00d7#layer's  neurons that represents the total number of bits in the output of a layer.\n\n\n    #layer's   random   faults = N/1 + (0.01^2 \u00d7N - 1/1.96^2 \u00d7 0.5^2)\n \n\nStep2 - Fault Categorization: \nOnce a fault is injected, a difference is produced in the output of the neuron in comparison with the golden model. In this step, the produced difference by a fault at the neuron's output is compared with the obtained vulnerability ranges, and faults are categorized as:\n\n\n    \n  * Non-critical fault: The produced difference is in the non-vulnerable range.\n    \n  * Critical fault: The produced difference is in the vulnerable range.\n\n\n\nStep3 - Validating DeepVigor:\nTo validate DeepVigor by FI, injected faults are propagated to the output and the network classification output is examined. The accuracy of the method is defined based on the two metrics as follows:\n\n\n    \n  * True non-critical faults: Percentage of faults that are categorized as non-critical and do not change the classification at the output;\n    \n  * True critical faults: Percentage of faults that are categorized as critical and change the classification at the output.\n\n\nAnother metric for validating the outputs of DeepVigor is the correlation between LVF and DNN's accuracy loss. This correlation shows that the obtained vulnerability factors from DeepVigor represent the criticality of the components properly.\nSince other vulnerability factors (NVF and BVF) are calculated using the same vulnerability ranges, by validating LVF, they will be also liable metrics for the resilience analysis, consequently.\n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n \n\n\n\n \u00a7.\u00a7 Experimental Setup\n\n\nAll DNNs, steps of DeepVigor, and its validation are implemented in PyTorch and run on NVIDIA 3090 GPU. To explore different DNN structures, six representative DNNs trained on three datasets are examined for the experimental results. We have experimented with two 5-layer MLPs (one with Sigmoid and one with ReLU) trained on MNIST, two LeNet-5 with 3 convolutional (CONV) layers, 2 max-pooling (POOL) layers, and 2 fully-connected (FC) layers trained on MNIST and CIFAR-10, AlexNet with 5 CONV, 3 POOLs, 2 batch normalization (BN) and 3 FCs trained on CIFAR-10, and VGG-16 with 13 CONV, 13 BNs, 5 POOLs and 2 FCs trained on CIFAR-100. The respective networks' accuracy on the corresponding test sets are 94.64%, 90.55%, 90.4%, 66.15%, 72.73%, and 69.41%.\n\n\nData representation in this work is 32-bit floating point IEEE-754 and the word   length in equations (<ref>)-(<ref>) is 32 bits. For validation, a layer-wise statistical random FI is performed that satisfies a 95% confidence level and 1% error margin. \n\n\nIn the first step of DeepVigor \u03f5_k^l is considered -/+10000 for range initialization and the whole search range is [-5 \u00d7 10^5, 5 \u00d7 10^5]. Finding \u03b4_k^l in all networks by a logarithmic search is performed for negative and positive numbers separately, considering a 0.05 difference from the main value. Also, based on empirical explorations the threshold of neurons' zero-gradients for inputs is considered 98% for all experiments. Corresponding experiments are performed on the whole sets of training (as the input data set1) and test (as the input data set2) data. \n\n\n\n\n\n \u00a7.\u00a7 Results and Validation\n\n\nWe analyze all neurons of the representative DNNs with training sets as the input data set1 by DeepVigor and obtain the vulnerability ranges. In the fault categorization step, faults are categorized into critical and non-critical classes with an accuracy close to 100%. Throughout the results from FI experiments, DeepVigor identified 66.63% to 99.42% of faults as non-critical over different layers of analyzed networks. \n\nFor validation, Table\u00a0<ref> presents the range of obtained accuracy values of the method through all layers of DNNs in terms of true non-critical and critical faults. It is observed that the accuracy of the method for categorizing non-critical faults is 99.950% to 100% and for critical faults ranging from 99.955% to 100% for the same data set. \n\n\n\nThe minor error seen in the results is due to: 1) Considered error in finding vulnerability values, 2) FI results in \"NaN\" values in 32-bit floating point IEEE-754 while the computations are being done on a GPU. We have categorized them as critical faults, 3) the effect of few inputs with non-zero gradients in step1 as described in <ref>.\n\nWe have also experimented with FI on the test sets (input data set2) to see the validity of the analysis on different sets reported in Table <ref>. As it can be seen, similar high accuracy values to input data set1 are obtained.\n\n\n\n\n\nTo validate the vulnerability factors, Fig.\u00a0<ref> illustrates the correlation between LVF and accuracy loss for a layer-wise FI on AlexNet. As demonstrated, there is a close relationship between the LVF obtained from DeepVigor and accuracy loss in FI, either the input sets are similar or different.\nThis correlation is observed similarly in the results for all experimented DNNs. Therefore, LVF represents the vulnerability of layers competently. \n\n\n\n\n\n\n\n\nDeepVigor also provides NVF and BVF metrics as vulnerability factors for neurons and bits, respectively. As a representative example, Fig.\u00a0<ref> depicts NVF for layer conv3 of LeNet5-mnist and LeNet5-cifar10 that the more vulnerable neurons can be identified. In this figure, the number of neurons is sorted in each DNN separately, in the ascending order of NVF.  Also, BVF for all neurons in DNNs is obtained and the results show that the most significant bit of exponents is the most vulnerable bit in most cases. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Run-Time Analysis\n\nDeepVigor enables a fine-grain reliability evaluation for DNNs faster than exhaustive FI. In our experiments, step1 of DeepVigor have removed up to 48% of neurons' vulnerability analysis to be processed in step2. Moreover, the range initialization in step1 has accelerated the search for finding the vulnerability values for 50% to 99% of neurons in step2 among the DNNs. Based on our experiments, a complete vulnerability range (as in Fig. <ref>) for one neuron can be obtained by 9.1 times feed-forward execution per neuron on average. While an exhaustive FI experiment runs the feed-forward by the number of bits (32 in our case) per neuron. Therefore, DeepVigor requires 3.5 times fewer feed-forwards translating into a similar amount of speed-up in run-time.\n\nThe run-time of DeepVigor depends on:\n\n    \n  * Backpropagation execution by the number of neurons step1 (one for positive and one negative numbers per neuron);\n    \n  * Feed-forward execution by the number of searches for finding a positive or negative \u03b4_k^l per neuron, in which the best case is 0 searches (in case of zero gradients), the moderate case is 14 searches (in case of limited range initialization), and the worst case is 22 searches;\n    \n  * Vulnerability analysis of the neurons in the last layer is performed by simplified mathematics similar to Fig. <ref> and requires no iterative feed-forward or searching process through a wide range of numbers;\n    \n  * Bitflip mapping is merely performing a bitflip at each neuron and a comparison with the obtained vulnerability ranges.\n\n \n\n\n\n\u00a7 DISCUSSION\n \n\nDeepVigor method is validated in the previous section, and it is shown how it can evaluate the reliability of DNNs proficiently with shorter run-times than FI. Vulnerability ranges enable a fine-grain and accurate resilience evaluation for neural networks. They are not limited to representing the single bitflip fault model and the outcome of the analysis is valid for an erroneous output for the neurons covering several fault models. This method enables an accelerator-agnostic analysis for DNNs and results can be applied to different accelerators. \n\nThe outputs of DeepVigor provide different possibilities for exploiting techniques of reliability improvement, including:\n\n    \n  * Selective bits/neurons/layers hardening in accelerators based on the obtained BVF/NVF/LVF metrics;\n    \n  * Fault-aware mapping for neurons on the processing elements of accelerators as in <cit.>;\n    \n  * Applying range restriction for neurons' or layers' outputs for preventing faults propagation as in <cit.>.\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSIONS\n \n\nIn this work, a novel resilience analysis method for DNNs reliability assessment named DeepVigor is proposed. The output of this method is the vulnerability value ranges for all neurons through the DNNs which result in vulnerability factors for all layers, neurons, and bits of the DNN, separately. The method is validated extensively by fault injection and its feasibility to categorize non-critical and critical faults on complex DNNs with 99.9% to 100% accuracy is demonstrated. Moreover, vulnerability factors obtained by the proposed analysis provide fine-grain criticality metrics for DNNs' components leading to different reliability improvement techniques. The DeepVigor method is very proficient in the evaluation and explanation of the reliability of DNNs with shorter run-times than fault injection. \n\n\n\n\n\n\n\nIEEEtran\n\n\n\n"}