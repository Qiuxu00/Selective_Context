{"entry_id": "http://arxiv.org/abs/2303.06815v1", "published": "20230313021442", "title": "Provable Convergence of Tensor Decomposition-Based Neural Network Training", "authors": ["Chenyang Li", "Bo Shen"], "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "text": "\n\t\n\t\n\t\n\t\n   \n\n\n\n    \n\n\nProvable Convergence of Tensor Decomposition-Based Neural Network Training\n    Chenyang Li, Bo Shen*\n\nDepartment of Mechanical and Industrial Engineering, New Jersey Institute of Technology\nCorresponding Author: mailto:bo.shen@njit.edubo.shen@njit.edu  \n    \n\n==================================================================================================================================================================================\n\nfirstpage\n\n\n\n    Abstract\n\n\nAdvanced tensor decomposition, such as tensor train (TT), has been widely studied for tensor decomposition-based neural network (NN) training, which is one of the most common model compression methods. However, training NN with tensor decomposition always suffers significant accuracy loss and convergence issues. In this paper, a holistic framework is proposed for tensor decomposition-based NN training by formulating TT decomposition-based NN training as a nonconvex optimization problem. This problem can be solved by the proposed tensor block coordinate descent (tenBCD) method, which is a gradient-free algorithm. The global convergence of tenBCD to a critical point at a rate of \ud835\udcaa(1/k) is established with the Kurdyka \u0141ojasiewicz (K\u0141) property, where k is the number of iterations. The theoretical results can be extended to the popular residual neural networks (ResNets). The effectiveness and efficiency of our proposed framework are verified through an image classification dataset, where our proposed method can converge efficiently in training and prevent overfitting. \n \n\n\n\n\u00a7 KEYWORDS\nModel Compression, Tensor Train Decomposition, Global Convergence, Gradient-free Training.\n\n\n\n\n\n\u00a7 INTRODUCTION\n \n\nNeural network (NN) has revolutionized many facets of our modern society, such as image classification\u00a0<cit.>, object detection\u00a0<cit.>, speech recognition\u00a0<cit.>, etc. These advances have become possible because of algorithmic advances, large amounts of available data, and modern hardware. Despite their widespread success and popularity, there still remains a significant challenge in executing NNs with many parameters on edge devices. For most embedded and Internet-of-Things (IoT) systems, the sizes of many state-of-the-art NN models are too large, thereby causing high storage and computational demands and severely hindering the practical deployment of NNs. For example, wearable robots\u00a0<cit.>, such as exoskeletons, typically have limited processing power, memory, storage, and energy supply due to their small size and portability. In addition, these wearable devices rely on wireless communication with remote servers, as larger models would require more bandwidth and higher latency, leading to slower and less reliable performance. \n\nTo address this issue, numerous model compression techniques are proposed in the literature, which can be summarized into the following categories. (1) Pruning\u00a0<cit.>: this technique involves removing unnecessary connections or neurons from a pre-trained model.  This can result in a smaller network with similar performance. (2) Quantization\u00a0<cit.>: this involves reducing the number of bits required to represent the weights and activations in a neural network. For example, weights and activations may be represented using 8-bit integers instead of 32-bit floating-point numbers. (3) Structured sparsity <cit.>: this involves imposing a structured sparsity pattern on the weights of a model, such as by sparsifying entire rows or columns of weight matrices. (4) Knowledge distillation\u00a0<cit.>: this involves training a smaller model to mimic the behavior of a larger, more complex model, using the outputs of the larger model as labels. (5) Low-rank approximation\u00a0<cit.>: this technique involves approximating the weight matrices/tensors of a deep learning model with low-rank matrices/tensors.\n\nAmong all model compression methods, low-rank approximation, especially tensor decomposition\u00a0<cit.>, is an extremely attractive NN model compression technique since it can reduce the number of parameters in a model while maintaining a high level of accuracy. Specifically, tensor decomposition is a mathematical tool that explores the low tensor rank characteristics of large-scale tensor data, which stands out by offering an ultra-high compression ratio. By utilizing advanced tensor decomposition techniques like tensor train (TT)\u00a0<cit.>, it is possible to achieve more than a 1,000\u00d7 reduction in parameters for the input-to-hidden layers of neural network models\u00a0<cit.>. Moreover, these compression methods can also enhance the classification accuracy in video recognition tasks significantly. Given such impressive compression performance, there has been a surge of interest in exploring the potential of tensor decomposition-based neural network models in prior research efforts\u00a0<cit.>. Due to the benefits brought by the TT-based NN models, several TT-based NN hardware accelerators have been developed and implemented in different chip formats including digital CMOS ASIC\u00a0<cit.>, memristor ASIC\u00a0<cit.> and IoT board\u00a0<cit.>.\n\nAlthough tensor decomposition shows strong compression performance, the training of tensor decomposition-based NN is a quite challenging task\u00a0<cit.> because it involves tensor decomposition in NN training. In general, there are two ways to use tensor decomposition to obtain a compressed model: (1) Train from scratch in the decomposed format, and (2) Decompose a pre-trained uncompressed model and then retrain. In the first case, when the required tensor decomposition-based, e.g. TT-format model, is directly trained from scratch because the structure of the models is already pre-set to low tensor rank format before the training, the corresponding model capacity is typically limited as compared to the full-rank structure, thereby causing the training process being very sensitive to initialization and more challenging to achieve high accuracy. In the latter scenario, though the pre-trained uncompressed model provides a good initialization position, the straightforwardly decomposing full-rank uncompressed model into low tensor rank format causes inevitable and non-negligible approximation error, which is still very difficult to be recovered even after a long-time re-training period. \n\nNo matter which training strategy with tensor decomposition is adopted, the training of NN heavily relies on gradient-based methods, which make use of backpropagation\u00a0<cit.> to compute gradients of network parameters. These gradient-based methods are based on the Stochastic Gradient Descent (SGD) method\u00a0<cit.>. In recent years, a considerable amount of research has been dedicated to developing adaptive versions of the vanilla SGD algorithm. These adaptive variants include AdaGrad\u00a0<cit.>, RMSProp\u00a0<cit.>, Adam\u00a0<cit.>, and AMSGrad\u00a0<cit.>.  Despite the great success of these gradient-based methods, tensor decomposition always brings a linear increase in network depth, which implies training the tensor decomposition format NNs are typically more prone to the gradient vanishing problem\u00a0<cit.> and hence being difficult to be trained well. \n\n\nThis paper aims to address the current limitations and fully unlock the potential of tensor decomposition-based NN training. To achieve this objective, a holistic framework for tensor decomposition-based NN training is proposed, which formulates tensor train decomposition-based NN training as a nonconvex optimization problem. This problem can be solved by the proposed tensor block coordinate descent (tenBCD) methods.   BCD is a gradient-free method that has been recently adapted to NN training\u00a0<cit.>. The main reasons for the surge of attention of BCD algorithms are twofold. One reason is that they are gradient-free, and thus are able to deal with non-differentiable nonlinearities and potentially avoid the vanishing gradient issue. The other reason is that BCD can be easily implemented in a distributed and parallel manner, therefore in favor of distributed/federated scenarios. To summarize, the contributions of this paper are as follows:\n \n\n    \n  * A holistic framework is proposed for  tensor decomposition-based NN training, which involves a highly nonconvex optimization problem.\n    \n  * An efficient tensor BCD (tenBCD) algorithm is implemented to solve the proposed optimization problem;\n    \n  * Convergence of the iterative sequence generated by the tenBCD algorithm is analyzed, which is proved to be globally convergent to a critical point at a rate of \ud835\udcaa(1/k).\n\n\n\n\n\u00a7 BACKGROUND AND PRELIMINARIES\n\n\nIn Section\u00a0<ref>,   the notation and basics of multi-linear/tensor algebra used in this paper are reviewed. Then, tensor train decomposition\u00a0<cit.> is reviewed briefly in Section\u00a0<ref>. Afterward, the tensor train fully-connected layer\u00a0<cit.> is reviewed in Section\u00a0<ref>.\n\n\n \u00a7.\u00a7 Notation and Tensor Basis\n \n\nThroughout this paper, scalars are denoted by lowercase letters, e.g., x; vectors are denoted by lowercase boldface letters, e.g., x; matrices are denoted by uppercase boldface, e.g., X; and tensors are denoted by calligraphic letters, e.g., X. The order of a tensor is the number of its modes or dimensions. A real-valued tensor of order-d is denoted by \ud835\udcb3\u2208\u211d^n_1\u00d7 n_2\u00d7\u22ef\u00d7 n_d and its entries by  \ud835\udcb3(i_1, \u22ef, i_d). The inner product of two same-sized tensors \ud835\udcb3 and \ud835\udcb4 is the sum of the products of their entries, namely,  \u27e8\ud835\udcb3,\ud835\udcb4\u27e9 =\u2211_i_1 \u22ef\u2211_i_d X (i_1,\u2026 ,i_d) \u00b7\ud835\udcb4(i_1,\u2026 ,i_d). Following  the definition of inner product, the Frobenius norm of a tensor \ud835\udcb3 is defined as \ud835\udcb3_F=\u221a(\u27e8\ud835\udcb3,\ud835\udcb3\u27e9). \n \n\n\n\n \u00a7.\u00a7 Tensor Train (TT) Decomposition\n \n\n Given a tensor \ud835\udc9c\u2208\u211d^n_1\u00d7 n_2\u00d7\u22ef\u00d7 n_d, it can be decomposed to a sort of 3-order tensors via Tensor Train Decomposition (TTD)\u00a0<cit.> as follows:\n\n    \ud835\udc9c(i_1, i_2, \u22ef, i_d)     = \ud835\udca2_1(:, i_1,:) \ud835\udca2_2(:, i_2,:) \u22ef\ud835\udca2_d(:, i_d,:)  \n        =\u2211_\u03b1_0, \u03b1_1\u22ef\u03b1_d^r_0, r_1, \u22ef r_d\ud835\udca2_1(\u03b1_0, i_1, \u03b1_1) \ud835\udca2_2(\u03b1_1, i_2, \u03b1_2) \u22ef\ud835\udca2_d(\u03b1_d-1, i_d, \u03b1_d),\n where \ud835\udca2_k\u2208\u211d^r_k-1\u00d7 n_k\u00d7 r_k are called TT-cores for k= 1,2, \u22ef, d, and r=[r_0, r_1, \u22ef, r_d], r_0=r_d=1 are called TT-ranks, which determine the storage complexity of TT-format tensor.  The representation of  \ud835\udc9c via the explicit enumeration of all its entries requires storing \u03a0_k=1^d n_k numbers compared with \u2211_k=1^d n_k r_k-1 r_k numbers if the tensor is stored in TT-format. \n\n\n\n \u00a7.\u00a7 Tensor Train Fully-Connected Layer\n\n\nConsider a simple fully-connected layer with weight matrix W\u2208\u211d^M \u00d7 N and input x\u2208\u211d^N, where M=\u220f_k=1^d m_k and N=\u220f_k=1^d n_k, the output y\u2208\u211d^M is obtained by y=Wx. In order to transform this standard layer to TT fully-connected (TT-FC) layer,  the weight matrix W is first  tensorized to a d-order weight tensor \ud835\udcb2\u2208\u211d^(m_1\u00d7 n_1) \u00d7\u22ef\u00d7(m_d\u00d7 n_d) by reshaping and order transposing. Then \ud835\udcb2 can be decomposed to TT-format:\n\n    \ud835\udcb2((i_1, j_1), \u22ef,(i_d, j_d))=\ud835\udca2_1(:, i_1, j_1,:) \u22ef\ud835\udca2_d(:, i_d, j_d,:)\n  \nHere, each TT-core \ud835\udca2_k\u2208\u211d^r_k-1\u00d7 m_k\u00d7 n_k\u00d7 r_k is a 4-order tensor, which is one dimension more than the standard one\u00a0(<ref>) since the output and input dimensions of W are divided separately. Hence, the forward propagation on the TT-FC layer can be expressed in the tensor format as follows (the bias term is ignored here):\n\n    \ud835\udcb4(i_1, \u22ef, i_d)=\u2211_j_1, \u22ef, j_d\ud835\udca2_1(:, i_1, j_1,:) \u22ef\ud835\udca2_d(:, i_d, j_d,:) \ud835\udcb3(j_1, \u22ef, j_d)\n\nwhere \ud835\udcb3\u2208\u211d^m_1\u00d7\u22ef\u00d7 m_d and \ud835\udcb4\u2208\u211d^n_1\u00d7\u22ef\u00d7 n_d are the tensorized input and output corresponding to x and y, respectively. The details about the TT-FC layer are introduced in\u00a0<cit.>. As the TT-FC layer and the corresponding forward propagation schemes are formulated, standard stochastic gradient descent (SGD) algorithm can be used to update the TT-cores with the rank set r, which determines the target compression ratio. The initialization of the TT-cores can be either\nrandomly set or obtained from directly TT-decomposing a\npre-trained uncompressed model.\n\n\n\n\n\u00a7 PROPOSED METHODOLOGY\n \n\nConsider N-layer feedforward neural networks with N-1 hidden layers of the neural networks. Particularly, let n_i \u2208\u2115 be the number of hidden units in the i-th hidden layer for i=1, \u2026, N-1. Let n_0 and n_N be the number of units of input and output layers, respectively. Let W_i \u2208\u211d^n_i \u00d7 n_i-1 be the weight matrix between the (i-1)-th layer and the i-th layer for any i= 1, \u2026, N. Let \ud835\udcb5:={(x_j, y_j)}_j=1^n \u2282\u211d^n_0\u00d7\u211d^n_N be n samples, where y_j's are the one-hot vectors of labels. Denote \ud835\udcb2{W_i}_i=1^N, X:=(x_1, x_2, \u2026, x_n) \u2208\u211d^n_0 \u00d7 n and Y:=(y_1, y_2, \u2026, y_n) \u2208\u211d^n_N \u00d7 n. \n\n\n\n\n \u00a7.\u00a7 Problem Formulation\n \n\nAs shown in Figure\u00a0<ref>, the weight in i-th layer, namely, W_i, can be transformed into a tensor \ud835\udcb2_i. The tensor can be further decomposed into TT-format.   Therefore, tensor train decomposition-based NN training problem can be formulated as the following empirical risk (i.e., training loss) minimization:\n\n    min_\ud835\udcb2\u211b_n(\u03a6(X ; \ud835\udcb2), Y),   subject to \ud835\udcb2_i = TTD(r_i)   i=1, \u2026, N\n\nwhere \u211b_n(\u03a6(X ; \ud835\udcb2), Y):=1/n\u2211_j=1^n \u2113(\u03a6(x_j ; \ud835\udcb2), y_j) with loss function \u2113: \u211d^n_N\u00d7\u211d^n_N\u2192\u211d_+\u222a{0}, \u03a6(x_j ; \ud835\udcb2)=\u03c3_N(W_N \u03c3_N-1(W_N-1\u22efW_2 \u03c3_1(W_1 x_j))) is the neural network model with N layers. TTD(r_i) is the tensor train decomposition with rank r_i in\u00a0(<ref>) for weight tensor \ud835\udcb2_i  and \u03c3_i is the activation function of the i-th layer (generally, \u03c3_N\u2261Id is the identity function). \n\n\n\nNote that the NN training model\u00a0(<ref>) is highly nonconvex as the variables are coupled via the NN architecture, which brings many challenges for the design of efficient training algorithms and also its theoretical analysis. To make Problem\u00a0(<ref>) more computationally tractable, variable splitting is one of the most commonly used ways\u00a0<cit.>. The main idea of variable splitting is to transform a complicated problem (where the variables are coupled nonlinearly) into a relatively simpler one (where the variables are coupled much looser) by introducing some additional variables.\n\n\n\n\n\n\n\n\nConsidering general NN architectures, the regularized NN training model is applied here, which can reduce the original NN training model\u00a0(<ref>). Specifically, the variable splitting model is: \n    min _\ud835\udcb2, \ud835\udcb1\u2112_0(\ud835\udcb2, \ud835\udcb1)     :=\u211b_n(V_N ; Y)+\u2211_i=1^N\u03c4_i(W_i)+\u2211_i=1^N s_i(V_i) \n     subject to   U_i   =W_iV_i-1,V_i =\u03c3_i(U_i), \ud835\udcb2_i = TTD(r_i)   i=1, \u2026, N,\n\nwhere \u211b_n(V_N ; Y):=1/n\u2211_j=1^n\u2113((V_N)_: j, y_j) denotes the empirical risk, \ud835\udcb1:={V_i}_i=1^N,(V_N)_: j is the j-th column of V_N. In addition, \u03c4_i and s_i are extended-real-valued, nonnegative functions revealing the priors of the weight variable W_i and the state variable V_i (or the constraints on W_i and V_i ) for each i=1, \u2026 N, and define V_0:=X. To solve the formulation in\u00a0(<ref>), the following alternative minimization problem was considered:\n\n    min_\ud835\udcb2, \ud835\udcb1,\ud835\udcb0,\ud835\udca2   \u2112(\ud835\udcb2, \ud835\udcb1,\ud835\udcb0,\ud835\udca2):=  \u2112_0(\ud835\udcb2, \ud835\udcb1)+\u03b3/2\u2211_i=1^NV_i-\u03c3_i(U_i)_F^2\n       +\u03c1/2\u2211_i=1^NU_i-W_iV_i-1_F^2+ \u03c4/2\u2211_i=1^N\ud835\udcb2_i - TTD(r_i)_F^2\n \nwhere \u03b3,\u03c1,\u03c4>0 are hyperparameters for different regularization terms, \ud835\udcb0:={U_i}_i=1^N, and \ud835\udca2:={\ud835\udca2_i}_i=1^N is the set of TT-cores \ud835\udca2_i from i-th layer. The NN training model\u00a0(<ref>) can be very general, where: (a) \u2113 can be the squared, logistic, hinge, cross-entropy or other commonly used loss functions; (b) \u03c3_i can be ReLU, leaky ReLU, sigmoid, linear, polynomial, softplus or other commonly used activation functions; (c) \u03c4_i can be the squared \u2113_2 norm, the \u2113_1 norm, the elastic net, the indicator function of some nonempty closed convex set (such as the nonnegative closed half-space or a closed interval [0,1]); (d) s_i can be the \u2113_1 norm, the indicator function of some convex set with simple projection. Particularly, if there is no regularizer or constraint on W_i (or V_i), then \u03c4_i (or s_i) can be zero. The network architectures considered in this paper exhibit generality to various types of NNs, including but not limited to the fully (or sparse) connected MLPs, convolutional neural networks (CNN) and residual neural networks (ResNets)\u00a0<cit.>. \n\n As mentioned before, an existing TT-format NN is either 1) trained from randomly initialized tensor cores; or 2) trained from a direct decomposition of a pre-trained model. For the first strategy, it does not utilize any information related to the high-accuracy uncompressed model; while other model compression methods, e.g. pruning and knowledge distillation, have shown that proper utilization of the pre-trained models is very critical for NN compression. For the second strategy, though the knowledge of the pre-trained model is indeed utilized, because the pre-trained model generally lacks low TT-rank property, after direct low-rank tensor decomposition the approximation error is too significant to be properly recovered even using long-time re-training. Such inherent limitations of the existing training strategies, consequently, cause significant accuracy loss for the compressed TT-format NN models. To overcome these limitations, it is to maximally retain the knowledge contained in the uncompressed model, or in other words, minimize the approximation error after tensor decomposition with given target tensor ranks. In our formulation\u00a0(<ref>), \u2112_0(\ud835\udcb2, \ud835\udcb1) is the loss function of the uncompressed model while the regularization term \ud835\udcb2_i - TTD(r_i)_F^2 can encourage the uncompressed DNN models to gradually exhibit low tensor rank properties.   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      \n\n\n\n     \n\n\n\n\n\n\n\n \u00a7.\u00a7 Tensor BCD Algorithms\n \n\nNote that (<ref>) is a nonconvex optimization problem with multi-block variables. BCD  is a Gauss-Seidel type method for a minimization problem with multi-block variables to update all the variables cyclically while fixing the remaining blocks at their last updated values\u00a0<cit.>.  A tensor BCD (tenBCD) algorithm is developed for solving (<ref>). In this paper, proximal terms are added to some sub-problems arising from the tenBCD algorithm for two major reasons: (1) To practically stabilize the training process; (2) To yield the desired \u201csufficient descrease\u201d property for theoretical justification. At each iteration k, the tenBCD method with the backward order is considered for the updates of variables, i.e., the variables are updated from the output layer (layer N) to the input layer (layer 1). For each layer, the variables {V_i, U_i, W_i,\ud835\udca2_i} are updated cyclically for Problem\u00a0(<ref>). Since \u03c3_N\u2261Id, the output layer is paid special attention.  The tenBCD algorithms for (<ref>) can be summarized in Algorithm\u00a0<ref>. \n\n\n\n  \u00a7.\u00a7.\u00a7 Optimization over V_i\n\nAt iteration k, V_N can be updated through the following optimization problem \n\n    V_N^k=argmin_V_N{s_N(V_N)+\u211b_n(V_N ; Y)+\u03b3/2V_N-U_N^k-1_F^2+\u03b1/2V_N-V_N^k-1_F^2},\n\nwhere s_N(V_N)+\u211b_n(V_N ; Y) is regarded as a new proximal function s\u0303_N(V_N).  When i<N, V_i can be updated through the following optimization problem \n\n    V_i^k=argmin_V_i{s_i(V_i)+\u03b3/2V_i-\u03c3_i(U_i^k-1)_F^2+\u03c1/2U_i+1^k-W_i+1^kV_i_F^2 }.\n\nFor subproblem\u00a0(<ref>), \u03b1/2V_N-V_N^k-1_F^2 is the proximal term, where \u03b1>0 is the positive coefficient. \n\nThe above two problems\u00a0(<ref>) and\u00a0(<ref>) are simple proximal updates\u00a0<cit.> (or just least squares problems), which usually have closed-form solutions to many commonly used NNs. For V_N^k-update,  s_N(V_N)+\u211b_n(V_N ; Y) is regarded as a new proximal function s\u0303_N(V_N). Some typical examples leading to the closed-form solutions include: (a) s_i are 0 (i.e., no regularization), or the squared \u2113_2 norm, or the indicator function of a nonempty closed convex set with a simple projection like the nonnegative closed half-space and the closed interval [0,1]; (b) the loss function \u2113 is the squared loss or hinge loss.[The V_N-update with hinge loss and other smooth losses is provided in Appendix\u00a0<ref>.]\n\n\n\n  \u00a7.\u00a7.\u00a7 Optimization over U_i\n\nAt iteration k, U_N can be updated through the following optimization problem \n\n    U_N^k=argmin_U_N{\u03b3/2V_N^k-U_N_F^2+\u03c1/2U_N-W_N^k-1V_N-1^k-1_F^2 }\n\nU_i,i<N can be updated through the following optimization problem \n\n    U_i^k=argmin_U_i{\u03b3/2V_i^k-\u03c3_i(U_i)_F^2+\u03c1/2U_i-W_i^k-1V_i-1^k-1_F^2  +\u03b1/2U_i-U_i^k-1_F^2}\n\nFor subproblem\u00a0(<ref>), \u03b1/2U_i-U_i^k-1_F^2 is the proximal term. The subproblem\u00a0(<ref>) is a least-square optimization where the closed-form solution can be derived. The subproblem\u00a0(<ref>)  is a nonlinear and nonsmooth optimization where  \u03c3_i is ReLU or leaky ReLU. Accordingly,   the closed-form solution to solve the subproblem\u00a0(<ref>) is provided in Appendix\u00a0<ref>.\n\n\n\n  \u00a7.\u00a7.\u00a7 Optimization over W_i\n\nAt iteration k, W_i,i=1,\u2026,N can be updated through the following optimization problem \n\n    W_i^k=argmin_W_i{\u03c4_i(W_i)+\u03c1/2U_i^k-W_i V_i-1^k-1_F^2+\u03c4/2\ud835\udcb2_i - TTD(r_i)_F^2},\n\n The closed-form solution to solve the above optimization problem can be obtained  when \u03c4_i is 0 (i.e., no regularization), or the squared \u2113_2 norm (i.e., weight decay), or the indicator function of a nonempty closed convex set with a simple projection like the nonnegative closed half-space and the closed interval [0,1].\n\n\n\n  \u00a7.\u00a7.\u00a7 Optimization over \ud835\udca2_i\n\nAt iteration k, \ud835\udca2_i,i=1,\u2026,N can be updated through the following optimization problem\n\n    \ud835\udca2_i^k = argmin_\ud835\udca2_i{\u03c4/2\ud835\udcb2_i^k - TTD(r_i)_F^2 +\u03b1/2\ud835\udca2_i-\ud835\udca2_i^k-1_F^2}\n  where \u03b1/2\ud835\udca2_i-\ud835\udca2_i^k-1_F^2 is the proximal terms. \nThis subproblem is implemented in TensorLy package\u00a0<cit.>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      \n\n\n\n     \n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Global Convergence Analysis of tenBCD\n\n\nIn this section, the global convergence of Algorithm\u00a0<ref> for Problem\u00a0(<ref>) is established. Firstly, let h: \u211d^p\u2192\u211d\u222a{+\u221e} be an extended-real-valued function, its graph is defined by\nGraph(h) :={(x, y) \u2208\u211d^p\u00d7\u211d: y=h(x)}, and its domain by dom(h):={x\u2208\u211d^p: h(x)<+\u221e}. The subdifferential of a function is defined as follows. \n\n\n\n\u00a0Assume that f:\u00a0\u211d^p \u2192 (-\u221e,+\u221e) is a proper and lower semicontinuous function. \n  \n\n\t\n  * The domain of f is defined and denoted by domf{x\u2208\u211d^p:f(x)<+\u221e}\n\t\n  * For a given x\u2208domf, the Fr\u00e9chet subdifferential of f at x, written \u2202\u0302f(x), is the set of all vectors u\u2208\u211d^p that satisfy\n\n    lim_y\u2260xinf_y\u2192xf(y)-f(x) - \u27e8u , y-x\u27e9/y-x\u2265 0.\n\n\n  * The limiting-subdifferential, or simply the subdifferential, of f at x, written \u2202 f(x) is defined through the following closure process\n\n    \u2202 f(x):={u\u2208\u211d^p: \u2203x^k \u2192x,f(x^k) \u2192 f(x)  and u^k \u2208\u2202\u0302f(x^k) \u2192u as  k\u2192\u221e}.\n\n\t\n\nNow, our first main lemma about the sufficient decrease property of the iterative sequence  {\ud835\udcab^k:=({W_i^k}_i=1^N,{V_i^k}_i=1^N,{U_i^k}_i=1^N),{\ud835\udca2_i^k}_i=1^N}_k \u2208\u2115 from Algorithm\u00a0<ref> is ready to be introduced.\n\n \nGiven that \u03b1,\u03b3,\u03c1,\u03c4>0, {\ud835\udcab^k}_k \u2208\u2115 is the sequence generated by the tenBCD algorithm\u00a0<ref>, then the sequence satisfies\n\n    \u2112(\ud835\udcab^k) \u2264\u2112(\ud835\udcab^k-1)-\u03bb\ud835\udcab^k-\ud835\udcab^k-1_F^2.\n\nFor the case that V_N is updated via the proximal strategy,  \u03bb:=min{\u03b1/2, \u03b3+\u03c1/2,\u03c4/2}. For the case that V_N is update via the prox-linear strategy, \u03bb:=min{\u03b1/2, \u03b3+\u03c1/2,\u03c4/2, \u03b1+\u03b3-L_R/2}, where \u2207\u211b_n is Lipschitz continuous with a Lipschitz constant L_R and \u03b1>max{0, L_R-\u03b3/2}.\n\n\n\n\nThe inequality\u00a0(<ref>) can be developed by considering the descent quantity along the update of each block variable, i.e., {V_i}_i=1^N, {U_i}_i=1^N, {W_i}_i=1^N, and {\ud835\udca2_i}_i=1^N. To begin with, the following notations are introduced. Specifically, \nW_<i:=(W_1, W_2, \u2026, W_i-1), W_>i:=(W_i+1, W_i+1, \u2026, W_N), and V_<i, V_>i, U_<i, U_>i,\ud835\udca2_<i,\ud835\udca2_>i are defined similarly. We will consider each case separately. \n\n\n\n  \u00a7.\u00a7.\u00a7 Optimization over V_i\n\nV_N^k-block: at iteration k,  there are two ways to update the variable: (1) proximal update with closed-form solution: the following inequality can be derived\n\n    \u2112({W_i^k-1}_i=1^N, V_i<N^k-1, V_N^k,{U_i^k-1}_i=1^N, {\ud835\udca2_i^k-1}_i=1^N)\n    \u2264   \u2112({W_i^k-1}_i=1^N, V_i<N^k-1, V_N^k-1,{U_i^k-1}_i=1^N, {\ud835\udca2_i^k-1}_i=1^N)-\u03b1/2V_N^k-V_N^k-1_F^2.\n \nThe above inequality\u00a0(<ref>) is due to the fact that V_N^k is the optimal solution for subproblem\u00a0(<ref>). (2) proximal-linear case: let h^k(V_N):=s_N(V_N)+\u211b_n(V_N ; Y)+\u03b3/2V_N-U_N^k-1_F^2 and h\u0305^k(V_N):=s_N(V_N)+\u211b_n(V_N^k-1 ; Y)+\u27e8\u2207\u211b_n(V_N^k-1 ; Y), V_N-V_N^k-1\u27e9+\u03b1/2V_N-V_N^k-1_F^2 +\u03b3/2V_N-U_N^k-1_F^2. By the optimality of V_N^k and the strong convexity[The function h is called a strongly convex function with parameter \u03b3>0 if h(u) \u2265 h(v)+\u27e8\u2207 h(v), u-v\u27e9+\u03b3/2u-v^2.] of h\u0305^k(V_N) with modulus at least \u03b1 +\u03b3, the following holds\n\n    h\u0305^k(V_N^k) \u2264h\u0305^k(V_N^k-1) -\u03b1+\u03b3/2V_N^k-V_N^k-1_F^2,\n\nwhich implies \n\n\n    h^k(V_N^k)    \u2264  h^k(V_N^k-1) + \u211b_n(V_N^k ; Y)-\u211b_n(V_N^k-1 ; Y)-\u27e8\u2207\u211b_n(V_N^k-1 ; Y), V_N^k-V_N^k-1\u27e9\n       -(\u03b1+\u03b3/2)V_N^k-V_N^k-1_F^2\n       \u2264 h^k(V_N^k-1)-(\u03b1+\u03b3-L_R/2)V_N^k-V_N^k-1_F^2,\n\n\nwhere inequality\u00a0(<ref>) is due to the inequality\u00a0(<ref>),  the relationship between h^k(V_N^k-1) and h\u0305^k(V_N^k-1),  and the relationship between h^k(V_N^k) and h\u0305^k(V_N^k). The inequality\u00a0(<ref>) holds for the L_R-Lipschitz continuity of \u2207\u211b_n, i.e., the following inequality by\u00a0<cit.>\n\n    \u211b_n(V_N^k ; Y) \u2264\u211b_n(V_N^k-1 ; Y)+\u27e8\u2207\u211b_n(V_N^k-1 ; Y), V_N^k-V_N^k-1\u27e9+L_R/2V_N^k-V_N^k-1_F^2.\n\nAccording to  the relationship between h^k(V_N) and \u2112({W_i^k-1}_i=1^N, V_i<N^k-1, V_N,{U_i^k-1}_i=1^N, {\ud835\udca2_i^k-1}_i=1^N), and the inequality\u00a0(<ref>),\n\n    \u2112({W_i^k-1}_i=1^N, V_i<N^k-1, V_N^k,{U_i^k-1}_i=1^N, {\ud835\udca2_i^k-1}_i=1^N)\n    \u2264   \u2112({W_i^k-1}_i=1^N, V_i<N^k-1, V_N^k-1,{U_i^k-1}_i=1^N, {\ud835\udca2_i^k-1}_i=1^N)-(\u03b1+\u03b3-L_R/2)V_N^k-V_N^k-1_F^2.\n \n\n\nV_i^k-block (i<N): V_i^k is updated according to the following\n\n    V_i^k\u2190V_iargmin{s_i(V_i)+\u03b3/2V_i-\u03c3_i(U_i^k-1)_F^2+\u03c1/2U_i+1^k-W_i+1^kV_i_F^2}.\n\nLet h^k(V_i)=s_i(V_i)+\u03b3/2V_i-\u03c3_i(U_i^k-1)_F^2+\u03c1/2U_i+1^k-W_i+1^kV_i_F^2. By the convexity of s_i, the function h^k(V_i) is a strongly convex function with modulus no less than \u03b3. By the optimality of V_i^k, the following holds\n\n    h^k(V_i^k) \u2264 h^k(V_i^k-1) - \u03b3/2V_i^k-V_i^k-1_F^2.\n\nBased on the inequality\u00a0(<ref>), it yields for \n\n    \u2112(W_\u2264 i^k-1, W_>i^k, V_<i^k-1, V_i^k, V_>i^k, U_\u2264 i^k-1, U_>i^k, \ud835\udca2_\u2264 i^k-1, \ud835\udca2_>i^k)\n    \u2264   \u2112(W_\u2264 i^k-1, W_>i^k, V_<i^k-1, V_i^k-1, V_>i^k, U_\u2264 i^k-1, U_>i^k, \ud835\udca2_\u2264 i^k-1, \ud835\udca2_>i^k) - \u03b3/2V_i^k-V_i^k-1_F^2\n\nfor i=1, \u2026, N-1, where \n\n    h^k(V_i^k) -  h^k(V_i^k-1)     = \u2112(W_\u2264 i^k-1, W_>i^k, V_<i^k-1, V_i^k, V_>i^k, U_\u2264 i^k-1, U_>i^k, \ud835\udca2_\u2264 i^k-1, \ud835\udca2_>i^k) \n        - \u2112(W_\u2264 i^k-1, W_>i^k, V_<i^k-1, V_i^k-1, V_>i^k, U_\u2264 i^k-1, U_>i^k, \ud835\udca2_\u2264 i^k-1, \ud835\udca2_>i^k).\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Optimization over U_i\n\nU_N^k-block: similar to the inequality\u00a0(<ref>), the  descent quantity is established as follows\n\n    \u2112(W_\u2264 N^k-1, V_<N^k-1, V_N^k, U_<N^k-1, U_N^k, \ud835\udca2_\u2264 N^k-1) \n    \u2264   \u2112(W_\u2264 N^k-1, V_<N^k-1, V_N^k, U_<N^k-1, U_N^k-1, \ud835\udca2_\u2264 N^k-1) - \u03b3+\u03c1/2U_N^k-U_N^k-1_F^2,\n\nwhere the above inequality is because the objective function in subproblem\u00a0(<ref>) is a strongly convex function with modulus at least \u03b3+\u03c1.\n\nU_i^k-block (i<N): the following can be obtained\n\n    \u2112(W_\u2264 i^k-1,  W_>i^k, V_<i^k-1, V_\u2265 i^k, U_<i^k-1, U_i^k, U_>i^k,\ud835\udca2_\u2264 i^k-1,  \ud835\udca2_>i^k) \n    \u2264   \u2112(W_\u2264 i^k-1,  W_>i^k, V_<i^k-1, V_\u2265 i^k, U_<i^k-1, U_i^k-1, U_>i^k,\ud835\udca2_\u2264 i^k-1,  \ud835\udca2_>i^k) - \u03b1/2U_i^k-U_i^k-1_F^2\n\nfor i=1, \u2026, N-1 since U_i^k is the optimal solution for subproblem\u00a0(<ref>).\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Optimization over W_i\n\nW_i^k-block (i\u2264 N): W_i^k is updated according to the following\n\n    W_i^k\u2192W_iargmin{r_i(W_i)+\u03c1/2U_i^k-W_iV_i-1^k-1_F^2+\u03c4/2\ud835\udcb2_i - TTD(r_i)_F^2},\n\nwhere h^k(W_i)=r_i(W_i)+\u03c1/2U_i^k-W_iV_i-1^k-1_F^2+\u03c4/2\ud835\udcb2_i - TTD(r_i)_F^2 is  a strongly convex function with modulus at least \u03c4. Accordingly, the following holds\n\n    \u2112(W_<i^k-1, W_i^k, W_>i^k, V_<i^k-1,  V_\u2265 i^k, U_<i^k-1,  U_\u2265 i^k, \ud835\udca2_\u2264 i^k-1,  \ud835\udca2_> i^k) \n    \u2264   \u2112(W_<i^k-1, W_i^k-1, W_>i^k, V_<i^k-1,  V_\u2265 i^k, U_<i^k-1,  U_\u2265 i^k, \ud835\udca2_\u2264 i^k-1,  \ud835\udca2_> i^k)-\u03c4/2W_i^k-W_i^k-1_F^2,\n\nwhich is due to the relationship between h^k(W_i) and \u2112(W_<i^k-1, W_i, W_>i^k, V_<i^k-1,  V_\u2265 i^k, U_<i^k-1,  U_\u2265 i^k, \ud835\udca2_\u2264 i^k-1,  \ud835\udca2_> i^k).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Optimization over \ud835\udca2_i\n\n\ud835\udca2_i-block (i\u2264 N): the descent quantity for \ud835\udca2_i can be derived as follows\n\n    \u2112(W_<i^k-1, W_\u2265 i^k, V_<i^k-1,  V_\u2265 i^k, U_<i^k-1,  U_\u2265 i^k, \ud835\udca2_< i^k-1,  \ud835\udca2_i^k, \ud835\udca2_> i^k) \n    \u2264   \u2112(W_<i^k-1, W_\u2265 i^k, V_<i^k-1,  V_\u2265 i^k, U_<i^k-1,  U_\u2265 i^k, \ud835\udca2_< i^k-1,  \ud835\udca2_i^k-1, \ud835\udca2_> i^k)-\u03b1/2\ud835\udca2_i^k-\ud835\udca2_i^k-1_F^2,\n\nwhere the above inequality\u00a0(<ref>) is due to the fact that \ud835\udca2_i^k is the optimal solution for subproblem\u00a0(<ref>).\n\nBy summing up inequalities\u00a0(<ref>) (or\u00a0(<ref>)), (<ref>), (<ref>), (<ref>), and\u00a0(<ref>), it yields the\n\n    \u2112(\ud835\udcab^k) \u2264\u2112(\ud835\udcab^k-1)-\u03bb\ud835\udcab^k-\ud835\udcab^k-1_F^2,\n\nwhere \u03bb:=min{\u03b1/2, \u03b3+\u03c1/2,\u03c4/2} (or \u03bb:=min{\u03b1/2, \u03b3+\u03c1/2,\u03c4/2, \u03b1+\u03b3-L_R/2}). \n\n\n\n\n\n\n\n\n\n\nFrom Lemma\u00a0<ref>, the Lagrangian sequence {\u2112(\ud835\udcab^k)}__k \u2208\u2115 is monotonically decreasing, and the descent quantity of each iterate can be lower bounded by the discrepancy between the current iterate and its previous iterate. This lemma is crucial for the global convergence of a nonconvex algorithm. It tells at least the following four important items: (i) {\u2112(\ud835\udcab^k)}_k \u2208\u2115 is convergent if \u2112 is lower bounded; (ii) {\ud835\udcab^k}_k \u2208\u2115 itself is bounded if \u2112 is coercive and \ud835\udcab^0 is finite; (iii) {\ud835\udcab^k}_k \u2208\u2115 is square summable, i.e., \u2211_k=1^\u221e\ud835\udcab^k-\ud835\udcab^k-1_F^2<\u221e, implying its asymptotic regularity, i.e., \ud835\udcab^k-\ud835\udcab^k-1_F\u2192 0 as k \u2192\u221e; and (iv) 1/K\u2211_k=1^K\ud835\udcab^k-\ud835\udcab^k-1_F^2\u2192 0 at a rate of \ud835\udcaa(1 / K). Leveraging Lemma\u00a0<ref>, we can establish the global convergence (i.e., the whole sequence convergence) of tenBCD algorithm\u00a0<ref> in NN training settings. In contrast, <cit.> only establish the subsequence convergence of SGD in NN training settings. Such a gap between the subsequence convergence of SGD in <cit.>  and the whole sequence convergence of tenBCD algorithm\u00a0<ref> in this paper exists mainly because SGD can only achieve the descent property but not the sufficient descent property.\n\nIt can be noted from Lemma\u00a0<ref> that neither multiconvexity and differentiability nor Lipschitz differentiability assumptions are imposed on the NN training models to yield this lemma, as required in the literature <cit.>. Instead, we mainly exploit the proximal strategy for all nonstrongly convex subproblems in Algorithm\u00a0<ref> to establish this lemma.\n\n Our second main lemma is about the subgradient lower bound.  \n \n    Under the same conditions of Lemma\u00a0<ref>, let \u212c be an upper bound of \ud835\udcab^k-1 and \ud835\udcab^k for any positive integer k, L_\u212c be a uniform Lipschitz constant of \u03c3_i on the bounded set {\ud835\udcab:\ud835\udcab_F\u2264\u212c}, and\n\n    \u03b4:=max{\u03b3, \u03b1+\u03c1\u212c, \u03b1+\u03b3 L_\u212c, 2 \u03c1\u212c+ 2\u03c1\u212c^2, \u03b1 + \u221a(N)\u03c4\u212c^N-1}\n\n(or, for the prox-linear case, \u03b4:=max{\u03b3, L_R+\u03b1+\u03c1\u212c, \u03b1+\u03b3 L_\u212c, 2 \u03c1\u212c+ 2 \u03c1\u212c^2, \u03b1 + \u221a(N)\u03c4\u212c^N-1}), then for any positive integer k, there holds,\n\n    dist(0, \u2202\u2112 (\ud835\udcab^k))    \u2264\u03b4\u2211_i=1^N[W_i^k-W_i^k-1_F+V_i^k-V_i^k-1_F+U_i^k-U_i^k-1_F+\ud835\udca2_i^k-\ud835\udca2_i^k-1_F] \n       \u2264\u03b4\u0305\ud835\udcab^k-\ud835\udcab^k-1_F\n\nwhere \u03b4\u0305:= \u03b4\u221a(4 N), dist(0, \ud835\udcae):=inf _s\u2208\ud835\udcaes_F for a set \ud835\udcae, and\n\n    \u2202\u2112(\ud835\udcab^k):=({\u2202_W_i\u2112}_i=1^N,{\u2202_V_i\u2112}_i=1^N,{\u2202_U_i\u2112}_i=1^N,{\u2202_\ud835\udca2_i\u2112}_i=1^N)(\ud835\udcab^k).\n\n \n\n    The inequality\u00a0(<ref>) is established via bounding each term of \u2202\u2112(\ud835\udcab^k). Specifically, the following holds\n\n\n    0\u2208\u2202 s_N(V_N^k)+\u2202\u211b_n(V_N^k ; Y)+\u03b3(V_N^k-U_N^k-1)+\u03b1(V_N^k-V_N^k-1), \n       0\u2208\u2202 s_N(V_N^k)+\u2207\u211b_n(V_N^k-1 ; Y)+\u03b3(V_N^k-U_N^k-1)+\u03b1(V_N^k-V_N^k-1),  (proximal-linear)\n       0=\u03b3(U_N^k-V_N^k)+\u03c1(U_N^k-W_N^k-1V_N-1^k-1), \n       0\u2208\u2202\u03c4_N(W_N^k)+\u03c1(W_N^kV_N-1^k-1-U_N^k) V_N-1^k-1^\u22a4+\u03c4(W_N^k-TTD^k-1(r_N)), \n       0\u2208\u2202(\u03c4/2\ud835\udcb2_N^k - TTD^k(r_N)_F^2) +\u03b1(\ud835\udca2_N^k-\ud835\udca2_N^k-1),\n \n\nwhere (<ref>), (<ref>), (<ref>),  (<ref>), and  (<ref>) are  due to the optimality conditions of all updates in (<ref>), (<ref>), (<ref>), (<ref>), and (<ref>), respectively.\n\nFor i=N-1, \u2026, 1, the following holds\n\n\n    0\u2208\u2202 s_i(V_i^k)+\u03b3(V_i^k-\u03c3_i(U_i^k-1))+\u03c1W_i+1^k^\u22a4(W_i+1^kV_i^k-U_i+1^k), \n       0\u2208\u03b3[(\u03c3_i(U_i^k)-V_i^k) \u2299\u2202\u03c3_i(U_i^k)]+\u03c1(U_i^k-W_i^k-1V_i-1^k-1)+\u03b1(U_i^k-U_i^k-1), \n       0\u2208\u2202\u03c4_i(W_i^k)+\u03c1(W_i^kV_i-1^k-1-U_i^k) V_i-1^k-1^\u22a4+\u03c4(W_i^k-TTD^k-1(r_i)), \n       0\u2208\u2202(\u03c4/2\ud835\udcb2_i^k - TTD^k(r_i)_F^2) +\u03b1(\ud835\udca2_i^k-\ud835\udca2_i^k-1),\n \n\nwhere (<ref>), (<ref>), (<ref>), and (<ref>) are due to the optimality conditions of all updates in (<ref>), (<ref>), (<ref>), and (<ref>), respectively. V_0^k\u2261V_0=X for all k, and \u2299 is the Hadamard product. Through the above relationship\u00a0(<ref>), we have\n\n    -\u03b1(V_N^k-V_N^k-1)-\u03b3(U_N^k-U_N^k-1) \u2208\u2202 s_N(V_N^k)+\u2202\u211b_n(V_N^k ; Y)+\u03b3(V_N^k-U_N^k)=\u2202_V_N\u2112(\ud835\udcab^k), \n       (\u2207\u211b_n(V_N^k ; Y)-\u2207\u211b_n(V_N^k-1 ; Y))-\u03b1(V_N^k-V_N^k-1)-\u03b3(U_N^k-U_N^k-1) \u2208\u2202_V_N\u2112(\ud835\udcab^k),  (proximal-linear)\n        -\u03c1(W_N^k-W_N^k-1) V_N-1^k-\u03c1W_N^k-1(V_N-1^k-V_N-1^k-1)=\u03b3(U_N^k-V_N^k)+\u03c1(U_N^k-W_N^k V_N-1^k)=\u2202_U_N\u2112(\ud835\udcab^k), \n       \u03c1W_N^k[V_N-1^k(V_N-1^k-V_N-1^k-1)^\u22a4+(V_N-1^k-V_N-1^k-1) V_N-1^k-1^\u22a4]-\u03c1U_N^k(V_N^k-V_N^k-1)^\u22a4+\u03c4(TTD^k(r_N)-TTD^k-1(r_N)) \n       \u2208\u2202 r_N(W_N^k)+\u03c1(W_N^k V_N-1^k-U_N^k) V_N-1^k^\u22a4+\u03c4(W_N^k-TTD^k(r_i))=\u2202_W_N\u2112(\ud835\udcab^k), \n        -\u03b1(\ud835\udca2_N^k-\ud835\udca2_N^k-1) \u2208\u2202_\ud835\udca2_N\u2112(\ud835\udcab^k).\n\nFor i=N-1, \u2026, 1, the relationship\u00a0(<ref>) implies\n\n    -\u03b3(\u03c3_i(U_i^k)-\u03c3_i(U_i^k-1)) \u2208\u2202 s_i(V_i^k)+\u03c1(V_i^k-\u03c3_i(U_i^k))+\u03b3W_i+1^k^\u22a4(W_i+1^k V_i^k-U_i+1^k)=\u2202_V_i\u2112(\ud835\udcab^k), \n        -\u03c1W_i^k-1(V_i-1^k-V_i-1^k-1)-\u03c1(W_i^k-W_i^k-1) V_i-1^k-\u03b1(U_i^k-U_i^k-1) \n       \u2208\u03b3[(\u03c3_i(U_i^k)-V_i^k) \u2299\u2202\u03c3_i(U_i^k)]+\u03c1(U_i^k-W_i^k V_i-1^k)=\u2202_U_i\u2112(\ud835\udcab^k) , \n       \u03c1W_i^k[V_i-1^k(V_i-1^k-V_i-1^k-1)^\u22a4+(V_i-1^k-V_i-1^k-1) V_i-1^k-1]-\u03c1U_i^k(V_i-1^k-V_i-1^k-1)^\u22a4+\u03c4(TTD^k(r_i)-TTD^k-1(r_i)) \n       \u2208\u2202 r_i(W_i^k)+\u03c1(W_i^k V_i-1^k-U_i^k) V_i-1^k^\u22a4=\u2202_W_i\u2112(\ud835\udcab^k),\n        -\u03b1(\ud835\udca2_i^k-\ud835\udca2_i^k-1) \u2208\u2202_\ud835\udca2_i\u2112(\ud835\udcab^k).\n\nBased on the above relationships, and by the Lipschitz continuity of the activation function on the bounded set {\ud835\udcab:\ud835\udcab_F\u2264\u212c} and the bounded assumption of both \ud835\udcab^k-1 and \ud835\udcab^k, we have\n\n    [                            \u03be_V_N^k_F\u2264\u03b1V_N^k-V_N^k-1_F+\u03b3U_N^k-U_N^k-1_F,                                                    \u03be_V_N^k\u2208\u2202_V_N\u2112(\ud835\udcab^k),; (or \u03be_V_N^k_F\u2264(L_R+\u03b1)V_N^k-V_N^k-1_F+\u03b3U_N^k-U_N^k-1_F)  proximal-linear                                                                        ;                      \u03be_U_N^k_F\u2264\u03c1\u212cW_N^k-W_N^k-1_F+\u03c1\u212cV_N-1^k-V_N-1^k-1_F,                                                    \u03be_U_N^k\u2208\u2202_U_N\u2112(\ud835\udcab^k),;                  \u03be_W_N^k_F\u2264 2 \u03c1\u212c^2V_N-1^k-V_N-1^k-1_F+\u03c1\u212cV_N^k-V_N^k-1_F                                                                        ;                                            +\u03c4TTD^k(r_N)-TTD^k-1(r_N)_F,                                                   \u03be_W_N^k \u2208\u2202_W_N\u2112(\ud835\udcab^k),;                                             \u03be_\ud835\udca2_N^k_F\u2264\u03b1\ud835\udca2_N^k-\ud835\udca2_N^k-1_F,                                                   \u03be_\ud835\udca2_N^k \u2208\u2202_\ud835\udca2_N\u2112(\ud835\udcab^k), ]\n\nand for i=N-1, \u2026, 1,\n\n    [                                     \u03be_V_i^k_F\u2264\u03b3 L_\u212cU_i^k-U_i^k-1_F,                                                \u03be_V_i^k\u2208\u2202_V_i\u2112(\ud835\udcab^k),; \u03be_U_i^k_F\u2264\u03c1\u212cV_i-1^k-V_i-1^k-1_F+\u03c1\u212cW_i^k-W_i^k-1_F+\u03b1U_i^k-U_i^k-1_F,                                                \u03be_U_i^k\u2208\u2202_U_i\u2112(\ud835\udcab^k),; \u03be_W_i^k_F\u2264(2\u03c1\u212c^2+\u03c1\u212c)V_i-1^k-V_i-1^k-1_F+\u03c4TTD^k(r_i)-TTD^k-1(r_i)_F,                                                \ud835\udca2_W_i^k\u2208\u2202_W_i\u2112(\ud835\udcab^k),;                                         \u03be_\ud835\udca2_i^k_F\u2264\u03b1\ud835\udca2_i^k-\ud835\udca2_i^k-1_F,                                               \u03be_\ud835\udca2_i^k \u2208\u2202_\ud835\udca2_i\u2112(\ud835\udcab^k). ]\n\nIn addition, we have the following bound\n\n    TTD^k(r_i)-TTD^k-1(r_i)_F\u2264\u221a(N)\u212c^N-1\ud835\udca2_i^k-\ud835\udca2_i^k-1_F.\n\nSumming the above inequalities\u00a0(<ref>),(<ref>), and\u00a0(<ref>), the subgradient lower bound\u00a0(<ref>) can be obtained for any positive integer k\n\n    dist(0, \u2202\u2112 (\ud835\udcab^k))    \u2264\u03b4\u2211_i=1^N[W_i^k-W_i^k-1_F+V_i^k-V_i^k-1_F+U_i^k-U_i^k-1_F+\ud835\udca2_i^k-\ud835\udca2_i^k-1_F] \n       \u2264\u03b4\u0305\ud835\udcab^k-\ud835\udcab^k-1_F,\n\nwhere\n\n    \u03b4:=max{\u03b3, \u03b1+\u03c1\u212c, \u03b1+\u03b3 L_\u212c, 2 \u03c1\u212c+ 2\u03c1\u212c^2, \u03b1 + \u221a(N)\u03c4\u212c^N-1},\n\n(or, for the prox-linear case, \u03b4:=max{\u03b3, L_R+\u03b1+\u03c1\u212c, \u03b1+\u03b3 L_\u212c, 2 \u03c1\u212c+ 2 \u03c1\u212c^2, \u03b1 + \u221a(N)\u03c4\u212c^N-1}). \n\n\n \u00a0A necessary condition for x to be a minimizer of a proper and lower semicontinuous (PLSC) function  f is that\n \n    0\u2208\u2202 f(x).\n\n A point that satisfies (<ref>) is called limiting-critical or simply critical.\n\n\n \nAny iterative algorithm for solving an optimization problem over a set X, is said to be globally convergent if for any starting point x_0 \u2208 X, the sequence generated by the algorithm always has an accumulation critical point.\n\n\nTo build the global convergence of our iterative sequence  {\ud835\udcab^k}_k \u2208\u2115 from Algorithm\u00a0<ref>, the function \u2112(\ud835\udcb2, \ud835\udcb1,\ud835\udcb0,\ud835\udca2) needs to have the Kurdyka \u0141ojasiewicz (K\u0141)  property as follows\n\nA real function f: \u211d^p \u2192 (-\u221e,+\u221e] has the  Kurdyka \u0141ojasiewicz (K\u0141) property, namely, for any point u\u0305\u2208\u211d^p, in a neighborhood N(u\u0305,\u03c3), there exists a desingularizing function \u03d5(s)=cs^1-\u03b8 for some c>0 and \u03b8\u2208 [0,1) such that \n\n    \u03d5'(|f(u)-f(u\u0305)|)d(0,\u2202 f(u))\u2265 1\n\nfor any  u\u2208 N(u\u0305,\u03c3) and  f(u)\u2260 f(u\u0305).\n    \nThe real analytic and semi-algebraic functions, which are related to K\u0141  property, are introduced below.\n\n    A function h with domain an open set U \u2282\u211d and range the set of either all real or complex numbers, is said to be real analytic at u if the function h may be represented by a convergent power series on some interval of positive radius centered at u, i.e., h(x)= \u2211_j=0^\u221e\u03b1_j(x-u)^j, for some {\u03b1_j}\u2282\u211d. The function is said to be real analytic on V \u2282 U if it is real analytic at each u \u2208 V <cit.>. The real analytic function f over \u211d^p for some positive integer p>1 can be defined similarly.\n\n\n\n   \nA subset S of \u211d^p is a real semi-algebraic set if there exists  a finite number of real polynomial functions g_ij,h_ij:  \u211d^p \u2192\u211d such that S=\u222a_j=1^q\u2229_i=1^m{u\u2208\u211d^p:g_ij(u)=0  and h_ij(u)<0 }. In addition, a function h:\u211d^p+1\u2192\u211d\u222a+\u221e is called \tsemi-algebraic if its graph {(u, t)\u2208\u211d^p+1: h(u)=t } is a real semi-algebraic set.\n \n \nBased on the above definitions, the following lemma can be obtained.\n\nMost of the commonly used NN training models\u00a0(<ref>) can be verified to satisfy the following \n  \n\n    \n  * the loss function \u2113 is a proper lower semicontinuous and nonnegative function. For example,  the squared, logistic, hinge, or cross-entropy losses.\n    \n  * the activation functions \u03c3_i(i=1 \u2026, N-1) are Lipschitz continuous on any bounded set. For example, ReLU, leaky ReLU, sigmoid, hyperbolic tangent, linear, polynomial, or softplus activations.\n    \n  * the regularizers \u03c4_i and s_i(i=1, \u2026, N) are nonegative lower semicontinuous convex functions. \u03c4_i and s_i are the squared \u2113_2 norm, the \u2113_1 norm, the elastic net, the indicator function of some nonempty closed convex set (such as the nonnegative closed half-space, box set or a closed interval [0,1]), or 0 if no regularization.\n    \n  * all these functions \u2113, \u03c3_i, \u03c4_i and s_i(i=1, \u2026, N) are either real analytic or semialgebraic, and continuous on their domains.\n\nAccordingly, the objective function \u2112(\ud835\udcb2, \ud835\udcb1,\ud835\udcb0,\ud835\udca2) in\u00a0(<ref>) has Kurdyka \u0141ojasiewicz (K\u0141) property. \n \n\n\n\nOn the loss function \u2113: Since these losses are all nonnegative and continuous on their domains, they are proper lower semicontinuous and lower bounded by 0. In the following, we only verify that they are either real analytic or semialgebraic.\n  \n\n    \n  * If \u2113(t) is the squared (t^2) or exponential  (e^t) loss, then according to\u00a0<cit.>, they are real analytic.\n    \n  * If \u2113(t) is the logistic loss (log (1+e^-t)), since it is a composition of logarithm and exponential functions which both are real analytic, thus according to\u00a0<cit.>, the logistic loss is real analytic.\n    \n  * If \u2113(u ; y) is the cross-entropy loss, i.e., given y\u2208\u211d^d_N, \u2113(u ; y)=-1/d_N[\u27e8y, logy(u)\u27e9+\u27e81-y, log (1-y(u))\u27e9], where log is performed elementwise and (y(u)_i)_1 \u2264 i \u2264 d_N:=((1+e^-u_i)^-1)_1 \u2264 i \u2264 d_N for any u\u2208\u211d^d_N, which can be viewed as a linear combination of logistic functions, then by (a2) and <cit.>, it is also analytic.\n    \n  * If \u2113 is the hinge loss, i.e., given y\u2208\u211d^d_N, \u2113(u ; y):=max{0,1-\u27e8u, y\u27e9} for any u\u2208\u211d^d_N, by\u00a0<cit.>, it is semialgebraic, because its graph is cl(\ud835\udc9f), the closure of the set \ud835\udc9f, where \ud835\udc9f={(u, z): 1-\u27e8u, y\u27e9-z=0, 1-u\u227b 0}\u222a{(u, z): z=0,\u27e8u, y\u27e9-1>0}.\n\n\n\n\nOn the activation function \u03c3_i: Since all the considered specific activations are continuous on their domains, they are Lipschitz continuous on any bounded set. In the following, we only need to check that they are either real analytic or semialgebraic.\n  \n\n    \n  * If \u03c3_i is a linear or polynomial function, then according to\u00a0<cit.> is real analytic.\n    \n  * If \u03c3_i(t) is sigmoid, (1+e^-t)^-1, or hyperbolic tangent, tanh(t):=e^t-e^-t/e^t+e^-t, then the sigmoid function is a composition g \u2218 h of these two functions where g(u)=1/1+u, u>0 and h(t)=e^-t (resp. g(u)=1-2/u+1, u>0 and h(t)=e^2 t in the hyperbolic tangent case). According to\u00a0<cit.>, g and h in both cases are real analytic. Thus,  sigmoid and hyperbolic tangent functions are real analytic.\n    \n  * If \u03c3_i is ReLU, i.e., \u03c3_i(u):=max{0, u}, then we can show that ReLU is semialgebraic since its graph is cl( .\ud835\udc9f), the closure of the set \ud835\udc9f, where \ud835\udc9f={(u, z): u-z=0, u>0}\u222a{(u, z): z=0,-u>0}.\n    \n  * Similar to the ReLU case, if \u03c3_i is leaky ReLU, i.e., \u03c3_i(u)=u if u>0, otherwise \u03c3_i(u)=a u for some a>0, then we can similarly show that leaky ReLU is semialgebraic since its graph is cl(\ud835\udc9f), the closure of the set \ud835\udc9f, where \ud835\udc9f={(u, z): u-z=0, u>0}\u222a{(u, z): a u-z=0,-u>0}.\n    \n  * If \u03c3_i is polynomial, then according to\u00a0<cit.>, it is real analytic.\n    \n  * If \u03c3_i is softplus, i.e., \u03c3_i(u)=1/tlog (1+e^t u) for some t>0, since it is a composition of two analytic functions 1/tlog (1+u) and e^t u, then according to\u00a0<cit.>, it is real analytic.\n\nOn \u03c4_i(W_i), s_i(V_i): By the specific forms of these regularizers, they are nonnegative, lower semicontinuous and continuous on their domain. In the following, we only need to verify they are either real analytic and semialgebraic.\n  \n\n\n  * the squared \u2113_2 norm \u00b7_2^2: According to\u00a0<cit.>, the \u2113_2 norm is semialgebraic, so is its square where g(t)=t^2 and h(W)=W_2.\n\n\n  * the squared Frobenius norm \u00b7_F^2: The squared Frobenius norm is semiaglebraic since it is a finite sum of several univariate squared functions.\n\n\n  * the elementwise 1-norm \u00b7_1,1: Note that W_1,1=\u2211_i, j|W_i j| is the finite sum of absolute functions h(t)=|t|. According to\u00a0<cit.>, the absolute value function is semialgebraic since its graph is the closure of the following semialgebraic set \ud835\udc9f={(t, s): t+s=0,-t>0}\u222a{(t, s): t-s=0, t>0}. Thus, the elementwise 1-norm is semialgebraic.\n\n\n  * the elastic net: Note that the elastic net is the sum of the elementwise 1-norm and the squared Frobenius norm. Thus, by (c2), (c3), and\u00a0<cit.>, the elastic net is semialgebraic.\n\n\n  * If \u03c4_i or s_i is the indicator function of nonnegative closed half-space or a closed interval (box constraints), by\u00a0<cit.>, any polyhedral set is semialgebraic such as the nonnegative orthant \u211d_+^p \u00d7 q={W\u2208\u211d^p \u00d7 q, W_i j\u2265 0, \u2200 i, j}, and the closed interval. Thus, \u03c4_i or s_i is semialgebraic in this case.\n\n\nWe first verify the K\u0141 property of \u2112. From\u00a0(<ref>), we have\n\n    \u2112(\ud835\udcb2, \ud835\udcb1,\ud835\udcb0,\ud835\udca2)\n            :=   \u211b_n(V_N ; Y)+\u2211_i=1^N r_i(W_i)+\u2211_i=1^N s_i(V_i)\n    \n            +   \u03b3/2\u2211_i=1^NV_i-\u03c3_i(U_i)_F^2+\u03c1/2\u2211_i=1^NU_i-W_iV_i-1_F^2+ \u03c4/2\u2211_i=1^N\ud835\udcb2_i - TTD(r_i)_F^2,\n\nwhich mainly includes the following types of functions, i.e.,\n\n    \u211b_n(V_N ; Y), \u03c4_i(W_i), s_i(V_i),V_i-\u03c3_i(U_i)_F^2,U_i-W_iV_i-1_F^2,\u2211_i=1^N\ud835\udcb2_i - TTD(r_i)_F^2.\n\nTo verify the K\u0141 property of the function \u2112, we consider the above functions one. \n\nOn \u211b_n(V_N ; Y): Note that given the output data Y, \u211b_n(V_N ; Y):=1/n\u2211_j=1^n\u2113((V_N)_: j, y_j), where \u2113: \u211d^d_N\u00d7\u211d^d_N\u2192 \u211d_+\u222a{0} is some loss function. If \u2113 is real analytic (resp. semialgebraic), then \u211b_n(V_N ; Y) is real-analytic (resp. semialgebraic).\n\nOn V_i-\u03c3_i(U_i)_F^2 : Note that V_i-\u03c3_i(U_i)_F^2 is a finite sum of simple functions of the form, |v-\u03c3_i(u)|^2 for any u, v \u2208\u211d. If \u03c3_i is real analytic (resp. semialgebraic), then v-\u03c3_i(u) is real analytic (resp. semialgebraic), and further |v-\u03c3_i(u)|^2 is also real analytic (resp. semialgebraic) since |v-\u03c3_i(u)|^2 can be viewed as the composition g \u2218 h of these two functions where g(t)=t^2 and h(u, v)=v-\u03c3_i(u).\n\nOn U_i-W_iV_i-1_F^2: Note that the function U_i-W_iV_i-1_F^2 is a polynomial function with the variables U_i, W_i and V_i-1, and thus according to\u00a0<cit.> and <cit.>, it is both real analytic and semialgebraic.\n\nOn \u03c4_i(W_i), s_i(V_i): All \u03c4_i's and s_i's are real analytic or semialgebraic.\n\nOn \ud835\udcb2_i - TTD(r_i)_F^2: Note that the function \ud835\udcb2_i - TTD(r_i)_F^2 is a polynomial function with the variables W_i, \ud835\udca2_i.\n\nSince each part of the function \u2112 is either real analytic or semialgebraic, \u2112 is a subanalytic function\u00a0<cit.>. Furthermore, by the continuity, \u2112 is continuous in its domain. Therefore, \u2112 is a K\u0141 function according to <cit.>.[Let h: \u211d^p\u2192\u211d\u222a{+\u221e} be a subanalytic function with closed domain, and assume that h is continuous on its domain, then h is a K\u0141 function.]\n\n\n\n\n\nBased on Lemmas\u00a0<ref>,\u00a0<ref>, and\u00a0<ref> and conclusions in\u00a0<cit.>, the following main theorem can be obtained.\n\n\n\n\n\n\n\n\n \nLet {\ud835\udcab^k:=({W_i^k}_i=1^N,{V_i^k}_i=1^N,{U_i^k}_i=1^N),{\ud835\udca2_i^k}_i=1^N}_k \u2208\u2115 be the sequences generated from Algorithm\u00a0<ref>.   Suppose that \u03c4_i  and \u2112 are coercive for any i=1, \u2026, N. Then for any \u03b1,\u03b3,\u03c1,\u03c4>0 and any finite initialization \ud835\udcab^0, the following hold\n  \n\n    \n  * {\u2112(\ud835\udcab^k)}_k \u2208\u2115 converges to \u2112^*.\n    \n  * {\ud835\udcab^k}_k \u2208\u2115  converges to a critical point of \u2112.\n    \n  * If further the initialization \ud835\udcab^0 is sufficiently close to some global minimum \ud835\udcab^* of \u2112, then \ud835\udcab^k  converges to \ud835\udcab^*.\n    \n  * Let \u03b8 be the K\u0141 exponent of \u2112 at \ud835\udcab^*. There hold: (a) if \u03b8=0, then {\ud835\udcab^k}_k \u2208\u2115 converges in a finite number of steps; (b) if \u03b8\u2208(0, 1/2], then \ud835\udcab^k-\ud835\udcab^*_F\u2264 C \u03b7^k for all k \u2265 k_0, for certain k_0>0, C>0, \u03b7\u2208(0,1); and (c) if \u03b8\u2208(1/2, 1), then \ud835\udcab^k-\ud835\udcab^*_F\u2264 C k^-1-\u03b8/2 \u03b8-1 for k \u2265 k_0, for certain k_0>0, C>0. \n    \n  * 1/K\u2211_k=1^Kg^k_F^2\u2192 0 at the rate \ud835\udcaa(1 / K) where g^k\u2208 \u2202\u2112(\ud835\udcab^k). \n\n\n\n\n\n\n\n Lispchitz differentiable property is a required for nonconvex optimizations with multi-block variables to build the convergence in the existing literature\u00a0<cit.>. However, the NN training problem\u00a0(<ref>) in this paper generally does not satisfy such a condition. For example, when ReLU activation is used. Theorem\u00a0<ref> establishes the global convergence under a very mild condition that most NN models satisfy.\n \nExtension to ResNets\u00a0<cit.>: the theoretical results in Theorem\u00a0<ref> can be extended to ResNets by considering the following optimization problem\n\n    min _\ud835\udcb2, \ud835\udcb1\u2112_0(\ud835\udcb2, \ud835\udcb1)      subject to U_i=W_iV_i-1,V_i-V_i-1 =\u03c3_i(U_i), \ud835\udcb2_i = TTD(r_i)   i=1, \u2026, N,\n \nwhere the residual term V_i-V_i-1 is considered instead of V_i. The corresponding algorithm can be easily modified from Algorithm\u00a0<ref>. \n\n\n\n\n\u00a7 CASE STUDY\n \n\n\n\n\n\n\n\n\n\n\n\nIn this experiment, to evaluate the effectiveness and efficiency of our proposed method, NN model\u00a0(<ref>) training with different compression ratios (determined by TT-rank r_i) is conducted on the image classification task. In terms of the NN model, ReLU activation, the squared loss, and the network architecture being an MLP with ten hidden layers are considered here. The number of hidden units in each layer is 2^9=512. The neural network is trained on the MNIST dataset, which is a handwritten digits dataset. The size of each input image is d_0=28\u00d728=784 and the output dimension is d_11=10.  The numbers of training and test samples are 60,000 and 10,000, respectively. For comparison, SGD is also considered as a benchmark method, where the learning rate is 0.001.\n\n\n\nFor each experiment, the same mini-batch sizes (512) and initializations for all algorithms. All the experiments are repeated ten times to obtain the average performance.  Specifically, all the weights {W_i}_i=1^N are initialized from a Gaussian distribution with a standard deviation of 0.01. The auxiliary variables {U_i}_i=1^N, state variables {V_i}_i=1^N, TT-cores {\ud835\udca2_i}_i=1^N are initialized by a single forward pass\u00a0<cit.>. Under these settings, the training loss, training accuracy, and test accuracy are shown in Table\u00a0<ref>.  With a smaller CR (# of parameters after compression/# of parameters without compression), a higher training loss is observed. Our proposed method with CR<1 can outperform the uncompressed method and SGD. In addition, the curves of the training loss and test accuracy are plotted in Figure\u00a0<ref>.  Figure\u00a0<ref> shows that the proposed method converges with different compression rates.  The training loss of our proposed method also shows the monotone decreasing trend, which verified the statements in Theorem\u00a0<ref>. Figure\u00a0<ref> shows that, for different CR (<1), the test accuracy of our proposed method keeps increasing as the number of iterations increases. When CR=1 (the model without compression), the test accuracy increases first and then decreases. This result demonstrates that model compression can prevent overfitting. In addition, our proposed method with CR<1 can outperform SGD significantly in terms of test accuracy. \n\n\n\n\n\u00a7 CONCLUSION\n \n\nIn this paper, a holistic framework is proposed for tensor decomposition-based NN model compression by formulating TT decomposition-based NN training as a nonconvex optimization problem. The framework can be extended to other formats of tensor decomposition such as Tucker decomposition, and CP decomposition. For the first time in the literature on tensor decomposition-based NN model compression, global convergence is guaranteed for the proposed tensor BCD (tenBCD) algorithm. Specifically, tenBCD converges to a critical point at a rate of \ud835\udcaa(1/k), where k is the number of iterations.  The empirical experiment shows that the proposed method can converge and run efficiently in practice. Compared with SGD, the proposed method can maintain a high compression rate and high accuracy simultaneously. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n    \n\n\n\n\n\u00a7 SOLUTIONS OF SOME SUBPROBLEMS\n\nIn this section, we provide the solution to subproblem\u00a0(<ref>), closed-form solutions to the ReLU-involved subproblem.\n\n\n \u00a7.\u00a7 Solutions to Subproblem\u00a0(<ref>)\n\nProx-linear algorithm to subproblem\u00a0(<ref>): in the V_N-update of Algorithm\u00a0<ref>, the empirical risk is involved in the optimization problems. It is generally hard to obtain its closed-form solution except for some special cases such as the case where the loss is the square loss. For other smooth losses such as the logistic, cross-entropy, and exponential losses, we suggest using the following prox-linear update strategies, that is, for some parameter \u03b1>0, the V_N-update in  Algorithm\u00a0<ref> is\n\n    V_N^k=V_Nargmin{s_N(V_N)+\u27e8\u2207\u211b_n(V_N^k-1 ; Y), V_N-V_N^k-1\u27e9 +\u03b3/2V_N-U_N^k-1_F^2+\u03b1/2V_N-V_N^k-1_F^2},\n\nThis V_N-update can be implemented with explicit expressions. Therefore, the specific uses of these tenBCD methods are very flexible, mainly depending on users' understanding of their own problems.\n\nThe closed-form of the proximal operator of hinge loss: consider the following optimization problem\n\n    u^*=uargmin g(u):=max{0,1-a \u00b7 u}+\u03b3/2(u-b)^2,\n\nwhere \u03b3>0\n\n    The optimal solution to Problem\u00a0(<ref>) is shown as follows\n    \n    hinge_\u03b3(a, b)= b,     if  a=0, \n     b+\u03b3^-1 a,     if  a \u2260 0  and  a b \u2264 1-\u03b3^-1 a^2, \n     a^-1,     if  a \u2260 0  and  1-\u03b3^-1 a^2<a b<1, \n     b,     if  a \u2260 0  and  a b \u2265 1 .\n\n\n\n\n\n \u00a7.\u00a7 The Closed-form Solution to Subproblem\u00a0(<ref>)\n\nFrom Algorithm\u00a0<ref>, when \u03c3_i is ReLU, then the U_i^k-update actually reduces to the following one-dimensional minimization problem\n\n    u^*=uargmin f(u):=1/2(\u03c3(u)-a)^2+\u03b3/2(u-b)^2,\n\nwhere \u03c3(u)=max{0, u} and \u03b3>0. The solution to the above one-dimensional minimization problem can be presented in the following lemma.\n\nThe optimal solution to Problem\u00a0(<ref>) is shown as follows\n\n    prox_1/2 \u03b3(\u03c3(\u00b7)-a)^2(b)={[                      a+\u03b3 b/1+\u03b3,           if  a+\u03b3 b \u2265 0, b \u2265 0,;                      a+\u03b3 b/1+\u03b3,     if -(\u221a(\u03b3(\u03b3+1))-\u03b3) a \u2264\u03b3 b<0,;                              b, if -a \u2264\u03b3 b \u2264-(\u221a(\u03b3(\u03b3+1))-\u03b3) a<0,;                      min{b, 0},                    if  a+\u03b3 b<0. ].\n\n\n\n\n\n\n\u00a7 KEY PROOF OF THEOREM\u00a0<REF>\n\nBased on Lemma\u00a0<ref> and under the hypothesis that \u2112 is continuous on its domain and there exists a convergent subsequence, the continuity condition required in <cit.> holds naturally, i.e., there exists a subsequence {\ud835\udcab^k_j}_j \u2208\u2115 and \ud835\udcab^* such that\n\n    \ud835\udcab^k_j\u2192\ud835\udcab^*   and \u2112(\ud835\udcab^k_j) \u2192\u2112(\ud835\udcab^*) , as  j \u2192\u221e\n\nBased on Lemmas\u00a0<ref>,\u00a0<ref>, and\u00a0<ref>, we can justify the global convergence of \ud835\udcab^k stated in Theorem\u00a0<ref>, following the proof idea of\u00a0<cit.>. For the completeness of the proof, we still present the detailed proof as follows.\n\nBefore presenting the main proof, we establish a local convergence result of \ud835\udcab^k, i.e., the convergence of \ud835\udcab^k when \ud835\udcab^0 is sufficiently close to some point \ud835\udcab^*. Specifically, let (\u03c6, \u03b7, U) be the associated parameters of the K\u0141 property of \u2112 at \ud835\udcab^*, where \u03c6 is a continuous concave function, \u03b7 is a positive constant, and U is a neighborhood of \ud835\udcab^*. Let \u03c1 be some constant such that \ud835\udca9(\ud835\udcab^*, \u03c1):={\ud835\udcab:\ud835\udcab-\ud835\udcab^*_F\u2264\u03c1}\u2282 U, \u212c:=\u03c1+\ud835\udcab^*_F, and L_\u212c be the uniform Lipschitz constant for \u03c3_i, i=1, \u2026, N-1, within \ud835\udca9(\ud835\udcab^*, \u03c1). Assume that \ud835\udcab^0 satisfies the following condition\n\n    \u03b4\u0305/\u03bb\u03c6(\u2112(\ud835\udcab^0)-\u2112(\ud835\udcab^*))+3 \u221a(\u2112(\ud835\udcab^0)/\u03bb)+\ud835\udcab^0-\ud835\udcab^*_F<\u03c1,\n\nwhere \u03b4\u0305=\u03b4\u221a(4 N), \u03bb and \u03b4 are defined in Lemmas\u00a0<ref> and\u00a0<ref>, respectively.\n\n     Under the conditions of Theorem 5, suppose that \ud835\udcab^0 satisfies the condition\u00a0(<ref>), and \u2112(\ud835\udcab^k)>\u2112(\ud835\udcab^*) for k \u2208\u2115, then\n\n    \n    \u2211_i=1^k\ud835\udcab^i-\ud835\udcab^i-1_F   \u2264 2 \u221a(\u2112(\ud835\udcab^0)/\u03bb)+\u03b4\u0305/\u03bb\u03c6(\u2112(\ud835\udcab^0)-\u2112(\ud835\udcab^*)), \u2200 k \u2265 1 \n    \ud835\udcab^k   \u2208\ud835\udca9(\ud835\udcab^*, \u03c1),   \u2200 k \u2208\u2115.\n\n\nAs k goes to infinity, (<ref>) yields\n\n    \u2211_i=1^\u221e\ud835\udcab^i-\ud835\udcab^i-1_F<\u221e,\n\nwhich implies the convergence of {\ud835\udcab^k}_k \u2208\u2115.\n\n\n    We will prove \ud835\udcab^k\u2208\ud835\udca9(\ud835\udcab^*, \u03c1) by induction on k. It is obvious that \ud835\udcab^0\u2208\ud835\udca9(\ud835\udcab^*, \u03c1). Thus, (<ref>) holds for k=0. For k=1, we have from\u00a0(<ref>) and the nonnegativeness of {\u2112(\ud835\udcab^k)}_k \u2208\u2115 that\n    \n    \u2112(\ud835\udcab^0) \u2265\u2112(\ud835\udcab^0)-\u2112(\ud835\udcab^1) \u2265 a\ud835\udcab^0-\ud835\udcab^1_F^2,\n\nwhich implies \ud835\udcab^0-\ud835\udcab^1_F\u2264\u221a(\u2112(\ud835\udcab^0)/\u03bb). Therefore,\n\n    \ud835\udcab^1-\ud835\udcab^*_F\u2264\ud835\udcab^0-\ud835\udcab^1_F+\ud835\udcab^0-\ud835\udcab^*_F\u2264\u221a(\u2112(\ud835\udcab^0)/\u03bb)+\ud835\udcab^0-\ud835\udcab^*_F,\n\nwhich indicates \ud835\udcab^1\u2208\ud835\udca9(\ud835\udcab^*, \u03c1).\n\nSuppose that \ud835\udcab^k\u2208\ud835\udca9(\ud835\udcab^*, \u03c1) for 0 \u2264 k \u2264 K. We proceed to show that \ud835\udcab^K+1\u2208\ud835\udca9(\ud835\udcab^*, \u03c1). Since \ud835\udcab^k\u2208\ud835\udca9(\ud835\udcab^*, \u03c1) for 0 \u2264 k \u2264 K, it implies that \ud835\udcab^k_F\u2264\u212c:=\u03c1+\ud835\udcab^* for 0 \u2264 k \u2264 K. Thus, by Lemma\u00a0<ref>, for 1 \u2264 k \u2264 K,\n\n    dist(0, \u2202\u2112(\ud835\udcab^k)) \u2264\u03b4\u0305\ud835\udcab^k-\ud835\udcab^k-1_F,\n\nwhich together with the K\u0141 inequality\u00a0(<ref>) yields\n\n    1/\u03c6^'(\u2112(\ud835\udcab^k)-\u2112(\ud835\udcab^*))\u2264\u03b4\u0305\ud835\udcab^k-\ud835\udcab^k-1_F\n\nBy inequality\u00a0(<ref>), the above inequality and the concavity of \u03c6, for k \u2265 2, the following holds\n\n    \u03bb\ud835\udcab^k-\ud835\udcab^k-1_F^2   \u2264\u2112(\ud835\udcab^k-1)-\u2112(\ud835\udcab^k)=(\u2112(\ud835\udcab^k-1)-\u2112(\ud835\udcab^*))-(\u2112(\ud835\udcab^k)-\u2112(\ud835\udcab^*)) \n       \u2264\u03c6(\u2112(\ud835\udcab^k-1)-\u2112(\ud835\udcab^*))-\u03c6(\u2112(\ud835\udcab^k)-\u2112(\ud835\udcab^*))/\u03c6^'(\u2112(\ud835\udcab^k-1)-\u2112(\ud835\udcab^*))\n       \u2264\u03b4\u0305\ud835\udcab^k-1-\ud835\udcab^k-2_F\u00b7[\u03c6(\u2112(\ud835\udcab^k-1)-\u2112(\ud835\udcab^*))-\u03c6(\u2112(\ud835\udcab^k)-\u2112(\ud835\udcab^*))],\n\nwhich implies\n\n    \ud835\udcab^k-\ud835\udcab^k-1_F^2\u2264\ud835\udcab^k-1-\ud835\udcab^k-2_F\u00b7\u03b4\u0305/\u03bb[\u03c6(\u2112(\ud835\udcab^k-1)-\u2112(\ud835\udcab^*))-\u03c6(\u2112(\ud835\udcab^k)-\u2112(\ud835\udcab^*))].\n\nTaking the square root on both sides and using the inequality 2 \u221a(\u03b1\u03b2)\u2264\u03b1+\u03b2, the above inequality implies\n\n    2\ud835\udcab^k-\ud835\udcab^k-1_F\u2264\ud835\udcab^k-1-\ud835\udcab^k-2_F+\u03b4\u0305/\u03bb[\u03c6(\u2112(\ud835\udcab^k-1)-\u2112(\ud835\udcab^*))-\u03c6(\u2112(\ud835\udcab^k)-\u2112(\ud835\udcab^*))].\n\nSumming the above inequality over k from 2 to K and adding \ud835\udcab^1-\ud835\udcab^0_F to both sides, it yields\n\n    \ud835\udcab^K-\ud835\udcab^K-1_F+\u2211_k=1^K\ud835\udcab^k-\ud835\udcab^k-1_F\u2264 2\ud835\udcab^1-\ud835\udcab^0_F+\u03b4\u0305/\u03bb[\u03c6(\u2112(\ud835\udcab^0)-\u2112(\ud835\udcab^*))-\u03c6(\u2112(\ud835\udcab^K)-\u2112(\ud835\udcab^*))]\n\nwhich implies\n\n    \u2211_k=1^K\ud835\udcab^k-\ud835\udcab^k-1_F\u2264 2 \u221a(\u2112(\ud835\udcab^0)/\u03bb)+\u03b4\u0305/\u03bb\u03c6(\u2112(\ud835\udcab^0)-\u2112(\ud835\udcab^*)),\n\nand further,\n\n    \ud835\udcab^K+1-\ud835\udcab^*_F\u2264\ud835\udcab^K+1-\ud835\udcab^K_F+\u2211_k=1^K\ud835\udcab^k-\ud835\udcab^k-1_F+\ud835\udcab^0-\ud835\udcab^*_F\n       \u2264\u221a(\u2112(\ud835\udcab^K)-\u2112(\ud835\udcab^K+1)/\u03bb)+2 \u221a(\u2112(\ud835\udcab^0)/\u03bb)+\u03b4\u0305/\u03bb\u03c6(\u2112(\ud835\udcab^0)-\u2112(\ud835\udcab^*))+\ud835\udcab^0-\ud835\udcab^*_F\n       \u2264 3 \u221a(\u2112(\ud835\udcab^0)/\u03bb)+\u03b4\u0305/\u03bb\u03c6(\u2112(\ud835\udcab^0)-\u2112(\ud835\udcab^*))+\ud835\udcab^0-\ud835\udcab^*_F<\u03c1,\n\nwhere the second inequality holds for\u00a0(<ref>) and\u00a0(<ref>), the third inequality holds for \u2112(\ud835\udcab^K)-\u2112(\ud835\udcab^K+1) \u2264\u2112(\ud835\udcab^K) \u2264 \u2112(\ud835\udcab^0). Thus, \ud835\udcab^K+1\u2208\ud835\udca9(\ud835\udcab^*, \u03c1). Therefore, we prove this lemma.\n\nWe prove the whole sequence convergence stated in Theorem\u00a0<ref> according to the following two cases.\n\nCase 1: \u2112(\ud835\udcab^k_0)=\u2112(\ud835\udcab^*) at some k_0. In this case, by Lemma\u00a0<ref>, \ud835\udcab^k=\ud835\udcab^k_0=\ud835\udcab^* holds for all k \u2265 k_0, which implies the convergence of \ud835\udcab^k to a limit point \ud835\udcab^*.\n\nCase 2: \u2112(\ud835\udcab^k)>\u2112(\ud835\udcab^*) for all k \u2208\u2115. In this case, since \ud835\udcab^* is a limit point and \u2112(\ud835\udcab^k) \u2192\u2112(\ud835\udcab^*), by Theorem 4 , there must exist an integer k_0 such that \ud835\udcab^k_0 is sufficiently close to \ud835\udcab^* as required in Lemma\u00a0<ref> (see the inequality\u00a0(<ref>)). Therefore, the whole sequence {\ud835\udcab^k}_k \u2208\u2115 converges according to Lemma\u00a0<ref>. Since \ud835\udcab^* is a limit point of {\ud835\udcab^k}_k \u2208\u2115, we have \ud835\udcab^k\u2192\ud835\udcab^*.\n\nNext, we show \ud835\udcab^* is a critical point of \u2112. By  lim _k \u2192\u221e\ud835\udcab^k-\ud835\udcab^k-1_F=0. Furthermore, by Lemma\u00a0<ref>,\n\n    lim _k \u2192\u221edist(0, \u2202\u2112(\ud835\udcab^k))=0,\n\nwhich implies that any limit point is a critical point. Therefore, we prove the global convergence of the sequence generated by Algorithm\u00a0<ref>.\n\nThe convergence to a global minimum is a straightforward variant of Lemma\u00a0<ref>.\n\nThe \ud835\udcaa(1 / k) rate of convergence is a direct claim according to the proof of Lemma\u00a0<ref> and lim _k \u2192\u221e\ud835\udcab^k-\ud835\udcab^k-1_F=0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}