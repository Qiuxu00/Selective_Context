{"entry_id": "http://arxiv.org/abs/2303.07287v1", "published": "20230313170319", "title": "Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm", "authors": ["Huiming Zhang", "Haoyu Wei", "Guang Cheng"], "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG", "econ.EM"], "text": "\n\nAnyon condensation in the string-net models\n    Fiona J. Burnell\n    March 30, 2023\n===========================================\n\n\n\n\n\n\n\n\nIn non-asymptotic statistical inferences, variance-type parameters of sub-Gaussian distributions play a crucial role. However, direct estimation of these parameters based on the empirical moment generating function (MGF) is infeasible. To this end, we recommend using a sub-Gaussian intrinsic moment norm [Buldygin and Kozachenko (2000), Theorem 1.3] through maximizing a series of normalized moments. Importantly, the recommended norm can not only recover the exponential moment bounds for the corresponding MGFs, but also lead to tighter Hoeffiding's sub-Gaussian concentration inequalities. In practice,  we propose an intuitive way of checking sub-Gaussian data with a finite sample size by the sub-Gaussian plot. Intrinsic moment norm can be robustly estimated via a simple plug-in approach.  Our theoretical results are applied to non-asymptotic analysis, including the multi-armed bandit.\n\n \n\n\n\n\u00a7 INTRODUCTION\n\n\nWith the advancement of machine learning techniques, computer scientists have become more interested in establishing rigorous error bounds for desired learning procedures, especially those with finite sample validity  <cit.>. In specific settings, statisticians, econometricians, engineers and physicist have developed non-asymptotic inferences to quantify uncertainty in data; see <cit.>. Therefore, the concentration-based statistical inference has received a considerable amount of attention, especially for bounded data <cit.> and Gaussian data <cit.>. For example, Hoeffding's inequality can be applied to construct non-asymptotic confidence intervals based on bounded data[Recently, <cit.> obtained a sharper result than Hoeffding\u2019s inequality for bounded data.]. \n\nHowever, in reality, it may be hard to know the support of data or its underlying distribution. In this case, misusing Hoeffding's inequality <cit.> for unbounded data will result in a notably loose confidence interval (CI); see Appendix <ref>. Hence, it is a common practice to assume that data follow  sub-Gaussian distribution <cit.>. \nBy the Chernoff inequality[For simplicity, we consider centered random variable (r.v.) with zero mean throughout the paper for all sub-Gaussian r.v..], we have (X \u2265 t)  \u2264inf_s > 0{exp{-st}exp{sX}},\u00a0\u2200\u00a0t \u2265 0. Hence, tightness of a confidence interval relies on how we upper bound \nthe moment generating function (MGF) exp{ s X} for all s > 0. This can be further translated into the following optimal variance proxy of sub-Gaussian distribution.\n\n\n\n\n\n\n\n\n    A r.v. X is sub-Gaussian (sub-G) with a variance proxy \u03c3^2 [denoted as X \u223csubG(\u03c3^2)] if its MGF satisfies exp(tX) \u2264exp(\u03c3^2 t^2 / 2) for all t \u2208\u211d. The sub-Gaussian parameter \u03c3_opt(X) is defined by the optimal variance proxy <cit.>:\n    \n    \u03c3^2_opt(X):= inf{\u03c3^2 > 0 : exp(tX) \u2264exp{\u03c3^2 t^2 / 2},   \u2200  t \u2208\u211d}=2 sup_t \u2208t^-2log [exp(t X)].\n\n\nNote that \n\u03c3^2_opt(X) \u2265VarX; see (<ref>) in Appendix <ref>. When \u03c3_o p t^2(X)=VarX, it is called strict sub-Gaussianity <cit.>. Based on Theorems 1.5 in <cit.>, we have  \n\n    ( X \u2265  t ) \u2264exp{- t^2/2 \u03c3^2_opt(X)},\u00a0\u00a0(|\u2211_i=1^nX_i| \u2265 t )\u2264 2exp{-t^2/2 \u2211_i=1^n\u03c3_opt^2(X_i)}.\n\nfor independent sub-G r.v.s X and { X_i}_i = 1^n. The above inequality (<ref>) provides the tightest upper bound over the form (X > t) \u2264exp (-C t^2) (or (|\u2211_i = 1^n X_i | > t) \u2264exp (-C t^2)) for some positive constant C via Chernoff inequality.\n\nGiven {X_i}_i = 1^n i.i.d.\u223csubG(\u03c3^2_opt(X)), a straightforward application of (<ref>) gives an non-asymptotic 100(1-\u03b1)% CI\n\n    X=0 \u2208 [X_n\u00b1\u03c3_opt(X)\u221a(2n^-1log (2/\u03b1))].\n\n\nA naive plug-in estimate[We point out that a conservative and inconsistent estimator 2 inf_t \u2208log (n^-1\u2211_i = 1^n exp(t X_i))/t^2 was proposed in statistical physics literature <cit.>.] of \u03c3_opt^2(X):=2 sup_t \u2208t^-2log [exp(t X)] <cit.> is \n\n    \u03c3_opt^2(X) := 2 sup_t \u2208t^-2log [n^-1\u03a3_i = 1^n exp(t X_i)].\n\nHowever, two weaknesses of (<ref>) substantially hinder its application: (i) the optimization result is unstable due to the possible non-convexity of the objective function; (ii) exponentially large n is required to ensure the variance term Var(n^-1\u2211_i = 1^n exp(t X_i)) not to explode when t is large.  In Section 3, we present some simulation evidence.\n\nOn the other hand, we are aware of other forms of variance-type parameter. For instance, <cit.> introduced the Orlicz norm as X_w_2:=inf{c>0: exp{|X|^2/c^2 }\u2264 2}, frequently used in empirical process theory. Additionally, <cit.>  suggested a norm based on the scale of moments as X_\u03c8_2 := max_k \u2265 2  k^- 1/ 2 (   |X|^k )^1 / k in Page 6 of <cit.>. However, as shown in Table <ref> and Appendix <ref>, both types of norm fail to deliver sharp probability bounds even for strict sub-G distributions, such as the standard Gaussian distribution and symmetric beta distribution.\n\n\n\n\n\n \u00a7.\u00a7 Contributions\n\n\nIn light of the above discussions, we advocate the use of the intrinsic moment norm in the Definition <ref> in the construction of tight non-asymptotic CIs. There are two specific reasons: (i) it approximately recovers tight inequalities (<ref>); (ii) it can be estimated friendly (with a closed form) and robustly. \n\nThe following definition <ref> is from Page 6 and Theorem 1.3 in <cit.>.\n\nX _G :=max_k \u2265 1[ 2^kk!/(2k)!X^2k]^1/(2k) = max_k \u2265 1[ 1/(2k - 1)!!X^2k]^1/(2k).\n\n\n\nFrom the sub-G characterization (see Theorem 2.6 in <cit.>),  X_G < \u221e iff \u03c3_opt(X) < \u221e for any zero-mean r.v. X. Hence, the finite intrinsic moment norm of a r.v. X ensures sub-Gaussianity (satisfying  Definition\u00a0<ref>).\n\nOur contributions in this paper can be summarized as follows. \n\n\n\n\n\n\n    \n\n\n\n    \n  1. By X _G, we achieve a sharper Hoeffding-type inequality under asymetric distribution; see Theorem <ref>(b). \n    \n    \n  2. Compared to the normal approximation based on Berry-Esseen (B-E) bounds, our results are more applicable to data of extremely small sample size. We illustrate Bernoulli observations with the comparison of two types of CIs based on the B-E-corrected CLT and Hoeffding's inequality in Figure <ref>; see Appendix <ref> for details.\n    \n    \n  3. A novel method called sub-Gaussian plot is proposed for checking  whether the unbounded data are sub-Gaussian. We introduce plug-in and robust plug-in estimators for X _G, and establish finite sample theories.\n\n    \n  4. Finally, we employ the intrinsic moment norm estimation in the non-asymptotic inference for a bandit problem: Bootstrapped UCB-algorithm for multi-armed bandits. This algorithm is shown to achieve feasible error bounds and competitive cumulative regret on unbounded sub-Gaussian data. \n \n\n    \n\n\n\n\u00a7 SUB-GAUSSIAN PLOT AND TESTING\n\nBefore estimating X_G, the first step is to verify X is indeed sub-G given its i.i.d. copies {X_i}_i=1^n. Corollary 7.2 (b) in <cit.> shows for r.v.s X_i \u223csubG(\u03c3_opt^2(X)) (without independence assumption)\n\n    ( max_1 \u2264 i \u2264 j X_i \u2264\u03c3_opt(X) \u221a(2 (log j + t))) \u2265 1-exp{ -t},\n\nwhich implies max_1 \u2264 i \u2264 j X_i =O_(\u221a(log j)). Moreover, we will show the above rate is indeed sharp for a class of unbound sub-G r.v.s characterized by the lower intrinsic moment norm below.\n\nThe lower intrinsic moment norm for a sub-G X is defined as\n\n    X _G\u0303 := min_k \u2265 1{[(2k - 1)!!]^-1X^2k}^1/(2k).\n\n\nBy the method in Theorem 1 of <cit.>, we obtain the following the tight rate result with a lower bound.\n\n(a). If X _G\u0303>0 for i.i.d. symmetric sub-G r.v.s { X_i}_i = 1^n \u223c X, then with probability at least 1-\u03b4\n\n    X _G\u0303/ X _G/2\u221a(2 X _G^2/ X _G\u0303^2-1)\u221a(log n-log C^-2(X)-loglog(2/\u03b4))\u2264max _1 \u2264 i \u2264 nX_i/ X _G\u2264\u221a(2[log n + log(2/\u03b4)]),\n\nwhere C(X)< 1 is constant defined in Lemma <ref> below; (b) if X is bounded variable, then X _G\u0303=0.\n\n\nThe upper bound follows from the proof of (<ref>) similarly. The proof of lower bound relies on the sharp reverse Chernoff inequality from Paley\u2013Zygmund inequality (see <cit.>).\n\nSuppose X _G\u0303>0 for a symmetric sub-G r.v. X. For t>0, then\n\n\n(X \u2265 t) \u2265 C^2(X) exp{-4[2 X _G^2/ X _G\u0303^4- X _G\u0303^-2]t^2},\n\n\nwhere C(X):=(  X _G\u0303^2/4 X _G^2- X _G\u0303^2)( 4 X _G^2-2 X _G\u0303^2/4 X _G^2- X _G\u0303^2)^2[2 X _G^2/ X _G\u0303^2-1]\u2208 (0,1).\n\nTheorem 1 of <cit.> does not optimize the constant in Paley\u2013Zygmund inequality. In contrast, our Lemma <ref> has an optimal constant; see Appendix <ref> for details.\n\n\nSub-Gaussian plot under unbounded assumption[Sub-G plot can only be applied to data with enough samples. When n is very small, there is not enough information to suggest unbounded trends. We roughly treat the data as bounded r.v. for a very small n, and there is no need to use a sub-G plot in this case.]. By Theorem <ref>, we propose a novel sub-Gaussian plot check whether i.i.d data { X_i}_i = 1^n follow a sub-G distribution. Suppose that for each j, { X_i^*}_i = 1^j are independently sampled from the empirical distribution \ud835\udd3d_n(x) = 1/n\u2211_i = 1^n 1(X_i \u2264 x) of { X_i}_i = 1^n. Specifically, we plot the order statistics {max_1 \u2264 i \u2264 j X_i^*}_j = 1^n on the plane coordinate axis, where x axis represents \u221a(log j + 1) and y axis the value of max_1 \u2264 i \u2264 j X_i^*. We check whether those points have a linear tendency at the boundary: the more they are close to the tendency of a beeline, the more we can trust the data are sub-Gaussian. \n\nThe Figure <ref> shows \nsub-Gaussian plot of N(0, 1) and Exp(1). It can be seen that sub-Gaussian plot of N(0, 1) shows linear tendency  at the boundary, while Exp(1) shows quadratic tendency at the boundary. For the quadratic tendency, we note that if { X_i}_i = 1^n have heavier tails such as sub-exponentiality, then max_1 \u2264 i \u2264 j X_i =O_( log j) instead of the order O(\u221a(log j)); see Corollary 7.3 in <cit.>. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 FINITE SAMPLE PROPERTIES OF INTRINSIC MOMENT NORM\n\nIn this section, we characterize two important properties of the intrinsic moment norm that are used in constructing non-asymptotic confidence intervals.\n\n\n\n\n \u00a7.\u00a7 Basic Properties\n\nLemma <ref> below establishes that the intrinsic moment norm is estimable. \n\n\nFor sub-G X, we have\nmax_m \u2208 2 \u2115[  X^m/(m - 1)!!]^1 / m < \u221e, where 2 \u2115:={2 ,4, \u22ef} is the even number set.\n\n\n\nLemma <ref> ensure that for any sub-Gaussian variable X, its intrinsic moment norm can be computed as\n\n\n    X _G := max_m \u2208 2 \u2115[  X^m/(m - 1)!!]^1 / m = max_1  \u2264 k \u2264 k_X[  X^2k/(2k - 1)!!]^1 / (2k)\nwith some finite k_X < \u221e.\n\n\nThis is an important property that other norms may not have. The X_\u03c8_2 := max_k \u2265 2  k^- 1/ 2 (   |X|^k )^1 / k for Gaussian X achieves its optimal point at k = \u221e; see Example <ref> in Appendix <ref>. As for \u03c3_opt^2(X):=2 sup_t \u2208log [exp(t X)]/t^2, it is unclear that its value can be achieved at a finite t. Note that if k_X = 1, one has X_G^2 =Var(X). \n\nNext, we present an example in calculating the values of k_X. Denote . Exp(1) |_[0, M] as the truncated standard exponential distribution on [0, M] with the density as f(x) = e^-x/\u222b_0^M e^-x  dx 1_{x \u2208 [ 0, M]}.   \n\n\na. X \u223c U[-a, a], k_X = 1 for any a \u2208\u211d; b. X \u223c. Exp(1) |_[0, 2.75] - . Exp(1) |_[0, 2.75], k_X = 2; c. X \u223c. Exp(1) |_[0, 3] - . Exp(1) |_[0, 3], k_X = 3. Indeed, for any fixed k_0 \u2208\u2115, we can construct a truncated exponential r.v. X:=. Exp(1) |_[0, M] such that k_X = k_0 by properly adjusting the truncation level M. \n\n\nThe above Theorem <ref> gives an important advantage of our norm. Moreover, our norm has another important advantage: it will give exactly the simplest structure for bounded random variable.\n\n\n    Let X be any zero-mean bounded random variable, then\n    \n    max_m \u2208 2 \u2115[  X^m/ Z^m]^1 / m = 2\n\n\n\n\n    Suppose that X \u2208 [a, b], then \n    \n    X_2k = [  X^2k]^1 / (2k)\u2264[  (a^2 \u2228 b^2)^k ]^1 / (2k) = \u221a((a^2 \u2228 b^2)).\n\n    On the other hand,\n    \n    Z_2k =  [ 2^k/\u221a(\u03c0)\u0393( k + 1/2)]^1 / (2k)\n\n    is a strictly increasing function of k. This implies X_2k / Z_2k is a strictly decreasing function. Hence, it will achieve maximum at k = 1, i.e. m = 2.\n\nTheorem <ref> gives a nice property that, for any bounded random variable X, X will be strictly sub-Gaussian, and hence its sub-G is its variance itself.\n\n\n\n\n \u00a7.\u00a7 Concentration for summation\n\n\nIn what follows, we will show another property of X _G that it recovers nearly tight MGF bounds in Definition <ref>. More powerfully, it enables us to derive the sub-G Hoeffding's inequality (<ref>).  \n\n    Suppose that {X_i}_i=1^n are independent r.v.s with max_i \u2208 [n] X_i _G <\u221e. We have \n    \n    (a). If X_i is symmetric about zero, then exp{ t X_i}\u2264exp{t^2 X_i _G^2/2} for any t \u2208\u211d, and\n\n\nP(|\u2211_i = 1^n X_i | \u2265 s ) \u2264 2exp{-s^2/ [2\u2211_i = 1^n  X_i _G^2 ]},   s \u2265 0.\n\n\n   (b). If X_i is not symmetric, then exp{tX_i}\u2264exp{(17/12)t^2 X_i _G^2/2} for any t \u2208\u211d, and\n    \n\n        P(|\u2211_i = 1^n X_i | \u2265 s ) \u2264 2 exp{-(12/17)s^2/ [2\u2211_i = 1^n  X_i _G^2 ]},     s \u2265 0.\n    \n\n\n\nTheorem <ref>(a) is an existing result in Theorem 2.6 of <cit.>. For Theorem <ref>(b), we obtain \u221a(17/12)\u2248 1.19, while Lemma 1.5 in <cit.> obtained exp{tX_i}\u2264exp{t^2/2(\u221a(3.1) X_i_G)^2} for t \u2208\u211d with \u221a(3.1)\u2248 1.32. Essentially, \u221a(17/12)>1 appears for asymmetric variables, since \u00b7_G is defined by comparing a Gaussian variable G that is symmetric. A technical reason for this improvement is that \u00b7_G does not need Stirling's approximation for attaining a sharper MGF bound when expanding the exponential function by Taylor's formula. To show the tightness of Theorem <ref>(b), in Figure <ref>, we give some comparisons with \u03c3_o p t(X), \u221a(17/12) X _G, \u221a(2e)X_\u03c8_2, X_w_2/\u221a(2) and \u221a(VarX) in terms of confidence length in Table <ref>, when X is Bernoulli or beta distribution. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 ESTIMATION OF THE INTRINSIC MOMENT NORM\n\n\n\nA first thought to estimate \nX_G is by the plug-in approach. Although k_X is proven to be finite in Lemma <ref>, its (possibly large) exact value is still unknown in practice. Instead, we use a non-decreasing index sequence {\u03ba_n} to replace k_X in the estimation. Hence, we suggest a plug-in feasible estimator\n\n    X _G = max_1 \u2264 k \u2264\u03ba_n[ 1/(2k - 1)!!1/n\u2211_i = 1^n  X_i^2k]^1/(2k).\n\n\n\n\n    \n\n\n\n\n\n\n\nDeriving the non-asymptotic property of the X_G is not an easy task: the maximum point k\u0302(\u03ba_n):=max_1 \u2264 k \u2264\u03ba_n[ 1/(2k - 1)!!1/n\u2211_i = 1^n  X_i^2k]^1/(2k) will change with the sample size n even \u03ba_n is fixed. \n\nTo resolve this, we first examine the oracle estimator defined as X_G = [ 1/(2k_X - 1)!!1/n\u2211_i = 1^n  X_i^2k_X]^1/2 k_X.\n\nHere, based on Orlicz norm Y_\u03c8_\u03b8:=inf{t>0: Eexp{|Y|^\u03b8 / t^\u03b8}\u2264 2 } of sub-Weibull r.v. Y with \u03b8 >0 <cit.>, we present the non-asymptotic concentration of X_G around it ture value X _G.\n\n\n    Suppose {X_i}_i = 1^n i.i.d.\u223c X and X satisfies X_\u03c8_1/k_X<\u221e, then for any t > 0,\n\n\n( | X _G^2 k_X -  X_G^2 k_X| \u2264 2 e  X_\u03c8_1/k_X C(k_X^-1){\u221a(t/n) +\u03b3^2 k_X A(k_X^-1) t^k_X/n}) \u2265 1-2 e^-t,\n\n\n    where the constant \u03b3\u2248 1.78, and the constant functions C(\u00b7) and A(\u00b7) are defined in Appendix <ref>.\n\n\n\nThe exponential-moment condition X_\u03c8_1/k_X<\u221e is too strong for the error bound of X _G^2 k_X -  X_G^2 k_X in Proposition <ref>, although it has exponential decay probability 1- 2exp( - t). \n\n\n\nExcept for the direct plug-in estimator, here we resort to the median-of-means (MOM, Page244 in <cit.>) as the robust plug-in estimator of intrinsic moment norm. Let m and b be a positive integer such that n=m b and let B_1, \u2026, B_b be a partition of [n] into blocks of equal cardinality m. For any s \u2208[b], let _m^B_s X = m^-1\u2211_i \u2208 B_s X_i for independent data {X_i}_i=1^n. The MOM version intrinsic moment norm estimator is defined as\n\n    X _b,G:= max_1 \u2264 k \u2264\u03ba_nmed_s \u2208 [b]{[[(2 k-1) ! !]^-1_m^B_s X^2k]^1/(2k)}.\n\nAs stated in Proposition <ref>, the naive plug-in estimator X _G =  X _1,G is not robust. MOM estimators (<ref>) with b\u226b 1 have two merits: (a) it only needs finite moment conditions, but the exponential concentration bounds are still achieved; (b) it permits some outliers in the data.  Non-asymptotic inferences require to bound for X_G exactly by a feasible estimator X _b,G up to a sharp constants. Next, we establish a high-probability upper bound for the estimated norm, if the data has O \u222a I outlier assumptions as follows.\n\n\n\n    \n  * (M.1)  Suppose that the data {X_i}_i=1^n contains n-n_o inliers {X_i}_i \u2208 I drawn i.i.d. according to a target distribution, and there are no distributional assumptions on n_o outliers {X_i}_i \u2208 O; \n    \n   \n  * (M.2) b=b_O+b_S, where b_O is the number of blocks containing at least one outliers and b_S is the number of sane blocks containing no outliers. Let \u03b5:=n_o/n be the fraction of the outliers and n_o/b<1/2. Assume here exists a fraction function \u03b7(\u03b5) for sane block such that\nb_S \u2265\u03b7(\u03b5) b\u00a0for a function\u00a0\u03b7(\u03b5)\u2208 (0,1].\n    \n\nTo serve for error bounds in the presence of outliers, (M.2) considers the specific fraction function of the polluted inputs; see <cit.>.\n\nDefine g_k,m(\u03c3 _k) and g\u0305_k,m(\u03c3 _k) as the sequences for any m \u2208\u2115 and 1 \u2264 k \u2264\u03ba_n:\n\n    g_k,m(\u03c3 _k) : =  1 - [ X^2k/(2k - 1)!!]^-1/(2k)max_1 \u2264 k \u2264\u03ba_n[  -2[m/\u03b7(\u03b5)]^-1/2\u03c3_k^k/( X^2 k)+ X^2k/(2k - 1)!!]^1/(2k) ;\n\nand g_k,m(\u03c3 _k):=[2[m/\u03b7(\u03b5)]^-1/2\u03c3_k^k/( X^2 k)+1]^1 /(2 k)-1. We obtain a robust and non-asymptotic CI for X _G.\n\n\n Suppose \u221a(VarX^2k)\u2264\u03c3 _k^k for a sequence {\u03c3 _k}_k = 1^\u03ba_n, we have \n\n    { X _G\u2264[1 - max_1 \u2264 k \u2264\u03ba_ng_k,m(\u03c3 _k)]^-1 X _b,G} > 1 - \u03ba_n\u00b7e^ - 2b\u03b7(\u03b5) (1-3/4\u03b7(\u03b5))^2;\n\nand { X _G\u2265 [1 +max _1 \u2264 k \u2264\u03ba_ng_k,m(\u03c3 _k)]^-1 X _b,G} > 1 - \u03ba_n\u00b7e^ - 2b\u03b7(\u03b5) (1-3/4\u03b7(\u03b5))^2 for \u03ba_n\u2265\u03ba_X under (M.1-M.2). \n\n\n\n\n\nTheorem <ref> ensures the concentration of the estimator X_b,G when \u03ba_n\u2265 k_X under enough sample. If \u03b7(\u03b5)=1 with \u03b5=0, then the data are i.i.d., which have no outlier, and outlier assumptions in M.1-M.2 can be dropped in Theorem <ref>. When the data is i.i.d. Gaussian vector, Proposition 4.1 in <cit.> also gave a high-probability estimated upper bound for \u2113_p-norm of the vector of Gaussian standard deviations, our result is for intrinsic moment norm. \n\nIn practice, the block number b can be taken by the adaptation method based on the Lepski method <cit.>. To guarantee high probability events in Theorem <ref>, it is required that the index sequence \u03ba_n should not be very large for fixed b. The larger \u03ba_n needs larger b in blocks B_1, \u2026, B_b. In the simulation, we will see that an increasing index sequence \u03ba_n with slow rate will lead a good performance.\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n    Finally, we compare our two estimators (<ref>) and (<ref>), as well as the estimator (<ref>) in Figure <ref>. We consider the standard Gaussian and Rademacher variable distributed X, in the two case we have X_G^2 = \u03c3_opt^2 (X) = Var(X) = 1. The following figure shows the performance of three estimators under sample n=10 to 1000 with \u03ba_n just chosen as \u2308log n \u2309. \n\nFor the MOM method, we use five blocks in this simple setting. For a more complex case, one can use Lepski's method to choose b (see <cit.>), but some considerable computation cost may be introduced. From Figure <ref>, we know that the performance of the MOM estimator is best, while the naive estimator (<ref>) is worst. For the high-quality data of extremely small sample size, we can apply the leave-one-out Hodges-Lehmann method <cit.> for further numerical improvement; see Appendix <ref> for details.\n\n\n\n\n\n\u00a7 APPLICATION IN MULTI-ARMED BANDIT PROBLEM\n\n\nIn the multi-armed bandit problem (MAB), \na player chooses between K different slot machines (an K-armed bandit), each with a different unknown random reward r.v.s {Y_k}_k=1^K \u2286\u211d, while each realization of a fixed arm k is independent and shares the same distribution. Further, we assume the rewards are sub-Gaussian, i.e.\n\n    Y_k-\u03bc_k_G  < \u221e,      k \u2208 [K].\n\n\n\nOur goal is to find the best arm with the largest expected reward, say Y_t*, by pulling arms. \n\nIn each round t \u2208 [T], the player pulls an arm (an action) A_t\u2208 [K]. Conditioning on  {A_t=k}, we define the observed reward {Y_k,t}_t \u2208 [T]i.i.d.\u223c P_k. The goal of the exploration in MAB is to minimize the cumulative regret after T steps:\nReg_T(Y,A):=\u2211_t=1^T(\u03bc_t^\u22c6-\u03bc_A_t), \ni.e. the performance of any exploration strategy {A_t}_t \u2208 [T]. The exploration performance is better, if we have smaller Reg_T(Y,A). Without loss of generality, we assume t^\u22c6=1. We seek to evaluate the expected bounds from the  decomposition (see Lemma 4.5 in <cit.>),\n\n    Reg_T := Reg_T(Y,A) = \u2211_k = 1^K \u0394_k  [\u2211_t=1^T1 {A_t=k}],\n\nwhere  is taken on the randomness of the player's actions {A_t}_t \u2208 [T], and \u0394_k=\u03bc_1-\u03bc_k is the sub-optimality gap for arm k\u2208 [K]/{1}. The upper bound of Reg_T is called problem-independent if the regret bound depends on the distribution of the data and does not rely on the gap \u0394_k. \n\n\nFor each iteration t, let T_k(t):=card{1 \u2264\u03c4\u2264 t: A_\u03c4=k} be the number of pull for arm k until time t during the bandit process. Then if we define Y_T_k(t):=1/T_k(t)\u2211_\u03c4\u2264 t, A_\u03c4=k Y_k,\u03c4 as the running average of the rewards of arm k at time t. Suppose we obtain a 100(1-\u03b4)% CI [Y_T_k(t)-c_k(t), Y_T_k(t)+c_k(t)] for \u03bc_k from a tight concentration inequality. Therefore, we confidently\nreckon that the reward of arm k is Y_T_k(t)+c_k(t),  and play the arm A_t=k, hoping to maximize the reward with a high probability for finite t. This is upper confidence bound (UCB, <cit.>) algorithms. And many works based on this methods appears recently, for example, <cit.> use bootstrap method with the second order correction to give a algorithm with the explicit regret bounds for sub-Gaussian rewards. However, many existent algorithms contain unknown norms for the random rewards, they are actually infeasible. And Theorem 4 is one example with explicit regret bound. For instance, the algorithm <cit.> needs to use the unknown Orlicz-norm of Y_k - \u03bc_k in the algorithm. Thus, it is actually infeasible in practice. \n\nFortunately, our estimator can solve this problem. Suppose that Y_k-\u03bc_k is symmetric around zero, by one-side version of Theorem <ref>, the (<ref>) implies that for all k and all t,\n(Y_T_k(t) > \u03bc_k+Y_k-\u03bc_k_G\u221a(2/T_k(t)log1/\u03b4) )\u2264\u03b4.\nLet sub-sample size m_k and block size b_k be positive integer such that T_k(t) = m_k b_k for MOM estimators Y_k-\u03bc_k_b_k,G in Section 3. Theorem <ref> (a) guarantee that true norms can be replaced by MOM-estimated norms such that\n(Y_T_k(t)\u2264\u03bc_k+Y_k-\u03bc_k_b_k,G/1-o(1)\u221a(2/T_k(t)log1/\u03b4))\u2265 1-\u03b4- k_Y_k\u00b7exp (- b_k / 8) if \u03b7(\u03b5)=1 with \u03b5=0.\n     \n\n\nIf the UCB algorithm is correctly applied, for a finite T_k(t), with high probability, we will pull the best arm.\n\nIn practice, we nearly do not know any knowledge about the data. As a flexible way of uncertainty qualification, the multiplier bootstrap <cit.> enables mimicking the non-asymptotic properties of the target statistic by reweighing its summands of the centralized empirical mean. The multiplier bootstrapped quantile for the i.i.d. observation _n:={ Y_i}_i = 1^n is the (1 - \u03b1)-quantile of the distribution of n^-1\u2211_i = 1^n  w_i (Y_i - Y_n), which is defined as\n\n    q_\u03b1 (_n - Y_n, \ud835\udc30) := inf{ x \u2208\u211d  |  _w (n^-1\u2211_i = 1^n  w_i (Y_i - Y_n) > x) \u2264\u03b1},\n\nwhere \ud835\udc30 := { w_i }_i = 1^n are bootstrap random weights independent of _n. We denote the statistics \u03c6_G (\ud835\udc18_n) as something satisfying _\ud835\udc18_n ( |Y_n  -  Y_1 | \u2265\u03c6_G (\ud835\udc18_n) ) \u2264\u03b1. \n\nMotivated by <cit.>, we design Algorithm 1 based on some estimators of the UCB. It guarantees a relatively small regret by bootstrapped threshold q_\u03b1 / 2 (_T_k(t) - Y_T_k(t), \ud835\udc30) adding a concentration based second-order correction \u03c6_G (_T_k(t) ) that is specified in Theorem <ref>.\n\nIn the following regret bounds, we assume the mean reward from the k-th arm \u03bc_k is known. In practice, it can be replaced by a robust estimator, and we obtain the results of MOM estimator.\n\n\n\n\n\n \n\n\n\n\n\n    Consider a K-armed sub-G bandit under (<ref>) and suppose that Y_k - \u03bc_k is symmetric around zero. For any round T, according to moment conditions in Theorem <ref>, choosing \u03c6_G (_T_k(t)) as\n    \n    \u03c6_G (_T_k(t)) =  \u221a(2 log(4 / \u03b1))/T_k^1 / 2(t) - 1Y_k - \u03bc_k _b_k,G\n\n    as a re-scaled version of MOM estimator Y_k - \u03bc_k _b_k,G with block number b_k satisfying the moment assumptions <ref>[UCB1]  and <ref>[UCB2] in Appendix <ref>. Fix a confidence level \u03b1 = 4 / T^2, if the player pull an arm  A_t\u2208 [K] according to  Algorithm 1, then we have the problem-dependent regret of Algorithm 1 is bounded by\n\n\nReg_T\u2264 16 (2 + \u221a(2))^2 max_k \u2208 [K]Y_k - \u03bc_k _G^2 log T \u2211_k = 2^K \u0394_k^-1 + ( 4 T^-1 + 2 T^- 25 - 16 \u221a(2) + 8) \u2211_k = 2^K \u0394_k,\n\n\n        where \u0394_k is the sub-optimality gap. Moreover, let \u03bc_1^* := max_k_1 \u2208 [K]\u03bc_k_1 - min_k_2 \u2208 [K]\u03bc_k_2 be the range over the rewards, the problem-independent regret  \n    Reg_T\u2264 8 (2 + \u221a(2)) max_k \u2208 [K]Y_k - \u03bc_k _G \u221a(T K log T) + ( 4 T^-1 + 2 T^-25 - 16 \u221a(2) + 8 ) K \u03bc_1^*.\n\n\nFrom Theorem <ref>, we know that the regret of our method achieve minimax rate log T for a problem-dependent problem and \u221a(KT) for a problem-independent case (see <cit.>), so Algorithm <ref> can be seen as an optimal algorithm. Compared with the traditional vanilla UCB, we do improve the constant. When Y_k \u223c N(\u03bc_k, 1), the constant factor in regret bound in <cit.> is 256, which is larger than 16 (2 + \u221a(2))^2 in our theorem. \n\nWhen the UCB has unknown sub-G parameters, Theorem 4 first studies a feasible UCB algorithm with sub-G parameter plugging estimation. Many previous UCB algorithms based on non-asymptotic inference in the literature assume that the sub-G parameter is a preset constant, see the algorithm in <cit.> for instance.\n\nNext, we give an simulation for Theorem <ref> in two sub-G cases to verify the performance of estimated norms. Similar to <cit.>, we design the three methods as follows:\n\n    \n  1.  Use our method \u03c6(_T_k(t)) with Estimated Norm in Theorem <ref>;\n    \n  2.  Use Asymptotic Naive varphi \u03c6(_T_k(t)) satisfying (|Y_T_k(t) - \u03bc_k| \u2264\u03c6(_T_k(t)) ) \u2192\u03b1 by CLT, i.e.\n   \u03c6(_T_k(t)) = \u03c3_k \u03a6^-1 (1 - \u03b1 / 2) /\u221a(T_k(t))\n    with \u03c3_k = \u221a(1/T_k(t)\u2211_\u03c4\u2264 t, A_\u03c4=k  (Y_k,\u03c4 - Y_T_k(t))^2) as the estimated standard deviation;\n\n    \n  3.  Regard all the unbounded rewards as bounded r.v. and use Hoeffding's inequality (wrongly use Hoeffding's inequality) to construct \u03c6, i.e.\n \u03c6\u030c(_T_k(t)) = [max{_T_k(t)} - min{_T_k(t)}] \u221a(log (2 / \u03b1)/2T_k(t)).\n    \n\n    \n\n\nFor our detailed MAB simulation, we consider as follows, in each case, the number of arms is assigned as K = 10, and the mean reward from k-th arm \u03bc_k is independently drawn from Exp(1). We consider two types of unbounded reward distributions: EG1. Gaussian N(\u03bc_k, 1); EG2. Mixture Gaussian p_k \u00d7 N(2 \u03bc_k, 1) + (1 - p_k) \u00d7 N (1 - 2p_k/1 - p_k\u03bc_k, 1) with p_k  \u223c  U(0, 1/2).\nWe also use Thompson Sampling <cit.> with Gaussian for both its reward and prior distribution and tuning the prior parameters on { 2^4- k}_k = 1^6 with its best performance as a strong baseline in this simulation. \n\n\n\nAs we can see, EG1 and EG2 are both sub-Gaussian rewards. In the simulation, \u03bc_k may not be bounded, complicating this problem. The simulation results are shown in Figure <ref>. These results show that our method outperforms the other two methods under unbounded sub-Gaussian rewards and is even comparable to Thompson Sampling when sufficient correct prior knowledge is available. Furthermore, the smallest standard derivation of our method demonstrates the strong robustness of our estimated norm method.\n\n\n\n \u00a7.\u00a7 Acknowledgments\n\nThe research of H. Zhang was supported in part by National Natural Science Foundation of China (Grant 12101630). The research of G. Cheng was supported in part by NSF \u2013 SCALE MoDL (2134209). \n\n\nThe authors thank  Prof. Hongjun Li for the early discussions, as well as Dr. Ning Zhang for valuable suggestions. The early version of this manuscript was submitted to ICLR 2023 on September 22, 2022; see <https://openreview.net/forum?id=c9QTkDGJ_cB>.\n\n\niclr2023_conference \n\n\n\n\n\n\n\n\nHere we also give an simulation for Theorem <ref> in two sub-G and two sub-E cases to verify the performance of estimated norms. Similar to <cit.>, we design the four methods as follows:\n\n    \n  1.  Use our method \u03c6(_T_k(t)) with Estimated Norm in Theorem <ref>;\n    \n  2.  Use Asymptotic Naive varphi \u03c6(_T_k(t)) satisfying (|Y_T_k(t) - \u03bc_k| \u2264\u03c6(_T_k(t)) ) \u2192\u03b1 by CLT, i.e.\n   \u03c6(_T_k(t)) = \u03c3_k \u03a6^-1 (1 - \u03b1 / 2) /\u221a(T_k(t))\n    with \u03c3_k = \u221a(1/T_k(t)_T_k(t) - Y_T_k(t) 1_T_k(t)_\u2113_2^2) as the estimated standard deviation;\n\n    \n  3.  Regard all the rewards as bounded r.v. and use Hoeffding's inequality (wrongly use Hoeffding's inequality) to construct \u03c6, i.e.\n \u03c6\u030c(_T_k(t)) = [max_T_k(t) - min_T_k(t)] \u221a(log (2 / \u03b1)/2T_k(t)).\n    \n\n    \n\n\nAnd for our detailed MAB simulation, we consider as follows, in each case, the number of arms is assigned as K = 5, and (\u03bc_1, \u2026, \u03bc_5) = 0.9 \u00b7 (0.1, 0.05, 0.02, 0.01, 0.01)^\u22a4\n\n    A1. Y_k  \u223c  N(\u03bc_k, \u03bc_k^2);     B1. Y_k  \u223c sym-Exp(\u03bc_k);     B2. Y_k  \u223c sym-Poisson(\u03bc_k);\n\nwhere the sym-Exp represents symmetric exponential distribution that can be generated from X_1 - X_2 + \u03bc with X_1, X_2 i.i.d.\u223cExp(\u03bc), and sym-Poisson have the similar meaning.\n\n\n\n\n\nAs we can see, A1 is a sub-G case and B1 and B2 are sub-E cases. In the simulation, \u03bc_k is assigned sightly small is for bounded max_k\u0394_k, which is a standard-setting in the MAB problem (see the condition of Corollary 1 in <cit.> for instance). The simulation results are shown in Figure <ref>. Under the smaller sample size, our method (the green lines) has good performance compared with the other methods, and the misusing Hoeffding will give a bad result for both sub-G and sub-E cases. This phenomenon is more pronounced in the sub-E case. On the other hand, the CLT-based \u03c6 renders a moderate result that performs better for a larger sample size. As for the sub-G case, even if the rewards have the exact Gaussian tails, our method can also give a satisfactory result when the number of rounds is relatively small.\n\nIn turn, for iid sample of { X_i}_i = 1^n, we have its estimated value as\n\n    X _G =max_1 \u2264 k \u2264 k_X[ 2^kk!/(2k)!1/n\u2211_i = 1^n X_i^2k]^1/(2k).\n\n\n\n\nBefore doing this, a direct lemma of \u00b7_G is that it will derive the tail probability with the exact same structure as the optimal variance proxy in Definition <ref>.\n\n\n    For any zero-mean random variable X, X _\u03c8_2 < \u221e will give X_G < \u221e and X_G < \u221e will imply X _w_2 < \u221e.\n\n\n\n    Suppose X _\u03c8_2 < \u221e. From Corollary 3.1 in <cit.>, there exists \u03c3^2 > 0 such that \n    \n    exp{ sX}\u2264exp{\u03c3^2 s^2},      s \u2208\u211d.\n\n    Hence, from Proposition 3.1 in <cit.>, we have X_k : = [  |X|^k ]^1 / k\u2264\u03c3 e^1 / e\u221a(k) for any k \u2265 2. Therefore, \n    \n    X _G    \u2264max_k \u2265 2 X_k/ Z_k = max_k \u2265 2[ 2^k/2/\u221a(\u03c0)\u0393( k + 1/2)]^-1 / k X_k \n       \u2264 X_2/ Z _2 + max_k \u2265 3[ 2^k/2/\u221a(\u03c0)\u221a(2 \u03c0)( k - 1/2e)^(k - 1)/2]^-1 / k X_k \n       \u2264 X_2/ Z _2 + max_k \u2265 3 2^-1 / k( k - 1/e)^-k - 1/2 k\u00b7\u03c3 e^1 / e\u221a(k)\n        =  X_2/ Z _2 + \u03c3 e^1 / emax_k \u2265 3\u221a(k)/2^1 / k( k - 1/e)^k - 1/2 k < \u221e\n\n    by the fact that \u221a(2 \u03c0 x)(x/e)^x\u2264\u0393(x+1) for any x \u2265 2 and \n    \n    lim_k \u2192\u221e\u221a(k)/2^1 / k( k - 1/e)^k - 1/2 k = lim_k \u2192\u221e e^k - 1/2k = \u221a(e).\n\n    On the other hand, if X_G < \u221e, let c = C \u00b7 X_G < \u221e with some positive constant C > 0, then\n    \n    exp{ |X|^2 / c^2 }   \u2264 1 + 2 \u222b_0^\u221e s exp{s^2 / c^2} (|X| / c \u2265 s )   dt \n       \u2264 1 + 4 \u222b_0^\u221e s exp{s^2 / c^2}exp{ -  c^2 s^2 / (2  X_G^2)}  ds \n        = 1 + 4 \u222b_0^\u221e s exp{ - (c^2C^2- 2)/2c^2 s^2}  d s  = 1 + 4c^2/(c^2C^2- 2)\u2264 2\n\n    as long as we take 4c^2/(c^2C^2- 2)\u2264 1, i.e. C^2 \u2265 2 + \u221a(4  X_G^2 + 2) /  X_G. Therefore, we obtain that X_w_2\u2264[ 2 + \u221a(4  X_G^2 + 2) /  X_G ]^1 / 2 X_G < \u221e.\n\n\nA direct result from Theorem <ref> is that the intrinsic moment norm we defined is compatible with the existent definition and the condition that X_G < \u221e is exactly equivalent to X is sub-Gaussian.\n\n\n\n\n\n\n\n\n\nAppendix\n\n\n\n\u00a7 MORE DETAILS IN THE INTRODUCTION PART\n\n\n\n\n\n\n \u00a7.\u00a7 Wrong usage of Hoeffding's inequality\n\n\nThis part mainly discusses some results when we use Hoeffding's inequality wrongly for the unbounded data {X_i}_i = 1^n.\n\n\nIf the Gaussian data {X_i}_i = 1^n i.i.d.\u223c N(\u03bc,\u03c3^2) are misspecified as bounded variable (a bound as a function of n) with high probability, and Hoeffding's inequality is wrongly adopted for the unbounded Gaussian data, it gives\n\n    ( \u03bc\u2208 [X\u0305_n\u00b1  2 \u03c3\u221a(n^ - 1log (4/\u03b1 )) [\u221a(log (4/\u03b1 ))  + \u221a(log n) ]) \u2265 1 - \u03b1.\n\n\n\nLemma <ref> gives a loose CI, since it contains a \u221a(log n) factor. \n\n\nThe Borell\u2013TIS inequality <cit.> gives the probability of a deviation of the maximum of a centered Gaussian random variables (or stochastic processes) above from its expected value. WLOG, we assume {X_i}_i = 1^n i.i.d.\u223c N(0,\u03c3^2) are misspecified as bounded variable, by Borell-TIS inequality\n\n\n (max_i \u2208 [n]X_i -   [max_i \u2208 [n]X_i] > t) \u2264e^ - t^2/(2\u03c3 ^2),\n\n\n we have with probability at least 1 - \u03b1/4 \n\n\nmax_i \u2208 [n]X_i\u2264\u221a(2\u03c3 ^2log (4/\u03b1 ))  +   [max_i \u2208 [n]X_i] \u2264\u03c3\u221a(2) [\u221a(log (4/\u03b1 ))  + \u221a(log n) ],\u00a0i=1,2,\u22ef,n,\n\n\nwhere we use the maximal inequality [max_1 \u2264 i \u2264 nX_i] \u2264\u03c3\u221a(2 log n) [see Corollary 7.2 (a) in <cit.>]. Then,\n\n\n(max_i \u2208 [n]X_i\u2265\u03c3\u221a(2) [\u221a(log (4/\u03b1 ))  + \u221a(log n) ]) \u2264\u03b1/4\n\n\nand (max_i \u2208 [n] (-X_i) \u2265\u03c3\u221a(2) [\u221a(log (4/\u03b1 ))  + \u221a(log n) ]) \u2264\u03b1/4 by symmetric property.\nConditioning on event\n\n\nmax_i \u2208 [n]| X_i| \u2264\u03c3 _n,\u03b1: = \u03c3\u221a(2)[\u221a(log (4/\u03b1 ))  + \u221a(log n) ],\n\n\nFor i.i.d. {X_i}_i = 1^n with a\u2264X_i\u2264b, Hoeffding's inequality gives\n( |X\u0305_n - \u03bc | \u2265b - a/\u221a(2)\u221a(1/nlog (2/\u03b4)) ) \u2264\u03b4). Let \u03b4= \u03b1 /2 and a=\u03c3 _n,\u03b1,b=-\u03c3 _n,\u03b1\n\n    [                                                                             ( |X\u0305_n - \u03bc | \u2265\u221a(2)\u03c3 _n,\u03b1\u221a(n^ - 1log (4/\u03b1 ))); \u2264( |X\u0305_n - \u03bc | \u2265\u221a(2)\u03c3 _n,\u03b1\u221a(n^ - 1log (4/\u03b1 )) ,max_i \u2208 [n]| X_i| \u2264\u03c3 _n,\u03b1) + ( max_i \u2208 [n]| X_i| > \u03c3 _n,\u03b1) \u2264\u03b1 /2 + \u03b1 /2= \u03b1 ]\n\nThen, ( \u03bc\u2208 [X\u0305_n\u00b1  2 \u03c3\u221a(n^ - 1log (4/\u03b1 )) [\u221a(log (4/\u03b1 ))  + \u221a(log n) ] ) \u2265 1 - \u03b1.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Historical notes for sub-Gaussian and its optimal parameter\n\nThe MGF-based variance proxy in Definition <ref> for sub-Gaussian distribution dates back to <cit.>, and it is not unique which cannot be view as the parameter. So it motivates <cit.> to defined the optimal variance proxy \u03c3^2_opt(X) as the unique parameter of sub-Gaussian distribution. \u03c3^2_opt(X) is also called the sub-Gaussian norm in <cit.> or sub-Gaussian diameter in <cit.>. The monograph <cit.> gave comprehensive studies concerning metric characterizations for various sub-Gaussian norms of certain random variables. \n\nFrom Chernoff's inequality, the exponential decay of the sub-Gaussian tail is obtained \n\n\n( X \u2265  t )\u2264inf_s > 0exp{-s t}exp{s X}\u2264inf_s > 0exp(-s t + \u03c3^2_opts^2/2)= exp(- t^2/2 \u03c3^2_opt)\n\n\nby minimizing the upper bound via putting s=t/\u03c3^2. Moreover, for independent {X_i} _i = 1^n with X_i\u223csubG(\u03c3_i^2), we have sub-Gaussian Hoeffding's inequality <cit.>\n\n    (|\u2211_i=1^nX_i| \u2265 t)\u2264 2exp{-t^2/2 \u2211_i=1^n\u03c3_opt^2(X_i)}\u2264 2exp{-t^2/2 \u2211_i=1^n\u03c3_i^2},\u00a0t \u2265 0\n\nfor any variance proxies {\u03c3_i^2}_i=1^n of {X_i} _i = 1^n. \n\nThe \u03c3^2_opt(X)\nnot only characterizes the speed of decay in (<ref>) but also naturally bounds the variance of X as well. To appreciate this, observe that by the definition of sub-Gaussian:\n\n    s^2/2\u03c3^2_opt(X)+o(s^2)= exp(\u03c3^2_opt(X) s^2/2) -1    \u2265exp(s X)-1=s   X +s^2/2  X^2+\u22ef\n       =  s^2/2\u00b7VarX+o(s^2)\n\n(by dividing s^2 on both sides and taking s \u2192 0)\nwhich implies\n\n    \u03c3^2_opt(X) \u2265VarX.\n\nThus, \u03c3^2_opt(X) provides a conservative lower bound for optimal proxy variance.\n\nInterestingly, some special distributions whose variance can attain the \u03c3^2_opt(X). For example, Bernoulli r.v. X \u2208{0,1} with mean \u03bc\u2208(0,1) [denote X \u223cBern(\u03bc)] is sub-Gaussian with\n\n\n1/4\u2265\u03c3_o p t^2(X)=(1-2 \u03bc)/2 log1-\u03bc/\u03bc\u2265\u03bc(1-\u03bc)=Var(X)\n\n\nin <cit.>, while, Hoeffding's inequality shows a crude bound X-\u03bc\u223csubG(1/4). The inequality holds if the Bernoulli distributions is symmetric with \u03bc=1/2, i.e. \u03c3_o p t^2(X)=lim_\u03bc\u2192 1/2(1 - 2\u03bc )/2log1 - \u03bc/\u03bc=1/4=VarX, and the inequality \u03c3_o p t^2(X)=VarX define the strict sub-Gaussianity:\n\nFor zero-mean X\u223csubG(\u03c3^2) is called strict sub-Gaussian if VarX=\u03c3^2_opt(X) [denote X \u223cssubG(\u03c3^2_opt(X))].\n\n  The strict sub-Gaussian r.v.s include Gaussian, symmetric Beta, symmetric Bernoulli and U[-c,c]; <cit.> showed that by a second order ODE (with a unique solution of the Cauchy problem) Beta(\u03b1, \u03b2) has \n  \n\n  \u03c3_opt^2(\u03b1, \u03b2)=\u03b1/(\u03b1+\u03b2) x_0(_1 F_1(\u03b1+1 ; \u03b1+\u03b2+1 ; x_0)/_1 F_1(\u03b1 ; \u03b1+\u03b2 ; x_0)-1) \u2265Var[Beta(\u03b1, \u03b2)],\n  \n\n  where x_0 is a unique solution of\nlog(_1 F_1(\u03b1 ; \u03b1+\u03b2 ; x_0))=\u03b1 x_0/2(\u03b1+\u03b2)(1+_1 F_1(\u03b1+1 ; \u03b1+\u03b2+1 ; x_0)/_1 F_1(\u03b1 ; \u03b1+\u03b2 ; x_0)). Finding the explicit expression and giving the iff condition for general distributions (such as unbounded or asymmetrical distributions) are still an open questions <cit.>.\n\n\nSimilar to Definition <ref>, if sub-G variable is unbounded, we define the optimal lower variance proxy that renders a sharp reverse Chernoff inequality and a sharp lower tails of sub-Gaussian maxima in below. \n\nThe optimal lower variance proxy for a sub-G X is defined as\n \n    l_opt^2(X) := sup{ l^2 \u2265 0 : \ud835\udd3cexp(tX) \u2265exp{l^2 t^2 / 2},  \u2200  t \u2208\u211d}=2 inf_t \u2208t^-2log [exp(t X)].\n\n\n\nSuppose that  l^2_opt(X)>0 for a sub-G r.v. X. For t>0, then\n\n\n(X \u2265 t) \u2265 C_\u03c3,l^2(X) exp{-4[2\u03c3^2_opt(X)/l^4_opt(X)-l^-2_opt(X)]t^2},\n\n\nwhere C_\u03c3,l(X):=( l^2_opt(X)/4\u03c3^2_opt(X)-l^2_opt(X))( 4\u03c3^2_opt(X)-2l^2_opt(X)/4\u03c3^2_opt(X)-l^2_opt(X))^2[2\u03c3^2_opt(X)/l^2_opt(X)-1]\u2208 (0,1).\n\n\n\n(a). Suppose that  l^2_opt(X)>0 for i.i.d.  sub-G r.v. { X_i}_i = 1^n \u223c X. With probability at least 1-\u03b4,\n\n\nl_opt(X)/\u03c3_opt(X)/2\u221a(2\u03c3^2_opt(X)/l^2_opt(X)-1)\u221a(log n-log C_\u03c3,l^-2(X)-loglog(2/\u03b4))\u2264max _1 \u2264 i \u2264 nX_i/\u03c3_opt(X)\u2264\u221a(2[log n + log(2/\u03b4)),\n\n\nwhere C_\u03c3,l(X)< 1 is constant defined in Lemma <ref> below; (b) if X is bounded variable, then l^2_opt(X)=0.\n\n\nThe proof of Lemma <ref> and Proposition <ref> is similar to Lemma <ref> and Theorem <ref>.\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Remarks for Orlicz norm and other commonly used norms\n\n\nIn Remarks <ref> and <ref> below, we will show (|X| \u2265 t)\u2264 2exp{ - t^2/2/(2eX_\u03c8_2^2)} and ( |X| > t  ) \u2264 2 exp{-t^2/[2(X_w_2/\u221a(2))^2]}\u00a0for all  t \u2265 0. The variance can be upper bounded by both norms as \n\n\n(\u221a(5/2)X_w_2)^2 \u2265Var X and ( \u221a(10e)X_\u03c8_2)^2 \u2265Var X.\n\n\n\n\n\nIf exp(|X|^2/X_w_2^2) \u2264 2 for X_w_2<\u221e, then Markov's inequality gives for all  t \u2265 0\n\n    ( |X| > t  ) \u2264(e^|X / X_w_2|^2\u2265 e^t^2 / X_w_2^2) \u2264 2 exp{-t^2/2/(X_w_2/\u221a(2))^2}.\n\n\n\n\n\nUsing Notes 5.4.1 in <cit.>: \n\n    if\u00a0 (|X|>t)\u2264 2 exp{-t^2/(2\u03c3^2)}\u00a0with\u00a0 X = 0,\u00a0then\u00a0exp{s X}\u2264exp{5 \u03c3^2 s^2/2}\n\nfor any s\u2265 0. Therefore, we have \n\n\nexp{s X}\u2264exp{(\u221a(5/2)X_w_2)^2s^2/2}.\n\n\nBy the same argument in (<ref>), one has\n(\u221a(5/2)X_w_2)^2 \u2265Var X.\n\n\n\n    (i) For X \u223c N(0,\u03c3^2), one has X_w_2=\u221a(8/3)\u03c3 [see Example 3.4 in <cit.>], and then we have a crude tail bound from (<ref>), (|X|>t) \u2264 2exp{-t^2/2/(4/3\u03c3^2)}; (ii) For X \u223cBern(0.5), one has X - 0.5_w_2 = 1/2\u221a(log 2), which leads to the bound (|X - 0.5| > t) \u2264 2 exp{ - t^2/2 / 1/4 log 2}; (iii) For X \u223c [a, b], we have X - a + b/2_w_2\u2264b - a/2 \u221a(log 2) by and then (|X - a + b/2| > t) \u2264exp{ - t^2/2 / (b -a)^2/2 log 2}. The results in (ii) and (iii) comes from the conclusion about bounded variables in Example 1 in <cit.>, while Hoeffding's inequality gives sharper tail inequalities (|X - \u03bc| > t) \u2264 2 exp{ - 2 t^2} and (|X - a + b/2| > t) \u2264 2exp{ - 2t^2/(b - a)^2} in (ii) and (iii).\n\n\n\n\nRecall <cit.>'s definition of sub-Gaussian norm  \n\n\nX_\u03c8_2 = max_p \u2265 1  p^-1(   |X|^p )^1/ p.\n\n\nBy p ! \u2265(p / e)^p, a crude bounds also appears. Indeed, by (   |X|^p )^1 / p\u2264 K \u221a(p) for all integer p \u2265 1,\n\n    e^c^ - 1X^2 = 1 + \u2211_p = 1^\u221ec^ - pX^2p/p!   \u2264 1 + \u2211_p = 1^\u221ec^ - p(2K^2p)^p/p!\u2264 1 + \u2211_p = 1^\u221e(2eK^2/c)^p  = 1 + (2eK^2/c)\u2211_p = 0^\u221e(2eK^2/c)^p\n    \n    [2eK^2/c  < 1]   = 1+(2eK^2/c )\u2211_p=0^\u221e(2eK^2/c )^p= 1+(2eK^2/c )1/1-2eK^2/c .\n\nSetting 2eK^2/c\u2264  s<1 and assign s such that exp(c^-1 X^2) \u2264 1+s/1-s=: 2, the solution is s=1/2. Let K=X_\u03c8_2. We thus have c \u22654eX_\u03c8_2^2. The e^X^2/(4eX_\u03c8_2^2)\u2264 2 implies\n(|X| \u2265 t)\u2264 2e^ - t^2/(2\u00b72eX_\u03c8_2^2)\nby using (<ref>). Therefore, \n\n    exp{t X}\u2264exp{( \u221a(10e)X_\u03c8_2)^2t^2/2}.\n\nApplying the same argument in (<ref>), we get\n( \u221a(10e)X_\u03c8_2)^2 \u2265Var X.\n\n\nFor X \u223c N(0,1), observe that\n\n\n    X_\u03c6_2 =max_p \u2265 1 p^ - 1 / 2(  |X|^p )^1 / p = lim_p \u2192\u221e p^ - 1 / 2(  |X|^p )^1 / p =  lim_p \u2192\u221e\u221a(2/p)[ \u0393( (1 + p) / 2) /\u0393 (1/2)]^1 / p=1 / \u221a(2)\u2248 0.7071.\n\n\nFor uniform distributed X\u223c U[-1, 1], one has X_\u03c6_2  = 0.4082 and (|X| \u2265 t) \u2264 2e^ - t^2/(4e\u00b7 0.4082^2)=2e^ - t^2/ 1.8122\ncomparing to Hoeffding\u2019s inequality with a sharper bound (|X|  \u2265  t )\u2264  2 e^- t^2. For X \u223cBern(\u03bc),\n1/\u221a(p)[| X - \u03bc|^p]^1/p = 1/\u221a(p)[(1 - \u03bc)\u03bc^p + \u03bc(1 - \u03bc)^p]^1/p. Let \u03bc=0.3, we have X_\u03c6_2  = 0.3240 and (|X| \u2265 t) \u2264 2e^ - t^2/(4e\u00b7 0.3240^2)=2e^ - t^2/1.1417 comparing to Hoeffding\u2019s inequality with a sharper bound (|X|  \u2265  t )\u2264  2 e^- 2t^2.\n\n\nThe (<ref>) implies\n\n    (|\u2211_i=1^nX_i| \u2265 t)\u2264 2exp{-t^2/2 \u2211_i=1^n (\u221a(10e)X_i_w_2)^2},\u00a0t \u2265 0.\n\nThe 5X_w_2^2 \u2265 2Var X implies that for strictly sub-G independent variables {X_i}_i=1^n\n\n\n(|\u2211_i=1^nX_i| \u2265 t)\u2264 2exp{-t^2/2 \u2211_i=1^nVar(X_i)}\u2264 2exp{-t^2/5\u2211_i=1^nX_i_\u03c8_2^2}<2exp{-t^2/2 \u2211_i=1^n (\u221a(10e)X_i_w_2)^2}.\n\n\nHence, 2\u00b7_w_2-norm leads to a looser concentration bound, by comparing (<ref>).\n\n\n\n\n\n \u00a7.\u00a7 Details for Table 1\n\n\n    \nLet {X_i} _i = 1^n be i.i.d. r.v.s with X_1=0,   X_1^2=\u03c3^2>0, and |X_1|^3=\u03c1<\u221e. <cit.> gave a tighter estimate of the absolute constant in B-E bounds for X\u0305_n:=1/n\u2211_i=1^nX_i:\n\n    \u0394_n:=sup _x \u2208\u211d|(\u221a(n)/\u03c3X\u0305_n\u2264 x)-\u03a6(x)| \u22640.3328(\u03c1+0.429 \u03c3^3)/\u03c3^3\u221a(n),\u00a0\u2200\u00a0n \u2265 1,\n\nwhere \u03a6(\u00b7) is the cumulative distribution function of N(0,1).\n\nConsider Bernoulli samples {X_i} _i = 1^n  i.i.d.\u223cBer(1/2) with \u03c3=1/2 and \u03c1=1/8, and <cit.> shown \u0394_n \u2264 0.409954/\u221a(n). Put \u03b4  =0.05, 0.075, 0.1. For  n \u2265 1, Hoeffding's inequality gives\n\n\n   ( |X\u0305_n-1/2 | \u22641/2\u221a(n)\u00b7\u221a(2log (2/\u03b4))) \u2265 1-\u03b4.\n \n\n\nFrom B-E bounds (<ref>), we have\n(\u221a(n)/\u03c3 (X\u0305_n-1/2) \u2264 - x ) - \u03a6 (-x) \u2264\u0394_n and (\u221a(n)/\u03c3 (1/2-X\u0305_n) \u2264 - x ) - \u03a6 (-x) \u2264\u0394_n \nby the symmetry of Ber(1/2). Set\n\n\n(\u221a(n)/\u03c3 (X\u0305_n-1/2) \u2264 - x )  \u2264\u0394_n+\u03a6 (-x)\u22640.409954/\u221a(n)+\u03a6 (-x) =:\u03b4/2;\n(\u221a(n)/\u03c3 (X\u0305_n-1/2) \u2265 x )  \u2264\u0394_n+\u03a6 (-x)\u22640.409954/\u221a(n)+\u03a6 (-x) =:\u03b4/2,\n\n\nwhere x = - \u03a6^-1 (\u03b4/2 -0.409954/\u221a(n)) with \u03b4/2 -0.409954/\u221a(n)>0.\n\nThen, it results in a (1 - \u03b4)100%'s non-asymptotic CI:\n\n\n    ( |X\u0305_n-1/2 | \u2264 - 1/2\u221a(n)\u00b7\u03a6^-1(\u03b4/2-0.409954/\u221a(n))) \u2265 1-\u03b4\n\n\nfor n \u2265 (0.8199/\u03b4)^2, which require least sample sizes n\u2265 269, 120, 68 for \u03b4  =0.05, 0.075, 0.1 respectively.\n\nFor symmetric data with zero mean and finite third moment, (<ref>) gives a trivial bound if we put  0.3328(\u03c1+0.429 \u03c3^3)/\u03c3^3\u221a(n)\u2265 1, i.e. the B-E bound is useless when \nn \u2264[0.3328(\u03c1+0.429 \u03c3^3)]^2/\u03c3^6.\n\n\n\n\u00a7 SMALL SAMPLE LEAVE-ONE-OUT AVERAGE IN MOMENT NORM ESTIMATIONS\n\n\nFor the small sample zise (n \u2264 20), one has two other methods, except the direct empirical moment method (DE). The first one is the well-known Bootstrap. The non-parametric Bootstrap can reduce the estimator's variance and make it more robust (see p512 in <cit.>). Especially, here we use (n-1)-out-of-n Bootstrap and construct n - 1 Bootstrap estimators and then take the median of these estimators. The second robust method under small sample setting is called the leave-one-out Hodges-Lehmann method (LOO-HL) proposed by <cit.>. Specially, based on sample X = (X_1, \u2026, X_n)^\u22a4\u2208\u211d^n, define LOO-HL empirical mean estimator as\n\n    \u03bc\u0302_ LOO := med{DE (X_(-i)) + DE (X_(-j))/2 : 1 \u2264 i < j \u2264 n },\n\n where DE (X_(-i)):=\u2211_k  iX_k/(n-1) is the empirical mean estimator of \n X_(-i) = { X_1, \u2026, X_i - 1, X_i + 1, \u2026, X_n}.\n\n It is worthy to note that the leave-one-out method is different from classic  Hodges\u2013Lehmann empirical mean estimator \u03bc\u0302_ HL :=med_i \u2264 jX_i+X_j/2 which uses a single sample X_i instead of leave-one-out mean DE (X_(-i)) since in practice we find that classic Hodges-Lehmann method cannot render ideal performance.\n\n\n\nTo see the performance, we use the three methods above to calculate the relative estimators' errors based on small samples corresponding to the settings in the previous section, except we use 1% independent Cauchy(0, 5) perturbation to contaminate the original distribution. The results is shown in Figure <ref>. It can be seen that the three methods can achieve relatively good performance, while the LOO-HL method gives less error overall and is obviously better than the other two methods when 2\u2264 n \u2264 4.\n\n\n\n\n\n\u00a7 PROOFS OF MAIN RESULTS\n\n\n\n \n\n   Note that X_G := max_m \u2208 2 \u2115[  X^m/ Z^m]^1 / m, where Z\u223c N(0,1). If the maximum take at m = \u221e, \n    \n    max_m =  2k \u2208 2 \u2115[  X^m/ Z^m]^1 / m = max_k \u2208\u2115[ 2^k/\u221a(\u03c0)\u0393( k + 1/2)]^-1 / (2k) X_2k\n\n    is an increasing function for some sub-sequence {k_\u2113}\u2286{ k} such that lim_\u2113\u2192\u221e k_\u2113 = \u221e when \u2113 is large enough, where we use the formula of 2k-th moment of standard normal distribution [see (18) in <cit.>]. \n    \n    Therefore,\n    \n    [ 2^k_\u2113/\u221a(\u03c0)\u0393( k_\u2113 + 1/2)]^-1 / (2k_\u2113) X_2k_\u2113\u2264[ 2^k_\u2113 + 1/\u221a(\u03c0)\u0393( k_\u2113 + 3/2)]^-1 / (2k_\u2113 + 2) X_2k_\u2113 + 2\n\n    i.e.\n2 k_\u2113 + 1/2[ \u221a(\u03c0)/\u0393(k_\u2113 + 1 / 2)]^1 / k_\u2113\u2264 X_2 k_\u2113 + 2^2 k_\u2113 + 2/ X_2 k_\u2113^2 k_\u2113 + 2\n    for any k_\u2113 is large enough. Let \u2113\u2192\u221e, we have\n    \n    1 = lim sup_\u2113\u2192\u221e X_2 k_\u2113 + 2^2 k_\u2113 + 2/ X_2 k_\u2113^2 k_\u2113 + 2   \u2265lim_\u2113\u2192\u221e2 k_\u2113 + 1/2[ \u221a(\u03c0)/\u0393(k_\u2113 + 1 / 2)]^1 / k_\u2113\n        = lim_k_\u2113\u2192\u221e (k_\u2113 + 1 / 2) [ \u221a(\u03c0)/\u221a(2 \u03c0) (k_\u2113 + 1 / 2)^k_\u2113 e^- (k_\u2113 + 1 / 2)]^1 / k_\u2113 = lim_\u2113\u2192\u221ee^1 + 1 / (2 k_\u2113)/\u221a(2) = e/\u221a(2) > 1,\n\n    which leads to a contradiction, where we use a fact that lim_n \u2192\u221e X_n =  X_\u221e = esssup |X|, and hence\n    lim sup_\u2113\u2192\u221e X_2 k_\u2113 + 2^2 k_\u2113 + 2/ X_2 k_\u2113^2 k_\u2113 + 2 = 1. As a result, one must have max_m \u2208 2 \u2115[  X^m/ Z^m]^1 / m < \u221e.\n\n\n\n    lim sup_\u2113\u2192\u221e X_2 k_\u2113 + 2^2 k_\u2113 + 2/ X_2 k_\u2113^2 k_\u2113 + 2\u2264lim sup_k \u2192\u221e X_2 k + 2^2 k + 2/ X_2k^2 k + 2    = lim sup_k \u2192\u221elim_M \u2192\u221e X1(|X|\u2264 M \u2227 X_\u221e)_2 k + 2^2 k + 2/ X1(|X|\u2264 M \u2227 X_\u221e) _2k^2 k + 2\n       \u2264lim sup_M \u2192\u221elim_p \u2192\u221elim sup_k \u2192\u221e[   X1(|X|\u2264 M \u2227 X_\u221e)_2 k + 2/ X1(|X|\u2264 M \u2227 X_\u221e) _2k]^2p+2\n       \u2264lim sup_M \u2192\u221elim_p \u2192\u221e[ M \u2227 X_\u221e/M \u2227 X_\u221e]^2p + 2 = 1.\n\n\n\n\n\n\nIf X_i is symmetric around zero, then we have by X_i^2k+1=0 for k \u2208\u2115_+\n\n    e^tX_i = 1 + \u2211_k = 1^\u221et^2kX_i^2k/(2k)!   \u2264 1 + \u2211_k = 1^\u221et^2k/(2k)!(2k)! X_i _G^2k/2^kk! = 1 + \u2211_k = 1^\u221e(t^2 X_i _G^2/2)^k/k!  = exp{t^2 X_i _G^2/2}\n\nfor all t \u2208\u211d, where the last inequality is by the definition of X_i _G<\u221e such that X_i^2k\u2264(2k)!/2^kk! X_i_G^2k.\nThen it proves X_i \u223csubG( X_i _G^2), which shows case (a).\n\nFor case (b), if X_i has zero mean, then we bound the odd moment by even moments. For k=1,2,\u22ef and c_k>0, Cauchy's inequality and mean value inequality imply\n\n    |tX_i|^2k + 1\u2264( c_k^-1 |tX_i|^2k\u00b7 c_k |t X_i|^2k + 2)^1/2\u2264( c_k^-1t^2kX_i^2k + c_kt^2k + 2X_i^2k + 2)/2.\n\nSo, |tX_i|^3/3!\u2264c_1^-1t^2X_i^2 + c_1t^4X_i^4/2 \u00b7 3!, |tX_i|^5/5!\u2264c_2^-1t^4X_i^4 + c_2t^6X_i^6/2 \u00b7 5!, and so on, which implies\n\n    e^tX_i\u2264 1 + \u2211_k = 2^\u221et^k|X_i|^k/k!\u2264 1 + t^2X_i^2/2! +c_1^-1t^2X_i^2 + c_1t^4X_i^4/2 \u00b7 3!+ t^4X_i^4/4!\n       +c_2^-1t^4X_i^4 + c_2t^6X_i^6/2 \u00b7 5!+ t^6X_i^6/6!+c_3^-1t^6X_i^6 + c_3t^8X_i^8/2 \u00b7 7!+\u22ef\n       \u2264 1 + ( 1+ c_1^-1/ 3!)t^2X_i^2/2!+( 1+ 4!c_1/2\u00b7 3! + 4!c_2^-1/2\u00b7 5!)t^4X_i^4/4!+ ( 1+ 6!c_2/2\u00b7 5! + 6!c_3^-1/2\u00b7 7!)t^6X_i^6/6!+\u22ef\n       \u2264 1 + ( 1+ 1/6c_1)t^2X_i^2/2!+( 1+ 2c_1+ c_2^-1/10)t^4X_i^4/4!+( 1+ 3c_2+ c_3^-1/14)t^6X_i^6/6!+\u22ef\n       \u2264 1 + ( 1+ 1/6c_1)t^2X_i^2/2!+( 1+ 2c_1+ c_2^-1/10)t^4X_i^4/4!+\u2211_k = 3^\u221e( 1 + kc_k-1 + c_k^-1/4k + 2)t^2kX_i^2k/(2k)!.\n\n To bound (<ref>), we assign c_k=x^-1\u00b7m^k+1/2k+2 for k \u2265 2 and x, m>0. Consider the following system of equations:\n\n    {[               1 + 1/6c_1 = m; 1 + 2c_1 + c_2^ - 1/10 = m^2 ].\n\nThis system with c_2 = 1/6\u00b7 xm^3 gives\n1 + 2c_1 + 0.6x( 1 + 1/6c_1)^ - 3 = ( 1 + 1/6c_1)^2\nwhich could implies c_1=0.4 if we set x=0.9806308. And then  m=1+ 1/6c_1= 17/12. Therefore,  (<ref>) has a further upper bound\n\n    e^tX_i   \u2264 1 + mt^2X_i^2/2!+m^2t^4X_i^4/4!+\u2211_k = 3^\u221e( 1 + kc_k-1 + c_k^-1/4k + 2)t^2kX_i^2k/(2k)!\n       \u2264\u2211_k = 0^\u221e(\u221a(m) t)^2kX_i^2k/(2k)!\u2264 1 + \u2211_k = 1^\u221e(\u221a(m) t)^2k/(2k)!(2k)! X_i _G^2k/2^kk!\u2264exp{t^2(\u221a(17/12) X_i_G)^2/2},\n\nwhere the first inequality stems from 1 + m^k/2x + (k + 1)x/2k + 1\u00b7m^ - k - 1\u2264m^k with m = 17/12,k = 3,4,\u22ef, and the last inequality is by the definition of X_i _G. Thus we show X_i \u223csubG(17 X_i _G^2/12).\n\n\n\n\n\n\nFor (a), it remains to show the lower tail bound. For t\u2265 0, by the independence of { X_i}_i = 1^n, \n\n    {max _1 \u2264 i \u2264 nX_i\u2264 t}   =(X_1\u2264 t, \u22ef,X_n\u2264 t) = \u220f_i=1^n(X_i\u2264 t) \n    \n    [Applying\u00a0Lemma\u00a0<ref>]\u00a0   \u2264(1-C^2(X)e^-4[2 X _G^2/ X _G\u0303^2-1]t^2)^n\u2264exp(-nC^2(X)e^-4[2 X _G^2/ X _G\u0303^2-1]t^2),\n\nwhere we use 1-x \u2264 e^-x for all  x \u2208\u211d in the last inequality.\n\nLet \u03b4=exp (-nC^2(X)e^-4[2 X _G^2/ X _G\u0303^2-1]) and we get \nt= X _G\u0303/ X _G/2\u221a(2 X _G^2/ X _G\u0303^2-1)\u221a(log n-log C^-2(X)-loglog(2/\u03b4)).\n\nFor (b), if X\u2264 M<\u221e, it shows\n\n    0\u2264 X _G\u0303\u2264min_k \u2265 1[ 2^kk!/(2k)!M^2k]^1/(2k)=Mmin_k \u2265 1[ 2^kk!/(2k)!]^1/(2k)= 0,\n\nSo we immediately get X _G\u0303=0.\n\n\n\nThe proof is based on Paley\u2013Zygmund inequality P(Z\u2265\u03b8EZ) \u2265(1-\u03b8)^2E[Z]^2/E[Z^2] for a positive r.v. Z with finite variance, where \u03b8\u2208 (0,1); see Page 47 in <cit.>. \n\nSince X is symmetric around zero, one has X^2k+1=0 for k \u2208\u2115_+, which gives for all s \u2208\u211d,\n\n    e^sX  = 1 + \u2211_k = 1^\u221es^2kX^2k/(2k)!\u2265 1 + \u2211_k = 1^\u221es^2k/(2k)!(2k)! X _G\u0303^2k/2^kk! = 1 + \u2211_k = 1^\u221e(s^2 X _G\u0303^2/2)^k/k!  = exp{s^2 X _G\u0303^2/2},\n\n where the last inequality stems from the definition of X _G\u0303<\u221e such that X^2k\u2265(2k)!/2^kk! X_G\u0303^2k.\n\nLet Z=exp{sX}. The above Paley\u2013Zygmund inequality and (<ref>) imply\n\n    (Z \u2265 t)    := (exp{sX}\u2265\u03b8exp{ X _G\u0303^2s^2 / 2}) \u2265(exp{sX}\u2265\u03b8Eexp{sX})\u2265 (1-\u03b8)^2[Eexp{sX}]^2/Eexp{2sX}\n       \u2265 (1-\u03b8)^2exp{ X _G\u0303^2s^2}/exp{2 X _ G^2 s^2}=(1-\u03b8)^2exp{-[2 X _G^2- X _G\u0303^2] s^2}\n\nwhere t:=  X _G\u0303^2s/2+log\u03b8/s>0, and the last inequality is from Theorem <ref>(a). Put s=t+\u221a(t^2+2 X _G\u0303^2log(1/\u03b8))/ X _G\u0303^2, which is solved from equation\n1/2 X _G\u0303s^2-ts-log(1/\u03b8)=0,\nthen we have \n\n    s^2= X _G\u0303^-4[t+\u221a(t^2+2 X _G\u0303^2log(1/\u03b8))]^2\u22644t^2+4 X _G\u0303^2log(1/\u03b8)/ X _G\u0303^4.\n\nSubstitute this upper bound into (<ref>), and it leads to\n\n    (X \u2265 t)    \u2265 (1-\u03b8)^2exp{-[2 X _G^2- X _G\u0303^2][4t^2+4 X _G\u0303^2log(1/\u03b8)]/ X _G\u0303^4}\n       =(1-\u03b8)^2exp{-[2 X _G^2/ X _G\u0303^4- X _G\u0303^-2][4t^2+4 X _G\u0303^2log(1/\u03b8)]}\n       =(1-\u03b8)^2\u03b8^4[2 X _G^2/ X _G\u0303^4- X _G\u0303^-2]exp{-4[2 X _G^2/ X _G\u0303^4- X _G\u0303^-2]t^2}.\n\nTaking sup on \u03b8\u2208 (0,1) over the two sides of (<ref>), we have\n\n    (X \u2265 t)\u2265exp{-4[2 X _G^2/ X _G\u0303^4- X _G\u0303^-2]t^2}sup_\u03b8\u2208 (0,1)(1-\u03b8)^2\u03b8^4[2 X _G^2/ X _G\u0303^2-1]\n        =(  X _G\u0303^2/4 X _G^2- X _G\u0303^2)( 4 X _G^2-2 X _G\u0303^2/4 X _G^2- X _G\u0303^2)^2[2 X _G^2/ X _G\u0303^2-1]exp{-4[2 X _G^2/ X _G\u0303^4- X _G\u0303^-2]t^2}\n\nwhere the supremum of sup_\u03b8\u2208 (0,1)(1-\u03b8)^2\u03b8^4[2 X _G^2/ X _G\u0303^2-1] is attained at \u03b8_0=4 X _G^2-2 X _G\u0303^2/4 X _G^2- X _G\u0303^2.\n\n\n\n    As in <cit.>, the sub-Weibull condition is that X \u223csubW (\u03b7) is defined as a sub-Weibull r.v. with sub-Weibull index \u03b7>0 if it has a finite sub-Weibull norm X_w_\u03b7:=inf{C\u2208(0, \u221e): E[exp(|X|^\u03b7/C^\u03b7)]\u2264 2}. It is easy to see that, for sub-G X, we have X \u223csubW(2). Write \n  \n    X _G^2 k_X -  X_G^2 k_X = 1/(2 k_X - 1)!![ 1/n\u2211_i = 1^n  X_i^2k_X -  X^2 k_X].\n  \n  Since X \u223csubW(2), by Corollary 4 in <cit.>, we have X^2k_X\u223csubW(1 / k_X).\n  Then apply Theorem 1 in <cit.>, we get \n\n\n    ( | X _G^2 k_X -  X_G^2 k_X| \u2264 2  e  n^- 1 / 2 X_\u03c8_1 / k_X  C(k_X^-1){\u221a(t) + L_n(k_X^-1, 1_n^\u22a4 n^-1 X_\u03c8_1 / k_X) t^k_X}) \u2265 1-2 e^-t,\n\n\n where constants C(\u00b7) and L_n(\u00b7, \u00b7) is defined in Theorem 1 of <cit.>, and\n    \n    L_n(k_X^-1,  n^-1 X_\u03c8_1 / k_X 1_n) \n    \n                :=    \u03b3^2 k_X A(k_X^-1) n^-1 X_\u03c8_1 / k_X 1_n_\u221e/n^-1 X_\u03c8_1 / k_X 1_n_2 1{0<k_X^-1\u2264 1}+\u03b3^2 k_X B(k_X^-1) n^-1 X_\u03c8_1 / k_X 1_n_\u03b2/n^-1 X_\u03c8_1 / k_X 1_n_2 1{k_X^-1 >1}\n    \n                =    \u03b3^2 k_X A(k_X^-1) n^-1 X_\u03c8_1 / k_X 1_n_\u221e/n^-1 X_\u03c8_1 / k_X 1_n_2 = \u03b3^2 k_X A(k_X^-1) / \u221a(n),\u00a0\u00a0(1 / \u03b8 + 1 / \u03b2 = 1).\n\n\n\n\n    Let MOM_b[Y]:=med_s \u2208 [b]{_m^B_s Y} be the MOM estimator for data {Y_i}_i=1^n.\n\nSince b_S represents the number of sane block containing no outliers, and \u03b7(\u03b5) is a possitive fraction function for sane block such that\nb_S \u2265\u03b7(\u03b5) b. For \u03f5 >0, in fact, if\n\n\n\u2211_k \u2208 [b_S]1_{|_m^B_k Y- Y|> \u03f5}\u2264 b_S-b/2,\nthen |MOM_b[Y]- Y| \u2264\u03f5.\n\n\nThe reason is that if at least b/2 sane block {B_k} s.t. |_m^B_k Y- Y|\n\u2264\u03f5, then |MOM_b[Y]- Y| \u2264\u03f5. We have\n\n    {|MOM_b[Y]- Y| \u2264\u03f5}\u2283{|{k \u2208 [b_S]:|_m^B_k Y- Y|\u2264\u03f5}| \u2265b/2}  ={\u2211_k \u2208 [b_S] 1_{|_m^B_k Y- Y|> \u03f5}\u2264 b_S-b/2}.\n\nThen,\n\n    {|MOM_b[Y]- Y| \u2264\u03f5}\u2265{\u2211_k \u2208 [b_S] 1_{|_m^B_k Y- Y|> \u03f5}\u2264 b_S-b/2}\n        = {\u2211_s \u2208 [b_S][ 1_{|_m^B_s Y- Y|>\u03f5}-{|_m^B_s Y- Y|>\u03f5}]<b_S-b/2-b_S{|_m^B_s Y- Y|>\u03f5}}\n       \u2265{\u2211_s \u2208 [b_S][ 1_{|_m^B_s Y- Y|>\u03f5}-{|_m^B_s Y- Y|>\u03f5}]< b_S[1-1/2\u03b7(\u03b5)-{|_m^B_s Y- Y|>\u03f5}]},\n\nwhere the last inequality is by (M.2): -b/2\u2265 -b_S/2\u03b7(\u03b5). In (<ref>), Chebyshev's inequality implies \n\n    {|_m^B_sY -  Y| \u2265 2\u221a(\u03b7(\u03b5)VarY/m)}\u2264{|_m^B_sY -  Y| \u2265 2\u221a(\u03b7(\u03b5)VarY/ m)}\u22641/4\u03b7(\u03b5).\n\nLet \u03f5=2\u221a(\u03b7(\u03b5)VarY/m), and the last inequality shows\n\n    {|MOM_b[Y]- Y|  \u2264\u03f5}\u2265{\u2211_s \u2208 [b_S][ 1_{|_m^B_s Y- Y|>\u03f5}-{|_m^B_s Y- Y|>\u03f5}]<b_S[1-3/4\u03b7(\u03b5)]}.\n\n\n\n\n\nSince {1_{|_m^B_s Y- Y|>\u03f5}}_s \u2208 [b_S] are independent r.v. which is bounded by 1, Hoeffding's inequality shows\n\n    {\u2211_s \u2208 [b_S][ 1_{|_m^B_s Y- Y|>\u03f5}-{|_m^B_s Y- Y|>\u03f5}]<b_S[1-3/4\u03b7(\u03b5)]}\u2265 1 - e^ - 2[b_S(1-3/4\u03b7(\u03b5))]^2/\u2211_s = 1^b_S(1 - 0)^2 = 1 - e^ - 2b_S(1-3/4\u03b7(\u03b5))^2.\n\nTherefore, we have\n\n    {|MOM_b[Y]-  Y | \u22652\u221a(\u03b7(\u03b5)VarY/m)}\u2264 e^ - 2b_S(1-3/4\u03b7(\u03b5))^2\u2264 e^ - 2\u03b7(\u03b5) b(1-3/4\u03b7(\u03b5))^2,\n\nwhere the last inequality is from b_S \u2265\u03b7(\u03b5) b.\n\nNext, recall that\n\n    X _G=max_1 \u2264 k \u2264 k_X[ X^2k/(2k - 1)!!]^1/(2k)=max_1 \u2264 k \u2264\u03ba_n[ X^2k/(2k - 1)!!]^1/(2k)\u00a0for any\u00a0\u03ba_n \u2265 k_X\n\nand \nX _b,G =max_1 \u2264 k \u2264 k_Xmed_s \u2208 [b]{[1/(2 k-1) ! !\u00b7_m^B_s X^2k]^1/(2k)}. Recall that g_k,m(\u03c3 _k) and g\u0305_k,m(\u03c3 _k) are the sequences s.t.\n\n    [ X^2k/(2k - 1)!!]^1/(2k)(1 - g\u0305_k,m(\u03c3 _k))=max_1 \u2264 k \u2264\u03ba_n[  -2[m/\u03b7(\u03b5)]^-1/2\u03c3_k^k/( X^2 k)+ X^2k/(2k - 1)!!]^1/(2k);\n\n    \n    [2[m/\u03b7(\u03b5)]^-1/2\u03c3_k^k/( X^2 k)+1]^1 /(2 k)=1+g_k,m(\u03c3 _k)\u00a0for any\u00a0m \u2208\u2115\u00a0and\u00a01 \u2264 k \u2264\u03ba_n\u00a0respectively.\n\n\n    \nFor the first inequality, we have by (<ref>)\n\n    { X _b,G\u2264 [1 - max_1 \u2264 k \u2264\u03ba_ng\u0305_k,m(\u03c3 _k)]  X _G}= { X _b,G\u2264max_1 \u2264 k \u2264\u03ba_n [ X^2k/(2k - 1)!!]^1/(2k)(1 - max_1 \u2264 k \u2264\u03ba_n g\u0305_k,m(\u03c3 _k)) }\n       \u2264{ X _b,G\u2264max_1 \u2264 k \u2264\u03ba_n [ X^2k/(2k - 1)!!]^1/(2k)(1 - g\u0305_k,m(\u03c3_k)) }\n    \n    [By\u00a0(<ref>)]\u00a0   = { X _b,G\u2264[  - \u03c3 _k^k/(2k - 1)!!\u00b72/[m/\u03b7(\u03b5)]^1 / 2 + X^2k/(2k - 1)!!]^1/(2k)}\n       \u2264\u2211_k = 1^\u03ba_n{med_s \u2208 [b]{[1/(2 k-1) ! !\u00b7_m^B_sX^2k]^1/(2k)}\u2264[ \u03c3 _k^k/(2 k-1) ! !\u00b72/[m/\u03b7(\u03b5)]^1 / 2 + X^2k/!k]^1/(2k)}\n        = \u2211_k = 1^\u03ba_n{med_s \u2208 [b]{1/(2 k-1) ! !\u00b7_m^B_sX^2k}\u2264X^2k/(2 k-1) ! !- \u03c3 _k^k/(2 k-1) ! !\u00b72/[m/\u03b7(\u03b5)]^1 / 2}\n        = \u2211_k = 1^\u03ba_n{med_s \u2208 [b]{1/(2 k-1) ! !\u00b7 [_m^B_sX^2k - X^2k]}\u2264 -\u03c3 _k^k/(2 k-1) ! !\u00b72/[m/\u03b7(\u03b5)]^1 / 2}\n        < \u2211_k = 1^\u03ba_n{|med_s \u2208 [b]{_m^B_s[X^2k -X^2k]} | \u2265\u03c3 _k^k\u00b72/[m/\u03b7(\u03b5)]^1 / 2}\u2264\u03ba_ne^ - 2\u03b7(\u03b5) b(1-3/4\u03b7(\u03b5))^2,\n\nwhere the last inequality is by (<ref>) with Y_i=X_i^2k; and the assumption that \u221a(VarX^2k)\u2264\u03c3 _k^k,\u00a01 \u2264 k \u2264\u03ba_n.\n\nLet g_m(\u03c3):=max_1 \u2264 k \u2264\u03ba_n g_k,m(\u03c3 _k). For the second inequality, the definition of g_k,m(\u03c3 _k) implies\n\n    { X _G <  X _b,G/1 + g_m(\u03c3)}={ X _b,G-max_1 \u2264 k \u2264\u03ba_n [ X^2 k/(2 k-1) ! !]^1 /(2 k)>g_m(\u03c3) max_1 \u2264 k \u2264\u03ba_n [ X^2 k/(2 k-1) ! !]^1 /(2 k)}\n       \u2264{ X _b,G-max _1 \u2264 k \u2264\u03ba_n [ X^2 k/(2 k-1) ! !]^1 /(2 k)>g_k, m(\u03c3 _k) max_1 \u2264 k \u2264\u03ba_n[ X^2 k/(2 k-1) ! !]^1 /(2 k)}\n       ={ X _b,G>max_1 \u2264 k \u2264\u03ba_n [ X^2 k/(2 k-1) ! !]^1 /(2 k)(1+g_k, m(\u03c3 _k))}\n       ={ X _b,G>max_1 \u2264 k \u2264\u03ba_n [ X^2 k/(2 k-1) ! !]^1 /(2 k)[\u03c3_k^k/ X^2 k\u00b72/[m/\u03b7(\u03b5)]^1 / 2+1]^1 /(2 k)}\n       ={ X _b,G>max_1 \u2264 k \u2264\u03ba_n [\u03c3_k^k/(2 k-1) ! !\u00b72/[m/\u03b7(\u03b5)]^1 / 2+ X^2 k/(2 k-1) ! !]^1 /(2 k)}\n       \u2264{max_1 \u2264 k \u2264\u03ba_n med_s \u2208 [b]{[1/(2 k-1) ! !\u00b7_m^B_s X^2k]^1/(2k)}>[\u03c3_k^k/(2 k-1) ! !\u00b72/[m/\u03b7(\u03b5)]^1 / 2+ X^2 k/(2 k-1) ! !]^1 /(2 k)}\n       \u2264\u2211_k=1^\u03ba_n{med_s \u2208 [b]{[1/(2 k-1) ! !\u00b7_m^B_s X^2 k]^1 / 2k}>[\u03c3_k^k/(2 k-1) ! !\u00b72/[m/\u03b7(\u03b5)]^1 / 2+ X^2 k/(2 k-1) ! !]^1 /(2 k)}\n       =\u2211_k=1^\u03ba_n{med_s \u2208 [b]{1/(2 k-1) ! !\u00b7_m^B_s X^2 k}>\u03c3_k^k/(2 k-1) ! !\u00b72/[m/\u03b7(\u03b5)]^1 / 2+ X^2 k/(2 k-1) ! !}\n       =\u2211_k=1^\u03ba_n{med_s \u2208 [b]{_m^B_s X^2 k}> 2\u03c3_k^k/[m/\u03b7(\u03b5)]^1 / 2+   X^2 k}=\u2211_k=1^\u03ba_n {med_s \u2208 [b]{_m^B_s[X^2 k-  X^2 k]}>2\u03c3_k^k/[m/\u03b7(\u03b5)]^1 / 2}\n    \n    [By\u00a0(<ref>)]\u00a0    < \u2211_k = 1^\u03ba_n {|med_s \u2208 [b]{_m^B_s[X^2k - X^2k]} | \u2265\u03c3 _k^k\u00b72/[m/\u03b7(\u03b5)]^1 / 2}\u2264\u03ba_n e^ - 2\u03b7(\u03b5) b(1-3/4\u03b7(\u03b5))^2,\n\nwhere the last inequality stems from \u221a(VarX^2k)\u2264\u03c3 _k^k,\u00a01 \u2264 k \u2264\u03ba_n for a sequence {\u03c3 _k}_k \u2265 1.\n\n\nProof of Theorem <ref>:\nDenote Y_k,i represents the i-th value of reward of arm k in its history, where i \u2208 [T_k(t)] for any round t.\nIt is needed to bound T_k(t) as one has \n\n    Reg_T \u2264\u2211_k = 2^K \u0394_k  T_k(t),\n\nfrom (<ref>), so we can only focus on some fixed arm. Hence, we can just drop the subscript k in { Y_k, i}_i = 1^T_k(t) as { Y_i}_i = 1^T_k(t).\n\nWe first give a lemma, which is crucial in the following proof.\n\n    Let { Y_i }_i = 1^n be independent r.v.s with \u03bc_i =  Y_i, and assume that Y_i - \u03bc_i is symmetric around zero with Y_i - \u03bc_i _G\u2264 C and {w_i }_i = 1^n are i.i.d. Rademacher r.v.s independent of { Y_i }_i = 1^n. Let w:=1/n\u2211_i=1^nw_i. Then,\n    \n    ( 1/n\u2211_i = 1^n (w_i - w) (Y_i - \u03bc_i ) \u2264 C \u221a(2log (1 / \u03b1)/n)) \u2265 1 - \u03b1.\n\n\n\n\nFrom Theorem <ref> , we know that under the conditions in the lemma,\n    \n    ( 1/n\u2211_i = 1^n a_i (Y_i - \u03bc_i) \u2264 C  a _2 \u221a(2log (1 / \u03b1))/n) \u2265 1 - \u03b1\n\n    for any vector a:=(a_1,\u2026,a_n)^\u22a4\u2208\u211d^n.\n\n    On the other hand, we have following inequalities,\n    \n    w - w_2^2 := \u2211_i = 1^n (w_i - w)^2 = \u2211_i = 1^n w_i^2 - n w^2 = n (1 - w^2) \u2264 n,\u00a0and\u00a0 w - w_\u221e\u2264 2.\n\nTherefore, one has \n    \n    ( 1/n\u2211_i = 1^n (w_i - w) (Y_i - \u03bc_i ) \u2264 C \u221a(2log (1 / \u03b1)/n))\n                = _w _y ( 1/n\u2211_i = 1^n (w_i - w) (Y_i - \u03bc_i ) \u2264 C \u221a(2log (1 / \u03b1)/n)) \n       \u2265_w _y ( 1/n\u2211_i = 1^n (w_i - w) (Y_i - \u03bc_i ) \u2264 C   w - w_2 \u221a(2log (1 / \u03b1))/n) \u2265 1 - \u03b1,\n\nwhere the second inequality is by (<ref>) and the last inequality applies (<ref>).\n\n\nBased on Lemma <ref>, next we can prove Theorem <ref>. We first state the assumptions for Theorem <ref> in detail.\n\n\n    \n  UCB.A1(UCB1) The rewards of k-the arm in round t, Y_k. And \u221a(VarY_k^2\u03ba)\u2264\u03c3_k, \u03ba^\u03ba,\u00a01 \u2264\u03ba\u2264 k_Y_k for a sequence {\u03c3_k, \u03ba}_\u03ba\u2265 1;\n    \n   \n  UCB.A2(UCB2) For any k \u2208 [K]\n   \n    g_\u03ba,m(\u03c3_k,\u03ba) : =  1 - [ Y_k^2\u03ba/(2\u03ba - 1)!!]^-1/(2\u03ba)max_1 \u2264\u03ba\u2264\u03ba_Y_k[  -2m^-1/2\u03c3_k, \u03ba^\u03ba/( Y_k^2 \u03ba)+ Y_k^2\u03ba/(2\u03ba - 1)!!]^1/(2\u03ba)\n\n   and\n   \n    g_\u03ba,m(\u03c3_k, \u03ba):=[2m^-1/2\u03c3_k, \u03ba^\u03ba/( Y_k^2 \u03ba)+1]^1 /(2 \u03ba) - 1\n\n   are both less than m^-1/2 for sufficient large m.\n   \n    \n        \n            \n        \n        \n            \n        \n        \n\n\n\nDenote the population version of \n \n\n     \u03c6_G (_n) = \u221a(2 log(4 / \u03b1)/n) Y - \u03bc_b_k,G/1 - n^- 1 / 2   for   \u03c6_G( _n ) :=  Y - \u03bc_G\u221a(2 log (4 / \u03b1)/n) \n  with   Y = \u03bc.    \n  \n\nTheorem <ref> gives\n    \n    ( | Y - \u03bc | \u2265\u03c6_G(_n) ) \u2264 2 exp{- n^2 \u03c6_G^2(\ud835\udc18_n)/2 n Y - \u03bc_G^2} = \u03b1 / 2.\n\nby the fact that Y - \u03bc is symmetric around zero.\n\n\n    From Theorem <ref>, we take b \u2265 8 log (k_Y / \u03b1) and define the event \u2130_Y bellow associated with the MOM estimation of the intrinsic moment norm\n    \n    \u2130_Y:={ Y - \u03bc_b,G\u2265 [ 1 - max_1 \u2264 k \u2264 k_Y / 2g_k, m (\u03c3_k)]  Y - \u03bc_G}\n\n    with probability at least 1 - \u03b1 / 2. Note that\n    \n    ( | Y - \u03bc | \u2265\u03c6_G (_n) ) \u2264( | Y - \u03bc | \u2265\u03c6_G (_n),  \u2130_Y) +( \u2130_Y^c)\n       \u2264( | Y - \u03bc | \u2265\u221a(2 log (4 / \u03b1)/n)(1 - n^- 1 / 2)^-1 Y - \u03bc_b,G,  \u2130_Y ) + \u03b1 / 2\n       \u2264( | Y - \u03bc | \u2265 (1 - n^- 1 / 2)^-1 [ 1 - max_1 \u2264 k \u2264 k_Y / 2g_k, m (\u03c3_k)]\u221a(2 log (4/ \u03b1)/n) Y - \u03bc_G) + \u03b1 / 2\n       \u2264( | Y - \u03bc | \u2265\u03c6_G (_n)) + \u03b1 / 2 \u2264\u03b1.\n\n    where the last inequality is by taking m big enough such that max_1 \u2264 k \u2264 k_Y / 2g_k, m(\u03c3_k) \u2264 1 / \u221a(n). Then, for s \u2208\u2115_+, \n    \n    ( |Y_s - \u03bc_k | \u2264\u03c6_G (_s)) \u2265 1 - \u03b1.\n\n    \n    Now for any k \u2208 [K] and fixed T_k(t) = s, we know that\n    \n    ( Y_s - \u03bc_k \u2265\u03c6_G (_s) )\u2264( | Y_k - \u03bc_k | \u2265\u03c6_G (_n) ) \u2264\u03b1.\n\nBy the non-asymptotic second-order correction [see Theorem 2.2 in <cit.>] and the assumption that Y - \u03bc is symmetric around zero, one has\n\n    {\u03bc_k - Y_s \u2265  q_\u03b1 / 2  (_s - Y_s) +  \u221a(2 log(4 / \u03b1)/s)\u03c6_G(_s) }\u2264 2 \u03b1,\n\nwhere q_\u03b1 / 2 (_B_k - Y_B_k) : = q_\u03b1 / 2(\ud835\udc18_s -Y_s, 1/s1_s ).\n\nDenote the UCB index UCB_k (t) = Y_T_k(t) + h_\u03b1 (_T_k(t)), and the good event\n    \n    \u2130_k := {\u03bc_1 < min_t \u2208 [T]UCB_1 (t)} \u2229 {Y_B_k + q_\u03b1 / 2  (_s - Y_s) +  \u221a(2 log(4 / \u03b1) / s)\u00b7\u03c6_G(_s)  < \u03bc_1 },      k \u2208 [K],\n\n    where B_k \u2208 [T] is a constant to be chosen later. Following from the proof in (B.16)-(B.18) of <cit.>, we can gives that T_k(t) \u2264 B_k and\n    \n    T_k(t) \u2264 B_k + T [ 2 \u03b1 T + ( Y_B_k + q_\u03b1 / 2  (_B_k - Y_B_k) +  \u221a(2 log(4 / \u03b1) / B_k)\u00b7\u03c6_G(_B_k) \u2265\u03bc_1 ) ].\n\n    On the other hand, from Lemma <ref>, then\n    \n    (  q_\u03b1 / 2 (_B_k - Y_B_k) \u2265 C \u221a(2log (4 / \u03b1)/B_k)) \u2264\u03b1 / 2.\n\nNext, by applying MOM estimator, we need to the following assumptions for block { b_k}_k \u2208 [K] corresponding to Theorem <ref>. Here we include the subscript k to avoid confusion.\n    \nUnder \u03b7(\u03b5)=1 with \u03b5=0, Theorem <ref> ensures for b_k \u2265 8 log (k_Y / \u03b1) and take m large enough such that max_1 \u2264\u03ba\u2264 k_Yg_\u03ba, m (\u03c3_\u03ba) \u2264 n^-1 / 2, then\n    \n    ( (1 + n^- 1 /2) C \u2264Y - \u03bc_b,G) \n       \u2264( (1 + max_1 \u2264\u03ba\u2264 k_Y / 2g_\u03ba,m(\u03c3 _k, \u03ba))Y - \u03bc_G\u2264Y - \u03bc_b_k,G) \u2264 k_Ye^ - b_k/8\u2264\u03b1 / 2\n\nHence, we have\n\n    (  q_\u03b1 / 2 (_B_k - Y_B_k) + \u221a(2log (4 / \u03b1)/B_k)\u03c6_G (_B_k)\u2265  C[1 + \u221a(2 log (4 / \u03b1)/B_k)1 + B_k^- 1 / 2/1 - B_k^- 1 / 2] \u221a(2 log (4 / \u03b1)/B_k))\n    \u2264   (  q_\u03b1 / 2 (_B_k - Y_B_k) + 2log (4 / \u03b1)/B_k Y - \u03bc_b_k,G/1 - B_k^- 1 / 2\u2265   C \u221a(2 log (4 / \u03b1)/B_k) + 2log (4 / \u03b1)/B_k(1 + B_k^- 1 / 2)C/1 - B_k^- 1 / 2) \n    \u2264   (  q_\u03b1 / 2 (_B_k - Y_B_k) \u2265 C \u221a(2log (4 / \u03b1)/B_k)) + (   Y - \u03bc_b_k,G\u2265 (1 + B_k^- 1 / 2)C )\n\nwhich implies with probability at least 1 - \u03b1,\n\n    q_\u03b1 / 2  (_B_k - Y_B_k) + \u221a(2 log (4 / \u03b1)/B_k)\u03c6_G(_B_k)    \u2264 C[1 + \u221a(2 log (4 / \u03b1)/B_k)1 + B_k^-1 / 2/1 - B_k^-1 / 2] \u221a(2 log (4 / \u03b1)/B_k)\n       \u2264  2 ( 2 + \u221a(2)) C \u221a(2 log (4 / \u03b1)/B_k)\n\n\n    \n\nwhere B_k \u2265 8 log ( k_Y_k/ \u03b1) \u2228 2 log (4 / \u03b1) and max_1 \u2264\u03ba\u2264 k_Y_k / 2g_\u03ba, m (\u03c3_k, \u03ba)\u2264 1 / \u221a(B_k) for each arm k.\n    \n\n\n    Now, define the event \u212c_k := {q_\u03b1 / 2  (_B_k - Y_B_k) +  \u221a(2 log(4 / \u03b1) / B_k)\u00b7\u03c6_G(_B_k) \u2264\u0394_k / 2 } with \u0394_k := \u03bc_1 - \u03bc_k. Choose B_k as\n    \n    B_k = 4^2 (2 + \u221a(2))^2 C^2/\u0394_k^2log (4 / \u03b1)\u2265 2,\n\n    we have\n\n    \n        \n        \n    \n\n    \n    ( \u212c_k^c )     = ( q_\u03b1 / 2  (_B_k - Y_B_k) +  \u221a(2 log(4 / \u03b1) / B_k)\u00b7\u03c6_G(_B_k) > \u0394_k / 2 ) \n       \u2264( 2 (2 + \u221a(2)) C \u221a(log (4 / \u03b1)/B_k) > \u0394_k / 2 ) + \u03b1 = 0 + \u03b1 = \u03b1.\n\n    Applying Theorem <ref> for concentration of Y_B_k - \u03bc_k  when B_k is chosen as in (<ref>),\n    \n    ( Y_B_k + q_\u03b1 / 2  (_B_k - Y_B_k) +  \u221a(2 log(4 / \u03b1) / B_k)\u00b7\u03c6_G(_B_k) \u2265\u03bc_1 )    \u2264( Y_B_k - \u03bc_k \u2265\u0394_k / 2) + ( \u212c_k^c) \n       \u2264 2 exp{ - (B_k \u0394_k / 2)^2/2 \u00b7 B_k C^2} + \u03b1\n        = 2 exp{ - B_k \u0394_k^2/8 C^2} + \u03b1.\n\nTaking account these results into (<ref>), we get that\n\n    T_k(t)    \u2264 B_k + 2 \u03b1 T^2 + \u03b1 T + 2 T exp{ - B_k \u0394_k^2/8 C^2}\n        = 4^2 (2 + \u221a(2))^2 C^2/\u0394_k^2log (4 / \u03b1) + 2 \u03b1 T^2 + \u03b1 T + 2 T exp{ - 2 (2 + \u221a(2))^2 log (4 / \u03b1)}\n        = 16 (2 + \u221a(2))^2 C^2/\u0394_k^2log T + 4 /T + 2/T^25 + 16 \u221a(2) + 8\n\n    by taking \u03b1 = 4 / T^2. Under the problem-dependent case, the regret is bounded by\n    \n    Reg_T = \u2211_k = 2^K \u0394_k  T_k(t) \u2264 16 (2 + \u221a(2))^2 C^2 log T \u2211_k = 2^K \u0394_k^-1 + ( 4/T + 2/T^25 + 16 \u221a(2) + 8 ) \u2211_k = 2^K \u0394_k.\n\n    \n    To get the problem-independent bound, we let \u0394 > 0 as an arbitrary threshold, then decompose Reg_T, we get\n    \n    Reg_T     = \u2211_\u0394_k : \u0394_k < \u0394\u0394_k  T_k(t) + \u2211_\u0394_k : \u0394_k \u2265\u0394\u0394_k  T_k(t) \n       \u2264 T \u0394 +  16 (2 + \u221a(2))^2 C^2 log T \u2211_\u0394_k : \u0394_k \u2265\u0394\u0394_k^-1 + ( 4/T + 2/T^25 + 16 \u221a(2) + 8 ) \u2211_\u0394_k : \u0394_k \u2265\u0394\u0394_k \n       \u2264 T \u0394 + 16 (2 + \u221a(2))^2 C^2 K log T/\u0394 + ( 4/T + 2/T^25 + 16 \u221a(2) + 8 ) K \u03bc_1^* \n        = 8 (\u221a(2) + 2) C \u221a(T K log T) + ( 4/T + 2/T^25 + 16 \u221a(2) + 8 ) K \u03bc_1^*,\n\n    by taking \u0394 = 8 (2 + \u221a(2)) C \u221a((K log T)/ T). And finally, we take C = max_k \u2208 [K]Y_k - \u03bc_k _G.\n\n\n\n"}