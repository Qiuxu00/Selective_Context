{"entry_id": "http://arxiv.org/abs/2303.06920v1", "published": "20230313083759", "title": "Pixel-wise Gradient Uncertainty for Convolutional Neural Networks applied to Out-of-Distribution Segmentation", "authors": ["Kira Maag", "Tobias Riedlinger"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n[\n    Jonathan Wagner, Reshef Meir\n    March 30, 2023\n================================\n\n\n\n* equal contribution\n\n\nIn recent years, deep neural networks have defined the state-of-the-art in semantic segmentation where their predictions are constrained to a predefined set of semantic classes. They are to be deployed in applications such as automated driving, although their categorically confined expressive power runs contrary to such open world scenarios. Thus, the detection and segmentation of objects from outside their predefined semantic space, i.e., out-of-distribution (OoD) objects, is of highest interest. Since uncertainty estimation methods like softmax entropy or Bayesian models are sensitive to erroneous predictions, these methods are a natural baseline for OoD detection. Here, we present a method for obtaining uncertainty scores from pixel-wise loss gradients which can be computed efficiently during inference. Our approach is simple to implement for a large class of models, does not require any additional training or auxiliary data and can be readily used on pre-trained segmentation models. Our experiments show the ability of our method to identify wrong pixel classifications and to estimate prediction quality. In particular, we observe superior performance in terms of OoD segmentation to comparable baselines on the SegmentMeIfYouCan benchmark, clearly outperforming methods which are similarly flexible to implement.\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nSemantic segmentation decomposes the pixels of an input image into segments which are assigned to a fixed and predefined set of semantic classes. \nIn recent years, deep neural networks (DNNs) have performed excellently in this task <cit.>, providing comprehensive and precise information about the given scene. \nHowever, in safety relevant applications like automated driving where semantic segmentation is used in open world scenarios, DNNs often fail to function properly on unseen objects for which the network has not been trained, see for example the bobby car in <ref> (top). \nThese objects from outside the network's semantic space are called of out-of-distribution (OoD) objects. \n\nIt is of highest interest that the DNN identifies these objects and abstains from deciding on the semantic class for those pixels covered by the OoD object. \nAnother case are OoD objects which might belong to a known class, however, appearing differently to substantial significance from other objects of the same class seen during training. \nConsequently, the respective predictions are prone to error. \nFor these objects, as for classical OoD objects, marking them as OoD is preferable to the likely case of misclassification which may happen with high confidence.\nFurthermore, this additional classification task should not substantially degrade the semantic segmentation performance itself outside the OoD region. \nThe computer vision tasks of identifying and segmenting those objects is captured by the notion of OoD segmentation <cit.>. \n\nThe recent contributions to the emerging field of OoD segmentation are mostly focused on OoD training, i.e., the incorporation of additional training data (not necessarily from the real world), sometimes obtained by large reconstruction models <cit.>. \nAnother line of research is the use of uncertainty quantification methods such as Bayesian models <cit.> or maximum softmax probability <cit.>. \nGradient-based uncertainties are considered for OoD detection in the classification task by <cit.>, <cit.> and <cit.> and up to now, have not been applied to OoD segmentation. \n<cit.> show that gradient norms perform well in discriminating between in- and out-of-distribution. \nMoreover, gradient-based features are studied by <cit.> for object detection to estimate the prediction quality.\n<cit.> investigate loss gradients w.r.t. feature activations in monocular depth estimation and show correlations of gradient magnitude with depth estimation accuracy.\n\nIn this work, we introduce a new method for uncertainty quantification in semantic segmentation and OoD segmentation based on gradient information. \nMagnitude features of gradients can be computed at inference time and provide information about the uncertainty propagated in the corresponding forward pass. \nThese features represent pixel-wise uncertainty scores applicable to prediction quality estimation and OoD segmentation. \nAn exemplary gradient uncertainty heatmap can be found in <ref> (bottom). \nSuch uncertainty scores have shown improved performance for the quality estimation task in image classification compared to uncertainties contained in the softmax output of DNNs <cit.>. \nIn addition, instance-wise gradient uncertainty outperforms sampling methods like Monte-Carlo (MC) Dropout <cit.> and Ensembles <cit.> in object detection <cit.>. \nCalculating gradient uncertainty scores does not require any re-training of the DNN or computationally expensive sampling. \nInstead, only one backpropagation step for the gradients with respect to the final convolutional network layer is performed per inference to produce gradient scores. \nNote, that more than one backpropagation step can be performed if deeper gradients need to be computed and other parameters of the model are considered. \nAn overview of our approach is shown in <ref>. \n\nAn application to dense predictions such as semantic segmentation has escaped previous research.\nSingle backpropagation steps per pixel on high-resolution input images quickly become infeasible given that 10^6 gradients have to be calculated. \nTo overcome this issue, we present a new approach to exactly compute the pixel-wise gradient scores in a batched and parallel manner applicable to a large class of segmentation architectures. \nWe use these gradient scores to estimate the model uncertainty on pixel-level and also the prediction quality on segment-level. \nSegments are connected components of pixels belonging to the same class predicted by the semantic segmentation network. \nFinally, the gradient uncertainty heatmaps are investigated for OoD segmentation where high uncertainties indicate possible OoD objects.\n\nWe only assume a pre-trained semantic segmentation network as our method is applicable to a wide range of architectures. \nIn our tests, we employ a state-of-the-art segmentation network <cit.> trained on Cityscapes <cit.> evaluating in-distribution uncertainty estimation and on four OoD segmentation datasets, namely LostAndFound <cit.>, Fishyscapes <cit.>, RoadAnomaly21 and RoadObstacle21 <cit.>, demonstrating OoD detection performance.\nThe source code of our method is publicly available at <https://github.com/tobiasriedlinger/uncertainty-gradients-seg>. \nOur contributions are summarized as follows:\n\n    \n  * We introduce a new method for uncertainty quantification in semantic segmentation. \n    Our gradient-based approach is applicable to a wide range of segmentation architectures.\n    \n  * For the first time, we show an efficient way of computing gradients in semantic segmentation on the pixel-level in a parallel manner making our method less computationally expensive than sampling-based methods.\n    \n  * For the first time, we demonstrate the robustness of gradient-based uncertainty quantification with respect to predictive error detection and OoD segmentation. \n    For the OoD segmentation task, we achieve area under precision-recall curve values of up to 69.3% on the LostAndFound benchmark outperforming a variety of methods that have substantially larger requirements. \n\nThe paper is structured as follows. We discuss the related work on uncertainty quantification and OoD segmentation in <ref>. \nIn <ref>, we introduce our method of computing pixel-wise gradient uncertainty scores. \nNumerical results are shown in <ref>, followed by concluding remarks in <ref>.\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n\n\n \u00a7.\u00a7 Uncertainty Quantification\n\nBayesian approaches <cit.> are widely used to estimate model uncertainty. \nThe well-known approximation, MC Dropout <cit.>, has proven to be practically efficient in uncertainty estimation and is also applied to semantic segmentation <cit.>. \nIn addition, this method is considered to filter out predictions with low reliability <cit.>. \n<cit.> benchmarked pixel-wise uncertainty estimation methods based on Bayesian models or the network's softmax output. \n<cit.> extracted uncertainty information on pixel-level by using the maximum softmax probability and MC Dropout.\nPrediction quality evaluation approaches are introduced by <cit.> and <cit.> working on single objects per image.\nThese methods are based on additional CNNs acting as post-processing mechanism.\n<cit.> present the concepts of meta classification (false positive detection) and meta regression (performance estimation) on segment-level using features as input extracted from the segmentation network's softmax output. \nThis line of research has been extended by a temporal component <cit.> and transferred to object detection <cit.> as well as to instance segmentation <cit.>.\n\nWhile MC Dropout as a sampling approach is computationally expensive to create pixel-wise uncertainties, our method computes only the gradients of the last layer during a single inference run and can be applied to a wide range of semantic segmentation networks without architectural changes.\nCompared with the work by <cit.>, our gradient information can extend the features extracted from the segmentation network's softmax output to enhance the segment-wise quality estimation.\n\n\n\n\n\n \u00a7.\u00a7 OoD Segmentation\n\nUncertainty quantification methods demonstrate high uncertainty for erroneous predictions, so they can intuitively serve for OoD detection as well. \nFor instance, this can be accomplished via maximum softmax (probability) <cit.>, MC Dropout <cit.> or ensembles <cit.> which capture model uncertainty by averaging predictions over multiple sets of parameters. \nAnother line of research is OoD detection training, relying on the exploitation of additional training data, not necessarily from the real world, but disjoint from the original training data <cit.>. \nIn this regard, an external reconstruction model followed by a discrepancy network are considered by <cit.>, <cit.>, <cit.> and <cit.> and normalizing flows are leveraged by <cit.> and <cit.>. \n<cit.> as well as <cit.> perform small adversarial perturbations on the input images to improve the separation of in- and out-of-distribution samples.\n\nSpecialized training approaches for OoD detection are based on different kinds of re-training with additional data and often require generative models.\nMeanwhile, our method does not require OoD data, re-training or complex auxiliary models. \nMoreover, we do not run a full backward pass as is required for the computation of adversarial samples.\nIn fact we found that it is often sufficient to only compute the gradients of the last convolutional layer. \nOur method is more related to classical uncertainty quantification approaches like maximum softmax, MC Dropout and ensembles. \nNote that the latter two are based on sampling and thus, much more computationally expensive compared to single inference. \n\n\n\n\n\n\n\u00a7 METHOD DESCRIPTION\n\nIn the following, we consider a neural network with parameters \u03b8 yielding classification probabilities \n\n    \u03c0\u0302(x, \u03b8) = (\u03c0\u0302_1, \u2026, \u03c0\u0302_C)\n\n over C semantic categories when presented with input x.\nDuring training on paired data (x, y) where y \u2208{1, \u2026, C} is the semantic label given to x, such a model is commonly trained by minimizing some kind of loss function \u2112 between y and the predicted probability distribution \u03c0\u0302(x, \u03b8) using gradient descent on \u03b8.\nThe gradient step \u2207_\u03b8\u2112(\u03c0\u0302(x, \u03b8)  y) then indicates the direction and strength of training feedback obtained by (x, y).\nAsymptotically (in the amount of data and training steps taken), the expectation \n\n    \ud835\udd3c_(X, Y) \u223c P [\u2207_\u03b8\u2112(\u03c0\u0302(X, \u03b8)  Y)]\n\nof the gradient w.r.t. the data generating distribution P will vanish since \u03b8 sits at a local minimum of \u2112.\nWe assume, that strong models which achieve high test accuracy can be seen as an approximation of such a parameter configuration \u03b8.\nSuch a model has small gradients on in-distribution data which is close to samples (x, y) in the training data.\nSamples that differ from training data will receive larger feedback.\nLike-wise, we assume large training gradients from OoD samples that are far away from the effective support of P.\n\nIn order to quantify uncertainty about the prediction \u03c0\u0302(x, \u03b8), we replace the label y from above by some auxiliary label for which we make two concrete choices in our method.\nOn the one hand, we replace y by the class prediction one-hot vector y^\ud835\udc5c\u210e_k = \u03b4_k \u0109 with \u0109 = arg max_k = 1, \u2026, C \u03c0\u0302_k and the Kronecker symbol \u03b4_ij.\nThis quantity correlates strongly with training labels on in-distribution data.\nOn the other hand, we regard a uniform, all-warm label y^\ud835\udc62\ud835\udc5b\ud835\udc56_k = 1/C for k = 1, \u2026, C as a replacement for y.\nTo motivate the latter choice, we consider that classification models are oftentimes trained on the categorical cross entropy loss \n\n    \u2112(\u03c0\u0302 y) = - \u2211_k = 1^C y_k log(\u03c0\u0302_k).\n\nSince the gradient of this loss function is linear in the label y, a uniform choice will return the average gradient per class which should be high on OoD data where all possible labels are similarly unlikely.\nThe magnitude of \u2207_\u03b8\u2112(\u03c0\u0302 y) serves as uncertainty / OoD score.\nIn the following, we explain how to compute such scores for pixel-wise classification models.\n\n\n\n\n\n\n \u00a7.\u00a7 Efficient Computation in Semantic Segmentation\n\n\nWe regard a generic segmentation architecture utilizing a final convolution as the pixel-wise classification mechanism.\nIn the following, we also assume that the segmentation model is trained via the commonly-used pixel-wise cross entropy loss \u2112_ab(\u03d5(\u03b8) | y) (cf. eq.\u00a0(<ref>)) over each pixel (a,b) with feature map activations \u03d5(\u03b8).\nPixel-wise probabilities are obtained by applying the softmax function  to each output pixel, i.e., \u03c0\u0302_ab, k = ^k(\u03d5_ab(\u03b8)).\nWith eq.\u00a0(<ref>), we find for the loss gradient \n\n    \u2207_\u03b8\u2112_ab((\u03d5(\u03b8))  y) \n        = \u2211_k = 1^C ^k(\u03d5_ab) (1 - y_k) \u00b7\u2207_\u03b8\u03d5^k_ab(\u03b8).\n\nHere, \u03b8 is any set of parameters within the neural network.\nDetailed derivations of this and the following formulas of this section are provided in <ref>. \nHere, \u03d5 is the convolution result of a pre-convolution feature map \u03c8 (see <ref>) against a filter bank K which assumes the role of \u03b8.\nK has parameters (K_e^h)_fg where e and h indicate in- and out-going channels respectively and f and g index the spatial filter position.\nThe features \u03d5 are linear in both, K and \u03c8, and depend on bias parameters \u03b2 in the form\n\n    \u03d5_ab^d\n        = \n        (K \u2217\u03c8)_ab^d + \u03b2^d\n        = \n        \u2211_j = 1^\u03ba\u2211_p,q=-s^s (K_j^d)_pq\u03c8^j_a + p, b + q + \u03b2^d.\n\nWe denote by \u03ba the total number of in-going channels and s is the spatial extent to either side of the filter K_j^d which has total size (2s+1) \u00d7 (2s+1).\nExplicitly for the last layer gradients we find\n\n    \u2202\u03d5_ab^d/\u2202 (K_e^h)_fg = \u03b4_dh\u03c8_a+f, b+g^e.\n\nTogether with eq.\u00a0(<ref>), we obtain an explicit formula for computing the correct backpropagation gradients of the loss on pixel-level for our choices of auxiliary labels which we state in the following paragraph.\nThe computation of the loss gradient can be traced in <ref>.\n\nIf we take the predicted class \u0109 as a one-hot label per pixel as auxiliary labels, i.e., y^\ud835\udc5c\u210e, we obtain for the last layer gradients\n\n    \u2202\u2112_ab((\u03d5)  y^\ud835\udc5c\u210e)/\u2202 K_e^h = ^h(\u03d5_ab) \u00b7 (1 - \u03b4_h \u0109) \u00b7\u03c8_ab^e\n\nwhich depends on quantities that are easily accessible during a forward pass through the network.\nNote, that the filter bank K for (1 \u00d7 1)-convolutions does not require spatial indices which is a common situation in segmentation networks.\nSimilarly, we find for the uniform label y^\ud835\udc62\ud835\udc5b\ud835\udc56_j = 1 / C\n\n    \u2202\u2112_ab((\u03d5)  y^\ud835\udc62\ud835\udc5b\ud835\udc56)/\u2202 K_e^h\n        =\n        C - 1/C^h(\u03d5_ab) \u03c8_ab^e.\n\nTherefore, pixel-wise gradient norms are simple to implement and particularly efficient to compute for the last layer of the segmentation model.\nIn the <ref>, we cover formulas for the more general case of deeper gradients as well. \n\n\n\n\n\n \u00a7.\u00a7 Uncertainty Scores\n\nWe obtain pixel-wise scores (still depending on a and b) by computing the partial norm \u2207_ K\u2112_ab_p of this tensor over the indices e and h for some fixed value of p.\nThis can again be done in a memory efficient way by the natural decomposition \u2202\u2112 / \u2202 K_e^h = S^h \u00b7\u03c8^e.\nIn addition to their use in error detection, these scores can be used in order to detect OoD objects in the input, i.e., instances of categories not present in the training distribution of the segmentation network.\nWe identify OoD regions with pixels that have a gradient score higher than some threshold and find connected components like the one shown in <ref> (bottom).\nDifferent values of p are studied in the supplementary material.\nWe also consider values 0 < p < 1 in addition to positive integer values.\nNote, that such choices do not strictly define the geometry of a vector space norm, however, \u00b7_p may still serve as a notion of magnitude and generates a partial ordering.\nThe tensorized multiplications required in eqs.\u00a0(<ref>) and (<ref>) are far less computationally expensive than a forward pass through the DNN.\nThis means that computationally, this method is preferable over prediction sampling like MC Dropout or ensembles.\nWe abbreviate our method using the pixel-wise gradient norms obtained from eqs.\u00a0(<ref>) and (<ref>) by PGN_\ud835\udc5c\u210e and PGN_\ud835\udc62\ud835\udc5b\ud835\udc56, respectively.\nThe particular value of p is denoted by superscript, e.g., PGN_\ud835\udc5c\u210e^p=0.3 for the (p=0.3)-seminorm of gradients obtained from y^\ud835\udc5c\u210e. \n\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\nIn this section, we present the experimental setting first and then evaluate the uncertainty estimation quality of our method on pixel and segment level. \nLast, we apply our gradient-based method to OoD segmentation and show some visual results.\n\n\n\n\n\n \u00a7.\u00a7 Experimental Setting\n\n\n\n\n\n\n  \nDatasets\nWe perform our tests on the Cityscapes <cit.> dataset for semantic segmentation in street scenes and on four OoD segmentation datasets, i.e., LostAndFound <cit.>, Fishyscapes <cit.>, RoadAnomaly21 and RoadObstacle21 <cit.>^1^1 Benchmark: (<http://segmentmeifyoucan.com/>).\nThe Cityscapes dataset consists of 2,975 training and 500 validation images of dense urban traffic in 18 and 3 different German towns, respectively.\nThe LostAndFound dataset contains 1,203 validation images with annotations for the road surface and the OoD objects, i.e., small obstacles on German roads in front of the ego-car. \nIn <cit.> this dataset is filtered (LostAndFound test-NoKnown) as children and bicycles are considered as OoD objects, although they are included in the Cityscapes training set.\nThe Fishyscapes LostAndFound dataset is based on the LostAndFound dataset and refines the pixel-wise annotations, i.e., it is distinguished between OoD objects, background (Cityscapes classes) and void (anything else). \nMoreover frames that do not contain OoD objects as well as sequences with children and bikes are removed resulting in 100 validation images (and 275 non-public test images).\nThe RoadObstacle21 dataset (412 test images) is comparable to the LostAndFound dataset as all obstacles appear on the road. \nHowever, the RoadObstacle21 dataset contains more diversity in the OoD objects as well as in the situations (night, snowy conditions and dirty roads) compared to the LostAndFound dataset where all sequences are recorded under good weather conditions.\nIn the RoadAnomaly21 dataset the objects (anomalies) appear anywhere on the image which makes it comparable to the Fishyscapes LostAndFound dataset. \nWhile the latter dataset consists of frames extracted from a small number of different scenes, every image of the RoadAnomaly21 dataset (100 test images) shows a unique scene and a wider variety of anomalous objects.\n\n\n\n\n\n  \nSegmentation Networks\nWe consider a state-of-the-art DeepLabv3+ network <cit.> with two different backbones, WideResNet38 <cit.> and SEResNeXt50 <cit.>. \nThe network with each respective backbone is trained on Cityscapes achieving a mean  value of 90.58% for the WideResNet38 backbone and 80.76% for the SEResNeXt50 on the Cityscapes validation set. \nWe use one and the same model trained exclusively on the Cityscapes dataset for both tasks, the uncertainty estimation and the OoD segmentation, as our method does not need to be re-trained on OoD data.\nTherefore, our method leaves the entire segmentation performance of the base model completely intact.\n\n\n\n\n\n \u00a7.\u00a7 Numerical Results\n\nIn our experiments, we apply different p-norms, p \u2208{ 0.1, 0.3, 0.5, 1, 2 }, to our calculated gradients, see <ref>. \nAs we provide only a selection of results in the following, we refer to <ref> for an ablation study on different values for p and to <ref> for further results where the gradients of deeper layers are computed.\n\n\n\n\n\n  \nPixel-wise Uncertainty Evaluation\n\n\n\nIn order to assess the correlation of uncertainty and prediction errors on the pixel level, we resort to the commonly used sparsification graphs <cit.>.\nSparsification graphs normalized w.r.t. the optimal oracle (sparsification error) can be compared in terms of the so-called area under the sparsification error curve ().\nThe closer the uncertainty estimation is to the oracle in terms of Brier score evaluation, i.e., the smaller the , the better the uncertainty eliminates false predictions by the model.\n\nThe  metric is capable of grading the uncertainty ranking, however, does not address the statistics in terms of given confidence.\nTherefore, we resort to an evaluation of calibration in terms of expected calibration error (, <cit.>) to assess the statistical reliability of the uncertainty measure.\n\nAs baseline methods we consider quantities that can be obtained without modifying the segmentation model (like MC Dropout) or retraining (like deep ensembles) and which do not need additional data (like re-calibration confidences).\nOur choices fall on the uncertainty ranking provided by the maximum softmax probabilities native to the segmentation model as well as the softmax entropy.\nAn evaluation of calibration errors requires normalized scores, so we will normalize our gradient scores according to the highest value obtained on the test data for the computation of .\n\nThe resulting metrics are compiled in <ref> for both architectures evaluated on the Cityscapes val split.\nWe see that both, the uncertainty ranking and the calibration of  are roughly on par with the stronger maximum softmax baseline.\nGenerally, we see the trend that  has higher calibration error by one order of magnitude and higher sparsification errors of up to 5.2 percentage points (pp) which may indicate that this kind of score is less indicative of in-distribution errors.\nNote, that although there is no immediate reason for  to be calibrated in any way, we find calibration errors that are competitive on pixel-level with those of the maximum softmax.\n\n\n\n\n\n  \nSegment-wise Prediction Quality Estimation\n\n\n\n\n\n\n\n\n\n\n\n\nTo reduce the number of false positive predictions and to estimate the prediction quality, we use meta classification and meta regression introduced by <cit.>. \nAs input for the post-processing models, the authors use information from the network's softmax output which characterize uncertainty and geometry of a given segment like the segment size. \nThe degree of randomness in semantic segmentation prediction is quantified by pixel-wise quantities, like entropy and probability margin. \nTo obtain segment-wise features from these quantities, they are aggregated over segments via average pooling. \nThese features used in <cit.> serve as a baseline in our tests (called MetaSeg).\n\nSimilarly, we construct segment-wise features from our pixel-wise gradient p-norms for p \u2208{ 0.1, 0.3, 0.5, 1, 2 }. \nWe compute the mean and the variance of these pixel-wise values over a given segment. \nThese features per segment are considered also for the inner and the boundary since the gradient scores may be higher on the boundary of a segment, see <ref> (bottom). \nThe inner of the segment consists of all pixels whose eight neighboring pixels are also elements of the same segment. \nFurthermore, we define some relative mean and variance features over the inner and boundary which characterizes the degree of fractality.\nThese hand-crafted quantities form a structured dataset where the columns correspond to features and the rows to predicted segments which serve as input to the post-processing models. \nDetails on the construction of these features can be found in <ref>.\n\nWe determine the prediction accuracy of semantic segmentation networks with respect to the ground truth via the segment-wise intersection over union (, <cit.>). \nOn the one hand, we perform the direct prediction of the  (meta regression) which serves as prediction quality estimate. \nOn the other hand, we discriminate between = 0 (false positive) and > 0 (true positive) (meta classification). \nWe use linear classification and regression models.\n\nFor the evaluation, we use  (area under the receiver operating characteristic) for meta classification and the determination coefficient R^2 for meta regression. \nAs baselines, we employ the maximum softmax probability <cit.>, the entropy and the MetaSeg framework.\nA comparison of these methods and our approach for the Cityscapes dataset is given in <ref>.\nFor the evaluation, we separate the gradient features obtained by one-hot () and by uniform () labels.\nAlso, we consider using all gradient features (PGN) as input to the meta models. \nWith PGN, we outperform the maximum softmax and the entropy baselines as well as the MetaSeg performance with the only exception of meta classification for the SEResNeXt backbone where MetaSeg achieves a marginal 0.41 pp higher  value than ours.\nMoreover, we enhance the MetaSeg performance for both networks and both tasks combining the MetaSeg features with PGN by up to 1.02 pp for meta classification and up to 3.98 pp for meta regression.\nThis indicates that gradient features contain information which is orthogonal to the information contained in the softmax output.\nIn particular, the highest  value of 93.31% achieved for the WideResNet backbone, shows the capability of our approach to estimate the prediction quality and filter out false positive predictions on the segment-level.\n\n\n\n\n\n  \nOoD Segmentation\nOur results in OoD segmentation are based on the evaluation protocol of the SegmentMeIfYouCan benchmark<cit.>.\nEvaluation on the pixel-level involves the threshold-independent area under precision-recall curve (AuPRC) and the false positive rate at the point of 0.95 true positive rate (FPR_95).\nThe latter constitutes an interpretable choice of operating point for each method where a minimum true positive fraction is dictated.\nOn segment-level, an adjusted version of the mean intersection over union (sIoU) which represents the accuracy of the segmentation obtained by thresholding at a particular point, positive predictive value (PPV or Precision) playing the role of binary instance-wise accuracy and the F_1-score.\nThe latter segment-wise metrics are averaged over thresholds between 0.25 and 0.75 with a step size of 0.05 leading to the quantities sIoU, PPV and F_1.\n\nIn contrast to the previous evaluations on predictive errors, we do not report here results for both of our models, rather we compare gradient scores as an OoD score as such against other methods on the benchmark and report the best result obtained for both,  and .\nAs baselines, we include the same methods as for error detection before since these constitute a reasonable comparison in addition to some of the well-established methods of estimating epistemic uncertainty like MC Dropout and deep ensemble.\nNote, that the standard entropy baseline is not featured in the official leaderboard, so we report our own results obtained by the softmax entropy with the WideResNet backbone which performed better.\nWe include a full table of methods in <ref> and find that our method is in several cases competitive with some of the stronger methods utilizing adversarial examples, auxiliary data or significant alterations of the model architecture.\n\nThe results verified by the official benchmark are compiled in <ref> and <ref>.\nAcross the board, PGN shows strong performance, almost exclusively delivering the best or second-best results with few exceptions.\nThe only prominent exception is in the segment-based PPV metric on LostAndFound test-NoKnown where both,  and  come in behind the maximum softmax and entropy baseline.\nMeanwhile, the segmentation accuracy in terms of sIoU, as well as the F_1 score which is the harmonic mean of Recall and Precision are far superior to these two baselines.\nThis indicates still better OoD segmentation quality and in particular better Recall achieved by PGN.\nGradient scores perform perhaps the least convincingly on the RoadObstacle21 benchmark, where in three cases only the second-best performance is achieved.\nOverall however, we conclude strong performance of our method in terms of out-of-distribution segmentation across different datasets.\nWe find a slight trend of  performing better in the Obstacle track which can be seen to be closer to in-distribution data in semantic segmentation for autonomous driving.\nThis connection would be consistent with our finding in actual in-distribution errors, while  performs better on the Anomaly track which is more clearly out-of-distribution for street scene recognition.\nIn several cases, our method even beats some of the stronger OoD segmentation methods utilizing, for example on RoadAnomaly21, adversarial samples (ODIN/Mahalanobis by over 9 pp in AuPRC), OoD data (Void Classifier by over 6 pp AuPRC) or generative models (JSRNet/Embedding Density by over 5 pp AuPRC and Image Resynthesis by over 10 pp in PPV).\n\n<ref> shows segmentation predictions of the pre-trained DeepLabv3+ network with the WideResNet38 backbone together with its corresponding ^p=0.5-heatmap.\nThe two panels on the left show an in-distribution prediction on Cityscapes where uncertainty is mainly concentrated around segmentation boundaries which are always subject to high prediction uncertainty.\nMoreover, we see some false predictions in the far distance around the street crossing which can be found as a region of high gradient norm in the heatmap.\nIn the two panels to the right, we see an OoD prediction from the RoadAnomaly21 dataset of a sloth crossing the street which is classified as part vegetation, terrain and person.\nThe outline of the segmented sloth can be seen brightly in the gradient norm heatmap to the right indicating clear separation.\n\n\n\n\n\n  \nLimitations\nWhile our method performs well in terms of segment-wise error detection and raises accuracy compared with previous methods, we declare that our method is merely on-par with other uncertainty quantification methods for pixel-level considerations.\nMoreover, there have been submissions to the SegmentMeIfYouCan benchmark which significantly outperform our method, however, due to the simplicity and flexibility of our method this comes at no surprise.\nNote, that there are some light architectural restrictions when it comes to our method in that it is required for the final layer to be a convolution. \n\n\n\n\n\n\n\u00a7 CONCLUSION\n\nIn this work, we presented an efficient method of computing gradient uncertainty scores for a wide class of deep semantic segmentation models.\nMoreover, we appreciate a low computational cost associated with them.\nOur experiments show that large gradient norms obtained by our method statistically correspond to erroneous predictions already on the pixel-level and can be normalized such that they yield similarly calibrated confidence measures as the maximum softmax score of the model.\nOn a segment-level our method shows considerable improvement in terms of error detection.\nGradient scores can be utilized to segment out-of-distribution objects significantly better than any other softmax- or output-based method on the SegmentMeIfYouCan benchmark and has competitive results with a variety of methods, in several cases clearly outperforming all of them.\nWe hope that our contributions in this work and the insights into the computation of pixel-wise gradient feedback for segmentation models will inspire future work in uncertainty quantification and pixel-wise loss analysis.\n\n\n\n\n \nWe thank Hanno Gottschalk and Matthias Rottmann for discussion and useful advice.\nThis work is supported by the Ministry of Culture and Science of the German state of North Rhine-Westphalia as part of the KI-Starter research funding program.\nThe research leading to these results is funded by the German Federal Ministry for Economic Affairs and Climate Action within the project \u201cKI Delta Learning\u201c (grant no. 19A19013Q). The authors would like to thank the consortium for the successful cooperation.\nThe authors gratefully acknowledge the Gauss Centre for Supercomputing e.V. <www.gauss-centre.eu> for funding this project by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS at J\u00fclich Supercomputing Centre (JSC).\n\n\n\n\n\n\n\n\u00a7 APPENDIX\n\n\n\n\n\n\n\u00a7 COMPUTATIONAL DERIVATIONS AND IMPLEMENTATION DETAILS\n\n\n\n\n \u00a7.\u00a7 Computations\n\nIn this work we assume that the final activation in the segmentation network is given by a softmax activation of categorical features \u03d5 in the last layer\n\n    ^j (\u03d5) = \u03d5^j/\u2211_i = 1^C \u03d5^i\n\nwhere C is the number of classes and \u03d5 = \u03d5(\u03b8) is now dependent on a set of parameters \u03b8.\nIn order to compute the gradients of the categorical cross entropy at pixel (a,b) given any auxiliary label y\n\n    \u2112_ab(\u03d5(\u03b8)  y) = - \u2211_j = 1^C y_ab, jlog(^j(\u03d5_ab(\u03b8))),\n\nwe require the derivative of the softmax function\n\n    \u2207_\u03b8^j(\u03d5(\u03b8)) \n            =   \u03d5^j\u2207_\u03b8\u03d5^j/\u2211_i = 1^C \u03d5^i - \u03d5^j\u2211_k = 1^C \u03d5^k\u2207_\u03b8\u03d5^k/( \u2211_i = 1^C \u03d5^i)^2\n    \n            =   ^j(\u03d5) \u00b7\u2207_\u03b8\u03d5^j \u2211_k = 1^C ^k(\u03d5) \n        - ^j(\u03d5) \u2211_k = 1^C ^k(\u03d5) \u00b7\u2207_\u03b8\u03d5^k \n    \n            =   ^j(\u03d5) ( \u2211_k = 1^C ^k(\u03d5) \u00b7\u2207_\u03b8\u03d5^j - ^k \u2207_\u03b8\u03d5^k) \n    \n            =   ^j(\u03d5) \u2211_k = 1^C ^k(\u03d5) (\u03b4_kj - 1) \u2207_\u03b8\u03d5^k.\n\nNote, that the auxiliary label y \u2208 [0, 1]^C may be anything, e.g., actual ground truth information about pixel (a,b), the predicted probability distribution at that location, the one-hot encoded prediction or a uniform class distribution.\nIn the following, we regard y to be independent of \u03d5.\nThe gradient of the cross entropy loss is\n\n    \u2207_\u03b8\u2112_ab(\u03d5(\u03b8)  y)\n            =    \n            - \u2211_j = 1^C y_ab,j1/^j(\u03d5_ab)\u2207_\u03b8^j(\u03d5_ab) \n    \n            =   \u2211_j = 1^C \u2211_k = 1^C (y_ab,j^k(\u03d5_ab) - y_j ^k(\u03d5_ab) \u03b4_kj) \n       \u00b7\u2207_\u03b8\u03d5^k_ab(\u03b8) \n            \n    \n            =   \u2211_k = 1^C ^k(\u03d5_ab) (1 - y_k) \u00b7\u2207_\u03b8\u03d5^k_ab(\u03b8)\n\nThe feature maps are the result of convolution \ud835\udc9e_K, \u03b2 against a filter bank K \u2208^\u03ba_in\u00d7\u03ba_out\u00d7 (2 s + 1) \u00d7 (2 s + 1) and addition of a bias vector \u03b2\u2208^\u03ba_out.\nHere, \u03ba_in and \u03ba_out denote the number of in- and out-going channels, respectively.\nK has parameters (K_e^h)_fg where e and h indicate in- and out-going channels respectively and f and g index the spatial filter position.\nIn particular, we obtain the value \u03d5_ab^d at pixel location (a, b) in channel d by\n\n    \u03d5_ab^d(K, \u03b2, \u03c8) \n        =    \n        [\ud835\udc9e_K, \u03b2(\u03c8)]_ab^d\n        = \n        (K \u2217\u03c8)_ab^d + \u03b2^d \n    \n        =    \u2211_c = 1^\u03ba_in\u2211_q=-s^s \u2211_r = -s^s (K_c^d)_qr\u03c8^c_a + q, b + r + \u03b2^d.\n\nHere, we assume the filters of size (2 s + 1) \u00d7 (2 s + 1) indexed symmetrically around their center and \u03c8 is the feature map pre-convolution.\nNote, also that the bias is constant across the indices a and b.\nTaking explicit derivatives of this expression with respect to one of the parameters in K yields\n\n    \u2202 (K\u2217\u03d5)_ab^d/\u2202 (K_e^h)_fg \n        =   \u2211_c = 1^\u03ba_in\u2211_q,r = -s^s \u2202 (K_c^d)_qr/\u2202 (K_e^h)_fg \u03c8_a+q, b+r^c \n    \n        =   \u2211_c = 1^\u03ba_in\u2211_q,r = -s^s \u03b4^dh\u03b4_ce\u03b4_pf\u03b4_qg\u03c8_a+q, b+r^c \n    \n        =   \u03b4^dh\u03c8_a+f, b+g^e.\n\nWhen utilizing the predicted one-hot vector (self-learning gradient), i.e., y^\ud835\udc5c\u210e_k = \u03b4_k \u0109 and using the fact that the last-layer convolution is (1 \u00d7 1), we obtain\n\n    \u2202/\u2202 K_e^f\u2112_ab\n        =   \u2211_k = 1^C ^k(\u03d5_ab) (1 - \u03b4_k \u0109) \u2202\u03d5_ab^k/\u2202 K_e^f\n    \n        =   ^f(\u03d5_ab) (1 - \u03b4_f \u0109) \u03c8_ab^e.\n\nwhile the uniform categorical label y_k = 1C yields\n\n    \u2202/\u2202 K_e^f\u2112_ab\n        =   \u2211_k = 1^C ^k(\u03d5_ab) (1 - 1/C) \u00b7\u2207_\u03b8\u03d5^k_ab(\u03b8) \n    \n        =   C - 1/C^f(\u03d5_ab) \u03c8_ab^e.\n\n\n\n\n \u00a7.\u00a7 Computing Deeper Gradients via Explicit Backpropagation\n\nWhile for the last-layer gradients the computation from above simplifies significantly due to the fact that pixel-wise gradients only depend on the feature map values at the same pixel-location.\nDeeper layers are usually not (1 \u00d7 1), so the forward pass couples feature map values over a larger receptive field, e.g., (3 \u00d7 3).\nHowever, gradients for the second-to-last layer can still be computed with moderate effort by extracting feature map patches via the unfold function.\nHere, we consider a DeepLabv3+ implementation^2^2<https://github.com/NVIDIA/semantic-segmentation/tree/sdcnet> <cit.> for which the feature maps depend in the following way on the weights of the second-to-last layer T - 1 (where T indicates the last layer):\n\n    \u03d5(K_T - 1) = \ud835\udc9e_K_T, \u03b2_T\u2218\ud835\uddb1\ud835\uddbe\ud835\uddab\ud835\uddb4\u2218\ud835\udda1\ud835\uddad_T \u2218\ud835\udc9e_K_T - 1, \u03b2_T - 1(\u03c8_T - 1),\n\nwhere \ud835\udda1\ud835\uddad_T is a batch normalization layer and \u03c8_T - 1 are the features prior to the convolution \ud835\udc9e_K_T - 1, \u03b2_T - 1 in question.\nWith fully expanded indices, this amounts to\n\n    \u03d5_ab^d =   \u2211_k_T K_k_T^d \ud835\uddb1\ud835\uddbe\ud835\uddab\ud835\uddb4( \ud835\udda1\ud835\uddad_T \u2211_k_T-1\u2211_q,r((K_T-1)_k_T-1^k_T)_qr\n        (\u03c8_T-1)_a+q, b+r^k_T-1 + \u03b2_T-1^k_T) + \u03b2_T^d\n\nwith k_T, k_T-1 indexing the respective amount of channels and q, r indexing the filter coordinates of K_T-1.\nNote, that we still assume here that the last convolutional layer has spatial extent 0 into both directions, so K_T \u2208^\u03ba_in\u00d7 C \u00d7 1 \u00d7 1.\nThe chain rule for this forward pass is then\n\n    \u2202\u03d5_ab^d/\u2202 ((K_T-1)_e^f)_gh   =\n            \u2211_k_T K_k_T^d \ud835\uddb1\ud835\uddbe\ud835\uddab\ud835\uddb4'(\u00b7) \u00b7\ud835\udda1\ud835\uddad_T' \u00b7\n    \u00b7[\u2211_k_T-1   \u2211_q,r\u2202 ((K_T-1)_k_T-1^k_T)_qr/\u2202 ((K_T-1)_e^f)_gh (\u03c8_T-1)_a+q, b+r^k_T-1] \n       =\n            \u2211_k_T K_k_T^d \ud835\uddb1\ud835\uddbe\ud835\uddab\ud835\uddb4'(\u00b7) \u00b7\ud835\udda1\ud835\uddad'_T \u00b7\n    \u00b7[\u2211_k_T-1   \u2211_q,r\u03b4^k_T f\u03b4_k_T-1e\u03b4_gp\u03b4_hq (\u03c8_T-1)_a+q, b+r^k_T-1] \n       =\n            K_f^d (\ud835\uddb1\ud835\uddbe\ud835\uddab\ud835\uddb4')_ab^f \u00b7 (\ud835\udda1\ud835\uddad'_T)_ab^f \u00b7\n         \u00b7[(\u03c8_T-1)_a+g, b+h^e]\n\nThe term with the derivative of the \ud835\uddb1\ud835\uddbe\ud835\uddab\ud835\uddb4 activation is simply the Heaviside function evaluated at the features pre-activation which have been computed in the forward pass anyway (the discontinuity at zero has vanishing probability of an evaluation).\nThe derivative of the batch normalization layer is multiplication by the linear batch norm parameter which is applied channel-wise.\nThe running indices g and h only apply to the last factor which can be computationally treated by extracting (3 \u00d7 3)-patches from \u03c8_T-1 using the unfold operation.\nIn computing the norm of \u2207_K_T-1\u03d5_ab^d with respect to the indices e, f, g and h, we note that the expression in eq.\u00a0(<ref>) is a product of two tensors S_f \u00b7\u03a8_gh^e for each pixel (a,b).\nL^p-norms of such tensors T_ij = S_i \u03a8_j factorize which makes their computation feasible:\n\n    T_p =    ( \u2211_i, j |T_ij|^p )^1/p\n        = (\u2211_i, j |S_i|^p |\u03a8_j|^p)^1/p\n    \n        =   (\u2211_i[|S_i|^p \u2211_j |\u03a8_j|^p])^1/p\n        = S_p \u03a8_p.\n\n\n\n\n\n\n\u00a7 DETAILS ON THE FEATURE CONSTRUCTION FOR SEGMENT-WISE PREDICTION QUALITY ESTIMATION\n\nThe tasks of meta classification (false positive detection) and meta regression (prediction quality estimation) based on uncertainty measures extracted from the network's softmax output were introduced in <cit.>. The neural network provides for each pixel z given an input image x a probability distribution f_z(y|x) over a label space C = {y_1, \u2026, y_c}, with y \u2208 C. The degree of randomness in the semantic segmentation prediction f_z(y|x) is quantified by pixel-wise dispersion measures, like entropy \n\n    E_z(x) =-1/log(c)\u2211_y\u2208\ud835\udc9ef_z(y|x)log f_z(y|x)\n \nand probability margin\n\n    M_z(x) = 1 - f_z(\u0177_z(x)|x)  + max_y\u2208\ud835\udc9e\u2216{\u0177_z(x)} f_z(y|x)\n\nwhere \n\n    \u0177_z(x)=_y\u2208\ud835\udc9ef_z(y|x)\n\nis the predicted class for each pixel z.\nTo obtain segment-wise features from these dispersion measures which characterize uncertainty, they are aggregated over segments via average pooling obtaining mean dispersions \u03bc E and \u03bc M. As erroneous or poor predictions oftentimes have fractal segment shapes, it is distinguished between the inner of the segment (consisting of all pixels whose eight neighboring pixels are also elements of this segment) and the boundary. This results in segment size S and mean dispersion features per segment also for the inner (in) and the boundary (bd). Furthermore, relative segment sizes S\u0303 = S/S_bd and S\u0303_in = S_in/S_bd as well as relative mean dispersions \u03bcD\u0303 = \u03bc D S\u0303 and \u03bcD\u0303_in = \u03bc D_inS\u0303_in where D \u2208{E,M} are defined. Moreover, the mean class probabilities P(y) for each class y \u2208 C are added resulting in the set of hand-crafted features \n\n    { S, S_in, S_bd, S\u0303, S\u0303_in}\u222a{ P(y)   :   y=1,\u2026,c }\n       \u222a{\u03bc D, \u03bc D_in, \u03bc D_bd, \u03bcD\u0303, \u03bcD\u0303_in  :   D \u2208{E,M}}  .\n\nThese features serve as a baseline in our tests.\n\nThe computed gradients in <ref> with applied p-norm, p \u2208{ 0.1, 0.3, 0.5, 1, 2 }, are denoted by G_z^p=#(x), #\u2208{ 0.1, 0.3, 0.5, 1, 2 },  for image x. Similar to the dispersion measures, we compute the mean and additionally the variance of these pixel-wise values of a given segment to obtain \u03bc G^p=# and vG^p=#, respectively. Since the gradient uncertainties may be higher on the boundary of a segment, the mean and variance features per segment are considered also for the inner and the boundary. Furthermore, we define relative mean and variance features to quantify the degree of fractality. This results in the following gradient features\n\n    {\u03bc G^p=#, \u03bc G_in^p=#, \u03bc G_bd^p=#, \u03bcG\u0303^p=#, \u03bcG\u0303_in^p=#, \n        vG^p=#, vG_in^p=#, vG_bd^p=#, v G\u0303^p=#, v G\u0303_in^p=# : \n       #\u2208{ 0.1, 0.3, 0.5, 1, 2 }}  .\n\nNote, these features can be computed for the gradient scores obtained from the predictive one-hot () and the uniform label () as well as for gradients of deeper layers (see <ref>).\n\n\n\n\n\n\u00a7 EXTENDED OOD SEGMENTATION RESULTS\n\nIn <ref>, we have compared our approach with comparable uncertainty estimation methods such as entropy and maximum softmax. Moreover, we considered two sampling approaches, MC Dropout and ensembles, as baselines. \nIn <ref> and <ref>, we provide the comparison of our method with more methods from the benchmark. \n\n\n\n\nIn detail, the first block consists of approaches using OoD data, i.e., Void Classifier <cit.>, SynBoost <cit.>, Maximized Entropy <cit.>, PEBAL <cit.> and DenseHybrid <cit.>. The methods of the second block use complex auxiliary/generateive models, namely Image Resynthesis <cit.>, Road Inpainting <cit.>, embedding density <cit.>, NFlowJS <cit.>, JSRNet <cit.> and ObsNet <cit.>. The ODIN <cit.> and Mahalanobis <cit.> baselines (in the third block) perform adversarial attacks on the input images and thus, require a full and expensive backward pass.\nPer block we indicate the best method for each of the considered metrics.\n\nWe outperform the two latter baselines (ODIN and Mahalanobis) for the LostAndFound as well as the RoadAnomaly21 dataset. In detail, we obtain AuPRC values up to 22.8 pp higher on segment-level and F_1 values values up to 24.8 pp higher on pixel-level. For the other two datasets we achieve similar results. \nFurthermore, we beat the Void Classifier method, that uses OoD data during training, in most cases. We improve the AuPRC metric by up to 64.5 pp and the sIoU metric by up to 48.2 pp, both for the LostAndFound dataset. \nIn addition, our gradient norms outperform in most cases the Embedding Density approach which is based on normalizing flows. \nSumming up, we have shown superior OoD segmentation performance in comparison to the other uncertainty based methods (see <ref>) and we outperform some of the more complex approaches (using OoD data, adversarial samples or generative models).\n\n\n\n\n\n\u00a7 ABLATION: DIFFERENT P-NORMS\n\nIn this section, we present an ablation study for different p-norms, p \u2208{ 0.1, 0.3, 0.5, 1, 2 }, that are applied to the calculated gradients of the last convolutional layer.\n\n\n\n\n  \nPixel-wise Uncertainty Evaluation\nThe area under the sparsification error curve () is considered to access the correlation of uncertainty and prediction errors on pixel level and the expected calibration error () to assess the statistical reliability of the uncertainty measure. In <ref> the results for the different p-norms in terms of these two metrics are given.\n\nWe observe improved performance for the gradient scores obtained from the predictive one-hot label with respect to both metrics. These scores are better calibrated for greater values of p, whereas there is no clear trend for the  metric. In contrast, for the gradient scores obtained from the uniform label the calibration ability as well as the sparsification error enhance mostly for decreasing values of p.\n\n\n\n\n  \nSegment-wise Prediction Quality Estimation\nFor the segment-wise prediction quality estimation, we consider meta classification, i.e., classifying between = 0 (false positive) and > 0 (true positive), and meta regression, i.e., direct prediction of the . The gradient features (see eq.\u00a0(<ref>)) which are computed for each value of p, also separated for predictive one-hot and uniform labels, serve as input for the meta models. The results are given in <ref> and visualized in <ref>.\n\n\n\n\nThe WideResNet backbone outperforms the SEResNeXt for meta classification and regression. Moreover, higher  and R^2 performances are achieved for greater values of p independent of the architecture or the label (predictive one-hot or uniform) used for gradient scores computation. \n\n\n\n\n  \nOoD Segmentation\nOur approach provides pixel-wise uncertainty scores obtained by computing the partial norm. In <ref> the pixel-wise heatmaps for both backbones, different p-norms and the predictive one-hot as well as the uniform label are shown.\n\nWe observe that for higher p values the number of uncertain pixels increases, the gradients are more sensitive to unconfident predictions. For a p value of 0.1 only a few pixels of OoD object have high uncertainty while the background is completely certain. For values of p=1 and p=2, in particular using the uniform label, the gradient scores show higher uncertainties in more sectors. To identify out-of-distribution regions, we threshold per pixel on our gradient scores, i.e., high uncertainty corresponds to out-of-distribution. Here, the OoD objects are mostly covered (and not so many background pixels) for p values of 0.3 and 0.5. These observations are reflected in the OoD segmentation results, given in <ref> and <ref>.\n\n\n\n\nFor the LostAndFound dataset and the Fishyscapes LostAndFound dataset, the best results are achieved for the 0.3 and 0.5 p-norms. For the RoadAnomaly21 dataset, also for higher p values strong (in one case even the best) results are obtained. Across these three datasets, there is no favorability which backbone architecture or label (predictive one-hot or uniform) performs better. In comparison for the RoadObstacle21 dataset, the WideResNet backbone with gradient scores obtained from the predictive one-hot performs best.\n\nIn summary, there is no clear tendency which p-norm outperforms the others for the different tasks of pixel- and segment-wise uncertainty estimation as well as for OoD segmentation. \nHowever, there is a strong trend towards p \u2208{0.3, 0.5} performing especially strongly.\n\n\n\n\n\n\u00a7 ABLATION: DEEPER LAYERS\n\nIn <ref>, we have shown how to compute the gradients also for deeper layers than the last convolutional one. In this section, we discuss the numerical results for the second-to-last layer in comparison to the last.\n\n\n\n\n  \nPixel-wise Uncertainty Evaluation\nThe results for the pixel-wise uncertainty estimation measured by  and  are given in <ref> for both layers. For the gradient scores obtained from the predictive one-hot, the evaluation metrics are mainly similar showing only small performance gaps. The results differ for the uniform labels, although there is no trend to which layer achieves the higher ones. \n\n\n\n\n  \nSegment-wise Prediction Quality Estimation\nThe segment-wise meta classification and regression results are shown in <ref>. We observe the same behavior for the second-to-last as for the last one, namely that as p increases, performance improves for both metrics. Furthermore, the performance is almost equal for the second-to-last and the last layer for the 1 and the 2 norm independent of the backbone and labels to obtain the gradient scores.\n\n\n\n\n  \nOoD Segmentation\nIn <ref> the OoD segmentation results are given. \n\nIn comparison to the performance of the gradient scores of the last layer (see <ref> and <ref>), the performance of the second-to-last layer is poor, i.e., the results for all evaluation metrics are worse than these of the last layer. In some cases, there is no detection capability at all for the gradient scores obtained from the uniform label.\n\nIn conclusion, the gradients of the second-to-last layer do not improve the uncertainty estimation (at pixel- and segment-level) nor the OoD segmentation quality, rather they perform worse in some cases.\nThe finding that deeper layer gradients contain less information than the final layer has been observed before outside the semantic segmentation setting by <cit.> and <cit.>.\n\n\n"}