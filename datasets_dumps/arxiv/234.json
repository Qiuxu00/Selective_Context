{"entry_id": "http://arxiv.org/abs/2303.07034v2", "published": "20230313115340", "title": "Pretrained ViTs Yield Versatile Representations For Medical Images", "authors": ["Christos Matsoukas", "Johan Fredin Haslum", "Magnus S\u00f6derberg", "Kevin Smith"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\nFireRisk: A Remote Sensing Dataset for Fire Risk Assessment with Benchmarks Using Supervised and Self-supervised Learning\n    Shuchang Shen\n\n chuchangs@student.unimelb.edu.au\n\n\n\n\n\nSachith Seneviratne\n sachith.seneviratne@unimelb.edu.au\nXinye Wanyan\n xwanyan@student.unimelb.edu.au\nMichael Kirley\n mkirley@unimelb.edu.au\n    March 30, 2023\n=====================================================================================================================================================================================================\n\n\n\n\nExtended version of <cit.> https://arxiv.org/abs/2108.09038\u201cIs it Time to Replace CNNs with Transformers for Medical Images?\u201d originally published at the ICCV 2021 Workshop on Computer Vision for Automated Medical Diagnosis.\n\n\n\n\nConvolutional Neural Networks (CNNs) have reigned for a decade as the de facto approach to automated medical image diagnosis, pushing the state-of-the-art in classification, detection and segmentation tasks.\nOver the last years, vision transformers (ViTs) have appeared as a competitive alternative to CNNs, yielding impressive levels of performance in the natural image domain, while possessing several interesting properties that could prove beneficial for medical imaging tasks.\nIn this work, we explore the benefits and drawbacks of transformer-based models for medical image classification.\nWe conduct a series of experiments on several standard 2D  medical image benchmark datasets and tasks.\nOur findings show that, while CNNs perform better if trained from scratch, off-the-shelf vision transformers can perform on par with CNNs when pretrained on ImageNet, both in a supervised and self-supervised setting, rendering them as a viable alternative to CNNs.\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nBreakthroughs in medical image analysis over the past decade have been largely fuelled by convolutional neural networks (CNNs).\nCNN architectures  have served as the workhorse for numerous medical image analysis tasks including breast cancer detection <cit.>, ultrasound diagnosis <cit.>, and diabetic retinopathy  <cit.>.\nWhether applied directly as a plug-and-play solution  or used as the backbone for a bespoke model, CNNs such as ResNet <cit.> are the dominant model for medical image analysis.\nRecently, however, vision transformers have gained increased popularity for natural image recognition tasks, possibly signalling a transition from convolution-based feature extractors to attention-based models (ViTs).\nThis raises the question: can ViTs also replace CNNs in the medical imaging domain?\n\nIn the natural image domain, transformers have been shown to outperform CNNs on standard vision tasks such as ImageNet  classification <cit.>, as well as in object detection <cit.> and semantic segmentation <cit.>.\nInterestingly, transformers succeed despite their lack of inductive biases tied to the convolution operation.\nThis may be explained, in part, by the attention mechanism central to transformers which offers several key advantages over convolutions: it explicitly models and more efficiently captures long-range relationships <cit.>, and it has the capacity for adaptive modeling via dynamically computed self-attention weights that capture relationships between tokens. \nIn addition, it provides a type of built-in saliency, giving insight as to what the model focused on\u00a0<cit.>.\n\nOver the years, we have gained a solid understanding of how CNNs perform on medical images, and which techniques benefit them.\nFor example, it is well-known that performance drops dramatically when training data is scarce <cit.>.\nThis problem is particularly acute in the medical imaging domain, where datasets are smaller and often accompanied by less reliable labels due to the inherent ambiguity of medical diagnosis.\nFor CNNs, the standard solution is to employ transfer learning <cit.>:\u00a0typically, a model is pretrained on a larger dataset such as ImageNet <cit.> and then fine-tuned for specific tasks using smaller, specialized datasets.\nFor medical imaging, CNNs pre-trained on ImageNet typically outperform those trained from scratch, both in terms of final performance and reduced training time\ndespite the differences between the two domains <cit.>,<cit.>,<cit.>, <cit.>.\nHowever, recent studies show that, for CNNs, transfer from ImageNet to medical tasks is not as useful as previously thought <cit.>,<cit.>.\n\nSupervised transfer learning requires many annotations.\nThis can be problematic for medical tasks where annotations are costly and require specialized experts.\nSelf-supervised approaches, on the other hand, can learn powerful representations by leveraging the intrinsic structure present in the image, rather than explicit labels <cit.>,<cit.>.\nAlthough self-supervision performs best when large amounts of unlabelled data are available, it has been shown to be effective with CNNs for certain medical image analysis tasks <cit.>,<cit.>. \n\n\n\nIn contrast to CNNs, we know relatively little about how vision transformers perform at medical image classification.\nAre vanilla ViTs competitive with CNNs?\nDespite their success in the natural image domain, there are questions that cast doubt as to whether that success will translate to medical tasks. \nEvidence suggests that ViTs require very large datasets to outperform CNNs \u2013 in <cit.>, the benefits of ViTs only became evident when Google's private 300 million image dataset, JFT-300M, was used for pretraining.\nReliance on data of this scale may be a barrier to the application of transformers for medical tasks.\nAs outlined above, CNNs rely on techniques such as transfer learning and self-supervision to perform well on medical datasets.\nAre these techniques as effective for transformers as they are for CNNs?\nFinally, the variety and type of patterns and textures in medical images differ significantly from the natural domain. \nStudies on CNNs indicate that the benefits from transfer diminish with the distance from the source domain <cit.>,<cit.>,<cit.>.\nCan ViTs cope with this difference?\n\n\nIn this work we explore the benefits and drawbacks of transformer-based models for 2D medical image\nclassification.\nGiven the enormous computational cost of considering the countless variations in CNN and ViT architectures, we limit our study to prototypical CNN and ViT models over a representative set of well-known publicly available medical image benchmark datasets.\nThrough these experiments we show that:\n\n    \n  * ViTs pretrained on ImageNet perform comparably to CNNs for classification tasks on medical images. While CNNs outperform ViTs when trained from scratch, ViTs receive a larger boost in performance from pretraining on ImageNet.\n    \n  * Despite their reliance on large datasets, self-supervised ViTs perform on par or better than CNNs, if ImageNet pretraining is utilized, albeit by a small margin.\n    \n  * Features of off-the-shelf ImageNet pre-trained ViTs are versatile.\n    k-NN tests show   self-supervised ViT representations outperform CNNs, indicating their potential for other tasks. We also show they can serve as drop-in replacements for segmentation. \n\nThese findings, along with additional ablation studies, suggest that ViTs can be used in medical image analysis, while at the same time potentially gaining from other properties of ViTs such as built-in explainability.\nThe source code to reproduce our work is available at https://github.com/ChrisMats/medical_transformershttps://github.com/ChrisMats/medical_transformers.\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n\n  \nTransformers in vision problems. \nFollowing the success of transformers <cit.> in natural language processing (NLP), attention-based models captured the interest of the vision community and inspired numerous improvements to CNNs <cit.>,<cit.>.\nThe first work to show that vanilla transformers from NLP can be applied with minimal changes to large-scale computer vision tasks and compete with standard CNNs was from <cit.>, which introduced the term vision transformer, or ViT[While Dosovitskiy's use of the term ViT referred to a direct adaptation of the original transformer model from NLP <cit.>, confusingly ViT has since been adopted as a term to refer to any transformer-based architecture for a vision task. We also use ViT in the latter sense, for brevity.].\nLike their NLP counterparts, the original vision transformers required an enormous corpus of training data to perform well.\nTo overcome this problem, <cit.> introduced DeiTs, which proposed a distillation token in addition to the cls token that allows transformers to learn more efficiently on smaller datasets. \nAs the number of applications of vision transformers and architectural variations have exploded\n(<cit.> provides a good review),\nDeiT and the original ViT <cit.> have become the standard benchmark architectures for vision transformers.\n\n\n\n  \nVision transformers in medical imaging.\nGiven the recent appearance of vision transformers, their application in medical image analysis has been limited.\nSegmentation has seen the most applications of ViTs.\n<cit.>, <cit.> and <cit.> independently suggested different methods for replacing CNN encoders with transformers in U-Nets <cit.>, resulting in improved performance for several medical segmentation tasks.\nVanilla transformers were not used in any of these works, however.\nIn each, modifications to the standard vision transformer from <cit.> are proposed,  either to adapt to the U-Net framework <cit.> or to add components from CNNs <cit.>.\nOther transformer-based adaptations of CNN based-architectures suggested for medical segmentation tasks include <cit.>, who combined pyramid networks with transformers, and <cit.> who used transformers in variational auto encoders (VAEs) for segmentation and anomaly detection in brain MR imaging. \n\nOnly a handful of studies have tackled tasks other than segmentation, such as 3-D image registration <cit.> and detection <cit.>.\nThe only work so far, to our knowledge, applying transformers to medical image classification is a CNN/transformer hybrid model proposed by <cit.> applied to a small (344 images) private MRI dataset.\nNotably, none of these works consider pure, off-the-shelf vision transformers \u2013 all propose custom architectures combining transformer/attention modules with components from convolutional feature extractors.\n\n\n\n\n  \nImproved initialization with transfer- and self-supervised learning.\nMedical imaging datasets are typically orders of magnitude smaller than natural image datasets due to cost, privacy concerns, and the rarity of certain diseases.\nA common strategy to learn good representations for smaller datasets is transfer learning. \nFor medical imaging, it is well-known that CNNs usually benefit from transfer learning with ImageNet <cit.> despite the distinct differences between the domains.\nHowever, the question of whether ViTs benefit similarly from transfer learning to medical domains has yet to be explored.\n\nWhile transfer learning is one option to initialize models with good features, another more recent approach is self-supervised pre-training.\nRecent advances in self-supervised learning have dramatically improved performance of label-free learning.\nState-of-the-art methods such as DINO \n<cit.> and BYOL <cit.> have reached performance on par with supervised learning on ImageNet and other standard benchmarks.\nWhile these top-performing methods have not yet been proven for medical imaging, there has been some work using earlier self-supervision schemes on medical data.\n<cit.> adopted SimCLR\u00a0<cit.>, a self-supervised contrastive learning method, to pretrain CNNs. \nThis yielded state-of-the-art results for predictions on chest X-rays and skin lesions. \nSimilarly, <cit.> employed MoCo <cit.> pre-training on a target chest X-ray dataset, demonstrating again the power of this approach.\nWhile these works exhibit promising improvements thanks to self-supervision, they have all employed CNN-based encoders. It has yet to be shown how self-supervised learning combined with ViTs performs in medical imaging, and how this combination compares to its CNN counterparts.\n\n\n\n\u00a7 METHODS\n\n\n\nThe main question we investigate is whether prototypical vision transformers can be used as a drop-in alternative to CNNs for medical diagnostic tasks.\nMore concretely, we consider how each model type performs on various tasks in different domains, with different types of initialization, and across a range of model capacities.\nTo that end, we conducted a series of experiments to compare representative ViTs and CNNs for various medical image analysis tasks under the same conditions.\nHyperparameters such as weight decay and augmentations were optimized for CNNs and used for both CNNs and ViTs \u2013 with the exception of the initial learning rate, which was determined individually for each model type using a grid search.\n\nTo keep our study tractable, we selected representative CNN and ViT model types as there are too many architecture variations to consider each and every one. \nFor CNNs, an obvious choice is the ResNet family <cit.> and Inception <cit.>, as they are the most common and highly cited CNN encoders, and recent works have shown that ResNets are  competitive with more recent CNNs when modern training methods are applied <cit.>.\nThe choice for a representative vision transformer is less clear because the field is still developing.\nWe considered several options including the original ViT <cit.>, SWIN transformers <cit.>, CoaT <cit.>, and focal transformers <cit.>, but we selected the DeiT family <cit.> for the following reasons: (1) it is one of the earliest and most established vision transformers, with the most citations aside from the original ViT, (2) it is similar in spirit to a pure transformer, (3) it was the first to show that vision transformers can compete on mid-sized datasets with short training times, \n(4) it retains the interpretability properties of the original transformer.\nBesides the DeiT family, we also employ for our main results SWIN transformers since they are more robust to data size and they have some properties which are typically encountered in CNN-based architectures, like hierarchical signal processing at different scales <cit.>.\n\n\nAs mentioned above, CNNs rely on initialization strategies to improve performance for smaller datasets.\nAccordingly, we consider three common initialization strategies: randomly initialized weights, transfer learning from ImageNet, and self-supervised pretraining on the target dataset.\nUsing these models and initialization strategies, we consider the following datasets and tasks:\n\n\n\n  \nMedical image classification. Five standard medical image classification datasets were chosen to be representative of a diverse set of target domains. These cover different imaging modalities, color distributions, dataset sizes, and tasks, with expert labels.\n\n\n    \n  * APTOS 2019 \u2013 In this dataset, the task is classification of diabetic retinopathy images into 5 categories of disease severity <cit.>.\n    APTOS 2019 contains 3,662 high-resolution retinal images.\n    \n  * CBIS-DDSM \u2013 \n    A mammography dataset containing 10,239 images. The task is to detect the presence of masses in the mammograms <cit.>.\n    \n  * ISIC 2019 \u2013 Here, the task is to classify 25,331 dermoscopic images among nine different diagnostic categories of skin lesions <cit.>.\n    \n  * CheXpert \u2013 This dataset contains 224,316 chest X-rays with labels over 14 categories of diagnostic observations <cit.>.\n    \n  * PatchCamelyon \u2013 Sourced from the Camelyon16 segmentation challenge <cit.>, this dataset contains 327,680 patches of H&E stained WSIs of sentinel lymph node sections. The task is to classify each patch as cancerous or normal <cit.>.\n\n\n\n\n  \nMedical image segmentation. \nAlthough there has been some recent work applying vision transformers to medical image segmentation, all works thus far have proposed hybrid architectures.\nHere, we evaluate the ability of vanilla ViTs to directly replace the encoder of a standard segmentation framework designed for CNNs.\nIn particular, we use DeepLab3 <cit.> featuring a ResNet50 encoder.\nWe merely replace the encoder of the segmentation model with DeiT-S and feed the outputs of the cls token to the decoder of DeepLab3.\nAs DeepLab3 was designed and optimized for ResNets, CNNs have a clear advantage in this setup.\nNevertheless, these experiments can shed some light into the quality and versatility of ViT representations for segmentation.\nNote that, unlike the classification experiments, all models were initialized with ImageNet pretrained weights.\nWe consider the following medical image segmentation datasets:\n\n\n    \n  * ISIC 2018 \u2013 This dataset consists of 2,694 dermoscopic images with their corresponding pixel-wise binary annotations. The task is to segment skin lesions <cit.>.\n    \n  * CSAW-S \u2013 This dataset contains 338 images of mammographic screenings. The task is pixel-wise segmentation of tumor masses <cit.>.\n\n\n\n\n\n  \nExperimental setup. \nWe selected the ResNet family <cit.> as representative CNNs, and DeiT[Note that we refer to the DeiT architecture pretrained without the distillation token <cit.>.] <cit.> as representative ViTs.\nSpecifically, we test ResNet-18, ResNet-50, and ResNet-152 against DeiT-T, DeiT-S, and DeiT-B.\nThese models were chosen because they are easily comparable in the number of parameters, memory requirements, and compute.\nThey are also some of the simplest, most commonly used, and versatile architectures of their respective type.\nNote that ViTs have an additional parameter, patch size, that directly influences the memory and computational requirement.\nWe use the default 16\u00d716 patches to keep the number of parameters comparable to the CNN models.\nUsing these models, we compare three commonly used initialization strategies:\n\n    \n  * Randomly initialized weights (Kaiming initialization from <cit.>),\n    \n  * Transfer learning using supervised ImageNet pretrained weights,\n    \n  * Self-supervised pretraining on the target dataset, following ImageNet initialization.\n\nAfter initialization using the strategies above, the models are fine-tuned on the target data using the procedure described below.\nEarly experiments indicated that initialization using self-supervision on the target dataset consistently outperforms self-supervision on ImageNet.\nWe also determined that DINO performs better than other self-supervision strategies such as BYOL <cit.>.\n\n\n3.25pt \n\n-3.25pt \n\n\n\n\n  \nTraining procedure.\nFor supervised training, we use the Adam optimizer <cit.>[DeiT trained with random initialization used AdamW <cit.> instead of Adam.]. We performed independent grid searches to find suitable learning rates, and found that 10^-4 works best for both pretrained CNNs and ViTs, while 10^-3 is best for random initialization.\nWe used these as base learning rates for the optimizer along with default 1,000 warm-up iterations.\nWhen the validation metrics saturated, the learning rate was dropped by a factor of 10 until it reached its final value of 10^-6.\nFor classification tasks, images were resized to 256\u00d7256 with the following augmentations applied: normalization; color jitter which consists of brightness, contrast, saturation, hue; horizontal flip; vertical flip; and random resized crops. \nFor segmentation,  all images were resized to 512\u00d7512 and we use the same augmentation methods as for classification except for Csaw-S, where elastic transforms are also employed. \nFor each run, we select the checkpoint with highest validation performance.\nFor self-supervision, pretraining starts with ImageNet initialization, then applies DINO <cit.> on the target data following the default settings \u2013 except for three small changes determined to work better on medical data:\n(1) the base learning rate was set to 10^-4, (2) the initial weight decay is set at 10^-5 and increased to 10^-4 using a cosine schedule, and (3) we used an EMA of 0.99.\nThe same settings were used for both CNNs and ViTs; both were pre-trained for 300 epochs using a batch size of 256, followed by fine-tuning as described above.\n\nThe classification datasets were divided into train/test/validation splits (80/10/10), with the exception of APTOS2019, which was divided 70/15/15 due to its small size.\nFor the segmentation datasets, we split the training set into train/validation with a 90/10 ratio. For ISIC2018 and CSAW-S, we used the provided validation and test sets for model evaluation. \n\n\n\n  \nEvaluation.\nUnless otherwise specified, each experimental condition was repeated five times.\nWe report the median and standard deviation of the appropriate metric for each dataset: Quadratic Cohen Kappa, Recall, ROC-AUC, and IoU.\nFor classification, we measure performance of the:\n\n    \n  * initialized model representations, through a k-NN evaluation protocol <cit.>\n    \n  * final model after fine-tuning on the target dataset.\n\nThe k-NN evaluation protocol is a technique to measure the usefulness of learned representations for some target task; it is typically applied before fine-tuning.\nFor CNNs, it works by freezing the pretrained encoder, passing test examples through the network, and performing a k-NN classification using the cosine similarity between embeddings of the test images and the k nearest training images (k = 200 in our case).\nFor ViTs, the principle is the same except the output of the cls token is used instead of the CNN's penultimate layer.\n\n\n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n\n\n\n\n\n  \nAre randomly initialized vision transformers useful?\nOur first experiment compares the performance of ViTs against CNNs when initialized with random weights <cit.>.\nThe results in Table <ref> and Figure <ref> indicate that in this setting, CNNs outperform ViTs across the board.\nThis is in line with previous observations in the natural image domain, where ViTs trained on smaller datasets are outperformed by similarly-sized CNNs, a trend that was attributed to the vision transformer's lack of inductive bias <cit.>.\nNote that the performance gap appears to shrink as the number of training examples n increases.\nHowever, since most medical imaging datasets are of modest size, the usefulness of randomly initialized ViTs appears to be limited.\n\n\n\n\n  \nDoes pretraining transformers on ImageNet work in the medical image domain?\nTo deal with the smaller size of medical imaging datasets, CNNs are often initialized with ImageNet pretrained weights.\nThis typically results in better performance than random initialization <cit.>, while recent works have questioned the usefulness of this approach for CNNs <cit.>,<cit.>.\n\nWe investigate if ViTs benefit from ImageNet pre-training in the medical domain and to which extent.\nTo test this, we initialize all models with supervised ImageNet-pretrained weights and then fine-tune on the target data using the procedure described in Section <ref>.\nThe results in Table <ref> and Figure <ref> show that both CNNs and ViTs benefit from ImageNet initialization.\nHowever, ViTs appear to benefit more from transfer learning, as they make up for the gap observed using random initialization, performing on par with their CNN counterparts.\n\n\n\n\n\n  \nDo transformers benefit from self-supervision in the medical image domain?\nRecent self-supervised learning schemes such as DINO and BYOL\nachieve performance near that of supervised learning.\nWhen used for pretraining in combination with supervised fine-tuning, they can achieve a new state-of-the-art <cit.>.\nWhile this phenomenon has been demonstrated for CNNs and ViTs on big data in the natural image domain, it is not clear whether self-supervised pretraining of ViTs helps for medical imaging tasks.\nTo test if ViTs benefit from self-supervision, we adopt the learning scheme of DINO, which can be readily applied to both CNNs and ViTs <cit.>.\nThe results reported in Table <ref> and Figure <ref> show that both ViTs and CNNs perform better with self-supervised pretraining.\nViTs appear to perform on par, or better than CNNs in this setting, albeit by a small margin.\n\n\n\n\n\n  \nAre pretrained ViT features useful for medical imaging, even without fine-tuning? \nWhen data is particularly scarce, features from pre-trained networks can sometimes prove useful, even without fine-tuning on the target domain.\nThe low dimensional embeddings from a good feature extractor can be used for tasks such as clustering or few-shot classification.\nWe investigate whether different types of pretraining in ViTs yields useful representations.\nWe measure this by applying the k-NN evaluation protocol described in Section <ref>.\nThe embeddings from the penultimate layer of the CNN and the cls token of the ViT are used to perform k-NN classification of test images, assigning labels based on the most similar examples from the training set.\nWe test whether out-of-domain ImageNet pretraining or in-domain self-supervised DINO pretraining <cit.> yields useful representations when no supervised fine-tuning has been done.\nThe results appear in Figure <ref> and Table <ref>, where we compare embeddings for DeiT-S and ResNet50.\nPretraining with ImageNet yields surprisingly useful features for both CNNs and ViTs, considering that the embeddings were learned on out-of-domain data.\nWe observe even more impressive results with in-domain self-supervised DINO pretraining.\nViTs appear to benefit more from self-supervision than CNNs \u2013 in fact, ViTs trained without any labels using DINO perform similar or better than the same model trained from scratch with full supervision.\nThe results indicate that representations from pretrained vision transformers, both in-domain and out-of domain, can serve as a solid starting point for transfer learning or k-shot methods.\n\n\n\n\n\n\n\n\n\n\n\n\n  \nDo ViTs learn meaningful representations for other tasks like segmentation?\nSegmentation is an important medical image analysis task.\nWhile previous works have shown that custom attention-based segmentation models can outperform CNNs (see Section <ref>), here we ask: can we achieve comparable performance to CNNs by merely replacing the encoder of a CNN-based segmentation model with a vision transformer?\nThis would demonstrate versatility of ViT representations for other tasks.\nWe consider DeepLab3 <cit.>, a mainstream segmentation model with a ResNet50 encoder.\nWe simply replace the ResNet50 with DeiT-S, and train the model as prescribed in Section <ref>.\nDespite the model being designed for ResNet50 and DeiT-S having fewer parameters connected to the DeepLab3 head, we observe that ViTs perform on par or better than CNNs in our segmentation tasks.\nThe results in Table <ref> and Figure <ref> clearly indicate that vision transformers are able to produce high quality embeddings for segmentation, even in this disadvantaged setting.\n\n\n\n\n\n  \nHow does capacity of ViT models affect medical image classification?\nTo understand the practical implications of ViT model size for medical image classification, we conducted an ablation study comparing different model capacities of CNNs and ViTs.\nSpecifically, we test ResNet-18, ResNet-50, and ResNet-152 against DeiT-T, DeiT-S, and DeiT-B.\nThese models were chosen because they are easily comparable in the number of parameters, memory requirements, and compute.\nThey are also some of the simplest, most commonly used, and versatile architectures of their respective type.\nAll models were initialized with ImageNet pretrained weights, and we followed the training settings outlined in Section <ref>.\nThe results are presented in Figure <ref> and Table <ref>.\nBoth CNNs and ViTs seem to benefit similarly from increased model capacity, within the margin of error.\nFor most of the datasets, switching from a small ViT to a bigger model results in minor improvements, with the exception of ISIC 2019 where model size appears to be an important factor for optimal performance and APTOS2019 where DeiT-T performs better than its larger variants. Perhaps the tiny size of APTOS2019 results in diminishing returns for large models.\n\n\n\n\n  \nOther ablation studies.\nWe also considered the effect of token size on ViT performance.\nTable <ref> shows that reducing patch size results in a slight boost in performance for DeiT-S, but at the cost of increasing the memory footprint.\nThis partially explains the success of SWIN-T, as it operates using 4\u00d74 pixel input patches.\nFinally, we investigate how ViTs attend to different regions of medical images.\nIn Figure <ref> we compute the attention distance, the average distance in the image across which information is integrated in ViTs.\nWe find that, in early layers, some heads attend to local regions and some to the entire image.\nDeeper into the network, attention becomes more global \u2013 though more quickly for some datasets than for others.\n\n\n\n\n\n\n\u00a7 DISCUSSION\n\n\n\nOur investigation of prototypical CNNs and ViTs indicate that CNNs can readily be replaced with vision transformers in medical imaging tasks without sacrificing performance. \nThe caveat being  that employing some form of transfer learning is necessary. \nOur experiments corroborate previous findings in the natural image domain and provide new insights, which we discuss below.\n\n\nDiscussion and implications of findings. \nUnsurprisingly, we found that CNNs outperform ViTs when trained from scratch on medical image datasets, which corroborates previous findings in the natural image domain <cit.>.\nThis trend appears consistently and fits with the \u201cinsufficient data to overcome the lack of inductive bias\u201d argument. Thus, the usefulness of randomly initialized ViTs appears to be limited in the medical imaging domain.\n\nWhen initialized with supervised ImageNet pretrained weights, the gap between CNN and ViT performance disappears on medical tasks.\nThe benefits of supervised ImageNet pretraining of CNNs  is well-known, but it was unexpected that ViTs would benefit so strongly.\n\nThis suggests that further improvements could be gained via transfer learning from other domains more closely related to the task, as is the case for CNNs\u00a0<cit.>.\nThe benefits of ImageNet pretraining was not only observed for the fine-tuned classification tasks, but also self-supervised learning, k-nn classification and segmentation.\n\nThe best overall performance on medical images is obtained using ViTs with in-domain self-supervision, where small \nimprovements over CNNs and other initialization methods were recorded.\nOur k-NN evaluation showed that ViT features learned this way are even strong enough to outperform supervised learning with random initialization.\nInterestingly, we did not observe as strong of an advantage for self-supervised ViTs over ImageNet pretraining as was previously reported in the natural image domain, e.g. in\u00a0<cit.>.\nWe suspect this is due to the limited size of medical datasets, suggesting a tantalizing opportunity to apply self-supervision on larger and easier to obtain unlabeled medical image datasets, where greater benefits may appear.\n\n\nOur other ablation studies showed that ViT performance scales with capacity in a similar manner to CNNs; that ViT performance can be improved by reducing the patch size; and that ViTs attend to information across the whole image even in the earliest layers.\nAlthough individually unsurprising, it is worthwhile to confirm these findings for medical images.\nIt should be noted however, that the memory demands of DeiTs increase quadratically with the image and patch size which might limit their application in large medical images. \nHowever, newer ViT architectures, like SWIN transformers <cit.> mitigate this issue \u2013 while demonstrating increased predictive performance. \n\n\n\n\nInteresting properties of ViTs.\nOur results indicate that switching from CNNs to ViTs can be done without compromising performance, but are there any other advantages to switching to ViTs?\nThere are a number of important differences between CNNs and ViTs.\nWe briefly discuss some of these differences and why they may be interesting for medical image analysis.\n\n\nLack of inductive bias \u2013 ViTs do away with convolutional layers.\nOur experiments indicate their implicit locality bias is only necessary when training from scratch on medical datasets.\nFor larger datasets, evidence suggests removing this bias improves performance <cit.>.\n\n\nGlobal + local features \u2013 ViTs, unlike CNNs, can combine information from distant and local regions of the image, even in early layers (see Figure <ref>). \nThis information can be propagated efficiently to later layers due to better utilization of residual connections that are unpolluted by pooling layers. \nThis may prove beneficial for medical modalities which rely on local features.\nIt may be necessary to use transfer learning to benefit from local features, as they require huge amounts of data to learn <cit.>.\n\n\nInterpretability \u2013 transformers' self-attention mechanism provides, for free, new insight into how the model makes decisions.\nCNNs do not naturally lend themselves well to visualizing saliency.\nPopular CNN explainability methods such as class activation maps <cit.> and Grad-CAM <cit.> provide coarse visualizations because of pooled layers.\nTransformer tokens give a finer picture of attention, and the self-attention maps explicitly model interactions between every region in the image.\nIn Figure <ref>, we show examples from each dataset along with Grad-CAM visualizations of ResNet-50 and the top-50% self-attention of 16\u00d716 DeiT-S cls token heads.\nThe ViTs provide a clear, localized picture of attention, e.g.\u00a0attention at the boundary of the skin lesion in ISIC, on hemorrhages and exudates in APTOS,  the membrane of the lymphatic nodule in PatchCamelyon, the opacity in the Chest X-ray, and a dense region in CBIS-DDSM.\n\n\n\n\n\u00a7 CONCLUSION\n\n\n\nIn this work we show that vision transformers can reliably replace CNNs on medical 2D image classification if appropriate training protocols are employed.\nMore precisely, ViTs reach the same level of performance as CNNs in an array of medical classification and segmentation tasks, but they require transfer learning to do so.\nBut since ImageNet pretraining is the standard approach for CNNs, this does not incur any additional cost in practice.\nThe best overall performance on medical imaging tasks is achieved using in-domain self-supervised pretraining, where ViTs show a small advantage over CNNs.\nAs the data size grows, this advantage is expected to grow as well, as observed by  <cit.> and our own experiments.\nAdditionally, ViTs have a number of properties that make them attractive: they scale similarly to CNNs (or better), their lack of inductive bias, global attention and skip connections may improve performance, and their self-attention mechanism provides a clearer picture of saliency. \nFrom a practitioner's point of view, these benefits are compelling enough to \nexplore the use of ViTs in the medical domain.\nFinally, modern CNNs have been studied extensively for more than a decade, while the first pure vision transformer appeared less than two years ago \u2013 the potential for ViTs to improve is considerable.\n\n\n  \u00a7.\u00a7.\u00a7 Acknowledgments\n\nThis work was supported by the Wallenberg Autonomous Systems Program (WASP), and\nthe Swedish Research Council (VR) 2017-04609.\nWe thank Konuk for the thoughtful discussions. \n\n\nplainnat\n\n\n\n\n\u00a7 APPENDIX\n\n\n\nThe Appendix contains several details and additional experimental results that did not fit within the page limit, organized by the order in which they are referenced in the main article. \n\n\n\n\n\u00a7 \u2013NN EVALUATION OF DEIT-S AND RESNET50\n\n\nThe k-NN evaluation protocol is a technique to measure the usefulness of learned representations for some target task; it is typically applied before fine-tuning.\nFor CNNs, it works by freezing the pretrained encoder, passing test examples through the network, and performing a k-NN classification using the cosine similarity between embeddings of the test images and the k nearest training images (k = 200 in our case).\nFor ViTs, the principle is the same except the output of the cls token is used instead of the CNN's penultimate layer.\n\nWe applied the k-NN evaluation protocol to test whether out-of-domain ImageNet pretraining or in-domain self-supervised DINO pretraining yields useful representations when no supervised fine-tuning has been done.\nThe results appear in Table <ref>.\nPretraining with ImageNet yields surprisingly useful features for both CNNs and ViTs, considering that the embeddings were learned on out-of-domain data.\nWe observe even more impressive results with in-domain self-supervised DINO pretraining.\nViTs appear to benefit more from self-supervision than CNNs \u2013 in fact, ViTs trained with DINO perform similar or better than the same model trained from scratch with full supervision!\nThe results indicate that representations from pretrained vision transformers, both in-domain and out-of domain, can serve as a solid starting point for transfer learning or k-shot methods.\n\n-3.0pt \n\n+3.0pt\n\n\n\n\n\u00a7 DEIT AND RESNET WITH DIFFERENT CAPACITIES\n\n\nTo understand the practical implications of ViT model size for medical image classification, we compare different model capacities of CNNs and ViTs.\n\nSpecifically, we test ResNet-18, ResNet-50, and ResNet-152 against DeiT-T, DeiT-S, and DeiT-B.\nThese models were chosen because they are easily comparable in the number of parameters, memory requirements, and compute.\nThey are also some of simplest, most commonly used, and versatile architectures of their respective type.\nAll models were initialized with ImageNet pretrained weights, and trained following the settings outlined in Section <ref>.\nThe results appear in Table <ref>.\n\n\nBoth CNNs and ViTs seem to benefit similarly from increased model capacity, within the margin of error.\nFor most of the datasets, switching from a small ViT to a bigger model results in minor improvements, with the exception of ISIC 2019 where model size appears to be an important factor for optimal performance.\n\n\n2.5pt \n\n-2.5pt \n\n2.5pt  \n\n-2.5pt\n\n\n\n\n\u00a7 EFFECT OF PATCH SIZE\n\n\nViTs have an additional parameter, patch size, that directly influences the memory and computational requirement.\nWe compare the default 16\u00d716 patches, used in this work, with 8\u00d78 patches, as it has been noted in previous works that reducing the patch size leads to increased performance <cit.>.\n\n\nThe results appear in Table <ref>.\nWe observe that smaller patch sizes result in marginally better performance, though to a lesser degree than has been observed in the natural image domain.\n\n\n\n\n\u00a7 MEAN ATTENDED DISTANCE OF VITS\n\n\nWe compute the attention distance, the average distance in the image across which information is integrated in ViTs.\nAs reported previously (e.g. <cit.>), in early layers, some heads attend to local regions and some to the entire image.\nDeeper into the network, attention becomes more global \u2013 though more quickly for some datasets than for others.\n\n\n\n\n\n\n"}