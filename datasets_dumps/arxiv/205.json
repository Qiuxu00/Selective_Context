{"entry_id": "http://arxiv.org/abs/2303.07072v1", "published": "20230313124815", "title": "A two-stage speaker extraction algorithm under adverse acoustic conditions using a single-microphone", "authors": ["Aviad Eisenberg", "Sharon Gannot", "Shlomo E. Chazan"], "primary_category": "cs.SD", "categories": ["cs.SD", "eess.AS"], "text": "\n\n\n\n\n\n\nSpatial Attention and Syntax Rule Enhanced Tree Decoder for Offline Handwritten Mathematical Expression Recognition\n    Zihao Lin1 Jinrong Li2 Fan Yang1Shuangping Huang13Xu Yang4\n\nJianmin Lin2Ming Yang2\n\n    March 30, 2023\n===================================================================================================================\n\n\n\n\nIn this work, we present a two-stage method for speaker extraction under reverberant and noisy conditions. Given a reference signal of the desired speaker, the clean, but the still reverberant, desired speaker is first extracted from the noisy-mixed signal. In the second stage, the extracted signal is further enhanced by joint dereverberation and residual noise and interference reduction. The proposed architecture comprises two sub-networks, one for the extraction task and the second for the dereverberation task. We present a training strategy for this architecture and show that the performance of the proposed method is on par with other SOTA methods when applied to the WHAMR! dataset. Furthermore, we present a new dataset with more realistic adverse acoustic conditions and show that our method outperforms the competing methods when applied to this dataset as well. \n\n\n\n\nSpeaker extraction, Dereverberation\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nExtracting a desired speaker from a mixture of overlapping speakers using  only a single microphone is a cumbersome task, particularly in noisy and reverberant environments. In this paper, we address this challenge by focusing on the extraction of a single participant from a mixture of two speakers acquired by a single microphone, given a prerecorded utterance of the speaker to be extracted. \n\n\n\n\n\n\n\n\n\nThere has been significant progress in the single-microphone BSS domain in the past years. The Conv-Tasnet <cit.> and the DPRNN <cit.>, are both applied in the time domain with similar encoder-masking-decoder architecture. \n\nOther works that followed this approach were presented\n<cit.>, demonstrating a considerable improvement in the separation results. The SepFormer was introduced in <cit.> leveraging the benefits of the attention layers, which led to a significant improvement in performance and to SOTA results. \n\nAn efficient CNN-based model, denoted Sudo rm-rf, was presented in <cit.> and demonstrated high separation capabilities.\n\n\n\n\nMost of the above-mentioned BSS models were trained and tested on clean and anechoic mixtures. Such acoustic conditions can hardly be met in reality. Several algorithms\u00a0<cit.> were also trained on reverberant data without any changes in their architecture. Cord-Landwehr et al.\u00a0showed in <cit.> that despite the significant improvement achieved in clean conditions, only marginal improvements can be obtained in realistic reverberant and noisy conditions. \n\n\n\n\n\n\n\n\n\nGiven a reference signal of the desired speaker turns the BSS problem into an extraction problem, in which the permutation problem is alleviated. \nThe SpeakerBeam algorithm, introduced in <cit.>, estimates a mask for the desired speaker in the spectral domain using the spectrum of the reference signal. While magnitude-domain processing might be sufficient in clean and anechoic conditions, it might be insufficient in noisy and reverberant conditions. In <cit.>, this model was improved by using the time-domain signal, as it allows the exploitation of the entire  signal information. A similar approach was presented in <cit.>, where the i-vector <cit.> of the reference signal was used as the embedding of the desired speaker. In <cit.>, a multi-task training procedure was proposed in which a speaker classification task is carried out in parallel for improving the embedding of the desired speaker.\n\nTime domain processing, despite the above advantages, ignores the time-frequency patterns typical to speech signals. In our prior work, <cit.>, a fully convolutional Siamse-Unet architecture was proposed. The algorithm is applied in the STFT domain to the Real-Imaginary (RI) representation of the signals while the loss is applied in the time-domain, exploiting the entire signal, on the one hand, and leveraging its spectral patterns, on the other hand. Yet, the performance of this approach is insufficient in adverse acoustic conditions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the current contribution, we present a two-stage algorithm to extract a desired speaker from a mixture of two signals under reverberant and noisy conditions. We split the extraction task into two stages. In the first stage, given the noisy and reverberant mixture and the reference signals, a Siamse-Unet architecture is applied to extract the reverberant desired speaker.\nThe encoders used for both the mixture and the reference signals are identical, thus the resulting outputs have matching dimensions. While the mixture encoder preserves the frame dimensions, which is essential for the mixture processing, the reference encoder aims to exclusively represent the desired speaker's identity while ignoring the content of the utterance. To achieve this outcome, we average the reference embedding over the frame dimension. The reference embedding vector is finally multiplied with each of the frames in the mixture embedding. The outcome of this multiplication is used as an input to the decoder, which in turn extracts the reverberant desired speaker.\n\nWe show that training this stage in an iterative manner is beneficial. \n\nIn the second stage, an additional Unet model is applied to dereverberate and enhance the output of the first stage. Similarly, the encoder output preserves the frame size \nof its input signal. The resulting embedding is multiplied by the embedding of the reference from the first stage. The second decoder is finally applied to extract the desired clean and dereverberated signal.  \n\nFurthermore, in this paper, we introduce a new simulated dataset with more realistic conditions than the WHAMR! dataset, and show that our model outperforms  other SOTA models on both the WHAMR! dataset and the new, more challenging, dataset. \n\n\n\n\n\n\n\u00a7 PROBLEM FORMULATION\n\nThe signal x(t), captured by a single microphone, is a combination of Q concurrent speakers, represented by:\n\n    x(t) = \u2211^Q_q=1{s_q\u2217h_q}(t) + v(t)   t=0,1,\u2026,T-1\n\nwhere s_q(t) is the signal of the qth speaker, h_q(t) is the RIR between the qth speaker position and the microphone position, and v(t) is an additive noise. In a noise-free, non-reverberant environment, h_q(t) is dominated by the first arrival, and v(t)=0 for all q.\n\nIn the STFT domain, the microphone signal can be approximately expressed as:\n\n    x(n,k) = \u2211^Q_q=1 s_q(n,k)  h_q(n,k) + v(n,k)\n\n\nwhere n=0,1,\u2026,N-1 and k=0,1,\u2026,K-1 represent the time-frame and frequency-bin indexes, respectively, and N and K are the total number of time-frames and frequency bands, respectively.\n\nThis paper focuses on the case where there are only two concurrent speakers, namely Q=2, referred to as the desired speaker s_d(n,k) and the interference speaker s_i(n,k). \nThe reverberant desired signal is defined as s\u0303_d(n,k) = s_d(n,k)  h_d(n,k).\nThe reference signal is denoted  s_d^ref(n,k). We aim at the extraction of the desired speaker signal, \u015d_d(n,k), using the mixed signal x(n,k), and a reverberant reference signal, s\u0303_d^ref(n,k) = s_d^ref(n,k) h_d^ref(n,k).\n\n\n\n\n\n\u00a7 PROPOSED MODEL\n\n\n\n\n\n \u00a7.\u00a7 Architecture and Training Procedure\n\nOur model is composed of two sub-stages. The first is a Simase-Unet, which consists of three parts: two  encoders and a decoder. We share weights between the encoders to encourage joint embeddings of both the mixture and the reference signals in the same latent space.\nThe encoder architecture consists of several convolution layers followed by two-dimensional batch normalization and a `Relu' function (similar to the one introduced in <cit.>). Next, we combine the dimensions of the channels and frequencies and employ a fully-connected layer to reduce the dimensions. After this step, we apply a single transformer-encoder layer. The decoder architecture consists of six transformer-encoder layers, followed by FC layer to restore the original dimension. Then transpose-convolution layers are employed to adapt to the convolution layers in the encoder, enabling the application of skip connections as required. A transformer-encoder layer is subsequently applied after all the steps mentioned above.\n\nWe repeat the first stage several times to further enhance the extraction process. In the first iteration, the mixture signal is processed, while in the subsequent iterations, the separated (but still reverberant) signals from the previous iteration are processed. \nFormally, the process can be expressed as:\n\n    Input^(\u2113) =\n        \n          x(n,k)    \u2113=0 \n    \u015d\u0303\u0302_d^(\u2113-1)(n,k)     \u2113>0\n\nwhere \u2113 = 0,...,L-1 is the iteration index.\nBy repeating this process for L iterations, we obtain L estimates of s\u0303_d(n,k), which are all used to train the entire model.\n\n\nThe second stage of the model uses the same architecture as the first stage. Our empirical results showed that using the reveberant reference signal in the second phase can improve the results. Rather than passing the reference signal again through an encoder, we can simply use the learned embedding vector from the first stage.\n\nAlternative ways for integrating the information from the reference signal are described in <cit.>, including concatenation, addition, and multiplication, the latter achieving the best results. To obtain a single vector that represents the speaker's identity, we average across the frame dimensions of the reference embedding, thus ignoring the temporal information and emphasizing the speaker's identity. The final embedding vector is denoted E^ref_d. Unlike <cit.>,  in the Unet architecture, skip connections are only implemented from the mixture encoder and not from the reference encoder. Instead, we only use the output of the last layer of the reference encoder in the bottleneck stage. While most single microphone DNN-based algorithms apply a masking operation to the mixture signal, the proposed scheme is trained to directly estimate the TF representation of the target source.\n\nThe two sub-stages are trained together in an end-to-end manner, while the first stage feeds the second phase with an estimate of the last iteration of the first stage and the reference embedding. A block diagram of the entire model is shown in Fig.\u00a0<ref>.\n\n\n\n \u00a7.\u00a7 Features\n\nIn this work, we adopted the RI components of the STFT as both the input features of the model and its output. The model is trained with the SI-SDR loss function, which is sensitive to phase distortion.  Using the RI features may alleviate such problems (see discussion in <cit.>).  \n\n\n\n\n\n \u00a7.\u00a7 Objectives\n\nAs mentioned above, we  use the SI-SDR loss function to train our model. The loss is formulated as\n\n    SI-SDR( s,\u015d)= 10 log_10( \u27e8\u015d,s\u27e9/\u27e8s,s\u27e9 s^2/\u27e8\u015d,s\u27e9/\u27e8s,s\u27e9 s-\u015d^2).\n\n\nThe model is trained using all output signals, namely, \u015d_d and  \u015d\u0303\u0302_d^(\u2113), \u2113 = 0 , \u2026 , L-1: \n\n\n    \u2112_SISDR_d = \u2211_\u2113=0^L-1SI-SDR( s\u0303_d, \u015d\u0303\u0302_d^(\u2113)) + SI-SDR( s_d,\u015d_d ).\n\n\nFor the extraction task to be successful, the network must be able to learn a unique embedding for each speaker to prevent errors in identifying the correct speaker. To achieve this goal, an additional, triplet loss function, was implemented:\n\n    TRIPLET(a,p,n) = max( cd(a,p) - cd(a,n) + m, 0)\n\nwhere a is the anchor input, p is the positive input and n is the negative input, with p closer to a than n. The function cd(\u00b7) is the cosine distance and m is a margin hyperparameter. The triplet loss function encourages the distance between the anchor and the positive inputs to be smaller than the distance between the anchor and negative inputs, by a margin of at least m.\nIn our case, we would like the embedding of the reverberant reference to be as close as possible to the embedding of the output of the first phase (namely, the estimated desired and reverberant speaker), and as far as possible from the embedding of the reference of the second speaker. In explicit terms:\n\n    \u2112_TRIPLET_d = \n    TRIPLET(E_\u015d\u0303\u0302_d,E^ref_d,E_d^ref )\n\nwhere E_\u015d\u0303\u0302_d is obtained by passing \u015d\u0303\u0302_d_L-1 through the encoder of stage 1 and E_d^ref is the embedding of the reference of the interference signal.\n\nDuring training, we encountered a convergence problem when using both loss functions simultaneously. To address this issue, we implemented a warm-up training procedure in which the network is initially trained using only the SI-SDR loss, and the triplet loss is added at a later stage in the training process. This approach successfully resolved the convergence issues.\n\nIn an effort to improve the training process, we alternated the desired and interference signals within each training batch, while maintaining consistency in the mixture employed. That is, inserting the mixture signal with the reference signal of one of the speakers and then repeating the process with the reference of the other speaker in the same batch, and summing the losses for both speakers. \nIn short, the overall loss function takes the following form:\n\n    \u2112 =  (\u2112_SISDR_d + \u2112_SISDR_i)/2    +   \n    \u03b1\u00b71_warm-up\u00b7 (\u2112_TRIPLET_d + \u2112_TRIPLET_i )/2\n\n\nwhere \u03b1 represents a hyperparameter, and the indicator function 1_warm-up determines the point at which the triplet objective function should be taken into consideration in the training process.\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTAL STUDY\n\n\n\n\n \u00a7.\u00a7 Datasets\n\nWe used the WHAMR! dataset to train our model. This dataset is created by taking the WSJ0-2Mix dataset <cit.> and modifying it by incorporating environmental noise from the WHAM dataset <cit.> and reverberation. \nTo adapt the dataset to the extraction task, we modified it in the following manner. For each speaker included in the mixture, we selected a different utterance and convolve it with the same room impulse response (RIR) used to generate the mixed signal, namely h_d^ref = h_d. This procedure reflects the fact that in a typical conversation, segments in which only a single speaker is active can always be found. However, it is implicitly assumed that the scenario is static, hence that the RIR does not significantly change during the entire conversation. \n\n\nWe note that, according to our tests, the reverberation level in the WHAMR! dataset does not exceed 600 milliseconds, in contradiction to the reported reverberation level, which is in the range of [0.2, 1].[Due to space constraints, we will not give a detailed analysis of the dataset in the current contribution.] \n\nThe dataset includes 20,000 signals for training, 5,000 for validation, and 300 for the test phase, and it uses the `min' and `8k' sampling rate configuration. (With `min' setting the longer target is truncated to match the length of the shorter target.)\n\nIn addition to WHAMR!, we generated a new dataset for the purpose of enriching the data. This is equivalent to dynamic mixing training, which randomly generates the mixture from the existing speakers during training. We also took speakers from the WSJ0 corpus, along with noise from the WHAM and the reverberation generated from an RIR generator <cit.> with parameters listed in Table\u00a0<ref>. \n\nDuring training, each signal is truncated to a variable length between 2 to 5 seconds. Since we are using a Siamese architecture, the mixture and the reference signal must have the same length. If the reference signal is longer, it will be truncated, and if it is shorter, it will be duplicated until it is the same length as the mixture.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Algorithm Settings\n\nThe frame-size of the STFT is  256 samples with 50% overlap.  Due to the symmetry of the DFT only the first half of the frequency bins are used. The value of \u03b1 was empirically set to 2, emphasizing the triplet loss due to the significant difference in scales between the two objective functions. The triplet loss margin was set to m=0.5.\n\nThe number of iterations for the first phase was chosen as L=2, because there was minimal improvement when increasing the number from 2 to 3 iterations, while a noticeable improvement was observed between 2 iterations to no iterations, L=1.\n\n\nIn the training procedure, we used the Adam optimizer\u00a0<cit.>. The learning rate was set to 0.001 and the training batch size to 6. The weights are randomly initialized, and the lengths of the signals were randomly changed at each batch.  \n\n\n\n\n \u00a7.\u00a7 Evaluation Measures\n\nTo evaluate the proposed algorithm we use five evaluation measures: SI-SDR, SIR, SDR, STOI, and PESQ. While the first three are used as a measurement of the quality of the speaker separation, the last two give an indication of the audio intelligibility and quality. \n\nThe proposed algorithm is compared to the current SOTA separation methods, i.e., the Sepformer <cit.> and the Sudo rm-rf <cit.>. These are time-domain blind source separation masking-based methods. We decided to compare our method with separation methods rather than extraction methods since these are the most effective methods in the field. \n\n\n\n\n\n \u00a7.\u00a7 Results\n\nThe results for the WHAMR! dataset are depicted in Table <ref>. Our model achieves an SI-SDR of 9.67\u00a0dB, SDR of 10.88\u00a0dB, and SIR of 24.2\u00a0dB. It is evident that our proposed method outperforms the SOTA methods in almost all measures. In addition, the method also achieves the best scores for the intelligibility measure (STOI) and the quality measure (PESQ), with scores 92% and 2.72, respectively.\n\nThe new dataset imposes a greater challenge on the extraction algorithm, as evidenced by the lower scores in Table\u00a0<ref> for all measures, compared to the scores obtained on the WHAMR! dataset, as reported in Table\u00a0<ref>. While the absolute separation results obtained for the new dataset are lower, the improvement in terms of SI-SDR is 14.2\u00a0dB, which is very high and significantly outperforms the competing methods. The intelligibility results (90.2%) are on par with the results obtained for the WHAMR! dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Ablation Study\n\nWe present an ablation study for our model. We examined four different configurations:\n\n\n\n\n     \n  * One iteration in the first stage. The loss function for the desired source is given by:\n\n    \u2112_SISDR_d = SI-SDR( s\u0303_d, \u015d\u0303\u0302_d^(L-1)) + SI-SDR( s_d,\u015d_d )\n\n      with L = 1, and the overall loss is given by\n      \u2112 =  (\u2112_SISDR_d + \u2112_SISDR_i)/2.\n      Triplet loss is not applied.\n     \n  * Two iterations in the first stage. The SI-SDR loss is only applied to the final output \u015d\u0303\u0302_d^(L-1), i.e. L=2 in (<ref>). Triplet loss is not applied. \n     \n  * The SI-SDR loss is applied to all intermediate results \u2113=0,\u2026, L-1, as in (<ref>), with L=2.\n     \n     Triplet loss is not applied. \n     \n     \n     \n  * The full implementation of the proposed model with all its components active.\n\n\n\nTable\u00a0<ref> depicts the breakdown of the results for the WHAMR! and the new datasets. It is evident that each additional component enhances the quality of the network output for both datasets. In total, the SI-SDR measure improved from 8.62\u00a0dB to 9.67\u00a0dB for the WHAMR! dataset and from 5.45\u00a0dB to 6.21\u00a0dB for the new dataset. Respectively, STOI improved from 90.4% to 92% for WHAMR!, and from 88% to 90.2% for the new dataset\n\n\nTraining the model to accurately identify the intended speaker from a mixture is challenging in speaker extraction, particularly in reverberant conditions and when the speakers have similar voices. This may result in the extraction of the incorrect speaker or a permutation between the output signals.\n\nTo address this issue, the triplet loss was added. Our experiments showed that the addition of the triplet loss alleviated such permutation problems.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSIONS\n\nWe have proposed a two-stage approach for speaker extraction under reverberant conditions. The first stage separates the desired and yet reverberated speaker, while the second stage reduces reverberation and further enhances separation quality. Our results indicate that our model performs comparably or better than current state-of-the-art separation methods, with the added benefits of faster and more consistent training. Furthermore, an ablation study identifies the role of the various components in improving performance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n IEEEtran\n\n"}