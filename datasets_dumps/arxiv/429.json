{"entry_id": "http://arxiv.org/abs/2303.06720v1", "published": "20230312175604", "title": "QTrail-DB: A Query Processing Engine for Imperfect Databases with Evolving Qualities", "authors": ["Maha Asiri", "Mohamed Y. Eltabakh"], "primary_category": "cs.DB", "categories": ["cs.DB"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n Imperfect databases are very common in many applications due to various reasons \nranging from data-entry errors,  transmission or integration errors, and wrong instruments' readings,\n to faulty experimental setups  leading to incorrect results.\nThe management and query processing of imperfect databases \nis a very challenging problem as it requires incorporating the data's qualities within the database engine.\nEven more challenging, the qualities are typically not static and may evolve over time. \nUnfortunately, most of the state-of-art techniques deal with the data quality problem as \nan offline task that is in total isolation of the \nquery processing engine  (carried out outside the DBMS). \nHence, end-users will receive the queries' results \nwith no clue on whether or not the results can be trusted for further analysis and decision making. In this paper, we propose the it\u201c\u201d system that fundamentally extends the \nstandard DBMSs to support imperfect databases with evolving qualities. \nintroduces a new quality model based on the new concept of \n\u201cQuality Trails\u201d, which captures the evolution of the data's qualities over time. \nextends the relational data model to incorporate the quality trails within the database system. \nWe propose a new query algebra, called \u201cQTrail Algebra\u201d, \nthat enables seamless and transparent propagation and derivations \nof the data's qualities within a query pipeline. As a result, a query's answer \nwill be automatically annotated with quality-related information at the tuple level. \npropagation model leverages the \nthoroughly-studied propagation semantics\npresent in the DB provenance and lineage tracking literature, \nand thus there is no need for developing a new query optimizer.\nis developed within PostgreSQL and experimentally evaluated using \nreal-world datasets to demonstrate its efficiency and practicality. \n\nQTrail-DB: A  Query Processing Engine for Imperfect Databases with Evolving Qualities\n    Maha Asiri,  Mohamed Y. Eltabakh\nComputer Science Department, Worcester Polytechnic Institute (WPI), MA, USA.\n{mmasiri,\u00a0meltabakh}@wpi.edu\n\n    March 30, 2023\n===============================================================================================================================================\n\n \n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\n\nIn most modern applications it is almost a fact  that the working databases may  not  be perfect and may contain \nlow-quality data records <cit.>.\nThe presence of such low-quality data is due to many reasons including missing or wrong values, \nredundant and conflicting information from multiple sources, human errors in data entry, \nmachine and network transmission errors, or even wrong assumptions or instruments' calibration \nduring scientific experimentations that lead to inaccurate results. \nA recent science survey has revealed that 80.3% of the participant research and scientific groups \nhave admitted that their working databases contain records of low quality, which puts their analysis \nand explorations at risk <cit.>. \nMoreover, it has been reported in <cit.> that wrong decisions and uninformed analysis resulting from imperfect databases cost US businesses around 600 billion dollars each year. \n\n\n\n\n\nEven more challenging, the qualities of the data tuples are typically not static, \ninstead they may change over time (evolve) depending on various events taking place in the database. \nThe emerging scientific applications are  excellent examples in which tracking and maintaining \nthe data's qualities is of utmost importance\u00a0<cit.>.\n For example, Figure\u00a0<ref>  illustrates  a possible sequence of operations that may take place \nin biological databases.  \nFirst, a data tuple r (e.g., a gene tuple) can be imported from an external source to the local database. \nAt that time, r would be assigned an initial quality score depending on the source's credibility.  \nThen, a scientist may insert a comment highlighting a possible  error in the tuple (e.g., the gene's start position does not seem correct), \nbased on which r's quality should be decreased. \nAfter a while, a verification step that compares the local data with an external repository \nmay confirm that r contains an incorrect value, which will further decrease r's quality. \nSubsequent actions in the database may either increase or decrease r's quality over time, e.g., \nSteps 4 and 5 in Figure\u00a0<ref>, which are an update operation on r (e.g., correcting the gene's start position), and \nthe addition of a  scientific article matching r's new content, respectively, \nshould both enhance r's quality.   \nIn general, each tuple in the database may have its quality and trustworthy changing over time \nbased on different operations taking place in the database.  \n\nIn such imperfect databases with dynamic and evolving qualities over time,\nthe standard query processing that treats all tuples the same while ignoring their qualities \nis indeed a very limited approach. For example, \nseveral interesting and challenging questions may arise beyond the standard data querying, which include:\n\n\n    \n  * What was the quality of tuple r before the last revision? \n    \n  * Why r's quality has drastically dropped at time t, and what did we do to fix that?\n    \n  * Given my complex query, e.g., involving selection, joins, grouping and aggregation, and set operators, what is quality of each output tuple? Can I trust the results and build further analysis on them or not? \n    \n  * Given my query, how to execute it  on only the high-quality tuples, e.g., the quality is above a certain threshold? How to join, select, or order the tuples based on their qualities?\n    \n  * Among the low-quality tuples in the database, which ones are more important, e.g., frequently participate in queries' answers,  to investigate first?\n\n\n\n\nCertainly, supporting these types of  questions is of critical importance to end-users and high-level applications. \nOn one hand, without modeling and keeping tracking of the quality information in a systematic way, crucial \ninformation will be lost.  \nOn the other hand, without assessing the quality of the output results, \nscientists and decision makers will become less confident about the obtained results.\nAnd hence, building any further analysis on low-quality data may not \nonly lead to wrong decisions, but also result in wasting scientists' efforts, resources, and budgets\u00a0<cit.>. \n\n\n\n\nIt is clear that supporting these types of questions warrants the need for fundamental changes in the underlying DBMS. \nIn this paper, we propose the \u201c\u201d system, \nan advanced query processing engine for imperfect databases with evolving qualities.  \nWe identify three major tasks to be addressed, which are:\n\n\n\n \u00a7.\u00a7 Task 1-Systematic Modeling of Evolving Qualities\n\nWith the large scale of modern databases, even a very small percentage of low-quality data may translate to a very large number of low-quality records. This makes it very challenging and time-consuming process to identify, isolate, or fix these records instantaneously. Therefore, the underlying database engine must be able to capture and model the data qualities in a systematic way, and also keep track of their evolution over time, e.g., when and why the quality changes (Refer to the aforementioned Questions 1 & 2).  \n\n\n \u00a7.\u00a7 Task 2-Quality Propagation and Assessment of Query Results\n\t\t\nIt is impractical to assume that applications can freeze their working databases until all records have been fixed, and then enable them for querying. It is a continuous process of collecting and generating data of various degrees of qualities\u2014with possible interleaving of offline efforts to verify and fix the imperfect tuples. Therefore, it is unavoidable to query the data while having  tuples of different qualities. Hence, the query processing engine must be extended to manipulate not only the data values but also their associated qualities. Each tuple r in the output results should have an inferred and derived quality based on input tuples contributed to r's computation (Refer to the aforementioned Question 3). \n\t\n\n\n \u00a7.\u00a7 Task 3-Quality-Driven Processing and Curation\n\nAnother important type of processing\u2014beyond only propagating and deriving the output's quality\u2014is the ability to query the data based on their qualities, i.e., quality-driven processing. \nThis includes the ability to, for example, select, join, or order the data tuples based on their qualities, and possibly combine such quality-driven processing with the standard query operators in a single query plan (Refer to the aforementioned Question 4). Another type of analytics is the quality-driven curation in which end-users may want to, for example, track how low-quality tuples affect queries' results, or  rank the low-quality tuples according to their participation in queries for investigation and fixing purposes (Refer to the aforementioned Question 5).\tEnabling this type of quality-driven processing mandates core changes in the database engine.  \n\t\t\n\nIn this paper, we will focus on the first two tasks (Tasks 1 & 2), while deferring Task 3 to future work. \n\nproposes a full integration of the data's qualities into all layers of a DBMS. This integration includes introducing a new quality model that captures the evolving qualities of each data tuple over time, called a \u201cQuality Trail\u201d, \nextending the relational data model to encompass the quality trails, and proposing a new relational algebra, called \u201cQTrail Algebra\u201d, that enables seamless and transparent propagation and derivations of the data's qualities within a query pipeline. \nHence, each output tuple from a complex query plan will be annotated with its derived and inferred quality based on the contributing input tuples. Moreover, we show that extensions do not alter the query optimization rules, and hence the standard optimizers can be still used.  \n\n\n\n\n\n\n\n\n\n\n \n\t\nThe key contributions of this paper are summarized as follows:\n\n\n    \n  * Proposing the \u201c\u201d system that treats data's qualities as\n\tan integral component within relational databases. In contrast to existing related work, e.g., the  offline quality management, and the generic provenance tracking systems, is the first to quantify and model the data's qualities, and fully integrate them within the data processing cycle. (Section\u00a0<ref>)\n \n    \n  * Introducing a new quality model based on the new concept of \u201cquality trails\u201d that captures the evolving quality history of each data tuple over time. The new model enables advanced processing and querying over the quality trails, which is not possible in the  traditional single-score quality models. (Section\u00a0<ref>)\n    \n    \n  * Proposing a new query algebra, called \u201cQTrail Algebra\u201d that extends the semantics of the standard query operators to manipulate and propagate the quality trails in a pipelined and transparent fashion. Each operator annotates each of its output tuples with a quality trail that is inferred and derived from the contributing input tuples. We also propose new quality-specific operators and integrate them within the other operators. (Section\u00a0<ref>)\n\n    \n  * Studying the viability of leveraging the powerful query optimizers of relational databases into . We prove that under the new QTrail Algebra and by following the propagation semantics of provenance-based techniques, the standard equivalence and transformation rules are still applicable. (Section\u00a0<ref>)\n\n    \n  * Developing the prototype system within the PostgreSQL engine, and evaluating its performance using real-world biological datasets. Given the new value-added features, the results show acceptable overheads of compared to the standard query execution of traditional databases. (Sections\u00a0<ref> and\u00a0<ref>)\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\nDue to its critical importance, data quality has been extensively studied in literature. \nThe most related to our work are the following.\n\n\n\n \u00a7.\u00a7 Cleaning and Repairing Technique\n\n\nA main thread of research is on data cleaning, repairing, and cleansing, where potential low-quality data records are identified, and then fixed\u00a0<cit.>. The underlying techniques in these system vary significantly from fully-automated heuristics-based techniques\u00a0<cit.>, comparison-based with external sources and repositories\u00a0<cit.>, and rule-driven techniques\u00a0<cit.>, to human-in-the-loop mechanisms\u00a0<cit.>. With the variety of algorithms and techniques for data cleaning, several extensible and generic frameworks have been proposed to integrate these algorithms, e.g.,\u00a0<cit.>. The common theme in all of these systems is that they all work offline and in total isolation  from query processing. And since the data is evolving and growing rapidly in all modern applications, the repairing task is a never-ending time-consuming process. \n\nTherefore, it is inevitable that the data will be subject to querying, analysis, and decision making, while it contains low-quality records. Unfortunately, during query processing, none of the above techniques can provide any support for assessing the \nquality of the results or enabling quality-aware processing. Even worse, if these techniques have identified potential erroneous records and marked them as pending verification or fixing\u2014which may take long time to complete, there is no mechanism to integrate such observations into query processing.\n\n\n\n \u00a7.\u00a7 Quality Assessment Techniques\n\n\nOn the other hand, very little attention is given to quality assessment at query time. It has beed addressed in the context of mining operations\u00a0<cit.>, sensor data\u00a0<cit.>, and relational databases\u00a0<cit.>. The core of  these techniques is based on statistical assumptions about the underlying data, e.g., defining statistical measures such as  completeness, soundness, and probability of error. And then, each technique studies its domain-specific operations and how they affect the statistical measures.\n\n\n\n\n\nA major limitation in these systems is that the assumed statistics may not be available in many applications. For example, the work in\u00a0<cit.>\u2014which is the most related to \u2014assume that the probability of error in each column in the database is known in advance, which is not the case in many applications. And even if this knowledge is available, it a coarse-grained knowledge over an entire column and not tied to specific tuples, e.g., 1% error rate in column X means that among every 100 values in X, it is expected to find 1 error. Consequently, the estimated output quality is also coarse-grained and cannot be linked to specific tuples. Moreover, these systems assume a single-score quality model without taking into account the fact that data records are long-lived and their qualities evolve over time.  \n\n\n\n \nis fundamentally different from the above mentioned two categories in that: (1)\u00a0It proposes a more rich quality model based on the quality trails instead of the single-score quality model, (2)\u00a0It does not put any assumptions on the data's characteristics, e.g., estimated error rate, instead the quality trails will be incrementally created and maintained as the database evolves over time, and (3)\u00a0Its quality model is fully integrated within the query processing engine.  \n\nis complementary to the  cleaning and repairing techniques in that they together can provide\na more comprehensive solution that combines the online quality-aware query processing, and the offline repairing process, respectively. \n\n\n\n\n\n\n \u00a7.\u00a7 Uncertain and Probabilistic Databases\n \n\nAnother big area of research is focusing on uncertain and probabilistic databases\u00a0<cit.>. In these systems, a given data value (or an entire tuple) can be uncertain, and hence it is represented by a possible set of values, a probability distribution function over a given range, or a probability of actual presence. In  uncertain databases, the query engine is extended to operate on these uncertain values and tuples, and enforce correct semantics (called \u201cpossible worlds\u201d).\n\nAlthough uncertainty is related to data qualities in some sense, these systems are fundamentally different from since the notion of \u201cquality\u201d is not part of these systems. Therefore, the uncertain and probabilistic DBs can neither model or keep track of the data's qualities, nor enable advanced quality-driven query processing as proposed by the system.\t\n\n\n\n \u00a7.\u00a7 Data Lineage and Provenance\n \n\nData provenance is directly related to data quality since the tuples' qualities are certainly based on their provenance. Several systems have addressed the derivation and propagation of the provenance information, e.g.,\u00a0<cit.>, and even some systems such as Trio\u00a0<cit.> have combined the uncertainty with the provenance. \n\nleverages the  thoroughly-studied  theoretical foundations and propagation semantics present in the provenance literature. However, there is still a big gap between capturing the raw provenance information, and the stage in which this information translates to quality-based knowledge and analytics.  For example, \n\n\n    \n  * Provenance systems can report the roots of each output tuple r\u2014which may include 100s of other tuples' Ids\u2014but they cannot inform scientists whether or not the results are of high quality and worthy of being used in further analysis. Currently, obtaining this knowledge involves a tremendous unrealistic manual effort  if at all possible.\n    \n    \n  * Provenance systems do not provide quantifiable measures on which queries can interact, i.e., lineage information are usually opaque objects with no easy way to apply conditions or transformations on. And\n    \n    \n  * Provenance systems do not capture the history (or evolution) of the data tuples, and thus executing the same query Q at times t_1 and t_2 would return the same provenance information even if some changes between t_1 and t_2 have altered the records' qualities without changing their content, e.g., curation information is added that increase the virtue of the records. \n\n\n\naddresses these issues and fills in this gap. \n\n\n\t\n\n\n\n\n\n\n\n\n\n\n\t \n\n\n\n\u00a7 QTRAIL-DB DATA & QUALITY MODELS\n\n\n\n\nhas an extended data model, where each data tuple carries\u2014in addition to its data values\u2014a \u201cquality trail\u201d \nencoding the evolving quality of this tuple. More formally, for a given relation R having n data attributes, \neach data tuple r \u2208 R has the schema of: r = \u27e8 v_1, v_2, ..., v_n,  \ud835\udcac_r\u27e9, where v_1, v_2, ..., v_n are the data values of r, and \ud835\udcac_r is r's quality trail.  \ud835\udcac_r is a vector in the form of \ud835\udcac_r =  \u27e8 q_1, q_2, ..., q_z \u27e9, where each point q_i is a quality transition defined as follows.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n A quality transition represents a change in a tuple's quality and it \n\tconsists of  a 4-ary vector \u27e8score, timestamp, triggeringEvent, statistics\u27e9,  where  \u201cscore\u201d is a quality score ranging between 1 (the lowest quality) and MaxQuality (the highest quality), \u201ctimestamp\u201d is the time at which the score becomes applicable, \u201ctriggeringEvent\u201d is a text field describing the event that triggered this quality transition,\tand \u201cstatistics\u201d field contains various statistics that will be maintained and updated during query processing.Only \u201cscore\u201d, and \u201ctimestamp\u201d are mandatory fields, while  \u201ctriggeringEvent\u201d, and \u201cstatistics\u201d are optional fields.\n\n\n\nSince r's quality is evolving over time, the length of  \ud835\udcac_r's vector is also increasing over \ntime by the addition of new transitions (Refer to  Figure\u00a0<ref>). The quality trail is formally defined as follows.\n\n  \n \n\n\n\n\n \n\n\n\n\n A quality trail of a given tuple r \u2208 R is denoted as \ud835\udcac_r and is represented as a vector of quality transitions. The transitions in \ud835\udcac_r are chronologically ordered, i.e., \ud835\udcac_r[i].timestamp < \ud835\udcac_r[i+1].timestamp\u00a0\u2200\u00a0i. Moreover, the quality transitins have a stepwise changing pattern, \n i.e., \ud835\udcac_r[i] is the valid transition over the time period [\ud835\udcac_r[i].timestamp, \ud835\udcac_r[i+1].timestamp). \n\n\n\n\n\n\nReferring to the data tuple r from Figure\u00a0<ref>, its corresponding quality trail is depicted in  Figure\u00a0<ref>. With each of the actions highlighted in Figure\u00a0<ref>, r's quality trail will change (evolve) from the L.H.S (the insertion time) to the R.H.S (the current time).  Each point in the quality trail is a quality transition. For example, at time t_4, a new quality transition is added to the trail consisting of: \u27e8 4, t_4, \u201cupdating\u00a0a\u00a0wrong\u00a0value\", {...}\u27e9. This transition remains  valid  (the most recent one) until time t_5 when a new transition is added. The statistics field and its usage will be discussed in more detail in Section\u00a0<ref>. \n  \n\n\n\n\u00a7 QUALITY PROPAGATION AND ASSESSMENT OF QUERY RESULTS\n\n\n\n\nIn this section, we present the extended query processing engine of for propagating the quality trails within a query plan. To enable such derivation and propagation in a transparent and pipelined way, we propose a new SQL algebra, called \u201cQTrail Algebra\", in which the standard query operators have been extended to seamlessly manipulate the quality trails associated with each tuple. In , each operator will consume and produce tuples conforming to the data model presented in Section\u00a0<ref>. In this section, we assume the quality trails have been created and maintained (The focus of Section\u00a0<ref>), and thus we will focus now on the query-time propagation.  \n\nFortunately, in the provenance literature, the propagation semantics of the tuples' lineage is a well studied problem under the \ndifferent operators. In specific, we use the same semantics as in the Trio system\u00a0<cit.>. Therefore, after each algebraic transformation, we can track  the  input tuples contributing to a specific output tuple  without the need for re-inventing the wheel.  Yet, the unsolved  challenge is how to translate this knowledge to derivations over the quality trails? For example, assume that two tuples r_1 and r_2 will join together to produce an output tuple r_o, and from the existing literature, it is established that both r_1 and r_2 form the lineage of r_o. The remaining question is that given the quality trails of r_1 and r_2, what will be the derived quality trail of r_o? In the following, we study the semantics of deriving the quality trails of each output record from its contributing input records.\n\n\n\n \u00a7.\u00a7 Selection Operator (\u03c3_p(R))\n\n\nThe operator applies data-based selection predicates p over relation R, and reports the qualifying tuples. Predicates p reference only the data values v_1, v_2, ..., v_n within the tuples. The extension to the selection operator is straightforward since the content of the qualifying tuples do not change, and thus the output quality trails remain unchanged. The algebraic expression is:\n\u03c3_p(R) = {r = \u27e8 v_1, v_2, ..., v_n, \ud835\udcac_r\u27e9\u2208 R\u00a0|\u00a0 p(r) = True } \n\n\n\n \u00a7.\u00a7 Projection Operator \u03c0_a_1, a_2, ..., a_n(R)\n\n\nIn , the quality trails are at the tuple level, and not tied to specific attribute(s) within the tuple. Therefore, the projection operator will not change the quality of its input tuples. \n\n\n\nThat is:\n\u03c0_a_1, a_2, ..., a_n(R) = {r' = \u27e8 a_1, a_2, ..., a_n, \ud835\udcac_r\u27e9}\u00a0\u2200\u00a0r \u2208 R. \n    \n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Merge Operator (\u03a9(\ud835\udcac_1, \ud835\udcac_2, ...))\n\n\nSeveral of the relational operators, e.g., join, grouping, aggregation, among others, involve merging (or combining) multiple input tuples together to form one output tuple. Therefore, the corresponding input quality trails  may also need to be merged and combined together. To perform this merge operation over  quality trails, we introduce the new Merge operator \u03a9(\ud835\udcac_1, \ud835\udcac_2, ...). This operator  is not a physical operator, instead it is a logical operator that executes within other physical operators, e.g., join, grouping, and duplicate elimination.\n\nThe Merge operator's logic is presented in Figure\u00a0<ref>, and its functionality is illustrated using the example in Figure\u00a0<ref>. Assume combining three tuples r_1, r_2, and r_3 having quality trails  \ud835\udcac_r1, \ud835\udcac_r2, and \ud835\udcac_r3, respectively. All quality trails are typically aligned from the R.H.S (which is the query time Q_t), i.e., each quality trial must have a valid transition at time Q_t. However, the trails are not necessarily aligned from the L.H.S since the data tuples may be inserted into the database at different times (See Figure\u00a0<ref>). The quality trail of the output tuple  \ud835\udcac_o is derived using a sweep line algorithm over the input quality trails  starting from left to right and jumping over the transition points as illustrated in Figure\u00a0<ref> (Lines 1-3 in Figure\u00a0<ref>). The basic idea behind the algorithm is that the quality of the output tuple at any given point in time t  should be the lowest among the qualities of the contributing tuples at time t. This is based on the intuition that low-quality inputs produce low-quality outputs, and that an output tuple should have a high quality at time t only if all its contributing input tuples have high qualities at t.  \n\nReferring to the example in Figure\u00a0<ref>, the sweep line starts at Position 1, where  only \ud835\udcac_r1 exists and has a quality level 4-star, which will be produced in the output. The line then jumps to Position 2, where \ud835\udcac_r3 starts participating with a  quality level 3-star, and hence a 3-star transition will be added to \ud835\udcac_o. The sweep line keeps moving to the subsequent positions,  and at each position, it calculates the lowest quality score among the input participants to be the output's quality score at this position (Lines 5-7 in Figure\u00a0<ref>). For example, referring to the example in Figure\u00a0<ref>(a), at time t_4, the contributing input qualities from \ud835\udcac_r1, \ud835\udcac_r2, and \ud835\udcac_r3, are 2-star, 3-star, and 5-star, and thus the corresponding quality transition on \ud835\udcac_o will have a 2-star score. \n\nAlthough \ud835\udcac_o's quality scores reflect only  the lowest score among the input values,  the statistics field associated with each quality transition is intended to provide deeper insights on the other values contributing to the score. Initially, the statistics associated with each quality transition, e.g., Min, Max, Avg, are set to the transition's score value as illustrated in Figure\u00a0<ref>. And then, as the transitions get merged, new statistics are computed and get attached to the new quality transition. For example, the sweep line at Position 6 encounters scores 4-star, 2-star, and 1-star transitions along with their initial statistics. Notice that \ud835\udcac_r2's active transition  at Position 6 is still the 2-star transition occurred at Position 5 (at time t_5). These statistics will be combined by the Merge operator to compute the new statistics of the \ud835\udcac_o's new transition (Line 8 in Figure\u00a0<ref>). The example illustrates maintaining the Min, Max, and Avg statistics. However, in general, any type of aggregates that can be incrementally computed under the addition of new values, e.g., aggregates that are algebraic, e.g.,  Avg() and Stddev(), or distributive under insertion, e.g., Sum(), Count(), Min(), Max() can be supported\u00a0<cit.>[Algebraic aggregates are those that need small or constant extra storage in order to be computed incrementally. Distributive-under-insertion aggregates are those that can be incrementally computed without extra storage when new data is added.]. \n\nFinally, the newly created transition is added to the output quality trail (Line 10 in Figure\u00a0<ref>).\n\n\n\n \u00a7.\u00a7 Theta Join Operator (R _p S)\n\n\nGiven two tuples r \u2208 R and s \u2208 S having quality trails  \ud835\udcac_r, and \ud835\udcac_s, respectively. If r and s qualify for the data-based join predicate p and produce a joint tuple z, then z will inherit the merged qualities of its two components r and s. The same intuition of the Merge operator applies where z's quality at any given point in time t should be the lowest among the contribution tuples. The algebraic expression is: \nR _p S = {\u27e8 r.v_1, ..., r.v_n, s.v'_1, ..., s.v'_m,\u00a0\u03a9(\ud835\udcac_r, \ud835\udcac_s) \u27e9\u00a0|\nr \u2208 R, s \u2208 S, and\u00a0p(r, s) = True }\n\nThe Cartesian Product (R\u00a0X\u00a0S) and Natural Join (R  S) operators follow the same algebraic expression as the Theta Join operator. The only difference is that p is True for the  Cartesian Product operator, whereas p consists of equality predicates on the common attributes for the Natural Join operator.\n\nThe Outer Join operators (Left, Right, and Full outer joins) are direct extensions to the above-mentioned inner join operators. Without loss of generality, assume a tuple r \u2208 R will be in the output without a counterpart joining tuple from S, i.e., S's values will be nulls, then r will inherit its own quality trail. That is, r's corresponding output tuple  will be: \n\n{\u27e8 r.v_1, ..., r.v_n, null, ...,null,\u00a0\ud835\udcac_r\u27e9},\u00a0\u2200\u00a0r \u2208 R,\u00a0\u2204\u00a0s \u2208 S\u00a0|\u00a0p(r, s) = True\n\n\n\n\t \n\n\n\n\n \u00a7.\u00a7 Grouping and Aggregation (\u03b3_a_1, a_2, ..., a_m, F_1(\u03b2_1), ...F_z(\u03b2_z)(R))\n\n\n\nGrouping and aggregation are among the most interesting and challenging operators in .\tA key challenge lies in that in order to accurately infer the qualities of the output tuples, we need to keep track of the actual tuples participating in the aggregation. For example, for some aggregation functions only few tuples within each group  influence the output values. Referring to Figure\u00a0<ref>, assume that we group tuples r_1, r_2, and r_3 based on column \u03b2 and aggregate over column \u03b1. The three illustrated queries in Figure\u00a0<ref>(a) compute different aggregation functions, i.e., Sum(), Max(), and { Min() & Max()}, respectively. Clearly, for the Sum() function all of the tuples  r_1, r_2, and r_3 have contributed to the output tuple.In contrast, for the Max() function, only r_1 contributed to the output, while in Query 3 computing both the Min() and Max(), both r_1, and r_3 have contributed to the output.\n\nIn , we support two semantics, which are the coarse-grained provenance-based semantics, and a new fine-grained semantics. In the coarse-grained semantics the aggregation functions are treated as black boxes, and hence all tuples within each group are assumed to participate in generating the output (black-box aggregation-unaware approach). In this case, all corresponding quality trails within each group will be merged together to derive the output's quality. This is the approach used in provenance-tracking Trio system\u00a0<cit.>. Although straightforward, this semantics is coarse grained and pessimistic, and may lead to under estimating the  outputs' qualities unnecessarily. Recall that, according to the Merge operator, as more quality trails are merged, the derived quality may only decrease and can never increase.  As these pessimistic derivations accumulate within a query plan, the final results may become useless and cannot be trusted because of its imprecisely-derived low quality.\n\nimplements supports also another open-box aggregation-aware semantics that identifies the minimal set of input tuples affecting the produced aggregation, and only those tuples are used to derive the output's quality. For example, as illustrated in Figure\u00a0<ref>(a), the different queries  will produce  different associated quality trails depending on the involved aggregation function, e.g., in Query 2, the output tuple has an assigned \ud835\udcac_r1\t quality trail since only tuple r1 was involved. The merit of this semantics lies in avoiding unnecessary degradation of the output's qualities, and thus increasing the worthiness and usage of the produced results. However, implementing the open-box semantics requires extending the communication mechanism between the aggregation functions and the grouping operator as described next. \n\nIn general, any aggregation function in \u2014which relies on PostgreSQL DBMS\u00a0<cit.>\u2014has three basic methods, i.e., Initialize(), Iterative(), and Finalize() (See Figure\u00a0<ref>(b)). The Initialize() function executes once before streaming the input records, Iterative()  is called for each input record to update the aggregator's internal state, and Finalize() executes once after all tuples have been processed to finalize the aggregator's output value. The proposed extension involves maintaining a character array, called contributionArray, by each aggregator function. And then, for each input tuple and based on the aggregator's semantics, the function decides on whether the tuple: (1)\u00a0Certainly contributes to the output, i.e., adding \u201c+\u201d to the array, (2)\u00a0Certainly does not contribute to the output, i.e., adding \u201c-\u201d to the array, or (3)\u00a0The tuple is in-doubt and it is not clear yet,  i.e., adding \u201c?\u201d to the array (See the R.H.S Iterative() method in Figure\u00a0<ref>(b)). In addition to accumulating each  tuple's status in the  contributionArray, this status is also returned to the grouping operator for an incremental update as we will discuss in sequel. Finally, the Finalize() method will return the  contributionArray to the grouping operator as illustrated in the figure.\n\nWhile the aggregator function is  manipulating the contributionArray, enforces two invariants that the grouping operator will depend on to ensure correctness, which are (1)\u00a0The \u201c+\u201d and \u201c-\u201d values are permanent and cannot be altered, and (2)\u00a0By the time of returning the contributionArray from the aggregator's Finalize() function, the array  must not have any in-doubt tuples, i.e., any \u201c?\u201d entires must be converted to either \u201c+\u201d or \u201c-\u201d. This is logical since the execution of the Finalize() method means that the aggregator has seen all its input tuples and knows exactly the ones actually contributed to the output. \n\nThe grouping operator on the other hand\u2014which may call multiple aggregation functions within a single query\u2014maintains a global state across the different aggregation functions (See L.H.S of Figure\u00a0<ref>(b)). More specifically, the operator maintains a character array, called globalArray_G for each constructed  group G. And then, for a given input tuple r belonging to G, if all aggregation functions return a \u201c-\u201d status, then r's quality trail \ud835\udcac_r is immediately purged and \u201c-\u201d  will be augmented to globalArray_G. This is because r is certainly not contributing to G's output. In contrast, if any aggregation function returns a \u201c+\u201d status, then \ud835\udcac_r is immediately merged into the quality trail of G's output  \ud835\udcac_G. Otherwise, r's status is not decided yet, and hence a \u201c?\u201d will be augmented to globalArray_G and \ud835\udcac_r will be buffered by the grouping operator (See the Grouping operator's Iterative method in Figure\u00a0<ref>(b)). Finally, in the operator's Finalize() method, after all aggregation functions complete their work, the grouping operator will check the final status of the in-doubt tuples, say r. If any aggregation function alters its \u201c?\u201d status to \u201c+\u201d, then the r's quality trail will be retrieved from the buffer and merged into  \ud835\udcac_G, otherwise r's quality trail will be dropped.\n \n\t\nThe proposed extension of the grouping operator is internal to the database system and entirely transparent to the outside world. For the aggregation functions, we have extended the built-in functions, e.g., Min(), Max(), and Avg(), to implement the new semantics. For new user-defined aggregation functions, the DB developer needs to implement the manipulation of the  contributionArray according to the function's semantics. In most aggregation functions that we have extended, \tthis manipulation requires minimal effort, e.g., less than  10 additional lines of code. In return, the effect on the accuracy of the inferred qualities can be significant [In the worst case, an aggregation function may blindly return \u201c+\u201d for each input, which will revert back to the black-box semantics.]. \n\t\n\n\n\n \u00a7.\u00a7 Grouping Under Memory Constraints\n\n\nAs presented in Figure\u00a0<ref>(b), the grouping operator may buffer the quality trail \ud835\udcac_r into memory if r's status is not decided yet (The last two lines of the Iterative() method). However, for scalability, the grouping operator must be able to operate under limited memory, e.g., a memory buffer of max size M, without crashing. To achieve this and to avoid frequent writes to disk, which slows down the processing, we deploy an algorithm called \u201cBufferClean()\u201d.  This algorithm is executed  by the grouping operator when the allotted memory buffer is full. The functionality of BufferClean() is simple and involves retrieving the up-to-date contributionArray from the aggregation functions with the hope that the status of some of the buffered tuples has changed to either \u201c+\u201d or \u201c-\u201d. In such case, their quality trails can be taken out from the buffer. In the worst case, if  BufferClean() did not free up some space, then the memory buffer is written to disk, and  will be later retrieved when the Finalize() method is called Interestingly, the  BufferClean()  algorithm can be very effective in many cases, and entirely avoids the need  to write to disk because several of the common aggregation functions that may require buffering, e.g., Min(), and Max(), may start discarding  previous candidates as they see few more tuples (See the experiments in Section\u00a0<ref>).  \n\n\t\n\n\n\n\n\n\n\n\n\t\n\n\n \u00a7.\u00a7 Set Operators\n\t\n\nThe semantics of the set operators  \u2229, \u222a, and - is extended in the same way as the other operators, i.e., if an output tuple is coming from both relations, then it will inherit the merged quality trails of the contributing input tuples. Otherwise, the output tuple is coming  from only one relation, and hence it will carry its own quality trail.The extended algebraic expressions of the set operators are defined as:\n\n\n\nR \u2229 S = {\u27e8 z.v_1, ..., z.v_n,\u00a0\u03a9(\ud835\udcac_R.z, \ud835\udcac_S.z) \u27e9\u00a0|\u00a0 z\u00a0\u2208\u00a0R\u00a0&\u00a0z\u00a0\u2208\u00a0S}\nR - S = {\u27e8 r.v_1, ..., r.v_n,\u00a0\ud835\udcac_r\u27e9\u00a0|\u00a0 r\u00a0\u2208\u00a0R\u00a0&\u00a0r\u00a0\u2209\u00a0S}\t\n\n    R \u222a S = { z\u00a0|\u00a0 z ={[ \u27e8 z.v_1, ..., z.v_n,\u00a0\u03a9(\ud835\udcac_R.z,\ud835\udcac_S.z)\u27e9                               if C_1;            \u27e8 z.v_1, ..., z.v_n,\u00a0\ud835\udcac_z\u27e9                               if C_2 ]}\n\n\nwhere \n\n\n\nC_1 = (z\u00a0\u2208\u00a0R\u00a0&\u00a0z\u00a0\u2208\u00a0S), and C_2 = (z\u00a0\u2208\u00a0R\u00a0xor\u00a0z\u00a0\u2208\u00a0S)\n\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Duplicate Elimination \u03b4(R)\n\n\nThis operator involves eliminating duplicate tuples and keeping only one copy. In , two tuples r = \u27e8 v_1, v_2, ..., v_n, \ud835\udcac_r\u27e9 and s = \u27e8 z_1, z_2, ...., z_n, \ud835\udcac_s\u27e9 are considered identical iff they are identical in the data part, i.e., r.v_i = s.z_i,\u00a0  1 \u2264 i \u2264 n. For each group G of identical tuples, the quality trail of G's output tuple is derived by merging the quality trails of G's input tuples. \n\t\n\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\n\n\n\n\n\n\t\n\t\n\n\n\n\u00a7 QUERY OPTIMIZATION IN \n\n\n\n\nA key advantage of building upon the propagation semantics in provenance literature such as the Trio system\u00a0<cit.>, i.e., the input tuples contributing to the quality of an output tuple o are those that are considered o's lineage, is the retention of the equivalence and optimization rules in relational databases. However, such retention cannot be directly claimed because some of the operators apply a complex merge function over the quality trails (See the  Merge operator \u03a9 in Section\u00a0<ref>). \n\n\n\n\n\n\n\n\n\n\tTherefore, it is important to study the properties of this Merge operator,\n\tbased on which  we can prove other algebraic properties.\n\t\n\n\n\t\n\t\n\n\n\n\nThe Merge operator has the following two properties:\n\n\n\u00a0\u00a0\u00a0\u00a0Commutativity:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u03a9(\ud835\udcac_1, \ud835\udcac_2) = \u03a9(\ud835\udcac_2, \ud835\udcac_1)\n\n\u00a0\u00a0\u00a0\u00a0Associativity:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u03a9(\u03a9(\ud835\udcac_1, \ud835\udcac_2), \ud835\udcac_3) = \n\t   \t\t\t\t\t\t\t\t\u03a9(\ud835\udcac_1, \u03a9(\ud835\udcac_2, \ud835\udcac_3))  \n \n=\u03a9(\ud835\udcac_1, \ud835\udcac_2, \ud835\udcac_3)\n\n\nSketch of Proof:\n\tThe proof of Commutativity is straightforward since\n\tthe computation of each new transition (Lines 5-9 in Figure\u00a0<ref>) \n\tare not sensitive to the order of the input quality trails, e.g., \ud835\udcac_1 and  \ud835\udcac_2. \n\tThe Min() and StatsCombine() functions in Lines 6 and 8, respectively, treat their input values as a set, \n\tand hence the order does not matter. \n\t\n\tThe proof of Associativity relies on that the primitive operations that generate each new transition from the input ones \n\tare themselves associative. That is, the Min() function in Line 6 is associative, and also \n\tStatsCombine() function, which combines the underlying statistics, involves associative functions. \n\tRecall that the statistics need to be either algebraic (E.g., AVG and STDEV), or \n\t distributive under insertion (E.g., SUM, COUNT, MIN, MAX), and all of these functions are associative. \n\n\t\n\nBased on Lemma\u00a0<ref>, we prove in the following theorem that \nthe 's operators are order insensitive as it is the case in the standard relational operators.\t\n\t\n\n\n\n\nEach operator in is guaranteed to generate the same quality-annotated output tuples independent of the inputs' order. \n\n\n\nSketch of Proof:\tThe standard relational operators are all order-insensitive as they execute under the Set or Bag semantics. And hence, they produce the same output independent of the inputs' order. The extended counterpart operators in can be categorized into three categories: (1)\u00a0Operators that do not apply any manipulation over the quality trails, e.g., \u03c3, \u03c0, and -, (2)\u00a0Operators that apply the Merge operator to combine the quality trails, e.g., , \u03b3, \u2229, and \u03b4, and (3)\u00a0Operators that fit in the two aforementioned cases, e.g., outer joins, and \u222a. The operators in the 1^st category, by default, retain the order-insensitive property. The operators in the 2^nd, and 3^rd categories are also guaranteed to infer the same quality for each output tuple independent of the inputs' order according to Lemma\u00a0<ref>. Therefore, Theorem\u00a0<ref> is guaranteed to hold.\n\n\nTheorem\u00a0<ref> is important as it guarantees the consistency of each operator's output independent of the inputs' order. \nOne direct application of Theorem\u00a0<ref> is proving that the grouping operator \nwill produce output tuples having the same inferred qualities \ndespite the possible buffering of some quality trails until the \nend of the operator's execution (See the Iterative() function inside the grouping operator in Figure\u00a0<ref>).     \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CREATION & MAINTENANCE OF QUALITY TRAILS\n\n\n\n \n\nIn this section, we present the creation and maintenance mechanisms of the quality trails. \nSince is a generic engine, the goal is to design a set of APIs that will act as the interface between and \nthe external world, e.g., the tools and applications' logics that can assess the quality scores and pass them to . \n More specifically, allows the database developers to manipulate the \n quality trails as any other attribute in the database, e.g., modifying and extending quality trails  \nin response to specific changes in the database from within the standard database triggers and UDFs. \nThe quality trails are designed as special attributes added to the database relations, i.e., each \n relation R has an automatically-added special attribute called \u201cQTrail\u201d. \n QTrail attribute is of a newly added user-defined type representing an \n array of quality transitions (Refer to Definitions\u00a0<ref>, and\u00a0<ref>). \n On top of this new type, a set of manipulation functions has been developed \n as presented  in Figure\u00a0<ref>. These built-in functions are by no means comprehensive, \n but they are basic functions on top of which \nthe database developers may create more semantic-rich functions. \n \n In Figure\u00a0<ref>, we present few of the developed functions.\n  For example, assuming a given quality trail r.QTrail, the \n  getQualityTrail() function returns the quality trail as an array of quality transitions, \n  while getSize() function returns the number of transitions in the quality tail. \n  The functions addTransition() and replaceTransition() can be used to \n  alter an exiting quality trail by adding a new transition, or by modifying and replacing an existing transition, respectively. \nThese functions internally double check that the timestamps of the transitions should be always monotonically increasing, \notherwise the modification will be rejected.  \n The trim() function enables trimming a given quality trail according to some \n criteria, e.g., by specifying a direction (L.H.S or R.H.S), and a number of \n transitions to keep. has several different signatures for the trim() function. \n This function is useful, especially if an application prefers not to keep the entire \n history of the quality trail\u2014which can be large, and \n instead limit the scope to specific number of transitions or until specific time in the past. \n In addition to these functions, we have also developed a set of function to manipulate \n a given quality transition, e.g., building a quality transition, and setting  or retrieving specific fields within a transition[Other storage schemes are possible without affecting \nthe core functionalities of . Only the implementation of the APIs may change.]. \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n It is important to highlight that the tuples' quality trails are \n exposed as normal attributes only for the maintenance and update purposes. \n The objective from this feature is to make a generic engine with broader applicability to a wide range of application. \nHowever, this is entirely orthogonal to the quality propagation and assessment proposed \n in Section\u00a0<ref>. \n First,  quality propagation does not require explicit querying \n of the quality trails, i.e., users' queries will be written in the standard way as \n in Example 1 in Section\u00a0<ref>, and the quality trails will automatically propagate along with queries' results. \n And second, the propagation semantics is too complex to be delegated to end-users and \n to be encoded in each query.  That is why automatically and transparently \n manage the propagation semantics.\n \n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\nSetup:\nis implemented within the PostgreSQL DBMS\u00a0<cit.>. A Quality Trail is modeled as a \nnew data type, i.e., a dynamic array of quality transitions. \nThe default storage scheme is called \u201cQTrail-Scheme\u201d, in which \neach of the users' relations is automatically augmented with \na new \u201cQTrail\u201d column as presented in Section\u00a0<ref>. \n\n\nis experimentally evaluated using an AMD Opteron Quadputer compute server with two 16-core\nAMD CPUs, 128GB memory, and 2 TBs SATA hard drive. \nis compared with the plain PostgreSQL DBMS to \nstudy the overheads  associated with the new functionalities (w.r.t both time and storage).\n\nNo other work in literature offer functionaries similar to to compare with, \nespecially that the work in\u00a0<cit.> has not been experimentally evaluated.\t\n\n\n\n\n\n \u00a7.\u00a7 Application Datasets\n\n\nWe use a subset of the curated UniProt real-world biological database\u00a0<cit.>.  UniProt offers a \ncomprehensive repository for protein and functional information for various species. \nWe extracted four main tables including Protein, Gene, Publication, and Comment.\n\nThe tables are connected through the following relationships:  The Protein table \nhas a many-to-one relationship with Gene, many-to-many relationship with Publication, and one-to-many relationship with Comment. \nThe Gene table has also  a many-to-many relationship with Publication, and one-to-many relationship with Comment.\nThe Comment table contains free-text comments and notes related to the genes and proteins. \nThese comments are extracted from the fields marked with \u201cCC\" within the UniProt entires\u2014which indicates a free-text comment field.  \n\nOur dataset consists of approximately 750,000 protein records (\u2248 4.7GBs), 1.3 x 10^6 gene records (\u2248 8GBs), \n 12 x 10^6 publication records (\u2248 4.5GBs), and  8 x 10^6 comment records (\u2248 6.5GBs).\nThus, the total size of the dataset is approximately 24GBs.     \n\n\n\n\n \u00a7.\u00a7 Workload\n\n\n\nWe focus on tracking the qualities of the tuples in the Gene and Protein tables under the addition of new publications and comments. \nThe quality score varies between 1 (the lowest quality), and 10 (the highest quality). \nTo build the quality trails, we implemented an \u201cAfter Insert\" database trigger on each of the Publication and Comment tables\nsuch that with the insertion of a new publication or comment, the quality of the corresponding gene or protein will be updated.  \nWe assume that the insertion of a new publication increases the \nquality (unless the quality score is already the maximum, in which case the new quality transition will have the same score as the previous one). \nFor the comment values, each comment in UniProt has a code indicating the type of this comment. One \nof these types is \u201cCAUTION\u201d, which indicates a possible error or confusion in the data. \nAll comments  having the \u201cCAUTION\u201d type are assumed to decrease the quality \n(unless the quality score is already the minimum, in which case the new quality transition will have the same score as the previous one). \nFor the other comment values, we randomly labeled each one as \u201c+\u201d, \u201c-\u201d,  or \u201c\u223c\u201d, which \nindicates increasing, decreasing, or retaining the previous quality score, \nrespectively\u00a0[We used random labeling since developing a free-text semantic extraction tool (or leveraging an existing tool)  is not the focus of this paper.].  \n\nUnless otherwise is specified, we assume the following: \n(1)\u00a0Each increase or decrease in the quality score is performed one step at a time, i.e.,  \u00b1 1, \nand (2)\u00a0If a quality transition is storing statistics\u2014Referred to as \u201cFull Transitions\u201d\u2014 then three types of statistics are maintained, \n\twhich are the {Min, Max, Avg = (Sum, Count)}.\nThe insertions of the publication and comment records are randomly interleaved because the \ndatabase does not maintain a global timestamp ordering the records' creation.\nWhen evaluating the query performance of , we compare against the standard query processing \nin which the quality trails are not even stored in the database.\nFinally, the query optimizer of PostgreSQL has not been touched or modified, and hence the queries \nused in the evaluation  are optimized in the standard way. \n\n\n\n \n\n\n\n \u00a7.\u00a7 Storage and Maintenance Evaluation\n\n\nIn Figure\u00a0<ref>, we study the storage overhead introduced by the quality trails. \nTo put the comparison into perspective, we compare \u201cQTrail-Scheme\"  with another alternative \nwhere the quality trails of a given table R are stored in a separate table R-QTrail (OID, QTrail) \nthat has one-to-one relationship with R. This scheme is referred to as \u201cOff-Table Scheme\".\nWe study the storage overheads under the two \ncases of:\u00a0(1)\u00a0 Full Transitions, where each  quality transition has content in all its fields; the mandatory ones (score, and timestamp), \nand optional ones (triggeringEvent, and statistics) (Figure\u00a0<ref>(a)). \nIn this case, the triggeringEvent field is a string of length varying between 50 and 100 bytes. \nAnd (2)\u00a0Minimal Transitions, where  each  quality transition has content in only  the mandatory fields (Figure\u00a0<ref>(b)). \n\nIn each of the two figures, we measure the overhead under different  constraints on the maximum size of a quality trail (the x-axis). \nThe values Limit-5, Limit-10, and Unlimited indicate keeping only up to the last 5, 10, or  \u221e transitions. \nThe y-axis shows the absolute storage overhead, while the percentages inside the rectangle boxes show the overhead\u2014more specifically that of  the QTrail-Scheme\u2014as \na percentage of the sum of Gene and Protein  sizes (\u2248 12.7GBs).\nAs Figures\u00a0<ref>(a) and\u00a0<ref>(b) show, there is no big difference between both storage schemes in all cases. \nThe Off-Table scheme is slightly higher because of the storage of the unique tuple Id values (OID column).  \nIn general, the quality trails do not introduce much storage overhead even under the worst case where the entire quality history is stored, e.g.,  \nthe overhead is around 18% (for Full Transitions), and 3.7% (for Minimal Transitions).\nIt is worth highlighting that under the Unlimited case, the longest quality trail consisted on 37 transitions.\n\nIn Figure\u00a0<ref>, we study the maintenance overhead of the quality trails. \nWe consider the Unlimited case of quality trails. \nTo have fair comparison, we measure the time of updating a quality trail, e.g., adding new transitions, \nw.r.t the time of updating other traditional fields, e.g., updating a text or integer fields. \nIn all cases, the update operation takes place from within an After Insert trigger over the Comment table as\ndescribed in the experimental workload above. Inside the trigger, the corresponding gene is retrieved  to update its data or its quality trail (See Example 2 in Section\u00a0<ref>). \nThe retrieval from the DB uses a B-Tree index on the Gene.ID column.  \nEach operation is repeated 20 times, and their average is what we report. \n\nAs illustrated in Figure\u00a0<ref>, we use the operation of updating a string field, more \nspecifically replacing a segment of Gene.Seq field with another segment, as our reference operation. That is, \nits execution time is normalized to value 1, and the other operations will be measured relative to this operation.  \nThe relative performance of updating an integer field, more specifically Gene.StartPos, it also depicted in the figure. \nOn average, the overhead of updating the integer field is around 94% of updating the string field.  \n\nFor updating the quality trails, we consider both the QTrail-Scheme, and the Off-Table Scheme, \nand the two cases of Full Transition, and Minimal Transition. For each case, \nwe run a batch that consists of 20 transactions inserting records into \nthe Comment table, which yield to updating the genes' quality trails.  \nOn the x-axis, we report the average performance over the entire batch under two scenarios: \n(1)\u00a0The batch is the 1^st, i.e., all quality trails are empty, and (2)\u00a0The batch is the Last, where all other records have been \ninserted and the quality trails are almost complete. \nThe results in Figure\u00a0<ref> show that operating on and updating a quality trail structure is very comparable to updating other database fields.\nUnder  the QTrail-Scheme, where the table to be queried and update is the Gene table, \nthe relative overhead ranged from 0.98% (The 1^st batch with minimal transitions) to 1.11% (The last batch with full transitions). \nThe performance of the Off-Table Scheme, where the table to be queried and updated is called Gene-QTrail, \nis almost the same except for the 1^st batch case, where the Gene-QTrail table is empty.\n\n\n\n\n \n\n \n\n\n\n\n\n\n \u00a7.\u00a7 SSP Query Performance\n\n\nIn the next experiments, we evaluate the propagation and derivation of the quality trails at query time under various types of queries. \n\nIn Figure\u00a0<ref>, we present the performance of Select-Project (SP) queries. The query templates are presented in  Figure\u00a0<ref>(c).\nOn the x-axis of Figures\u00a0<ref>(a) and\u00a0<ref>(b), we vary the query selectivity from 0.01% (corresponds to 75 protein tuples, or 130 gene tuples) \nto 0.5% (corresponds to 3,750 protein tuples, or 6,500 gene tuples),  and consider the two storage schemes QTrail-Scheme and Off-Table Scheme.\nThe y-axis measures the propagation overhead of the quality trails w.r.t the standard query \nperformance, i.e., no quality trail storage or propagation.\nWe consider the propagation of the quality trails under the max size constraints of Limit-5, Limit-10, and Unlimited.\nUnder each configuration, we execute 5 queries on each of the Gene and Protein tables, and then report the average of the \nobserved overheads across the 10 queries. \nFigures\u00a0<ref>(a) and\u00a0<ref>(b) show the results under the cases \nwhere the queries are executed without and with an index, respectively.  \n\nThe results show a big difference between the QTrail-Scheme and the Off-Table Scheme. \nThis is mainly because the two operators that directly read from disk, i.e., the table-scan and index-scan operators, \nhave different implementation under the two storage schemes. In the QTrail-Scheme, they are implemented such \nthat they read the quality trails from the data tuples without any additional overhead. Whereas in the Off-Table Scheme, \nthey are implemented to join the data tuples with the other table that contains the quality trails. \nAll the other operators are independent of the physical storage of the quality trails as they read them from the operators' buffers in the query pipeline. \nSince join is an expensive operation, the  Off-Table Scheme encounters higher overhead. \nIn Figure\u00a0<ref>(a), the query selectivity does not play a big factor because a complete table scan \nis performed regardless of the selectivity, which dominates most of the cost.\nIn contrast,  in Figure\u00a0<ref>(b), an index is used to select the data tuples satisfying \nthe query's predicates, and thus the performance is more sensitive to the \nquery selectivity. \nFor the ff-Table Scheme, the overhead increases as the selectivity increases\u2014and consequently the join cost increases. \nAs the figure shows the Off-Table Scheme encounters between 2.5x and 6x higher overhead compared to the \nQTrail-Scheme.\nIt is important to highlight that the select and project operators do not apply any manipulation over the quality trails, and thus the encountered \noverheads are mostly due to the additional storage introduced by the quality trails.\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Join Query Performance\n\n\nIn Figure\u00a0<ref>, we measure the performance of join queries. \nWe consider joining the Gene and Protein tables using the query syntax highlighted in Figure\u00a0<ref>(c).\nBefore the join operation, a selection predicated is applied over the Gene table, and \nthe selectivity of this predicates is varied between 0.01% to 0.5% as depicted on the x-axis of Figures\u00a0<ref>(a), and\u00a0<ref>(b).\nThe query is evaluated under two different query plans representing two different join types, i.e., \nin Plan 1 the join operator uses an index on the the gene Ids  stored in the Protein table (Protein.GID), \nwhereas in Plan 2 such index does not exist, and thus the join is performed using block-based nested-loop algorithm. \nIn Figure\u00a0<ref>, we only report the results of the Full Transition case as  \nthe results of the  Minimal Transition case exhibit  similar trends, but with smaller overhead percentages. \n\n\nIn Figure\u00a0<ref>(a), since both the selection and join operations are utilizing indexes, the quality trails' extra storage  \ndoes not contribute much to the execution time overhead. \nThat is why there is no big difference in performance under the different maximum sizes of \nquality trails (Limit-5, Limit-10, and Unlimited). \nIn the case of the QTrail-Scheme, the execution \noverhead is mostly contributed to the merge operations applied over the quality trails within the join operator. \nWhereas, the Off-Table Scheme encounters the same overhead from the merge operations plus \nthe additional join needed to link the data tuples to their quality trails, which is implicitly performed inside\nthe index-scan over both tables. This join overhead is clearly dominating the merge operations' overhead.\nOn the other hand, the performance results in Figure\u00a0<ref>(b) are mostly not sensitive to the \nquery's selectivity. This is because a full-table scan over the Protein table will be performed in all cases, \nwhich dominates the overhead. That is also the reason why the Unlimited case shows \na clear higher overhead compared to the Limit-5 and Limit-10 cases, i.e., the difference in I/Os\nbecomes a distinguishing factor among the three cases.\n \n \n\n\n\n \u00a7.\u00a7 Aggregation Query Performance\n\n\nFor the grouping queries, the type of the aggregation function(s) plays an important role. \nThis is because some aggregation functions, e.g., COUNT, SUM, AVG,  do not require any buffering for the input quality trails, \nwhile other functions, e.g., MIN, and MAX, will most probably require buffering. \nIn Figure\u00a0<ref>, we study the aggregation performance under these two types of aggregators (Figure\u00a0<ref>(c)). \nQuery Q1 involves one function COUNT(), which always returns \u201c+\u201d for each input tuple, and hence the quality trails are incrementally merged. \nIn contrast, Query Q2 involves two functions MIN() and MAX(), and each may return \u201c-\u201d if the tuple is certainly not contributing to the function's result, \nor \u201c?\u201d if the tuple may contribute. Since the minimum and maximum values cannot be determined until the last tuple is seen, then the quality trails of the  \nin-doubt tuples will need to be buffered. In our implementation of the MIN() (or MAX()) functions, if multiple tuples have the same \nminimum (or maximum) value, then they all contributing to the output result.\nIn this set of experiments, we will focus only on the QTrail-Scheme since it has proven superiority over the Off-Table Scheme.\n \nThe results in  Figure\u00a0<ref>(a) show that changing the maximum buffer size (the x-axis) has no effect on the performance since  COUNT(*) does not require buffering.  \nNevertheless, the transition size (either Full or Minimal) and the maximum \nallowed size of a quality trail (either Limit-5, Limit-10, or Unlimited), are both affecting the performance. \nThis is because the grouping query requires a full table scan, and thus as the storage overhead of the quality trails increases, the execution time also increases.  \nThe storage overhead also plays the same effect in Figure\u00a0<ref>(b). However, Q2 does buffering, and thus the allowed buffer size affects the performance. \nAs presented in Section\u00a0<ref>, when the buffer is full, the grouping operator will execute the BufferClean() function trying to avoid un-necessary disk-writes, and \nget rid of the in-doubt tuples (\u201c?\u201d ) that have changed their status to not-contributing (\u201c-\u201d).  \nFigure\u00a0<ref>(b) shows the number of  BufferClean() calls in each case. \nFor example, for the minimal-size transitions with 8MBs buffer size, only two BufferClean() calls are \nexecuted for the Unlimited case, and zero calls for the Limit-5 and Limit-10 cases.\nFor this experiment, these  BufferClean() calls are very effective since the in-doubt tuples can be resolved fast as more tuples are seen, and thus no disk writes are \nrequired even under the smallest buffer size of 2MBs. Therefore, depending on the different configurations, the relative execution overhead (w.r.t the standard query) \nvaries between 8% and 31% as illustrated in the figure.\n\n\n\n\n\n\n\nIn some aggregation function, the execution of BufferClean() may not be always effective, and thus the grouping operator will need to write the buffer's content to disk. \nTo study such effect, in Figure\u00a0<ref>, we repeat the same experiment of Query Q2 reported in Figure\u00a0<ref>(b), but in this case we disable the \nBufferClean() function. And hence, the grouping operator has to write the buffer to disk when full. \nThe results in Figure\u00a0<ref> confirm the same trend as the other experiment \nwith the exception of having a higher execution overhead (between 0% in the best case to 6.5% in the worst case). \nIt is worth mentioning that if an aggregation function requires extensive buffering for the quality trails, e.g., the MEDIAN() function would buffer all its input quality trails, \nthen most probably this function would also require extensive buffering of the data \nvalues. In this case and for performance considerations, these functions can easily implement the \ncoarse-grained semantics, i.e., returning \u201c+\u201d for each input tuple, and hence no quality trail buffering is needed. \n\n\n\nTo illustrate the benefits from implementing the open-box aggregation-aware quality derivation  for the grouping and aggregation operators,\nwe repeat the aggregation query Q2 under the black-box aggregation-unaware implementation, i.e., all input tuples within a group \ncontribute to the output's quality of the Min() and Max() functions (Figure\u00a0<ref>). \nWe randomly sampled 200 tuples from the query's output, and report in Figure\u00a0<ref>, \nthe quality score of the most recent transition for these tuples (in a sorted order). \nThe  results confirm that the black-box approach can severely and unnecessarily under estimate  the tuples' qualities.  \nFor example, in the open-box approach, most tuples have quality scores higher than 6. \nWhereas, in the black-box approach, most tuples have the lowest quality score of 1.  \n\n \n\n\n\n\u00a7 CONCLUSION\n\n\nWe proposed  as an advanced query processing engine for imperfect databases with evolving qualities. \nAt the conceptual level,  enables  high-level applications to\nmodel their data's qualities inside the database system, keep track of how the qualities evolve over time, \nand build more informed decisions based on the automatically quality-annotated query results.   \nAt the technical level, involves several novel  contributions including:\n(1)\u00a0Introducing a new quality model based on the new concept of \u201cquality trails\u201d in contrast to the commonly-used single-score quality model, \n(2)\u00a0Extending the relational data model to include the quality trails, \n(3)\u00a0Proposing a new query algebra, called \u201cQTrail Algebra\u201d, which extends the standard query operators as well as \n\tintroduces new quality-related operators for the propagation and derivation of quality trails at query time, \nand (4)\u00a0Proving that 's query optimizer can inherit and leverage the logic of the standard query optimizers \n\twithout the need for customized optimizers. \nThe experimental evaluation has shown the practicality of , and the efficiency of its design choices.\n\n\n\n\n\n\n\n\n\n"}