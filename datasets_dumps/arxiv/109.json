{"entry_id": "http://arxiv.org/abs/2303.07216v2", "published": "20230313155138", "title": "Parallel Vertex Diffusion for Unified Visual Grounding", "authors": ["Zesen Cheng", "Kehan Li", "Peng Jin", "Xiangyang Ji", "Li Yuan", "Chang Liu", "Jie Chen"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI"], "text": "\n\n\n\n\n\n\n\nParallel Vertex Diffusion for Unified Visual Grounding\n    \nZesen Cheng^1\u00a0Corresponding Author.   Kehan Li^1   Peng Jin^1   Xiangyang Ji^3\n\nLi Yuan^1,2   Chang Liu^3\u00a0   Jie Chen^1,2\u00a0^1 School of Electronic and Computer Engineering, Peking University \n\n^2 Peng Cheng Laboratory   ^3 Tsinghua University   \n\n\n\n    March 30, 2023\n============================================================================================================================================================================================================================================================\n\n\n\nempty\n\n\n\nUnified visual grounding pursues a simple and generic technical route to leverage multi-task data with less task-specific design.\n\nThe most advanced methods typically present boxes and masks as vertex sequences to model referring detection and segmentation as an autoregressive sequential vertex generation paradigm.\n\nHowever, generating high-dimensional vertex sequences sequentially is error-prone because the upstream of the sequence remains static and cannot be refined based on downstream vertex information, even if there is a significant location gap.\n\nBesides, with limited vertexes, the inferior fitting of objects with complex contours restricts the performance upper bound.\n\nTo deal with this dilemma, we propose a parallel vertex generation paradigm for superior high-dimension scalability with a diffusion model by simply modifying the noise dimension.\n\nAn intuitive materialization of our paradigm is Parallel Vertex Diffusion\u00a0(PVD) to directly set vertex coordinates as the generation target and use a diffusion model to train and infer.\n\nWe claim that it has two flaws: (1) unnormalized coordinate caused a high variance of loss value; (2) the original training objective of PVD only considers point consistency but ignores geometry consistency.\n\nTo solve the first flaw, Center Anchor Mechanism\u00a0(CAM) is designed to convert coordinates as normalized offset values to stabilize the training loss value. \n\nFor the second flaw, Angle summation loss\u00a0(ASL) is designed to constrain the geometry difference of prediction and ground truth vertexes for geometry-level consistency.\n\nEmpirical results show that our PVD achieves state-of-the-art in both referring detection and segmentation, and our paradigm is more scalable and efficient than sequential vertex generation with high-dimension data.\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nVisual grounding is an essential task in the field of vision-language that establishes a fine-grained correspondence between images and texts by grounding a given referring expression on an image\u00a0<cit.>. \n\nThis task is divided into two sub-tasks based on the manner of grounding: Referring Expression Comprehension (REC) using bounding box-based methods\u00a0<cit.>, and Referring Image Segmentation (RIS) using mask-based methods\u00a0<cit.>. \n\nREC and RIS are chronically regarded as separate tasks with different technology route, which requires complex task-specific design. However, REC and RIS share high similarities and have respective advantages so that it is natural and beneficial to unify two tasks. \n\nRecently, unified visual grounding becomes a trend of visual grounding because it avoids designing task-specific networks and can leverage the data of two tasks for mutual enhancement\u00a0<cit.>. \n\n\nThe most advanced unified visual grounding paradigm represents boxes and masks as vertex sequences and models both REC and RIS as an autoregressive sequential vertex generation problem\u00a0<cit.> which is solved by an autoregressive model\u00a0<cit.>.\n\nAlthough this paradigm unifies REC and RIS in a simple manner, it is hard to scale to high-dimension settings, which causes inferior fitting to objects with complex contours.\n\nThis flaw of sequential vertex generation paradigm mainly attributes to the sequential generation nature of its fundamental architecture, i.e., autoregressive model \u00a0<cit.>.\n\nSpecifically, if upstream of vertex sequence is not precise enough, this paradigm easily gets stuck in a trap of error accumulation because upstream of sequence is stationary during subsequent generation.\n\nIn Fig.\u00a0<ref>(b), we can find that sequential vertex generation methods easily generate error vertexes when upstream vertexes don't hit the right object.\n\nMoreover, sequential vertex generation methods face efficiency problem when scaling to high-dimension data because the iteration for generation sharply increases with the dimension of data\u00a0<cit.>.\n\n\n\n\nTo tackle the issues of sequential vertex generation paradigm, we design parallel vertex generation paradigm to parallelly generate vertexes, which is more scalable for high-dimension data.\n\nSpecifically, the parallel nature allows dynamic modification of all vertexes to avoid error accumulation and requires only a few iterations to accomplish generation, which ensures it is easier to scale to high-dimension data.\n\nFig.\u00a0<ref>(b) conceptually shows how our paradigm works.\n\nNoisy vertexes are first sampled and then are gradually refined to precise vertexes of the object.\n\nRecently, diffusion model\u00a0<cit.> is demonstrated as an effective and scalable model when processing high-dimension generation tasks\u00a0(Text-to-Image generation\u00a0<cit.>) and discriminative tasks\u00a0(Panoptic Segmentation\u00a0<cit.>). \n\nThe scalability of diffusion model mainly attributes to its parallel denoising mechanism. \n\nDiffusion model can easily scale to high-dimension settings by simply modifying the dimension of noise.\n\nTo leverage the scalability, we adopt a diffusion model to instantiate parallel vertex generation paradigm as Parallel Vertex Diffusion\u00a0(PVD) for more scalable unified visual grounding.\n\nMoreover, we find that there are two aspects for improving the convergence of PVD: (1) The coordinates of vertexes are not normalized so that the loss value has a high variance. (2) The vanilla training objective of PVD only considers point-level consistency and ignores the geometry-level consistency between prediction and ground truth vertexes.\n\nTo implement this, we extra design Center Anchor Mechanism\u00a0(CAM) to normalize coordinate values for stabilizing the optimization signal and Angle Summation Loss\u00a0(ASL) to constraint geometry difference for achieving geometry consistency\u00a0(Fig.\u00a0<ref>). \n\n\nExtensive experiments are conducted for both REC and RIS on three common datasets\u00a0(RefCOCO\u00a0<cit.>, RefCOCO+\u00a0<cit.> and RefCOCOg\u00a0<cit.>). On one hand, the empirical results show that PVD w/ CAM and ASL achieves SOTA on both REC and RIS tasks. On the other hand, the results also show that parallel vertex generation can better handle data with high-dimension and requires lower computation cost than sequential vertex generation paradigm.\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n \u00a7.\u00a7 Visual Grounding\n\n\nVisual grounding is essentially an object-centric fine-grained image-text retrieval\u00a0<cit.>, which has two forms: Referring Expression Comprehension\u00a0(REC) at box level\u00a0<cit.> and Referring Image Segmentation\u00a0(RIS) at mask level\u00a0<cit.>. \nAlthough they were parallel in the past, recent researches gradually focus on unified visual grounding because it requires little task-specific design and can leverage data from multiple tasks for mutual promotion.\n\nReferring Expression Comprehension.\nREC starts with the two-stage pipeline\u00a0<cit.>, where region proposals are first extracted by a pretrained detector\u00a0<cit.> and then ranked according to the reasoning similarity score with the referring expression. To tackle the two-stage pipeline's efficiency problem, researchers develop the one-stage pipeline to integrate location and cross-modal reasoning\u00a0<cit.> into a single network for end-to-end optimization\u00a0<cit.>.\n\nReferring Image Segmentation.\nRIS models the fine-grained referring object location as a pixel classification problem\u00a0<cit.>. The main focus of RIS research is to design better cross-modal alignment and fusion, e.g., concatenate\u00a0<cit.>, cross-modal attention mechanism\u00a0<cit.>, visual reasoning\u00a0<cit.>. Recent endeavors in RIS shift to simplify the pipeline of RIS system. EFN\u00a0<cit.> couples the visual and linguistic encoder with asymmetric co-attention. LAVT\u00a0<cit.> and ResTR\u00a0<cit.> replace those complex cross-modal attention and reasoning with a simple stack of transformer encoder\u00a0<cit.> layers.\n\nUnified Visual Grounding.\n\nUnified visual grounding has generally three types, i.e., two-stage paradigm\u00a0<cit.>, multi-branch paradigm\u00a0<cit.>, and sequential vertexes generation paradigm\u00a0<cit.>. \n\n(1) Two-stage: Methods in the two-stage paradigm are built upon a pretrained detector\u00a0(e.g., Mask R-CNN\u00a0<cit.>) to first generate region proposals, and then referring expression is used to retrieve the most confident region as results\u00a0<cit.>. \n\n(2) Multi-task: For breaking through the bottleneck of pretrained detector, multi-branch paradigm is proposed to assign task-specific branches to different tasks for joint end-to-end optimization\u00a0<cit.>. \n\n(3) Sequential Vertex Generation: To simplify previous schemes and reduce optimization bias resulting from a multi-branch architecture\u00a0<cit.>, SeqTR models both REC and RIS as a vertex generation problem\u00a0<cit.> and adopt an autoregressive model\u00a0<cit.> to sequentially generate vertices of objects, which is currently the most advanced unified visual grounding scheme\u00a0<cit.>.\nBecause of the sequential generation nature, SVG is easily trapped in error accumulation. This work proposes Parallel Vertex Generation to parallelly generate vertexes to avoid this issue.\n\n\n\n \u00a7.\u00a7 Generative Model for Perception\n\n \nSince Pix2Seq\u00a0<cit.> first claims that sequence generation modeling is a simple and generic framework for object detection, generative model for perception is gradually gaining more attention. Following Pix2Seq, Pix2Seq v2\u00a0<cit.> is proposed as a general vision interface for multiple location tasks, e.g., object detection, instance segmentation, keypoint detection, and image caption. Although sequence generation modeling shows its potential, its fundamental generative architecture\u00a0(autoregressive model) hard scales to high-dimension data\u00a0<cit.>. Subsequently, Pix2Seq-D shows that diffusion model\u00a0<cit.> is a better fundamental generative architecture for processing high-dimension task, i.e., panoptic segmentation\u00a0<cit.>. Then researchers expand diffusion model to other high-dimension tasks, e.g., semantic segmentation\u00a0<cit.> and pose estimation\u00a0<cit.>. \n\n\n\n\n\n\n\u00a7 METHOD\n\n\n\n\n \u00a7.\u00a7 Overall Pipeline\n\n\nFor claiming the relationship between different components of our workflow\u00a0(Fig.\u00a0<ref>), we describe the training and inference processes of the overall pipeline.\n\nTraining. An image-text pair {I, T} is input into visual and linguistic encoder to extract cross-modal features F_c\u00a0(Sec.<ref>). The cross-modal features are then used to regress center point c\u00a0(Sec.<ref>). To create the ground truth of parallel vertex diffusion, center point is used to normalize the vertexes sampled from bounding box and mask contour for acquiring  ground truth vertex vector V\u0302\u00a0(Sec.<ref>). The final step is to calculate loss for optimization:\n\n    \u2112 =  \u2112_c + \u2112_p + \u2112_g,\n\nwhere \u2112_c is the center point loss\u00a0(Eq.\u00a0<ref>) for optimizing center point prediction, \u2112_p is the point-to-point reconstruction loss\u00a0(Eq.\u00a0<ref>) for point level consistency, and \u2112_g is the angle summation loss\u00a0(Eq.\u00a0<ref>) for geometry consistency. The below sections will describe how to calculate these losses.\n\nInference. Same as the training process, we first predict the center point c\u00a0(Sec.<ref>). Then sampling a noisy state x_T from standard gaussian distribution \ud835\udca9(0, I). The noise is iteratively denoised to clean vertex vector V_0 by denoiser f_\u03b8\u00a0(Sec.<ref>). Finally, denormalizing the coordinates of vertexes and convert the denormalized coordinates of vertexes to bounding box and binary mask by toolbox of COCO\u00a0<cit.>.\n\n\n\n \u00a7.\u00a7 Cross-modal Feature Extraction\n\n\n\nThe first step is to prepare cross-modal fusion features. An image-text pair {I\u2208\u211d^h\u00d7 w\u00d73, T\u2208\u2115^n} is first sampled from dataset, where h, w, and n denote the image height, the image width, and the number of words.\n\n Note that the words of text are already tokenized to related numbers by Bert tokenizer\u00a0<cit.>. \nTo encoder image, the image is input in visual backbone\u00a0(e.g., ResNet\u00a0<cit.>, Darnet53\u00a0<cit.>, Swin\u00a0<cit.>) for acquiring  multi-scale visual features {\ud835\udc05_v_1\u2208\u211d^h/8\u00d7w/8\u00d7 d_1, \ud835\udc05_v_2\u2208\u211d^h/16\u00d7w/16\u00d7 d_2,\n\ud835\udc05_v_3\u2208\u211d^h/32\u00d7w/32\u00d7 d_3}.\nTo encoder text, the word tokens of text are input in language backbone\u00a0(e.g., Bert\u00a0<cit.>) for obtaining linguistic features F_l\u2208\u211d^n\u00d7 d_l. Specifically, the language backbone is the base bert model so that d_l is 768. Then we blend visual and linguistic features via element-wise multiplication\u00a0<cit.> and adopt multi-scale deformable attention Transformer (MSDeformAttn)\u00a0<cit.> for multi-scale cross-modal fusion:\n\n\n    \ud835\udc05_c_i = MSDeformAttn(MLP(\u03c3(\ud835\udc05_v_i) \u2299\u03c3(\ud835\udc05_l))),\n\nwhere \u2299 denotes element-wise multiplication, MLP(\u00b7) are three fully-connected layers and \ud835\udc05_c_i\u2208\u211d^h_i\u00d7 w_i\u00d7 d denotes cross-modal features of i-th stage. d is set to 256.\n\n\n\n \u00a7.\u00a7 Center Anchor Mechanism\n\n\n\nThe specific motivation of center anchor mechanism is to provide a coordinate anchor for converting coordinates to normalized offset values based on anchor, which is demonstrated to reduce the difficulty of coordinate regression\u00a0<cit.>.\n\nCenter Point Prediction. \nthe first step is to locate the rough center point c \u2208\u211d^2 of the object referred by text. \n\nFollowing previous advanced keypoint prediction network\u00a0(e.g., centernet\u00a0<cit.>, cornernet\u00a0<cit.>), the center point is represented as a gaussian heatmap Y\u2208[0, 1]^h\u00d7 w by a gaussian kernel:\n\n    Y_ij = exp(-(i - c_i)^2+(j - c_j)^2/2\u03c3^2),\n\nwhere \u03c3^2 is an image size-adaptive standard deviation\u00a0<cit.>. Firstly, we use this point representation method to generate target gaussian heatmap Y of ground truth center point c. Then the cross-modal features with largest spatial shape \ud835\udc05_v_1 are used to generate prediction heatmap Y, i.e., Y = MLP(\ud835\udc05_v_1). Finally, a focal loss\u00a0<cit.> is adopted as the training objective for optimization:\n\n    \u2112_c=1/hw\u2211^h,w_i,j{\n    (1 - Y_ij)^\u03b1log(Y_ij)     ,Y_ij = 1, \n    \n    (1 - Y_ij)^\u03b2Y_ij^\u03b1log(1 - Y_ij)     ,Y_ij < 1,\n    .\n\nwhere \u03b1 and \u03b2 are hyper-parameters of the focal loss. Following previous works\u00a0<cit.>, \u03b1 and \u03b2 are set to 2 and 4, respectively. For resolving the prediction heatmap to concrete coordinate, we set the point with peak value of prediction heatmap as the prediction center point c = {i_c, j_c}.\n\nCoordinate Normalization. To normalize a single point p = {i, j}, we adopt the scale and bias strategy:\n\n    p\u0302 = (\u00ee, \u0135) = (i - i_c/h, j - j_c/w),\n\nwhere p\u0302 denotes the point after normalizing.\n\n\n\n \u00a7.\u00a7 Parallel Vertex Diffusion\n\n\n\nParallel Vertex Diffusion\u00a0(PVD) is the key component for implementing parallel vertex generation paradigm. \nIn this paper, we mainly refer to the usage of diffusion model in Pix2Seq-D\u00a0<cit.>. The network which supports the reverse process of diffusion model is named \u201cDenoiser\".\n\n\n\nDenoiser. Because normalized coordinates of vertexes are range in [0, 1], it needs to be scaled to [-b, b] for approximating gaussian distribution, where b is set to 1\u00a0<cit.>. Before inputting into denoiser, the noisy state x_t needs to be clamped to [-1, 1] and descaled from [-1, 1] to [0, 1]:\n\n    V_t = (x_t / b + 1)/2,\n\nwhere x_t is the noisy state of t-th step, V_t is the noisy vertex vector of t-th step. \nDenoiser is the denoise function for reverse transition of diffusion model. Firstly, the Denoiser converts the noisy state x_t to vertex vector V_t by Eq.\u00a0<ref>. Then the vertex vector is embedded to vertex embeddings \ud835\udcac_t by 2D coordinate embedding\u00a0<cit.>. The embeddings are input into transformer decoder\u00a0<cit.> as queries and interacted with cross-modal features F_c. For keeping high-resolution features, three scales of cross-modal features are circularly utilized by different layer of the transformer decoder\u00a0<cit.>. The circle is repeated L times. After transformer decoder, the embedding is normalized by time embedding \u2130_t and is projected to refined vertex vector V_t-1. Finally, The refined vertex vector is processed by DDIM step\u00a0<cit.> for acquiring  next noisy state x_t-1. To better understand the Denoiser, the detailed workflow is illustrated in Fig.\u00a0<ref>.\n\nTraining phrase. Suppose that we already sample a single image I and its corresponding box B\u2208\u211d^2\u00d7 2 and mask M\u2208{0, 1}^h\u00d7 w from dataset. 182 Preparation: The first step is to extract and normalize the vertexes of the box and mask. The vertexes of box are left top corner and right bottom corner {p_b^lt = (i_b^lt, j_b^lt), p_b^rb=(i_b^lt, j_b^rb)}. To represent mask as point sets, we retrieve mask contour via a classical contour detection algorithm\u00a0<cit.> implemented by opencv\u00a0<cit.>. Then the mask contour is used to sample the vertexes of mask {p_m^1 = (i_m^1, j_m^1), \u22ef, p_m^N=(i_m^N, j_m^N)}, where N is the sampling number of mask vertexes. The vertex set is finally normalized by center anchor mechanism\u00a0(Sec.\u00a0<ref>) and flattened to vertex vector (V\u0302\u2208\u211d^4+2N): \n\n    V\u0302={\u00ee_b^lt, \u0135_b^lt, \u00ee_b^rb, \u0135_b^rb, \u00ee_m^1, \u0135_m^1, \u22ef, \u00ee_m^N, \u0135_m^N}.\n\n183 Forward transition: The normalized vertex vector V\u0302 is set as the initial state x_0 of diffusion model and is forward transitioned to noisy state x_t, i.e., q(x_t|x_0): \n\n    x_t = \u221a(\u03b3(t))x_0 + \u221a(1 - \u03b3(t))\u03f5,\n\nwhere \u03f5 and t are drawn from the standard normal distribution \ud835\udca9(0, I) and uniform distribution \ud835\udcb0({1,\u22ef,T}), \u03b3(t) denotes a monotonically decreasing function from 1 to 0. \n184 Point constraint: For training the denoiser, the noisy state x_t is reversely transitioned to the noisy state x_t-1, i.e., p_\u03b8(x_t-1|x_t). Following Bit Diffusion\u00a0<cit.>, the noisy state x_t-1 is required to approach initial state x_0:\n\n    \u2112_p =  \ud835\udd3c_t\u223c\ud835\udcb0({1,\u22ef,T}),\u03f5\u223c\ud835\udca9(0, I)\u2016 f_\u03b8(x_t, F_c,t) - x_0 \u2016^2,\n\nwhere f_\u03b8(\u00b7) is the parameterized Denoiser, x_t is derived by Eq.\u00a0<ref> from x_0, F_c={\ud835\udc05_c_1, \ud835\udc05_c_2, \ud835\udc05_c_3} denotes all of the cross-modal features mentioned in Sec.\u00a0<ref>. Eq.\u00a0<ref> is point-to-point reconstruction loss.\n\nInference phrase. To generate vertex vector of box and mask, it requires a series of state transitions x_T\u2192\u22ef\u2192 x_t \u2192\u22ef\u2192 x_0. Because T is set to a large value, x_T\u2208\u211d^4+2N follows standard normal distribution \ud835\udca9(0, I). Specifically, we sample a noise x_T from standard normal distribution \ud835\udca9(0, I) and iteratively apply Denoiser to denoise x_T to implement the state transition chain for generation. During the generation, the vertex coordinates contained in noise vector x_T are parallelly denoised to ground truth vertex coordinates, fully reflecting the characteristics of parallel generation.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Geometry Constraint\n\n\n\nThe main optimization constraint of vertex generation is point-to-point reconstruction loss (Eq.\u00a0<ref>). This loss can modify the prediction vertex set to one by one approach the ground truth vertex set and does not consider the geometry difference between two sets. Inspired by point-in-polygon problem of computational geometry, we introduce a simple geometry algorithm\u00a0(Angle Summation algorithm\u00a0<cit.>) to make an optimization objective. \n\nAngle Summation. Computing the sum of the angles between the test point and each pair of points making up the polygon. As shown in <ref>, if this sum is equal to 360^\u2218 then the point is inside the polygon. As shown in <ref>, if this sum is less than 360^\u2218 then the point is outside the polygon. Therefore, we can calculate an angle summation map \ud835\udc9c\u2208(0, 360]^h\u00d7 w according to the relative position between each pixel and vertexes of polygon V. The details of how to convert coordinates of polygon vertexes to angle summation polygon please refer to the appendix.\n\nTraining Objective. According to the \"Angle Summation\", prediction vertexes V and ground truth vertexes V\u0302 are converted to prediction angle summation map \ud835\udc9c and ground truth angle summation map \ud835\udc9c\u0302. The angle summation map reflects the global geometry of vertex vector so that we hope the prediction angle summation map to approach the ground truth angle summation for reducing geometry difference:\n\n    \u2112_g =  \ud835\udd3c_t\u223c\ud835\udcb0({1,\u22ef,T})\u2016\ud835\udc9c_t - \ud835\udc9c\u0302\u2016^2,\n\n\nwhere \u2112_g is named Angle Summation Loss\u00a0(ASL).\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n \u00a7.\u00a7 Experimental Setup\n\n\n\nOur model is evaluated on three standard referring image segmentation datasets: RefCOCO\u00a0<cit.>, RefCOCO+\u00a0<cit.> and RefCOCOg\u00a0<cit.>. \n\nThe maximum sentence length n is set 15 for RefCOCO, RefCOCO+, and 20 for RefCOCOg. The images are resized to 640\u00d7 640. The sampling number of mask vertexes N is set by default to 36. \nOther data preprocessing operations are generally in line with the previous methods\u00a0<cit.>.\nAs for the total diffusion step T, it is set to 1000 during training phrase. During inference phrase, T is set to 4 because DDIM step is adopted for accelerating sampling speed\u00a0<cit.>.\n\nBased on previous works <cit.>, mask IoU and det accuracy are adopted to evaluate the performance of methods.\n\nAdamW <cit.> is adopted as our optimizer, and the learning rate and weight decay are set to 5e-4 and 5e-2.  The learning rate is scaled by a decay factor of 0.1 at the 60th step. We train our models for 100 epochs on 4 NVIDIA V100 with a batch size of 64.\n\nAll of the quantitative analyses are based on the val split of RefCOCO dataset.\n\n\n\n \u00a7.\u00a7 Main Results\n\n\nReferring Expression Comprehension. Single-task part of Tab.\u00a0<ref> reports the comparison results between our method and previous referring expression comprehension methods.\n\nFrom Tab.\u00a0<ref>, our PVD boosts previous methods by a clear margin.\n\nFor example, PVD based on Darknet53\u00a0<cit.> respectively surpasses TransVG\u00a0<cit.> and TRAR\u00a0<cit.> with +2.74\u223c+6.13% and +3.58\u223c+7.73% absolute improvement on RefCOCO+.\n\nThe results show that our PVD generally achieves SOTA when compared to previous referring expression comprehension methods.\n\nBesides, we construct a stronger network based on Swin Transformer\u00a0<cit.> for achieving higher effectiveness, which gets larger improvement than previous methods.\n\n\n\n\n\nReferring Image Segmentation. Single-task part of Tab.\u00a0<ref> reports the comparison results between our method and previous methods.\n\nFor comparing to previous SOTA, i.e., vision transformer-based methods\u00a0(CoupleAlign, LAVT), we construct a stronger network based on Swin Transformer\u00a0<cit.>.\n\nIn this case, our PVD can outperform LAVT on all of datasets by +0.8\u223c+2.09% and CoupleAlign on most of datasets by +0.12\u223c1.4%, which demonstrates our PVD achieves SOTA for referring image segmentation task. \n\nUnified Visual Grounding. Multi-task part of Tab.\u00a0<ref> and Tab.\u00a0<ref> report the comparison results between our method and previous unified visual grounding methods.\n\nCompared to SOTA method\u00a0(SeqTR), our parallel vertex generation paradigm\u00a0(PVD w/ Darknet53) outperforms it by +0.74\u223c+1.46% for referring expression comprehension and +0.74\u223c+2.14% for referring image segmentation, which verifies the superiority of our paradigm.\n\n\n\n \u00a7.\u00a7 Quantitative Analysis\n\n\n\nHow does the number of points affect the effectiveness? \nThe main advantage of parallel vertex generation paradigm compared to sequential vertex generation paradigm is easier to scale to high-dimension tasks. To verify this statement, we check the effectiveness of two paradigm with different number of prediction points in Fig.\u00a0<ref>. Note that the number of points reflects the dimension of task. The figure provides two justifications: (1) the performance of sequential vertex generation is bottlenecked at 18 points, which substantiates the dimension dilemma of sequential vertex generation paradigm (the network is perturbed by error accumulation with large number of points and is underfitting to complex object with a small number of points). (2) the performance of parallel vertex generation stably increases with the number of points, which demonstrates that our paradigm is more scalable to high-dimension tasks.\n\nThe efficiency of Parallel Vertex Generation. Except for effectiveness, efficiency is also an aspect for verifying scalability. To further claim the advantage of parallel paradigm compared to sequential paradigm, we select several point settings\u00a0(\u201c9pts\", \u201c18pts\", \u201c27pts\", \u201c36pts\") to benchmark the efficiency of two paradigms in Tab.\u00a0<ref>. This table shows that the sequential paradigm is heavily impacted by the number of points and needs a large amount of extra computation overheads when scaling from a small number of points to a large number of points. For example, the inference speed becomes 4\u00d7 of previous speed when scaling from \u201c9pts\" to \u201c36pts\". However, our paradigm only requires a little extra overhead to scale to high-dimension tasks. Specifically, the inference speed only increases +9ms when scaling from \u201c9pts\" to \u201c36pts\".\n\n\n\n\n\n\n\nThe advantage of scalability. The scalability ensures that parallel vertex paradigm have favorable effectiveness and efficiency for a large number of points. This advantage makes methods based on our paradigm more robust to samples with complex contours than sequential paradigm. To quantitatively analyze the robustness, we define \u201cDifficulty Degree\" as the complexity metric and count the \u201cDifficulty-IoU\" density map of samples in Fig.\u00a0<ref> and Fig.\u00a0<ref>. The calculation details of the metric please refer to appendix. Comparing two figures, we can find that our paradigm still has high IoU for hard samples\u00a0(\u201cDifficulty Degree\" > 0.2) but the IoU of sequential paradigm heavily decreases for hard samples, which justifies that the scalability to high-dimension tasks of our paradigm prompts better robustness to hard samples than seuqnetial paradigm.\n\nThe effectiveness of Center Anchor Mechanism. As mentioned in Sec.\u00a0<ref>, CAM is proposed to cope with the high variance of unnormalized prediction vertex coordinates. To verify the effectiveness of CAM, we conduct ablation experiments. Tab.\u00a0<ref> shows that PVD w/ CAM boosts vanilla PVD by +1.86% Acc for referring expression comprehension task and +1.59% IoU for referring image segmentation task, which justifies the effectiveness of CAM.\n\nThe effectiveness of Angle Summation Loss. Since the original training objective of PVD only achieves point-level constraint between prediction and ground truth vertexes, we propose ASL in Sec.\u00a0<ref> for geometry constraint. In Tab.\u00a0<ref>, PVD w/ ASL improves vanilla PVD by +1.79% Acc for referring expression comprehension task and +2.12% IoU for referring image segmentation task. Besides, ASL also improves PVD w/ CAM. These results comprehensively verifies the effectiveness of ASL.\n\n\n\n \u00a7.\u00a7 Qualitative Analysis\n\n\nAs described in Sec.\u00a0<ref>, our parallel vertex generation paradigm is more effective and efficient than sequential vertex generation paradigm, especially for hard samples. To qualitatively verify this point, we select some easy cases and hard cases to illustrate the grounding difference between two paradigm. Fig.\u00a0<ref> shows that our parallel vertex generation paradigm generates high-quality vertexes of bounding box and mask contour but the sequential vertex generation paradigm easily hits error objects or generates inferior vertexes, which justifies our paradigm is more capable of grounding referring expression on the image.\n\n\n\n\u00a7 CONCLUSION\n\n\nIn this paper, we observe that the sequence vertex generation paradigm\u00a0(SeqTR) is trapped in a dimension dilemma although it is by far the best paradigm for unified visual grounding. On one hand, limited sequence length causes inferior fitting to objects with complex contours. On the other hand, generating high-dimensional vertex sequences sequentially is error-prone. To tackle this dilemma, we build a parallel vertex generation paradigm to better handle high-dimension settings. Since the diffusion model can scale to high-dimension generation by simply modifying the dimension of noise, it is adopted to instantiate our paradigm as Parallel Vertex Diffusion\u00a0(PVD) for pursuing highly scalable unified visual grounding. Subsequently, Center Anchor Mechanism\u00a0(CAM) and Angle Summation Loss\u00a0(ASL) are designed and introduced into vanilla PVD for improving convergence. Our PVD achieves SOTA on referring expression comprehension and referring image segmentation tasks. Moreover, PVD has a better fitting ability for objects with complex contours and requires fewer computation costs than SeqTR, which justifies our parallel paradigm is more scalable and efficient than sequential paradigm.\n\nieee_fullname\n\n\n\n"}