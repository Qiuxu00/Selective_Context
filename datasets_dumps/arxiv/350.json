{"entry_id": "http://arxiv.org/abs/2303.06840v1", "published": "20230313040642", "title": "DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion", "authors": ["Zixiang Zhao", "Haowen Bai", "Yuanzhi Zhu", "Jiangshe Zhang", "Shuang Xu", "Yulun Zhang", "Kai Zhang", "Deyu Meng", "Radu Timofte", "Luc Van Gool"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n    \n    \n    \n\n    \nDDFM: Denoising Diffusion Model for Multi-Modality Image Fusion\n    Zixiang Zhao^1,2  Haowen Bai^1  Yuanzhi Zhu^2  Jiangshe Zhang^1  Shuang Xu^3\n\n            Yulun Zhang^2  Kai Zhang^2  Deyu Meng^1  Radu Timofte^2,4  Luc Van Gool^2\n\n\t\t^1Xi\u2019an Jiaotong University   ^2Computer Vision Lab, ETH Z\u00fcrich\n\n            ^3Northwestern Polytechnical University   ^4University of Wurzburg\n\n\n    Received: date / Accepted: date\n============================================================================================================================================================================================================================================================================================================================\n\n\nempty\n\n\n\n\n    Multi-modality image fusion aims to combine different modalities to produce fused images that retain the complementary features of each modality, such as functional highlights and texture details. To leverage strong generative priors and address challenges such as unstable training and lack of interpretability for GAN-based generative methods, we propose a novel fusion algorithm based on the denoising diffusion probabilistic model (DDPM). The fusion task is formulated as a conditional generation problem under the DDPM sampling framework, which is further divided into an unconditional generation subproblem and a maximum likelihood subproblem. The latter is modeled in a hierarchical Bayesian manner with latent variables and inferred by the expectation-maximization algorithm. By integrating the inference solution into the diffusion sampling iteration, our method can generate high-quality fused images with natural image generative priors and cross-modality information from source images. Note that all we required is an unconditional pre-trained generative model, and no fine-tuning is needed. Our extensive experiments indicate that our approach yields promising fusion results in infrared-visible image fusion and medical image fusion. The code will be released.\n\n    \n\n\n\n\n\n\u00a7 INTRODUCTION\n\nImage fusion integrates essential information from multiple source images to create high-quality fused images\u00a0<cit.>, encompassing various source image types like digital\u00a0<cit.>, multi-modal\u00a0<cit.>, and remote sensing\u00a0<cit.>. This technology provides a clearer representation of objects and scenes, and has diverse applications such as saliency detection\u00a0<cit.>, object detection\u00a0<cit.>, and semantic segmentation\u00a0<cit.>. Among the different subcategories of image fusion, Infrared-Visible image Fusion (IVF) and Medical Image Fusion (MIF) are particularly challenging in Multi-Modality Image Fusion (MMIF) since they focus on modeling cross-modality features and preserving critical information from all sensors and modalities. Specifically, in IVF, fused images aim to retain both thermal radiation from infrared images and detailed texture information from visible images, thereby avoiding the limitations of visible images being sensitive to illumination conditions and infrared images being noisy and low-resolution. While MIF can assist in diagnosis and treatment by fusing multiple medical imaging modalities for precise detection of abnormality locations\u00a0<cit.>.\n\n\n\nThere have been numerous methods devised recently to address the challenges posed by MMIF\u00a0<cit.>, and generative models\u00a0<cit.> have been extensively utilized to model the distribution of fused images and achieve satisfactory fusion effects.\nAmong them, models based on Generative Adversarial Networks (GANs) <cit.> are dominant. The workflow of GAN-based models, illustrated in <ref>a, involves a generator that creates images containing information from source images, and a discriminator that determines whether the generated images are in a similar manifold to the source images.\nAlthough GAN-based methods have the ability to generate high-quality fused images, they suffer from unstable training, lack of interpretability and mode collapse, which seriously affect the quality of the generated samples. Moreover, as a black-box model, it is difficult to comprehend the internal mechanisms and behaviors of GANs, making it challenging to achieve controllable generation.\n\nRecently, Denoising Diffusion Probabilistic Models (DDPM) <cit.> has garnered attention in the machine learning community, which generates high-quality images by modeling the diffusion process of restoring a noise-corrupted image towards a clean image. Based on the Langevin diffusion process, DDPM utilizes a series of reverse diffusion steps to generate promising synthetic samples\u00a0<cit.>.\nCompared to GAN, DDPM does not require the discriminator network, thus mitigating common issues such as unstable training and mode collapse in GAN. Moreover, its generation process is interpretable, as it is based on denoising diffusion to generate images, enabling a better understanding of the image generation process\u00a0<cit.>.\n\nTherefore, we propose a Denoising Diffusion image Fusion Model (DDFM), as shown in <ref>c.\nWe formulate the conditional generation task as a DDPM-based posterior sampling model, which can be further decomposed into an unconditional generation diffusion problem and a maximum likelihood estimation problem. The former satisfies natural image prior while the latter is inferred to restrict the similarity with source images via likelihood rectification. Compared to discriminative approaches, modeling the natural image prior with DDPM enables better generation of details that are difficult to control by manually designed loss functions, resulting in visually perceptible images. As a generative method, DDFM achieves stable and controllable generation of fused images without discriminator, by applying likelihood rectification to the DDPM output.\n\nOur contributions are organized in three aspects:\n\n    \n  * We introduce a DDPM-based posterior sampling model for MMIF, consisting of an unconditional generation module and a conditional likelihood rectification module. Sampling of fused images is achieved solely by a pre-trained DDPM without fine-tuning.\n    \n  * In likelihood rectification, since obtaining the likelihood explicitly is not feasible, we formulate the optimization loss as a probability inference problem involving latent variables, which can be solved by EM algorithm. Then the solution is integrated into the DDPM loop to complete conditional image generation.\n    \n  * Extensive evaluation of IVF and MIF tasks shows that DDFM consistently delivers favorable fusion results, effectively preserving both the structure and detail information from the source images, while also satisfying visual fidelity requirements.\n\n    \n    \n    \n\n\n\n\n\u00a7 BACKGROUND\n\n\n\n \u00a7.\u00a7 Score-based diffusion models\n\nScore SDE formulation\n\nDiffusion models aim to generate samples by reversing a predefined forward process that converts a clean sample x_0 to almost Gaussian signal x_T by gradually adding noise.\nThis forward process can be described by an It\u00f4 Stochastic Differential Equation (SDE)\u00a0<cit.>:\n\n    dx = -\u03b2(t)/2x_tdt + \u221a(\u03b2(t))dw,\n\nwhere dw is standard Wiener process and \u03b2(t) is predefined noise schedule that favors the variance-preserving SDE\u00a0<cit.>.\n\nThis forward process can be reversed in time and still in the form of SDE <cit.>:\n\n    .9!dx = [-\u03b2(t)/2x_t - \u03b2(t) \u2207_x_tlog p_t(x_t)]dt + \u221a(\u03b2(t))dw,\n\nwhere dw corresponds to the standard Wiener process running backward and the only unknown part \u2207_x_tlog p_t(x_t) can be modeled as the so-called score function s_\u03b8(x_t,t) with denoising score matching methods, and this score function can be trained with the following objective <cit.>:\n\n    \ud835\udd3c_t\ud835\udd3c_x_0\ud835\udd3c_x_t | x_0 [s_\u03b8(x_t, t) - \u2207_x_tlog p_0t(x_t | x_0)_2^2 ],\n\nwhere t is uniformly sampled over [0, T] and the data pair (x_0,x_t) \u223c p_0(x) p_0t(x_t |x_0).\n\n\nSampling with diffusion models\nSpecifically, an unconditional diffusion generation process starts with a random noise vector x_T \u223c\ud835\udca9(0,\ud835\udc08) and updates according to the discretization of <ref>.\n\nAlternatively, we can understand the sampling process in the DDIM fashion <cit.>, where the score function can also be considered to be a denoiser and predict the denoised x\u0303_0|t from any state x_t at iteration t:\n\n    x\u0303_0|t = 1/\u221a(\u03b1\u0305_t)(x_t + (1 - \u03b1\u0305_t)s_\u03b8(x_t,t)),\n\nand x\u0303_0|t denotes the estimation of x_0 given x_t. We use the same notation \u03b1_t=1-\u03b2_t and \u03b1\u0305_t = \u220f_s=1^t \u03b1_s following Ho <cit.>.\nWith this predicted x\u0303_0|t and the current state x_t, x_t-1 is updated from\n\n    x_t-1 = \u221a(\u03b1_t)(1-\u03b1\u0305_t-1)/1-\u03b1\u0305_tx_t+\u221a(\u03b1\u0305_t-1)\u03b2_t/1-\u03b1\u0305_tx\u0303_0|t+\u03c3\u0303_t z,\n\nwhere z\u223c\ud835\udca9(0,\ud835\udc08) and \u03c3\u0303_t^2 is the variance which is usually set to 0.\nThis sampled x_t-1 is then fed into the next sampling iteration until the final image x_0 is generated.\nFurther details about this sampling process can be found in the supplementary material or the original paper <cit.>.\n\n\n\n\n\n\n\n\n\nDiffusion models applications\n\nRecently, diffusion models have been improved to generate images with better quality than previous generative models like GANs <cit.>.\n\nMoreover, diffusion models can be treated as a powerful generative prior and be applied to numerous conditional generation tasks.\nOne representative work with diffusion models is stable diffusion which can generate images according to given text prompts <cit.>.\nDiffusion models are also applied to many low-level vision tasks. For instance, DDRM <cit.> performs diffusion sampling in the spectral space of degradation operator \ud835\udc9c to reconstruct the missing information in the observation y.\nDDNM <cit.> shares a similar idea with DDRM by refining the null-space of the operator \ud835\udc9c iteratively for image restoration tasks.\nDPS <cit.> endorses Laplacian approximation to calculate the gradient of log-likelihood for posterior sampling and it is capable of many noisy non-linear inverse problems.\nIn \u03a0GDM <cit.>, the authors employ few approximations\nto make the log-likelihood term tractable and hence make it able to solve inverse problems with even non-differentiable measurements.\n\n\n\n\n\n \u00a7.\u00a7 Multi-modal image fusion\n\nThe deep learning-based multi-modality image fusion algorithms achieve effective feature extraction and information fusion through the powerful fitting ability of neural networks. Fusion algorithms are primarily divided into two branches: generative methods and discriminative methods.\nFor generative methods\u00a0<cit.>, particularly the GAN family, adversarial training\u00a0<cit.> is employed to generate fusion images following the same distribution as the source images.\nFor discriminative methods, auto encoder-based models\u00a0<cit.> use encoders and decoders to extract features and fuse them on a high-dimensional manifold.\nAlgorithm unfolding models\u00a0<cit.> combine traditional optimization methods and neural networks, balancing efficiency and interpretability.\nUnified models\u00a0<cit.> avoid the problem of lacking training data and ground truth for specific tasks.\nRecently, fusion methods have been combined with pattern recognition tasks such as semantic segmentation\u00a0<cit.> and object detection\u00a0<cit.> to explore the interactions with downstream tasks. Self-supervised learning\u00a0<cit.> is employed to train fusion networks without paired images. Moreover, the pre-processing registration module\u00a0<cit.> can enhance the robustness for unregistered input images.\n\n\n \u00a7.\u00a7 Comparison with existing approaches\n\nThe methods most relevant to our model are optimization-based methods and GAN-based generative methods.\nConventional optimization-based methods are often limited by manually designed loss functions, which may not be flexible enough to capture all relevant aspects and are sensitive to changes in the data distribution.\nWhile incorporating natural image priors can provide extra knowledge that cannot be modeled by the generation loss function alone.\nThen, in contrast to GAN-based generative methods, where unstable training and pattern collapse may occur, our DDFM achieves more stable and controllable fusion by rectifying the generation process towards source images and performing likelihood-based refinement in each iteration.\n\n\n\u00a7 METHOD\n\nIn this section, we first present a novel approach for obtaining a fusion image by leveraging DDPM posterior sampling. Then, starting from the well-established loss function for image fusion, we derive a likelihood rectification approach for the unconditional DDPM sampling.\nFinally, we propose the DDFM algorithm, which embeds the solution of the hierarchical Bayesian inference into the diffusion sampling. In addition, the rationality of the proposed algorithm will be demonstrated.\nFor brevity, we omit the derivations of some equations and refer interested readers to the supplementary material.\nIt is worth noting that we use IVF as a case to illustrate our DDFM, and MIF can be carried out analogously to IVF.\n\n\n\n \u00a7.\u00a7 Fusing images via diffusion posterior sampling\n\nWe first give the notation of the model formulation. Infrared, visible and fused images are denoted as i\u2208\u211d^HW, v\u2208\u211d^3HW and f\u2208\u211d^3HW, respectively.\n\nWe expect that the distribution of f given i and v, , p(f|i,v), can be modeled, thus f can be obtained by sampling from the posterior distribution.\nInspired by <ref>, we can express the reverse SDE of diffusion process as:\n\n    .896!d f=[-\u03b2(t)/2f-\u03b2(t) \u2207_f_tlog p_t(f_t|i,v)]dt+\u221a(\u03b2(t)) d w,\n\nand the score function, , \u2207_f_tlog p_t(f_t|i,v), can be calculated by:\n\n    .9!\u2207_f_tlog p_t(f_t|i,v)   =\u2207_f_tlog p_t(f_t)+\u2207_f_tlog p_t(i,v|f_t)\n       \u2248\u2207_f_tlog p_t(f_t)+\u2207_f_tlog p_t(i,v|f\u0303_0|t)\n\nwhere f\u0303_0|t is the estimation of f_0 given f_t from the unconditional DDPM. The equality comes from Bayes' theorem, and the approximate equation is proved in <cit.>.\n\nIn <ref>, the first term represents the score function of unconditional diffusion sampling, which can be readily derived by the pre-trained DDPM. In the next section, we explicate the methodology for obtaining \u2207_f_tlog p_t(i,v|f\u0303_0|t).\n\n\n\n \u00a7.\u00a7 Likelihood rectification for image fusion\n\nUnlike the traditional image degradation inverse problem y=\ud835\udc9c(x)+n where x is the ground truth image, y is measurement and \ud835\udc9c(\u00b7) is known, we can explicitly obtain its posterior distribution. However, it is not possible to explicitly express p_t(i,v|f_t) or p_t(i,v|f\u0303_0|t) in image fusion. To address this, we start from the loss function and establish the relationship between the optimization loss function \u2113(i,v,f\u0303_0|t) and the likelihood p_t(i,v|f\u0303_0|t) of a probabilistic model.\nFor brevity, f\u0303_0|t is abbreviated as f in <ref>.\n\n\n  \u00a7.\u00a7.\u00a7 Formulation of the likelihood model\n\nWe first give a commonly-used loss function\u00a0<cit.> for the image fusion task:\n\n    min_ff-i_1+\u03d5f-v_1 .\n\nThen simple variable substitution x=f-v and y=i-v are implemented, and we get\n\n    min_xy-x_1+\u03d5x_1.\n\nSince y is known and x is unknown, this \u2113_1-norm optimization equation corresponds to the regression model:\ny=kx + \u03f5\n\n\n\nwith k fixed to 1. According to the relationship between regularization term and noise prior distribution, \u03f5 should be a Laplacian noise and x is governed by the Laplacian distribution. Thus, in Bayesian fashion, we have:\n\n    p(x)   =\u2112\ud835\udc9c\ud835\udcab(x; 0, \u03c1) =\u220f_i, j1/2 \u03c1exp(-|x_i j|/\u03c1),\n    \n            p(y|x)   =\u2112\ud835\udc9c\ud835\udcab(y;x,\u03b3)=\u220f_i, j1/2 \u03b3exp(-|y_i j-x_i j|/\u03b3),\n\nwhere \u2112\ud835\udc9c\ud835\udcab(\u00b7) is the Laplacian distribution. \u03c1 and \u03b3 are scale parameters of  p(x) and p(y|x), respectively.\n\nIn order to prevent \u2113_1-norm optimization in <ref> and inspired by <cit.>, we give the <ref>:\n\n    \n    For a random variable (RV) \u03be which obeys a Laplace distribution, it can be regarded as the coupling of a normally distributed RV and an exponentially distributed RV, which in formula:\n    \n    .887!\u2112\ud835\udc9c\ud835\udcab(\u03be ; \u03bc, \u221a(b / 2))=\u222b_0^\u221e\ud835\udca9(\u03be ;\u03bc, a) \u2130\ud835\udcb3\ud835\udcab(a ; b) d a.\n\n\nTherefore, p(x) and p(y|x) in <ref> can be rewritten as the following hierarchical Bayesian framework:\n\n    {[ y_i j | x_i j, m_ij\u223c\ud835\udca9(y_i j ; x_i j, m_ij);                         m_ij\u223c\u2130\ud835\udcb3\ud835\udcab(m_ij ; \u03b3);          x_i j | n_i j\u223c\ud835\udca9(x_i j ; 0, n_i j);                       n_i j\u223c\u2130\ud835\udcb3\ud835\udcab(n_i j ; \u03c1) ].\n\nwhere i=1, \u2026, H and j=1, \u2026, W.\nThrough the above probabilistic analysis, the optimization problem in <ref> can be transformed into a maximum likelihood inference problem.\n\nIn addition, following\u00a0<cit.>, the total variation penalty item r(x) = \u2207x_2^2 can be also added to make the fusion image f better preserve the texture information from v, where \u2207 denotes the gradient operator. Ultimately, the log-likelihood function of the probabilistic inference issue is:\n\n    \u2113(x)     =log p(x, y)-r(x) \n        =-\u2211_i, j[(x_i j-y_i j)^2/2 m_ij+x_i j^2/2 n_i j]-\u03c8/2\u2207x_2^2,\n\nand probabilistic graph of this hierarchical Bayesian model is in <ref>b. Notably, in this way, we transform the optimization problem <ref> into a maximum likelihood problem of a probability model <ref>.\nAdditionally, unlike traditional optimization methods that require manually specified tuning coefficients \u03d5 in <ref>,  \u03d5 in our model can be adaptively updated by inferring the latent variables, enabling the model to better fit different data distributions. The validity of this design has also been verified in ablation experiments in <ref>.\nWe will then explore how to infer it in the next section.\n\n\n  \u00a7.\u00a7.\u00a7 Inference the likelihood model via EM algorithm\n\nIn order to solve the maximum log-likelihood problem in <ref>, which can be regarded as an optimization problem with latent variables, we use the Expectation Maximization (EM) algorithm to obtain the optimal x.\nIn E-step, it calculates the expectation of log-likelihood function with respect to p(a, b | x^(t), y), , the so-called \ud835\udcac-function:\n\n    \ud835\udcac(x | x^(t))=\ud835\udd3c_a, b | x^(t), y[\u2113(x)].\n\nThen in M-step, the optimal x is obtained by\n\n    x^(t+1)=max_x\ud835\udcac(x | x^(t)).\n\nNext, we will show the implementation detail in each step.\n\nE-step <ref> gives the calculation results for the conditional expectation of latent variables, and then gets the derivation of \ud835\udcac-function.\n\n    \n    The conditional expectation of the latent variable 1/m_ij and 1/n_ij in <ref> are:\n    \n    \ud835\udd3c_m_ij|x_ij^(t),y_ij[1/m_ij]   =\u221a(2(y_ij-x_ij^(t))^2/\u03b3),\n    \ud835\udd3c_n_ij|x_ij^(t)[1/n_ij]   =\u221a(2[x_ij^(t)]^2/\u03c1).\n\n\n\n    For convenience, we set m\u0303_i j\u22611 / m_ij and \u00f1_i j\u22611 / n_i j.\n    From <ref> we know that m_ij\u223c\u2130\ud835\udcb3\ud835\udcab(m_ij;\u03b3)=\u0393(m_ij;1,\u03b3). Thus, m\u0303_i j\u223c\u2110\ud835\udca2(1,\u03b3), where \u0393(\u00b7,\u00b7) and \u2110\ud835\udca2(\u00b7,\u00b7) are the gamma distribution and inverse gamma distribution, respectively.\n\n    Then we can get the posterior of m\u0303_i j by Bayes' theorem:\n    \n    log p(m\u0303_i j | y_i j, x_i j)=log p(y_i j | x_i j, m_ij)+log p(m\u0303_i j) \n    \n                =    -3/2logm\u0303_i j-m\u0303_i j(y_i j-x_i j)^2/2-1/\u03b3m\u0303_i j+ constant.\n\n    Subsequently, we have\n    \n    .887!p(m\u0303_i j | y_i j, x_i j)=\u2110 \ud835\udca9(m\u0303_i j;\u221a(2(y_i j-x_i j)^2 / \u03b3), 2 / \u03b3),\n\n    where \u2110\ud835\udca9(\u00b7,\u00b7) is the inverse Gaussian distribution. For the posterior of \u00f1_i j, it can be obtain similar to <ref>:\n    \n    log p(\u00f1_i j | x_i j)=log p(x_i j  |  n_i j)+log p(\u00f1_i j) \n        =-3/2log\u00f1_i j-\u00f1_i j x_i j^2/2-1/\u03c1\u00f1_i j+constant,\n\n    and therefore\n    \n    p(\u00f1_i j | x_i j)=\u2110 \ud835\udca9(\u00f1_i j ; \u221a(2 x_i j^2 / \u03c1), 2 / \u03c1).\n\n    Finally, the conditional expectation of 1/m_ij and 1/n_ij are the mean parameters of the corresponding inverse Gaussian distribution <ref>, respectively.\n\nAfterwards, the \ud835\udcac-function <ref> is derived as:\n\n    \ud835\udcac    =-\u2211_i, j[m_i j/2(x_i j-y_i j)^2+n_i j/2 x_i j^2]-\u03c8/2\u2207x_2^2 \n       \u221d -m\u2299(x-y)_2^2-n\u2299x_2^2-\u03c8\u2207x_2^2,\n\nwhere m_ij and n_i j represent \ud835\udd3c_m_ij|x_ij^(t),y_ij[1/m_ij] and \ud835\udd3c_n_ij|x_ij^(t)[1/n_ij] in <ref>, respectively. \u2299 is the element-wise multiplication. m and n are matrices with each element being \u221a(m_ij) and \u221a(n_ij), respectively.\n\nM-step Here, we need to minimize the negative Q-function with respect to x. The half-quadratic splitting algorithm is employed to deal with this problem, i.e.,\n\n    min_x,u,k ||m\u2299(x-y)||_2^2+||n\u2299x||_2^2+ \u03c8||u||_2^2, \n        s.t.  u=\u2207k, k=x.\n\nIt can be further cast into the following unconstraint optimization problem,\n\n    min_x,u,k    ||m\u2299(x-y)||_2^2+||n\u2299x||_2^2+ \u03c8||u||_2^2  \n       + \u03b7/2( ||u-\u2207k||_2^2+||k-x||_2^2 ) .\n\nThe unknown variables k,u,x can be solved iteratively in the coordinate descent fashion.\n\nUpdate k: It is a deconvolution issue,\n\n    min_k\u2112_k = ||k-x||_2^2+||u-\u2207k||^2_2.\n\nIt can be efficiently solved by the fast Fourier transform (fft) and inverse fft (ifft) operators, and the solution of k is\n\n    k =  ifft{ fft(x)+ fft(k_h)\u2299 fft(u)/1+ fft(k_h)\u2299 fft(k_h)},\n\nwhere \u00b7 is the complex conjugation.\n\nUpdate u: It is an \u2113_2-norm penalized regression issue,\n\n    min_u\u2112_u = \u03c8||u||_2^2+\u03b7/2 ||u-\u2207k||_2^2.\n\nThe solution of u is\n\n    u = \u03b7/2\u03c8+\u03b7\u2207k.\n\n\nUpdate x: It is a least squares issue,\n\n    min_x\u2112_x = ||m\u2299(x-y)||_2^2+||n\u2299x||_2^2+\u03b7/2||k-x||_2^2.\n\nThe solution of x is\n\n    x = (2m^2\u2299y+\u03b7k) \u2298 (2m^2+2n^2+\u03b7),\n\nwhere \u2298 denotes the element-wise division, and final estimation of f is\n\n    f\u0302 = x + v.\n\n\nAdditionally, hyper-parameter \u03b3 and \u03c1 in <ref> can be also updated after the sampling from x (<ref>) by\n\n    \u03b3 = 1/hw\u2211_i,j\ud835\udd3c[m_ij], \u03c1 = 1/hw\u2211_i,j\ud835\udd3c[n_ij].\n\n\n\n\n\n\n \u00a7.\u00a7 DDFM\n\nOverview\nIn <ref>, we present a methodology for obtaining a hierarchical Bayesian model from existing loss function and perform the model inference via the EM algorithm.\nIn this section, we present our DDFM, where the inference solution and diffusion sampling are integrated within the same iterative framework for generating f_0 given i and v. The algorithm is illustrated in <ref>.\n\nThere are two modules in DDFM, the unconditional diffusion sampling\u00a0(UDS) module and the likelihood rectification, or say, EM module.\nThe UDS module is utilized to provide natural image priors, which improve the visual plausibility of the fused image. The EM module, on the other hand, is responsible for rectifying the output of UDS module via likelihood to preserve more information from the source images.\n\nUnconditional diffusion sampling module\nIn <ref>, we briefly introduce diffusion sampling. In <ref>, UDS (in grey) is partitioned into two components, where the first part estimates f\u0303_0|t using f_t, and the second part estimates f_t-1 using both f_t and f\u0302_0|t. From the perspective of score-based DDPM in <ref>, a pre-trained DDPM can directly output the current \u2207_f_tlog p_t(f_t), while \u2207_f_tlog p_t(i,v|f\u0303_0|t) can be obtain by the EM module.\n\nEM module\nThe role of the EM module is to update f\u0303_0|t\u21d2f\u0302_0|t.\nIn <ref>, the EM algorithm (in blue and yellow) is inserted in UDS (in grey). The preliminary estimate f\u0303_0|t produced by DDPM sampling (line 5) is utilized as the initial input for the EM algorithm to obtain f\u0302_0|t (line 6-13), which is an estimation of the fused image subjected to likelihood rectification.\nIn other words, EM module rectify f\u0303_0|t to f\u0302_0|t to meet the likelihood.\n\n\n\n\n\n\n \u00a7.\u00a7 Why does one-step EM work?\n\nThe main difference between our DDFM and conventional EM algorithm lies in that the traditional method requires numerous iterations to obtain the optimal x, , the operation from line 6-13 in <ref> needs to be looped many times. However, our DDFM only requires one step of the EM algorithm iteration, which is embedded into the DDPM framework to accomplish sampling. In the following, we give <ref> to demonstrate its rationality.\n\n    \n    One-step unconditional diffusion sampling combined with one-step EM iteration is equivalent to one-step conditional diffusion sampling.\n\n\n    The estimation of f\u0302_0|t in conditional diffusion sampling, refer to <ref>, could be expressed as:\n    \n    \n\n        \n        \n        \n    f\u0302_0|t(f_t,i,v)=1/\u221a(\u03b1\u0305_t)[f_t+(1-\u03b1\u0305_t) s_\u03b8(f_t, i,v)]                        \n    \n                =          1/\u221a(\u03b1\u0305_t){f_t+(1-\u03b1\u0305_t)[s_\u03b8(f_t)+\u2207_f_tlog p_t(i,v|f_t)]}\n    \u2248   f\u0303_0|t(f_t)+1-\u03b1\u0305_t/\u221a(\u03b1\u0305_t)\u2207_f_tlog p_t(i,v|f\u0303_0|t)                                                                                      \n    \n                =          f\u0303_0|t(f_t)-\u03b6_t \u2207_x\u0303_0\u2112_x(i,v,x\u0303_0).\n\n    \n    <ref> are respectively based on the definition of Score-based DDPM, Bayes' theorem, and proof in <cit.>.\n    For <ref>, although optimization <ref> has a closed-form solution (<ref>), it can also be solved by gradient descent:\n    \n    .88!x\u0302_0 = x\u0303_0 + \u2207_x\u0303_0\u2112_x(k,u,x\u0303_0)\n                =x\u0303_0 + \u2207_x\u0303_0\u2112_x(i,v,x\u0303_0)\n\n    where the second equation holds true because as the input for updating x\u0302_0 (<ref>), k and u are functions of i and v. \u03b6_t in <ref> can be regraded as the update step size.\n\n    Hence, conditional sampling f\u0302_0|t(f_t,i,v) can be split as an unconditional diffusion sampling f\u0303_0|t(f_t) and one-step EM iteration  \u2207_x\u0303_0\u2112_x(i,v,x\u0303_0), corresponding to UDS module (part 1) and EM module, respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 INFRARED AND VISIBLE IMAGE FUSION\n\nIn this section, we elaborate on numerous experiments for IVF task to demonstrate the superiority of our method. More\nrelated experiments are placed in supplementary material.\n\n\n \u00a7.\u00a7 Setup\n\nDatasets and pre-trained model\nFollowing the protocol in <cit.>, IVF experiments are conducted on the four test datasets, , TNO\u00a0<cit.>, RoadScene\u00a0<cit.>, MSRS\u00a0<cit.>, and M^3FD\u00a0<cit.>. Note that there is no training dataset due to that we do not need any fine-tuning for specific tasks but directly use the pre-trained DDPM model. We choose the pre-trained model proposed by <cit.>, which is trained on ImageNet\u00a0<cit.>.\n\nMetrics\nWe employ six metrics including entropy (EN), standard deviation (SD), mutual information (MI), visual information fidelity (VIF), Q^AB/F, and structural similarity index measure (SSIM) in the quantitative experiments to comprehensively evaluate the fused effect. The detail of metrics is in <cit.>.\n\nImplement details\nWe use a machine with one NVIDIA GeForce RTX 3090 GPU for fusion image generation. All input images are normalized to [-1, 1]. \u03c8 and \u03b7 in <ref> are set to 0.5 and 0.1, respectively.\nPlease refer to the supplementary material for selecting \u03c8 and \u03b7 via grid search.\n\n\n\n \u00a7.\u00a7 Comparison with SOTA methods\n\nIn this section, we compare our DDFM with the state-of-the-art methods, including\n\n\n\n\n\n\n\nthe GAN-based methods group:\nFusionGAN\u00a0<cit.>,\nGANMcC\u00a0<cit.>,\nTarDAL\u00a0<cit.>, and\nUMFusion\u00a0<cit.>;\nand the discriminative methods group:\nU2Fusion\u00a0<cit.>,\nRFNet\u00a0<cit.>, and\nDeFusion\u00a0<cit.>.\n\nQualitative comparison\nWe show the comparison of fusion results in <ref>. Our approach effectively combines thermal radiation information from infrared images with detailed texture information from visible images. As a result, objects located in dimly-lit environments are conspicuously accentuated, enabling easy distinguishing of foreground objects from the background. Moreover, previously indistinct background features due to low illumination now possess clearly defined edges and abundant contour information, enhancing our ability to comprehend the scene.\n\nQuantitative comparison\nSubsequently, six metrics previously mentioned are utilized to quantitatively compare the fusion outcomes, as presented in <ref>. Our method demonstrates remarkable performance across almost all metrics, affirming its suitability for different lighting and object categories.\nNotably, the outstanding values for MI, VIF and Qabf across all datasets signify its ability to generate images that adhere to human visual perception while preserving the integrity of the source image information.\n\n\n \u00a7.\u00a7 Ablation studies\n\nNumerous ablation experiments are conducted to confirm the soundness of our various modules. The above six metrics are utilized to assess the fusion performance for the experimental groups, and results on the Roadscene testset are displayed in <ref>.\n\nUnconditional diffusion sampling module\nWe first verify the effectiveness of DDPM. In Exp.\u00a01, we eliminate the denoising diffusion generative framework, thus only the EM algorithm is employed to solve the optimization <ref> and obtain the fusion image. In fairness, we keep the total iteration number consistent with DDFM.\n\nEM module\nNext, we verify the components in the EM module. In Exp.\u00a02, we removed the total variation penalty item r(x) in <ref>. Then, we remove the Bayesian inference model. As mentioned earlier, \u03d5 in <ref> can be automatically inferred in the hierarchical Bayesian model. Therefore, we manually set \u03d5 to 0.1 (Exp.\u00a03) and 1 (Exp.\u00a04), and used the ADMM algorithm to infer the model.\n\nIn conclusion, the results presented in Tab.\u00a0<ref> demonstrate that none of the experimental groups is able to achieve fusion results comparable to our DDFM, further emphasizing the effectiveness and rationality of our approach.\n\n\n\n\u00a7 MEDICAL IMAGE FUSION\n\nIn this section, MIF experiments are carried out to verify the effectiveness of our method.\n\n\nSetup\nWe choose 50 pairs of medical images from the Harvard Medical Image Dataset\u00a0<cit.> for the MIF experiments, including image pairs of MRI-CT, MRI-PET and MRI-SPECT. The generation strategy and evaluation metrics for the MIF task are identical to those used for IVF.\n\nComparison with SOTA methods\nQualitative and quantitative results are shown in <ref>. It is evident that DDFM retains intricate textures while emphasizing structural information, leading to remarkable performance across both visual and almost all numerical metrics.\n\n\n\n\u00a7 CONCLUSION\n\nWe propose DDFM, a novel generative image fusion algorithm based on the denoising diffusion probabilistic model (DDPM).\nThe generation problem is split into an unconditional DDPM to leverage image generative priors and a maximum  likelihood subproblem to preserve cross-modality information from source images.\nWe model the latter using a hierarchical Bayesian approach and its solution based on EM algorithm can be integrated into unconditional DDPM to accomplish conditional image fusion.\nExperiments on infrared-visible and medical image fusion demonstrate that DDFM achieves promising fusion results.\n\n\n\n\n\n\n\n\n\nieee_fullname\n    \n\n\n"}