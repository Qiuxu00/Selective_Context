{"entry_id": "http://arxiv.org/abs/2303.06980v2", "published": "20230313103002", "title": "Self-supervised based general laboratory progress pretrained model for cardiovascular event detection", "authors": ["Li-Chin Chen", "Kuo-Hsuan Hung", "Yi-Ju Tseng", "Hsin-Yao Wang", "Tse-Min Lu", "Wei-Chieh Huang", "Yu Tsao"], "primary_category": "cs.LG", "categories": ["cs.LG"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL-C. Chen et al.: Generalized laboratory progress pretrain model for cardiovascular diseases progress forecasting\n\n\n[\n\\begin@twocolumnfalse\n  \n\nSelf-supervised based general laboratory progress pretrained model for cardiovascular event detection\n    Li-Chin Chen^1, \n        Kuo-Hsuan Hung^1,\n        Yi-Ju Tseng^2, \u00a0Member,\u00a0IEEE,\n        Hsin-Yao Wang^3, \n        Tse-Min Lu^4,5,6,\n\t  Wei-Chieh Huang^7,4,6,\n        Yu Tsao^1, \u00a0Senior Member,\u00a0IEEE\n       \n\n    March 30, 2023\n===================================================================================================================================================================================================================\n\n\n\n\n\n\n\n\nObjective: \nRegular surveillance is an indispensable aspect of managing cardiovascular disorders. Patient recruitment for rare or specific diseases is often limited due to their small patient size and episodic observations, whereas prevalent cases accumulate longitudinal data easily due to regular follow-ups. These data, however, are notorious for their irregularity, temporality, absenteeism, and sparsity. In this study, we leveraged self-supervised learning (SSL) and transfer learning to overcome the above-mentioned barriers, transferring patient progress trends in cardiovascular laboratory parameters from prevalent cases to rare or specific cardiovascular events detection. Methods and procedures: We pretrained a general laboratory progress (GLP) pretrain model using hypertension patients (who were yet to be diabetic), and transferred their laboratory progress trend to assist in detecting target vessel revascularization (TVR) in percutaneous coronary intervention patients. GLP adopted a two-stage training process that utilized interpolated data, enhancing the performance of SSL. After pretraining GLP, we fine-tuned it for TVR prediction. Results: The proposed two-stage training process outperformed SSL. Upon processing by GLP, the classification demonstrated a marked improvement, increasing from 0.63 to 0.90 in averaged accuracy. All metrics were significantly superior (p < 0.01) to the performance of prior GLP processing. The representation displayed distinct separability independent of algorithmic mechanisms, and diverse data distribution trend. Conclusion: Our approach effectively transferred the progression trends of cardiovascular laboratory parameters from prevalent cases to small-numbered cases, thereby demonstrating its efficacy in aiding the risk assessment of cardiovascular events without limiting to episodic observation. The potential for extending this approach to other laboratory tests and diseases is promising. \n\n\nClinical impact: This study demonstrated the efficacy of laboratory progress transformation \n between patient groups, demonstrating that the trend of prevalent cases is informative for small-numbered events detection without limited to episodic observations.  \n\n\n\n\nCardiovascular diseases, cardiometabolic disease, laboratory examinations, time-series data, representation learning, self-supervised learning, transfer learning.\n\n\n\\end@twocolumnfalse]\n\n\n\n  \n  [1]^1 Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan.\n  [2]^2 Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan.\n  [3]^3 Department of Laboratory Medicine, Chang Gung Memorial Hospital at Linkou, Taoyuan City, Taiwan.\n  [4]^4 Division of Cardiology, Department of Internal Medicine, Taipei Veterans General Hospital, Taipei, Taiwan.\n  [5]^5 Department of Health Care Center, Taipei Veterans General Hospital, Taipei, Taiwan.\n  [6]^6 Department of Internal Medicine, School of Medicine, College of Medicine, National Yang Ming Chiao Tung University, Taipei, Taiwan.\n  [7]^7 Department of Biomedical Engineering, National Taiwan University, Taipei, Taiwan.\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegular monitoring is indispensable in the management of cardiovascular diseases  <cit.>. Laboratory analysis constitutes a vital component, involving multifarious chemical tests that scrutinize blood, urine, or body tissue specimens. These tests gauge the body's response to food intake, medication, and treatment, thus providing crucial insights into disease progression and signaling the need for medication or dietary modifications. For chronic diseases, laboratory results are more meaningful when observed longitudinally rather than episodically. Extensive longitudinal data have been amassed for prevalent diseases such as hypertension (HTN) and diabetes mellitus (DM). On the contrary, rare or specific cases are often characterized by a diminutive patient population and episodic observations, which curtail the incorporation of machine learning technology in their progress assessment. However, transfer learning, which improves a classifier from one domain (source domain) with more readily obtainable data, and applies this knowledge to another domain (target domain), is a promising approach <cit.>. The transfer of progress from prevalent cases to rare or specific cases were seldom discussed. This study aims to employ self-supervised learning (SSL) to pretrain a generalized laboratory progress (GLP) model that captures the overall progression of common laboratory markers in prevalent cardiovascular cases and transfer this progress to aid in the evaluation of target vessel revascularization (TVR) occurrence in percutaneous coronary intervention (PCI) patients.\n\n\n\n \u00a7.\u00a7 Problematic nature of laboratory data\n \n\nFor rare or specific diseases, patient recruitment is typically limited due to small patient size and episodic observations. Patient status at a single time point, as observed in cross-sectional studies, provides only a temporary snapshot and preliminary insights into future disease progression. In contrast, longitudinal observations, as seen in cohort studies, offer a more complete understanding of disease development <cit.>. However, the collection of such data over an extended period can be complex, time-consuming, and expensive, often resulting in patient dropout and limiting the availability of data to small patient groups or episodic events <cit.>.\n\nFor prevalent cardiovascular cases, longitudinal data are more readily available due to regular follow-ups. However, these observations are heavily reliant on patient adherence, insurance regulations, clinical guidelines, and the clinical judgment of physicians. Any disruption to these factors can lead to irregularity and sparsity; that is, the observation may be skipped, or it may be sampled irregularly or over a prolonged period <cit.>. Similarly to electronic health records (EHRs), laboratory test records contain a wealth of abundant and longitudinal patient information but are notorious for their irregularity, temporality, and sparsity, often with noisy outliers and missing values <cit.>. \n\nTo overcome these challenges, we propose the following solutions: (1) For small patient populations and episodic observations, we designed a laboratory progress-specific pretrain model to capture the temporal latent representation of prevalent cases and transfer disease progress to small-numbered cases. (2) To address irregularity, absenteeism, and sparsity, we adopted SSL to estimate missing or absent data in longitudinal observations.\n\n\n\n \u00a7.\u00a7 Related works\n \n\nMachine learning techniques have been extensively studied to enable targeted tasks using EHRs <cit.>. These approaches include vector-based representations, which transform each patient into a vector, sequential-based representations that capture clinical events based on temporal distance <cit.>, graph-based representations that represent the relationship between clinical events using a compact graph, and tensor-based representations that use high-dimensional tensors to model patient-level data, incorporating diagnosis, medication, treatments, and clinical events in each dimension <cit.>. SSL has recently drown attention due to its ability to label training data from the data itself <cit.>. The combination of SSL and semi-supervised methods have been proposed to impute corrupted value and training on unlabeled data in genomics and clinical data <cit.>. Similarly, continuous value embedding, represented as a #time \u00d7 #feature \u00d7 #value vector, has been utilized to process irregular, absent, and sparse observations from ICUs, leveraging SSL <cit.>. \n\nOne of the most renowned language-pretrained models is the Bidirectional Encoder Representations from Transformers (BERT) <cit.>, which analyzes and predicts future words based on prior words. It has been adapted for the medical domain and trained using combinations of general and biomedical-specific corpora, such as BioBERT, BlueBERT, and PubMedBERT <cit.>, to learn temporal latent representations from extensive data and transfer the learned embedding to downstream tasks. Unlike previous studies that aimed to process all EHR components in a single design, BERT-based models primarily focus on clinical notes.\n\n\n\n\n\n\n \u00a7.\u00a7 Cardiovascular disease and associated risk factors\n\nCardiovascular disease is among the leading causes of death worldwide. Currently, the known traditional risk factors include HTN, DM, and smoking <cit.>. These risk factors may result in endothelial injury, plaque formation, and coronary thrombus <cit.>, leading to the progression of cardiovascular disease. PCI has been widely used to treat cardiovascular disease <cit.>. Although the clinical application of drug-eluting stents has significantly reduced the incidence of TVR in recent years <cit.>, preventing TVR and reducing re-admission rates remain major clinical issues in cardiovascular medicine after PCI. TVR is associated with complicated pathophysiological mechanisms, including lipid metabolic disorders <cit.> and inflammatory disease <cit.>. Regarding the time interval of cardiac events, patients who undergo PCI are at risk of subsequent cardiac events, including TVR <cit.>. However, a consensus on accurate preprocedural risk stratification and prognosis assessment to identify high-risk patients before PCI has not yet been reached.\n\n\n\n \u00a7.\u00a7 Laboratory markers of cardiovascular diseases\n\nEarlier studies indicated that preprocedural parameters were associated with cardiovascular disease. Total cholesterol levels (Chol) and low-density lipoprotein cholesterol (LDL-c) are strongly associated with cardiometabolic diseases and accepted in diagnostic practices. The plasma level of high-density lipoprotein cholesterol (HDL-c) and risk for cardiovascular diseases show an inverse relationship <cit.>. The relationship between circulating white blood cells (WBCs) and cardiovascular outcomes were noted in clinical studies, which showed that elevated WBC count increased short- and long-term risk in patients with acute coronary syndromes (ACSs) <cit.>; Hage et al. <cit.> reported that restenosis was predicted by baseline fasting blood glucose (glucose AC), suggesting that focusing on lowering glucose rather than the tool to normalize glucose is more beneficial <cit.>. Serum uric acid (UA) was indicated as a prognostic cardiovascular biomarker, predicting total and cardiovascular mortality in the setting of secondary prevention of coronary artery disease within the framework of the Verona Heart Study <cit.>.\n\nThe National Cholesterol Education Program III (NCEP III) recommends Chol or LDL-c in combination with HDL-c (Chol/HDL-c, LDL-c/HDL-c) as markers for the screening and treatment of patients with cardiovascular disease, as well as the 10-year risk Framingham scoring assessment <cit.>. Therefore, we accumulated six preprocedural parameters in this study, namely, the ratio of Chol and HDL-c (Chol/HDL-c), LDL-c, the ratio of LDL-c and HDL-c (LDL-c/HDL-c), glucose AC, WBC, and UA.\n\n\n\n\n\n\u00a7 METHODOLOGY\n\nOur objective is to construct a pretraining model that captures the laboratory progress of general cases and transfers this information to assist in predicting cardiac events in specific patient groups. The following section provides a detailed description of (1) patient recruitment and datasets, which include both source and target domain datasets, (2) the experimental and model design, as well as the model training process for GLP and the fine-tuning classifier, and (3) validation of both tasks.\n\n\n\n \u00a7.\u00a7 Patient recruitment and datasets\n\nTwo datasets were obtained from two diverse medical institutes. The source domain dataset was obtained from the Chang Gung Research Database <cit.>, a multi-institutional electronic medical records database comprising original medical records of seven medical institutes in Taiwan. We included patients who were diagnosed with HTN before DM. Patients diagnosed with hypertension with age less than 40 years, with any oncology visits, or with observations of less than a year were excluded. The diagnosis date was indicated by the International Classification of Diseases (ICD) encoding or medication prescription date. The ICD codes and medications used for the indications are listed in Appendix\u00a0<ref>. When a patient had been coded as having HTN or DM at least twice a year, the disease onset date was defined using the first coded date. If the first medication prescription date was earlier than the coded date, then the earlier date was defined as the date of disease onset. The date of diagnosis between HTN and DM was set to be \u2265 three months. We collected the data of patients between their HTN and DM onset; that is, the patient was identified as having HTN onset, but was yet to be determined as having DM. We collected demographic information and laboratory data of the recruited patients, including age, sex, Chol/HDL-c, LDL-c, LDL-c/HDL-c, glucose AC, WBC, and UA. Probable erroneous values such as \"NA\" or \".\" were excluded. A total of 9,720 patients were included, and laboratory data were collected between January 2001 and December 2019.\n\n\n\nThe targeted domain dataset was obtained from the Taipei Veterans General Hospital, which is a tertiary hospital located in northern Taiwan. We recruited 891 patients with noninvasive evidence of myocardial ischemia who underwent PCI between January 2005 and January 2022. Patients were excluded if they had ACS, acute decompensated congestive heart failure, acute and chronic infections, autoimmune diseases, malignancy with an expected life span of less than one year, unstable hemodynamic status, or those unable to take dual antiplatelet therapy. Angiographically successful coronary intervention was defined as residual stenosis of less than 30%, and coronary thrombolysis in myocardial infarction grade 3 flow was achieved at the end of the procedure without major complications. All patients were followed up and traced for TVR requirements. The patients required to receive TVR were labeled as occurring, and the dates were recorded. Patients without TVR by the end of January 2022 were labeled as non-occurring, and the date was recorded as January 31, 2022. In this dataset, the information included age, sex, PCI date, TVR date, and the six laboratory values mentioned above, which were collected when PCI was performed. The time differences between PCI and TVR dates were calculated. As the imputation of missing values may diverge the main attempt of this study, patients with incomplete data were excluded; the remaining 483 patients were included in the analysis.\n\nThe source domain dataset are longitudinal observations that consists of multiple events, whereas the target domain dataset are episodic records that consists of one observation event for each patient. All patient data were de-identified prior to analysis. This study was approved by the Institutional Review Board of the Chang Gung Medical Foundation (No. 202000376B0) and Taipei Veterans General Hospital (No. 2019-12-012CC).\n\n\n\n \u00a7.\u00a7 Pretrain model experiment design\n\nOur aim is to develope a pretraining model that captures the laboratory progress of patients and predict their observed values for the subsequent month. We have implemented SSL to estimate irregular and absent data in longitudinal observations <cit.>. Moreover, previous studies have shown that hierarchical pretraining leads to improved performance representations <cit.>. Therefore, we propose a two-stage training process that initially learns general laboratory progress information based on interpolated data (Stage 1), followed by refinement into domain-specific progression through SSL (Stage 2). In our experiment, we used SSL as a baseline and compared it with the following training approaches: (1) a two-stage training process, whereby GLP first attains a local minimum loss with interpolated data and then makes domain-specific adjustments using SSL; (2) training with interpolated data (pure Stage 1 training); and (3) training with hybrid data, which mixes interpolated data with irregular and absent data during training. The models were trained simultaneously for supervised and self-supervised learning.\n\n\n\n \u00a7.\u00a7 Longitudinal data framing\n\nIn this study, the sequential results were framed into interpolated and non-interpolated data. Interpolated data were used in Stage 1 training, and non-interpolated data were used in Stage 2. Defining the laboratory observations of each patient during the study period as y_0, y_1, \u2026, y_n at time t_0, t_1, \u2026, t_n, where n symbolized the months in the timeline. Setting y_0 and y_n as real observed values, let y_m on t_m symbolize the second-last real observed values of a patient. Interpolation occurred from t_0 to t_m when y_i was missing. Given a defined range r, we framed the interpolated data as t_i:t_i+r, and the next frame would take a step forward (i+1) as long as i+r \u2264 m-1. These frames were the input for Stage 1 training, and the prediction target was set at t_i+r+1, where i+r+1 \u2264 m. The frame is omitted if t_i, t_m-1\u2264 r. \n\nFrame t_m-r:t_m and the last observed value y_n on t_n were isolated from Stage 1, which were not interpolated and became the training data for Stage 2. Starting from frame t_m-r:t_m, the model predicted t_m+1, and the next input frame became t_m-r+1:t_m+1 with the prediction target located on t_m+2, until the prediction target reached t_n. The predicted data becomes parts of the training data in the next frame; that is, the model learns from the prediction it generated. In summary, there is no time gap g between the input frame and the prediction target (g = 0) in Stage 1, and traditional supervised learning was performed; whereas in Stage 2, g = n-m-1 and g > 0, therefore, SSL was performed. The framing process is illustrated in Fig.\u00a0<ref>. Due to coronary stent trials focus on target vessel/lesion-related clinical outcomes in the shorter term with a particular emphasis on outcomes in the first 12-months post-PCI <cit.>, r was set to 12 months. \n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Interpolation methods\n\n\n\nIn Stage 1 training, three interpolation methods were examined: linear, piecewise cubic Hermite interpolating polynomial (PCHIP), and barycentric interpolation. We used interpolations to estimate the irregularity and absented values between two observations based on monthly visits. In linear interpolation, the values were estimated based on the slope of the known observations, denoted as:\n\n    \u0177_j = y_i + (t_j - t_i)(y_k-y_i)/(t_k- t_i)\n\n, where t indicates the time of the estimation, \u0177_j is the estimated value at t_j, and i < j < k. In the PCHIP interpolation <cit.>, denoting d_j = (y_k-y_j)/(t_k- t_j) as the slopes at x_j, when the signs of d_j and d_i are different, or either of them equals zero, \u0177_j = 0. Otherwise, it is given by the weighted harmonic mean, formulated as\n\n    \u0177_j = (w_1+w_2)/w_1/d_i+w_2/d_j\n\n, where w_1 = 2(t_k - t_j) + (t_j-t_i) and w_2 = (t_k - t_j) + 2(t_j - t_i). Finally, barycentric interpolation <cit.> was performed through a given set of nodes x_0, x_1, \u2026, x_n and masses w_0, w_1, \u2026, w_n, finding function w_0(x), w_1(x), \u2026, w_n(x) such that:\n\n    x = \u2211^n_i=0w_i(x)x_i/\u2211^n_i=0w_i(x)\n, and x is the barycenter of the nodes, which can be used to interpolate through \n\n    \u0177 = \u2211^n_i=0b_i(x)f_i\n\n, where b_i is the linear function. Fig.\u00a0<ref> visualized the same segmented period of glucose AC based on different interpolation methods.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Self-supervised learning based on autoregressive model\n\nIn Stage 2, we adopted an autoregressive model to perform SSL, which received inputs from a time series regressed on previous inputs from the same time series. The probability of each input is conditioned on the previous input, and can be formulated as:\n\n\n    max_\u03b8 p_\u03b8(x) = \u2211_t=1^Tlog p_\u03b8(x_t| x_1:t-1)\n\n, where x_t denotes the input at time t, p_\u03b8 denotes the probability, and max_\u03b8 p_\u03b8 denotes the maximized likelihood <cit.>. In summary, the model learned the progress trend by paving the self-predicted path toward the prediction target.\n\n\n\n \u00a7.\u00a7 GLP model design\n\n\n\n\nFig.\u00a0<ref> illustrates the model design of GLP, which comprises of two blocks: the longitudinal iterative block (LIB) and the condense block (CB). The LIB is made up of a bidirectional long short-term memory (BiLSTM) layer and a fully connected (FC) condensing layer. ReLU activation function is applied after each layer to enhance the non-linearity of the model. BiLSTM processes the input data in both forward and backward directions <cit.>, which helps to capture more contextual information from the past data. The number of hidden nodes for BiLSTM was set to 5. The FC condensing layer is used to condense the output of BiLSTM layer back into the original input, thus enabling an autoregressive flow. The CB, on the other hand, is composed of an FC layer followed by a ReLU activation function, which is applied twice. The number of hidden nodes for CB were set to 5, 2, 2, and 1. When g = 0, the input passes through LIB once and then enters CB. However, when g > 0, the process iterates until the timeline before the prediction target is reached.\n\n\n\n \u00a7.\u00a7 Input vector and normalization\n\nThe data that was collected was comprised of five input features, including age, gender, certainty mask, discrete value encoding, and normalized laboratory values. Numeric values, such as age and laboratory values, underwent normalization using the natural logarithm of one plus the input (log1p). This projection of values into a vector space above zero prevents potential errors that could have arisen from maldistribution between positive and negative values. Furthermore, log1p is accurate for small values of x, thus ensuring that 1+x=1 in floating-point accuracy without producing a significant shift from the original value <cit.>. The use of log1p also precludes the leakage of data distribution information <cit.>. The scaling of data is unnecessary as maximum and minimum values were not required.\n\nOne-hot encoding was employed for gender, certainty mask, and discrete value encoding. Gender was encoded as binary by assigning male and female values of 1 and 0, respectively. The certainty mask was binary-encoded to indicate whether the value was real or estimated through interpolation (true observation/estimated value). The discrete value encoding categorized laboratory results into two groups (low and high) or three groups (low, normal, and high) based on different thresholds (summarized in Appendix\u00a0<ref>). The number of certainty mask (certain) was a parameter set during model training. It specifies the number of real observations in a frame that symbolized the necessity for the models to generate substantial predictions. Patients were required to return every three months for cardiovascular disease examinations, resulting in a maximum of four real observations in a 12-month frame, in line with insurance regulations. Therefore, certain was verified from 0 to 5. The optimal certain was selected as the parameter setting for GLP. \n\n\n\n \u00a7.\u00a7 Training and validation settings for GLP\n\n\nDespite employed training approaches, the data were randomly split into training and testing datasets at a ratio of 80:20. All training processes applied the 5-fold cross validation technique, and the reported results reflect the mean of five repetitions of the training process <cit.>. Our main objective was to train the model to forecast future progression during the r/2 month period. The final reported validation was carried out in forecasting y_n without interpolation support, as this approach is impractical in real-world scenarios where future data is unknown. Only true observation values that were segmented as the prediction target were employed as the model validation data.\n\nIn this study, Mean squared error (MSE) was used as the model convergence loss, while R-squared (R^2) was used as the performance metric for evaluating the model performance. MSE measures the average squared difference between the predicted values and the actual values. The lower the MSE, the better the model's performance. R^2, on the other hand, measures the proportion of the variance in the dependent variable that is predictable from the independent variables in the model. Normally, R^2 ranges from zero to one. A value of one means that the model perfectly fits the data, while a value of zero means that the model does not fit the data. When R^2 < 0 means the model performs worse than the horizontal mean line that went through the mean value of data. The statistical significance of the differences in model performance was assessed using an independent T-test, with a significance level of p < 0.05.\n\n\n\n \u00a7.\u00a7 Fine-tune classifier for TVR prediction\n\nLaboratory analyses in TVR prediction were obtained concomitantly with percutaneous coronary intervention (PCI). Subsequently, g was computed based on the temporal disparity between PCI and TVR dates using follow-up months. Patient information, comprising gender, age, and six laboratory parameters, was collected and normalized congruously with aforementioned procedures. All episodic observations were assigned with certainty masks as value one. Given the uneven distribution of patients between those with TVR occurrence (42) and those without (441), we performed balanced sampling prior to introducing data into the fine-tune classifier training. Therefore, only 84 patients were included in the training due to random downsampling of patients without TVR. The fine-tune classifier was trained with non-neural network algorithms based on outputs of frozen GLP (shown in Fig.\u00a0<ref>). Non-neural network algorithms necessitates fewer samples than deep neural networks to achieve consistent performance <cit.>. The algorithms selected was based on their diverse mechanisms, including Light Gradient Boosting Machine (LightGBM), Support Vector Machine (SVM), Logistic Regression (LR), and K-Nearest Neighbors (KNN). \n\nMoreover, we compared the latent progress representation produced by GLP by extracting the outputs of LIB (Progress_emb) and CB (Progress_out) as delineated in Fig.\u00a0<ref>. Subsequently, the predictive results based on the original data (normalized, but not processed with GLP), Progress_emb, and Progress_out were compared. The reported performance was also the mean value obtained after executing the training process five times, with TVR negative cases randomly downsampled and data partitioned into training and testing sets. The fine-tune classifier is a binary classifier distinguishing TVR occurrence (Yes/No). The assessment metrics for TVR incidence included the area under the receiver operating characteristic curve (AUROC), accuracy, sensitivity, specificity, precision, and F1 score <cit.>. The differentiation in evaluation metrics between the original data and the representations was also statistically validated using an independent T-test to determine whether the differentiation achieved statistical significance.\n\nFurthermore, we plotted the distribution of the original data, Progress_out when GLP reached g/2, and Progress_out when GLP reached g to graphically represent the variations throughout the process. g/2 signifies that the iteration did not reach the targeted incident, indicating outputs when GLP had not reached the targeted time yet.\n\n\n\n\u00a7 RESULTS\n\nTable\u00a0<ref> illustrates the demographic information of the patients enlisted from the two datasets. Notably, patients in the source domain (HTN to DM patients) are comparatively younger than those in the target domain (TVR patients), and it is observed that TVR usually occurs within a period of 2.32 \u00b1 2.64 years. Fig.\u00a0<ref>a showcases the averaged R^2 of different training approaches (encompassing all interpolation methods with certain ranging from 0 to 5). The results indicate that SSL and two-stage training processes exhibit similar performances, with both achieving mean R^2 = 0.46; in contrast, interpolated and hybrid training techniques achieved lower mean with larger variation. Figs.\u00a0<ref>b to d, R^2 of GLP training based on different training approaches with certain ranging from 0 to 5 is depicted. Notably, the two-stage training process (as shown in Fig.\u00a0<ref>b) surpasses SSL (baseline) as the only approach that outperforms with mean values of 0.49 (p = 0.508) and 0.48 (p = 0.603) for linear and PCHIP interpolation, respectively. While not statistically significant, our findings suggest that the two-stage training process outperformed SSL. Moreover, linear interpolation performs better than PCHIP (p = 0.824) and barycentric (p = 0.031), while PCHIP outperforms barycentric (p = 0.046). On the other hand, other approaches (depicted in Fig.\u00a0<ref>c and d) appear to have a weaker performance than the horizontal mean line (R^2 < 0) and larger variation.\n\n\n\n\n\n\n\nTable\u00a0<ref> presents a summary of the best-performing certain setting for each examination, which was found to be inconsistent among different examinations. The response of individual examinations to GLP was observed to be different. As previously mentioned, the two-stage training process outperformed SSL in most examinations, although the degree of improvement was small. While specifying certain did improve model performance (with an increment from 0.49 to 0.57 for linear interpolation), the correlation between R^2 and certain was found to be weak when verified with a Pearson correlation test (linear: -0.002, PCHIP: 0.026, and barycentric: -0.040). Detailed certain performances of each examination are provided in Appendix\u00a0<ref>. Based on the best performed linear interpolation, we trained the GLP pretrain model by leveraging optimized certain settings tailored to each laboratory test. \n\n\n\n\n\nTable\u00a0<ref> demonstrates the fine-tuned results of the target domain task. Upon being processed by GLP, the Progress_out exhibited a marked improvement in classification performance, with the average performance achieving an AUROC of 0.91, accuracy of 0.90, sensitivity of 0.80, specificity of 0.98, and F1 score of 0.86; all metrics were significantly superior to the performance of the original data (p < 0.01) and Progress_emb (p < 0.01). LGBM proved to be the best-performing algorithm in both Progress_emb and Progress_out, while SVM performed best using the original data.\n\nFig.\u00a0<ref> provides a visual representation of the distribution changes resulting from GLP, using glucose AC and LDL-c as examples. The figure reveals that prior to processing, it was difficult to separate the distributions of TVR and non-TVR cases (as demonstrated in Fig.\u00a0<ref>a); however, after processing with GLP, non-TVR cases gradually converged to a single point, whereas TVR cases exhibited a more scattered distribution (as shown in Fig.\u00a0<ref>b to c).\n\n\n\n\u00a7 DISCUSSION\n\n\nLaboratory analysis is a critical component of disease progress monitoring and a commonly used element in data-driven research and healthcare applications. Our study employed SSL and transfer learning to successfully transfer the trend of patient progress of cardiovascular laboratory parameters from one patient group to another. To our knowledge, this is the first study to apply laboratory progress of cardiovascular disease between patient groups, demonstrating that disease progress can be transferred with adequate processing, and the trend of general cases can be beneficial for developing data-driven applications for small-numbered patient groups.\n\nPretrained models capture the temporary status of patient progress across time and generate latent representations that enhance the prediction ability for other tasks. With the adoption of EHRs in modern hospitals, a large number of general cases such as HTN and DM patients were collected, whereas specific cases such as patients with TVR after PCI remained small in number. As HTN and DM are risk factors of TVR <cit.>, we successfully transferred the laboratory progress trend of HTN patients (who were yet to be diabetic) to predict the progress of PCI patients. This transformation enhanced our ability to infer patient progress, which was not limited to episodic observation. Despite the diverse nature of the two patient groups, cardiometabolic diseases were intercorrelated. The datasets were from two different medical institutes, indicating diverse patients, staff, workflows, and measuring equipment; therefore, the generality of our work is ensured.\n\nObserving disease progression requires long-term monitoring. SSL is feasible in bridging the gap in data irregularity, absenteeism, and sparsity, and achieving adequate prediction, indicating auto-annotated labeling can provide sustainable training. Learning from interpolated data in advance is capable of enhancing prediction performance; however, the effects vary between interpolation methods. Linear and PCHIP are designed to obtain a continuous function, whereas barycentric is designed to interpolate based on the center of mass. The distribution of laboratory progress is more similar to a continuous curve that extends across time and is less likely to be distributed in bundles. Wrong estimation of absent data induces higher noise in the data. Our results show that linear and PCHIP interpolation provide a more informative estimation of the progress trend of laboratory results, leading to better performance than SSL. The learned representation from supervised objectives tends to be more specific to a single domain and may have limited transferability to out-of-distribution domains <cit.>; therefore, adjusting the model based on SSL provides more extensible extrapolation ability.\n\nCurrent practice recommends patients return on a 3-month basis. Our proposed approach interpolates the input frame monthly, implying that the input frame will inevitably consist of estimated values. certain is a method for regulating the proportion of real values within the input frame. Our results did not indicate a strong relationship between R^2 and the number of real observations; nevertheless, for laboratory tests such as LDL-c and LDL-c/HDL-c, linear interpolation enlarges the tolerance of absent periods. This infers that, with adequate estimation, inferences regarding patient outcomes can be based on less frequent visits, thereby mitigating the travel burdens and medical expenses for patients.\n\nDue to limited data size, TVR prediction was trained with non-neural network algorithms. Deep neural networks did not perform well in this scenario. The results show that without the support of GLP, original data were not distinguishable enough to make substantial predictions; however, after being processed with GLP, the classifier was able to adequately separate TVR and non-TVR cases. The main difference between Progress_emb and Progress_out is that Progress_out is a condensed version that indicates laboratory values for the subsequent month, whereas Progress_emb still possesses information on age, gender, certainty mask, discrete value range encoding, and laboratory values. The condensed version has a distillation effect and provides a more beneficial indication of future trends. Comprehensive output of GLP is more informative than a single block. \n\nThe collective performance metrics present a holistic evaluation of the four algorithms that employ varying mechanisms, implying that the output of GLP is notably more distinctive. In the original dataset, SVM exhibited superior performance compared to the other algorithms, while in Progress_out, LGBM surpassed the remaining methods. SVM is recognized for utilizing kernels and identifying a hyperplane that maximizes the margin between the plane and classification clusters. On the other hand, tree-based algorithms emphasize individual variables and are more explicit in interpreting inherited rules. The findings demonstrate that GLP effectively transformed the data into a more targeted representation, which facilitated the creation of variable-specific rules.\n\nUpon examining the distribution shifts from the original data to Progress_out, which were iteratively applied to g, we observed that non-TVR cases gradually converged to a singular point, whereas TVR cases appeared more scattered. These findings are in line with clinical observations that indicate stable patients are more predictable, whereas those with scattered observations are at higher risks. The transferred progression enables improved patient risk assessment. Nevertheless, we were unable to obtain genuine patient observations at the target event, which prevented us from verifying the output's accuracy. Furthermore, it is important to note that the output of GLP does not necessarily correspond to real-world laboratory values even after reverse-normalized. The value itself does not correspond to a real-world value. To achieve this, a reverse network or decoder is required to map the latent output to real-world values.\n\nThis study is subjected to certain limitations. Presently, GLP and certain respond differently to laboratory tests. Models are pretrained for each laboratory test independently. Although combining all laboratory pretrained models into a single model may seem like a feasible solution, aggregating all laboratory data leads to an expansion of the interpolation range for individual examinations, while the number of true observations remains constant. This results in the inability to regulate the optimal number of true observations independently, which lead to suboptimal training. Furthermore, there are various types of laboratory analyses, with some being numerical values and others being categorical variables. This study solely focused on the examination of numeric results and can only be applied to other numeric examinations.\n\n\n\n\u00a7 CONCLUSION\n\nOur research capitalized on the benefits of SSL and pre-trained models, and effectively transferred the progression trends of cardiovascular laboratory parameters between patient groups, thereby demonstrating its efficacy in aiding the risk assessment of cardiovascular events. This transformation enhanced our ability to infer patient progress without limited to episodic observation. Auto-annotated labeling has the potential to overcome the difficulties associated with time-series data and provide sustainable training. Pre-learning from interpolated data can enhance the predictive outcomes. The fine-tuned results exhibit distinct separability independent of algorithmic mechanisms. The potential for extending this approach to other laboratory tests and diseases is promising, and will be explored in future investigations.\n\n\n\n\n\n\n\u00a7 ACKNOWLEDGMENT\n\nThis work was supported by the Ministry of Science and Technology in Taiwan under Grant MOST 111-2636-E-A49-014, 111-2628-E-A49-026-MY3 and Taipei Veterans General Hospital under Grant V1118-031.\n\n\n\n\n\n\n\u00a7 DEFINITION OF PATIENTS WITH HYPERTENSION AND DIABETES MELLITUS.\n\n\n\n\n\n\n\n\u00a7 DISCRETE VALUE RANGE OF LABORATORY TESTS\n\n\n\n\n\n\n\n\u00a7 R^2 OF INDIVIDUAL LABORATORY TESTS BASED ON LINEAR AND PCHIP TWO-STAGE TRAINING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nieeetr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}