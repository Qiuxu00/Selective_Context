{"entry_id": "http://arxiv.org/abs/2303.07186v1", "published": "20230313152847", "title": "Audio-based Roughness Sensing and Tactile Feedback for Haptic Perception in Telepresence", "authors": ["Bastian P\u00e4tzold", "Andre Rochow", "Michael Schreiber", "Raphael Memmesheimer", "Christian Lenz", "Max Schwarz", "Sven Behnke"], "primary_category": "cs.RO", "categories": ["cs.RO"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAudio-based Roughness Sensing and Tactile Feedback for Haptic Perception in Telepresence\n\n\n    \nBastian P\u00e4tzold,\nAndre Rochow,\nMichael Schreiber,\nRaphael Memmesheimer,\n\nChristian Lenz,\nMax Schwarz, and\nSven Behnke\nAutonomous Intelligent Systems\n\nUniversity of Bonn, Germany\n\n\n\n    March 30, 2023\n=========================================================================================================================================================================================\n\n\n\n \n\nHaptic perception is incredibly important for immersive teleoperation of robots, especially for accomplishing manipulation tasks. \nWe propose a low-cost haptic sensing and rendering system, which is capable of detecting and displaying surface roughness.\nAs the robot fingertip moves across a surface of interest, two microphones capture sound coupled directly through the fingertip and through the air, respectively.\nA learning-based detector system analyzes the data in real-time and gives roughness estimates with both high temporal resolution and low latency.\nFinally, an audio-based haptic actuator displays the result to the human operator.\nWe demonstrate the effectiveness of our system through experiments and our winning entry in the ANA Avatar XPRIZE competition finals, where impartial judges solved a roughness-based selection task even without additional vision feedback.\nWe publish our dataset used for training and evaluation together with our trained models to enable reproducibility.\n\n\n\n\nHaptics, Audio, Telepresence, Machine Learning\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nSensing surface properties through haptics is one of the fundamental ways humans perceive their environment. \nHumans are able to perform a variety of exploratory movements with their hands and fingertips to discern aspects such as roughness, hardness, and shape of objects they manipulate\u00a0<cit.>.\nIt is widely understood that integrating haptics into VR, AR and teleoperation systems is a key step towards increasing realism and acceptance of such systems\u00a0<cit.>.\nConsequently, numerous methods for sensing\u00a0<cit.> and displaying\u00a0<cit.> haptic sensations have been developed.\n\nThese systems are often very complex, though, and require too much space, especially on the side, where, if we aim at using humanoid-like robot hands, the volume of a fingertip has to suffice.\n\nIn this work, we showcase the haptic system our team NimbRo developed for the ANA Avatar XPRIZE competition[<https://www.xprize.org/prizes/avatar>]\u00a0<cit.>.\nThe competition focused on immersive telepresence in a mobile robot including social interaction as well as manipulation capabilities.\nTo evaluate the intuitiveness of the developed telepresence systems, judges had to solve a sequence of increasingly difficult tasks after very little training (30min).\nThe most difficult task to be performed in the finals was to discern two stone types based on their surface roughness. \n\nIn contrast to previous works, our haptic sensing and display system achieves roughness sensing at very low cost by using off-the-shelf audio components.\nBoth sensing and display components are of very small size and are easily integrated into teleoperation systems.\nThe audio signal captured by two microphones is analyzed by a neural network, which was trained on a dataset of exemplary surfaces. \nThe operator is notified about the presence and roughness of the perceived surface through low-latency vibratory feedback.\n\nThe system was successfully evaluated at the ANA Avatar XPRIZE finals, where three different operator judges solved all tasks in the fastest time,\nwinning our team NimbRo the $5M grand prize.\n\nIn summary, our contributions include:\n\n     \n  * Low-cost hardware design of both sensing and display components,\n     \n  * a learning-based method for online and low-latency roughness analysis, and\n     \n  * an evaluation of this system in the competition as well as offline experiments.\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n \u00a7.\u00a7 Tactile Sensing\n\n\nTactile sensors are based on a wide range of sensing principles, including capacitance, resistance, magnetism, and optics. \nFor example, <cit.> introduced the BioTac tactile sensor, based on an incompressible liquid as an acoustic conductor.\nIn addition to its capability of measuring shear forces, skin stretch and temperature, it detects vibrations with up to 1040 using a pressure sensor.\nBy using only a single sensor per fingertip, it has a low spatial resolution, though.\nGelSight, proposed by <cit.>, is capable of measuring high-resolution geometry as well as local/shear forces by visually observing the deformation of an elastomer sensor surface.\n\nDespite the promising capabilities of such devices, they suffer from two drawbacks: \nFirst, their size is too large for deployment in large quantities with high spatial resolution, or integration in existing hardware solutions.\nSecond, their high cost limits their feasibility for many applications.\n\nOur work focuses on deploying considerably smaller and lower-cost audio-based hardware.\nIn a similar manner, <cit.> describe the utilization of microphones to classify road surfaces by capturing the tire-pavement interaction noise.\nThey convert the audio signals to time-frequency RGB images and feed them to a CNN.\nEven though the captured audio signals also depend on other factors than the road surface, such as the car speed, tire type and wheel torque, they demonstrate the effectiveness of their approach for classifying snow and asphalt surfaces. \n<cit.> use a piezo acoustical sensor to analyze irregularities in materials, such as aluminum or stainless steel, occurring during manufacturing.\nThey capture the friction sounds when moving a stylus with a diamond tip over the surface of the specimen.\nThis controlled environment allows the determination of roughness parameters\u00a0<cit.> using classical signal processing approaches.\nMicrophones have also been used to identify touch and swiping gestures in smartphone contexts\u00a0<cit.>.\n\nMost similar to our use case, <cit.> help prosthetic users feel textures\nby stroking a microphone across object surfaces. In contrast to our system, the signal\nis filtered in a fixed manner, extracting the median frequency, which is then applied to the\nuser using electrostimulation. This limits the scope to regular textures (such as mesh, rubber, etc),\nwhere a frequency is easily extracted.\nIn contrast, our method works on irregular surfaces such as natural stones.\n\n\n\n\n \u00a7.\u00a7 Tactile Rendering\n\n\nTactile actuators are integrated into numerous devices, such as phones or game controllers.\nDue to the rising interest in telepresence and VR applications, a wide range of haptic displays \u2013 typically referred to as Haptic Gloves \u2013 emerged in recent years\u00a0<cit.>, promising to convey realistic kinesthetic and tactile feedback.\nTypically, tactile feedback is achieved by utilizing eccentric rotating masses, linear resonant actuators or piezoelectric actuators to display vibrations.\nTheir limitations often include supporting only a single resonant frequency, poor intensity resolution, as well as slow response times.\nInstead, we utilize an acoustic actuator to address these issues while maintaining comparable size and costs.\n\n\n\n\u00a7 METHOD\n\n\nWe now describe our method in detail. <Ref> gives an overview of the data flow.\n\n\n\n \u00a7.\u00a7 Sensing\n\n\nThe surface point of the avatar robot to be provided with roughness sensing capabilities (e.g. the tip of an index finger) is equipped with two microphones. \nA piezo microphone is attached directly to the inside of the chosen surface to measure vibrations within the robot structure,\nwhile a MEMS-type microphone is placed in close proximity (\u223c2) to the outside of the chosen surface measuring vibrations in the air around it.\nOnce the surface makes contact with and slides over the unknown texture of an object, a sound gets induced into both microphones. \nThese audio signals are then leveraged to classify the unknown texture as either smooth or rough.\n\n\n\n \u00a7.\u00a7 Classical Detection\n\n\nInitially, we attempted to find a direct mapping between the piezo microphone and our haptic display, utilizing classical DSP approaches like dynamics processors and filters.\nWhile it seemed easy for us to classify the considered textures from hearing the piezo microphone signal, we could not find a suitable transformation to accommodate our haptic perception and the properties of the considered display.\nOur attempts focused on isolating frequency regions that seemed critical for this perception, enhancing their transients and pitch-shifting them into lower registers supported by the actuator. \n\nNext, we tried to first classify the piezo microphone signal using similar DSP techniques, to then generate a new audio signal using a sine oscillator that conveys the haptic perception associated with the classification result.\nIn general, rough textures induce louder signals into the piezo microphone than smooth textures.\nHowever, the ambiguous amount of pressure applied by the operator masks this effect.\nLikewise, a hand-held solid natural stone induces a signal that differs strongly in level and frequency spectrum from a hollow artificial stone mounted inside a box, although their textures are very similar.\n\nIn summary, while we found a functioning configuration for a limited number of objects and scenarios, the classical approach lacked generalization across situations and users.\n\n\n\n\n \u00a7.\u00a7 Learned Detection\n\n\nInstead of hand-designed filters, we opted for a learning-based approach.\nAs the teleoperation task demands low-latency haptic feedback, we update the prediction with every received audio buffer (\u223c10) by constructing chunks that have access to 256 of the past.\nAfter low-pass filtering and reducing the sampling frequency, we calculate the FFT and concatenate the norm of both signals.\nExperiments showed a sampling frequency of 2 to be sufficient for representing the relevant features for the described task.\nClassification is then performed by an MLP with 15 hidden layers of which ten layers, with 256 hidden units each, are equipped with residual connections for a better gradient flow. \nExperiments have shown that the classification accuracy is increased when the unnormalized input of both the piezo and MEMS microphones are used. \nWhen sliding over smooth surfaces\u2014in contrast to rough ones\u2014the MEMS microphone's level tends to be significantly quieter than that of the piezo microphone.\nSuch patterns can easily be learned by a neural network and should therefore not be discarded through normalization.\nIn fact, they constitute our motivation for deploying a MEMS microphone in addition to the piezo microphone.\n\nDuring inference, we detect the loudness of the piezo microphone in real-time and compare it against a preset threshold that slightly exceeds its noise floor.\nThis allows distinguishing between contact and no\u00a0contact situations, which is used to either pass or block the network classification output.\n\n\n\n \u00a7.\u00a7 Actuation\n\n\nThe classification results are used to update the amplitude and frequency parameters of a simple sine oscillator.\nIn the case of a smooth classification, we set the oscillator to a low amplitude and a high frequency (e.g. 120),\nwhile for a rough classification result, we set a higher amplitude and a lower frequency (e.g. 60).\nFor the no\u00a0contact case, the amplitude is set to zero.\n\nThe audio signal generated by the oscillator is fed into a compact loudspeaker with a special voice-coil design, capable of reproducing frequencies between 10 and 250 in the form of intense but mostly inaudible vibrations.\nThe speaker is attached to the operator station matching the sensor position on the avatar robot (e.g. a fingertip of a hand exoskeleton).\n\nThe latency of the end-to-end haptic feedback is defined by the chunk size of the network, the buffer size of the avatar- and operator audio systems, as well as the transmission latency between them.\nThe haptic feedback allows the operator to intuitively distinguish between no\u00a0contact, contact\u00a0with\u00a0a\u00a0smooth\u00a0texture and contact\u00a0with\u00a0a\u00a0rough\u00a0texture situations.\nThe operator's haptic perception of smooth textures can be described as fizzy, while the perception of rough textures might be described as bumpy. \n\n\n\n\u00a7 IMPLEMENTATION\n \n\n\n\n\n\n\n \u00a7.\u00a7 Avatar Robot\n\n\nBoth microphones are attached to the left index finger of the avatar robot's Schunk SIH hand (<ref>).\nWe replaced the original index finger with two 3D printed components: The distal phalanx and the finger pad (<ref>). \nBoth feature hollow cylinders inside which allow a silicone layer to connect both components.\nThe piezo microphone is glued to the inside surface of the finger pad.\nThe MEMS microphone is placed on the side of the finger, where it is close to the fingertip, but does not interfere during manipulation tasks.\nIn order to avoid any implications caused by the proximity effect on the network's classification performance, we choose this microphone to have an omnidirectional polar pattern.\nThe silicone layer is slightly compliant and decouples the finger pad from the rest of the robot, preventing vibrations to spill over into the piezo microphone.\nThe finger pad shape is designed to allow for sliding over a wide range of textures without getting stuck, while also producing a suitable amount of vibrations in the finger to allow for reliable classification.\n\nThe microphones are connected to a Focusrite\u00a0Scarlett\u00a02i2 interface, which is used for pre-amplification and A/D conversion.\nWe set both inputs to high\u00a0impedance mode, which maximizes the microphones' frequency response.\nThe digital audio signals are forwarded and processed using the JACK\u00a0Audio\u00a0Connection\u00a0Kit which operates on top of the Advanced\u00a0Linux\u00a0Sound\u00a0Architecture.\nBoth signals are transmitted from the avatar robot to the operator station by a low-latency UDP transmitter utilizing the OPUS\u00a0audio\u00a0codec\u00a0<cit.>.\n\n\n\n \u00a7.\u00a7 Operator Station\n\n\nThe operator station receives the audio signals of both microphones within a similar JACK environment as described for the avatar robot.\nFirst, the signal originating from the piezo microphone is routed directly to the headphones of the Valve\u00a0Index head-mounted display that the operator is wearing.\nThis helps to convey a more detailed and complete haptic perception of the texture in question.\n\nThen, both audio signals are fed to the classification network as described in <ref>. \nThe confidence of the rough class is used to modulate the amplitude of a sine oscillator set to a frequency of 60 and a maximum level of 0.\nWe apply a low-pass filter on this modulation to prevent artifacts in the generated waveform arising from immediate jumps in confidence.\nA second sine oscillator is set to a frequency of 120 and a level of -25.\nBefore mixing the two generated signals, we use a side-chain gate to mute the latter oscillator in no\u00a0contact situations. \nThis is achieved by feeding the audio signal originating from the piezo microphone to the respective side-chain input.\nThe low noise floor and mechanical decoupling of the piezo microphone allow us to easily find a suitable threshold parameter to be set within the sidechain gate, in order to facilitate a reliable and sensitive way of sensing contact.\nAgain, nonzero attack and release parameters prevent the gate from inducing artifacts arising from fast amplitude modulations.\n\nThe generated audio signal is forwarded to our haptic display shown in <ref>.\nIts actuator is extracted from a Lofelt\u00a0Basslet, which is a wearable consumer device designed to convey the sense of bass while listening to music via headphones.  \nOriginally, the device is wrist-worn, houses a rechargeable battery and offers audio connectivity via Bluetooth.\nInstead, we embed its haptic actuator in a small-footprint 3D printed case and drive it using the mainboard-integrated soundcard for D/A conversion and a Fosi\u00a0Audio\u00a0TP-02 subwoofer amplifier.\nThe case is attached to the left index finger of the SenseGlove\u00a0DK1 hand exoskeleton worn by the operator.\n\nAs the chunk size processed by the MLP matches the buffer size of 512 samples set on both, the Avatar robot's and the Operator station's audio system running with a sampling frequency of 48, the latency of the entire audio system is 21 (omitting further network transmission delays). \n\n\n\n \u00a7.\u00a7 Data Acquisition and Network Training\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Dataset\n\n\n\n\nWe recorded a custom dataset using our instrumented robot hand, making contact with and sliding over various textures with multiple patterns and intensities.\nIt consists of a training set with 20 labeled objects, as shown in <ref>, and two test objects <ref>.\nBoth sets feature an equal number of rough and smooth textured objects.\nAs the task description in the Avatar XPRIZE finals was clear in defining the requirement of classifying the texture of stones, we include various natural and artificial stones in the dataset.\nHowever, to improve generalization, we also included other common textures like ingrain wallpaper and a wooden table surface.\nSome objects are measured multiple times under varying acoustical scenarios (handheld, on a table, inside a box, etc.) to improve domain robustness.\nFor each object, we obtain seven recordings with a duration of 30, including light, medium, and strong pressure levels, with long and short strokes,\nas well as a recording where we apply longer continuous wiggles, to support other interactive sensing approaches w.r.t. the operator.\nAs inference is run on the operator station, we encode all recordings using the OPUS\u00a0audio\u00a0codec before training, mimicking transmission effects.\n\n\n\n  \u00a7.\u00a7.\u00a7 Training\n\n\nWe split the training data into chunks of 256 and adopt the label of the respective file if the RMS loudness of the chunk exceeds a threshold determining a valid contact, similar to the threshold used to forward or block the oscillator output. \nAs the specific value of this threshold varies between experiments we report it in the evaluation, respectively.\nThis ensures that all labeled chunks correspond to surface contacts, but conversely, not every contact is assigned to a labeled chunk.\nWithout consideration of these unlabeled chunks, the network would show unpredictable behavior at inference time.\nIn the worst case, smooth chunks would be assigned to the rough class, which was particularly undesirable in the context of the competition.\nTherefore, we introduce a third non-valid class which is comprised of chunks below the set threshold. \n\nThe network is trained for five epochs with a batch size of 6000 chunks.\nWe use negative log-likelihood loss and the Adam optimizer with a learning rate of 1e-4.\nOn each chunk, we randomly add Gaussian distributed noise to prevent overfitting and enhance generalizability.\nThis is particularly important, as external noises and the capturing circuitry might induce disturbances.\n\nBoth our dataset and the trained models are made public to allow for reproducibility of results[<https://github.com/AIS-Bonn/Roughness_Sensing>].\n\n\n\n\u00a7 EVALUATION\n\n\nOur system has been developed and evaluated in several steps,\nfocusing on a quantitative analysis of the model training\nas well as the intuitiveness and immersion in a longer integrated mission.\n\n\n\n \u00a7.\u00a7 Quantitative Analysis\n\n\n\n\n \n\nWe compare two model variants that differ in the data used during training.\nFirst, we propose a general variant trained using the entire dataset and the threshold for distinguishing contact set to -26, slightly exceeding the noise floor of the piezo microphone.\n<ref> shows the confusion matrices of the general model variant for both the test set and the competition runs.\nIt is to be pointed out that we explicitly tuned the model to produce low false-positive rates.\nDue to the vibration motor inside the actuator being slow in its response, w.r.t. prediction rate, and the vibration intensity of rough classification results set relatively high, even misclassifications of single chunks can give the operator the false impression of sensing a rough surface.\nAccordingly, the correct classification of only several chunks suffices to convey the desired impression when sliding the finger over a rough texture.\n\nSecond, we show a fine-tuned model variant optimized for participation in the competition, using a reduced set of training objects including samples of the stones encountered during the competition runs (<ref>).\nHere, we set the threshold for distinguishing contact dynamically to 50 of the RMS loudness of all files with the respective label.\nThis increases the amount of non-valid classification results when applying light pressure onto an object at the benefit of further decreasing the false-positive rate. \n<ref> compares the classification accuracy of both model variants during the competition runs and for the test set.\nWhile the accuracy for classifying smooth objects shows to be very high with both model variants, the fine-tuned variant does substantially exceed the general one but falls slightly shorter w.r.t. rough objects.\n\nFurthermore, to justify the usage of the additional MEMS microphone, we show the accuracy of the general model using only the piezo microphone during training and inference.\nThe low accuracy for classifying smooth textures and the associated false-positive rates of 3.3 for the competition runs and 15.5 for the test set are insufficient for our application.\n\n\n\n\n\n\n \u00a7.\u00a7 Integrated Mission\n\n\n\n\n\n\n\n\n\n\n<ref> shows our avatar robot during the last task in the finals testing event of the ANA Avatar XPRIZE competition.\nDuring this task, the operator was required to find and retrieve one of the rough stones, purely based on their haptic perception.\nIn particular, there were five stones lined up on an anti-slip mat in a small box, with an opening that blocked the operator's vision through a curtain.\nThree of the stones had a smooth texture, while two had a rough texture and were highlighted in pink color, which was only relevant for the audience to distinguish the stones.\n<ref> shows a close-up of sample textures encountered in the competition.\n\nIn total, the track was encountered up to three times per team during the event.\nEach time, a different operator was controlling our avatar robot.\nThe operators were members of the XPRIZE jury and impartial in their judgment of task completion.\nThey were trained for 30min, directly before the run, to familiarize themselves with the system.\nHowever, only a fraction of this time was allocated to training for this specific task, as nine previous tasks needed to be completed to advance to the final task.\nAll three encounters were successful.\n<ref> and <ref> show the measured audio and generated feedback signals of Day 1 & 2, respectively.\n\n\n\nWhile the first run on Qualification Day was not public and results are not available, the latter two runs on testing Days 1 & 2 were broadcasted by the organizers, allowing for a comparison with all other teams that completed the task.\n<ref> shows that we completed the task considerably faster than any other team.\nHowever, it is worth mentioning that other factors besides the haptic feedback contributed to these times, such as fitting the hand inside the box and successfully grasping the stone after identification.\n\n\n\n\n\n \u00a7.\u00a7 Operation without Sight\n\n\n\n\nDuring our qualification run, we also demonstrated operation without direct sight, i.e. the operator was not able to\nsee the scene and the stones through the robot's cameras as usual.\nIn this scenario, it is difficult for the operator to even find the stones in order to touch them.\nFor this reason, we developed a 3D visualization based on geometry captured by a depth camera mounted in the left palm of the\nrobot. It allows the operator to locate the stones, hold them in place using the right hand, and move the instrumented index\nfinger of the left hand over them. We note that the system is designed to not give away the surface roughness: The measured\ndepth data is adaptively downsampled to remove surface details, leaving only the rough geometric shape.\nUsing this depth-based guidance, the operator judge was able to solve the task without live color view, which demonstrates\nthat our haptic feedback alone is sufficient for the classification of the stones.\n\n\n\n\u00a7 CONCLUSION\n\n\nWe demonstrated a haptic teleperception system consisting of a microphone sensing setup, which is low-cost and very compact, and a simple haptic rendering method on the operator side.\nThe system was proven to be very effective at the ANA Avatar XPRIZE competition finals, winning the first prize.\nEven though the system was mostly trained and tested on stone surfaces, the method can be adapted easily to other surface kinds by collecting the appropriate training data.\n\n\n\n"}