{"entry_id": "http://arxiv.org/abs/2303.06660v1", "published": "20230312134250", "title": "P-MMF: Provider Max-min Fairness Re-ranking in Recommender System", "authors": ["Chen Xu", "Sirui Chen", "Jun Xu", "Weiran Shen", "Xiao Zhang", "Gang Wang", "Zhenghua Dong"], "primary_category": "cs.IR", "categories": ["cs.IR"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    Renmin University of China\n  \nxc_chen@ruc.edu.cn\n\n\n\n\n  School of Information\n   Renmin University of China\n  \n\tcsr16@ruc.edu.cn\n\n\n\n\nJun Xu is the corresponding author. Work partially done at Beijing Key Laboratory of Big Data Management and Analysis Methods.\n\n  \n    Renmin University of China\n  \njunxu@ruc.edu.cn\n\n\n\n\n  Gaoling School of Artificial Intelligence\n    Renmin University of China\n  \nshenweiran@ruc.edu.cn\n\n\n\n\n\n  Gaoling School of Artificial Intelligence\n   Renmin University of China\n  \nzhangx89@ruc.edu.cn\n\n\n\n\n\n  \n  \n   Huawei Noah's Ark Lab\n  \n\twanggang110@huawei.com\n  \ndongzhenhua@huawei.com\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nIn this paper, we address the issue of recommending fairly from the aspect of providers, which has become increasingly essential in multistakeholder recommender systems. \nExisting studies on provider fairness usually focused on designing proportion fairness (PF) metrics that first consider systematic fairness. However, sociological researches show that to make the market more stable, max-min fairness (MMF) is a better metric. The main reason is that MMF aims to improve the utility of the worst ones preferentially, guiding the system to support the providers in weak market positions. \nWhen applying MMF to recommender systems, how to balance user preferences and provider fairness in an online recommendation scenario is still a challenging problem. In this paper, we proposed an online re-ranking model named Provider Max-min Fairness Re-ranking (P-MMF) to tackle the problem. Specifically, P-MMF formulates provider fair recommendation as a resource allocation problem, where the exposure slots are considered the resources to be allocated to providers and the max-min fairness is used as the regularizer during the process. We show that the problem can be further represented as a regularized online optimizing problem and solved efficiently in its dual space. During the online re-ranking phase, a momentum gradient descent method is designed to conduct the dynamic re-ranking. Theoretical analysis showed that the regret of P-MMF can be bounded. Experimental results on four public recommender datasets demonstrated that P-MMF can outperformed the state-of-the-art baselines. Experimental results also show that P-MMF can retain small computationally costs on a corpus with the large number of items.\n\n\n\n[500]Information systems\u00a0Recommender systems\n\n\n\nP-MMF: Provider Max-min Fairness Re-ranking \nin Recommender System\n    Gang Wang\nZhenghua Dong\n    \n==================================================================\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaised out of social, ethical, and economic considerations, the fairness problem becomes non-negligible in recommendation\u00a0<cit.>. In multi-stakeholder recommender systems (RS), there are several different participants, including users, items, providers, etc\u00a0<cit.>. Various models have been proposed for recommending fairly from the viewpoints of users\u00a0<cit.>, providers\u00a0<cit.>, or both\u00a0<cit.>. In this paper, we are concerned about the problem of ensuring provider fairness in multi-stakeholder recommender systems.  \n\n\nImage by Interaction Institute for Social Change | Artist: Angus Maguire. (interactioninstitute.org and madewithangus.com). Saskatoon Health Region.\n\nThere are many cultural variations regarding fairness\u00a0<cit.> in sociological researches. One practical fairness definition is based on two sociological concepts: equality and equity\u00a0<cit.>. According to <cit.>, equality can be defined as: everyone is treated the same and provided the same resources to succeed, while equity can be defined as: ensuring that resources are equally distributed based on needs. \nFigure\u00a0<ref> gives an intuitive example of the two types of fairness under the recommendation scenario. Suppose we have some resources (i.e., exposure slots) to ensure that providers can get the apples (i.e., survival in the market). As shown in Figure\u00a0<ref>, equality ensures that the RS evenly gives each provider the same support, while equity (known as distributive justice) emphasizes that RS will assign resources to providers as different ratios\u00a0<cit.>.\n\n\n\n\nBased on the concepts of equity\u00a0<cit.>, the metrics of proportion fairness (PF) and max-min fairness (MMF)\u00a0<cit.> have been respectively proposed. PF and MMF have been widely used for computation network and transportation\u00a0<cit.>. Their formal formulations in the provider fair recommendation scenario will be explained in Section\u00a0<ref>. Intuitively, PF and MMF tries to assign resources to the specified ratios\u00a0<cit.>. PF is based on welfare-based principles (known as Aristotle\u2019s Principle and Nash solution\u00a0<cit.>), which maximizes the sum of welfare of providers. MMF (known as Rawls\u2019 Principle and Kalai-Smorodinsky solution\u00a0<cit.>) aims to improve the worst-off providers\u2019 utilities. MMF has proven to be a better metric for the provider fairness problem, because worse-off providers, who occupy the majority of the platform, can survive with these supports. Taking care of these weak providers will increase the stability of the recommender market\u00a0<cit.>. For example, according to the report by <cit.>, small sellers in Amazon have difficulty facing the challenges of \u201clower profitability\u201d and \u201cinability to personalize\u201d on their own. The unfair system without ensuring MMF may result in a broken relationship between the providers and RS and, finally, force the providers to leave.\n\n\n\n\n\n\n\n\nExisting provider fair recommendation models either consider the PF of providers\u00a0<cit.> or use heuristics to guarantee the utilities of the worse-off providers\u00a0<cit.>. However, these heuristic methods do not directly consider the MMF and lack theoretical guarantees when adapting to online scenarios. Moreover, the heuristic methods lack the flexibility to trade-off with user utility, which inevitably hurts the users' experience.  \n\n\n\n\n\n\n\n\nThis paper aims to develop a practical re-ranking model that considers MMF from the providers' perspectives.\nWe formulate the provider fair recommendation as a process of resource allocation\u00a0<cit.>. In such a process, resources can be regarded as limited ranking slots, and providers are viewed as the demanders. The cost of allocation is defined as the preference of the users. Moreover, an MMF regularizer is imposed on the allocation to maximize the minimum allocation to a specific provider. \n\n\n\nTo adapt the method to the online recommendation scenarios, we proposed an online re-ranking algorithm called P-MMF that focuses on provider max-min fairness by viewing the provider fair online recommendation as a regularized online optimizing problem. However, the optimization problem contains many integral variables, making it notoriously difficult to solve. We then derive its dual problem and propose an efficient algorithm to optimize the problem in the dual space. In the online setting, a momentum gradient descent method was developed to make an effective and efficient online recommendation. Our theoretical analysis shows that the regret of the P-MMF can be well-bounded. Furthermore, the P-MMF is also computationally efficient due to its insensitivity to the number of items.\n\n\n\nWe summarize the major contributions of this paper as follows:\n\n(1) We analyzed the importance of ensuring equity for providers in multi-stakeholder recommender systems, and propose to use the max-min fairness metric in provider fair recommendation. \n\n\n(2) We formulated the provider fair recommendation as a resource allocation problem regularized by the max-min fairness, and proposed a re-ranking model called P-MMF that balances the provider fairness and the user preference. Our theoretical analysis showed that the regret of P-MMF can be bounded.\n\n\n(3) Simulations on a small dataset showed the superiority of MMF compared to PF in recommending fairly to providers. Extensive experiments on four publicly available datasets demonstrated that P-MMF outperformed state-of-the-art baselines, including the PF-based and MMF-based methods. \n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\nFairness has become a hot research topic in multi-stakeholder recommender systems\u00a0<cit.>. Researchers have proposed several user-oriented and item/provider-oriented fairness re-ranking models. For user-oriented fairness,\u00a0<cit.> proposed a user-centered evaluation that measures users' different interest levels in popular items.\u00a0<cit.> addressed the user-centered fairness from a group fairness perspective. For provider-oriented fairness,\u00a0<cit.> proposed a rule-based algorithm to ensure that the exposures of items should be equally distributed.\u00a0<cit.> aimed to enhance dynamic fairness when item popularity changes over time. However, these provider-oriented methods did not consider the user's perspective.\n\nRecently, there are also studies that jointly consider the trade-off between user preference and provider fairness.\u00a0<cit.> claimed that all providers should receive the amount of exposures proportional to their relevance in economy platforms.\u00a0<cit.> studied the trade-offs between the user and producer fairness in Point-of-Interest recommendations. TFROM\u00a0<cit.> and CPFair\u00a0<cit.> formulated the trade-off as a knapsack problem and a relaxed linear programming problem, respectively. However, they used greedy-based algorithms in online scenarios, which only improves the proportion fairness of providers. Some studies noticed that the utilities of worse-off providers should also be guaranteed. For example, FairRec\u00a0<cit.> and its extension FairRec+\u00a0<cit.> proposed hard constraints to ensure that every provider should have the lowest exposures. Welf\u00a0<cit.> proposed a Frank-Wolfe algorithm to maximize the welfare functions of worse-off items. However, they were all designed for offline scenarios and suffered from high computational costs, which prevented them from being applied to real online recommendation systems <cit.>.\n\nIn this paper, we formulate the re-ranking task as the resource allocation problem\u00a0<cit.>, which is crucial in communications and transportation.  \nIn online resource allocation, most studies\u00a0<cit.> focused on designing the time-separable reward functions, which is the sum of rewards over periods. \u00a0<cit.> proposed a local-based sub-gradient algorithm for the linear reward.\u00a0<cit.> designed a dual-based online algorithm with learning from the distribution of requests.\u00a0<cit.> proposed a mirror-descent method to solve the fairness-regularized online allocation problem. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 PROVIDER FAIR RE-RANKING\n\nIn this section, we first define the notations in a multi-stakeholders recommender system. Then we give the formal definitions of the proportion and max-min fairness. \n\nIn a multi-stakeholders recommender system, multiple participants exist, including users, item providers, and other stakeholders. Let \ud835\udcb0, \u2110, and \ud835\udcab be the set of users, items, and providers, respectively. Each item i\u2208\u2110 is associated to a unique provider p\u2208\ud835\udcab. The set of items associated with a specific provider p is denoted as \u2110_p. When a specific user u\u2208\ud835\udcb0 accesses the recommender system, a list of K items, denoted by L_K(u)\u2208\u2110^K, is provided to the user. For each user-item pair (u, i), the recommender model estimates a preference score s_u,i\u2208\u211d. These items are ranked according to their preference scores. \nIn this paper, we define the user-side utility of exposing item list L_K(u) to u as the summation of the preference scores in the list, denoted by f(L_K(u)) = \u2211_i\u2208 L_K(u) s_u,i. We follow the literature convention <cit.> and define the fairness vector of providers as \ud835\udc1e\u2208\u211d^|\ud835\udcab|, where for a specific provider p, \ud835\udc1e_p\u2208\u211d^+ denotes the number of exposed items of provider p. The goal of provider fair re-ranking is to compute a new fair list L^F_K(u)\u2208\u2110^K which well balances the user utilities f(L^F_K(u)) and a provider fairness metric defined over \ud835\udc1e. \n\n\n\n\nIn real-world applications, the users arrive at the recommender system sequentially. Assume that at time t user u_t arrives. The recommender system needs to consider long-term provider exposure during the entire time horizon from t=0 to T.\nOur task can be formulated as a resource allocation problem\u00a0<cit.> with time-separable fairness. \nSpecifically, the optimal utility of the recommender system can be defined as a time-separable utility function, which is the accumulated reward <cit.> over periods from 0 to T. In this case, \ud835\udc1e_p can be seen as the total number of exposed items of provider p, accumulated over the period 0 to  T. Formally, when trading-off the user preference and provider fairness,  we have the following mathematical program:\n\n    max_L^F_K     1/T\u2211_t=1^T f(L^F_K(u_t)) + \u03bb r(\ud835\udc1e)\n    s.t.     \ud835\udc1e\u2264\u03b3,\n\nwhere \u03b3\u2208\u211d^|\ud835\udcab| denotes the weights of different providers, e.g., weighted PF or MMF\u00a0<cit.>, and r(\ud835\udc1e)\u2208\u211d is a provider fairness metric that serves as a fairness regularizer. Note that the constraint \ud835\udc1e\u2264\u03b3 can also be viewed as the maximum resources allocated to the providers. Following the definitions of PF and MMF\u00a0<cit.>, r(\ud835\udc1e) also has two different forms:\n\nProportion Fairness (PF): r(\ud835\udc1e) = \u2211_p\u2208\ud835\udcablog[1+\ud835\udc1e_p/\u03b3_p].\n\nMax-min Fairness (MMF): r(\ud835\udc1e) = min_p\u2208\ud835\udcab[\ud835\udc1e_p/\u03b3_p].\n\nUsing the proportion fairness as the regularizer, we can reduce the difference between \ud835\udc1e and \u03b3, while using the max-min fairness we can improve the relative exposure (w.r.t. weights \u03b3) of the least exposed provider.\n\n\n\n\n\u00a7 OUR APPROACH: P-MMF\n\nIn this section, we first formulate provider fairness in the recommendation as a resource allocation problem. Then, we propose an algorithm called P-MMF for the online recommendation.\n\n\n\n \u00a7.\u00a7 Resource allocation with provider fairness\n\nWe formulate the providers' fair recommendation problem as a resource allocation process\u00a0<cit.>. In such a process, the resources can be regarded as limited ranking slots that are allocated to providers. The allocation cost is defined based on the preference of the users and the max-min fairness. \n\nFormally, based on Equation\u00a0(<ref>), the fair recommendation problem can be written as a linear programming:\n\n    max_\ud835\udc31_t     1/T\u2211_t=1^Tg(\ud835\udc31_t) + \u03bb r(\ud835\udc1e)\n    s.t.     \u2211_i\u2208\u2110\ud835\udc31_ti = K, \n              \u2200 t\u2208 [1,2,\u2026,T]\n       \ud835\udc1e_p = \u2211_t=1^T\u2211_i\u2208\u2110_p\ud835\udc31_ti, \n               \u2200 p\u2208\ud835\udcab\n       g(\ud835\udc31_t) = \u2211_i\u2208\u2110\ud835\udc31_tis_u_t,i,   \u2200 t\u2208 [1,2,\u2026,T]\n       \ud835\udc1e\u2264\u03b3, \ud835\udc31_ti\u2208{0, 1}, \n              \u2200 i \u2208\u2110, \u00a0\u2200 t\u2208 [1,2,\u2026,T]\n    ,\n\nwhere \ud835\udc31_t\u2208{0, 1}^|\u2110| is the decision vector for user u_t, and g(\u00b7) is the user-side utility function. Specifically, for each item i, \ud835\udc31_ti = 1 if it is added to the fair ranking list L_K^F(u_t), otherwise, \ud835\udc31_ti = 0. Note that g(\u00b7) is equivalent to f(\u00b7) in the sense that they produce the same result, while g(\u00b7) takes the binary decision vector as input. The first constraint in Equation\u00a0(<ref>) ensures that the recommended lists are of size K. The second constraint in Equation\u00a0(<ref>) suggests that the exposures of each provider p are the accumulated exposures of the corresponding items over all periods. In general, we think time-separable fairness would be preferred under the scenarios with weak timeliness. For example, recommending the items with long service life (e.g., games and clothes etc.).\n\n\n\n\n\n\nFigure\u00a0<ref> gives a toy example of the linear programming problem\u00a0(<ref>). In the example, we set T=2. Suppose there are two users, u_1 and u_2, arriving at the system one by one. At each time step, the system recommends a list of K=3 items. Therefore, the system has overall 2\u00d7 3 = 6 slots to expose. Suppose the system has two providers p_1 and p_2, each owning three items. Let's set identical weights for p_1 and p_2 (i.e., \u03b3 = [\u03b3_1=6, \u03b3_2=6]). The model solves problem\u00a0(<ref>) and the solution is two binary vectors: \ud835\udc31_1 for u_1 and \ud835\udc31_2 for u_2. Finally, we count the exposures over the time T=2: \ud835\udc1e = [e_p_1 = 2, e_p_2 =4], which means by recommending the ranking lists L_K^F(u_1) (created based on \ud835\udc31_1) and L_K^F(u_2) (created based on \ud835\udc31_2) to u_1 and u_2, provider p_1 and _2 get 2 and 4 exposures, respectively.\n\n\n\nAlthough here we have already given a linear programming solution Eq.(<ref>) to the problem, it can only be solved small-scale problems in an offline way. In online recommendation systems, for each user u_t access, the model needs to generate a fair ranking list L^F_K(u_t) from large-scale item corpus immediately. This means we have no idea about the information after t. Next, we will discuss how to use MMF in the online recommendation problem.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 P-MMF for online applications\n\nIn this section, firstly, we formulate the MMF in online scenarios. Next, we consider the dual of the original problem. Focusing on the dual problem has several advantages: the dual problem has significantly fewer variables, and the variables no longer need to be integers as in the original problem. Finally, we proposed a momentum gradient descent algorithm for efficient online learning. \n\n\n\n  \u00a7.\u00a7.\u00a7 \n\nAt time step t, the recommender system receives a request from user u_t. An online algorithm h produces a real-time decision vector \ud835\udc31_t\u2208{0,1}^|\u2110| based on the current user u_t and the previous history\n\u210b_t-1 = {u_s,\ud835\udc31_s}_s=1^t-1:\n\ud835\udc31_t = h(u_t |\u210b_t-1).\nWe define the online reward of max-min fairness to be the summation of rewards over all time steps:\n\n    W = 1/T\u2211_t=1^Tg(\ud835\udc31_t) + \u03bbmin[\ud835\udc1e/\u03b3],\n\nwhere min[\ud835\udc1e/\u03b3] corresponds to the max-min fairness regularizer. \n\nOur goal is to design an algorithm h that attains low\nregret. Denote by W_OPT the optimal value, we measure the\nregret of an algorithm as the expectation difference between the optimal performance W_OPT of the offline problem and that of the online algorithm W over user distributions \ud835\udcb0:\n\n    Regret(h) = \ud835\udd3c_u_t \u223c\ud835\udcb0[W_OPT - W].\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Dual problem\n\nFrom the original problem in Equation\u00a0(<ref>), we know that the integer decision variable \ud835\udc31_t is of size |\u2110| for each time t, which is hard to solve. However, considering its dual problem,  we can significantly reduce its computational cost.\n\n\n\nThe dual problem of Equation\u00a0(<ref>) can be written as:\n\n    W_OPT   \u2264 W_Dual = min_\u03bc\u2208\ud835\udc9f[g^*(\ud835\udc00\u03bc) + \u03bb r^*(-\u03bc)],\n\nwhere \ud835\udc00\u2208\u211d^|\u2110|\u00d7|\ud835\udcab| is the item-provider adjacent matrix, and A_ip = 1 indicates item i\u2208\u2110_p, and 0 otherwise.\nLetting \ud835\udcb3 = {\ud835\udc31_t|\ud835\udc31_t \u2208{0,1}\u2211_i\u2208\u2110\ud835\udc31_ti = K}, g^*(\u00b7),r^*(\u00b7) are the conjugate functions:\n\n    g^*(c) = max_\ud835\udc31_t\u2208\ud835\udcb3\u2211_t=1^T[g(\ud835\udc31_t)/T - \ud835\udc1c^\u22a4\ud835\udc31_t], \n        r^*(-\u03bc) = max_\ud835\udc1e\u2264\u03b3[r(\ud835\udc1e)+\u03bc^\u22a4\ud835\udc1e/\u03bb],\n\nand \ud835\udc9f = {\u03bc|r^*(-\u03bc)<\u221e} is the feasible region of dual variable \u03bc. \n\n\n\n\nProof of Theorem\u00a0<ref> can be found in Appendix\u00a0<ref>. From Theorem\u00a0<ref>, we can have a new non-integral decision variable \u03bc\u2208\u211d^|\ud835\udcab|. In practice, the provider size |\ud835\udcab|\u226a |\u2110|. Besides, due to \ud835\udc00's sparsity, it is very efficient to compute \ud835\udc00\u03bc, which aims to project the variable \u03bc from provider space into item spaces.\n\nIn our online algorithm discussed in Section\u00a0<ref>, we can have a closed form of the conjugate function g^*(\u00b7) in constant time. As for the feasible region \ud835\udc9f and the conjugate function r^*(\u00b7), we have Theorem\u00a0<ref> and Lemma\u00a0<ref>.\n\n\nIn the MMF, the feasible region of the dual problem \n\n    \ud835\udc9f = {\u03bc\u00a0\u00a0|\u00a0\u00a0\u2211_p\u2208\ud835\udcae\u03b3_p\u03bc_p \u2265 -\u03bb, \u2200\ud835\udcae\u2208\ud835\udcab_s.},\n\nwhere \ud835\udcab_s is the power set of \ud835\udcab, i.e., the set of all the subsets of \ud835\udcab.\n\n\nThe proof of Theorem\u00a0<ref> can be found in Appendix\u00a0<ref>, which implies the following Lemma\u00a0<ref>:\n\n\nThe conjugate function r^*(\u00b7) has a closed form:\nmax_\u03bc\u2264\u03b3r^*(-\u03bc) = \u03b3^T\u03bc/\u03bb + 1,\nand the optimal dual variable is:\n_\u03bc\u2264\u03b3r^*(-\u03bc) = \u03b3/\u03bb.\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 The P-MMF algorithm\n\nAlgorithm\u00a0<ref> illustrates P-MMF algorithm. Following <cit.>, P-MMF keeps a dual variable \u03bc_t, the remaining resources \u03b2_t and the gradient \ud835\udc20_t for each time t.\n\nWhenever a user arrives, the algorithm computes the recommended variable \ud835\udc31_t based on the remaining resources and the dual variable \u03bc_t (line 7). The final dual variable is estimated as the average dual variable for each time t: \u03bc = \u2211_t=1^T\u03bc_t/T. Intuitively, for \u03bc_t, when the values of dual variables are higher, the algorithm naturally recommends fewer items related to the corresponding provider. The remaining resources \u03b2_t ensure that the algorithm only recommends items from providers with remaining resources. Note that in line 7, the formulation is linear with respect to \ud835\udc31_t. Therefore, it is efficient to compute \ud835\udc31_t through a top-K sort algorithm in constant time.\n\nThe online learning process is as follows. Firstly, we get the closed form of the conjugate function of max-min regularizer r^*(-\u03bc_t) according to Lemma\u00a0<ref>. Then we can get the subgradient of the dual function g^*(\ud835\udc00\u03bc)+\u03bb r^*(-\u03bc):\n\n    -\ud835\udc00^\u22a4\ud835\udc31_t + \ud835\udc1e_t \u2208\u2202(g^*(\ud835\udc00\u03bc_t)+\u03bb r^*(-\u03bc_t)).\n\n\nWe also add the last time momentum\u00a0<cit.> to the updated gradient \ud835\udc20_t. Finally, we utilize \ud835\udc20_t to update the dual variable by performing the online descent in line 14, where we used weighted \u2113_2-norm:\u03bc_\u03b3^2^2 = \u2211_j=1^|\ud835\udcab|\u03b3_j^2\u03bc_j^2. Therefore, the dual variable will move towards the directions of the providers with fewer exposures, and the primal variable \ud835\udc31_t will move to a better solution.\n\nNote that the projection step in line 14 can be efficiently solved using convex optimization solvers\u00a0<cit.> since \ud835\udc9f is coordinate-wisely symmetric. \n\n\n\n\n    min_\u03bc\u2208\ud835\udc9f\u03bc-\u03bc_t _\u03b3^2^2     = min_\u2211_p\u2208\ud835\udcab(\u03bc_p\u03b3_p - \u03bc_p\u03b3_p)^2\n    s.t.\u2211_j=1^m \u03b3_j\u03bc_j + \u03bb   \u2265 0,\u00a0\u2200 m = 1, 2 , \u2026, |\ud835\udcab|,\n\n\nwhere \u03bc satisfies:\n\u03b3_1\u03bc_1 \u2264\u03b3_2\u03bc_2 \u2264\u22ef, \u2264\u03b3_|\ud835\udcab|\u03bc_|\ud835\udcab|.\n\n    \nWe provide a regret bound on Regret(h) in Theorem\u00a0<ref> but defer the proof to Appendix\u00a0<ref> due to space limit. Intuitively, larger ranking size K and time T will lead to larger biases in P-MMF.\n\n\n\n\n\n\n\n\n\nAssume that the function \u00b7_\u03b3^2^2 is \u03c3-strong convex and there exists a constant G\u2208\u211d^+ and H>0 such that g_t<G, \u03bc_t-\u03bc_0_\u03b3^2^2\u2264 H..\nThen, the regret can be bounded as follows:\n\n    Regret(h) \u2264K(1+\u03bbr\u0305 + r\u0305)/min_p \u03b3_p + H/\u03b7 + G^2/(1-\u03b1)\u03c3\u03b7(T-1) + G^2/2(1-\u03b1)^2\u03c3\u03b7,\n\nwhere r\u0305 is the upper bound of MMF regularzier, and in practice, r\u0305\u2264 1.\n\nSetting the learning rate as \u03b7  = O(T^-1/2), we can obtain a sublinear regret upper bound of order  O(T^1/2) of magnitude.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENT\n\nWe conducted experiments to show the effectiveness of the proposed P-MMF for provider-fair recommendations. \nThe source code and experiments have been shared at github\u00a0[<https://github.com/XuChen0427/P-MMF>. For the MindSpore\u00a0<cit.> version, please see\u00a0<https://gitee.com/mindspore/models/tree/master/research/recommend/pmmf>].\n\n\n\n \u00a7.\u00a7 Experimental settings\n\n\n\n  \u00a7.\u00a7.\u00a7 Datasets\n\nThe experiments were conducted on four large-scale, publicly available recommendation datasets, including:\n \n Yelp[<https://www.yelp.com/dataset>]: a large-scale businesses recommendation dataset. We only utilized the clicked data, which is simulated as the 4-5 star rating samples. The cities of the items are considered as providers. It has 154543 samples, which contains 17034 users, 11821 items and 23 providers.\n\n Amazon-Beauty/Amazon-Baby: Two subsets (beauty and digital music domains) of Amazon Product dataset[<http://jmcauley.ucsd.edu/data/amazon/>]. We only utilized the clicked data, which is simulated as the 4-5 star rating samples. Also, the brands are considered as providers. They has 49217/59836 samples, which contains 9625/11680 users, 2756/2687 items and 1024/112 providers.\n \n Steam[<http://cseweb.ucsd.edu/\u00a0wckang/Steam_games.json.gz>]\u00a0<cit.>: We used the data for gamed played for more than 10 hours in our experiments. The publishers of games are considered as providers. It has 29530 samples, which contains 5902 users, 591 items and 81 providers.\n\n \n \n\nAs a pre-processing step, the users, items, and providers who interacted with less than 5 items/users were removed from all dataset to avoid the extremely sparse cases. We also removed the providers associated with less than 5 items.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Evaluation\n\nWe sorted all the interactions according to the time and used the first 80% of the interactions as the training data to train the base model (i.e., BPR\u00a0<cit.>). The remaining 20% of interactions were used as the test data for evaluation. Based on the trained base model, we can obtain a preference score s_u, i for each user-item pair (u, i).\nThe chronological interactions in the test data were split into interaction sequences where the horizon length was set to T. We calculated the metrics separately for each sequence, and the averaged results are reported as the final performances.\n\n\n\nAs for the evaluation metrics, the performances of the models were evaluated from three aspects: user-side preference, provider-side fairness, and the trade-off between them. As for the user-side preference, following the practices in\u00a0<cit.>, we utilized the NDCG@K, which is defined as the ratio between the sum of position-based user-item scores\u00a0<cit.> in the original ranking list \ud835\udc0b_K(u_t) and that in the re-ranked list \ud835\udc0b^F_K(u_t):\n\n    NDCG@K =1/T\u2211_t=1^T\u2211_i\u2208\ud835\udc0b_K(u_t)s_u_t,i/log(rank_i+1)/\u2211_i\u2208\ud835\udc0b_K^F(u_t)s_u_t,i/log(rank_i^F+1),\n\nwhere rank_i and rank_i^F are the ranking positions of the item i in \ud835\udc0b_K(u_t) and \ud835\udc0b_K^F(u_t), respectively.  \n\n\nAs for the provider fairness, we directly utilized the definition of MMF in Section\u00a0<ref> as the metric:\n\n    MMF@K = min_p\u2208\ud835\udcab{\u2211_t=1^T\u2211_i\u2208\ud835\udc0b_K^F(u_t)\ud835\udd40(i\u2208\u2110_p)/\u03b3_p},\n\nwhere \ud835\udd40(\u00b7) is the indicator function. \n\nAs for the trade-off performance, we used the online objective traded-off with Equation\u00a0(<ref>):\n\n    W_\u03bb@K = 1/T\u2211_t=1^T(\u2211_i\u2208\ud835\udc0b_K^F(u_t)s_u_t,i) + \u03bb\u00b7MMF@K,\n\nwhere \u03bb\u2265 0 is the trade-off coefficient.\n\n\n\n  \u00a7.\u00a7.\u00a7 Baselines\n\nThe following representative provider fair re-ranking models were chosen as the baselines:\nFairRec\u00a0<cit.> and FairRec+ <cit.> aimed to guarantee at least Maximin Share (MMS) of the provider exposures.\nCPFair\u00a0<cit.> formulated the trade-off problem as a knapsack problem and proposed a greedy solution.\n\nWe also chose the following MMF models:\nWelf\u00a0<cit.> use the Frank-Wolfe algorithm to maximize the Welfare functions of worst-off items. However, it is developed under off-line settings; RAOP\u00a0<cit.> is a state-of-the-art online resource allocation method. We applied it to the recommendation by regarding the items as the resources and users as the demanders. \n\nWe also compared the proposed P-MMF with two heuristic MMF baselines:\nK-neighbor: at each time step t, only the items associated to the top-K providers with the least cumulative exposure are recommended;  min-regularizer: at each time step t, we add a regularizer that measures the exposure gaps between the target provider and the worst-providers. Appendix\u00a0<ref> shows the algorithm details.\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Implementation details\n\nAs for the hyper-parameters in all models, the learning rate was tuned among [1e-2,1e-4]/T^1/2 and the momentum coefficient \u03b1 was tuned among [0.2,0.6]. For the maximum resources (i.e., the weights) \u03b3, following the practices in\u00a0<cit.>, we set \u03b3 based on the number of items provided by the providers:\n\u03b3_p = KT\u03b7|\u2110_p|/|\u2110|,\nwhere \u03b7 is the factor controlling the richness of resources. In all the experiments, we set \u03b7=1+1/|\ud835\udcab|. We implemented P-MMF with both CPU and GPU versions based on cvxpy\u00a0<cit.> and its PyTorch version\u00a0<cit.>, respectively. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Simulation on a small dataset\n\nNote that Problem\u00a0(<ref>) can use MMF or PF as its regularizer, though directly solving it on large-scale datasets is difficult. \n\nTo verify the correctness of the formulation and to investigate different impacts of MMF-based and PF-based regularizers on provider fair recommendation, we first conducted a numerical simulation using 5% of the Yelp data, which consists of 844 users, 813 items, 10 providers, and with the length of the horizon T=256 . \n\n\nSpecifically, we solved the Problem\u00a0(<ref>) with PF based or MMF based regularizers (i.e., setting their provider weights as even distribution \u03b3_p = KT, \u2200 p\u2208\ud835\udcab), using the solver cvxpy\u00a0<cit.>.\nAt each time t, after receiving the recommendation decision variable \ud835\udc31_t, we calculated the overall provider exposure \ud835\udc1e on the dataset. Then the Lorenz curves\u00a0<cit.> of provider exposures were drawn and shown in Figure\u00a0<ref>. The Lorenz curve is often used to represent exposure distribution. Here it shows the proportion of overall exposure percentage assumed by the bottom x% providers. In other words, for the bottom x% providers, what percentage (y%) of the total exposures they have. Note that the diagonal line to the upper right is known as the absolute fair line.\n\nFrom the curves shown in Figure\u00a0<ref>, we can observe that when the \u03bb\u2192\u221e (i.e., only consider provider's fairness), both PF and MMF can achieve the expected proportion \u03b3 (i.e., even exposures here). However, after considering the user preference, the MMF-based regularizer tends to consider the worst providers' exposure first, while PF does not. For example, when the trade-off coefficient \u03bb changes among [0.01,0.05,0.1], 60% of the least exposure providers will increase [18.81%,18.96%, 12.58%] exposures by MMF. If using PF-based regularizer, then the increased exposure ratios become [10.1%,16.27%, 9.58%]. The results verified that formulating recommendations as a resource allocation problem regularized by MMF leads to better provider-fair recommendations than PF. \n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Experimental results on full datasets\n\n\nIn this section, we conducted the experiments with the online algorithm developed in Section\u00a0<ref>. In all of the experiments, BPR\u00a0<cit.> was chosen as the base ranking model for generating preference scores. We set the length of the horizon T=256.\n\n\nTable\u00a0<ref> reports the experimental results of P-MMF and the baselines on all datasets in terms of the metric W_1@K. Underlined numbers mean the best-performed baseline. To make fair comparisons, all the baselines were tuned and used W_1@K as the evaluation metric. Note that similar experiment phenomena has also been observed on other \u03bb values.\n\nFrom the reported results, we found that P-MMF outperformed all of the PF-based baselines in terms of W_1@K (K=5, 10, 20), verified that P-MMF can give supports to the poor-conditioned providers. We also observed that P-MMF outperformed all the MMF-based baselines, indicating P-MMF's effectiveness in enhancing provider fairness while keeping high user preference.\n\n\n\n\n\n\n\n\n\n\nFigure\u00a0<ref> shows the Pareto frontiers\u00a0<cit.> of NDCG@K and MMF@K on four datasets with different ranking size K. The Pareto frontiers were drawn by tuning different parameters of the models and choosing the (NDCG@K, MMF@K) points with the best performances. In the experiment, we selected the baselines of CPFair, min-regularizer, Welf, and ROAP, which achieved relatively good performances among all baselines.\n\n\nFrom the Pareto frontiers, we can see that the proposed P-MMF Pareto dominated the baselines (i.e., the P-MMF curves are at the upper right corner). Pareto dominance means that under the same NDCG@K level, P-MMF achieved better MMF@K; Under the same MMF@K level, P-MMF achieved better NDCG@K. The results demonstrate that P-MMF splendidly improves the utilities of poor-conditioned providers without sacrificing users' utilities too much. \n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Experiment analysis\n\nWe also conducted experiments to analyze P-MMF on Yelp. \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Ablation study on different base models\n\nP-MMF and other baselines are re-ranking models which re-rank the results outputted by a base recommender model. To verify the effectiveness of P-MMF with different base ranking models, we choose three other base models to generate user-item preference scores s_u,i, including DMF\u00a0<cit.> which optimizes the matrix factorization with the deep neuarl networks; LINE\u00a0<cit.> is a matrix factorization model based on graph embeddings; and LightGCN\u00a0<cit.> which builts a user-item interaction graph and adapts Graph Convolutional Network\u00a0<cit.> to conduct recommendation. All the experiments were also conducted on the full Yelp dataset with T=256.\n\nTable\u00a0<ref> shows the experimental results of P-MMF with different base models. We observed that P-MMF consistently outperformed the PF-based and MMF-based baselines. The results also verified that P-MMF is more effective than the baselines in re-ranking the results outputted by different base models.\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Regret bound experiments\n\nTo further show the effectiveness of the P-MMF, we directly compute the regret Regret(h)=W_OPT - W, where W_OPT is the oracle performance defined in Equation\u00a0(<ref>), and W is the online performance defined in Equation\u00a0(<ref>). Due to the huge computational cost of obtaining W_OPT on large-scale datasets, we conducted the experiments on the 5% of the Yelp data created in Section\u00a0<ref>. We compared P-MMF with the state-of-the-art provider-fair online MMF baselines: min-regularizer and ROAP. The experiments were conducted on ranking size K=10. Note that in the experiment, we fixed the user arriving size N, and the regret is computed through the summation over the N/T samples where T is the length of the horizon. According the Theorem\u00a0<ref>, the summation Regret(h) is comparable with O(T^1/2N/T) = O(N/T^1/2).\n\n\nFigure\u00a0<ref> shows the summation Regret(h) curves w.r.t. T. Figure\u00a0(a) and (b) show the curves when the trade-off co-efficient \u03bb was set as 1 and 0.1, respectively. \nFrom Figure\u00a0<ref>, we can see that P-MMF has lower regret bound than other online models, especially when \u03bb is large. Moreover, we can see that the regret bound of P-MMF is decreasing with the increase of T, which verified the theoretical analysis results.\nSimilar results have also been observed for other \u03bb values.\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Online inference time\n\nWe experimented with investigating the online inference time of P-MMF and the most practical RS model DMF\u00a0<cit.>. \nIn real recommender systems, the number of providers is relatively small and steady. However, the number of users and items is usually huge and grows rapidly. Therefore, we tested the inference time under CPU and GPU implementations of P-MMF and DMF w.r.t. the different number of items while keeping the number of users and providers unchanged. The GPU implementation is based on PyTorch\u00a0<cit.>.\n\nFigure\u00a0<ref> reports the curves of inference time (ms) per user access w.r.t. item size. We can see that P-MMF with CPU and GPU versions need only about 20-40ms and 17-18ms for online inference, respectively. Moreover, we can see that the inference time of P-MMF did not increase much with the increasing number of items. For example, by increasing the item size from 0 to 200000, the P-MMF CPU version only needs a little bit more time (19ms) for online inference. The inference time for the GPU version almost kept unchanged. As for comparisons, DMF's inference time increased rapidly: both CPU and GPU versions need much more time (about 65ms). The phenomenon can be easily explained with the dual problem analysis in Theorem\u00a0<ref>. We see the parameter size of P-MMF is provider size, which is far less than the item size |\ud835\udcab|\u226a |\u2110|. Therefore, the online inference time is not sensitive to item numbers. We conclude that P-MMF can be adapted to the real online recommendation scenarios efficiently because of its low and robust online computational cost, even when the number of items grows rapidly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\nThis paper proposes a novel re-ranking model called P-MMF that aims to balance provider fairness and user preference.  Firstly, We formalize the task as a resource allocation problem regularized by a max-min fairness metric.  Then, to adapt the online scenarios in the recommendation, we proposed a momentum gradient descent method to conduct online learning for resource allocation in the dual space.  Theoretical analysis showed that the regret of P-MMF is bounded.  Extensive experimental results on four large-scale datasets demonstrated that P-MMF outperformed the baselines and Pareto dominated the state-of-the-art provider fair baselines.\n\n\nThis work was funded by the National Key R&D Program of China (2019YFE0198200), the National Natural Science Foundation of China (61872338, 62006234, 61832017), Intelligent Social Governance Interdisciplinary Platform, Major Innovation & Planning Interdisciplinary Platform for the \u201cDouble-First Class\u201d Initiative and the Outstanding Innovative Talents Cultivation Funded Programs 2023 of Renmin University of China.\n\n\n\n\nACM-Reference-Format\n\n\n\n\n\u00a7 APPENDIX\n\n\n\n\n \u00a7.\u00a7 Proof of Theorem\u00a0<ref>\n\n\nFor max-min fairness, we have the regularizer as\nr(\ud835\udc1e) = min_p\u2208\ud835\udcab(\ud835\udc1e_p/\u03b3_p),\nwe can easily proof that the exposure vector \ud835\udc1e can be represented as the dot-product between decision varible \ud835\udc31_t and the item-provider adjacent matrix \ud835\udc00:\n\ud835\udc1e = \u2211_t=1^T(\ud835\udc00^\u22a4\ud835\udc31_t).\nThen we treat the \ud835\udc1e as the auxiliary variable, and the Equation\u00a0(<ref>) can be written as:\n\n    W_OPT   = max_\ud835\udc31_t\u2208\ud835\udcb3,\ud835\udc1e\u2264\u03b3[\u2211_t=1^Tg(\ud835\udc31_t)/T + \u03bb r(\ud835\udc1e)]\n    \n        s.t. \ud835\udc1e   = \u2211_t=1^T(\ud835\udc00^\u22a4\ud835\udc31_t),\n\nwhere \ud835\udcb3 = {\ud835\udc31_t|\ud835\udc31_t \u22080,1\u2211_i\u2208\u2110\ud835\udc31_ti = K}. \nThen we move the constraints to the objective using a vector of Lagrange multipliers \u03bc\u2208\u211d^|\ud835\udcab|:\n\n    W_OPT = max_\ud835\udc31_t\u2208\ud835\udcb3,\ud835\udc1e\u2264\u03b3min_\u03bc\u2208\ud835\udc9f[\u2211_t=1^Tg(\ud835\udc31_t)/T + \u03bb r(\ud835\udc1e) - \u03bc^\u22a4(-\ud835\udc1e+\u2211_t=1^T\ud835\udc00^\u22a4\ud835\udc31_t)]\n       \u2264min_\u03bc\u2208\ud835\udc9f[max_\ud835\udc31_t\u2208\ud835\udcb3[\u2211_t=1^Tg(\ud835\udc31_t)/T - \u03bc^\u22a4\u2211_t=1^T\ud835\udc00^\u22a4\ud835\udc31_t] + max_\ud835\udc1e\u2264\u03b3(\u03bb r(\ud835\udc1e)-\u03bc^\u22a4\ud835\udc1e)]\n       =min_\u03bc\u2208\ud835\udc9f[f^*(\ud835\udc00\u03bc) + \u03bb r^*(-\u03bc)] = W_Dual,\n\nwhere \ud835\udc9f = \u03bc|r^*(-\u03bc)<\u221e} is the feasible region of dual variable \u03bc. According to the Lemma 1 in the \u00a0<cit.>, we have \ud835\udc9f is convex and positive orthant is inside the recession cone of \ud835\udc9f.\n\n\n\n\n\n \u00a7.\u00a7 Proof of Theorem\u00a0<ref>\n\n\nWe let the variable \ud835\udc33_p = (\ud835\udc1e_p/\u03b3_p-1), we have:\n\n    r^*(\u03bc)    = max_\ud835\udc1e\u2264\u03b3[min(\ud835\udc1e_p/\u03b3_p) + \u03bc^\u22a4\ud835\udc1e/\u03bb]\n       = \u03bc^\u22a4\u03b3 /\u03bb +  1 + max_\ud835\udc33_p \u2264 0[min(\ud835\udc33_p) + 1/\u03bb\u2211_p\u2208\ud835\udcab\u03bc_p\u03b3_p\ud835\udc33_p]\n\n    \nLet s(\ud835\udc33) = min_p \ud835\udc33_p and \ud835\udc2f = (\u03bc\u2299\u03b3)/\u03bb, \u2299 is the hadamard product. Then we define s^*(\ud835\udc2f) = max_\ud835\udc33\u2264 0(s(\ud835\udc33) + \ud835\udc33^T\ud835\udc2f). We firstly show that if \u2211_p\u2208\ud835\udcae\ud835\udc2f_p \u2265 -1, \u2200\ud835\udcae\u2208\ud835\udcab_s, then s^*(\ud835\udc2f) = 0 and \ud835\udc33 = 0 is the optimal solution, otherwise s^*(\ud835\udc2f) = \u221e.\n\nWe can equivalently write \ud835\udc9f = {\ud835\udc2f|\u2211_p\u2208\ud835\udcae\ud835\udc2f_p \u2265 -1, \u2200\ud835\udcae\u2208\ud835\udcab_s}. We firstly show that s^*(\ud835\udc2f) = \u221e for \ud835\udc2f\u2209\ud835\udc9f. Suppose that there exists a subset \ud835\udcae\u2208\ud835\udcab_s such that \u2211_p\u2208\ud835\udcae\ud835\udc2f_p < -1. For any b > 1, we can get a feasible solution:\n\n    \ud835\udc2f_p= {[        -b,        p\u2208\ud835\udcae;         0, otherwise. ].\n\nThen, because such solution is feasible and s(\ud835\udc33) = -b, we obtain that \n\n\n    s^*(\ud835\udc2f) \u2265 s(\ud835\udc33) - b(\u2211_p\u2208\ud835\udcae\ud835\udc2f_p) = -b(\u2211_p\u2208\ud835\udcae\ud835\udc2f_p+1).\n\nLet b\u2192\u221e, we have s^*(\ud835\udc2f)\u2192\u221e.\n\nThen we show that s^*(\u03bc) = 0 for \ud835\udc2f\u2208\ud835\udc9f. Note that \ud835\udc33 = 0 is feasible. Therefore, we have\n\n    s^*(\ud835\udc2f)\u2265 s^*(0) = 0.\n\nThen we have \ud835\udc33\u2264 0 and without loss of generality, that the vector \ud835\udc33 is sorted in increasing order, i.e., \ud835\udc33_1\u2264\ud835\udc33_2, \u22ef, \u2264\ud835\udc33_|\ud835\udcab|.\nThe objective value is\n\n    s^*(\ud835\udc2f)    = \ud835\udc33_1 + \u2211_j\u2208|\ud835\udcab|\ud835\udc33_j\ud835\udc2f_j \n       = \u2211_j=1^|\ud835\udcab|(\ud835\udc33_j-\ud835\udc33_j+1)(1+\u2211_i=1^j\ud835\udc2f_j)\u2264 0.\n\n\nThus we can have s^*(\u03bc) = 0 for \ud835\udc2f\u2208\ud835\udc9f and we have \n\n    r^*(-\u03bc) = \u03bc^\u22a4\u03b3/\u03bb + 1.\n\n\n\n\n\n\n \u00a7.\u00a7 Proof of Theorem\u00a0<ref>\n\n\nFirstly, in practice, we normalize the user-item preference score s_u,i to [0,1]. Therefore, \u2211_t=1^Tg(\ud835\udc31_t)/T\u2264 K. In max-min regularizer r(\ud835\udc1e). Let's  abbreviate its upper bound to r\u0305. In practice, r\u0305\u2264 1 We have\n\n    W_OPT\u2264 K + \u03bbr\u0305.\n\n\nWe consider the stopping time \u03c4 of Algorithm\u00a0<ref> as the first time the provider will have the maximum exposures, i.e.\n\n    \u2211_t=1^\u03c4\ud835\udc00^\u22a4\ud835\udc31_t \u2265\u03b3.\n\nNote that is \u03c4 a random variable.\n\nSimilarly, followed the prove idea of\u00a0<cit.>, First, we analysis the primal performance of the objective function. Second, we bound the complementary slackness term by the momentum gradient descent. Finally, We conclude by putting it to achieve the final regret bound.\n\nPrimal performance proof: Consider a time t<\u03c4, the recommender action will not violate the resource constraint. Therefore, we have:\n\n    g(\ud835\udc31_t)/T = g^*(\ud835\udc00\u03bc_t) + \u03bb\u03bc_t^T\ud835\udc00^\u22a4\ud835\udc31_t,\n\nand we have \ud835\udc1e_t = _\ud835\udc1e\u2264\u03b3{r(\ud835\udc1e) + \u03bc^\u22a4\ud835\udc1e/\u03bb}\n\n    r(\ud835\udc1e_t) = r^*(-\u03bc) - \u03bc_\ud835\udc2d^T\ud835\udc1e_t/\u03bb.\n\n\nWe make the expectations for the current time step t for the primal functions:\n\n    \ud835\udd3c[g(\ud835\udc31_t)/T + \u03bb  r(\ud835\udc1e_t)]    = \ud835\udd3c[g^*(\ud835\udc00\u03bc_t) + \u03bc_\ud835\udc2d^T\ud835\udc00^\u22a4\ud835\udc31_t + \u03bb r^*(-\u03bc) - \u03bc_\ud835\udc2d^T\ud835\udc1e_t] \n       = W(\u03bc_t) - \ud835\udc04[\u03bc_t^T(-\ud835\udc00^\u22a4\ud835\udc31_t + \ud835\udc1e_t)].\n\n\nConsider the process Z_t = \u2211_j=1^T\u03bc_j^t(-\ud835\udc00^\u22a4\ud835\udc31_t + \ud835\udc1e_t)-\ud835\udc04[\u03bc_t^T(-\ud835\udc00^\u22a4\ud835\udc31_t + \ud835\udc1e_t)] is a martingale process. The Optional Stopping Theorem in martingale process\u00a0<cit.> implies that \ud835\udd3c[Z_\u03c4] = 0. Consider the variable w_t(\u03bc_t) = \u03bc_t^T(-\ud835\udc00^\u22a4\ud835\udc31_t + \ud835\udc1e_t), we have\n\n    \ud835\udd3c[\u2211_t=1^\u03c4w_t(\u03bc_t)] = \ud835\udd3c[\u2211_t=1^\u03c4\ud835\udd3c[w_t(\u03bc_t)]]\n\n\nMoreover, in MMF, the dual function W_Dual is convex proofed in Theorem\u00a0<ref>, we have\n\n    \ud835\udd3c[\u2211_t=1^\u03c4g(\ud835\udc31_t)/T + \u03bb r(\ud835\udc1e_t)]     = \ud835\udd3c[\u2211_t=1^\u03c4W_Dual(\u03bc_t)] - \ud835\udd3c[\u2211_t=1^\u03c4w_t(\u03bc_t)]\n       \u2264\ud835\udd3c[\u03c4 W_Dual(\u03bc_\u03c4)] - \ud835\udd3c[\u2211_t=1^\u03c4w_t(\u03bc_t)],\n\nwhere \u03bc_\u03c4 =\u2211_t=1^\u03c4\u03bc_t/\u03c4.\n\nComplementary slackness proof\nThen we aim to proof the complementary slackness \u2211_t=1^Tw_t(\u03bc_t) - w_t(\u03bc) is bounded. Suppose there exists G, s.t. the gradient norm is bounded \ud835\udc20_t \u2264 G. Then we have:\n\n    \u2211_t=1^\u03c4w_t(\u03bc_t) - w_t(\u03bc) \u2264\u03bb^2/\u03b7 + G^2/(1-\u03b1)\u03c3\u03b7(\u03c4-1) + G^2/2(1-\u03b1)^2\u03c3\u03b7,\n\nwhere the project function \u03bc-\u03bc_t _\u03b3^2 is \u03c3-strongly convex.\n\nNext we prove the inequality in Equation\u00a0(<ref>). \nAccording to the Theorem 1 in \u00a0<cit.>, \nwe have \n\n    \ud835\udc20_t _2^2 = (1-\u03b1)\u2211_s=1^t\u03b1^t-s(\ud835\udc20_s)_2^2 \u2264 G^2,\n\nand \n\n    \u2211_t=1^\u03c4w_t(\u03bc_t) - w_t(\u03bc) \u2264\u03bc_t-\u03bc_0_\u03b3^2^2/\u03b7 + G^2/(1-\u03b1)\u03c3\u03b7(\u03c4-1) + G^2/2(1-\u03b1)^2\u03c3\u03b7, \u2200\u03bc.\n\n\n\n\n\nAssuming there exists H>0, s.t. \u03bc_t-\u03bc_0_\u03b3^2^2\u2264 H. According to the Cauchy-Schwarz' inequality.\nThe results follows.\nLet  M=H/\u03b7 + G^2/(1-\u03b1)\u03c3\u03b7(T-1) + G^2/2(1-\u03b1)^2\u03c3\u03b7. \nWe now choose a proper \u03bc, s.t. the complementary stackness can be further bounded.\n\nFor \u03bc = \u03bc\u0302 + \u03b8, where \u03b8\u2208\u211d^|P| is non-negative to be determined later and \u03bc\u0302 = _\u03bc-\u03bc^\u22a4(\u2211_i=1^T\ud835\udc00^\u22a4\ud835\udc31_t)/\u03bb. According to the constraint \ud835\udc1e = \u2211_i=1^T\ud835\udc00^\u22a4\ud835\udc31_t, we have that\n\n    \u2211_t=1^T(r(\ud835\udc1e_t) + \u03bc^\u22a4\ud835\udc1e_t\u03bb) \u2264 r^*(-\u03bc\u0302) = r(\u2211_i=1^T\ud835\udc00^\u22a4\ud835\udc31_t) + \u03bc\u0302^T(\u2211_i=1^T\ud835\udc00^\u22a4\ud835\udc31_t)/\u03bb.\n\nNote that in proof of Theorem\u00a0<ref>, the feasible region \ud835\udc9f is recession cone, therefore, \u03bc\u2208\ud835\udc9f.\n\nTherefore, we have \n\n    \u2211_t=1^\u03c4w_t(\u03bc_t)\n          = \u2211_t=1^Tw_t(\u03bc\u0302) - \u2211_t=\u03c4+1^Tw_t(\u03bc\u0302)  + \u2211_t=1^\u03c4w_t(\u03b8) + M.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPut them together: \nUnder the max-min fair, we obtain that\n\n    W_OPT = \u03c4/TW_OPT + T-\u03c4/TW_OPT\u2264\u03c4 W_Dual(\u03bc_\u03c4) + (T-\u03c4)(K+\u03bbr\u0305).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\nTherefore, combining Eq.\u00a0(10,12,13) the regret Regret(h) can be bounded as:\n\n    Regret(h)    = \ud835\udd3c[W_OPT-W]\n       \u2264\ud835\udd3c[W_OPT-\u2211_t=1^\u03c4(g(\ud835\udc31_t)/T - \u03bb r(\ud835\udc00^\u22a4\ud835\udc31_t/\u03b3))]\n       \u2264\ud835\udd3c[W_OPT-\u03c4 W_Dual(\u03bc_t) + \u2211_t=1^\u03c4w_t(\u03bc_t) + \u2211_t=1^T(\ud835\udc1e_t-\ud835\udc00^\u22a4\ud835\udc31_t)] \n       \u2264\ud835\udd3c[(T-\u03c4)(K+\u03bbr\u0305)+\u2211_t=1^Tw_t(\u03bc\u0302) + \u2211_t=1^\u03c4w_t(\u03b8)] + M\n       \u2264 (T-\u03c4)(K+\u03bbr\u0305+\u03bb K) + \u2211_t=1^\u03c4w_t(\u03b8) + M.\n\n\nLet C = K+\u03bbr\u0305+\u03bb K, then setting the \u03b8 = Cmin_p \u03b3_p\ud835\udc2e_p, where \ud835\udc2e_p is the p-th unit vector. We have\n\n    \u2211_t=1^\u03c4w_t(\u03b8) = C/(min_p \u03b3_p)  - C(T-\u03c4).\n\nThen the Regret(h)\u2264 M + C/(min_p \u03b3_p), when we set \u03b7 = O(T^-1/2), the Regret(h) is comparable with O(T^1/2). The result follows.\n\n\n\n\n\n    \n\n\n \u00a7.\u00a7 Algorithm for Min-reguarlizer\n\nIn this section, we proposed a heuristic method for provider MMF online application, named Min-reguarlizer. It has a regularizer that measures the exposure gaps between the target provider and the worst-providers. The detailed algorithm is shown in Algorithm\u00a0<ref>. The notations are the same as P-MMF in Algorithm\u00a0<ref>.\n\n\n\n\n\n\n\n\n"}