{"entry_id": "http://arxiv.org/abs/2303.07263v1", "published": "20230313164247", "title": "InferFix: End-to-End Program Repair with LLMs", "authors": ["Matthew Jin", "Syed Shahriar", "Michele Tufano", "Xin Shi", "Shuai Lu", "Neel Sundaresan", "Alexey Svyatkovskiy"], "primary_category": "cs.SE", "categories": ["cs.SE"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Microsoft\n  Redmond\n  WA\n  USA\n\n\n\n\n  UCLA\n  Los Angeles\n  CA\n  USA\n\n\n\n\n  Microsoft\n  Redmond\n  WA\n  USA\n\n\n\n\n  Microsoft\n  Redmond\n  WA\n  USA\n\n\n\n\n  Microsoft Research\n  Beijing\n  China\n\n\n\n\n  Microsoft\n  Redmond\n  WA\n  USA\n\n\n\n\n  Microsoft\n  Redmond\n  WA\n  USA\n\n\n\n\n\n\n\nSoftware development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. \n\nLarge language models have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose : a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs.  combines a Retriever \u2013 transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator \u2013 a large language model (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. \n\nTo train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that  outperforms strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java. We discuss the deployment of  alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration pipeline to automate the software development workflow.\n\n\n\n\n\n: End-to-End Program Repair with LLMs over Retrieval-Augmented Prompts\n    Alexey Svyatkovskiy\n    March 30, 2023\n======================================================================\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nThe software development lifecycle is profoundly affected by bugs. Traditional program analyses techniques can detect and localize bugs through formal reasoning, leaving the task of categorizing the bugs and coding a patch to a developer. The traditional approach of manually generating patches through examination of code changes is a time consuming and error-prone task, which could be automated.\n\n\nMany of the recently proposed approaches for bug prediction, detection, and repair rely on machine learning algorithms \u2013 infamously data hungry \u2013  depending on large amounts of high quality data for effective training. Large language models have been successfully adapted to the program repair tasks through few-shot demonstration learning and instruction prompting, treating this as an infilling task\u00a0<cit.>. However, while focusing on solving specific research problems they failed to provide a reliable end-to-end program repair solution that could be productized. \n\nStatic analysis tools like Infer can be used to identify critical security and performance issues. This can preempt large parts of the software development cycle, including the process of creating detailed unit tests, which can be extremely time-consuming and difficult for a large, complex project whose code is broken down into many modules or across many files. They can also identify bugs and produce bug reports in a way that is machine-readable and conducive to usage in conjunction with patch generation models.\n\nIn this work we focus on three types of bugs reported by Infer: Null Pointer Dereference (NPD), Resource Leak (RL), and Thread Safety Violation (TSV). We focus on these because they pose critical performance, reliability and security issues, and can also be more difficult to fix than other issue types which are also more commonly detected and studied. \n\nLanguage models commonly adopt two paradigms for task-specific generalization \u2013 via finetuning or few-shot learning. In the former paradigm, the canonical model training structure is divided into two phases \u2013 pretraining and finetuning. In pretraining stage, a model is trained in a self-supervised way to perform denoising or generic sequence-to-sequence transformations, geared towards improving the performance on a variety of downstream tasks. In the finetuning stage the model is trained on a specialized supervised dataset to perform a concrete task, such as question answering, text summarization, or in our case, program repair. The few-shot learning paradigm allows model specialization for a downstream task via prompt augmentation, composition, or ensembling\u00a0<cit.>. A variant of prompt augmentation, commonly called demonstration learning, introduces a few input-output examples for a given task, for instance \u201cThe capital of China is Beijing. The capital of Italy is Rome. The capital of South Africa is \u201d, allowing to achieve good performance on a downstream task without any gradient updates, which is crucial for very large language models like GPT-3\u00a0<cit.>, T5\u00a0<cit.>, and PaLM\u00a0<cit.>. Another variant of prompt augmentation commonly referred to as instruction prompting aims to introduce a natural language description of a task, for instance: \u201cwrite a program to determine whether a graph is bipartite\u201d. It may utilize prompt templates filling in necessary information from an external source (a database, a neural model). In our approach we combine the benefits of both paradigms, by augmenting the prompts and then finetuning our model on the dataset of augmented prompts and predictions to get the best performance.\n\n\nIn this paper, we introduce \u2013 a program repair framework which combines a transformer encoder model pretrained via contrastive learning serving as a retriever over a database of historic bugs and fixes, and a large language model (12 billion parameter Codex Cushman model, ) instrumented with the facility to leverage retrieved information from the external database. Given the baseline Codex model has been shown to occasionally predict insecure or buggy code\u00a0<cit.>, we prioritized finetuning it on a bug-free supervised dataset of bugs and fixes with contexts enriched via relevant program repair patterns from an external non-parametric memory. The contributions of the paper are as follows: (i) we propose a program repair framework that leverages static analyses for bug detection, localization, and categorization paired with a large language model finetuned for program repair task on a dataset of augmented prompts, (ii) we curate : a metadata-rich dataset of bugs and fixes in Java and C# programming languages extracted with the Infer static analyzer, (iii) we introduce a dedicated prompt augmentation technique for program repair task, which leverages dense retrieval from an external database of historic bugs and fixes, bug type annotations, and syntactic hierarchies across the entire source code file affected by a bug, (iv) we evaluate our model on the  dataset, achieving an impressive 76% top-1 accuracy of patch generation in Java, and over 65% in C#, across null pointer dereference, resource leak, and thread safety violation bug types, and finally (v) we deploy  as a GitHub action and as part of the Azure DevOps continuous integration pipeline internally at Microsoft, and document aspects of deployment.\n\n\n\n\u00a7 MOTIVATING EXAMPLE\n\n\nTo provide the intuition about how our approach works and to describe the concrete details of the bug detection, localization, and repair scenario we begin with a motivating example.\n\nIn a typical continuous software development workflow, software engineers make atomic, iterative changes to feature branches periodically merging to the  production branch, which is then continuously and automatically deployed to the end users. Consider a large software project with a modular code base spread across multiple source code files. It can be extremely inefficient, in terms of developer time and effort, to detect, localize, and fix errors manually before they are merged to main. In addition, it requires the creation of an extensive unit test suite to ensure that a feature or change works across all possible versions of the software, and no regressions are introduced. \n\n<ref> illustrates a typical software development workflow at Microsoft Developer Division in presence of . As a pull request proposing code changes is created, continuous integration pipeline (CI) triggers unit testing, build, and Infer static analysis steps. If bugs are detected, the patch generation module will be invoked to propose a fix. The proposed bug fix is then validated and subsequently served as a bug-fixing pull request to a feature branch allowing developer to catch bugs before merging the code to the production branch.\n\n\n\n\nOur approach combines a static analyzer to detect, localize, and classify bugs with a powerful LLM (finetuned 12 billion parameter Codex model) to generate fixes.\n\n<ref> provides details about  workflow based on a real-world bug example from the acs-aem-common\u00a0<cit.> repository, which is a unified collection of code for content management that optimizes authoring, and delivery of content and digital media written in Java. The Infer static analyzer detects a null pointer dereference error, due to an object in the code returned by  call, which could be null and is dereferenced at line 168. The context preprocessing module utilizes the information provided by the analyzer to extract the buggy method, and retains surrounding context most relevant to fixing the bug \u2013 import statements, class signature, body of the  method which is invoked at buggy line. The retrieval augmentation engine then searches for semantically similar buggy code snippets in the historic database, prepending similar bug-fixes to the prompt. Finally, the augmented prompt is sent to the finetuned Codex model for inference. The predicted patch is then validated by executing the Infer static analyzer and unit tests as part of the continuous integration pipeline to ensure the error is indeed fixed and no regressions are introduced in the code base.\n\n\n\n\n\n\u00a7 DATASET\n\n\n\n\n\nWe collect a supervised dataset of bugs detected with  (Infer Static Analyzer), which performs semantic analysis via Separation Logic. \n\nWe executed  and  over the change histories of approximately 6.2k Java and C# open-source repositories (2.9k Java, 3.3k C#) hosted on GitHub, analyzing more than 1 million commits. While a few bug datasets are already available, such as Defects4j\u00a0<cit.>, QuixBugs\u00a0<cit.>, ManySStuBs4J\u00a0<cit.>, UnifiedBugDataset\u00a0<cit.> and many others, the dataset we introduce is differentiated by the amount and quality of information provided about each bug by the static analysis. Specifically, each bug in the dataset is associated with several pieces of metadata, including:\n\n    \n  * Bug Type: each detected or fixed bug is marked with a bug type extracted with , such as: null dereference, resource leak, immutable cast, etc. This information could be potentially used by automated program repair techniques to guide the bug-fixing attempts. Alternatively, these instances can be used as labeled data points for bug classification techniques.\n    \n    \n  * Bug Location: the dataset provides localization info at different levels of granularity: file, class, method, and line. For specific types of bugs, also affected variables/methods are reported.\n    \n    \n  * Change History: bugs are linked with the change history of the software project. Specifically, the dataset provides information on when a bug was introduced or fixed throughout the development process. Additionally, each analyzed commit is associated with the introduced/fixed or preexisting bugs involving the file touched in the commit.\n\n\n\n\n\n \u00a7.\u00a7 Background on Infer Static Analyzer\n\n\nInfer is an open-source static analysis tool originating from program analysis research on separation logic. It was first developed by the startup Monoidics Ltd, which was acquired by Facebook in 2013 and open-sourced in 2015. It computes program specifications to detect errors related to memory safety, concurrency, security, and more. It is industrially deployed at companies including Meta, Amazon, and Microsoft. Although in this work we will focus on null dereferences, resource leaks, and thread safety violations detected by Infer, it is able to detect a much wider variety of security and performance issues. For example, via taint tracking it is able to detect dataflow-related issues such as SQL injections. We believe our framework will be capable of mining and generating patches for these bug types as well, but leave the examination of this to future efforts.\n\nAt Meta, Infer runs within the internal continuous integration (CI) system of repositories consisting of 10s and 100s millions of lines of code, including those for WhatsApp, Instagram, and Facebook core. Infer runs on diffs and reports issues to developers by writing comments within the code review system. A study conducted at Meta\u00a0<cit.> saw a false positive rate under 20%, and issues posted saw a fix rate of 70%. The high issue relevance driven by this diff-time deployment of this system is critical; the same study saw a near-zero fix rate when it was deployed to developers as a list of assigned issues outside of the CI system. This underlines the value of our proposed system being deployed as a bug-detection-and-fix-recommendation code review module.\n\nInferSharp\u00a0<cit.> is the compiler frontend developed by Microsoft which translates the Common Intermediate Language (CIL) to the Smallfoot Intermediate Language interpreted by Infer, thereby enabling Infer's capabilities on all CIL languages (including C# and F#). For the purposes of this paper, InferSharp refers to the static analysis of Infer applied to CIL languages. Notably, to our knowledge it is the only interprocedural static analysis for CIL languages which is free-to-use and MIT-licensed. Considering Infer's industry track record, this creates unique opportunities in both research and industry to build bug-detection-and-fix product capabilities for a relatively underserved developer segment.\n\n\n\n \u00a7.\u00a7 Collecting Data with Infer\n\n\nIn this section we describe the data extraction process that culminated in the creation of the \u00a0dataset. Specifically, we provide details on how we executed Infer over the change histories of software projects in order to detect introduced and fixed bugs.\n\n\nGiven as input the current commit curr and the previous commit prev, we begin by computing a  to identify the files involved in the change performed by the developer in the commit curr. Next, we analyze the status of the files at commit prev. Specifically, we checkout the snapshot of the system at commit prev, and we build the system using the project-specific build tool. During the build process, the  command intercepts calls to the compiler to read source files and translates them into an intermediate representation which will allow Infer to analyze these files. Next, we invoke the  command specifying the files to be analyzed (the files diff involved in the commit). This analysis produces a report reportPrev detailing the bugs identified within the specified files.\n\n\n\n\n\nSubsequently, we move to the current commit curr and perform the same steps described for the commit prev, that is: checking out the commit, building system while capturing the source files, and analyzing the diff files in order to detect bugs. \n\nFinally, with the  command, we compute the differences between the two infer reports reportPrev and reportCurr. The output bugs contain three categories of issues:\n\n    \n  * introduced: issues found in curr but not in prev;\n    \n  * fixed: issues found in prev but not in curr;\n    \n  * preexisting: issues found in both prev and curr.\n\n\n\n\n\n\n  \n\n\n  \n\n\n\n  \n\n\n\n  \n\n  \n\n  \n\n\n\n  \n\n  \n\n  \n\n\n\n  \n\n\n\n\n\n\nWe perform these steps for each pair of commits (prev, curr) over the change histories of the analyzed software projects. We optimize this process by obviating the need to build the same commit twice (once as curr and next as prev) by instead reusing the build and capture stages in the next iteration.\n\n\n\n \u00a7.\u00a7 Filtering\n\nAlgorithm <ref> detects commits which remove bugs detected by Infer; however, some of the bugs are removed as a result of a whole method refactoring, or as the result of a fix in a different method. Such diffs are unlikely to be useful for training a bug patch model that takes as input method-level context. Thus, we apply additional heuristics intended to filter out such commits:\n\n\n    \n  * The pointer is dereferenced within the method itself, rather than dereferenced in a call to another method.\n    \n  * With respect to the method, the lines edited are within 4 lines of the reported buggy line.\n    \n  * The edit itself is at most 8 lines.\n\n\nFilter (1) ensures the dereference occurs within the method, which is necessary as the model is intended to generate patches local to the method in which the bug occurs. Filter (2) ensures that the fix occurs local to the dereference, which filters out samples which contain edits irrelevant to the bug. Filter (3) tends to filter out samples in which the bug disappears as a result of method rewrite.\n\n\n\n\n \u00a7.\u00a7 Dataset Statistics\n\n\nAfter running the extraction pipeline on 2937 repositories, we identified a total of 8280 bug patches. Of these bugs, 259 of these are null dereference patches which pass the filtering process, and 462 of these are resource leaks which pass the filtering process. We note that the filtered dataset contains commits which might have been detected by traditional methods involving extracting commits with certain keywords related to the desire bug type. Of the 259 null patches, 59 contain \u201cnull\u201d or \u201cnpe\u201d in the corresponding commit message, and of the 462 resource leak patches, 15 contain the \u201cleak\u201d keyword. We see from this that we are able to extract many additional fixes that would not have appeared using naive commit message keyword matching.\n\n\n\n\nAs shown in <ref> the  is composed of multi-line bugs, which represents a challenging case for program repair tools.\n\n\n\n\n\u00a7 BASELINES\n\n\n\nIn the following, we explore several program repair baselines which are constructed around powerful LLMs ( and ) for tasks of completing code, filling code in the middle, or generating a fix following a natural language instruction. In the following, we evaluate the performance based on the accuracy of exact string match of a generated patch to the ground truth fix.\n\n\n\n \u00a7.\u00a7 Demonstration Prompting\n\nDemonstration learning is a prompt augmentation technique in which a few answered prompts are prepended to the context with the purpose of demonstrating how a language model should approach a downstream task. For program repair, we introduce a prefix constructed of two answered prompts as, followed by the actual buggy code snippet , as shown in <ref>. \n\nOur few-shot demonstration learning experiments are based on the strong 12 billion parameter Codex language model of code.  \n\n\n\n \u00a7.\u00a7 Conditional Language Modeling\n\nOur next baseline is the zero-shot conditional language generation (code completion), which aims to utilize the next token prediction to repair programs. Specifically, given a bug-free prefix, we run Codex model inference to complete the buggy code snippet, aiming to rewrite a program without bugs. In our experiments, we apply nucleus sampling decoding algorithm with top_p = 1 and a temperature T = 0.7 generating top 10 samples up to the length of 1024 tokens with a total length for prefix and completion of 2048. Our conditional language modeling experiments are also based on the .\n\n\n\n \u00a7.\u00a7 Instruction Prompting\n\nInstruction learning is a prompt augmentation technique that introduces a natural language description of the task. To approach program repair, we prepare prompts following a template:\n\nWe utilize OpenaAI GPT-3 Davinci model, a 175 billion parameter language model and a close sibling of ChatGPT, to complete the prompts. Typically, Davinci outputs a natural language summary of the proposed fix followed by a code snippet. \nAn example prediction by ChatGPT model is illustrated in <ref>. \n\nFor the sake of evaluation, we instruct  to only output code snippet in its response which otherwise normally accompanied by the natural language descriptions.\n\n\n\n\n\n\u00a7  FRAMEWORK\n\n\n program repair framework is composed of three following key modules: (i) a static analysis tool that detects, localizes, and classifies bugs, (ii) retrieval module \u2013 a large index of historic bugs and fixes, equipped with a facility to efficiently search and retrieve \u201chints\u201d \u2013 semantically-similar source code segments \u2013  given a query, and (iii) generator module \u2013 a large language model finetuned on a dataset of prompts enriched with the information provided by the static analyzer and the retriever to generate fixes.\n\n\n\n \u00a7.\u00a7 Bug Detection & Classification Module\n\nOur bug detection, localization, and classification module is powered by the Infer, which performs program analysis via Separation Logic. Although Infer's Pulse framework has recently been released, for the purposes of this paper we examine bugs generated by Infer's biabduction framework. Compiler frontends for Infer, such as InferSharp, translate source code into the control-flow-graph intermediate representation understood by Infer, known as the Smallfoot Intermediate Language. Infer performs automated program analysis over this graph and produces compositional method summaries in order to determine whether there are defects present in the source code.  \n\n\n\n \u00a7.\u00a7 Retrieval Module\n\n\nOur retrieval module closely follows the ReACC formulation\u00a0<cit.>. The retriever searches for semantically equivalent vulnerable code given a buggy code snippet and retrieves corresponding fix candidates based on cosine similarity between the embedding of query vector q and a buggy code snippet c. \n\nDense retrieval maps each code snippet to a d-dimension dense vector. The relevance of a code snippet to a given query can then be determined as a dot product of the query vector and each document vector. We closely follow the Dense Passage Retriever (DPR) model\u00a0<cit.>. At the training stage, we adopt in-batch negatives to calculate the contrastive loss by InfoNCE\u00a0<cit.>.\n\nOur dense retriever utilizes a bidirectional transformer encoder \u2130 to obtain encoded dense vector representations of the query (\u2130(q)), and for each buggy code snippet c indexed in the retrieval database (\u2130(c)). The retrieval database is a key-value store with encoded buggy code snippets \u2130(c) serving as keys, and string representations of the corresponding fixes f serving as values. \n\nWe take the representation of the  token as a summary of the encoded sequences of tokens, and compute similarity between the query and each code snippet in the database as a dot-product: sim(q, c) = \u2130(c)^T \u00b7\u2130(q). \n\nThe bidirectional transformer encoder \u2130 is pretrained with the contrastive learning objective. Contrastive learning\u00a0<cit.> is a self-supervised learning technique, in which the machine learning model is aiming to learn from the commonality of the training samples but also the attributes that make samples different. Given a contrastive pretraining dataset D = {q_i, p_i^+, p_(i,1)^-,...,p_(i,h)^-}, i = 0...N, where each sample consists of a query \u2013 an encoding of a buggy code snippet; a positive sample representing a semantically similar code snippet of the same bug type; and a set of negative samples which are irrelevant code snippets of different bug types. The contrastive loss is then given by the following formula (negative log likelihood of the positive sample):  \n\n    L (q_i, p_i^+, p_(i,1)^-,...,p_(i,n)^-) = \n    \n    - log e^sim( q_i,p_i^+)/e^sim( q_i, p_i^+) + \u2211_i=1^n e^sim(q_i, p_(i,j)^-),\n\nwhere sim is the cosine similarity between the embedding vectors.\n\n\n\n\n \u00a7.\u00a7 Generator Module\n\nOur generator model is based on Codex Cushman (), a 12B parameter decoder-only transformer language model\u00a0<cit.> developed by OpenAI, which is a descendant of GPT-3, trained on source code.\n\nWe finetune Codex on a supervised corpus extracted from the  dataset, with the goal of teaching the model to generate a fix for the given buggy code. Specifically, the input to the model is the buggy code augmented with additional information such as bug localization and categorization, hierarchical extended context, and retrieved similar fixes. We discuss the prompt augmentation process in detail in <ref>.\n\nWe perform full model finetuning (updating all weights of the model), on sixty four 32 GB V100 GPU for 5 epochs, retaining best model checkpoint by the exact match accuracy metric. We utilize Babel platform \u2013 a model repository and an AzureML designer component family bringing together state-of-the-art transformer models on Azure ML compute for rapid experimentation. We use Adam stochastic optimization procedure with the learning rate of 0.01, warmup period of 1000 optimization steps, and global batch size of 256.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 PROMPT AUGMENTATION\n\n\n\nPrompt augmentation has been shown to be a powerful technique for extracting high-quality outputs from large language models, and, in particular, for domain and task adaptation. In the following we describe our dedicated prompt augmentation approach for program repair task. The proposed approach is two-fold: (i) we extract and prioritize syntax hierarchies which are most relevant to the buggy snippet region, including focal context, and (ii) retrieve hints \u2013 structurally similar bug fixes from commit histories on GitHub. By doing so we are constructing a loosely structured template which includes the following: \n\n\n  * Retrieved hints\n\n  * Bug type annotation\n\n  * Syntactic hierarchies and peer methods\n\n  * Focal methods\n\n  * Buggy method with location markers\n\n\n<ref> shows an example of augmented prompt input for a null pointer dereference bug in Java, which includes the buggy code region surrounded by location markers, containing the method with surrounding most relevant syntax hierarchies, but type annotation string, and \u201chints\u201d \u2013 structurally similar bug fixes retrieved from the historic database. \n\n\nIn the following subsections, we will describe each prompt augmentation technique and quantify its impact on bug-fixing performance by adding features incrementally.\n\n\n\n\n \u00a7.\u00a7 Basic Prompt\n\nThe most basic prompt we can construct for the model is to provide the buggy method as input while expecting the model to generate the fix by outputting the fixed version of the given method. Thus, we perform task-oriented finetuning of our Generator model (Codex) using the buggy and fixed versions of the methods from the  dataset. \n\nWe compare this basic prompting and finetuning against powerful LLM baselines described in Sec. <ref>. <ref> illustrates the effect of finetuning as compared to zero-shot and few-shot variants. Demonstration learning appears to be the most successful few-shot learning strategy for adapting the Codex model () to downstream patch generation task, yielding a modest 19\u201325% accuracy of fixing Java bugs. Instruction learning, which also includes natural language descriptions of the downstream task in the prompt, only becomes viable as the model size increases \u2013 we repeated the instruction learning experiments with the 175 billion parameter Davinci model (), a close sibling of ChatGPT. We observe a very competitive performance with the Davinci variant, with 40\u201353% accuracy of fixing Java bugs via prompt augmentation alone. Task oriented finetuning, without any prompt crafting, outperforms all few-shot baselines by a good margin, showing 11\u201355% relative improvement of accuracy across all bug types in Java. This improvement comes at a cost of computing resources necessary to finetune the Codex model, but provide an advantage of higher accuracy and cheaper inference as compared to few-shot Davinci. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Bug Type Annotations\n\n\nThe simplest prompt augmentation step is to prepend a bug type annotation to a basic prompt consisting of a buggy method only. As shown in <ref>, this improves performance across all bug categories and languages (Java and C#) yielding 2.7\u20135.6% relative improvement in accuracy.\n\n\n\n\n \u00a7.\u00a7 Bug Localization\n\n\nBug location information is crucial for accurate program repair. Infer static analyzer can localize bugs by tracking the flow of data through the program and detecting any violations of predefined rules or programming patterns. Infer static analyzer outputs a line number on which an error could occur at runtime, which, however, does not mean that the fix would require to edit this line only. In our dataset\u00a0<ref>, the bugs are often spanning over multiple lines of code, having disjoint diff regions.\n\nWe utilize the bug location information output by Infer in two ways: (i) we parse the source code file affected by the bug to extract a method which contains the buggy line, and (ii) we surround the buggy region with special sentinel  and  symbols. During training, we refine the bug location by looking at the two-way diff markers with respect to the fix. During test time, we only use the information provided by the static analyzer as the fix is unknown. \n\n<ref> demonstrates the impact of adding bug location markers in the prompt in addition to the bug type annotations. As seen, this leads to a positive effect across all categories studies, up to 3.4% relative improvement in accuracy. The effect is more pronounced for larger methods.\n\n\n\n\n \u00a7.\u00a7 eWASH extended context\n\n\nA source code file may have nested scopes and references to other external libraries or other files.  To accurately suggest patches a model must leverage knowledge across different parts of the file. The length of source code files will often exceed the fixed-length window of transformer models (2048 tokens in our case), which could potentially lead to a loss of information relevant for learning to repair programs. To overcome this limitation, we utilize eWASH\u00a0<cit.> to prioritize syntax hierarchies which are most relevant to the buggy snippet region. Extracting syntactic hierarchies from the entire source code files, as opposed to the tokens immediately preceding the bug location, we are able to retain most relevant code context, such as class-level fields and method arguments, and peer methods which are highly relevant to program repair. Starting with a concrete syntax tree of a source file, we organize and prioritize class-level and method-level syntactic elements such as global import statements and assigned values, class attributes, method signatures, class docstring, and global expressions in the input. \n\nQuite often, a method affected by a bug will only contain an invocation expression or a call to a method defined elsewhere in the file \u2013 what we refer to as buggy focal method. For instance, in <ref> the buggy line of code has a return statement which is composed of a chain of method invocations with  and  focal methods. We conjecture that retaining the focal method implementation (signature, docstring, and body) in the prompt is crucial for program repair. We utilize stack trace provided as part of the Infer bug report to determine relevant focal method name, and include it in the prompt. <ref> shows the effect of adding the eWASH syntax hierarchies and focal context in the prompt. As seen, patch generation accuracy is further improved by over 7.2\u20137.8% for Java and by 4.0\u20136.7% for C#. \n\n\n\n\n\n \u00a7.\u00a7 Enriching Context with Hints\n\n\nTo further enrich prompts, we perform a nearest neighbor search in the retrieval database for semantically similar fixes \u2013 so called hints. The resulting fixes are then prepended to the context with an instruction string . \n\nBy default, we extract and prepend 2 nearest neighbors for each query. We apply quality criteria to avoid obviously incorrect matches: (i) retrieved fixes must be of the same bug type as the query, and (ii) impose a minimum similarity threshold between retrieved fixes and a query of 60%.\n\nTo focus on extracting structurally similar fixes and reduce the dependency on identifier naming we obfuscate code snippets serving as keys in the database and search queries. Namely, we parse and analyze the code identifier types and mask the names of classes, methods, and identifiers with placeholder symbols: , , and , where  is a unique number. An example obfuscated representation is shown in <ref>.\n\n\n\n\n\nTable <ref> shows the improvements in bug-fixing capabilities for  with prompt which incorporates retrieved hints. This prompt augmentation further improves  performances by 1\u20132% in absolute top-1 performances.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Inference\n\nThe Inference step for  involves utilizing nucleus sampling decoding with a top_p parameter of 1.0 and a temperature of 0.7. During this step, the tool decodes the top-10 best predictions generated by the large language model, and ranks them according to their sequence log probabilities. This ranking helps to ensure that the most likely and relevant fixes are presented to the user. The use of nucleus sampling decoding, with its specific top_p and temperature parameters, helps to balance the trade-off between diversity and quality in the generated predictions, making it possible to obtain highly accurate and diverse patch candidates.\n\n\n\n\u00a7 RESULTS\n\n<ref> shows the results achieved by  on the  dataset compared against the LLM baselines discussed in <ref>.  is able to fix between 57% and 82% of the three categories of bugs for Java and C#, just with the top-1 prediction. The performance gap between our approach and the best performing baseline (Finetuned Codex) is between 8.6% and 13% in absolute terms. \n\nIt is important to keep in mind that the results shown in <ref> present a conservative estimate of  's potential for generating fixes. The percentages displayed in the table are based on generated patches that exactly match the original developer's token-by-token fix. However, there may be other candidate patches that correctly fix the bug using a different token sequence.\n\nThe impressive top-1 results achieved by  are critical for its efficient and effective integration into the software development cycle. With the ability to propose high-quality fixes for critical bugs,  has the potential to greatly enhance the productivity and reliability of the software development process.\n\n\n\n\n\n\u00a7 DEPLOYMENT\n\nThe deployment of  at Microsoft as part of the Azure DevOps and GitHub continuous integration pipeline (CI) has significantly improved the software development workflow for our internal projects at Developer Division. Such tight integration has enabled our software development teams to automate the bug detection and fixing process, reducing the time and effort required to manually identify and fix bugs, and ensuring that bugs are addressed quickly and accurately. <ref> provides an overview of our CI pipeline with the integrated  stages. When a pull request proposing code changes is created, the CI pipeline automatically triggers three steps: (i) build, (ii) testing, and (iii) Infer (or InferSharp) static analysis. If bugs are detected, the  patch generation module is invoked to propose a fix.  leverages the detailed information about the bug provided by Infer, such as context, location, and classification of the bug type.\n\nThe  module proposes a (configurable) set of candidate patches. Each candidate patch is packaged as a separate Pull Request, which is individually validated. The validation process is seamless and reuses the three CI pipeline steps mentioned above. Specifically, the PR containing the candidate patch is validated through: (i) build \u2013 checking that the candidate patch is syntactically and semantically correct w.r.t. the source project; (ii) testing \u2013 ensuring that the candidate patch does not introduce regressions (failing tests); (iii) Infer static analysis \u2013 validating that the candidate patch actually fixes the previously detected bug. The validated fix is then provided to the developer within the feature branch of the developer's Pull Request. The complexity of these stages are abstracted away from the developer, who will simply receive a PR comment within the system they are using (GitHub or Azure DevOps). We implemented a GitHub action which receives a validated patch from  and surfaces it to the developer in form of a GitHub comment in the PR. The comment provides detailed information about the bug (extracted by Infer), and the resolution (served by ). The developer has the option to accept or decline the recommended fix.\n\nThe deployment of  in the CI pipeline for our internal projects has provided significant benefits. Our software development teams can now focus on more important tasks, confident in the knowledge that bugs are being detected and fixed in a timely and efficient manner. We are currently in the process of expanding the number of projects that integrate into their CI pipeline, and the benefits of this integration have been demonstrated through the seamless validation process and abstracted complexity for the developer.\n\n\n\n\n\u00a7 RELATED WORK\n\n\nOur approach is related to a broad set of literature on patch generation and prompting and task-oriented finetuning. We refer a reader to\u00a0<cit.> for a more comprehensive overview on the prior research in the area of program repair, and\u00a0<cit.> for a systematic survey of prompting methods in NLP.\n\nPatches in the Wild\u00a0<cit.>, utilize supervised machine translation to learn bug-fixing patterns for various common code defects. They mine bug-fixes from the change histories of projects hosted on GitHub and define the learning task on a method level, disregarding the surrounding code context. SequenceR\u00a0<cit.> improves upon the Patches in the Wild by leveraging the extended context available through the source code file containing the buggy code, showing first attempt at prompt crafting. SequenceR learning objective is based around supervised machine translation with encoder-decoder recurrent neural network. Copy That!\u00a0<cit.> builds upon an observation that patches typically only affect isolated spans of tokens, leaving most tokens unchanged. By introducing a span copying decoder they improve results upon the previous state-of-the-art. While also utilizing neural machine translation, DeepDebug\u00a0<cit.> leverage extensive self-supervised pretraining to improve upon the prior art. BugLab\u00a0<cit.> takes a step towards self-supervised bug detection and repair, co-training two neural models: a detector model that learns to detect and repair bugs in code, and a selector model that learns to create buggy code for the detector to use as training data. CODIT\u00a0<cit.> uses a tree-based model to encode source code changes, learning bug-fixing activities. Recoder\u00a0<cit.> generates edits in a syntax-guided manner and with a provider/decider architecture and placeholder generation. Lutellier \u00a0<cit.> employed ensemble learning with CNNs and NMT to generate patches with CoCoNuT. DLFix\u00a0<cit.> is a two-tier model with the first layer focusing on learning the context of bug fixes and the second layer trying to generate the bug-fixing patch. Recently CURE\u00a0<cit.> has reported state-of-the-art results on Defects4J and QuixBugs datasets, improving over NMT-based APR techniques with the use of a pre-trained programming language model, code-aware search, and sub-word tokenization. \n\nThese works are trained on generic, unclassified bugs mined from change histories of open source repositories, and do not utilize the bug type information during learning. Differently, our proposed approach take advantage of the close relationship with the Infer static analyzer tool and leverages the bug type information during the learning process to generate specific fixes tailored for that category of bugs. Additionally, none of these aforementioned papers attempted to capitalize on large language models, as well as the effectiveness of prompt augmentation methods in connections to LLMs, combined with task-oriented finetuning.  \n\nOur work is also aligned with a category of research that examines pretraining strategies and prompt augmentation. <cit.> permute ordering of the spans in the original prompt to train the model to infill. Specifically, by randomly replacing spans of code with a sentinel token and moving them to the end of the sequence they yield a unified approach for both program synthesis (via left-to-right generation) and editing (via infilling). <cit.> introduce a seminal LAMA dataset providing manually curated cloze templates to probe knowledge in language models. <cit.> investigate a template-based method for exploiting the few-shot learning potential of generative pre-trained language models to sequence labeling. Specifically, they define templates such as \u201c is a  entity\u201d, where  can be \u201cperson\u201d and \u201clocation\u201d, etc, and train a model using a filled template. <cit.> introduce a concept of chain of thought prompting, in which a task is broken down into a series of intermediate reasoning steps which significantly improves the ability of large language models to perform complex reasoning\n\nOur proposed approach, , performs prompt augmentation by incorporating similar fixes identified in a historical database of bugs, along with other information. The concept of leveraging similar fixes has also been explored in other approaches, such as SimFix\u00a0<cit.>, which extracts frequent abstract modifications from existing patches to form an abstract space for program repair. It then analyzes similar code snippets in the same program to extract concrete modifications, which forms a concrete space. The intersection of these two spaces is used to perform fine-grained code adaptation for patch generation. Differently from the AST-differencing approach proposed in SimFix, we rely on a dense retrieval model which allows for more flexibility in identifying similar code snippets with arbitrary length, not constrained by specific AST-subtrees. Furthermore, our approach enhances the prompt by providing additional information and cues to the LLM model to facilitate the repair process.\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nWe introduced : an end-to-end program repair framework based on Codex and a state-of-the-art static analyzer designed to fix critical security and performance bugs in Java and C#.  is based on a retrieval-based prompt augmentation technique and task-oriented finetuning that leverages bug-type annotations and extended source code context. We have also curated a , a novel, metadata-rich dataset of bugs extracted by executing the Infer and InferSharp static analyzers on the change histories of thousands of Java and C# repositories. Our experiments demonstrated that  outperforms strong LLM baselines, reaching a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java on the  dataset. \n\nWe deployed  internally at Microsoft as a GitHub action and as an Azure DevOps plugin operating as part of the continuous integration pipeline. This tool has significantly improved the software development workflow for our internal projects at Developer Division.\n\n\n\n\n\nACM-Reference-Format\n\n\n"}