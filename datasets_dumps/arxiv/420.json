{"entry_id": "http://arxiv.org/abs/2303.06737v1", "published": "20230312194156", "title": "Non-Trivial Query Sampling For Efficient Learning To Plan", "authors": ["Sagar Suhas Joshi", "Panagiotis Tsiotras"], "primary_category": "cs.RO", "categories": ["cs.RO"], "text": "\n\n[\n    Daodao Yang\n    \n===============\n\nempty\nempty\n\n\n\nIn recent years, learning-based approaches have revolutionized motion planning. \nThe data generation process for these methods involves caching a large number of high quality paths for different queries (start, goal pairs) in various environments. \nConventionally, a uniform random strategy is used for sampling these queries. \nHowever, this leads to inclusion of \"trivial paths\" in the dataset (e.g.,, straight line paths in case of length-optimal planning), which can be solved efficiently if the planner has access to a steering function.  \nThis work proposes a \"non-trivial\" query sampling procedure to add more complex paths in the dataset.\nNumerical experiments show that a higher success rate can be attained for neural planners trained on such a non-trivial dataset.\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nMotion planning, a core problem in artificial intelligence and robotics, is one of finding a collision free, low cost path connecting a start and goal state in a search-space. \nPopular discrete-space planners such as A*\u00a0<cit.> and LPA*\u00a0<cit.> conduct a prioritized search using heuristics and guarantee resolution-optimal paths. \nOn the other hand, single-query sampling-based motion planning (SBMP) algorithms, such as RRT\u00a0<cit.>, solve this problem by constructing a connectivity graph online using a set of probing samples.\n\nThe RRT algorithm is probabilistically complete, while it's asymptotically optimal variants such as RRT*\u00a0<cit.>, RRT^#\u00a0<cit.>, BIT*\u00a0<cit.>, FMT*\u00a0<cit.> converge to the optimal solution almost surely, as the number to samples tends to infinity.  \nHowever, these algorithms may suffer from a slow convergence rate, especially in higher dimensional settings. \n\nIn order to address this issue, several techniques that leverage heuristics and collision information have been suggested to improve the performance of these planners. \nThese include <cit.>, <cit.>, <cit.>, <cit.>, <cit.> among others.\nThe recently proposed Informed Set\u00a0<cit.>, <cit.>, <cit.> and  Relevant Region\u00a0<cit.>, <cit.> family of algorithms utilize current solution information and heuristics to focus the search onto a subset of the search-space, while still maintaining the theoretical guarantees of asymptotic optimality.  \n\nAlthough the above methods can improve the performance of motion planning algorithms, they require handcrafted heuristics for efficacy. \nAlso, these methods do not leverage prior experience or data gathered from expert demonstrations. \nDeep learning based approaches for motion planning address these two limitations by generating a dataset of high quality paths in various environments. \nIn the offline phase, this dataset is used to train a deep neural network (DNN) model to predict quantities of interest, such as cost-to-go\u00a0<cit.>, next point along the optimal path\u00a0<cit.>, or a sampling distribution\u00a0<cit.>.\nThe DNN model can then be used online to focus search and dramatically increase the efficiency of planning algorithms. \n\n\n\nThe data generation process for DNN-based motion planning methods involves sampling and solving a set of queries (start, goal pairs) in a given environment.\nFor many applications, the map/environment in which the robot needs to operate may be fixed or given apriori.\nHowever, the query sampling distribution can be modified in order to extract more informative paths beneficial to the learning process.\nConventionally, uniform random sampling is used to generate the start and goal states. \nMany queries in this uniformly sampled dataset can be solved by greedily connecting the start and goal state using a steering function (if available).\nSuch a steering function provides the optimal path between any two states after relaxing the collision constraint.\nThis work proposes adding more \"non-trivial\" queries to the dataset, which cannot be solved by a simple greedy connection.\nThe efficiency of the neural planners can be boosted by training deep models on this dataset comprising of relatively more complex paths. \nThis is demonstrated by creating datasets with different degrees of non-triviality and benchmarking the performance of the neural planner on various robotic planning tasks.\n\n\n\n\n\u00a7 RELATED WORK\n\nExciting progress has been made at the intersection of learning and motion planning in recent years.\nTo improve the performance of discrete space planners, methods that leverage  reinforcement learning <cit.> and imitation learning <cit.>, <cit.> have been proposed.  \nNeural A* <cit.> presents a data-driven approach that reformulates conventional A* as a differentiable module.\nHuh et al <cit.> present a higher-order function network that can predict cost-to-go values to guide the A* search. \nFor sampling-based planning, techniques such as <cit.> and <cit.> learn a local sampling strategy for global planning.\nZhang et al <cit.> present a deep learning based rejection sampling scheme for SBMP.\nIchter et al\u00a0<cit.> train a conditional variational autoencoder (CVAE) model to learn sampling distributions and generate samples along the solution paths. \nNEXT\u00a0<cit.> learns a local sampling policy and cost-to-go function using \"meta self-improving learning\".\nKuo et al\u00a0<cit.> input the planner state and local environment information into a deep sequential model to guide search during planning.\nApproaches such as <cit.>, <cit.> focus on identifying critical or bottleneck states in an environment to generate a sparse graph while planning. \nWhile the above techniques may differ in terms of their model architectures or outputs, they do not tackle the problem of improving the data-generation process via query sampling for increasing the efficacy of neural planners.\n\n\n.5em\n\n.5em\n\n\nIn <cit.>, Huh et al extend their previous work to present a query sampling technique for non-holonomic robots.\nHowever, this technique is purely based on the dynamics of the robot, does not utilize obstacle information and is only applicable for car-like robots.\nThe recently proposed OracleNet <cit.> and Motion Planning Networks (MPNet) algorithm <cit.> learn a deep model to recursively predict the next state along the solution path, given a query.\nThe authors in <cit.> use \"Active Continual Learning (ACL)\" to improve the data-efficiency of the training process. \nSimilar to the DAGGER algorithm\u00a0<cit.> the ACL process involves training a MPNet deep model on a set of expert demonstrations for a N_c >0 number of initial iterations.\nThe ACL algorithm then finds the cases where the MPNet planner fails and invokes the expert planner only to solve them.\nThe solutions generated by the expert planner are stored in a replay buffer to be used during training.\nACL also leverages the \"Gradient Episodic Memory (GEM)\" technique <cit.> to alleviate the problem of catastrophic forgetting during the learning process.\nWhile more data-efficient, the ACL process can be tricky to implement and computationally more expensive, as it involves interleaving the training process with running the neural planner multiple times.  \nThe performance of ACL models can also be worse than that of batch-offline models in many cases <cit.>.\nIn contrast, this work proposes a modified query sampling procedure for data-generation and trains all models in a batch-offline manner.\n\n\n\n\n\n\n\n\n\u00a7 PROBLEM DEFINITION\n\n\n\n\n \u00a7.\u00a7 Path Planning Problem\n\nLet \ud835\udcb3\u2282\u211d^d denote the search-space for the planning problem with dimension d \u2265 2.\nLet \ud835\udcb3_obs\u2282\ud835\udcb3 denote the obstacle space and \ud835\udcb3_free= \ud835\udcb3\u2216\ud835\udcb3_obs denote the free space.\nLet c_\u03c0(x_s,x_g) denote the cost of moving from a point x_s\u2208\ud835\udcb3 to x_g\u2208\ud835\udcb3 along a path \u03c0.\nThis path \u03c0 can be represented as an ordered list of length L\u22652, \u03c0={\u03c0_1,\u03c0_2\u2026\u03c0_L}. \n\nLet \u03c0_end denote the last point on the path \u03c0.\nThen, the optimal path planning problem is one of finding the minimum cost, feasible path \u03c0^* connecting a start x_s and goal x_g state.\n\n    \u03c0^*=min_\u03c0\u2208\u03a0    c_\u03c0(x_s,x_g), \n    subject to:    \u03c0_1=x_s, \u03c0_end=x_g\n        \u03c0_i\u2208\ud835\udcb3_free, \u00a0\u00a0\u00a0 i = 1,2 \u2026 L.\n\nClassical planners discretize \ud835\udcb3_free or build a connectivity graph and then perform a search over this graph to solve the above planning problem (<ref>).\n\n\n\n \u00a7.\u00a7 Supervised Learning for Planning\n\nLearning-based methods use the data gathered from successful plans to train models in the offline phase. In the online phase, this learned model can be used to solve (<ref>) or assist the classical planners.\nThe data generation process involves creating an environment and sampling a set of K_train>0 queries or (start, goal) pairs in \ud835\udcb3_free. \nLet \ud835\udcac=\ud835\udcb3_free\u00d7\ud835\udcb3_free denote the \"query-space\".\nA classical planner is then used to solve the path planning problem (<ref>) for each of the K_train queries and obtain a good quality solution.\nA quantity of interest to be learned as output (such as cost-to-go, next state on the optimal path etc) is extracted from these solution paths. \nThe objective of the training process is to learn a function f_\u03b8 (usually a deep neural network), on the dataset \ud835\udc9f by minimizing an empirical loss with respect to weight parameters \u03b8\n\n\n\n \u00a7.\u00a7 Neural Planner\n\nThe MPNet procedure involves learning a planning network \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd, that predicts the next state on the optimal path, given a current state, a goal state and environment information as the input.  \nA typical neural planning algorithm, in line with the one described in <cit.>, is illustrated in Algorithm\u00a0<ref>.\nGiven a query x_s,x_g, the path \u03c0 to be returned is initialized with the start-state. \nAt each iteration, the neural planner attempts a greedy connection to the goal using the \ud835\uddcc\ud835\uddcd\ud835\uddbe\ud835\uddbe\ud835\uddcb\ud835\uddb3\ud835\uddc8 function. \nFor length-optimal or geometric planning case, the \ud835\uddcc\ud835\uddcd\ud835\uddbe\ud835\uddbe\ud835\uddcb\ud835\uddb3\ud835\uddc8 connects any two points using a straight line.\nFor car-like robots, this steering function can use the Dubins curves for dynamically feasible connections <cit.>.\nTo probe the feasibility of this greedy connection, the \ud835\uddcc\ud835\uddcd\ud835\uddbe\ud835\uddbe\ud835\uddcb\ud835\uddb3\ud835\uddc8 procedure discretizes the path and collision-checks the points on it.\nIf the greedy connection \u03c0_end to x_g is valid, the goal-state is appended to the path and the planning loop terminates. \nElse, the learned planning network \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd, is used to predict the next state on the optimal path. \nIf the path \u03c0 is infeasible, a neural replanning procedure is performed on this coarse path in an attempt to repair it. \nPlease see <cit.> for more details about these procedures.\n\n\n\n\u00a7 NON-TRIVIAL QUERY SAMPLING\n\n\n.5em\n\n.5em\n\n\n\n\n \u00a7.\u00a7 Non-trivial Queries\n\nConventionally, uniform random sampling is used in the data generation process to obtain a query x_s,x_g\u2208\ud835\udcac.\n\n\nHowever, this may result in the inclusion of \"trivial\" queries in the dataset, for which \ud835\uddcc\ud835\uddcd\ud835\uddbe\ud835\uddbe\ud835\uddcb\ud835\uddb3\ud835\uddc8(x_s,x_g)=\ud835\uddb3\ud835\uddcb\ud835\uddce\ud835\uddbe. \nIn case of such trivial queries, the neural planning Algorithm <ref> terminates in the first iteration after processing lines 4-6.\nThus, a key observation is that \ud835\uddcc\ud835\uddcd\ud835\uddbe\ud835\uddbe\ud835\uddcb\ud835\uddb3\ud835\uddc8 procedure in Algorithm <ref>, line 4 performs an implicit classification of queries, so that only \"non-trivial\" queries are passed over to the \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd.\nConcretely, the set of non-trivial queries can be defined as\n\n    \ud835\udcac_nt={x_s,x_g\u2208\ud835\udcac |  \ud835\uddcc\ud835\uddcd\ud835\uddbe\ud835\uddbe\ud835\uddcb\ud835\uddb3\ud835\uddc8(x_s,x_g)=\ud835\udda5\ud835\uddba\ud835\uddc5\ud835\uddcc\ud835\uddbe}.\n\nThis motivates the proposed data generation Algorithm <ref>, which aims to increase the number of non-trivial data samples in \ud835\udc9f.\nAfter an environment \ud835\udcb3, \ud835\udcb3_obs is created, data is generated by solving a total of K_train queries. \nWith probability p_nt, the proposed \ud835\uddc7\ud835\uddc8\ud835\uddc7\ud835\uddb3\ud835\uddcb\ud835\uddc2\ud835\uddcf\ud835\uddc2\ud835\uddba\ud835\uddc5\ud835\uddb0\ud835\uddce\ud835\uddbe\ud835\uddcb\ud835\uddd2\ud835\uddb2\ud835\uddba\ud835\uddc6\ud835\uddc9\ud835\uddc5\ud835\uddc2\ud835\uddc7\ud835\uddc0 procedure is used to obtain x_s,x_g\u2208\ud835\udcac_nt. \nElse, conventional \ud835\uddce\ud835\uddc7\ud835\uddc2\ud835\uddbf\ud835\uddc8\ud835\uddcb\ud835\uddc6\ud835\uddb2\ud835\uddba\ud835\uddc6\ud835\uddc9\ud835\uddc5\ud835\uddc2\ud835\uddc7\ud835\uddc0 returns a query in \ud835\udcac.\nA classical planner such as A* or BIT* then solves this query and outputs a good quality solution path \u03c0.\nSamples from this path \u03c0 are appended to the dataset \ud835\udc9f with the proposed data inclusion procedure \ud835\uddc2\ud835\uddc7\ud835\uddbc\ud835\uddc5\ud835\uddce\ud835\uddbd\ud835\uddbe\ud835\udda3\ud835\uddba\ud835\uddcd\ud835\uddba.\n\n\n\n \u00a7.\u00a7 Non-trivial Query Sampling\n\nA rejection sampling algorithm to generate new queries in \ud835\udcac_nt is given in Algorithm <ref>.\nFor N_nt number of attempts, uniform sampling is used to first generate a valid query x_s,x_g\u2208\ud835\udcac. \nThe \ud835\uddcc\ud835\uddcd\ud835\uddbe\ud835\uddbe\ud835\uddcb\ud835\uddb3\ud835\uddc8 module then validates the connection between start and goal state. \nIf found invalid, the corresponding non-trivial query is returned.\nThus, this procedure intends to filter out trivial paths while maintaining the exploratory/coverage property of uniform sampling. \nPlease see Fig. <ref> and Fig. <ref> for a visualization of queries generated using the proposed non-trivial sampling procedure.\n\n\n\n \u00a7.\u00a7 Data Inclusion\n\nThe data inclusion Algorithm <ref> iterates over the segments of path \u03c0 and logs the current state, goal state (\u03c0_i,\u03c0_end) as the input and the next state (\u03c0_i+1) as the output/label.\nHowever, if the flag \ud835\uddc9\ud835\uddcb\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda3\ud835\uddba\ud835\uddcd\ud835\uddba=\ud835\uddb3\ud835\uddcb\ud835\uddce\ud835\uddbe, the algorithm skips including the data-sample {(\u03c0_i,\u03c0_end),\u03c0_i+1} if \u03c0_i,\u03c0_end\u2209\ud835\udcac_nt. \nThus, \ud835\uddc9\ud835\uddcb\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda3\ud835\uddba\ud835\uddcd\ud835\uddba=\ud835\uddb3\ud835\uddcb\ud835\uddce\ud835\uddbe ensures that only the non-trival segments of \u03c0 are incorporated in \ud835\udc9f. \nPlease see Fig. <ref> for an illustration of this step.\n\nDepending on the topology of \ud835\udcb3_obs, the neural planner may find it relatively harder to predict feasible paths in certain environments. \nThe notion of non-trivial queries can be used to define a metric that captures this level of difficulty. \nConsider a \"non-triviality ratio\", which can be defined as,\n\n    \u03b3_nt=# Non-trivial queries/# Uniformly sampled queries .\n\nThus, \u03b3_nt is the ratio of number of non-trivial queries found in a (large enough) set of uniformly sampled queries. \nThis ratio will be high for complex, cluttered and narrow-passage type environments and low for relatively simpler, single-obstacle type environments.\n\n\n.5em\n\n.5em\n\n\n.5em\n\n.5em\n\n\n\n\n\u00a7 NUMERICAL EXPERIMENTS\n\nIn order to benchmark the proposed data generation algorithm, the following procedure was implemented for each planning environment.\nFirst, four datasets with different parameter settings were created. \nThese were as follows: \ud835\udc9f_0 \u00a0( p_nt=0, \u00a0\ud835\uddc9\ud835\uddcb\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda3\ud835\uddba\ud835\uddcd\ud835\uddba=\ud835\udda5\ud835\uddba\ud835\uddc5\ud835\uddcc\ud835\uddbe), \ud835\udc9f_1 \u00a0( p_nt=0.5,\u00a0\ud835\uddc9\ud835\uddcb\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda3\ud835\uddba\ud835\uddcd\ud835\uddba=\ud835\udda5\ud835\uddba\ud835\uddc5\ud835\uddcc\ud835\uddbe), \ud835\udc9f_2 \u00a0( p_nt=1.0,\u00a0\ud835\uddc9\ud835\uddcb\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda3\ud835\uddba\ud835\uddcd\ud835\uddba=\ud835\udda5\ud835\uddba\ud835\uddc5\ud835\uddcc\ud835\uddbe), \ud835\udc9f_3 \u00a0( p_nt=1.0,\u00a0\ud835\uddc9\ud835\uddcb\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda3\ud835\uddba\ud835\uddcd\ud835\uddba=\ud835\uddb3\ud835\uddcb\ud835\uddce\ud835\uddbe).\nThus, \ud835\udc9f_0  represents the dataset generated using the conventional uniform query sampling, whereas \ud835\udc9f_3 is created using the proposed non-trivial query sampling and data pruning procedure. \nFour deep models, \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_0,\ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_1,\ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_2,\ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_3 were then trained on their respective datasets.\nThe neural network shape, size and the training parameters were held constant while learning all four models.\nPerformance of the neural planner <ref> using these four models was evaluated on 1) K_test number of new uniform queries and 2) K_test number of new non-trivial queries.\nTwo performance metrics, namely, success ratio and cost ratio were considered. \nSuccess ratio gives the number of times (out of K_test in total) the neural planner was successful in finding a feasible (collision-free) solution.\nCost ratio denotes the ratio of the neural planner's solution cost to that of classical planner, averaged over K_test trials.  \nModel training and evaluation was performed using the Python PyTorch API on a 64 bit, 16 GB RAM laptop with Intel i7 processor and a NVIDIA GeForce RTX 2060 GPU.\nA description of robotic planning tasks along with a discussion of results is given below.\n\n\n\nPoint Robot:\nFour different 20 \u00d7 20 environments, illustrated in Fig. <ref>, were considered for the case of point robot planning.\nFour datasets {\ud835\udc9f_i }^3_i=0, as described above, were generated for each environment. \nA total of K_train=3000 number of queries were sampled for each dataset.\nAn A* planner, followed by post-processing/smoothening, was used to solve these queries and obtain length-optimal paths.\nA small padding of 0.8 units around the obstacles was propagated during the data generation step, and was relaxed during the final performance evaluation step. \nThis was found to greatly boost the success ratio of the neural planner, while making slight compromise in the cost ratio metric. \nThe performance metrics were logged by solving K_test=500 number of unseen uniform and non-trivial queries with the four learned \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd models.\n\n\nRigid Body Planning:\nFig. <ref> shows the instance of planning for a rigid robot in four 10 \u00d7 10 environments. \nA total of K_train=5000 queries were considered to create each of the four datasets {\ud835\udc9f_i }^3_i=0.\nAll the queries were solved in the SE(2) space using OMPL's <cit.> implementation of the BIT* planner.\nAn obstacle-padding of 0.4 units was propagated during the data-generation phase. The learned \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd models predicted a three dimensional [x,y,\u03b8] vector representing the robot's pose. \nThese models were evaluated on K_test=500 unseen uniform and non-trivial queries. \nThe BIT* planner was allowed a run-time of 3 seconds during the data-generation phase and 2 seconds during the evaluation phase.\n\n\n\nn-link Manipulator Planning:\nTo observe performance of the neural planner in higher dimensions, a planning problem for 2,4 and 6-link manipulator robot was considered. Please see Fig. <ref>.\nThe joint angles were constrained to lie between -\u03c0 and \u03c0.\nFour datasets {\ud835\udc9f_i }^3_i=0 were created for each of the 2,4 and 6-link case by considering a total of 3000,4000 and 5000 queries respectively. \nThese queries were solved using OMPL's BIT* planner with an padding of 0.8 units around the workspace obstacles.\nThe final performance evaluation was done by solving K_test=500 new queries with the neural planner.\nThe BIT* planner was run for 2,4,6 seconds during the data-generation stage and 1,2,4 seconds during the evaluation stage for the 2,4 and 6-link planning respectively.\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral trends seen from the results in Table <ref>, <ref>, <ref> are as follows. \nIn most cases, \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_2 and \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_3 outperform \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_0 in terms of success ratio, where as the performance of \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_1 is more sporadic. \nThe cost ratio is relatively lower for the rigid body and 6-link case compared to others, as the BIT* planner may not find a good quality solution in the given planning time for these challenging cases. \nThe success ratio for all \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd models is naturally higher over uniform test queries rather than non-trivial test queries.\nThe success ratio also generally has an inverse relation with \u03b3_nt, as seen strongly in the case of rigid body and n-link manipulator planning.\nFor the case of point robot planning, all models perform well with a success rate of over 90 % (see Table <ref>). \nHowever, slight performance gains due to the proposed method can be seen for Environments 0,2,3. \nThese gains are much more noticeable for the rigid body planning case (see Table <ref>). \nThe \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_3 model has the highest success ratio in all cases except one (Environment 3, Non-trivial Query), where its performance is comparable to \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_1. \nThe gradation in performance due to dimensionality and \u03b3_nt can be seen clearly in the n-link planning case (see Table <ref>). \nThe success ratio over uniform queries is in the range of 0.9,0.8 and 0.7 for the case of 2,4 and 6-link planning case respectively.\nFor the relatively simpler 2-link planning case with \u03b3_nt=0.225, only small gains in the success ratio over non-trivial queries can be seen. \nHowever, the improvement in performance is much more evident for the higher dimensional 4 and 6-link cases.  \nThe \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_3 model shows about a 25% increase in the success ratio over \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_0 for the 6-link (non-trivial queries) case.\n\nThe neural planning Algorithm <ref> and the corresponding results discussed above assume the availability of a steering function. \nWhile this is readily available for cases such as geometric or non-holonomic (car-like) planning <cit.>, it may not be computationally tractable for others. \nTo analyze the performance of \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd models without the \ud835\uddcc\ud835\uddcd\ud835\uddbe\ud835\uddbe\ud835\uddcb\ud835\uddb3\ud835\uddc8 function, simulations were performed by only executing the lines 8 and 9 of the neural planner <ref> for maximum N_plan iterations. \nInstead of lines 4-6 in Algorithm <ref>, the following termination condition was implemented, \u03c0_end - x_g_2 \u2264\u03b4, with a small \u03b4>0.\nAs illustrated in Fig. <ref>, the \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_3 model, which has no trivial sample in its training dataset, naturally cannot solve a trivial query. \nNumerical results for the rigid body planning without the \ud835\uddcc\ud835\uddcd\ud835\uddbe\ud835\uddbe\ud835\uddcb\ud835\uddb3\ud835\uddc8 function and \u03b4=1.0 are tabulated in Table <ref>.\nThe success ratio of all \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd models is adversely affected in this case. \nThe \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_0 model performs best on uniform queries in all environments, whereas the performance of \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_3 is the worst. \nHowever, \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_1 or \ud835\uddaf\ud835\uddad\ud835\uddbe\ud835\uddcd_2 show better performance over non-trivial queries in some cases. \nThus, without a steering function, a uniformly sampled training dataset might be the best choice if the test queries are uniformly distributed too. \nHowever, a model trained over a dataset with an appropriate value of p_nt may perform better over non-trivial test queries. \nThis makes a case for an ensemble model.\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nMany of the previous techniques in the literature have focused on exploring different deep architectures for planning, while using a uniformly sampled dataset for training.  \nThis work, on the other hand, investigates the problem of improving the data-generation process while holding the model architecture and planning algorithm constant.\nThe proposed query sampling and data pruning procedures add more complicated paths in the dataset. \nNumerical experiments show that the success rate of the neural planner can be boosted using the deep models trained on such non-trivial datasets.\n\nThis work presents many opportunities for future research.\nAn ensemble model can be constructed by combining predictions from different models trained on datasets with varying degrees of non-triviality. \n\nInstead of a Boolean \ud835\uddc9\ud835\uddcb\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda3\ud835\uddba\ud835\uddcd\ud835\uddba flag, calling the pruning procedure with a probability of \u03b3_nt can be explored. \nThis can prevent excessive pruning and result in a drastic reduction in the size of the dataset for relatively less cluttered environments. \n\nAcknowledgements: Authors would like to thank Prof. Le Song, Binghong Chen and Ethan Wang for insightful discussions on this topic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIEEEtran\n\t\n\n"}