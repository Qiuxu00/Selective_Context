{"entry_id": "http://arxiv.org/abs/2303.06852v1", "published": "20230313044715", "title": "One-Shot Segmentation of Novel White Matter Tracts via Extensive Data Augmentation", "authors": ["Wan Liu", "Qi Lu", "ZhiZheng Zhuo", "Yaou Liu", "Chuyang Ye"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI"], "text": "\n\n\nOne-Shot Novel WM Tract Segmentation via Data Augmentation\n\n\n\n\n\n\n\nLiu et al.\n\n^1School of Integrated Circuits and Electronics, Beijing Institute of Technology, Beijing, China\n\nchuyang.ye@bit.edu.cn\n\n^2Department of Radiology, Beijing Tiantan Hospital, Capital Medical University, Beijing, China\n\n\nOne-Shot Segmentation of Novel White Matter Tracts via Extensive Data Augmentation\n    Wan Liu^1, Qi Lu^1, Zhizheng Zhuo^2, Yaou Liu^2, Chuyang Ye^1()\n    Received: date / Accepted: date\n==================================================================================\n\n\n\n\n\n\n\nDeep learning based methods have achieved state-of-the-art performance for automated white matter\u00a0(WM) tract segmentation.\nIn these methods, the segmentation model needs to be trained with a large number of manually annotated scans, which can be accumulated throughout time.\nWhen novel WM tracts\u2014i.e., tracts not included in the existing annotated WM tracts\u2014are to be segmented, additional annotations of these novel WM tracts need to be collected.\nSince tract annotation is time-consuming and costly, it is desirable to make only a few annotations of novel WM tracts for training the segmentation model, and previous work has addressed this problem by transferring the knowledge learned for segmenting existing WM tracts to the segmentation of novel WM tracts. \nHowever, accurate segmentation of novel WM tracts can still be challenging in the one-shot setting, where only one scan is annotated for the novel WM tracts. \nIn this work, we explore the problem of one-shot segmentation of novel WM tracts. \n\n\nSince in the one-shot setting the annotated training data is extremely scarce, based on the existing knowledge transfer framework, we propose to further perform extensive data augmentation for the single annotated scan, where synthetic annotated training data is produced. We have designed several different strategies that mask out regions in the single annotated scan for data augmentation.\nTo avoid learning from potentially conflicting information in the synthetic training data produced by different data augmentation strategies, we choose to perform each strategy separately for network training and obtain multiple segmentation models.\n\nThen, the segmentation results given by these models are ensembled for the final segmentation of novel WM tracts.\n\nOur method was evaluated on public and in-house datasets. The experimental results show that our method improves the accuracy of one-shot segmentation of novel WM tracts.\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nThe segmentation of white matter\u00a0(WM) tracts based on diffusion magnetic resonance imaging\u00a0(dMRI) provides an important tool for the understanding of brain wiring\u00a0<cit.>.\nIt allows identification of different WM pathways and has benefited various brain studies\u00a0<cit.>.\nSince manual delineations of WM tracts can be time-consuming and subjective, automated approaches to WM tract segmentation are developed\u00a0<cit.>, and methods based on convolutional neural networks\u00a0(CNNs) have achieved state-of-the-art performance\u00a0<cit.>.\nThe success of CNN-based WM tract segmentation relies on a large number of annotated scans that are accumulated throughout time for network training.\nHowever, in a new study, novel WM tracts that are not included in the existing annotated WM tracts may be of interest\u00a0<cit.> and need to be segmented. \n\n\nRepeating the annotation for the novel WM tracts on a large number of scans can be very laborious and prohibitive, and accurate segmentation of novel WM tracts becomes challenging when only a few annotations are made for them.\n\n\nPrevious work has addressed this few-shot segmentation problem with a transfer learning strategy, where the knowledge learned for segmenting existing WM tracts with abundant annotated data is transferred to the segmentation of novel WM tracts\u00a0<cit.>.\nIn\u00a0<cit.>, a CNN-based segmentation model pretrained for segmenting existing WM tracts is used to initialize the target network for segmenting novel WM tracts, so that even with only a few annotations of novel WM tracts the network can learn adequately for the segmentation during fine-tuning.\nIn addition, instead of using classic fine-tuning that discards the pretrained task-specific weights, an improved fine-tuning strategy is developed in\u00a0<cit.> for more effective knowledge transfer, where all weights in the pretrained model can be exploited for initializing the target segmentation model.\nDespite the promising results achieved in\u00a0<cit.> for few-shot segmentation of novel WM tracts, when the number of annotated scans for novel WM tracts decreases to one, the segmentation is still challenging. Since fewer annotations are preferred to reduce the annotation time and cost, the development of accurate approaches to one-shot segmentation of novel WM tracts needs further investigation.\n\n\n\nIn this work, we seek to improve one-shot segmentation of novel WM tracts. \nWe focus on volumetric WM tract segmentation\u00a0<cit.>, where voxels are directly labeled without necessarily performing tractography\u00a0<cit.>.\nSince in the one-shot setting annotated training data is extremely scarce, based on the pretraining and fine-tuning framework developed in\u00a0<cit.>, we propose to address the one-shot segmentation problem with extensive data augmentation. \nExisting data augmentation strategies can be categorized into those based on basic image transformation\u00a0<cit.>, generative models<cit.>, image mixing\u00a0<cit.>, and image masking\u00a0<cit.>. \nBasic image transformation is already applied by default in CNN-based WM tract segmentation\u00a0<cit.>, yet it is insufficient for the one-shot segmentation due to the limited diversity of the augmented data. \nThe training of generative models usually requires a large amount of annotated data, or at least a large amount of unannotated data\u00a0<cit.>, which is not guaranteed in the one-shot setting. \nImage mixing requires at least two annotated images\u00a0<cit.>, which is also infeasible in the one-shot setting.\nTherefore, we develop several strategies based on image masking for data augmentation, where the single annotated image is manipulated by masking out regions in different ways to synthetize additional training data.\n\nThe masking is performed randomly either with uniform distributions or according to the spatial location of novel WM tracts, and the annotation of the synthetic image can also be determined in different ways.\nThe augmented data is used to fine-tune the model for segmenting novel WM tracts.\nTo avoid learning from potentially conflicting information in the synthetic data produced by different strategies, we choose to perform each data augmentation strategy separately to train multiple segmentation models, and the outputs of these models are ensembled for the final segmentation.\nWe evaluated the proposed method on two brain dMRI datasets. The results show that our method improves the accuracy of one-shot segmentation of novel WM tracts. The code of our method is available at <https://github.com/liuwan0208/One-Shot-Extensive-Data-Augmentation>. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 METHODS\n\n\n\n\n\n\n \u00a7.\u00a7 Background: Knowledge Transfer for Segmenting Novel WM Tracts\n\n\n\n\n\nSuppose we are given a CNN-based model \u2133_e that segments M existing WM tracts, for which a large number of annotations have been accumulated for training.\nWe are interested in training a CNN-based model \u2133_n for segmenting N novel WM tracts, for which only one scan is annotated due to annotation cost.\n\nExisting work\u00a0<cit.> has attempted to address this problem with a transfer learning strategy based on the pretraining and fine-tuning framework, where \u2133_e and \u2133_n share the same network structure for feature extraction, and their last task-specific layers are different.\nIn classic fine-tuning, the network weights of the learned feature extraction layers of \u2133_e are used to initialize the feature extraction layers of \u2133_n, and the task-specific layer of \u2133_n is randomly initialized.\nThen, all weights of \u2133_n are fine-tuned with the single scan annotated for novel WM tracts.\nHowever, the classic fine-tuning strategy discards the information in the task-specific layer of \u2133_e. As different WM tracts cross or overlap, existing and novel WM tracts can be correlated, and the task-specific layer of \u2133_e for segmenting existing WM tracts may also bear relevant information for segmenting novel WM tracts.\nTherefore, to exploit all the knowledge learned in \u2133_e, in\u00a0<cit.> an improved fine-tuning strategy is developed, which, after derivation, can be conveniently achieved with a warmup stage. Specifically, the feature extraction layers of \u2133_n are first initialized with those of \u2133_e.\nThen, in the warmup stage, the feature extraction layers of \u2133_n are fixed and only the last task-specific layer of \u2133_n (randomly initialized) is learned with the single annotated image. Finally, all weights of \u2133_n are jointly fine-tuned with the single annotated image.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Extensive Data Augmentation for One-Shot Segmentation of Novel WM Tracts\n\n\n\nAlthough the transfer learning approach in\u00a0<cit.> has improved the few-shot segmentation of novel WM tracts, when the training data for novel WM tracts is extremely scarce with only one annotated image, the segmentation is still challenging. \n\nTherefore, we continue to explore the problem of one-shot segmentation of novel WM tracts.\nBased on the pretraining and fine-tuning framework developed in\u00a0<cit.>, we propose to more effectively exploit the information in the single annotated image with extensive data augmentation for network training.\n\n\nSuppose the annotated image is \ud835\udc17 and its annotation is \ud835\udc18 (0 for background and 1 for foreground); then we obtain a set of synthetic annotated training images \ud835\udc17 and the synthetic annotations \ud835\udc18 by transforming \ud835\udc17 and \ud835\udc18. \nWe develop several data augmentation strategies for the purpose, which are described below.\n\n\n\n  \u00a7.\u00a7.\u00a7 Random Cutout\n\nFirst, motivated by the Cutout data augmentation method\u00a0<cit.> that has been successfully applied to image classification problems, we propose to obtain the synthetic image \ud835\udc17 by transforming \ud835\udc17 with region masking:\n\n    \ud835\udc17   =   \ud835\udc17\u2299 (1-\ud835\udc0c),\n\nwhere \u2299 represents voxelwise multiplication and \ud835\udc0c is a binary mask representing the region that is masked out.\n\n\ud835\udc0c is designed as a 3D box randomly selected with uniform distributions. Mathematically, suppose the ranges of \ud835\udc0c in the x-, y-, and z-direction are (r_x, r_x + w_x), (r_y, r_y + w_y), and (r_z, r_z + w_z), respectively; then we follow <cit.> and select the box as\n\n\n    r_x\u223cU(0,R_x),  r_y\u223cU(0,R_y),  r_z\u223cU(0,R_z),\n          w_x=R_x\u221a(1-\u03bb),   w_y=R_y\u221a(1-\u03bb),   w_z=R_z\u221a(1-\u03bb),  \u03bb\u223cBeta(1, 1),\n\nwhere U(\u00b7, \u00b7) represents the uniform distribution, R_x, R_y, and R_z are the image dimensions in the x-, y-, and z-direction, respectively, and \u03bb is sampled from the beta distribution Beta(1, 1) to control the size of the masked region.\n\nThe voxelwise annotation \ud835\udc18 for \ud835\udc17 also needs to be determined. Intuitively, we can obtain \ud835\udc18 with the same masking operation for \ud835\udc17:\n\n    \ud835\udc18   =   \ud835\udc18\u2299 (1-\ud835\udc0c).\n\nThe strategy that obtains synthetic training data using Eqs. (<ref>) and (<ref>) with the sampling in Eqs.\u00a0(<ref>) and (<ref>) is referred to as Random Cutout One\u00a0(RC1).\nBesides RC1, it is also possible to keep the original annotation \ud835\udc18 for the masked image \ud835\udc17, so that the network learns to restore the segmentation result in the masked region. In this case, the synthetic annotation \ud835\udc18 is simply determined as\n\n    \ud835\udc18   =   \ud835\udc18.\n\nThe use of Eqs. (<ref>) and (<ref>) for obtaining synthetic training data with the sampling in Eqs. (<ref>) and (<ref>) is referred to as Random Cutout Two\u00a0(RC2).\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Tract Cutout\n Since we perform data augmentation for segmenting novel WM tracts, in addition to RC1 and RC2, it is possible to obtain \ud835\udc0c with a focus on the novel WM tracts.\nTo this end, we design the computation of \ud835\udc0c as \n\n    \ud835\udc0c=\u23081/N\u2211_j=1^Na^j\ud835\udc18^j\u2309,\n\nwhere \ud835\udc18^j denotes the annotation of the j-th novel WM tract in \ud835\udc18,  a^j is sampled from the Bernoulli distribution Bernoulli(0.5) to determine whether \ud835\udc18^j contributes to the computation of \ud835\udc0c, and \u2308\u00b7\u2309 represents the ceiling operation. \n\nIn this way, \ud835\udc0c is the union of the regions of a randomly selected subset of the novel WM tracts, and thus the masked region depends on the novel WM tracts.\n\nWith the masking strategy in Eq.\u00a0(<ref>),  we can still use Eq.\u00a0(<ref>) or (<ref>) to determine the synthetic annotation.\nWhen Eq.\u00a0(<ref>) or (<ref>) is used, the data augmentation strategy is named Tract Cutout One\u00a0(TC1) or Tract Cutout Two\u00a0(TC2), respectively.\nNo duplicate synthetic images are allowed in TC1 or TC2.\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Network Training with Augmented Data\n\n\n\n\nBy repeating the region masking in each data augmentation strategy, a set of synthetic annotated images can be produced.\n\n\nSince the synthetic images can appear unrealistic, they are used only in the warmup stage of the improved fine-tuning framework in\u00a0<cit.>, where the last layer of \u2133_n is learned.\n\nIn the final fine-tuning step that updates all network weights in \u2133_n, only the real annotated training image is used.\nIn addition, to avoid that the network learns from potentially conflicting information in the synthetic data produced by different strategies, we choose to perform each data augmentation strategy separately and obtain four different networks for segmenting novel WM tracts.\n\nAt test time, the predictions of the four networks are ensembled with majority voting[The tract label is set to one when the votes are tied.] to obtain the final segmentation.\n\n\n\n\n \u00a7.\u00a7 Implementation Details\n\n\n\nWe use the state-of-the-art TractSeg architecture\u00a0<cit.> for volumetric WM tract segmentation as our backbone segmentation network, which takes fiber orientation maps as input.  \nLike\u00a0<cit.>, the fiber orientation maps are computed with constrained spherical deconvolution\u00a0(CSD)\u00a0<cit.> or multi-shell multi-tissue CSD\u00a0(MSMT-CSD)\u00a0<cit.> for single-shell or multi-shell dMRI data, respectively, and three fiber orientations are allowed in the network input\u00a0<cit.>.\n\n\n\nWe follow\u00a0<cit.> and perform 2D WM tract segmentation for each image view separately, and the results are fused to obtain the final 3D WM tract segmentation.\n\nThe proposed data augmentation is performed offline.\nSince given N novel WM tracts TC1 or TC2 can produce at most 2^N-1 different images, we set the number of synthetic scans produced by each data augmentation strategy to min(2^N-1,100). \nNote that traditional data augmentation, such as elastic deformation, scaling, intensity perturbation, etc., is applied online in TractSeg to training images. Thus, these operations are also applied to the synthetic training data online. \nThe training configurations are set according to TractSeg, where Adamax\u00a0<cit.> is used to minimize the binary cross entropy loss with a batch size of 56, an initial learning rate of 0.001, and 300 epochs\u00a0<cit.>. \nThe model corresponding to the epoch with the best segmentation accuracy on a validation set is selected.\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n \u00a7.\u00a7 Data Description and Experimental Settings\n\n\nFor evaluation, experiments were performed on the publicly available Human Connectome Project\u00a0(HCP) dataset\u00a0<cit.> and an in-house dataset. \nThe dMRI scans in the HCP dataset were acquired with 270 diffusion gradients (three b-values) and a voxel size of 1.25 mm isotropic. \nIn\u00a0<cit.> 72 major WM tracts were annotated for the HCP dataset, and the annotations are also publicly available. \n\nFor the list of the 72 WM tracts, we refer readers to\u00a0<cit.>.\nThe dMRI scans in the in-house dataset were acquired with 270 diffusion gradients (three b-values) and a voxel size of 1.7 mm isotropic.\nIn this dataset, only ten of the 72 major WM tracts were annotated due to the annotation cost. \n\n\nFollowing\u00a0<cit.>, we selected the same 60 WM tracts as existing WM tracts, and a segmentation model was pretrained for these tracts with the HCP dMRI scans, where 48 and 15 annotated scans were used as the training set and validation set, respectively. \nTo evaluate the performance of one-shot segmentation of novel WM tracts, we considered a more realistic and challenging scenario where novel WM tracts are to be segmented on dMRI scans that are acquired differently from the dMRI scans annotated for existing WM tracts.\nSpecifically, instead of using the original HCP dMRI scans for segmenting novel WM tracts, we generated clinical quality scans from them. The clinical quality scans were generated by selecting only 34 diffusion gradients at b = 1000 s/mm^2 and downsampling the selected diffusion weighted images in the spatial domain by a factor of two to the voxel size of 2.5\u00a0mm isotropic.\n\n\nThe tract annotations were also downsampled accordingly.\nSince the dMRI scans in the in-house dataset were acquired differently from the original HCP dMRI scans, they were directly used for evaluation together with their original annotations.\n\n\n\nThe two datasets were used to evaluate the accuracy of one-shot segmentation of novel WM tracts separately based on the model pretrained on the original HCP dataset for segmenting existing WM tracts. \nWe considered three cases of novel WM tracts for each dataset. For the clinical quality scans, the three cases are referred to as CQ1, CQ2, and CQ3, and for the in-house dataset, the three cases are referred to as IH1, IH2, and IH3. The details about these cases are summarized in Table\u00a0<ref>.\nFor each dataset, only one scan was selected from it for network training, together with the corresponding annotation of novel WM tracts.[The single annotated scan was also used as the validation set for model selection.]\nFor the clinical quality dataset and the in-house dataset, 30 and 16 scans were used for testing, respectively, where the annotations of novel WM tracts were available and only used to measure the segmentation accuracy.\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Evaluation of Segmentation Results\n\n\n\nWe compared the proposed method with two competing methods, which are the classic fine-tuning strategy and the improved fine-tuning strategy\u00a0<cit.> described in Section\u00a0<ref> with the same pretrained model and single annotated training scan used by the proposed method.\nBoth competing methods were integrated with TractSeg\u00a0<cit.> like the proposed method.\nFor convenience, the classic fine-tuning strategy and the improved fine-tuning strategy are referred to as CFT and IFT, respectively.\nNote that as shown in\u00a0<cit.>, in the one-shot setting directly training a model that segments novel WM tracts from scratch without the pretrained model would lead to segmentation failure. Thus, this strategy was not considered.\n\n\n\nWe first qualitatively evaluated the proposed method.\nExamples of the segmentation results for novel WM tracts are shown in Fig.\u00a0<ref> for the proposed and competing methods, where the annotations are also displayed for reference. \nFor demonstration, here we show the results obtained in the case of CQ1 for CST_left and OR_right. We can see that the segmentation results of our method better resemble the annotations than those of the competing methods.\n\n\n\nNext, we quantitatively evaluated our method. The Dice coefficient was computed for the segmentation result of each novel WM tract on each test scan for each case.\nFor demonstration, we have listed the average Dice coefficient of each novel WM tract in Table\u00a0<ref> for the cases of CQ1 and IH1.\n\nFor each tract and each case, our method has a higher average Dice coefficient than the competing methods, and the improvement is statistically significant.\nWe have also summarized the mean of the average Dice coefficients of the novel WM tracts for all the cases in Table\u00a0<ref> (upper half table). In all cases our method outperforms the competing methods with higher mean Dice coefficients.\n\n\n\n\n\nFinally, we confirmed the benefit of each proposed data augmentation strategy, as well as the benefit of ensembling their results.\nFor each case, the mean value of the average Dice coefficients of all novel WM tracts was computed for the segmentation results achieved with RC1, RC2, TC1, or TC2 individually, and the results are also given in Table\u00a0<ref> (lower half table).\n\nCompared with the results of IFT that did not use the proposed data augmentation, the integration of IFT with RC1, RC2, TC1, or TC2 led to improved segmentation accuracy, which indicates the individual benefit of each proposed data augmentation strategy.\n\nIn addition, the Dice coefficients of the proposed method achieved with ensembling are higher than those achieved with a single data augmentation strategy, which confirms the benefit of ensembling.\nNote that there is not a data augmentation strategy that is better or worse than the others in all cases, which is possibly because of the randomness in RC1 and RC2 and the dependence of TC1 and TC2 on the spatial coverages of the novel WM tracts that vary in different cases.\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nWe have proposed an approach to one-shot segmentation of novel WM tracts based on an improved pretraining and fine-tuning framework via extensive data augmentation.\nThe data augmentation is performed with region masking, and several masking strategies are developed. The segmentation results achieved with these strategies are ensembled for the final segmentation.\n\n\nThe experimental results on two brain dMRI datasets show that the proposed method improves the accuracy of novel WM tract segmentation in the one-shot setting.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Acknowledgements\n\nThis work is supported by Beijing Natural Science Foundation (L192058).\n\nsplncs04\n\n\n"}