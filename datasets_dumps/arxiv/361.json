{"entry_id": "http://arxiv.org/abs/2303.06828v1", "published": "20230313030701", "title": "Two-step Band-split Neural Network Approach for Full-band Residual Echo Suppression", "authors": ["Zihan Zhang", "Shimin Zhang", "Mingshuai Liu", "Yanhong Leng", "Zhe Han", "Li Chen", "Lei Xie"], "primary_category": "eess.AS", "categories": ["eess.AS"], "text": "\n\n[\n    [\n    March 30, 2023\n==================\n\n\n\n\n\n\n\n\n\n\nThis paper describes a Two-step Band-split Neural Network (TBNN) approach for full-band acoustic echo cancellation. Specifically, after linear filtering, we split the full-band signal into wide-band (16KHz) and high-band (16-48KHz) for residual echo removal with lower modeling difficulty. The wide-band signal is processed by an updated gated convolutional recurrent network (GCRN) with U^2 encoder while the high-band signal is processed by a high-band post-filter net with lower complexity. Our approach submitted to ICASSP 2023 AEC Challenge has achieved an overall mean opinion score (MOS) of 4.344 and a word accuracy (WAcc) ratio of 0.795, leading to the 2^nd (tied) in the ranking of the non-personalized track.\n\n\n\n\n\n\n\nAcoustic echo cancellation, noise suppression, band-split, two-step network\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\nThe 4th acoustic echo cancellation (AEC) challenge\u00a0<cit.> is a flagship event of the ICASSP 2023 signal processing grand challenge, with the aim to benchmark AEC techniques in real-time full-band (48KHz) speech communication. In this challenge, our team\nhas submitted a hybrid approach that combines a linear filter with a neural post-filter to the non-personalized AEC track (i.e., without using target speaker embedding). \n\n\nSince noise-like components usually dominate in higher bands of speech and structured harmonics are mainly found in wide-band signals, we propose a two-step band-split neural network (TBNN) approach to particularly handle full-band residual echo removal on wide-band (16KHz) and high-band (16-48KHz) in a two-step process, as an extension of our previous work\u00a0<cit.> to better model full-band signals with low complexity. Specifically, the wide-band poster filter (WBPF) is based on the gated convolutional recurrent network (GCRN)\u00a0<cit.> but with upgraded U^2 encoder\u00a0<cit.> for better latent feature extraction. We also redesign the data simulation method and the loss function to accommodate full-band signals. \n\n\nAccording to the results, our system has ranked 2nd place (tied rank) in the non-personalized track with an overall mean opinion score (MOS) of 4.344 and a word accuracy (WAcc) ratio of 0.795.\n\n\n\n\n\n\n\u00a7 PROPOSED METHOD\n\n\n\n\n\n\n \u00a7.\u00a7 Problem formulation\n\n\n\nFor a typical full-duplex communication system consisting of a microphone and a loudspeaker, the signal model can be expressed as\n\n    d(n)=s(n)+r(n)+v(n)+z(n)\n\n\n    z(n)=h(n) \u2217\u2131{x(n)}\n\nwhere the microphone signal d(n) is composed of the near-end speech signal s(n) which may include early reflections, late reverberation r(n), additive noise v(n), and echo signal z(n), where n is the sampling index. \nThe echo signal z(n) is obtained by convolving the echo path h(n) with the nonlinear distorted reference signal x(n), shown in Eq.\u00a0(<ref>), where \u2131 refers to the nonlinear distortions, * refers to convolution. An AEC system aims to cancel z(n) from d(n) given x(n). For complex real-life scenarios, noise v(n) and reverberation r(n) also need to be suppressed.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe adopt a time delay estimation (TDE) module based on the sub-band cross-correlation to align the reference signal. Our linear filter uses a sub-band adaptive filtering method based on the NLMS algorithm to estimate the linear echo signal y and the error signal e.\n\n\n\n\n \u00a7.\u00a7 TBNN post-filter\n\n\n\n\nThe TBNN post-filter takes d, e, and y after a short-time Fourier transform (STFT) as input. The complex-valued input spectra are compressed by a factor of 0.5, stacked by real and imaginary parts, and divided into wide-band signal I_WB and high-band signal I_HB.\n\nConsidering the complexity of direct full-band modeling and the high-band has less structural information, we use a two-step band-split approach to model full-band signals.\nThe wide-band post-filter (WBPF) first uses spectral mapping to estimate the wide-band output \u015c_WB. Then the high-band post-filter (HBPF) uses I_HB and priori information \u015c_WB to predict the complex-valued mask M_HB, which is multiplied by I_HB to obtain the high-band output \u015c_HB.\n\n\nThe backbone of the WBPF module is based on our previous gated convolutional recurrent network (GCRN)\u00a0<cit.>. But differently, the encoder consists of 5 U^2-Encoder layers as shown in Fig.\u00a0<ref>(b). U^2-Net\u00a0<cit.> is a two-level nested U-structure designed to capture more contextual information from different scales (through Residual U-blocks) without significantly increasing the computational cost. Specifically, each U^2-Encoder layer is composed of a gated convolution (GConv) layer, a batch-norm (BN) layer, a PReLU and an UNet-block. The number of input and output channels remains the same. The FTLSTM layer is served as a bottleneck for temporal modeling, as defined in\u00a0<cit.>. The decoder consists of 5 decoder blocks as shown in Fig.\u00a0<ref>(c), where TrConv refers to the Transpose-Conv2d layer, and \u2297 refers to multiply.\nThe skip connection (implemented with 1\u00d71 convolution) is applied between the encoder and decoder. \nWe also add a voice activity detection (VAD) module to avoid over-suppression of near-end speech\u00a0<cit.>.\n\nThe HBPF module uses three 2D-Conv modules to extract high-band features. Each 2D-Conv module consists of a 2D convolution (Conv2d) layer, an ELU layer, a BN layer and a dropout layer with a 0.25 dropout rate\u00a0<cit.>. \nThe wide-band output serves as prior information and it is concatenated with the high-band features after feature dimension alignment. Then we use the GRU layer and Conv2d layer to predict the real/imaginary mask, which is applied to the high-band input to obtain the high-band output.\n\n\n\n\n\n\n \u00a7.\u00a7 Loss function\n\n\n\nThe loss function is composed of an echo power weighted magnitude loss and a power-law compressed phase aware (PLCPA) loss, our previous work has proved the effectiveness of echo-weighted loss\u00a0<cit.>.\nThe difference is that we calculate the loss separately for wide-band and high-band.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA VAD loss and a masked mean square error (MSE) loss are added as auxiliaries to avoid near-end speech over-suppression. Thus for the wide-band part, the loss is\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \u2112_WB=\u2112_echo-weighted + \u2112_plcpa + 0.5\u00b7\u2112_mask\u00a0+\u00a00.1\u00b7\u2112_vad.\n\n\nFor the high-band prediction, the loss is\n\n\n\n\n\n    \u2112_HB=\u2112_echo-weighted + \u2112_plcpa + 0.5 \u00b7\u2112_mask.\n\n\nThe final loss function is\n\n\n\n\n\n\n    \u2112_final = \u03b1\u2112_HB + \u2112_WB.\n\n\nDue to the dynamic range difference between high-band and wide-band, we empirically set \u03b1=10.\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n\n \u00a7.\u00a7 Dataset\n\n\n\nThe near-end signals are taken from the 4th deep noise suppression (DNS) challenge, and the noises are taken from Freesound and AudioSet. The echo dataset is composed of real recordings and synthetic data. Real recordings are taken from the AEC-Challenge far-end single-talk dataset. We remove the recordings that contain near-end speech, in order to avoid model over-suppression on near-end speech. Synthetic data is generated by convolving room impulse responses (RIRs) with the DNS data. A total of 50,000 RIRs are generated using the HYB method<cit.>, with random room size 5\u00d73\u00d73 to 8\u00d75\u00d74 and rt60 0.2-1.2s. Totally 30% of the near-end speech is convoluted with RIRs to simulate reverbed signals. The total training data has 2000 hours of real-recorded echo and 1000 hours of simulated echo, while SNR randomly ranges from 0dB to 25 dB and the SER randomly ranges from -15dB to 15 dB. \nThe validation and test sets are generated in the same way as above, with 50 hours and 1500 clips, respectively. \n\n\n\n\n\n\n \u00a7.\u00a7 Experimental setup\n\n\n\nWindow length and frame shift are 20 ms and 10 ms, respectively and 960-point STFT is applied. For each Conv2d, we use a convolution kernel of (2, 3) and a stride of (1, 2). The FTLSTM configuration is the same as\u00a0<cit.>. We experiment with two model sizes \u2013 the number of Conv2d channels in the encoder/decoder is 80 for the smaller one and 128 for the large one (with -L in the model name). We take our previous GCRN\u00a0<cit.> for comparison, which is a wide-band model without HBPF and thus our previous subband method\u00a0<cit.> is adopted to process full-band signal. U^2-based encoder is also tested for GCRN, named as GCRN-U^2. For the proposed TBNN model, the 2D-conv block in the HBPF module has an output channel of 128, the 1D convolution layer (Conv1d) is a point-wise convolution with an output channel of 48, and the GRU layer has a hidden state of 256. We particularly compare the performance of masking and mapping in HBPF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Results and conclusions\n\n\n\n\n\n\nWe use a 400-hour subset for quick experimentation. As shown in Table\u00a0<ref>, We have the following conclusions. First, the use of U^2-Encoder can achieve better performance with much fewer parameters compared to simply increasing the channel number (GCRN-U^2 vs. GCRN-L). Second, the proposed TBNN is better than GCRN which mainly focuses on the wide-band signal. Third, masking in HBPF leads to better near-end speech quality as compared with mapping. Finally, by enlarging the model size, TBNN-L-masking obtains the best performance. Thus the model trained with the entire 3000 hours of data is used to process the blind test set as our submission and results are shown in Table\u00a0<ref>. Our submitted model achieved 2nd place (tied) in the challenge ranking. Its real-time factor (RTF) is 0.35, tested on Intel(R) Xeon(R) E5-2678 v3 @2.50GHz using a single thread, comprising 0.28 for the TBNN post-filter (exported by ONNX) and 0.07 for the time delay estimation and adaptive filter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIEEEbib\n\n\n\n\n"}