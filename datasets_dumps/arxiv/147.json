{"entry_id": "http://arxiv.org/abs/2303.07153v1", "published": "20230313142734", "title": "SA-CNN: Application to text categorization issues using simulated annealing-based convolutional neural network optimization", "authors": ["Zihao Guo", "Yueying Cao"], "primary_category": "cs.LG", "categories": ["cs.LG"], "text": "\n\n\n\n\n\n\n\n\n\n\n\nSA-CNN: Application to text categorization issues using simulated annealing-based convolutional neural network optimization\n    1st Zihao GUO\u2020\n\u00c9cole Centrale de Nantes \n\nNantes, France \n\nzihao.guo@eleves.ec-nantes.fr\n\n\n2nd Yueying CAO\n\u00c9cole Centrale de Nantes \n\nNantes, France \n\nyueying.cao@eleves.ec-nantes.fr\n\n    March 30, 2023\n===========================================================================================================================================================================================\n\n\nConvolutional neural networks (CNNs) are a representative class of deep learning algorithms including convolutional computation that perform translation-invariant classification of input data based on their hierarchical architecture. However, classical convolutional neural network learning methods use the steepest descent algorithm for training, and the learning performance is greatly influenced by the initial weight settings of the convolutional and fully connected layers, requiring re-tuning to achieve better performance under different model structures and data. Combining the strengths of the simulated annealing algorithm in global search, we propose applying it to the hyperparameter search process in order to increase the effectiveness of convolutional neural networks (CNNs). In this paper, we introduce SA-CNN neural networks for text classification tasks based on Text-CNN neural networks and implement the simulated annealing algorithm for hyperparameter search. Experiments demonstrate that we can achieve greater classification accuracy than earlier models with manual tuning, and the improvement in time and space for exploration relative to human tuning is substantial.\n\n\n\nSimulated Annealing Algorithm; Text Classification; Deep Learning; Self-optimization\n\n    \n\n\u00a7 INTRODUCTION\n\nIn recent years, significant breakthroughs have been achieved in the field of convolutional neural networks for text classification, and Yoon Kim<cit.> proposed a straightforward single-layer CNN architecture that can outperform traditional algorithms in a variety of uses. Rie Johnson and Tong Zhang<cit.> applied CNNs to high-dimensional text data and learned with embed small text regions for classification. Tong He and Weilin Huang<cit.> proposed a convolutional neural network that extracts regions and features related to text from image components. This type of model use vectors to characterize each sentence in the text, which are then merged into a matrix and utilized as input for constructing a CNN network model.\n\nNumerous experiments have shown, however, that the performance of neural networks is highly dependent on their architecture<cit.><cit.><cit.>. Due to the discrete nature of these parameters, accurate optimization algorithms cannot be used to resolve the architecture optimization problem<cit.>. Manually tuning the parameters of a model to optimize its performance for different tasks is not only inefficient, but also likely to miss the optimal parameters, resulting in a network architecture that does not achieve maximum performance, which is not advantageous in comparison to traditional classification algorithms<cit.>. In addition, the widely utilized grid search is an enumerative search, i.e., it tries every possibility by performing a cyclic traversal of all candidate parameter choices, which is marked by its high time consumption and limited globalization. Therefore, it is practical to use an algorithm to automatically and fairly rapidly determine the optimal architecture of a neural network.\n\nIt has been shown that tuning neural network hyperparameters with metaheuristic algorithms not only simplifies the network<cit.><cit.>, but also enhances its classification performance. In this paper, we use simulated annealing algorithm to optimize the neural network architecture, and we model the neural network hyperparameter optimization problem as a dual-criteria optimization problem of classification accuracy and computational complexity. The resulting network achieves improved classification performance in the text classification task.\n    \n\n\u00a7 BACKGROUND AND RELATED WORK\n\n\n\n\n\n\n\n \u00a7.\u00a7 The current utilisation text classification in neural networks\n\nText classification was initially done by using knowledge engineering to build an expert system and perform classification, which is a laborious task with limited accuracy. After that, along with the development of statistical learning methods and machine learning disciplines, the classical approach of feature engineering plus shallow classification models developed gradually (Fig.1). During this period, rule-based models: decision trees<cit.>, probability-based models: Nave Bayes classification algorithms<cit.><cit.>, geometry-based models: SVM<cit.>and statistical models: KNN<cit.>, etc. However, these models typically rely heavily on time-consuming feature engineering or large quantities of additional linguistic resources, and are ineffective at learning semantic information about words.\n\n\n\n\nIn recent years, research on deep learning that incorporates feature engineering into the process of model fitting has significantly enhanced the performance of text classification tasks. Kim <cit.> explored the use of convolutional neural networks with multiple windows for text classification, a method that has been widely adopted in industry due to its high computational speed and parallelizability. Yang et al <cit.> proposed HAN, a hierarchical attention mechanism network that mitigates the gradient disappearance problem caused by RNNs in neural networks. Johnson and Zhang <cit.> proposed a word-level deep CNN model that improves network performance by increasing network depth without significantly increasing computational burden.\n\nAdditionally to convolutional neural networks and recurrent neural networks, numerous researchers have proposed more intricate models in recent years.  The capsule networks-based text classification model proposed by Zhao et al <cit.>. outperformed conventional neural networks. Google proposed the BERT model <cit.>, which overcomes the problem that static word vectors cannot solve the problem of a word having multiple meanings. However, the parameters of each of the aforementioned deep learning models have a substantial effect on network performance and must be optimized for optimal network performance.\n\n\n\n\n\n\n \u00a7.\u00a7 Current research status on the simulated annealing method and associated neural network optimization\n\nThe Simulated Annealing Technique is a stochastic optimization algorithm based on the Monte Carlo iterative solution approach that was first designed for combinatorial optimization and then adapted for general optimization. Its fundamental concept is based on the significance sampling approach published by Metropolis in 1953, but Kirkpatrick et al.<cit.> did not properly implement it into the field of combinatorial optimization until 1983.\n\n\n\nThe algorithm for simulated annealing is taken from the process of metal annealing<cit.>, which may be summarized roughly as follows. The simulated annealing technique begins with a high initial temperature, then as the temperature parameter decreases, it searches for the optimal solution among all conceivable possibilities. The SA method has a probability of accepting a solution that is worse than the initial one, i.e. the locally optimal solution can probabilistically jump out and finally converge to the global optimum. This likelihood of accepting a suboptimal answer decreases until the SA approaches the global optimal solution.\n\nThe standard optimization process of the simulated annealing algorithm can be described as follows.\n\n\n\n\n\n\n\n\nSA has been utilized extensively in VLSI<cit.>, production scheduling<cit.>, machine learning<cit.>, signal processing<cit.>, and other domains as a general-purpose stochastic search method. Boltzmann machine<cit.>, the first neural network capable of learning internal expressions and solving complicated combinatorial optimization problems, utilizes the SA principle for optimization precisely, therefore the optimization potential of SA is evident.\n\nRasdiRere et al.<cit.> employed simulated annealing to automatically construct neural networks and alter hyperparameters, and experimental findings revealed that the method could increase the performance of the original CNN, demonstrating the efficacy of this optimization technique. Mousavi et al.<cit.> updated a solar radiation prediction model by integrating artificial neural networks and simulated annealing with temperature cycling to increase ANN calibration performance. Choubin et al.<cit.> utilized the simulated annealing (SA) feature selection approach to find the influential factors of PM modeling based on current air detection machine learning models for the spatial risk assessment of PM10 in Barcelona.\n\nAccording to the study, however, there is no research on the combination of simulated annealing and neural networks for text categorization tasks. This research conducts tests on neural networks employing simulated annealing to enable automated hyperparameter search, based on the fact that neural networks now generate superior outcomes in text categorization.\n    \n\n\u00a7 METHODS\n\n\n\n\n\n\n \u00a7.\u00a7 Convolutional neural networks for text processing tasks (Text-CNN)\n\nConvolutional neural networks (CNN) originated in the field of computer vision; however, with the recent deformation of the CNN input layer, this neural network structure has been steadily transferred to the field of natural language processing, where it is often referred to as Text CNN. The schematic is seen below.\n\n\n\nA text statement consists of n words, therefore the text is separated according to words and each word is mapped to the Word Embedding layer as a k-dimensional vector. At this point, for this input model, the text may be regarded as a n\u00d7 k single-channel picture. During the processing of the convolution layer, the convolution is used to extract the relationships between tuples containing different numbers of words, i.e., to generate new features and obtain different feature maps, when the width of the convolution kernel is equal to the dimension k of the word vector.\n\nThe feature map in the form of an n-dimensional vector is then sampled using max-pooling to determine the maximum value, and the data is pooled and utilized as input to the fully connected layer. The softmax function[Eq. (1)] is then employed to convert these probabilities into discrete 0 or 1 class labels in order to solve this classification challenge.\n\n\n    y = softmax(W_1h + b_1)\n\n\nAs demonstrated in Eq. (2), a cross-entropy loss function is frequently utilized for the classification job in model training.\n\n    Loss = -\u2211_i=1^ny_i\u00d7 log(y_i^')\n\nwhere y_i is the label value corresponding to the real probability distribution of the i^th sample, y_i^' is the prevalence measure corresponding to the projected probability distribution of the i^th sample, and n is the number of samples in the training set.\n\nThe hyperparameter optimization process of the neural network by simulated annealing method is shown below.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Hyperparametric optimization method based on multi-objective simulated annealing method\n\nIn this study, we use the MOSA algorithm proposed by G\u00fclc\u00fc and Ku\u015f <cit.> to optimize the hyperparameters of the convolutional neural network in order to find the most suitable parameters quickly and achieve a higher accuracy rate. Similar to the single-objective SA algorithm, we extend the SA algorithm, which only considers the error rate of the network implementation, to consider the two objectives of the number of FLOPs required by the network and the error rate of the network, respectively, and define the stopping criterion of the simulated annealing method as the number of iterations.\n\n\n\n  \u00a7.\u00a7.\u00a7 Main flow of MOSA algorithm\n\n\nThe MOSA algorithm primarily uses Smith's Pareto dominance rule <cit.> due to complications such as the need for two target values to be on the same scale, followed by the application of decision rules to aggregate the probabilities, and the need to maintain different temperatures due to the different probabilities evaluated for each target. All non-dominated solutions encountered during the search are stored in an external archive, A, when the first iteration begins at the initial temperature. As new solutions are accepted, A is updated (by inserting the new solution X' and removing all solutions dominated by it) and a superior solution is eventually formed as the Pareto frontier. As depicted in the following flowchart, whenever a new solution X' is obtained, X and A are updated based on the dominance relationship between the current solution X, the new solution X', and the solution in the external archive A. This process of re-visiting previously visited archive solutions is known as the return-to-base strategy. In contrast to the single-target SA algorithm, the \u0394 F calculation used to determine the probability of acceptance is different in this method. For calculation purposes, a single temperature will be maintained regardless of the number of targets.\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Setting and calculation of temperature and probability for SA method\n\n\nFirst, the initial and finale temperatures. Theoretically, the higher the initial temperature setting, the better, but since the time required for convergence increases correspondingly with the temperature, it is necessary to make a compromise between convergence time and convergence accuracy. To define T_init, we apply a real-time initial temperature selection strategy. In this strategy, rather than using Eq. (3) to calculate the initial probability value for a given initial temperature (where \u0394 F is the amount of deterioration in the objective function and T_cur is the current temperature). \n\n    p_acc=min{1, exp( \u2212\u0394 F/T_cur )}\n\nWe use Eq. (4) to calculate the initial temperature with an initial probability value (where Fave is the average amount of deterioration penalty calculated during short-term \"aging\"). tfinal is also defined by this method of real-time temperature adjustment)\n\n    T_init= \u2212(\u0394 F_ave/(ln(p_acc)))\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Acceptance criteria in searching parameters\n\nIn this experiment, the number of iterations is used to define the stopping criterion for the simulated annealing method. The rationale is as follows: if poor early performance is observed on the validation set, the current training process is terminated and the next search round is initiated. This approach has the benefit of introducing noise while decreasing the total running time of the HPO method. In each iteration of the simulated annealing method, the newly generated configuration is trained on the training set of the original training set and evaluated on the validation set (i.e., the test split of the original training set).\nWe apply the Xavier weight initial value setting term, a learning rate of 0.001, the Rmsprop optimizer, and a batch size of 32.\n\n\n\n  \u00a7.\u00a7.\u00a7 Optimization of the Simulated Annealing method\n\nStarting with the initial solution I and initial temperature T_init, the iterative process \u201cgenerate a new solution \u2192 calculate the objective function difference \u2192 accept or reject\" is repeated for the current solution and temperature. T_cur is gradually decayed, and if the optimal error rate achieved on the validation set does not improve after three consecutive calendar hours, the training procedure is terminated, indicating that the current solution is the approximate optimal solution, i.e. the optimal state of the network has been reached.\n    \n\n\u00a7 EXPERIMENTS AND RESULTS\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Introduction to the experimental data set\n\nThis experiment utilized two short text categorization datasets, the MR dataset and the TREC dataset. MR is a dataset for the sentiment analysis of movie reviews, with each sample classified as positive sentiment or negative sentiment. The CR dataset consists of reviews of five electronic products, and these sentences have been manually tagged with the sentiment of the reviews. TREC is a question classification dataset in which each data point represents a question description, and the job entails categorizing a question into six question kinds, including person, place, numerical information, abbreviation, description, and entity.\n\n\nThe table displays the statistical information of the three data sets.\n\n\n\nSeveral samples from each of the two datasets are provided below.\n\n\n  * It's a square, sentimental drama that satisfies, as comfort food often can. [MR Dataset, tags: positive].\n\n  * The sort of movie that gives tastelessness a bad rap. [MR Dataset, tags: negative].\n\n  * this camera is so easy to use ! [CR Dataset, tags: positive].\n\n  * the sound level is also not as high as i would have expected . [CR Dataset, tags: negative].\n\n  * What is Australia's national flower? [TREC Dataset, tags: place]\n\n  * Who was the first man to fly across the Pacific Ocean? [TREC Dataset, tags: person]\n\n\n\n\n\n\n\n \u00a7.\u00a7 Introduction to the comparison model\n\nComparing the model in the article to the experimental model in Kim's<cit.> study for experimentation.\n\n\n  * CNN-rand: All word vectors are initialized at random before being utilized as optimization parameters during training.\n\n  * CNN-static: All word vectors are directly acquired with the Word2Vec tool and are fixed.\n\n\n  * CNN-multichannel: A mix of CNN-static and CNN-non-static, i.e. two types of inputs.\n\n  * DCNN<cit.>: Dynamic Convolutional Neural Network with k-max pooling.\n\n  * MV-RNN<cit.>: Matrix-Vector Recursive Neural Network with parse trees.\n\n\n\n\n\n\n\n \u00a7.\u00a7 SA-CNN parameter setting\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Parameter setting of MOSA\n\nIn this research, we employed the identical parameter settings for the simulated annealing approach as G\u00fclc\u00fc and Ku\u015f<cit.>, set the starting initial probability value to 0.5, and derived T_init\u2248 0.577 and T_finial\u2248 0.12 in a similar fashion. The link between the number of exterior and internal iterations estimated for different cooling rate values is depicted in the table.2.  \nThe number of outer iterations defines the number of search networks, whereas the number of inner iterations determines the number of single network training iterations. As the cooling rate, we chose 0.95, where the number of external cycles is greater than the number of internal cycles, to ensure that as many network structures as possible are searched for and to avoid repeated training of a single network structure as much as possible, thereby avoiding becoming trapped in a local optimal solution of network selection.\n\n\n\n  \u00a7.\u00a7.\u00a7 Search range of neural network hyperparameters\n\nIn this study, we utilize a 300-dimensional word2vec trained by Mikolov<cit.> as the initial representation and continually update the word vector during the training process, similar to Kim's<cit.> approach. In this paper, the empirical range of hyperparameters to be tuned in the network is provided so that the simulated annealing technique can search for a new solution from the existing one. Expanding the range of searchable hyperparameters may result in improved experimental outcomes, provided computational resources permit.\n\n \n\n  * Conv:\n     \n    \n  * kernelCount: [32, 64, 96, 100, 128, 160, 256]\n    \n  * dropoutRate: [0.1, 0.2, 0.3, 0.4, 0.5]\n    \n\n  * fullyConnected:\n     \n    \n  * unitCount: [16, 32, 64, 128, 256, 512]\n    \n  * dropoutRate: [0.1, 0.2, 0.3, 0.4, 0.5]\n    \n\n  * learningProcess:\n     \n    \n  * activation: [relu, leaky_relu, elu, tanh, linear]\n    \n  * learningRate: [0.0001, 0.001, 0.01, 0.0002, 0.0005, 0.0008, 0.002, 0.004, 0.005, 0.008]\n    \n  * batchSize: [64, 128, 256]\n    \n\n  * seedNumber: 40\n\n  * ratioInit: 0.9\n\n\n\n\n\n\n\n \u00a7.\u00a7 Experimental results and discussion\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Comparison of model accuracy results\n\nThe following table shows the accuracy of different CNN models for text classification tasks on MR, CR and TREC datasets.\n\n\n\n\nAs shown in the table.3, on the MR and CR datasets, the model presented in this paper outperformed other neural network structures, leading the authors to conclude that, due to a lack of computational resources, the parameter search range was restricted and the optimal network was not found. Nevertheless, the performance of the convolutional neural network was utilized, and on the TREC dataset, the model SA-CNN achieved the highest accuracy rate. Under the assumption of using the same model structure, the experimental results demonstrate that using the simulated annealing algorithm to find the optimal hyperparameters not only reduces the tediousness of manual parameter tuning, but also yields better parameters than manual tuning if the search range is correct, thereby achieving a high test accuracy.\n\n\n\n  \u00a7.\u00a7.\u00a7 Discussion of experimental results\n\nIn order to comprehend the characteristics of the ideal hyperparameters discovered by the simulated annealing technique, the following tables list the top 3 optimal hyperparameters sought by the algorithm in the TREC dataset.\n\nCompared to manual tuning, the simulated annealing algorithm may search for hyperparameter combinations that one would not ordinarily consider; for instance, the number of CNN convolutional kernels for the Top1 model on the TREC dataset is 100, 64, and 32 for three different strings, which is a combination that one would not ordinarily consider. Therefore, by utilizing the simulated annealing process to optimize the neural network's hyperparameters, it is theoretically possible to acquire hyperparameter combinations that have not been considered or disregarded based on prior experience, and so achieve improved performance.\n\n    \n\n\u00a7 CONCLUSION\n\n\n\n\nIn this article, we proposed a machine learning method combining simulated annealing and convolutional neural networks. The main goal is to adjust the hyperparameters of neural networks using simulated annealing in order to prevent manual tuning parameters into local optima, thus failing to enhance neural network performance. The experimental results demonstrate that the method of implementing simulated annealing to tune the hyperparameters of a neural network is effective in overcoming the constraints of manual parameter tuning, is practical, and can be theoretically applied to additional natural language processing problems. Due to the limited resource space and the different cooling rate for defining the initialization, which may result in different time costs and architecture, the final solution may only be an approximate optimal solution, and this approximation may differ. Consequently, the simulated annealing method may be integrated with other algorithms or the multi-objective simulated annealing algorithm may be further optimized, thereby further enhancing the efficiency of the simulated annealing algorithm on neural network optimization.\n\n\n\n\nunsrt\n \n\n"}