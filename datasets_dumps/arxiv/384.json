{"entry_id": "http://arxiv.org/abs/2303.06797v1", "published": "20230313010732", "title": "Orthogonal Transform Domain Approaches for the Convolutional Layer", "authors": ["Hongyi Pan", "Xin Zhu", "Salih Atici", "Ahmet Enis Cetin"], "primary_category": "cs.CV", "categories": ["cs.CV", "eess.IV", "eess.SP"], "text": "\n\n\n\n\n\n\nOrthogonal Transform Domain Approaches for the Convolutional Layer\n    Hongyi Pan,\u00a0Student Member,\u00a0IEEE, Xin Zhu, Salih Atici, Ahmet Enis Cetin,\u00a0Fellow,\u00a0IEEE\nThis work was supported by National Science Foundation (NSF) under grant 1934915. The source code of this work will be released at https://github.com/phy710/transform-based-layers.\n    March 30, 2023\n===============================================================================================================================================================================================================================================================================\n\n\n\n\nIn this paper, we propose a set of transform-based neural network layers as an alternative to the 3\u00d73 Conv2D layers in Convolutional Neural Networks (CNNs). The proposed layers can be implemented based on orthogonal transforms such as Discrete Cosine Transform (DCT) and Hadamard transform (HT), and the biorthogonal Block Wavelet Transform (BWT). Convolutional filtering operations are performed in the transform domain using element-wise multiplications by taking advantage of the convolution theorems. Trainable soft-thresholding layers that remove noise in the transform domain bring nonlinearity to the transform domain layers. Compared to the Conv2D layer which is spatial-agnostic and channel-specific, the proposed layers are location-specific and channel-specific. The proposed layers reduce the number of parameters and multiplications significantly while improving the accuracy results of regular ResNets on the ImageNet-1K classification task. Furthermore, the proposed layers can be inserted with a batch normalization layer before the global average pooling layer in the conventional ResNets as an additional layer to improve classification accuracy with a negligible increase in the number of parameters and computational cost. \n\n\n\n\nTransform-based convolutional layer, convolution theorem, soft-thresholding, image classification.\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nRecent literature states that convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks, such as image classification\u00a0<cit.>, object detection\u00a0<cit.>, semantic segmentation\u00a0<cit.> and clustering\u00a0<cit.>. One of the most famous and successful CNNs is ResNet\u00a0<cit.>, which enables the construction of very deep networks to get better performance in accuracy. However, with more and more layers being used, the network becomes bloated. In this case, the huge amount of parameters increases the computational load for the devices, especially for the edge devices with limited computational resources. The convolutional layer, which is the critical component in the convolutional neural network, is our target not only to slim but also to improve the accuracy of the network.\n\nIn the convolutional layer, convolution kernels are spatial-agnostic and channel-specific. Because of the spatial-agnostic characteristic, a convolutional layer cannot adapt to different visual patterns with respect to different spatial locations. Therefore, in the convolutional layer, many redundant convolutional filters and parameters are required for feature extraction in location-related problems.\n\nThe well-known Fourier convolution theorem states that the convolution in the space domain is equivalent to the element-wise multiplication in the Fourier domain. In another word, \ud835\udc32=\ud835\udc1a*\ud835\udc31 in the space domain can be implemented in the Discrete Fourier Transform (DFT) domain by element-wise multiplication: \n\n    \ud835\udc18[k] = \ud835\udc00[k]\ud835\udc17[k],\n\nwhere \ud835\udc18= \u2131(\ud835\udc32), \ud835\udc00= \u2131(\ud835\udc1a), and \ud835\udc17= \u2131(\ud835\udc31) are the DFTs of \ud835\udc32, \ud835\udc1a, and \ud835\udc31, respectively, and \u2131(\u00b7) is the Fourier transform operator. Eq.\u00a0(<ref>) holds when the size of the DFT is larger than the size of the convolution output \ud835\udc32. Since there is a one-to-one relationship between the kernel coefficients \ud835\udc1a and the Fourier transform representation \ud835\udc00,     \nkernel weights can be learned in the Fourier transform domain using backpropagation-type algorithms. Since the DFT is an orthogonal (\ud835\udc05_N^H \ud835\udc05_N= \ud835\udc05_N\ud835\udc05_N^H =\ud835\udc08, where \ud835\udc05_N is the N by N transform matrix),\nand complex-valued transform, we consider other orthogonal transforms such as the Discrete Cosine Transform (DCT) and the Hadamard transform (HT). DCT is a real-valued transform and HT is a binary transform that is even multiplication-free. In this paper, we use the DCT and HT convolution theorems to develop novel network layers. Other orthogonal transforms such as the orthogonal transforms generated from wavelet packet filter banks and the discrete Hartley transform can be also used in our approach <cit.>. Furthermore, we implement the block wavelet transform (BWT) using the Wavelet biorthogonal 1.3 filters (Bior 1.3) to develop novel network layers. \n\n\n\nOther related transform domain methods include:\n\nDFT-based methods The Fast Fourier transform (FFT) algorithm is the most important signal and image processing method. As it is well-known Eq.\u00a0(<ref>), convolutions in time and image domains can be performed using elementwise multiplications in the Fourier domain. However, the Discrete Fourier Transform (DFT) is a complex transform. In\u00a0<cit.>, the fast Fourier Convolution (FFC) method is proposed in which the authors designed the FFC layer based on the so-called Real-valued Fast Fourier transform (RFFT). In RFFT-based methods, they concatenate the real and the imaginary parts of the FFT outputs, and they perform convolutions in the transform domain with ReLU in the concatenated Fourier domain. Therefore, they do not take advantage of the Fourier domain convolution theorem. Moreover, concatenating the real and the imaginary parts of the complex tensors increases the number of parameters so their model requires more parameters than the original ResNets as the number of channels is doubled after concatenating. On the contrary, our method takes advantage of the convolution theorem and it can reduce the number of parameters significantly while producing comparable and even higher accuracy results.\n\nDCT-based methods Other DCT-based methods include\u00a0<cit.>. Since the images are stored in the DCT domain in JPEG format, the authors in\u00a0<cit.> use DCT coefficients of the JPEG images but they did not take the transform domain convolution theorem to train the network. In\u00a0<cit.>, transform domain convolutional operations are used only during the testing phase. The authors did not train the kernels of the CNN in the DCT domain.  \n\nThey only take advantage of the fast DCT computation and reduce parameters by changing 3\u00d7 3 Conv2D layers to 2\u00d7 2. In contrast, we train the filter parameters in the DCT domain and we use the soft-thresholding operator as the nonlinearity. We should not train a network using DCT without soft-thresholding because both positive and negative entries in the DCT domain are equally important. \nIn\u00a0<cit.>, Harmonic convolutional networks based on DCT were proposed. Only forward DCT without inverse DCT computation is employed to obtain Harmonic blocks for feature extraction. In contrast with spatial convolution with learned kernels, this study proposes feature learning by weighted combinations of responses of predefined filters. The latter extracts harmonics from lower-level features in a region, and the latter layer applies DCT on the outputs from the previous layer which are already encoded by DCT.\n\nHT-Based Neural Networks \nIn\u00a0<cit.>, authors use HT is used to assist their \"ZerO Initialization\" method. They apply the HT in the skip connections, but they did not use the convolution theorem and they did not replace the convolutional layers with multiplicative layers in the Hadamard domain. This method does not reduce the number of parameters or the computational cost. Their goal is to improve the accuracy of their networks. HT-based neural networks from our early work including\u00a0<cit.> are used to reduce the computational cost. In\u00a0<cit.>, a binary neural network with a two-stream structure is proposed, where one input is the regular image and the other is the HT of the image, and convolutions are not implemented in the transform domain. The HT is only applied at the beginning in\u00a0<cit.>. The HT-based layers in\u00a0<cit.> also use take the element-wise multiplication in the transform domain like this work, but they do not extract any channel-wise feature. They only extract the features width-wise and height-wise, as they have a similar scaling layer to this work. On the contrary, we have channel-wise processing layers in the transform domain to extract the channel-wise features, and this revision improves the performance of the HT-based layer significantly.\n\n\n\n\nWavelet-based methods Wavelet transform (WT) is a well-established method in signal and image processing. WT-based neural networks include\u00a0<cit.>. However, the regular WT does not have a convolution theorem. In another word, convolutions cannot be equivalently implemented in the wavelet domain. However, we convert the wavelet transform to block wavelet transform (BWT) as in <cit.>, and assign weights to the BWT coefficients as in convolution theorems and use them in a network layer as if we are performing convolutions in the transform domain; i.e., we perform elementwise multiplications in the transform domain instead of convolutions. Authors in \u00a0<cit.> did not take advantage of the BWT.\n    \n\n    \nTrainable soft-thresholding The soft-thresholding function is commonly used in wavelet transform domain denoising\u00a0<cit.>. It is a proximal operator for the \u2113_1 norm\u00a0<cit.>. With trainable threshold parameters, soft-thresholding and its variants can be employed as the nonlinear function in the frequency domain-based networks\u00a0<cit.>. ReLU is not suitable because both positive and negative values are important in the transform domain. A completely positive waveform can have both significant positive and negative values in the transform domain. Lacking the negative-valued frequency components may cause issues when the inverse transform is applied. The computational cost of the soft-thresholding function is similar to the ReLU, and it keeps both positive and negative valued frequency components whose magnitudes exceed the trainable threshold. \n\nOur contribution is summarized as follows:\n\n\u2219 We propose a family of orthogonal transform domain approaches to replace the convolutional layer in a CNN to reduce parameters and computational costs. With the proposed layers, a CNN can reach a comparable or even higher accuracy with significantly fewer parameters and less computational consumption.\n\n\u2219 We extend the proposed structures to biorthogonal wavelet transforms. Both orthogonal such as the Haar wavelet and biorthogonal such as Bior 1.3 can be used to construct Conv2D layers.\n\n\u2219 We propose multi-channel structures for the proposed layers. In this paper, we use the tri-channel structure. The tri-channel layer contains more parameters and MACs than the single-channel layer, but it makes the revised CNNs reach higher accuracy results. For example, compared to the regular ResNet-50 model, ResNet-50 with the tri-channel DCT-perceptron layer reaches 0.82% higher center-crop top-1 accuracy on the ImageNet-1K dataset with 11.5% fewer parameters and 11.5% fewer MACs. \n\n\u2219 The proposed single-channel layer can be inserted before the global average pooling layer or the flattened layer to improve the accuracy of the network with a negligible number of extra parameters and computation time. For instance, without changing the convolutional base, an extra single-channel DCT-perceptron layer improves \n\nthe center-crop top-1 accuracy of ResNet-18 0.74% on the ImageNet-1K dataset with only 2.3% extra parameters and 0.3% extra MACs.\n\n\n\n\n\u00a7 METHODOLOGY\n\n\n\n \u00a7.\u00a7 Orthogonal Transforms and Their Convolution Theorems\n\nLike the discrete Fourier transform (DFT), the discrete cosine transform (DCT) is also widely used for frequency analysis because it can be considered a real-valued version of the DFT\u00a0<cit.>. The type-II DCT (Eq.\u00a0<ref>) and its inverse (IDCT) (Eq.\u00a0<ref>) are commonly used as the general DCT and the inverse DCT. In detail, the DCT of  a sequence \ud835\udc31=[\ud835\udc31[0] \ud835\udc31[1] ... \ud835\udc31[N-1]]^T is computed as\n\n    \ud835\udc17[k] = \u2211_n=0^N-1\ud835\udc31[n]cos[\u03c0/N(n+1/2) k],\n\nand its inverse is computed as\n\n    \ud835\udc31[n] = 1/N\ud835\udc17[0]+2/N\u2211_k=1^N-1\ud835\udc17[k]cos[\u03c0/N(n+1/2)k],\n\nfor 0\u2264 k \u2264 N-1, 0\u2264 n \u2264 N-1. \n\nOn the other hand, the Hadamard transform (HT) can be considered a binary version of the DFT. No multiplication is required if we ignore normalization. The HT is identical to the inverse HT (IHT) if the same normalization factor\u221a(1/N) is applied:\n\n    \ud835\udc17=\u221a(1/N)\ud835\udc07_N\ud835\udc31, \ud835\udc31=\u221a(1/N)\ud835\udc07_N\ud835\udc17,\n\nwhere, \ud835\udc17=[\ud835\udc17[0], ..., \ud835\udc17[N-1]]^T. \ud835\udc07_N is the N by N Hadamard matrix that can be constructed as\n\n\n    \ud835\udc07_N = \n    \t\t\t\n    \t\t\t\t1,    N = 1,\n    [  1  1;  1 -1 ],    N =2,\n    [  \ud835\udc07_N/2  \ud835\udc07_N/2;  \ud835\udc07_N/2 -\ud835\udc07_N/2 ]=\ud835\udc07_2 \u2297\ud835\udc07_N/2,    N \u2265 4,\n\n\t\twhere, \u2297 stands for the Kronecker product.\nIn the implementation, we can combine two \u221a(%s/%s)1N normalization terms from the HT and its inverse to one 1/N to avoid the square-root operation. \n\n\n\n\n\n\n\n\n\n\n\n\nThe Hadamard transform \ud835\udc07_4\n can be implemented using a wavelet filterbank with binary convolutional filters \ud835\udc21[n] = { 1, 1 } and the highpass filter \ud835\udc20[n] = {-1, 1 } and dropout in two stages <cit.> as shown in Fig.\u00a0<ref>. This process is the basis of the fast algorithm. An N=2^M\n can be implemented in M stages. In fact the low-pass filter\n \ud835\udc21[n] = { 1, 1 } and the highpass filter \ud835\udc20[n] = {-1, 1 } are the building blocks of the butterfly operation of the fast Fourier transform (FFT) algorithm and the first stage of the Hadamard and the FFT are the same. In the FFT algorithm, complex exponential terms are also used in addition to butterflies in other stages. Each stage of the convolutional filterbank shown in Fig. 1 can be also expressed as a matrix-vector multiplication as in the FFT algorithm. \n \n We can also use the wavelet filter bank to implement orthogonal and biorthogonal transforms such as \"Bior 1.3\", whose coefficients \ud835\udc21[n] = {-0.125, 0.125, 1, 1, 0.125, -0.125} and g[n] = {-1, 1}\u00a0<cit.>. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe HT and DCT can be implemented using butterfly operations described in Eq. (1) in\u00a0<cit.> and the filterbank implementation of BWT is also fast <cit.>. Therefore, the complexity of each transform on a N-length vector is O(Nlog_2 N). The 2-dimensional (2D) transform is obtained from the 1-dimensional (1D) transform in a separable manner for computational efficiency. Thus, the complexity of a 2D transform on an N\u00d7 N image is O(N^2log_2 N)\u00a0<cit.>. Furthermore, using the hybrid quantum-classical approach\u00a0<cit.>, the 1D-HT and the 2D-HT can be implemented in O(N) and O(N^2) time, respectively, as a quantum logic gate can be implemented efficiently in parallel. \n\n\n\nThe DCT convolution theorem Theorem\u00a0<ref> is related to the DFT convolution theorem:\n\nLet N\u2208\u2115_+, and \ud835\udc1a, \ud835\udc31\u2208\u211d^N. Then, \ud835\udc32=\ud835\udc1a*_s\ud835\udc31\u27fa\ud835\udc18[k] = \ud835\udc00[k]\ud835\udc17[k], where, \ud835\udc18=\ud835\udc9f(\ud835\udc32), \ud835\udc00=\ud835\udc9f(\ud835\udc1a), \ud835\udc17=\ud835\udc9f(\ud835\udc31). \n\nWhere, \ud835\udc9f(\u00b7) stands for DCT, *_s stands for the symmetric convolution.\nThe relation between the symmetric convolution *_s and the linear convolution * is given by\n\n    \ud835\udc1a*_s\ud835\udc31 = \u00e3*\ud835\udc31\n\nwhere \u00e3 is the symmetrically extended kernel in \u211d^2N-1:\n\n    \u00e3_k = \ud835\udc1a_|N-1-k|,\n\nfor k=0, 1, ..., 2N-2. The proof of Theorem\u00a0<ref> is presented in Section III in\u00a0<cit.>. \n\nSimilarly, we have the HT convolution theorem\u00a0<ref>:\n\nLet M\u2208\u2115, N=2^M, and \ud835\udc1a, \ud835\udc31\u2208\u211d^N. Then \ud835\udc32=\ud835\udc1a*_d\ud835\udc31\u27fa\ud835\udc18[k] = \ud835\udc00[k]\ud835\udc17[k], where, \ud835\udc18=\u210b(\ud835\udc32), \ud835\udc00=\u210b(\ud835\udc1a), \ud835\udc17=\u210b(\ud835\udc31).\n\nWhere, \u210b(\u00b7) stands for HT, *_d represents the dyadic convolution. Although the dyadic convolution *_d is not the same as circular convolution, we can still use the HT for convolutional filtering in neural networks. This is because \nHT is also related to the block Haar wavelet packet transform\u00a0<cit.> and each Hadamard coefficient approximately represents a frequency band. As a result, applying weights onto frequency bands and computing the IHT is an approximate way of frequency domain filtering similar to the Fourier transform-based convolutional filtering. The proof of Theorem\u00a0<ref> is presented in\u00a0<cit.>.\n\nIn brief, convolution theorems have the property that convolution in the time or space domain is equivalent to element-wise multiplication in the transform domain. This property inspires us to design the neural network layers to replace the Conv2D layers in a deep neural network. \n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Transform-Based Perceptron Layers\n\nFig.\u00a0<ref> presents the structures of the proposed transform-based perceptron layers. We propose two types of perceptron layers for each transform. One is a single-channel layer as shown in Fig.\u00a0<ref>, and the other is a multi-channel layer as shown in Fig.\u00a0<ref>. The multi-channel layer contains multiple times parameters as the single-channel layer, but it can produce a higher accuracy according to our experiments. In the transform domain of each channel, a scaling layer performs the filtering operation by element-wise multiplication. Then a 1\u00d7 1 Conv2D layer performs channel-wise filtering.\nAfter this step, there is a trainable soft-thresholding layer denoising the data and acting as the nonlinearity. In the single-channel layer, a shortcut connection is implemented. In the multi-channel layer, the outputs of all channels are summed up. We do not implement the shortcut connection in the multi-channel layer because there are multiple channels replacing the shortcut and they effectively transmit the derivatives to earlier layers. The 2D transform and its inverse along the width and height convert the tensor between the space domain and the transform domain. \n\n\n\n\n\n\n\nThe scaling layer is derived from Theorems\u00a0<ref> and\u00a0<ref>, which state that the convolution in the space domain is equivalent to the element-wise multiplication in the transform domain. In detail, the input tensor in \u211d^C\u00d7 W \u00d7 H is element-wise multiplied with a weight matrix in \u211d^W \u00d7 H to perform an operation equivalent to the spatial domain convolutional filtering. \n\nThe trainable soft-thresholding layer is applied to remove small entries in the transform domain. It is similar to image coding and transform-domain denoising. It is defined as:\n\n    \ud835\udc32 = \ud835\udcae_T(\ud835\udc31) = sign(\ud835\udc31)(|\ud835\udc31|-T)_+,\n\nwhere, T is a non-negative trainable threshold parameter, (\u00b7)_+ stands for the ReLU function. An input tensor in \u211d^C\u00d7 W \u00d7 H is computed with threshold parameters in \u211d^W \u00d7 H, and these threshold parameters are determined using the back-propagation algorithm. The ReLU function is not suitable because both positive and negative values are important in the transform domain. A completely positive waveform can have both significant positive and negative values in the transform domain. Our ablation study on the CIFAR-10 dataset experimentally shows that the soft-thresholding function is superior to the ReLU function in the transform-based layers.\n\nThe overall process of each channel for the transform domain analysis (scaling, 1\u00d71 Conv2D, and soft-thresholding) is depicted in Figure\u00a0<ref>. The single-channel and the P-channel transform-based perceptron layers are computed as:\n\n    \ud835\udc32=\ud835\udcaf^-1(\ud835\udcae_\ud835\udc13(\ud835\udcaf(\ud835\udc31)\u2218\ud835\udc00\u229b\ud835\udc15))+\ud835\udc31,\n\n\n    \ud835\udc32=\ud835\udcaf^-1(\u2211_i=0^P-1\ud835\udcae_\ud835\udc13_i(\ud835\udcaf(\ud835\udc31)\u2218\ud835\udc00_i\u229b\ud835\udc15_i)),\n\nwhere, \ud835\udcaf(\u00b7) and \ud835\udcaf^-1(\u00b7) stand for 2D transform (DCT, HT, or BWT) and 2D inverse transform, \ud835\udc00 and \ud835\udc00_i are the scaling matrices, \ud835\udc15 and \ud835\udc15_i are the kernels in the 1\u00d71 Conv2D layers, \ud835\udc13 and \ud835\udc13_i are the threshold parameter matrices in the soft-thresholding, \u2218 stands for the element-wise multiplication, and \u229b represents the 1\u00d7 1 2D convolution over an input composed of several input planes performed using PyTorch's Conv2D API. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Comparison with the Convolutional Layer\n\n\n\n  \u00a7.\u00a7.\u00a7 Spatial and Channel Characteristics Comparison\n\nThe Conv2D layer has spatial-agnostic and channel-specific characteristics. Because of the spatial-agnostic characteristics of a Conv2D layer, the network cannot adapt different visual patterns corresponding to different spatial locations. On the contrary, the transform-based perceptron layer is location-specific and channel-specific. The 2D transform is location-specific but channel-agnostic, as it is computed using the entire block as a weighted summation on the spatial feature map. The scaling layer is also location-specific but channel-agnostic, as different scaling parameters (filters) are applied on different entries, and weights are shared in different channels. That is why we also use PyTorch's 1\u00d7 1 Conv2D to make the transform-based perceptron layer channel-specific.\n\nFigure\u00a0<ref> shows the comparison of the spatial and channel characteristics. Because the Conv2D layer is spatial-agnostic, the convolutional kernels cannot utilize the information from the location of the pixels. Conversely, the transform-based perceptron layer is location-specific, which means each weight parameter gains the location information. \n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Number of Parameters and MACs Comparison\n\nThe comparison of the number of parameters and  Multiply\u2013Accumulate (MACs) is presented in Table\u00a0<ref>. In a Conv2D layer, we need to compute N^2K^2 multiplications for a C-channel feature map of size N\u00d7 N with a kernel size of K\u00d7 K. In the DCT-perceptron layer, there are N^2 multiplications from the scaling and N^2 multiplications from the 1\u00d71 Conv2D layer. There is no multiplication in the soft-thresholding function because the product between the sign of the input and the subtraction result can be implemented using the copysign operation. \nAlthough the 2D-DCT and the 2D-IDCT can be implemented in O(N^2Clog_2N) using the fast algorithm\u00a0<cit.>, this fast approach is not officially supported in PyTorch. In this paper, we implement the 2D-DCT and the 2D-IDCT using the product between the DCT/IDCT weight matrix and the input tensor. Its complexity is O(N^3C), and its MACs are 2N^3C. Therefore, A P-channel DCT-perceptron layer has 2PN^2+PC^2 parameters with 4N^2C+PN^2C^2+PN^2C MACs. \n\nThe BWT-perceptron layer has the same number of parameters and MACs as the DCT-perceptron layer. As for the HT-perceptron layers, because the HT is a multiple-free transform, there are only 2PN^2+PC^2 parameters with PN^2C+PN^2C^2 MACs in the P-channel HT-perceptron layer. A 3-channel perceptron layer has the same amount of parameters but fewer MACs compared to a 3\u00d7 3 Conv2D layer if C=N. Therefore, in the experiment section, we mainly use the 3-channel perception layer to compare with the Conv2D layer. Furthermore, in most main-stream CNNs, C is usually much larger than N in the hidden layers, then our proposed perceptron layers can reduce parameters and computational cost for these CNNs even with the matrix-vector product for the transforms.\n\n\n\n \u00a7.\u00a7 Introduce Transform-based Perceptron Layers into ResNets\n\nIn this paper, we introduce the proposed layers into ResNet-18, ResNet-20, and ResNet-50. \nFig.\u00a0<ref> shows the commonly used convolutional blocks in ResNets. The V1 version that contains two 3\u00d7 3 Conv2D layers are used in ResNet-18 and ResNet-20, and the V2 version that contains one 3\u00d7 3 and two one 1\u00d7 1 Conv2D layers is used in ResNet-50. As Fig.\u00a0<ref> shows, we replace the second 3\u00d7 3 Conv2D layer in each convolutional block V1, and the 3\u00d7 3 Conv2D layer in some convolutional blocks V2.\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo revise ResNet-20, ResNet-18, and ResNet-50, we retain the first 3\u00d7 3 Conv2D layer, then we replace those 3\u00d7 3 Conv2D layers at the even indices (the second Conv2D in each convolutional block in ResNet-20 and ResNet-18, and Conv2_2, Conv3_2, Conv3_4, etc., in ResNet-50) with the proposed transform-based perception layers. We keep the 3\u00d73 Conv2D layers at odd indices because, in this way, we use the regular 3\u00d7 3 Conv2D layer and the proposed HT-perceptron layer by turns, then the network can extract features in different manners efficiently. Table\u00a0<ref> describes the method we revising the ResNet-20 for the CIFAR-10 classification task, and Tables\u00a0<ref> and\u00a0<ref> present the revised ResNet-18 and ResNet-50 for the ImageNet-1K classification task.\n\n\n\n \n\nIn the following sections, to call the revised ResNets, we add PC-Transform in front of their name. For example, 3C-DCT-ResNet-20 is the ResNet-20 revised by the three-channel DCT-perceptron layer. The BWT-ResNets are implemented using Bior 1.3 coefficients\u00a0<cit.>.\n\n\n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n\nOur experiments are carried out on a workstation computer with an NVIDIA RTX 3090 GPU. The code is written in PyTorch in Python 3. The experiments include the CIFAR-10 classification task and the ImageNet-1K classification task. In the CIFAR-10 experiments, ResNet-20 is used as the backbone network. In the ImageNet-1K experiments, ResNet-18 and ResNet-50 are used as the backbone networks. The CIFAR-10 experiments carry out our ablation study. Fig.\u00a0<ref> presents the test error history of our models. \n\n\n\n\n\n\n \u00a7.\u00a7 CIFAR-10 Experiments\n\nTraining ResNet-20 and its revisions follow the implementation in\u00a0<cit.>. In detail, we use an SGD optimizer with a weight decay of 0.0001 and momentum of 0.9. Models are trained with a mini-batch size of 128 for 200 epochs. The initial learning rate is 0.1, and the learning rate is reduced by a factor of 1/10 at epochs 82, 122, and 163, respectively. Data augmentation is implemented as follows: First, we pad 4 pixels on the training images. Then, we apply random cropping to get 32 by 32 images. Finally, we randomly flip images horizontally. We normalize the images with the means of [0.4914, 0.4822, 0.4465] and the standard variations of [0.2023, 0.1994, 02010]. During the training, the best models are saved based on the accuracy of the CIFAR-10 test dataset, and their accuracy numbers are reported in Table\u00a0<ref>. \n\n\n\n\nAs Table\u00a0<ref> shows, on the one hand, DCT-ResNet-20, which is implemented by revising Conv2D layers using the single-channel DCT-perceptron layers, contains 44.39% fewer parameters than the baseline ResNet-20 model, but it only suffers a 0.07% accuracy lost. On the other hand, the 3C-DCT-ResNet-20, which is implemented using the tri-channel DCT-perceptron layers, contains 22.64% fewer parameters than the baseline ResNet-20 model, but it even reaches a 0.09% higher accuracy. The 1C-HT-ResNet-20 and the 3C-HT-ResNet-20 have the same parameter savings (44.39% and 26.64%) as the 1C-DCT-ResNet-20 and the 3C-DCT-ResNet-20, and their accuracy losses are both less than 0.6% compared to the baseline model. The reason that the HT models get lower accuracy results than the corresponding DCT models is that HT is a binary transform, which means its ability for frequency information extraction is weaker than the DCT. As compensation, fewer computational cost is required to compute those HT models. Figure\u00a0<ref> shows the test error history of the ResNet-20, the 3C-HT-ResNet-20, and the 3C-DCT-ResNet-20 on the CIFAR-10 test dataset.\n\nCompare to our previous HT-based work in\u00a0<cit.> which is also single-channel, the proposed 1C-HT-ResNet-20 reaches a 1.03% higher accuracy. This is because the HT layer in\u00a0<cit.> is channel-agnostic, while the proposed layer is channel-specific. Therefore, the proposed layers can utilize channel-wise information.\n\nAnother application for the proposed layers is that we can insert one additional layer before the global average pooling layer in the ResNets (the location is after the output of the final convolutional block). For example, an extra DCT-perceptron layer improves the accuracy of the regular ResNet-20 to 91.82%. This method is named ResNet-20+1C-DCT-P.\n\n\n\n\nTable\u00a0<ref> presents our ablation study on the CIFAR-10 dataset. First, we remove the residual design (the shortcut connection) in the single-channel DCT-perceptron layers and we obtain a worse accuracy (91.12%). Therefore, the shortcut connection can improve the performance of the single-channel DCT-perceptron layer. Then, we remove scaling which is the convolutional filtering in the single-channel DCT-perceptron layers. In this case, the multiplication operations in the single-channel DCT-perceptron layer are only implemented in the 1\u00d7 1 Conv2D. The accuracy drops from 91.59% to 90.46%. Therefore, scaling is necessary to maintain accuracy, as it makes use of the convolution theorems. Next, we apply ReLU instead of soft-thresholding in the single-channel DCT-perceptron layers. We first apply the same thresholds as the soft-thresholding thresholds on ReLU. In this case, the number of parameters is the same as in the proposed 1C-DCT-ResNet-20 model, but the accuracy drops to 91.30%. We then try regular ReLU with a bias term in the 1\u00d7 1 Conv2D layers in the single-channel DCT-perceptron layers, and the accuracy drops further to 91.06%. These experiments show that the soft-thresholding is superior to the ReLU in this work, as the soft-thresholding retains negative frequency components with high amplitudes, which are also used in denoising and image coding applications. Furthermore, we replace all Conv2D layers in the ResNet-20 model with the single-channel DCT-perceptron layers. We implement the downsampling (Conv2D with the stride of 2) by truncating the IDCT2D so that each dimension of the reconstructed image is one-half the length of the original. In this method, 81.31% parameters are reduced, but the accuracy drops to 85.74% due to lacking parameters. \n\n\nIn the ablation study on the multi-channel DCT-perceptron layer, we further try the structures with 2/4/5 channels, but their accuracy results are all lower than the tri-channel. A similar ablation study is taken on the HT-perceptron layer, and the best accuracy result is obtained using 5 channels. Normally, the more channels the layer has, the better performance it can reach because of the more parameters. However, too many channels may cause redundancy and make it hard to train the layer well using the back-propagation algorithm. \n\n\nFurthermore, we add the shortcut connection in the tri-channel DCT-perception layer, but our experiments show that the shortcut connection is redundant in the tri-channel DCT-perception layer. This may be because three channels are replacing the shortcut. The multi-channel structure allows the derivatives to propagate to earlier channels better than the single-channeled structure. \n\n\n\n    \n\n\n\n \u00a7.\u00a7 ImageNet-1K Classification\n\nIn this section, we employ PyTorch's official ImageNet-1K training code\u00a0<cit.>. Because we use the PyTorch official training code with the default training configuration for the revised ResNet-18s training, we use PyTorch Torchvision's officially trained ResNet-18 model as the baseline. On the other hand, the default training for ResNet-50 requires about 26\u201330 GB GPU memory, while an NVIDIA RTX3090 GPU only has 24GB. To overcome the issue of memory deficiency, we halve the batch size and the learning rate correspondingly. We use an SGD optimizer with a weight decay of 0.0001 and momentum of 0.9 for 90 epochs. Revised ResNet-18's are trained with the default setting: a mini-batch size of 256, and an initial learning rate of 0.1. ResNet-50 and its revised models are trained with a mini-batch size of 128, the initial learning rate is 0.05. The learning rate is reduced by a factor of 1/10 after every 30 epochs. For data argumentation, we apply random resized crops on training images to get 224 by 224 images, then we randomly flip images horizontally. In testing on the ImageNet-1K validation dataset, we resize the images to make the smaller edge of images 256, then apply center crops on the resized testing images to get 224 by 224 images. We normalize the images with the means of [0.485, 0.456, 0.406] and the standard variations of [0.229, 0.224, 0.225], respectively. We evaluate our models on the ImageNet-1K validation dataset and compare them with the state-of-art papers. During the training, the best models are saved based on the center-crop top-1 accuracy on the ImageNet-1K validation dataset, and their accuracy numbers are presented in Table\u00a0<ref>. Furthermore, we carry out experiments with the input size of 256 by 256, for 256 is the smallest integer power of 2 which is larger than 224. The training strategies except for the image size are the same as the experiments with the image size of 224 by 224. We resize the validation images to make the smaller edge 292, then apply center crops on the resized testing images to get 256 by 256 images.\nIn Table\u00a0<ref>, the input sizes in the last 4 rows are 256\u00d7256, and in other rows are 224\u00d7 224. \nFig.\u00a0<ref> shows the test error history of the ResNet-50 and our revised ResNet-50s on the ImageNet-1K validation dataset.\n\nWhen the input size is 224 by 224, the 3C-HT-ResNet-50 and the 3C-BWT-ResNet-50 have more parameters and more MACs than the 3C-DCT-ResNet-50. This is because the input size is not a pair of integers power of 2, so the hidden tensors' sizes in the HT-perceptron layer and the BWT-perceptron layer are larger than in the DCT-perceptron layer. Although the HT is still more efficient than the DCT, the scaling, 1\u00d7 1 Conv2D, and soft-thresholding in the HT-perceptron layers need to be computed with tensors with larger sizes. Furthermore, if the input size satisfies the constraint of integers power of 2, the 3C-HT-ResNet-50 takes less computational cost than the 3C-DCT-ResNet-50, and the 3C-BWT-ResNet-50 takes the same computational cost as the 3C-DCT-ResNet-50. As Table\u00a0<ref> presents, all transform-based perceptron layers manage to improve the accuracy for ResNet-50 by reducing the number of parameters and the MACs. For example, the 3C-BWT-ResNet layer improves the center-crop top-1 accuracy of ResNet-50 from 76.18% to 77.07%, while 11.5% parameters and MACs are reduced.\n\n\n\nOther state-of-art transform-based methods are listed in Table\u00a0<ref> for comparison. \nIn FFC-ResNet-50 (+LFU, \u03b1=0.25)\u00a0<cit.>, authors use their proposed Fourier Units (FU) and local Fourier units (LFU) to process only 25% channels of each input tensor and use the regular 3\u00d7 3 Conv2D to process the remaining to get the accuracy of 77.8%. Although this method improves the accuracy, it also increases the number of parameters and MACs. We can also use a combination of regular the 3\u00d7 3 Conv2D layers and our proposed layers to increase the accuracy by increasing the parameters in their method. However, according to the ablation study in\u00a0<cit.>, if all channels are processed by their proposed FU, FFC-ResNet-50 (\u03b1=1) only gets the accuracy of 75.2%. The reported number of parameters and MACs are much higher than the baseline in\u00a0<cit.> even though MACs from the FFT and IFFT are not counted. Our methods improve accuracy for regular ResNet-50s with significantly reduced parameters and MACs. Models in\u00a0<cit.> were trained with 10 additional epochs. Our results can also be improved with further training. Pruning-based methods\u00a0<cit.> can be implemented as a part of our method to prune some layers because we implement convolutions in the transform domain. For example, removing convolutional filtering is equivalent to removing the corresponding DCT, scaling, soft-thresholding, and inverse DCT operation.\n\nThe experiments of inserting one additional layer before the global average pooling layer in the ResNet-18 and the ResNet-50 are presented in Table\u00a0<ref>. The additional single-channel DCT-perceptron layer improves the accuracy of ResNet-18 and ResNet-50 respectively with negligible extra parameters and computational cost.\n\n\n\n \u00a7.\u00a7 Wall-Clock Time\n\nAlthough in this work, we do not apply any CUDA-based coding to accelerate the proposed layers, it is worthy to present the wall-clock time of the models. Specially, we find a CUDA implementation for HT at <cit.>, so we implement HT using it in this section. DCT and BWT are still implemented as a matrix-vector product without taking advantage of the fast (O(N^2log_2 N)) algorithm\u00a0<cit.> because PyTorch does not support DCT and the wavelet transforms at the moment. Scaling and Soft-thresholding are also implemented using pure PyTorch. \n\nSince we use the matrix-vector product to implement DCT and BWT, the time consumption of each transform is the same if the padding is not needed (i.e., the input size is 32^2 or 256^2). The speed test is performed on the NVIDIA RTX3090 GPU using the batch size of 1. The CPU of the computer is Intel(R) Xeon(R) W-2255 CPU @ 3.70GHz. We first warm up the device by 10 inference times, then compute the average inference time of 100  inferences. The wall-clock time is presented in Table\u00a0<ref>. Even without any CUDA-based optimization, our transform-based ResNet models take less than 3\u00d7 time that of the regular ResNet models which are well-optimized using CUDA. The time differences are small as shown in Table\u00a0<ref>. Furthermore, we calculate the transforms of the entire tensor in the current work, but we can calculate them using smaller blocks such as 8\u00d78 blocks as in JPEG and MPEG video coding methods to reduce memory usage and latency. \n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\nIn this paper, we proposed a set of novel layers based on the discrete cosine transform (DCT), the Hadamard transform (HT), and the biorthogonal wavelet transform (BWT) to replace some of the 3\u00d7 3 Conv2D layers in convolutional neural networks (CNNs). All of our transform domain layers can be implemented with single-channel and multi-channel structures. The idea of this work is derived from the convolution theorems, that the convolution in the time and the space domain is equivalent to the element-wise multiplication in the transform domain. With the proposed layers, CNNs such as ResNets can obtain comparable or even higher accuracy results in image classification tasks with much fewer parameters. ResNets with the HT-perceptron layers usually produce a little lower accuracy than ResNets with the DCT-perceptron layers. This is because as a binary transform, the ability for frequency analysis of the HT is inferior to the DCT. As compensation, there is no multiplication to implement HT, while multiplications between the tensors and the DCT weights are required in DCT. Another application for the proposed layers is the addition of an extra layer to the regular ResNets. An additional transform domain layer before the global average pooling layer improves the accuracy of ResNets with a negligible increase in the number of parameters and computational time. In addition to ResNets,\nthe proposed transform domain layers can be used in any convolutional neural network using the Conv2D layers. \nIEEEtran\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}