{"entry_id": "http://arxiv.org/abs/2303.07064v1", "published": "20230313123807", "title": "A Generalized Multi-Modal Fusion Detection Framework", "authors": ["Leichao Cui", "Xiuxian Li", "Min Meng", "Xiaoyu Mo"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI"], "text": "\n\n\n\nMSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID\n    Jianyang Gu^1 Kai Wang^2 Hao Luo^3 Chen Chen^4 Wei Jiang^1*\n Yuqiang Fang^5 Shanghang Zhang^6 Yang You^2 Jian Zhao^7Co-corresponding authors \n\n^1Zhejiang University ^2National University of Singapore ^3Alibaba Group\n\n^4OPPO Research Institute ^5Space Engineering University ^6Peking University\n\n^7Institute of North Electronic Equipment \n\n{gu_jianyang, jiangwei_zju}@zju.edu.cn zhaojian90@u.nus.edu\n\n\n\n\n\n\n\n\n\n\n\n    March 30, 2023\n=============================================================================================================================================================================================================================================================================================================================================================================================================================\n\nempty\nempty\n\n\n\n\n\nLiDAR point clouds have become the most common data source in autonomous driving. However, due to the sparsity of point clouds, accurate and reliable detection cannot be achieved in specific scenarios. Because of their complementarity with point clouds, images are getting increasing attention. Although with some success, existing fusion methods either perform hard fusion or do not fuse in a direct manner. In this paper, we propose a generic 3D detection framework called MMFusion, using multi-modal features. The framework aims to achieve accurate fusion between LiDAR and images to improve 3D detection in complex scenes. Our framework consists of two separate streams: the LiDAR stream and the camera stream, which can be compatible with any single-modal feature extraction network. The Voxel Local Perception Module in the LiDAR stream enhances local feature representation, and then the Multi-modal Feature Fusion Module selectively combines feature output from different streams to achieve better fusion. Extensive experiments have shown that our framework not only outperforms existing benchmarks but also improves their detection, especially for detecting cyclists and pedestrians on KITTI benchmarks, with strong robustness and generalization capabilities. Hopefully, our work will stimulate more research into multi-modal fusion for autonomous driving tasks. https://github.com/achao-c/MMFusionCode. \n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nWith the development of autonomous driving, 3D object detection is getting more and more attention. Its goal is to classify and locate objects in 3D space accurately. Recently, LiDAR and cameras have been the primary sources of information for 3D object detection.\n\nAs point clouds can provide accurate depth information about objects, in the beginning, researchers mainly used LiDAR-based methods <cit.> for object detection. However, point clouds have less color or semantic information due to sparsity. As a result, small, distant, or obscured objects are challenging to detect in the single LiDAR mode because of the lack of sufficient points.\n\nImages, on the other hand, can be a powerful addition to LiDAR information because they have a lot of semantic information but no depth information. Therefore, multi-modal fusion is of great importance for accurate and reliable perception.\n\n \n\nAs shown in Fig <ref>, existing LiDAR-camera fusion methods can be broadly classified into result-level, feature-level, and point-level fusion. Result-level fusion methods <cit.> fuse the results of different detectors. Having been constrained by the results of each sensor, they are relatively ineffective in terms of detection and, therefore, rarely used nowadays. This paper focuses on illustrating the other two categories.\n\nPoint-level fusion methods <cit.> establish links between point clouds and pixels, primarily through calibration matrices, so that data of one kind can be projected into another space, enhancing the data representation. Although some success has been achieved, the coarse mapping relationships inevitably introduce a degree of interference into the network. Also, due to the difference between point clouds' sparsity and pixels' density, only a little information will complete the matching fusion, resulting in a severe loss of information. Feature-level fusion methods use different modal networks to obtain the corresponding modal data features and subsequently fuse them. Effectively combining the features of other modalities is a crucial factor limiting detection performance. Specifically, Chen et al. <cit.> utilize coarse geometric relationships to fuse features, obtaining suboptimal fusion features. Liu et al. <cit.> exploit a transformation network to construct 3D features of multi-view images. Although effective, these methods can only be applied to vehicles with both LiDAR and multi-view cameras. Also, the 3D information constructed using the multi-view map has some duplication with the information contained in the point clouds.\n\nTo address the above issues, we propose a generic multi-modal 3D detection framework, i.e., MMFusion, in this paper. It contains two separate data streams: the LiDAR and Camera streams. To improve the LiDAR stream's performance, a plug-and-play module called the Voxel Local Perception Module (VLPM) has been designed so that voxels can fully express internal local information. Furthermore, the feature output from both streams is fused by the Multi-modal Feature Fusion Module (MFFM), and the output is then obtained using the detection head.\n\nSpecifically, the detection framework proposed here employs different data streams to encode the raw inputs into features within their respective feature spaces. As our framework is a generic approach, it can incorporate the current single-modal models of LiDAR and cameras into its respective streams. We also find that the voxel features previously obtained using local averages <cit.> do not accurately reflect the actual voxel features. This motivates us to design a dynamic selection module for local features based on the attention mechanism, which can autonomously select the features of interest to enhance the representation of the model. In order to find precise connections between point cloud features and image features, we introduce MFFM to fuse features of different modalities. It avoids the direct use of hard or coarse fusion using geometric relations in previous approaches, and adaptively selects the desired part for fusion. Extensive experiments on the KITTI open dataset have shown that our approach can perform better than current state-of-the-art methods. Our contributions are summarized as follows:\n\n\n  * A generic detection network framework for multi-modal data fusion (i.e., LiDAR-camera fusion) is proposed that can leverage multiple existing network architectures.\n\n  * A plug-and-play module VLPM is proposed for fully acquiring local features during voxelization, which enhances the representability of voxel grids.\n\n  * We propose a multi-modal fusion module MFFM to adaptively select different parts of different modal features for fine-grained fusion, which can make full use of the complementary properties of multi-modal features.\n\n  * Our framework MMFusion outperforms existing benchmarks and improves their detection on the KITTI dataset, especially for small objects.\n\n\n\n\n\u00a7 RELATED WORKS\n\n\n\n\n \u00a7.\u00a7 LiDAR-based 3D Object Detection\n\n\nThere are two categories based on how the point clouds are handled: voxel-based and point-based methods.\n\nVoxel-based methods. Voxel-based methods first voxelize the original point clouds using local averages <cit.>, then utilize 3D convolution kernel <cit.> to extract voxel features into the bird's eye view (BEV), and finally use a one-stage detection head <cit.> to detect objects on projected BEV plane. Some other works add a refinement network <cit.> to the existing one-stage detection network. Recent works <cit.> employ the attention mechanism for feature extraction and target detection, and Deng et al. <cit.> use the range view (RV) features to augment the BEV features. These methods are faster but do not exploit all the original information because of voxelization.\n\nPoint-based methods. Point-based methods take the original point clouds directly as input, then apply the point cloud encoder <cit.> for feature extraction to feed into the detection network <cit.>. Point-based approaches are usually computationally expensive as the encoder directly processes the original point clouds.\n\n\n\n \u00a7.\u00a7 Image-based 3D Object Detection\n\n\nLiDAR sensors are expensive compared to cameras, which are easily available and inexpensive. In recent years, image-based 3D object detection has been considerably explored. Wang et al. <cit.> extend image detectors with additional 3D branches using monocular images. With the advent of autonomous driving datasets <cit.> and more camera sensors, works <cit.> with multi-view images are gradually emerging. Due to the lack of direct depth information in the images, researchers have found that accurately predicting object depth from object and scene cues becomes a key constraint on detection capabilities. The methods <cit.> rely on a depth estimation network to extract implicit depth information and thus improve the network's effectiveness.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Multi-modal 3D Object Detection\n\n\nDue to the complementary nature of the information in point clouds and images, multi-sensor fusion methods are gradually gaining attention. Current methods are divided into result-level, point-level, and feature-level fusion shown in Fig <ref>. The latter two fusion methods are currently used primarily due to limitations in the performance of individual detectors.\n\nPoint-level fusion methods. Methods <cit.> typically combine LiDAR points with image features at the corresponding location and apply LiDAR-based detection to the decorated point clouds. This fusion relies on coarse positional correspondence and has a high information loss <cit.> due to the sparsity of the point clouds.\n\nFeature-level fusion methods. These methods first exploit different networks to obtain features of different modal data and then use the fusion network to obtain fused features for detection. Specifically, the works <cit.> perform a coarse fusion of features based on geometric relationships at corresponding locations. In <cit.>, a transformation network is leveraged to transfer image features to the 3D space before fusing them with point-level element features. However, the switching network has additional computing costs. Wu et al. <cit.> converts the image into pseudo point clouds to extract features and then fuses them with the point cloud features. Furthermore, an attention mechanism is employed in <cit.> to get different weights for features to be fused.\n\n\n\n\u00a7 METHOD\n\n\nIn this section, we will detail the overall structure of our network, which is a framework for 3D target detection based on multi-modal data fusion. As shown in Fig <ref>, the network mainly consists of independent point clouds and image data streams, a multi-modal fusion module, and a 3D object detection head. Given data input from different modalities, we design specific encoders to extract features separately. Then we use the Multi-modal Data Fusion Module to dynamically extract different modal features, which can alleviate the misalignment problem of different modal features. Rather than using deep building networks, the network can achieve faster detection speeds. More details are elaborated in the following subsections.\n\n\n\n \u00a7.\u00a7 Voxel Local Perception Module\n\nDue to the sparse characteristic of point clouds, extracting features directly from point clouds is computationally intensive. Therefore, methods <cit.> first voxelize point clouds and then extract features from them. The details are as follows:\n\nThe point clouds in the range (W, H, D) are first divided into voxel meshes of the same size, each with a size of (w, h, d). To facilitate processing, set the number of point clouds sampled in each voxel grid as N. Finally, a maximum pooling operation or an average pooling operation is performed for each voxel grid.\n\nAlthough effective, direct pooling of voxel grids inevitably results in a significant loss of local information. Using only one pooling operation is not enough to accurately obtain the local features within a voxel grid, and the weight of each point within the voxel grid is also an important aspect to think about. As a result, in order to retain as much of the required point information as possible, we propose the Voxel Local Perception Module (VLPM), as shown in Fig <ref>. Our Voxel Local Perception Module consists of Point Attention Module and Dynamic Weights Module.\n\n\n\nPoint Attention Module. To adaptively obtain local information from other points in the same voxel grid, we design a simple but effective module inspired by <cit.>. \n\nAs shown in Fig <ref>, an FC block contains two linear layers and a ReLU non-linear layer. For each voxel grid V_k\u2208\u211d^N \u00d7 4, we use FC blocks \u03b1 and \u03b2 that act on the spatial coordinates c_i=[x_i, y_i, z_i] of each point to obtain Query Q_i and Key K_i of the self-attention mechanism. At the same time, point features v_i\u2208 V_k are fed into another FC block \u03b3 to obtain Value V_i, which is\n\n    Q_i = \u03b1(c_i),  K_i = \u03b2(c_i), V_i = \u03b3(p_i). (1)\n\n\nAfterward, we use an FC block \u03f5 to establish the similarity link between the points p_i and p_j under the same voxel grid (i.e., using additive attention to obtain the correlation coefficient):\n\n    w_ij = \u03f5(Q_i - K_j + p_ij), (2)\n\nwhere p_ij represents the position encoding between p_i and p_j from an FC block \u03b4:\n\n    p_ij = \u03b4(c_i - c_j). (3)\n\n\nConsequently, for points in the same voxel grid V_k, one can get the new features of point p_i:\n\n    f_i = \u2211_p_j \u2208\ud835\udcb1_k w_ij\u2299 V_j. (4)\n\n\nDynamic Weights Module. Different points within the same pixel grid usually have different weights (e.g., object edge points require a higher weight than background points). Inspired by this observation, we dynamically obtain the corresponding weights based on the point's coordinate values, as shown in Fig <ref>. \n\nUnlike Point Attention Module, we use the average point p\u0305=[x\u0305, y\u0305, z\u0305, ...] of the current voxel grid V_k as the initial source of the Query. Then we regard the point p_j within the same voxel grid as Key and Value, thus obtaining the features of the current voxel grid f_V. That is, \n\n    f_V = \u2211_p_j \u2208\ud835\udcb1_k (\u03b8(\u03b6(c\u0305) - \u03b7(c_j))) \u2299 p_j.  (5)\n\n\nHere, the Dynamic Weights Module does not utilize the position coding information since, generally speaking, the Point Attention Module has already given the point clouds location information.\n\n\n\n \u00a7.\u00a7 Data Streams\n\nDifferent data stream architectures extract data features from different modalities separately.\n\nLiDAR Stream. LiDAR point clouds can provide accurate spatial information. Objects on the road do not generally overlap vertically, so the projection of features onto the BEV plane does not cause interference. The methods <cit.> of extracting voxel features to obtain BEV features have become the de-facto standard.\n\nOur framework can incorporate any feature extraction network to map the 3D features onto a BEV 2D space. We follow the design of <cit.> to build our backbone networks.\n\nCamera Stream. Camera images can provide high-resolution information on shapes and textures. For example, distant and small objects are difficult to detect due to the sparseness of the point clouds, but they are still clear and distinguishable in images.\n\nLike the LiDAR stream, any 2D feature extraction network can be added to our framework to extract images' rich semantic and textural information. Instead of the widely used convolutional neural network ResNet <cit.>, we use an attention-based representative network called Swin Transformer <cit.> as the backbone of the camera stream. Following <cit.>, we then use a standard Feature Pyramid Network (FPN) to obtain multi-scale features.\n\n\n\n \u00a7.\u00a7 Multi-modal Feature Fusion Module\n\n\nDifferent sensors acquire data from different views; therefore, different modal features are in different feature spaces. For instance, LiDAR stream features are in BEV space, and camera stream features are in RV space (i.e., front, back, left, right).\n\nIn this case, the same element in different modal features might correspond to entirely different spatial locations. So we cannot use element-level operations (like element-wise sum) to directly combine feature maps from different modalities. As such, a Multi-modal Feature Fusion Module is proposed here that transforms different features into a unified feature space.\n\nOverall Architecture. \nAs shown in Fig\u00a0<ref>, we make the LiDAR features dynamically fuse different regional features of interest in the image so that the LiDAR and image features are in the same feature space. Specifically, we use the attention mechanism to construct a correlation of regional features across modalities.\n\nAfter the pooling and adding the position encoding p_Le and p_Ie, LiDAR features f_L\u2208\u211d^C \u00d7 H_1 \u00d7 W_1 and image features f_I\u2208\u211d^C \u00d7 H_2 \u00d7 W_2 will become the Query Q\u2208\u211d^n \u00d7 C and Key K\u2208\u211d^m \u00d7 C by passing convolution layers \u03d5 and \u03c8 as below.\n\n    Q = \u03d5(Pool(f_L)+p_Le),   K = \u03c8(Pool(f_I)+p_Ie). (6)\n\n\n\n\n\nBecause of the large size of the feature map, we use scaled dot-product attention presented in <cit.> instead of additive attention to calculate the correlation coefficient matrix W\u2208 [0, 1]^n \u00d7 m and speed up the computation, as:\n\n    W = softmax(QK^T/\u221a(C)).   (7)\n\n\nAt this step, we can obtain the corresponding image features f_I^' of LiDAR features by a convolution layer \u03b3 and fuse them with an upsampling layer \u03f5 and simple addition for fusion features f_F, i.e.,\n\n    f_F = f_L + \u03f5(f_I^'),  f_I^' = \u03b3(Wf_I + f_I). (8)\n\n\nPooling Operation. \nUsually, the product of H_1W_1 and H_2W_2 is large, causing the network's training to be prolonged. It is also found that too many feature sequences are not conducive to extracting key features, as shown in Sec\u00a0<ref>. \n\nHence, we first pool the features f_L, f_I to smaller sizes (H_1^', W_1^'), (H_2^', W_2^'). The fused features f_F are then scaled to their original size (H_1, W_1) and then added to LiDAR features f_L.\n\n\n\n \u00a7.\u00a7 Detection Head and Loss Function\n\nAfter the series of operations mentioned above, we can obtain the fused feature map. The network generates results through the downstream task-specific heads. In the field of 3D target detection, we use the detection head of <cit.> to verify the validity of our network.\n\nFor the one-stage detection head, inspired by <cit.>, we adopt the Region Proposal Network (RPN) loss \u2112_RPN, which consists of anchor classification loss \u2112_cls, regression loss \u2112_reg, and direction classification loss \u2112_dir. In short,\n\n    \u2112_total = \u2112_RPN = \u2112_cls + \u03b1\u2112_reg + \u03b2\u2112_dir,\n    (9)\n\nwhere \u03b1 is set to 2.0 and \u03b2 is set to 0.2.\n\nFor the two-stage detection head, Proposal Refinement Network (PRN) loss \u2112_PRN will be part of the loss function following <cit.>. In brief,\n\n    \u2112_total = \u2112_RPN + \u2112_PRN. (10)\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n \u00a7.\u00a7 Dataset\n\n\nKITTI <cit.> has now been widely used as a benchmark for autonomous driving datasets. It contains three categories in total: cars, cyclists, and pedestrians. Each category is divided into three difficulty levels: easy, medium, and hard, based on the size, occlusion level, and truncation level of the objects. The total dataset contains 7481 training samples and 7518 test samples. For validation purposes, the training samples are typically divided into a training set of 3712 and a validation set of 3769. We use the training set for the experimental study of the validation test and test set.\n\n\n\n \u00a7.\u00a7 Setup Details\n\n\n\n\n\n\n\nData augmentation. \nData augmentation is performed to improve the generalization performance of the network. Following <cit.>, we first extract some ground truth boxes and place them at random locations. Then, the point clouds are randomly flipped along the x-axis, rotated along the z-axis in the range [-\u03c0/4, \u03c0/4], and scaled in the range [0.95, 1.05].\n\nInput Parameters.\nSince KITTI only provides annotations in the front camera's field of view. We set the range of point clouds to [0, 70.4m], [-40m, 40m], [-3m, 1m] in the (x, y, z) axis following <cit.>. The input voxel size is set to (0.05m, 0.05m, 0.1m). So the size of the whole 3D space after voxelization is 1600 \u00d7 1408 \u00d7 40. The maximum number of non-empty voxels is\nset to 16,000 in training and 40,000 in testing. We reshape the dimensions of the images to 3 \u00d7 1216 \u00d7 352 and take them as the input to the image stream.\n\nNetwork Architecture. The backbone of LiDAR Stream is based on SECOND <cit.>. We add the Voxel Local Perception Module with two stages before the backbone network to acquire voxel features. The 3D backbone has four stages with filter numbers of 16, 32, 48, and 64. We then utilize two 2D network blocks to process the features. The first block has the same X and Y resolution as the 3D backbone output, while the second block has half the resolution of the first. Moreover, the LiDAR Stream features have a size of 256 \u00d7 200 \u00d7 176.\n\n\n\n\nWe employ Swin Transformer <cit.> with four blocks as the backbone of the Camera Stream. The window size is changed to M=5, and the channel number of the first stage's hidden layers is changed to C=32. The feature dimension of each block of output is 32, 64, 128, and 256, and the size becomes one-half of its original size with each passing block. After that, we leverage a Feature Pyramid Network <cit.> to assemble them and grab the image features with a size of 256 \u00d7 39 \u00d7 11.\n\n\nData from different modalities is fed into the MFFM for fusion. Finally, downsampling and upsampling 2D networks are employed with feature dimensions (128, 256) to obtain fusion features whose size is 256 \u00d7 200 \u00d7 176. \n\nTo verify the validity and generality of our framework, different detection heads <cit.> are used in the experiments.\n\nTraining.\nOpenPCDet <cit.> is adopted as our codebase. All the models are trained on two 3090Ti GPU cards with a batch size of 4 and 80 epochs. We keep the same training settings as for the baseline detector. Specifically, the Adam optimizer is applied with a learning rate of 0.003 for <cit.> and 0.01 for <cit.>. The weight decay parameter of 0.01 is the same for all models.\n\n\n\n\n\n \u00a7.\u00a7 Comparison with the State of the Art\n\nWe evaluate our detection network on the KITTI test set and then submit the results to the official online benchmark server. As shown in Table <ref>, \nour MMFusion outperforms PointRCNN <cit.>, PV-RCNN <cit.>, and EPNet++ <cit.>  with 5.09%, 0.51%, and 1.19% AP|_R_40 respectively on the mean average precision (mAP). As a two-stage detection network, it not only shows good performance in detecting cars, but also performs well in small target detection, which usually needs improvement in LiDAR measurements. In particular, benefiting from the complementary multi-modal information, our network has significantly better detection of cyclists than other competitors.\n\nIn addition, we show the detection results on the KITTI validation set in Table <ref>. On the mAP, our MMFusion outperforms the baseline SECOND <cit.>, PDV <cit.>, and Voxel-RCNN <cit.> by 1.42%, 0.35%, and 0.65% AP|_R_40, respectively. Notably, our MMFusion improves the cyclists' detection performance by 1.34%, 0.55% and 2.06%, respectively, further illustrating the effectiveness of information fusion. Some detection results are shown in Fig <ref>, where the network also maintains good detection results in complex environments.\n\n\n\n \u00a7.\u00a7 Ablation Studies\n \nComponents. By setting SECOND <cit.> as the baseline model, we examine the impact of each component of MMFusion on the network performance. Table <ref> shows that both VLPM and MFFM have a remarkable promotion effect on the model.\n\nPooling Sizes. We set different pooling sizes of LiDAR features in the MFFM, and Table <ref> shows that the smaller pooling size helps to obtain a better performance of the model.\n\n\n\n\u00a7 CONCLUSIONS\n\nIn this paper, we propose a generic 3D detection framework (MMFusion) using multi-modal features. The framework aims to achieve accurate fusion between LiDAR and images to improve 3D detection. We created a voxel local perception module to better preserve internal information during voxelization. Benefiting from the decoupling of data streams, our framework can take full advantage of existing feature extraction and detection networks. A Multi-modal Feature Fusion Module is subsequently designed to selectively combine feature output from different streams to achieve accurate fusion. Extensive experiments have demonstrated that our framework not only achieves superior performance but also improves the detection of existing benchmarks, especially bicycle and pedestrian detection on KITTI benchmarks, with strong robustness and generalization capabilities. We plan to use the framework to adapt to more types of data and downstream tasks in the future. Hopefully, our work will stimulate more research into multi-modal fusion for autonomous driving tasks.\n-2cm   \n                                  \n                                  \n                                  \n                                  \n                                  \n\n\n\n\n\n\n\n\n\n\n\nIEEEtran\n\n"}