{"entry_id": "http://arxiv.org/abs/2303.07257v2", "published": "20230313162902", "title": "The Audio-Visual BatVision Dataset for Research on Sight and Sound", "authors": ["Amandine Brunetto", "Sascha Hornauer", "Stella X. Yu", "Fabien Moutarde"], "primary_category": "cs.RO", "categories": ["cs.RO"], "text": "\n[\n   \\begin@twocolumnfalse\n        This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. \\end@twocolumnfalse\n]\n\n\n\n\nAb initio electron-lattice downfolding: potential energy landscapes, anharmonicity, and molecular dynamics in charge density wave materials\n    Tim\u00a0O.\u00a0Wehling\n    March 30, 2023\n===========================================================================================================================================\n\n\n\nempty\nempty\n\n\n\n\nVision research showed remarkable success in understanding our world, propelled by datasets of images and videos. Sensor data from radar, LiDAR and cameras supports research in robotics and autonomous driving for at least a decade. However, while visual sensors may fail in some conditions, sound has recently shown potential to complement sensor data. Simulated room impulse responses (RIR) in 3D apartment-models became a benchmark dataset for the community, fostering a range of audiovisual research. In simulation, depth is predictable from sound, by learning bat-like perception with a neural network. Concurrently, the same was achieved in reality by using RGB-D images and echoes of chirping sounds. Biomimicking bat perception is an exciting new direction but needs dedicated datasets to explore the potential. Therefore, we collected the BatVision dataset to provide large-scale echoes in  complex real-world scenes to the community. We equipped a robot with a speaker to emit chirps and a binaural microphone to record their echoes. Synchronized RGB-D images from the same perspective provide visual labels of traversed spaces. We sampled modern US office spaces to historic French university grounds, indoor and outdoor with large architectural variety. This dataset will allow research on robot echolocation, general audio-visual tasks  and sound ph\u00e6nomena unavailable in simulated data. We show promising results for audio-only depth prediction and show how state-of-the-art work developed for simulated data can also succeed on our dataset. The data can be downloaded at <https://forms.gle/W6xtshMgoXGZDwsE7>\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nLarge-scale datasets propelled research in past decades, providing first static images and later an abundance of videos for tasks from object detection to activity recognition. \n\n\nSounds, correlated with visual data from their source, provide exploitable information about an action or context, often at marginal computational overhead. \n\n\nA novel research direction aims to listen to the environment for improved task performance. Simulated room impulse response (RIR) datasets allow researchers to investigate the interaction of sounds with known space layouts for e.g. depth prediction, obstacle avoidance or to drive towards an alarm beyond the line of sight. They have been used successfully to predict 3D layouts from simulated chirps, similar to how bats find their prey. Robots mastering this echolocation could create instant maps beyond their immediate surroundings without LiDAR, overcoming their field of view and operate even in darkness and smoke.\n\n\n\n\n\n \n\nWhile some datasets provide recorded RIRs within limitations, there exists no large-scale real dataset to investigate audio-visual 3D scene understanding for robots. This hinders domain adaptation from simulation as well as learning to be robust against or even exploit real world background sounds.\n\n\nBats are capable of navigating and localizing prey in flight using echolocation. We showed successfully how to adopt this feat for machine listening, using only audible echoes of chirps for depth prediction <cit.>. To understand the extend to which this method can complement failing vision sensors and even extend beyond the field of view, more real data in typical robotic scenarios is needed. We therefore present the BatVision Dataset providing publicly available large-scale real data of complex scenes for research on audio-visual 3D scene understanding and robotic echolocation.\n\nSound-augmented Task Performance. Sound arrives omnidirectional and provides rich information about the space it crosses. Due to the correlation of sound and vision it can provide complementary information when visual sensors fail. Cameras and LiDAR struggle with occlusions, reflections and smoke. Depth-from-stereo algorithms produce artifacts in low-light conditions and when objects lack textures. Combining audio-visual information can improve task performance with one modality providing labels for the other schulz2021hearing,valverde2021there. Biomimicking the echolocation principle can even improve 3D scene understanding using sound christensen2020batvision,gao2020visualechoes,zhang2022stereo. \n\nAudio-visual data suitable for these tasks is sparse because real-world recordings require complex setups and long sessions <cit.>. In consequence, researchers often create small datasets tailored to their tasks <cit.>). For research on robotic echolocation, only very few simple datasets exists.\n\n\n\nThe BatVision Dataset.\nMotivated by the success of simulated datasets and the lack of real data for robotic echolocation and audio-visual scene understanding, we recorded the real-life BatVision dataset. We provide large-scale recordings from traversing real campus spaces with a robot, covering a wide range of materials, room shapes and objects, shown in <Ref>. We provide synchronized RGB-D camera images and recorded echoes of chirping sounds emitted with a forward facing speaker. Similar to RIRs, echoes can be used to infer the geometry of the room to allow the robot bat-like perception even in darkness, smoke and fog. \n\n\n\n\n\n\nData was collected at UC Berkeley (52,220 instances) and Mines Paris PSL (3,120 instances). Emitted chirp signals range between 20Hz and 20kHz. RGB-D images were recorded with an Intel Realsense camera mounted on a robot (Mines Paris) and a ZED camera (UC Berkeley). \n\n\nIn the following we place the BatVision dataset among similar ones and argue its unique utility. We then detail the collection process, data and showcase performance of depth prediction methods trained on it. UC Berkeley data will be referred to as BV1 and Mines Paris data as BV2.\n\n\n\n\n\n\u00a7 RELATED WORK\n \n\nAudio-visual data is available in many datasets, collected in reality or simulation, suitable for different tasks. However, to our knowledge, no existing dataset supports audio-visual 3D scene understanding with large-scale real data.\n\nAcoustic Room Impulse Response Datasets. \nRIR datasets are used in room acoustics, speech and audio processing, sound localization and separation and virtual reality. RIRs account for the effect of room acoustics on audio signals and describe sound propagation. By convolving with RIRs, acoustic properties, such as reverb, can be transferred onto arbitrary dry sounds.\nSeveral datasets exist with RIRs, sampled in real spaces using a complex measurement routine.\n \nThe Acoustic Multichannel RIR Dataset  <cit.> contains samples from one rectangular room. Panels, moved between recording sessions, emulate environments such as a small office, meeting or lecture room. The dEchorate dataset <cit.> provides 1.8k sampled RIRs with annotated early echo timings and 3D positions of microphones and sound sources. For the MIRaGe dataset <cit.> 371k RIRs are measured in a dense grid of 4104 source positions in different rooms. \n<cit.> estimates RIRs from spherical camera images. RIRs were sampled as ground truth in living room style environments using a custom array of 48 microphones and a soundfield microphone. Object shapes are simple rectangles, aligned towards the main axis of the room.\n\n\nWhile these datasets were sampled with high quality equipment and allow room geometry and RIRs estimation their environments are rectangular rooms with cuboid objects and simple materials. In contrast, for BatVision we sampled data in real public spaces, selected for their variety of materials, shape and architectural properties.\n\nAudio-Visual Simulation.\n\nDatasets containing simulated data are widely used, cost-effective, allow large scale generation, controlled conditions and easy data annotation <cit.>,  <cit.>). Simulation of sound propagation has been extensively used in games and AR applications.\n\nSoundSpaces 1.0 <cit.> allows simulating sound propagation by providing RIR renderings built with bidirectional path-tracing in 3D-scanned apartment models.\nThey were generated for discreet positions and orientations in a grid in two 3D environments, Matterport3D <cit.>) and Replica<cit.>). SoundSpaces 2.0 <cit.> provides continuous on-the-fly rendering and improved the sound propagation. \nIt has since become a widely used benchmark showing impressive performance on a wide range of tasks  <cit.>. \n\nWhile it is a huge contribution to the community, bridging the gap between simulation and the real world stays a significant challenge. Models trained in simulation often overfit to characteristics of the simulator in surprising ways kadian2020sim2real. However, for audio-based 3D scene understanding there is no real-life dataset of a size, comparable to SoundSpaces which motivated our data collection.\n\n\nSound Event Localization and Detection Datasets. \nSound event localization and detection (SELD) aims to infer the azimuth, elevation and distance of sounds relative to an observer, with or without additional classification. Datasets in the domain contain one or several, moving or static, clear or noisy sounds in different environments.\nThe TUT Acoustic Scenes 2016 dataset <cit.> consists of 15 real acoustic scenes, such as City center or Metro station, with annotated sound event classes, onset and offset times, and spatial information. The STARSS22 dataset <cit.> is a collection of 22 real-world acoustic scene recordings with sound event annotations, captured with a high-resolution spherical microphone array. Annotations cover 13 classes and direction of arrivals. \n\n\n\nFor 3D scene understanding and especially reconstruction, SELD datasets often lack sufficient spatial ground truth information of the environments the sounds occur in. For the BatVision dataset forward facing camera images and depth information is provided to allow exploring the correlations in the audio and visual modality.\n\n\nReal-World Audio-Visual Datasets. \n\nThe huge EGO 4D dataset <cit.> contains videos from the perspective of persons performing a wide range of activities. Parts contain audio and 3D meshes of the environment. It is an impressive effort and  contribution for many research areas such as activity recognition. In contrast, for BatVision we emit chirps into recorded scenes to allow inferring RIRs and finally the 3D scene. It is magnitudes smaller but better suited for our tasks.\n\n<cit.> proposed \u201cThe Greatest Hits\u201d dataset. By filming objects being struck with a drumstick, they aim to study physical interactions with a visual scene and synthesize plausible impact sounds. It includes 46,577 actions of hitting and scratching objects. The research is a step towards understanding the link between material, action and emitted sound. Similarly, with our recorded echoes we collect interactions between chirps and scene materials.\n\n\nThe authors in <cit.> predict approaching vehicles at blind intersections from sound before they enter the line-of-sight. They captured crossing vehicles at intersections with a custom microphone array and a front-facing camera mounted on a car. <cit.> recorded the \"Omni Auditory Perception Dataset\" standing next to streets with eight binaural microphones and a 360\u00b0 camera. Pseudo ground truth depth is predicted from monocular images. The authors emphasize its mid-size and the great effort required to create it.\n\n\n<cit.> introduced the \"Quiet Campus Dataset\" of ambient sounds from a variety of quiet indoor scenes. Paired with RGB-D images, they predict distances to walls from the whirling of a fan or noise coming through a window. While this unique idea shows impressive task performance with passive observations, we focus on active sounds which are humanly audible for improved performance. \n\n\nRecording interactions of chirps with spaces has a recent history. The BatVision depth-from-binaural audio idea <cit.> inspired <cit.> to collect similar data with added 360\u00b0 LiDAR scans for depth and four ear-shaped microphones. With 5,000 samples the dataset falls between BV1 and BV2. \n<cit.> predict an occupancy map from conversations in spaces. Beyond using SoundSpaces they also captured real data in a mock-up apartment, citing the lack of publicly available real world data. To compute RIRs they capture chirps from a speaker with an Eigenmike. \nThe \"Studio Dataset\" <cit.> records 1478 samples of chirp echoes with four microphones and RGB-D images with a RealSense camera. Everything is fixed into one metal frame facing in one direction. While also testing in SoundSpaces, their real-world recordings are done in one rectangular room showing simple cuboid objects.  \n\n\n\nThe recording of so many datasets, tailored to specific needs show the potential impact that more and larger publicly available datasets can have. Presented tasks from depth prediction to occupancy mapping can be investigated with our BatVision dataset. We will describe in the following how and give details on the extend of our data collection. \n\n\n\n\n\u00a7 THE AUDIO-VISUAL BATVISION DATASET\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Dataset Overview\n\nWe collected the Audio-Visual BatVision Dataset at various locations at the UC Berkeley (ICSI), and Ecole des Mines Paris using small robots, carrying all sensors. The sites provide varied architectural styles, room shapes, materials and therefore acoustic impressions. \n\nWe crossed lecture halls, corridors, offices and cobblestone paths recording while emitting humanly audible sine swept signals (chirps) with a forward facing speaker (<Ref>). We recorded their echoes, forward facing camera images and provide depth-maps from the same perspective. Chirps ascend from 20Hz to 20kHz in 3 ms. Their use is motivated by their counterparts in nature, helping animals echolocalize, but they are also common in sound engineering to record RIRs farina2000simultaneous. Binaural microphones record audio at 44.1kHz with 24 bits to keep the full frequency range of the chirps. Each instance is one chirp synchronized with one 1280x720 RGB-D image.\nAn overview of this dataset and other real-world dataset is available in <Ref>\n\nThe distribution of the collected depth can be seen at <ref> and <ref>. By comparing the average depth value per instances it shows BV2 is more long-tailed. BV1 depth values are clipped to the BV2 max depth value to remove outliers of the stereovision algorithm used. Pixel-wise average depth shows a more complex scene distribution in BV2.\n\n\n\nData collected at the UC Berkeley (BV1). \nIn Berkeley, 52,220 instances were collected at two floors of an institute containing hallways, open areas, conference rooms and offices. \nAt two distinct areas of one floor, 39,564 and 7,618 instances were collected for training and validation and 5,038 instances on another floor for testing. While similar, the floors' spatial layout, furniture, occupancy, and decorations are different, see <Ref>. For details, please see christensen2020batvision showcasing the initial depth prediction-from-audio idea.\n\n \nA JBL Flip4 Bluetooth speaker emitted chirps while two USB Lavalier MAONO AU-410 microphones, embedded into silicone ears, recorded their echoes. They were mounted 23.5cm apart while the speaker sat between (<Ref>). We excluded motor sounds by pushing the robot on a trolley. \n\nWe used a ZED stereo camera to record images of the scene ahead and calculated depth maps with the  camera API. Depth-from-stereo fails for some pixel which are NAN. We provide depth maps with RGB images from the left camera.\n\n\nDesigned for smaller spaces, audio recordings were cut at 72.5ms, including echoes from objects at 12m distance. This trades-off perception at a relevant distance while excluding later noisy reverberations. We also clip depth to 12m during training even though further distances are available.\n\n\n\nData collected in Mines Paris (BV2). \nIn Ecole des Mines Paris, we collected 3,120 instances with large visual and acoustic variety (See <Ref>).\n\nWe split data into sets of 1,911 train, 625 validation and 584 test instances. Given the multi-modal scene distribution we aim to balance task difficulty. We split instances by time of recording when moving through rooms, avoiding loops. That way, our incremental coverage of rooms will lead to poses being sufficiently separated in the sets. Even if we revisit parts of rooms we chose different trajectories thereby avoiding repeated poses. Tasks on this split will be harder than if instances were randomly assigned, which could result in neighbouring poses ending up in training and test. Simple interpolation will not yield best performance. Admittedly our split is easier than separating complete rooms as done for BV1. We observed outdoor reconstruction performance is acceptable but sub-par suggesting domain shift. If very different features, i.e. outdoors, carpeted rooms or stone corridors, are not in the training data further domain adaptation would be necessary which is left for future work.\n\n\nWe provide monocular RGB images and depth from an Intel RealSense D455 camera. The chirp-emitting speaker is identical to BV1 but we recorded echoes with a binaural 3Dio Free Space microphone. A robot carried all hardware and an Nvidia Jetson TX-1 to run all recording software (see <Ref>). We excluded motor noise by switching between stopping and driving and filtered out instances while moving. \n\nSynchronization was achieved using ROS timestamps and chirps detection with manual checks. Audio data instances were cut to be 0.45s long which includes echoes from objects up to 75m away. Because of larger spaces in BV2, we keep the long tail to sense far away reflection. The maximum depth value of the camera is \u224864m.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Limitations\n\nUnlike in simulation, physically recording introduces a number of limitations. Echoes were recorded in real rooms next to noisy streets and with typical sounds of busy academic institutions. Some data may therefore contain audible noise, typical for the recording context. Models trained will need to learn to be robust to these noise profiles.\n\nSome approaches predicting RIRs need independently changing emitter and microphone positions to record sounds on a direct path carlo2021dechorate, luo2022learning. In our data collection by driving, the robot carries all the equipment so the emitter is fixed close to the microphone and only the echoes change.\n\n\nAfter filtering instances in motion few remain from the same pose. We kept these to allow learning a noise-model but they can be filtered by thresholding optical flow.\n\n\nCollection with different hardware for BV1 and 2 leads to different formats. Audio instances collected at UC Berkeley are shorter. Some very bright images from Mines Paris shows slight purple discoloration due to an issues with the Intel RealSense D455 RGB-D camera. Finally, our consumer-grade speaker can not produce the full frequency spectrum.\n\n\n\n\n\n\n\n\n\n\n\u00a7 DEPTH PREDICTION ON BATVISION DATA\n\nThe data can be used for depth-prediction from audio-visual data with approaches developed for real or simulated data. For illustration we trained a U-Net audio-only depth prediction baseline and compared with one state-of-the-art audio-visual approach developed by its authors in simulation.\n\n\n\n\n\nBeyond Image to Depth.\nRecent work improves audio-visual depth prediction using a pre-trained material classifier to decide which modality to pay attention to. The authors parida2021beyond extract latent material features from SoundSpaces 1.0 camera images and generate attention maps for the visual and audio-stream in the network. They convolve a dry-chirp sound with SoundSpace RIRs to simulate emitting it into spaces from different poses. Captured echoes and RGB-D images make up their dataset with which they predict depth from RGB images and audio.\n\n\nTraining their code on the BatVision dataset shows similar performance as on SoundSpaces (see <Ref>). Apart from changing the spectrogram resolution, no further adjustments were needed when switching to real data. We ablate results in a study using the audio signal only by setting the RGB images to zero. That way we keep the architecture unchanged.\n\n\n\n\nQualitatively, fine structures are harder to predict which may stem from our coarser ground truth depth compared to simulation (See <Ref>). Measuring depth in reality, either from stereo (ZED camera) or active stereo (RealSense camera), limits the accuracy depending on the object distance. \n\n\n\nComparable results show that an approach developed for simulated data can be adapted and works with the same hyperparameters on our real dataset.\n\n\n\nU-Net Baseline.\nThe extend to which audio can complement visual sensor data is not clear. Related work showed repeatedly the quality of the visual signal strongly dominates task performance. The compared work parida2021beyond investigates material-based attention to let the network choose the influence of each modality more explicitly. We investigate the claim that audio can help in bad visual conditions with a baseline, based on the audio signal alone. This serves to investigate the contribution of the audio signal in isolation with clear quality attribution. However, in all practical applications, the visual signal should be used as well.\n\n\nWe train a U-Net from audio only, similar to related work <cit.> and isolated on BV2 and BV1. We achieve solid results, correctly predicting free space, obstacles and the general room layout (see <Ref>). Without further tweaks such as GCC-Phat features or using a GAN christensen2020batvision,christensen2020gccbatvision, the performance shows the exploitable quality of the data itself.\n\n\n\nImplementation Details.\nThe only input are spectrograms, generated from waveforms with 512 frequency bins (nfft), 64 window and 16 hop length, resized to 256x256.\n\nFor BV2, best performance is obtained with depth clipped to 30m and audio cut accordingly. This can be explained as audio energy decays with distance so far traveling echoes are hard to distinguish from noise. For BV1, depth is clipped to 12m and the audio accordingly. Ground truth depth is always normalized using the chosen max depth value.\n\nWe use an 8 block U-Net with skip connection. Each encoder block is composed of 2D convolutions, batch normalization and leaky ReLUs. Each decoder block is composed of 2D transposed convolution, batch normalization and ReLUs (see <Ref> ). We found skip-connections improve performance though contrary to a segmentation task their contribution is not yet clearly understood.\nWe train with 256 batch size, 0.002 learning rate for BV2 and 0.001 learning rate for BV1 with the AdamW optimizer and L1 loss.\n\n\nResults on Depth Prediction.\nWith this simple baseline we retrieve the general geometry of the space (see <Ref> ). \nOn BV2, complex obstacles such as chairs are visible. When the visual sensors fails (e.g on glass), audio gives correct information about the depth.\nThe network generalizes well between different acoustic environment. It reproduces corridors robustly built with various material (e.g carpet floor and glass wall, stone and tiled floor, see <Ref>). Outdoors the performance is diminished with the network underestimating depth systematically. This shows some inevitable bias of the data having a majority of indoor data.\nTrained on BV1 data, corridors and obstacles are well reconstructed even though finer structures are equally lost.\n\n\n\n\n\u00a7 CONCLUSION\n\n\nRecent success in using simulated audio-visual data for scene understanding shows the potential of the audio modality. Depth prediction from sound alone or in addition to vision is possible, allowing to perceive the environment like a bat. The BatVision dataset will support the research community with large-scale real audio-visual data to improve task performance and uncover novel uses. \nWe present an audio-only depth-prediction baseline as starting point and obtain good results when training a state-of-the-art approach on our data. Future datasets could include ultrasound chirps to enable inaudible human-robot collaboration. \n\n\n\n\u00a7 ACKNOWLEDGMENT\n\nWe thank all our collaborators, especially Daniel Lin for data collection, Jesper Haahr Christensen for insights into sonar and collaboration on the original idea, Karl Zipser for conception of the robot for BV1. Finally David Mazouz and Jacky Lech for advice and help building the robot for BV2. \nThe authors acknowledges the support of the French Agence Nationale de la Recherche (ANR), under grant ANR-22-CE94-0003 (projet Omni-BatVision).\n\n\n\n\n\n\n"}