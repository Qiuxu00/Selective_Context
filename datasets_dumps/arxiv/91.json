{"entry_id": "http://arxiv.org/abs/2303.07245v1", "published": "20230313161612", "title": "Concentration without Independence via Information Measures", "authors": ["Amedeo Roberto Esposito", "Marco Mondelli"], "primary_category": "cs.IT", "categories": ["cs.IT", "math.IT", "math.PR"], "text": "\nModelling self-consistently beyond General Relativity\n    Luis Lehner\n    March 30, 2023\n=====================================================\n\n\n\n\nWe propose a novel approach to concentration for non-independent random variables. \n  The main idea is to \u201cpretend\u201d that the random variables are independent and pay a multiplicative price measuring how far they are from actually being independent. This price is encapsulated in the Hellinger integral between the joint and the product of the marginals, which is then upper bounded leveraging tensorisation properties. Our bounds represent a natural generalisation of concentration inequalities in the presence of dependence: we recover exactly the classical bounds (McDiarmid's inequality) when the random variables are independent. Furthermore, in a \u201clarge deviations\u201d regime, we obtain the same decay in the probability as for the independent case, even when the random variables display non-trivial dependencies.\n  To show this, we consider a number of applications of interest. First, we provide a bound for Markov chains with finite state space. Then, we consider the Simple Symmetric Random Walk, which is a non-contracting Markov chain, and a non-Markovian setting in which the stochastic process depends on its entire past. To conclude, we propose an application to Markov Chain Monte Carlo methods, where our approach leads to an improved lower bound on the minimum burn-in period required to reach a certain accuracy. In all of these settings, we provide a regime of parameters in which our bound fares better than what the state of the art can provide.\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nIt is well-known that, given a sequence X^n=(X_1,\u2026, X_n) of independent, but not necessarily identically distributed, random variables with joint measure _X^n, one can prove that for every function f satisfying proper Lipschitz assumptions:\n\t\n    _X^n(|f-_X^n(f)|\u2265 t) \u2264 2exp(-t^2/k \u2016 f\u2016_Lip^2).\n\n\tHere, \u2016 f\u2016_Lip^2 depends on the metric structure of the measure space, and k is a constant depending on the approach used to prove the inequality, e.g., transportation-cost inequalities, log-Sobolev inequalities, martingale method, see the survey\u00a0<cit.>. \n\tOne notable example is McDiarmid's inequality for functions with \u201cbounded jumps\u201d: i.e., if for every x^n,x\u0302 and every 1\u2264 i \u2264 n one has that \n    |f(x_1,\u2026,x_i,\u2026,x_n)-f(x_1,\u2026,x\u0302,\u2026,x_n)|\u2264 \n    c_i ,\n then the following holds <cit.>:\n\t\n    _X^n(|f-_X^n(f)|\u2265 t) \u2264 2exp(-2t^2/\u2211_i=1^n c^2_i).\n\n\n\n\n\n\tThis represents the \u201cgolden standard\u201d of concentration. Interestingly, as underlined above, McDiarmid's inequality does not require the X_i's to be identically distributed; it does, however, require the random variables to be independent.\nMost of the methods in the literature that tried to relax the latter assumption \n required the development of novel techniques. However, existing results generally do not recover the rate of decay provided in the independent setting.\n \n In this paper, we present a novel approach that \n outperforms the state of the art in various settings and regimes. Specifically, we show improved bounds for \n finite-state space Markov chains (<Ref>), the Simple Symmetric Random Walk (SSRW, <Ref>), a non-Markovian process (<Ref>), and Monte Carlo Markov Chain (MCMC, <Ref>).\n  In the case of the SSRW, our improvements are the most dramatic: in sharp contrast with existing techniques, we are able to capture the correct scaling between the distance from the average t, the number of variables n, and the decay probability in the concentration bound. \nWe remark that our new method \u2013 based on a change of measure argument \u2013 is rather flexible and can be employed in most settings. In fact, it only requires the absolute continuity between the joint and the product of the marginals, while existing approaches generally have more restrictive assumptions (e.g., Markovity with stationary distribution\u00a0<cit.> or contractivity\u00a0<cit.>). The key idea is to shift the focus \nfrom proving concentration to bounding an information measure (i.e., the Hellinger integral, see\u00a0<Ref>) between the joint distribution and the product of the marginals. Crucially, the Hellinger integral satisfies tensorisation properties that allow us to easily upper bound it, even in high-dimensional settings (see\u00a0<Ref>).\nWe highlight that our approach provides a natural generalisation of the existing concentration of measure results to dependent random variables, in the sense that we recover exactly McDiarmid's inequality when the random variables are independent. Furthermore, for sufficiently large t, namely, in a \u201clarge deviations\u201d regime, we approach the decay rate (<ref>) for the independent case, even when the random variables are actually dependent.\n\nThe rest of the paper is organized as follows. In\u00a0<Ref>, we discuss related work in the area.\nin\u00a0<Ref> we cover the preliminaries, namely, information measures (<Ref>), Markov kernels (<Ref>), and strong data-processing inequalities (SDPIs, <Ref>). We then provide the main result of this work in\u00a0<Ref>, which is then applied in\u00a0<Ref> to four different settings: finite-state space Markov chains (<Ref>), the SSRW (<Ref>) a non-Markovian stochastic process (<Ref>), and MCMC methods (<Ref>). Concluding remarks are provided in\u00a0<Ref>. Part of the proofs and additional discussions are deferred to the appendices. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Related Work\n\n The problem of concentration for dependent random variables has been addressed in multiple ways. The first results in the area are due to Marton\u00a0<cit.> who heavily relied on transportation-cost inequalities (Pinsker-like inequalities) and an elegant mixture of information-theoretic and geometric approaches. Another important contribution, building upon Marton's work, was given by Samson in\u00a0<cit.> where some of Marton's results were extended to include \u03a6-mixing processes. More recent advances, complementing and generalising the work by Samson and Marton, were provided in\u00a0<cit.>, where the Martingale method was employed to prove concentration for dependent (but defined on a countable space) processes and in\u00a0<cit.>, where Marton's couplings were exploited. In particular, the results derived in\u00a0<cit.> are equivalent to the ones advanced in\u00a0<cit.> but obtained through couplings rather than linear programming. All of these approaches measure the degree of dependence by looking at distances between conditional distributions (organised in matrices whose norms are then computed, see\u00a0<Ref>) or by constructing \u201cminimal couplings\u201d between conditional distributions (see\u00a0<Ref>). The resulting quantities, which are necessary in order to \n analyse the corresponding probabilities, can be difficult to compute, especially in non-Markovian settings. \n Another approach similar in spirit to ours is given in\u00a0<cit.>, where a generalisation of Hoeffding's inequality for stationary Markov chains is provided. Other related work can be found in\u00a0<cit.>. These results aim to establish Hoeffding-like inequalities for Markov chains by relating it to a different Markov chain whose cumulant generating function can be bounded under different assumptions:\n \n \u00a0<cit.> are restricted to discrete and ergodic Markov chains, while\u00a0<cit.> extends to general state-space but requires geometric ergodicity. All these generalisations of Hoeffding's inequality do not, however, allow for arbitrary functions of a sequence of random variables, but they are restricted to (sums of) bounded functions applied to each individual sample and they all require the existence of a stationary distribution.  Given that the approach presented in\u00a0<cit.> is more general than the one proposed in\u00a0<cit.>, our results will be compared directly with <cit.>.\n    Another (less related) approach can be found in\u00a0<cit.>, where the strength of the dependence is measured in a different way with respect to both this work and the related work mentioned above. Moreover, the approach in\u00a0<cit.> is mostly restricted to empirical averages of bounded random variables and includes an additional additive factor that grows with the number of samples. Finally, we remark that \n\t<cit.> exploits a technique similar to what is pursued in this work, in order to extend McDiarmid's inequality to the case where the function f depends on the random variables themselves (while the random variables remain, in fact, independent). Said result was then applied to a learning setting.\n\n\n \n\n\u00a7 PRELIMINARIES\n\n\n In this section, we will define the main objects utilised throughout the document and define the relevant notation.\n\tWe will \n adopt a measure-theoretic framework.\n\tGiven a measurable space (,\u2131) and two measures \u03bc,\u03bd which render it a measure space, if \u03bd is absolutely continuous with respect to \u03bc (denoted with \u03bd\u226a\u03bc), then we will represent with d\u03bd/d\u03bc the Radon-Nikodym derivative of \u03bd with respect to \u03bc. Given a (measurable) function f:\u2192\u211d and a measure \u03bc, we denote with \u03bc(f) = \u222b f d\u03bc the \nLebesgue integral of f with respect to the measure \u03bc. \nThe Radon-Nikodym derivatives represent the main building-block of the following fundamental objects.\n\n\t\n\n \u00a7.\u00a7 Hellinger integral, \u03b1-norm and R\u00e9nyi's \u03b1-divergence\n\n An important ingredient of this work are information measures. In particular, we will focus on Hellinger integrals which can be seen as a transformation of the L^\u03b1-norms and of \n R\u00e9nyi's \u03b1-divergences. \n\t\n\n  \u00a7.\u00a7.\u00a7 R\u00e9nyi's \u03b1-divergences\n\n\tIntroduced by R\u00e9nyi as a generalization of the KL-divergence, the \u03b1-divergence has found many applications in \n statistical inference\u00a0<cit.>, and it has  \n \n several \n operational interpretations (e.g., hypothesis testing, and the cut-off rate in block coding <cit.>).\n\tIt can be defined as follows\u00a0<cit.>.\n\t\n\t\tLet (\u03a9,,),(\u03a9,,) be two probability spaces. Let \u03b1>0 be a positive real number different from 1. Consider a measure \u03bc such that \u226a\u03bc and \u226a\u03bc (such a measure always exists, e.g., \u03bc=(+)/2)) and denote with p,q the densities of , with respect to \u03bc. Then, the \u03b1-divergence of  from  is defined as \n\t\t\n    D_\u03b1():=1/\u03b1-1log\u222b p^\u03b1 q^1-\u03b1 d\u03bc.\n\n\t\n\n\t\tDefinition <ref> is independent of the chosen measure \u03bc. \n\t\tIn fact, \n\t\t\u222b p^\u03b1q^1-\u03b1 d\u03bc = \u222b(q/p)^1-\u03b1d and, whenever \u226a or 0<\u03b1<1, we have \u222b p^\u03b1q^1-\u03b1 d\u03bc= \u222b(p/q)^\u03b1d, see <cit.>. Furthermore, it can be shown that, if \u03b1>1 and \u226a\u0338, then D_\u03b1()=\u221e. The behavior of the measure for \u03b1\u2208{0,1,\u221e} can be defined by continuity. In general, one has that D_1() = D() which denotes the KL-divergence between  and ; furthermore, if D()=\u221e or there exists \u03b2>1 such that D_\u03b2()<\u221e, then lim_\u03b1\u21931D_\u03b1(Q)=D()\u00a0<cit.>. For an extensive treatment of \u03b1-divergences and their properties, we refer the reader to\u00a0<cit.>. \n \n\t\n\t\n\t\n\t\n\n  \u00a7.\u00a7.\u00a7 \u03c6-divergences\n\n\tAnother generalization of the KL-divergence is obtained by considering a generic convex function \u03c6:\u211d^+\u2192\u211d \n with the \n constraint \n \u03c6(1)=0. The constraint can be ignored as long as \u03c6(1)<+\u221e by simply considering a new mapping \u03c6\u0303(x) = \u03c6(x) - \u03c6(1). \n\t\n\t\tLet (\u03a9,,),(\u03a9,,) be two probability spaces. Let \u03c6:\u211d^+\u2192\u211d be a convex function such that \u03c6(1)=0. Consider a measure \u03bc such that \u226a\u03bc and \u226a\u03bc. Denoting with p,q the densities of the measures with respect to \u03bc, the \u03c6-divergence of  from  is defined as \n\t\t\n    D_\u03c6():=\u222b q \u03c6(p/q) d\u03bc.\n\n\t\n\tDespite the fact that the definition uses \u03bc and the densities with respect to this measure, \n \u03c6-divergences are \n independent from the dominating measure. In fact, \n when absolute continuity between , holds, i.e., \u226a,[We will make this assumption throughout the paper.] \n we obtain \n <cit.>\n\t\n    D_\u03c6()= \u222b\u03c6(d/d)d.\n\n\nThe KL-divergence is retrieved by setting \u03c6(t)=tlog(t). Other common examples are the Total Variation distance (\u03c6(t)=1/2|t-1|), the Hellinger distance (\u03c6(t)=(\u221a(t)-1)^2), and Pearson \u03c7^2-divergence (\u03c6(t)=t^2-1). We remark that \n\u03c6-divergences do not include the family of R\u00e9nyi's \u03b1-divergences.\n\n\tParticularly relevant to us will be the family of parametrized divergences that stems from \u03c6_\u03b1(x)=x^\u03b1 for \u03b1>1. The function \u03c6_\u03b1(x) is convex on the positive axis for every \u03b1>1. However, it does not satisfy the property that \u03c6(1)=0. Said requirement \n can \n be lifted with the consequence of losing the property that _\u03c6(\u03bd\u03bc)=0 if and only if \u03bd = \u03bc. We will call the family of divergences stemming from such functions the Hellinger integrals of order \u03b1.\n\t\n\t\tLet (\u03a9,,\u03bd),(\u03a9,,\u03bc) be two probability spaces, and let \n  \u03c6_\u03b1:\u211d^+\u2192\u211d be defined as \n  \u03c6_\u03b1(x)=x^\u03b1. Let \u03bc and \u03bd be two probability measures such that \u03bd\u226a\u03bc, then the Hellinger integral of order \u03b1 is given by\n\t\t\n    H_\u03b1(\u03bd\u03bc):= D_\u03c6_\u03b1(\u03bd\u03bc) = \u222b(d\u03bd/d\u03bc)^\u03b1 d\u03bc.\n\n\t\n\n\t    Let us highlight that \n     we are not considering Hellinger divergences of order \u03b1 (including the so-called \u03c7^2-divergence) which consists of divergences stemming from \n     x^\u03b1-1/(\u03b1-1), but rather a transformation of said family. In fact, the Hellinger divergences are \n     equal to 0 if and only if the measures coincide. In contrast, the Hellinger integral is equal to 1 if the two (probability) measures coincide. \n\n\n\tAnother notable object for this work will be the 1/\u03b1 power of the Hellinger integral, i.e.,\n\t\n    H_\u03b1^1/\u03b1(\u03bd\u03bc)= \u2016d\u03bd/d\u03bc\u2016_L^\u03b1(\u03bc),\n\n\twhich \n represents the L^\u03b1-norm of the Radon-Nikodym derivative with the respect to the measure \u03bc.\n    Moreover, the following holds \n    <cit.>:\n    \n    H_\u03b1^1/\u03b1(\u03bd\u03bc) = exp(\u03b1-1/\u03b1D_\u03b1(\u03bd\u03bc)).\n\n   \n    \n\n \u00a7.\u00a7 Markov kernels\n\n    Most of the comparisons with the state of the art will be drawn in \n    Markovian settings. In this section, we will define the main objects necessary in order to carry out said confrontation.\n    \n    Let (\u03a9,\u2131) be a measurable space. A Markov kernel K is a mapping K:\u2131\u00d7\u03a9\u2192 [0,1] such that:\n    \n        \n  * for every x\u2208\u03a9, the mapping E\u2208\u2131\u2192 K(E|x) is a probability measure on (\u03a9,\u2131);\n        \n  * for every E\u2208\u2131 the mapping x \u2208\u03a9\u2192 K(E|x) is an \u2131-measurable real-valued function.\n    \n    \n    A Markov kernel can be seen as acting on measures \u201cfrom the right\u201d, i.e., given a measure \u03bc on (\u03a9, \u2131), \n    \n    \u03bc K(E) = \u03bc(K(E|\u00b7)) = \u222b d\u03bc(x) K(E|x),\n\n    and on functions \u201cfrom the left\u201d,  i.e., given a function f:\u03a9\u2192\u211d,\n    \n    K f(x) = \u222b dK(y|x) f(y).\n\n    Given a sequence of random variables (X_n)_n\u2208\u2115, one says that it represents a Markov chain if, given i\u2265 1,  there exists a Markov kernel K_i such that for every measurable event E:\n    \n    \u2119(X_i \u2208 E | X_1,\u2026,X_i-1) = \u2119(X_i \u2208 E | X_i-1) = K_i(E|X_i-1)     almost surely.\n\n    If for every i\u2265 1, K_i = K for some Markov kernel K, then the Markov chain is said to be time-homogeneous. Whenever the index is suppressed from K, we will be referring to a time-homogeneous Markov chain. The kernel K of a Markov chain describes the probability of getting from x to E in one step, i.e., for every i\u2265 1, \n    K(E|x) = \u2119(X_i \u2208 E| X_i-1=x). One can then define (inductively) the \u03ba-step \n    kernel K^\u03ba as follows:\n    \n    K^\u03ba(E|x) = \u222b K^\u03ba-1(E|y)dK(y|x).\n\n    Note that K^\u03ba is also a Markov kernel, and it represents the probability of getting from x to E in \u03ba steps: K^\u03ba(E|x)= \u2119(X_\u03ba+1\u2208 E|X_1=x).\n    If (X_n)_n\u2208\u2115 is the Markov chain associated to the kernel K and X_0 \u223c\u03bc, then \u03bc K^m denotes the measure of X_m+1 at every m\u2208\u2115. Furthermore, a probability measure \u03c0 is a stationary measure for K if \u03c0 K(E) = \u03c0(E) for every measurable event E. We also note that, if the state space is discrete, then K can be represented using a stochastic matrix. \n    \n    Given this dual perspective on Markov operators (acting on measures or functions), one can then study their contractive properties. In particular, let us define \n    \n    \u2016 K\u2016_\u03b1\u2192\u03b1 := sup_f\u2260 0\u2016 K f\u2016_\u03b1/\u2016 f \u2016_\u03b1.\n\n    Then, Markov kernels are generally contractive\u00a0<cit.>, meaning that \u2016 K\u2016_\u03b1\u2192\u03b1\u2264 1 and, consequently, \u2016 Kf \u2016_\u03b1\u2264\u2016 f \u2016_\u03b1 for every f.\n        Similarly, given \u03b3\u2264\u03b1, one can define the following quantity \n    \u2016 K\u2016_\u03b1\u2192\u03b3 := sup_f\u2260 0\u2016 K f\u2016_\u03b1/\u2016 f \u2016_\u03b3.\n\n        It has been proven that many Markovian operators are hyper-contractive\u00a0<cit.>, meaning that \u2016 K\u2016_\u03b1\u2192\u03b3\u2264 1\n        for some \u03b3 < \u03b1.  Given a kernel K and \u03b1>1, we denote by \u03b3^\u22c6_K(\u03b1) the smallest \u03b3 such that K is hyper-contractive, i.e., such that  \u2016 K\u2016_\u03b1\u2192\u03b3\u2264 1. Said coefficient has been characterised for some Markov operators\u00a0<cit.>. In case the Markov kernel is not time-homogeneous, in order to simplify the notation, instead of denoting the corresponding coefficient with \u03b3^\u22c6_K_i(\u03b1), we will simply denote it with \u03b3^\u22c6_i(\u03b1). \n        \n        Given a Markov kernel K and a measure \u03bc, one can also define the adjoint/dual operator (or backward channel) K^\u2190 as the operator such that\n         \u27e8 g, Kf\u27e9 = \u27e8 K^\u2190 g, f\u27e9 for all g and f\u00a0<cit.>. While one can define dual Markovian operators more generally, here we will focus on discrete settings where they can be explicitly specified via K and \u03bc\u00a0<cit.>:\n    \n    K_\u03bc^\u2190(y|x) = K(y|x)\u03bc(x)/\u03bc K (y).\n\n     \n\n \u00a7.\u00a7 Strong Data-Processing Inequalities\n\n    An important property shared by divergences is the Data-Processing Inequality (DPI): given two measures \u03bc,\u03bd and a Markov kernel K, one has that, for every convex \u03c6,\n    \n    D_\u03c6(\u03bd K\u03bc K) \u2264 D_\u03c6(\u03bd\u03bc).\n\n    This property holds as well for R\u00e9nyi's \u03b1-divergences, despite them not being a \u03c6-divergence\u00a0<cit.>.\n    DPIs represent a widely used tool and a line of work has focused on tightening them. In particular, in many settings of interest, given a reference measure \u03bc, one can show that D_\u03c6(\u03bd K\u03bc K) is strictly smaller than D_\u03c6(\u03bd\u03bc)  unless \u03bd=\u03bc. Furthermore, the characterization of the ratio D_\u03c6(\u03bd K\u03bc K)/D_\u03c6(\u03bd\u03bc) has lead to the study of \u201cstrong Data-Processing Inequalities\u201d\u00a0<cit.>.  \n    \n\n    Given a probability measure \u03bc, a Markov kernel K and  a convex function \u03c6, we say that K satisfies a \u03c6-type Strong Data-Processing Inequality (SDPI) at \u03bc with constant c\u2208[0,1) if\n    \n    D_\u03c6(\u03bd K\u03bc K) \u2264 c\u00b7 D_\u03c6(\u03bd\u03bc),\n\n    for all \u03bd\u226a\u03bc. The tightest such constant c is denoted by\n\n    \u03b7_\u03c6(\u03bc,K)    = sup_\u03bd\u2260\u03bcD_\u03c6(\u03bd K\u03bc K) /D_\u03c6(\u03bd\u03bc), \n    \u03b7_\u03c6(K)    = sup_\u03bc\u03b7_\u03c6(\u03bc,K).\n\n\n [SDPI for the KL and the BSC]\n     Let \u03bc=Ber(1/2), \u03f5<1/2 and K=BSC(\u03f5), i.e., K(y|x) =\u03f5 if x=y and K(y|x) =1-\u03f5 otherwise. Then, one has that \u03b7_xlog x(\u03bc,K)=(1-2\u03f5)^2\u00a0<cit.>, which implies that  \u03b7_xlog x(\u03bc,K) < 1 for all \u03f5>0. \n  \n While \u03b7_\u03c6 can be a difficult object to compute even for simple channels, some universal upper and lower bounds are known\u00a0<cit.>:\n \n    \u03b7_\u03c6(K) \u2264sup_x,x\u0302\u2016 K(\u00b7|x)-K(\u00b7|x\u0302)\u2016_TV = \u03b7_|x-1|(K)=\u03b7_TV(K),\n\n  \n    \u03b7_\u03c6(\u03bc,K) \u2265\u03b7_(x-1)^2(\u03bc,K) = \u03b7_\u03c7^2(\u03bc,K).\n\n We remark that these bounds hold for functions \u03c6 such that \u03c6(1)=0 or, equivalently, when the divergence D_\u03c6(\u03bd\u03bc) is defined to be \u03bc(\u03c6(d\u03bd/d\u03bc))-\u03c6(1). For general convex functions \u03c6, as well as for R\u00e9nyi's divergences, the DPI holds and SDPI constants are still defined analogously, however one cannot use common techniques to bound said quantities. The following counter-example highlights the issue.\n [Counter-example for Hellinger integrals and R\u00e9nyi's divergences] \n     Let \u03bd=(1/3,2/3) and K_1=BSC(1/3). Then, the stationary distribution \u03c0 is given by (1/2,1/2) and \u03bd K_1= (5/9,4/9). A direct calculation gives that H_2(\u03bd K_1\u03c0 K_1) = 82/81 and H_2(\u03bd\u03c0) = 10/9. Moreover, if K=BSC(\u03bb), one has that \u03b7_TV(K)=|1-2\u03bb| (see\u00a0<cit.>  and\u00a0<Ref>). Thus,  \n        \n    H_2(\u03bd K_1\u03c0 K_1)/H_2(\u03bd\u03c0) = 41/45 > \u03b7_TV(K_1) = 1/3,\n\n        which means that the inequality (<ref>) is violated. This is due to the fact that \u03c6_2(x)=x^2 is not equal to 0 at x=1. In fact, \n        renormalising H_2 leads to the \u03c7^2-divergence, which satisfies \n    H_2(\u03bd K_1\u03c0 K_1)-1/H_2(\u03bd\u03c0)-1=\u03c7^2(\u03bd K_1\u03c0 K_1)/\u03c7^2(\u03bd\u03c0 )=1/9 < \u03b7_TV(K_1) = 1/3.\n\n        \n  Similarly, let K_2=BSC(1/5), which gives that \u03b7_TV(K_2)=3/5. Consider now D_\u03b1(K_2(\u00b7|0)\u03c0)=D_\u03b1(\u03b4_0 K_2 \u03c0 K) = 1/\u03b1-1log ( 2^1-\u03b1( 0.2^\u03b1 +(0.8)^\u03b1)). Moreover, D_\u03b1(\u03b4_0\u03c0) = log(2). Thus, by setting \u03b1=6, one has that\n     \n    \u03b7_D_\u03b1(K_2) > D_\u03b1(\u03b4_0 K_2 \u03c0 K_2)/D_\u03b1(\u03b4_0\u03c0) = 0.6138 > \u03b7_TV(K_2) = 0.6,\n\n     which violates again the inequality (<ref>).\n \n    \n\n\u00a7 MAIN RESULT\n\n\n    \n    Let _X^n be the joint distribution of (X_1,\u2026, X_n), _X_i the marginal corresponding to X_i, and _\u2297_i=1^n X_i the joint measure induced by the product of the marginals.     \n  Suppose that (X_1,\u2026, X_n) are Markovian under _X^n, i.e., _X_i|X^i-1 = _X_i|X_i-1 almost surely. If _X^n\u226a_\u2297_i=1^n X_i, for any function f satisfying\u00a0<Ref>, any t>0 and \n  \u03b1>1, \n  one has\n  \n    \n    _X^n(| f-_\u2297_i=1^n X_i(f)|\u2265 t )   \u2264 2^1/\u03b2exp(-2t^2/\u03b2\u2211_i=1^n c_i^2)H_\u03b1^1/\u03b1(_X^n_\u2297_i=1^n X_i) \n       \u2264 2^1/\u03b2exp(-2t^2/\u03b2\u2211_i=1^n c_i^2)(\u220f_i=2^nH^\u03b1_i)^1/\u03b1,\n\n    with \u03b2=\u03b1/(\u03b1-1), H_i^\u03b1 = _X_i-1^1/\u03b2_i-1(H_\u03b1\u03b1_i^\u03b2_i-1/\u03b1_i(_X_i|X_i-1_X_i)), \u03b1_i>1 for i\u2265 0, \u03b2_0=1, \u03b1_n=1 , and \u03b2_i = \u03b1_i/(\u03b1_i-1). \n    \n    The proof of\u00a0<Ref> is in\u00a0<Ref>. \n    \n    \n    If the function f satisfies\u00a0<Ref> with c_i=1/n, like in the case of the empirical mean, one obtains \n    \n    _X^n(| f-_\u2297_i=1^n X_i(f)|\u2265 t ) \u2264 2^1/\u03b2exp(-n(2t^2/\u03b2-1/n\u03b1log H_\u03b1(_X^n_\u2297_i=1^n X_i))).\n\n    This means that if \n    t > \u221a(1/2n(\u03b1-1)log H_\u03b1(_X^n_\u2297_i=1^n X_i)),\n\n    then\u00a0<Ref> guarantees an exponential decay. If the sign of the inequality\u00a0(<ref>) is reversed, then the bound actually becomes trivial, for n large enough. The threshold behavior just described characterises the main difference of this bound with respect to existing approaches: while there are no restrictive assumptions required (other than absolute continuity of the measures at play), the bound can be trivial if the joint distribution is \u201ctoo far\u201d from the product of the marginals. In contrast, other approaches, like the one described in\u00a0<cit.>, do not generally exhibit such a \u201cphase transition-like\u201d behavior. Next, we will characterize the behaviour of the key quantity H_\u03b1(_X^n_\u2297_i=1^n X_i) as a function of n \n    in the concrete examples of <Ref>. Before doing that, a \n    few additional remarks are in order. \n    \n        The expression on the RHS of\u00a0<Ref> can be complicated to compute, especially due to the presence of {\u03b1_i}_i=2^\u221e. \n        Making a specific choice, which meaningfully reduces the number of parameters (i.e., taking \u03b1_i\u2192 1 for every i\u2265 2),\u00a0<Ref> boils down to the following, simpler, expression:\n   \n    _X^n(| f-_\u2297_i=1^n X_i(f)|\u2265 t ) \u2264 2^1/\u03b2exp(-2t^2/\u03b2\u2211_i=1^n c_i^2)\u00b7\u220f_i=2^n max_x_i-1 H_\u03b1^1/\u03b1(_X_i|X_i-1=x_i-1_X_i).\n\n    Moreover,\u00a0<Ref> can be re-written as follows:\n    \n    _X^n(| f-_\u2297_i=1^n X_i(f)|\u2265 t ) \u2264 2^1/\u03b2exp(1/\u03b2(-2t^2/\u2211_i=1^n c_i^2+\u2211_i=2^n max_x_i-1 D_\u03b1(_X_i|X_i-1=x_i-1_X_i))).\n\n    \u00a0<Ref> allows to exploit the SDPI coefficient for D_\u03b1 (see\u00a0<Ref>), which in some settings improves upon leveraging\u00a0<Ref> along with hypercontractivity, see\u00a0<Ref>.\n    \n    \n<Ref> can be proved in more generality. Indeed, for any measurable event E, one can say that, for every \u03b1>1, \n    _X^n(E) \u2264^1/\u03b2_\u2297_i=1^n X_i(E)\u00b7 H_\u03b1(_X^n_\u2297_i=1^n X_i).\n Thus, our framework is not restricted to a McDiarmid-like setting, but it can be used to generalise any concentration of measure approach to dependent random variables. The idea is that concentration holds when random variables are independent, namely, _\u2297_i=1^n X_i(E) decays exponentially in n under suitable assumptions. Then,\u00a0<Ref> shows that a similar \nexponential decay \nholds also in the presence of dependence, as long as the measure of the joint is not \u201ctoo far\u201d from the product of the marginals. The \u201cdistance\u201d between joint and product of the marginals is captured by the Hellinger integral H_\u03b1. In particular, if the joint measure corresponds to the product of the marginals, then H_\u03b1^1/\u03b1(_X^n_\u2297_i=1^nX_i)=1 for every \u03b1. Thus, taking the limit of \u03b1\u2192\u221e, one recovers \n\n    _\u2297 X_i(E)= _X^n(E ) \u2264_\u2297 X_i(E).\n\n\n\nOn the RHS of both\u00a0<Ref>, the probability term is raised to the power \u03b1-1/\u03b1 and multiplied by the \u03b1-norm of the Radon-Nikodym derivative. On the one hand, as \u03b1 grows, the \u03b1-norm grows as well, which increases the Hellinger integral; on the other hand, as \u03b1 grows, \u03b1-1/\u03b1 tends to 1, which reduces the probability. This introduces a trade-off between the two quantities that renders the optimisation over \u03b1 non-trivial. We highlight that considering the limit of \u03b1\u2192\u221e provides the fastest exponential decay and it recovers the probability for independent random variables. This has the cost of rendering the multiplicative constant larger, and we will discuss in detail the choice of \u03b1 in the various examples of\u00a0<Ref>.\n\n\n\n Note that the first inequality in <Ref> holds without the Markovity assumption. The second inequality, instead, leverages tensorisation properties of H_\u03b1 which are particularly suited for Markovian settings. In the general case,  \none can still \nreduce \nH_\u03b1(_X^n_\u2297_i=1^n X_i) (a divergence between n-dimensional measures) to n one-dimensional objects, see <Ref> for details about the tensorisation of both the Hellinger integral H_\u03b1 and R\u00e9nyi's \u03b1-divergence D_\u03b1. \nIn particular, following the approach undertaken to derive\u00a0<Ref>, one obtains \n   \n    _X^n(| f-_\u2297_i=1^n X_i(f)|\u2265 t ) \u2264 2^1/\u03b2exp(-2t^2/\u03b2\u2211_i=1^n c_i^2)  \u00b7\u220f_i=2^n max_x^i-1 H_\u03b1^1/\u03b1(_X_i|X^i-1=x^i-1_X_i).\n\nNote that\u00a0<Ref>\ngives a natural generalisation of concentration inequalities to the case of arbitrarily dependent random variables (just like\u00a0<Ref> generalises them to Markovian settings). Indeed, if _X^n=_\u2297_i X_i, then taking the limit of \u03b2\u2192 1 in both\u00a0<Ref> and\u00a0<Ref>, one recovers the classical concentration bound for independent random variables (see the discussion in\u00a0<Ref> recalling that \u03b2=\u03b1/(\u03b1-1)). \n\n    \n    We will now compare our bounds with a number of related works: 1) the approach proposed in\u00a0<cit.> and based on the martingale method; 2) the approach proposed in\u00a0<cit.> which leverages properties of contracting Markov chains; and finally 3) the approach advanced in\u00a0<cit.> showing concentration around the median via transportation-cost and isoperimetric inequalities. \n  \n\n\u00a7 APPLICATIONS\n\n    Let us now apply\u00a0<Ref> to four settings:\n    \n        \n  * In\u00a0<Ref>, we consider a discrete Markovian setting. Here, we specialise\u00a0<Ref>  leveraging the  properties of the Markov kernel along with the discrete structure of the problem, \n        thus showing that \n        in certain parameter regimes our bound fares better than what the state of the art can provide;\n        \n  * In\u00a0<Ref>, we consider a non-contracting Markovian setting that  does not admit a stationary distribution. Both these properties do not allow the application of most of the existing work in the literature. In contrast, not only our approach can be applied, but it provides exponentially decaying probability bounds, while\u00a0<cit.> can only provide an upper bound that does not vanish as n grows;\n        \n  * In\u00a0<Ref>, we consider a non-Markovian setting where the entire past of the process influences each step.  Here, to the best of our knowledge, we provide the first bound that exponentially decays in n and has a closed-form expression, while existing approaches either cannot be employed or require the computation of complicated quantities (e.g.,\u00a0<Ref>);\n        \n  * Finally, in\u00a0<Ref>, we apply\u00a0<Ref> to provide error bounds on Markov Chain Monte Carlo methods. Similarly to the other settings, we propose a regime of parameters in which our results fare better and, consequently, provide an improved lower bound on the minimum burn-in period necessary to achieve a certain accuracy in MCMC.\n    \n  We will hereafter assume, for simplicity of exposition, that c_i=1/n in\u00a0<Ref> like in the case of the empirical mean. All the results hold for general c_i's, but the expressions and comparisons would become more cumbersome.\n \n    \n\n \u00a7.\u00a7 Discrete Markov chains\n \n    \n    Consider a discrete setting and a Markov chain (X_n)_n\u2208\u2115 determined by a sequence of transition matrices (K_n)_n\u2208\u2115. Assume that X_1\u223c P_1 and let X_i denote the random variable whose distribution is given by P_1 K_1\u2026 K_i-1.[One can also see X_i as the outcome of X_i-1 after being passed through the channel K_i-1.] \n   \n   \n    \n        For i\u2265 1, suppose K_i is a discrete-valued Markov kernel, and let \u03b3^\u22c6_i(\u03b1) be the smallest parameter making it hyper-contractive, see\u00a0<Ref>.\n        Then, for every function f satisfying\u00a0<Ref> with c_i=1/n and every \u03b1>1, \n        \n    _X^n(| f-_\u2297_i=1^n X_i(f)|\u2265 t) \u2264 2^1/\u03b2exp(-2nt^2/\u03b2+\u2211_i=1^n-1( log\u2016 K_i^\u2190\u2016_\u03b1\u2192\u03b3^\u22c6_i(\u03b1) - 1/\u03b3\u0305_i^\u22c6(\u03b1)min_j\u2208supp(_i)log P_i(j))).\n\n        Moreover, if the Markov kernel is time-homogeneous, i.e., K_i=K for every i\u2265 1, then \n        \n    _X^n   (| f-_\u2297_i=1^n X_i(f)|\u2265 t)\n       \u2264 2^1/\u03b2exp(-2nt^2/\u03b2+(n-1) log\u2016 K^\u2190\u2016_\u03b1\u2192\u03b3^\u22c6_K(\u03b1) - 1/\u03b3\u0305^\u22c6_K(\u03b1)\u2211_i=1^n-1(min_j\u2208supp(_i)log P_i(j)))\n       \u2264 2^1/\u03b2exp(-2nt^2/\u03b2+(n-1) log\u2016 K^\u2190\u2016_\u03b1\u2192\u03b3^\u22c6_K(\u03b1) - n-1/\u03b3\u0305^\u22c6_K(\u03b1)(min_i=1,\u2026,(n-1)min_j\u2208supp(_i)log P_i(j))).\n\n    In the above equations \u03b3\u0305_i^\u22c6(\u03b1) and \u03b3\u0305_K^\u22c6(\u03b1) denote the H\u00f6lder conjugates of, respectively, \u03b3_i^\u22c6(\u03b1) and \u03b3_K^\u22c6(\u03b1).\n    \n\n\n        The main object one has to bound, according to\u00a0<Ref>, is the following:\n    \n    max_x_i-1 H_\u03b1(_X_i|X_i-1=x_i-1_X_i),  with  i\u2265 2.\n\nFrom the properties of the Markov kernel, one has that\nP_X_i = P_X_i-1 K_i-1. Furthermore, P_X_i|X_i-1=x_i-1 can be seen as \u03b4_x_i-1 K_i-1 where \u03b4_x_i-1 is a Dirac-delta measure centered at x_i-1. Thus, recalling the definition of \n\u03b3^\u22c6_i-1(\u03b1) from \n<Ref>, \n\n    H^1/\u03b1_\u03b1(_X_i|X_i-1=x_i-1_X_i)     = H^1/\u03b1_\u03b1(\u03b4_x_i-1K_i-1_X_i-1K_i-1)\n       = \u2016d\u03b4_x_i-1K_i-1/d_X_i-1K_i-1\u2016_L^\u03b1(_X_i-1K_i-1)\n       \u2264\u2016 K_i-1^\u2190\u2016_\u03b1\u2192\u03b3^\u22c6_i-1(\u03b1) H_\u03b3^\u22c6_i-1(\u03b1)^1/\u03b3^\u22c6_i-1(\u03b1)(\u03b4_x_i-1_X_i-1),\n\nwhere\u00a0<Ref> follows from\u00a0<cit.>. \nMoreover, for every \u03ba>1,\n \n    H^1/\u03ba_\u03ba(\u03b4_x_i-1_X_i-1) = _X_i-1({x_i-1})^1-\u03ba/\u03ba = _X_i-1({x_i-1})^-1/\u03ba\u0305,\n\n where \u03ba\u0305 denotes the H\u00f6lder's conjugate of \u03ba and _X_i-1({x_i-1}) the measure that _X_i-1 assigns to the point x_i-1.\n Thus, the following sequence of steps, along with\u00a0<Ref>, concludes the argument:\n \n    \u220f_i=2^n max_x_i-1 H^1/\u03b1_\u03b1(_X_i|X_i-1=x_i-1_X_i)    \u2264\u220f_i=2^nmax_x_i-1\u2016 K^\u2190_i-1\u2016_\u03b1\u2192\u03b3^\u22c6_i-1(\u03b1) H^1/\u03b3^\u22c6_i-1(\u03b1)_\u03b3^\u22c6_i-1(\u03b1)(\u03b4_x_i-1_X_i-1) \n        = (\u220f_i=2^n\u2016 K^\u2190_i-1\u2016_\u03b1\u2192\u03b3^\u22c6_i-1(\u03b1))(\u220f_i=2^n  max_x_i-1(_X_i-1({x_i-1})^-1/\u03b3\u0305^\u22c6_i-1(\u03b1)))\n       = (\u220f_i=1^n-1\u2016 K^\u2190_i \u2016_\u03b1\u2192\u03b3^\u22c6_i(\u03b1))(\u220f_i=1^n-1(min_x_i_X_i({x_i}))^-1/\u03b3\u0305^\u22c6_i(\u03b1)).\n\n Moreover, given the discrete setting, one can replace the measure _X_i with the corresponding pmf which is denoted by P_i.\n the norm which gives the for a given \n\n    H^1/\u03b1_\u03b1(_X_i|X_i-1=x_i-1_X_i) = H^1/\u03b1_\u03b1(\u03b4_x_i-1K_X_i-1K) \u2264 c H^1/\u03b1_\u03b1(\u03b4_x_i-1_X_i-1),\n\n with c\u2264 1. The inequality in\u00a0<Ref> follows from interpreting H_\u03b1 in one of two ways:\n \n     \n  * as a \u03c6-divergence, then the inequality follows from the Data-Processing Inequality that H_\u03b1 satisfies (a consequence itself of the convexity of x^\u03b1 and Jensen's inequality). In this case c can be seen as the \u201cStrong Data-Processing Inequality Coefficient\u201d of H_\u03b1 raised to the 1/\u03b1 power\u00a0<cit.>;\n     \n  * as the L^\u03b1-norm of the Radon-Nikodym derivative and the inequality following from contractivity of the Markov Oeprators K and K^\u2190 (along with the fact that d\u03bd K/d\u03bc K = K^\u2190(d\u03bd/d\u03bc)\u00a0<cit.>). In this case c can be seen as the contractive coefficient of the operators K and the induced adjoint K^\u2190 according to\u00a0<Ref>.  \n \n \n   \n        If \n        the Markov kernel K_i is only contractive (and not hyper-contractive), then \n        \u03b3^\u22c6_i(\u03b1)=\u03b1 and \u03b3\u0305^\u22c6_i(\u03b1)=\u03b2, which allows to simplify <Ref>. \n \n  \n   \n    \n     \n        \n      \n       \n       \n       \n\n\n\n\n\n   In this case, \nif \n   \n    t^2    \u2265\u03b2/2n-1/nlog\u2016 K^\u2190\u2016_\u03b1\u2192\u03b1 - n-1/2n(min_i min_j log P_i(j))\n       = (1+o_n(1))(\u03b2/2log\u2016 K^\u2190\u2016_\u03b1\u2192\u03b1^\u03b2 -1/2(min_i min_j log P_i(j)) ),\n\n   then <Ref> gives \n   exponential (in n) concentration even in the case of dependence. \n   \n   \n   \n    \n    Another perspective naturally \n    stems from\u00a0<Ref>. Indeed, similarly to\u00a0<Ref>, one has \n    \n        \n    _X^n(| f-_\u2297_i=1^n X_i(f)|\u2265 t)     \u2264 2^1/\u03b2exp(-1/\u03b2(2nt^2-\u2211_i=2^n max_x_i-1 D_\u03b1(_X_i|X_i-1=x_i-1_X_i)))\n       \u2264 2^1/\u03b2exp(-1/\u03b2(2nt^2-\u03b7_\u03b1(K)  \u2211_i=2^nmax_x_i-1 D_\u03b1(\u03b4_x_i-1_X_i-1))) \n        = 2^1/\u03b2exp(-1/\u03b2(2nt^2+\u03b7_\u03b1(K)  \u2211_i=2^nmin_x_i-1log P_i-1(x_i-1))).\n\n    We remark that \n    D_\u03b1 can also be hyper-contractive with respect to some Markovian operators, meaning that in\u00a0<Ref> for instance, one could consider D_\u03b3(\u03b4_x-1_X_i-1) with \u03b3 < \u03b1 (this is equivalent to hyper-contractivity of Markov operators,\u00a0<cit.>). One such example is the Ornstein\u2013Uhlenbeck channel with noise parameter t, cf.\u00a0<cit.>, for which one can prove hyper-contractivity with respect to D_\u03b1\u00a0<cit.>. Moreover, in some settings, leveraging SDPIs for D_\u03b1 can provide an improvement over\u00a0<Ref>, see <Ref> for a detailed\n    comparison.\n    \n   \n    We now compare the concentration bound provided by <Ref> with existing bounds in the literature. In this section, the comparison concerns a general Markov kernel, and the explicit calculations for a binary kernel are deferred to <Ref>. \n    \n    \n    \n\n  \u00a7.\u00a7.\u00a7 Comparison with <cit.>\n  Let us consider the same setting as in\u00a0<Ref>. Then, <cit.> gives \n\n    \n    \u2119(|f-_X^n(f)|\u2265 t ) \u2264 2 exp(-nt^2/2 M_n^2),\n\n    where M_n = max_1\u2264 i\u2264 n-1(1+\u2211_j=i^n-1\u220f_k=i^j \u03b7_KL(K_k) ) and we recall that \u03b7_KL(K_i)=sup_x,x\u0302TV(K_i(\u00b7|x),K_i(\u00b7|x\u0302)) is the contraction coefficient of the Markov kernel K_i. First, note that, \n    if the random variables are independent and thus _X^n=_\u2297 X_i, then\u00a0<Ref> reduces to \n    _X^n(E) \u2264 2 exp(-nt^2/2),\n while\u00a0<Ref> with \u03b3^\u22c6_K(\u03b1)=\u03b1\u2192\u221e and \u03b3\u0305^\u22c6_K(\u03b1)=\u03b2\u2192 1 recovers McDiarmid's inequality with the correct constant in front of n, i.e.,\n    \n    _X^n(E) \u2264 2 exp(-2nt^2).\n\n \n    \n    Assume now that the Markov kernel is time-homogeneous and has a contraction coefficient \u03b7_TV(K) < 1. Then, M_n = max_1\u2264 i\u2264 n-11-\u03b7_TV(K)^n+1-i/1-\u03b7_TV(K)= 1-\u03b7_TV(K)^n/1-\u03b7_TV(K). For compactness, define P_i^\u22c6(j^\u22c6):=min_imin_j P_i(j). Making a direct comparison, one has that if \n    \n    t^2    > n-1/n(2M_n^2/4M_n^2-\u03b2)log\u2016 K^\u2190\u2016_\u03b1\u2192\u03b1^\u03b2/P_i^\u22c6(j^\u22c6)\n       = (1+o_n(1)) (2/4-\u03b2(1-\u03b7_TV(K))^2)log\u2016 K ^\u2190\u2016_\u03b1\u2192\u03b1^\u03b2/P_i^\u22c6(j^\u22c6),\n\n    then\u00a0<Ref> (with \u03b3^\u22c6_K(\u03b1)=\u03b1 and \u03b3\u0305^\u22c6_K(\u03b1)=\u03b2) provides a faster exponential decay than\u00a0<Ref>. The explicit calculations for the special case of a binary kernel are provided in\u00a0<Ref>.\n\n  \n  Considering this setting,  one has the following bound:\n\n    \n    _X^n(E) \u2264 2^1/\u03b2exp(-1/\u03b2(2nt^2- (n-1)log(m \u2016 K\u2016_\u03b1\u2192\u03b1^\u03b2))).\n\n    Consequently, one can guarantee an exponential decay whenever \n    \n    t^2 > (1+o_n(1))log(m \u2016 K\u2016_\u03b1\u2192\u03b1^\u03b2)/2.\n\n    However, in this setting one can say something more specific. In particular, one can compute the ratio \n    H^1/\u03b1_\u03b1(_X_i|X_i-1=x_i-1_X_i)/H^1/\u03b1_\u03b1(\u03b4_x_i-1_X_i-1) = r_\u03b1(i)\n explicitly. \n    K is a m\u00d7 m matrix characterised by a vector \u03bb\u0305 of m parameters \u03bb_j such that \u2211_j=1^m \u03bb_j=1 and each row is a permutation of these parameters in such a way that every column also sums to 1 (Da elaborarci su, conseguenza del Birkhoff\u2013von Neumann theorem? Non sono 100% sicuro che sia corretto o se sia da assumere. Anche la notazione non mi piace r_\u03b1(i) ma poi ci riflettiamo). Hence one can see that \n    \n    r_\u03b1(i) = \u2016\u03bb\u0305\u2016_\u2113^\u03b1 = (\u2211_j=1^m \u03bb_j^\u03b1)^1/\u03b1.\n  This implies that one does not need the quantity \u2016 K \u2016_\u03b1\u2192\u03b1 and can provide the following, tighter bound:\n    \n    _X^n(E) \u2264 2^1/\u03b2exp(-1/\u03b2(2nt^2- (n-1)log(m \u2016\u03bb\u0305\u2016^\u03b2))).\n\n    We will now compare\u00a0<Ref>, with m=2 for simplicity (Controllo poi se posso ricalcolare tutto per m generale negli altri lavori e fare un confronto senza settare m=2. Un altro motivo per considerare m=2 viene dal fatto che si pu\u00f2 calcolare esplicitamente l'ipercontrattivit\u00e0 perche il caso della DSBS \u00e8 stato ampliamente studiato)., with the current state of the art. \n    One can then consider the limit of \u03b1\u2192\u221e which renders \u03b2\u2192 1. On the one hand, this implies a larger multiplicative coefficient (as H_\u03b1 grows with \u03b1, see\u00a0<Ref>) and, consequently, it increases the minimum value of t one can consider in\u00a0<Ref>. On the other hand, it guarantees a faster exponential decay in\u00a0eq:generalResultDiscreteHomogeneousHypereq:generalResultDiscreteHomogeneous2Hyper. \n In fact, as \u03b2\u2192 1, the RHS of\u00a0(<ref>) scales as exp(-2nt^2(1+o_n(1))) for large enough t, which matches the behavior of\u00a0<Ref>. \n Let us highlight that, to the best of our knowledge, our approach is the first to recover the same exponential decay rate obtained in the independent case, even in the presence of correlation among the random variables. \n \nWe remark that the convergence results in <Ref> and <Ref> are with respect to different constants: <Ref> considers the concentration of f around _\u2297_i=1^n X_i(f), while\u00a0<Ref> around _X^n(f). \nHowever, given the faster rate of convergence guaranteed by our framework \u2013 for this example and, even more impressively so, for the one in\u00a0<Ref> \u2013 the mean of f under the product of the marginals might be regarded as a natural object to consider when proving concentration results for these processes.  \n    \n    This hypothesis is corroborated by the approach presented in\u00a0<cit.>, where concentration for stationary Markov chains is provided around the mean with respect to the stationary measure \u03c0. Indeed, when X_1\u223c\u03c0, the product of the marginals reduces to the tensor product of the stationary measure \u03c0^\u2297 n. \n   \n \n    To make a direct comparison with\u00a0<cit.>, one can leverage\u00a0<cit.> (reproduced in <Ref>) and either reduce both results to concentration bounds around the median, or transform\u00a0<Ref> in a result on concentration around the mean with respect to _X^n. This would introduce additional constants, rendering the comparison cumbersome and outside the scope of this work. \n \n\n     \n\n  \u00a7.\u00a7.\u00a7 Comparison with <cit.>\n \n     \n<cit.> considers a more restricted setting in which f is the sum of n bounded functions that separately act on each of the n random variables, i.e., f=\u2211_i=1^n f_i(X_i) with f_i \u2208 [a_i,b_i].[This choice of f in\u00a0<Ref> transforms the result in a generalisation of Hoeffding's inequality to dependent settings. It is easy to see that if f_i\u2208 [a_i,b_i] then\u00a0(<ref>) holds with c_i = (b_i-a_i) and the statement follows.] By assuming further that a_i=0 and b_i=1/n for every i (e.g., empirical mean), we are in a setting in which\u00a0<Ref> holds as it is. Moreover, the setting in <cit.> requires the Markov chain to be time-homogeneous and admit a stationary distribution \u03c0 (notice that none of these assumptions are necessary for Theorems\u00a0<ref> and\u00a0<ref> to hold). In this case\u00a0<cit.> gives:\n\n    _X^n(|f -  \u03c0^\u2297 n(f)| \u2265 t ) \u2264 2 exp(-1-\u03bb/1+\u03bb2nt^2),\n\nwhere (1-\u03bb) denotes the absolute spectral gap of the Markov chain, see\u00a0<cit.>, which characterizes the speed of convergence to the stationary distribution. Comparing with\u00a0<Ref> with \u03b3^\u22c6_K(\u03b1)=\u03b1\u2192\u221e and \u03b3\u0305^\u22c6_K(\u03b1)=\u03b2\u2192 1,[As mentioned earlier, this optimizes the rate of convergence, at the expense of the value of t from which we obtain an improvement.] one has that if\n\n    t^2 > (1+o_n(1))1+\u03bb/4\u03bblog\u2016 K^\u2190\u2016_\u221e\u2192\u221e/P_i^\u22c6(j^\u22c6),\n\nthen\u00a0<Ref> provides a faster decay than\u00a0<Ref>. A more explicit comparison in which the absolute spectral gap is computed for a binary Markov kernel can be found in\u00a0<Ref>.\n   \n\n\n  \u00a7.\u00a7.\u00a7 Comparison with <cit.>\n    \n    Let us now derive the corresponding result of concentration around the median in order to compare with\u00a0<cit.>. Leveraging\u00a0<cit.> (see also <Ref>) along with\u00a0<Ref> (again, with \u03b3^\u22c6_K(\u03b1)=\u03b1\u2192\u221e and \u03b3\u0305^\u22c6_K(\u03b1)=\u03b2\u2192 1), we have \n    \n    _X^n(|f-m_f|\u2265 t) \n     \u2264 2 exp(-2n(t-\u221a(ln4+C_n/2n))^2+C_n),\n\n    where C_n=(n-1)log(\u2016 K^\u2190\u2016_\u221e\u2192\u221e/P_i^\u22c6(j^\u22c6)). \n    This also implies (see, e.g.,\u00a0<cit.> or\u00a0<cit.>) that, if t>\u221a(log4+C_n/2n) and given any event E such that _X^n(E)\u22651/2, then \n    \n    _X^n(E^c_t)    \u2264 2 exp(-2n(t-\u221a(log4+C_n/2n))^2+C_n)\n       = 1/2exp(-2nt^2+2t\u221a(2n(log4+C_n))),\n\n    where E_t = {y\u2208^n : d(x,y)\u2264 t  for some  x\u2208 E} and d denotes the normalised Hamming metric. \n    \n    Let us denote \n    a:=1-max_i max_x,x\u0302 TV(_X_i|X_i-1=x,_X_i|X_i-1=x\u0302).\n    \n    Hence, assuming t>1/a\u221a(log(1/_X^n(E))/n), <cit.> give\n    \n    _X^n(E^c_t)    \u2264exp(-2n(at -\u221a(log(1/_X^n(E))/2n))^2) \n       \u2264exp(-2nt^2a^2+2ta\u221a(2nlog2)).\n\n    Ignoring multiplicative constants and comparing the exponents of\u00a0(<ref>) and\u00a0(<ref>), one can see that, whenever\n     \n    t \u2265(1+o_n(1))\u221a(2log(\u2016 K^\u2190\u2016_\u221e\u2192\u221e/P_i^\u22c6(j^\u22c6)))/(1-a^2)\n    ,\n \n    \n    t \u2265\u221a(2ln2)(1-a)-\u221a(2(ln4+C_n))/\u221a(2n)(1-(1-a)^2),\n    then our approach improves upon <cit.>. \n    \n    The Dobrushin coefficient of the kernel, captured by the quantity (1-a), measures the degree of dependence of the stochastic model. The smaller 1-a is, the less \u201cdependent\u201d the model is. If _X^n reduces to a product distribution then a=1. In this case, <Ref> \n    boils down to McDiarmid's inequality. In contrast, the larger 1-a is, the worse the behavior of\u00a0<Ref>. If a=0, then the Markov chain is not contracting and violates the assumption of\u00a0<cit.>. Our approach, instead, can still provide meaningful results, as we will see in\u00a0<Ref>. A more explicit comparison for the case of a binary kernel can be found in\u00a0<Ref>.\n    \n    \n\n \u00a7.\u00a7 A non-contracting Markov chain\n\n     In order to provide concentration for Markov chains, existing work \n    requires either contractivity of the Markov chain\n    \u00a0<cit.>, stationarity\n    \u00a0<cit.> or some form of mixing\u00a0<cit.>.\n    A well-known Markov chain that evades most of these concepts is the SSRW. Suppose to have a sequence of i.i.d. Rademacher random variables, i.e., \u2119(X_i =  -1)=\u2119(X_i= +1)=1/2, for i\u2265 1; the initial condition is X_0=0 w.p. 1. Then, a SSRW is the Markov chain (S_i)_i\u2208\u2115 defined as S_i = S_i-1 + X_i. \n    This Markov chain does not admit a stationary distribution, it is not contracting, but it is expanding (in this case, both towards the positive and the negative axes of the real line). Let us denote with K_i the kernel at step i. Then, at each step i>2, one can always find two different realisations of S_i-1, let us call them s_1,s_2, such that  supp(K_i(\u00b7|s_1))\u2229supp(K_i(\u00b7|s_2))=\u2205, i.e., the support of S_i is constantly growing. \n    This implies that the approaches in\u00a0<cit.> cannot be employed, while\u00a0<cit.> yields:  \n    \n    \u2119(| f-_X^n(f)|\u2265 t) \u2264 2exp(-t^2/2n).\n\n    A meaningful regime (given also the expanding nature of the Markov chain along the integers)[The standard deviation of S_n is \u221a(n), hence it is expected that S_n is \u00b1 O(\u221a(n)).] \n    arises when considering t \u2273\u221a(n).\n    Let us now compute the Hellinger integral in this specific setting. This is done in the lemma below, proved in\u00a0<Ref>.\n    \n    Let i\u2265 1, x\u2208supp(S_i-1), 0\u2264 j\u2264 i, and \u03b1\u2265 1. Then,\n    \n    H_\u03b1^1/\u03b1(_S_i|S_i-1=x_S_n) \u2264 2^i 1/\u03b2-1+1/\u03b1,\n\n        and thus\n\n\n\n    \n    H_\u03b1^1/\u03b1(_S^n_\u2297_j=1^n S_j) \u2264 2^n(n-1)/2\u03b2.\n\n    \n   \n   Denoting  with 1/\u03b2= \u03b1-1/\u03b1,\u00a0<Ref> implies that\n    \n    H_\u03b1^1/\u03b1(_S^n_\u2297_j=1^n S_j)    \u2264\u220f_i=2^n max_x\u2208suppS_i-1 H_\u03b1^1/\u03b1(_S_i|S_i-1=x_S_i) \n       \u2264\u220f_i=2^n 2^1/\u03b1-1 + i/\u03b2\n       = 2^1/\u03b2\u2211_j=1^n-1 j = 2^n(n-1)/2\u03b2.\nCombining\u00a0<Ref> and\u00a0<Ref>, one has that\n    \n    \u2119(| f-_\u2297_i=1^n X_i(f)|\u2265 t )\u2264 2^1/\u03b2exp(-2nt^2/\u03b2+ n(n-1)/2\u03b2ln 2).\n\n    It is easy to see that, whenever t > \u221a((n-1)ln(2))/2, <Ref> gives an exponential decay. For instance, choosing t =\u221a(n),  \n    one retrieves\n    \n    \u2119(| f-_\u2297_i=1^n X_i(f)|\u2265\u221a(n)) \u2264 2^1/\u03b2exp(-n^2/\u03b2(2 - ln2/2 +ln2/2n)).\n\n    In contrast, the same choice in\u00a0<Ref> gives\n    \n    \u2119(| f-_X^n(f)|\u2265\u221a(n)) \u2264 2exp(-1/2).\n\n    More generally, selecting t of order \u221a(n) suffices to achieve an exponential decay in\u00a0<Ref>, while to obtain a similar speed of decay in\u00a0<Ref> t needs to be at least of order n. The approach advanced in this work can, thus, not only be employed in settings where most of the other approaches fail (e.g.,\u00a0<cit.>), but it also brings a significant improvement over the rate of decay that one can provide.\n    \n\n \u00a7.\u00a7 A non-Markovian Process\n \n    Next, we consider a non-Markovian setting in which each step of the stochastic process depends on its entire past: \n\n    \n    X_n = \n        +1,     with probability \u2211_i=0^n-1p_iX_i, \n    \n        -1,     with probability  \n        1 - \u2211_i=0^n-1p_iX_i,\n\n    for n\u2265 1, p_i>0 and X_0=+1 with probability 1.\n    The choice of the parameters p_i is arbitrary (given the constraint that \u2211_i=0^n-1 p_i < 1) but for concreteness we will set p_i = 2^-i-1 for every i\u2265 0. Then, \u2119_X_1(1|x_0)=1/2=\u2119_X_1(-1|x_0) and for each n\u2265 1,\n    \n    \u2119_X_n(1|x_0^n-1)=1/2+ \u2211_i=1^n-1p_ix_i= \u2211_i=0^n-1x_i 2^-i-1= 1- \u2119_X_n(-1|x_0^n-1),\n \n    \n    with x_0=1.\n    Consequently, following the calculations detailed in\u00a0<Ref>, we have\n    \n    H_\u03b1(_X_n(\u00b7|x_0^n-1) (1/2,1/2)) \n           < 2\u2211_j=0^\u230a\u03b1/2\u230b\u230a\u03b1\u230b2j(2\u2211_i=1^n-1p_ix_i)^2j,\n\n\nwhich, as p_i = 2^-i-1, gives \n    \n    max_x_1^n-1H_\u03b1(_X_n(\u00b7|x_0^n-1) (1/2,1/2))  < 2^\u03b1.\n Thus, H_\u03b1^1/\u03b1(_X^n(1/2,1/2)^\u2297 n)< 2^n-1 and an application of Theorem <ref> yields:\n    \n    \u2119({| f-_\u2297_i=1^n X_i(f)|\u2265 t}) \u2264inf_\u03b2>1 2^1/\u03b2exp(-2n/\u03b2(t^2-n-1/n\u03b2ln2/2)),\n\n    with exponential decay whenever\n    \n    t^2 > (1+o_n(1))\u03b2ln2/2.\n\n    \n    Like before, choosing a larger \u03b2 slows down the exponential decay, but it reduces the multiplicative coefficient introduced via H_\u03b1. For n \n    and t large enough, one can pick \u03b2\u2192 1 and retrieve a McDiarmid-like exponential decay.\n     Given that this setting does not characterise a Markovian dependence (at each step the stochastic process depends on its entire past), one cannot employ the technique described in\u00a0<cit.> or in\u00a0<cit.>. One can, however, employ the technique described in\u00a0<cit.> and\u00a0<cit.>. Both these approaches require the computation of {\u03b8\u0305_ij}_1\u2264 i < j \u2264 n with\n    \n    \u03b8\u0305_ij:=   sup_x^i-1, w,\u0175\u2016\u2112(X_j^n|X^i=(x^i-1, w)) - \u2112(X_j^n|X^i=(x^i-1,\u0175)) \u2016_TV,\n\n    where \u2112(X_j^n|X^i=(x^i-1,w)) denotes the conditional distribution of X_j^n given X^i=(x^i-1,w). The \u03b8\u0305's are then organised in n\u00d7 n upper-triangular matrices whose norms are computed in order to provide an upper bound on the probability of interest. In particular,\u00a0<cit.> requires the \u2113_2-norm, while\u00a0<cit.> requires the operator norm of the matrix.  \n    \n    \u2112(X_j^n = x_j^n|X^i=x^i-1w) = \u2211_x_i+1^j-1\u220f_k=i+1^n (1/2 + x_k(\u2211_m\u2260 i^k-1 p_m x_m + p_iw )).\n\n    Even for the simpler case j=i+1, computing\u00a0<Ref> is not easy. Indeed, one has  that:\n    \n    \u2112(X_i+1^n = x_i+1^n|X^i=x^i-1w) =  \u220f_k=i+1^n (1/2 + x_k(\u2211_m\u2260 i^k-1 p_m x_m + p_iw )).\n\n    Moreover,denoting with a_i+\u03c4 = 1/2 + x_i+\u03c4\u2211_m\u2260 i^k-1p_mx_m, with \ud835\udcae^i = {1,\u2026, n-i} and with \ud835\udcae^i_q the set of all the subsets of \ud835\udcae^i of size q one can see that\n    \n    \u03b7\u0305_i,i+1   = sup_(x^i-1\u2208 ,w,\u0175)1/2|\u2211_x_i+1^n\u2211_p=1^n-i\u2211_q=1^n-ip(w^|\ud835\udcae^i\u2216\ud835\udcae^i_q|-\u0175^|\ud835\udcae^i\u2216\ud835\udcae^i_q|) p_i^|\ud835\udcae^i\u2216\ud835\udcae^i_q|\u220f_\u03c4\u2208\ud835\udcae^i_q a_i+\u03c4\u220f_\u03c4\u0303\u2208\ud835\udcae^i\u2216\ud835\udcae^i_q x_i+\u03c4\u0303|\n\n    and it is not clear, a priori, how to compute such a supremum with respect to w,\u0175 and x^i-1. Picking a general j>i renders the computations even more difficult.Similarly, to employ the technique provided in\u00a0<cit.> one would need to compute the following quantity\n    \n    C\u0305 = max_1\u2264 i \u2264 nsup_x^i-1\u2208^i-1\n     w,\u0175\u2208inf_\u03c0\u2208\u03a0(\u2112(X^n|x^i,w), \u2112(X^n|x^i,\u0175))\u03c0(d).\n\n    Here, d denotes the normalised Hamming metric and \u03a0(\u2112(X^n|x^i,w), \u2112(X^n|x^i,\u0175)) represents the set of all the couplings defined on ^n\u00d7^n such that the corresponding marginals are \u2112(X^n|x^i,w) and \u2112(X^n|x^i,\u0175). However, computing any of these objects in practice can be complicated even in simple settings. In contrast, with the framework proposed here and thanks to the tensorisation properties of the Hellinger integral, one can easily bound the information measure and provide an exponentially  decaying probability (whenever the probability of the same event under independence decays exponentially and for opportune choices of the parameters).\n    \n    \n\n \u00a7.\u00a7 Markov Chain Monte Carlo\n\n    An intriguing application of the method proposed in\u00a0<cit.> consists in providing error bounds for Markov Chain Monte Carlo methods. For instance, assume that one is trying to estimate the mean \u03c0(f)  for some function f and some measure \u03c0 which cannot be directly sampled. A common approach consists in considering a Markov chain {X_i}_i\u2265 1 whose stationary distribution is \u03c0 and estimating \u03c0(f) via empirical averages of samples {X_i}_n_0+1^n_0+n, where n_0 characterises the so-called \u201cburn-in period\u201d. This period ensures that enough time has passed and the Markov chain is sufficiently close to the stationary distribution \u03c0 before sampling from it.\u00a0<cit.> gives that \n    \n    \u2119(1/n\u2211_i=1^n f(X_n_0+i)-\u03c0(f) > t ) \u2264 C(\u03bd,n_0,\u03b1)exp(-1/\u03b2\u00b71-max{\u03bb_r,0}/1+max{\u03bb_r,0}\u00b72nt^2/(b-a)^2),\n\n    where f:\u2192 [a,b] is uniformly bounded, \u03bb_r represents the right-spectral gap (see\u00a0<cit.>), \u03b1\u2208(1,+\u221e), \u03b2 denotes its H\u00f6lder's conjugate and C is a constant depending on the burn-in period n_0, the Radon-Nikodym derivative between the starting measure \u03bd and the stationary measure \u03c0, and \u03b1.\n    Using the tools provided in this work, we obtain:\n      \n    \u2119(1/n\u2211_i=1^n f(X_n_0+i)-\u03c0(f) > t )    \u2264exp(-2nt^2/\u03b2(b-a)^2) H_\u03b1^1/\u03b1(\u03bd K^n_0\u03c0) \u220f_i=2^n max_x_n_0+i-1 H_\u03b1^1/\u03b1(K(\u00b7|x_n_0+i-1)\u03c0)\n       \u2264 C(\u03bd,n_0,\u03b1)exp(-2nt^2/\u03b2(b-a)^2)max_x\u03c0({x})^-n-1/\u03b2 ,\n\n   where\u00a0<Ref> follows from the fact that H^1/\u03b1_\u03b1(\u03bd K^n_0\u03c0) represents the L^\u03b1(\u03c0)-norm of the Radon-Nikodym derivative and can thus be bounded like in\u00a0<cit.>.\n  Using the tools provided in this work, we obtain \n    \n    \u2119(1/n\u2211_i=1^n f(X_n_0+i)-\u03c0(f) > t ) \u2264exp(-2nt^2/\u03b2 (b-a)^2)H_\u03b1^1/\u03b1(_X_n_0+1\u2026 X_n_0+n\u03c0^\u2297 n).\nThe idea behind the result is as follows. Given that one is trying to estimate the mean of f under \u03c0 using empirical averages, if one had samples taken in an i.i.d. fashion from \u03c0, the exponential convergence would be guaranteed. However, the issue is that one does not have access to samples of \u03c0. Thus, changing the measure to an n-fold tensor product of \u03c0, one can still guarantee an exponential decay at the cost of a multiplicative price depending on how far the samples are from the stationary distribution.\n    By making a direct comparison,  assuming \u03bb_r>0, one can see that if \n    t^2    \u2265n-1/n(b-a)^2/21+\u03bb_r/2\u03bb_rlog(1/min_x \u03c0({x}))\n       =(1+o_n(1))(b-a)^2/21+\u03bb_r/2\u03bb_rlog(1/min_x \u03c0({x})),\n then the RHS of\u00a0<Ref> decays faster than the RHS of\u00a0<Ref>. A comparison for the binary symmetric channel, with the computations of all the parameters,  can be found in\u00a0<Ref>. \nLet K be a discrete contractive Markov kernel (as in\u00a0<Ref>) and, for simplicity, take n_0=0 (no burn-in period). Pick q\u2192 1 in\u00a0<Ref>, which gives that C= \u2016d\u03bd/d\u03c0\u2016_L^\u221e(\u03c0), i.e.,\n   \n    \u2119(1/n\u2211_i=1^n f(X_i)-\u03c0(f) > t ) \u2264exp(-1-max{\u03bb_r,0}/1+max{\u03bb_r,0}\u00b72nt^2/(b-a)^2)\u2016d\u03bd/d\u03c0\u2016_L^\u221e(\u03c0).\n\nFurthermore, <Ref> can be rewritten as\n     \n    \u2119(1/n\u2211_i=1^n f(X_i)-\u03c0(f) > t )    \u2264exp(-2nt^2/\u03b2(b-a)^2) H_\u03b1^1/\u03b1(\u03bd\u03c0) \u220f_i=2^n max_x_i-1 H_\u03b1^1/\u03b1(K(\u00b7|x_n_0+i-1)\u03c0)\n       \u2264exp(-2nt^2/\u03b2(b-a)^2)H_\u03b1^1/\u03b1(\u03bd\u03c0) max_x\u03c0({x})^-n-1/\u03b2  .\n\n    Taking then the limit of \u03b1\u2192\u221e (which allows to compare the fastest exponential decays between our approach and <cit.>), one has \n    \n    \u2119(1/n\u2211_i=1^n f(X_i)-\u03c0(f) > t ) \u2264exp(-2nt^2/(b-a)^2)  \u2016d\u03bd/d\u03c0\u2016_L^\u221e(\u03c0)(1/min_x\u03c0({x}))^n-1.\n\n\n\n    \n    \n    Similarly to\u00a0<cit.>, one can also show that an exponential decay is guaranteed in\u00a0<Ref> if n_0 = \u03a9(log n). Furthermore, as\u00a0<Ref> improves the exponential decay for t satisfying\u00a0<Ref>, in the same regime the induced lower bound over n_0 will be improved as well. Finally, we highlight that  \n    <Ref> applies to a much larger family of functions f than what can be handled by  <cit.>. \n    \n    \n    \n   \n    \n    n_0 > \u03b1/2log(1/\u03bb)log(2^2/\u03b2\u2016d\u03bd/d\u03c0-1\u2016_L^\u03b1(\u03c0)/2nt^2/d^2\u03b2+n-1/\u03b2log\u03c0^\u22c6),\n\n   where, similarly to before, d^2=(b-a)^2 and \u03c0^\u22c6 denotes max_x \u03c0({x}). Moreover, in the same regime of t as the one depicted in\u00a0<Ref> then\u00a0<Ref> improves over the lower bound which can be achieved from \u00a0<cit.>.\n    L'espressione mi sembra abbia senso, se lo spectral  gap \u03bb \u00e8 piccolo, allora la MC converge velocemente alla stazionaria ed n_0 diminuisce. Pi\u00f9 campioni considero (n grande) meno vincoli ho su n_0. Se \u03bd=\u03c0 allora \u2016d\u03bd/d\u03c0-1\u2016_L^\u03b1(\u03c0)=0 e non ho vincoli su n_0.\n    \n    Da qui in poi non so bene cosa voglio ottenere, pi\u00f9 o meno capire cosa posso ottenere con SDPI\n    \n    We also know that \n    \n    H^1/\u03b1_\u03b1(\u03bd K^n_0\u03c0 )    =H^1/\u03b1_\u03b1(\u03bd K^n_0\u03c0 K^n_0 ) \n       = ((\u03b1-1)\u210b_\u03b1(\u03bd K^n_0\u03c0 K^n_0) + 1)^1/\u03b1\n       \u2264 ((\u03b1-1)\u03b7_\u03b1(K^n_0)\u210b_\u03b1(\u03bd\u03c0) + 1)^1/\u03b1\n       = (\u03b7_\u03b1(K^n_0)(H_\u03b1(\u03bd\u03c0)-1) + 1)^1/\u03b1\n       = (\u03b7_\u03b1(K^n_0)H_\u03b1(\u03bd\u03c0) - \u03b7_\u03b1(K^n_0) + 1)^1/\u03b1\n       = H_\u03b1(\u03bd\u03c0)^1/\u03b1(\u03b7_\u03b1(K^n_0)-\u03b7_\u03b1(K^n_0)/H_\u03b1(\u03bd\u03c0)+1/H_\u03b1(\u03bd\u03c0))^1/\u03b1\n       =(\u03b7_\u03b1(K^n_0) H_\u03b1(\u03bd\u03c0))^1/\u03b1(1- 1/H_\u03b1(\u03bd\u03c0)+\u03b7_\u03b1(K^n_0)/H_\u03b1(\u03bd\u03c0))^1/\u03b1\n       \u2264 (2(1-(1-\u03b7_\u03b1(K))^n_0)H_\u03b1(\u03bd\u03c0))^1/\u03b1\n\n   \n\n   \n\n\u00a7 CONCLUSIONS\n\n   We introduced a novel approach to the concentration of measure for dependent random variables. The generality of our framework allows to consider arbitrary kernels without requiring either stationarity (as opposed to\u00a0<cit.>) or contractivity (as opposed to\u00a0<cit.>). Moreover, our technique applies to any family of functions which is known to concentrate when the random variables are actually independent. Said technique is employed and compared to the state of the art in four different settings: finite-state Markov chains, a non-contractive one (the SSRW), a non-Markovian process, and Monte Carlo Markov Chain methods. In each of these settings, we provide a regime of parameters in which we guarantee a McDiarmid-like decay and improve over existing results. \n    The improvement is the most striking in the case of the SSRW, where the only (closed-form) alternative approach gives a constant probability of deviation from the average, as opposed to the exponentially decaying probability guaranteed by our framework.\n    The bounds provided are usually characterised by a phase-transition-like behavior in terms of the accuracy t, i.e., one can show concentration only for values of t larger than a threshold depending on the Hellinger integral (and its scaling with respect to the number of variables n). \n    Consider for instance\u00a0<Ref>: if t^2> \u03b2/2nln H^1/\u03b1_\u03b1\u2248 (1+o_n(1))(\u03b2ln(2)/2), then the exponent is negative and one has exponential concentration, otherwise the exponent becomes positive and the bound trivialises to something larger than 1. We believe this to be an artifact of the analysis and not an intrinsic property of the concentration of measure phenomenon.\n    \n\n\u00a7 ACKNOWLEDGMENTS\n\nThe authors are partially supported by the 2019 Lopez-Loreta Prize. They would also like to thank Professor Jan Maas for providing valuable suggestions and comments on an early version of the work.\nIEEEtran\n\n\n\n\n\n\n\nsectionappendix\nsubsectionappendix\n\n\n\n\u00a7 PROOF OF\u00a0<REF>\n\n\n\nAssume that E={|f-_\u2297_i=1^n X_i(f)|\u2265 t}. Then, one has that\n\n    _X^n(E)    = \u222b1_E d_X^n\n       =  \u222b1_E d_X^n/d_\u2297_i=1^n X_i d_\u2297_i=1^n X_i\n       \u2264(\u222b1_E d_\u2297_i=1^n X_i)^\u03b1-1/\u03b1(\u222b(d_X^n/d_\u2297_i=1^n X_i)^\u03b1 d_\u2297_i=1^n X_i)^1/\u03b1\n       = _\u2297_i=1^n X_i^1/\u03b2(E) H_\u03b1^1/\u03b1(_X^n_\u2297_i=1^n X_i),\n\nwhere the inequality in the third line follows from H\u00f6lder's inequality. Moreover, \n\n    H_\u03b1(_X^n_\u2297_i=1^n X_i)     =  _X_1(_\u2297_i=2^n X_i(d_X_2^n|X_1/d_\u2297_i=2^n X_i)^\u03b1) \n       =_X_1(_X_2((d_X_2|X_1/d_X_2)^\u03b1(_\u2297_i=3^n X_i(d_X_3^n|X_2/d_\u2297_i=3^n X_i)^\u03b1))) \n       \u2264_X_1(_X_2((d_X_2|X_1/d_X_2)^\u03b1\u03b1_2)^1/\u03b1_2_X_2((_\u2297_i=3^n X_i(d_X_3^n|X_2/d_\u2297_i=3^n X_i)^\u03b1)^\u03b2_2)^1/\u03b2_2)  \n       = H_\u03b1^2 \u00b7_X_2((_\u2297_i=3^n X_i(d_X_3^n|X_2/d_\u2297_i=3^n X_i)^\u03b1)^\u03b2_2)^1/\u03b2_2,\n\nwhere the inequality follows from H\u00f6lder's inequality, the fact that the expectation is taken with respect to the product measure _\u2297_i X_i as well as the Markovity of _X^n. H_\u03b1^2 = _X_1^1/\u03b2_1(H_\u03b1\u03b1_2^\u03b2_1/\u03b1_2(_X_2|X_1_X_2)) with \u03b2_1=1 will now be the only term depending on X_1. Repeating the same sequence of steps (an application of the Disintegration Theorem\u00a0<cit.> to a Markovian setting, like in\u00a0<cit.>, followed by H\u00f6lder's inequality) (n-2) more times leads to the product of H_\u03b1^i as defined in the statement of the theorem.\nThe result then follows by noticing that \n    ^1/\u03b2_\u2297_i X_i(E)\u2264 2^1/\u03b2exp(-2nt^2/\u03b2),\n by McDiarmid's inequality.  \n\n\n\n\n\u00a7 CONCENTRATION AROUND MEAN AND MEDIAN\n\n  The following result is a useful tool that allows us to compare concentration around a constant, concentration around the mean and concentration around the median. It is used in order to compare our results with the ones proposed in\u00a0<cit.>. A proof can be found in\u00a0<cit.> and the statement is provided here for ease of reference.\n    Let f be a measurable function on a probability space (,\u03a9,\u03bc). Assume that, for some a_f\u2208\u211d and a non-negative function h:\u211d_+\u2192\u211d_+ such that lim_r\u2192\u221e h(r)=0,\n    \n    \u03bc({|f-a_f|\u2265 r })\u2264 h(r)\n for all r>0, then\n    \n    \u03bc({|f-m_f|\u2265 r +r_0})\u2264 h(r),\n\n    where m_f is the median of f and r_0 is such that h(r_0)<1/2. Moreover, if h\u0305 = \u222b_0^\u221e h(x)dx < \u221e, then f is \u03bc-integrable, |a_f-\u03bc(f)|\u2264h\u0305 and, for every r>0, \n    \n    \u03bc({|f-\u03bc(f)|\u2265 r +h\u0305})\u2264 h(r).\n\n    In particular, if h(r)\u2264 Cexp(-cr^p) with 0<p<\u221e, then\n    \n    \u03bc({|f-M|\u2265 r })\u2264 C'exp(-\u03ba_pcr^p),\n\n    where C' depends only on C and p, \u03ba_p depends only on p, and M is either the mean or the median.\n    \n \n\n\u00a7 TENSORISATION\n\n      Many information measures satisfy tensorisation properties, meaning that, if \u03bd and \u03bc are measures acting on an n-dimensional space (typically an n-fold Cartesian product of one-dimensional spaces), it is possible to compute the divergence between \u03bd and \u03bc using \u201cone-dimensional projections\u201d. This is particularly useful when the second measure is a product-measure. Indeed, if  and  are two probability measures on the space ^n and  is a product-measure, denoting with X\u0305^i the (n-1)-tuple (X_1,\u2026,X_i-1,X_i+1,\u2026,X_n), then\u00a0<cit.> gives that\n    \n    D() \u2264\u2211_i=1^n \u222b d_X\u0305^i D(Q_X_i|X\u0305^iP_X_i),\n\n    where D() denotes the KL-divergence between  and . \n    Hence, having access to a bound on D(Q_X_i|X\u0305^iP_X_i) for every 1\u2264 i\u2264 n implies a bound on the KL-divergence between the two n-dimensional measures  and . This property is pivotal in proving concentration results in a variety of ways\u00a0<cit.>. Since of independent interest, we will now state the tensorisation properties of H_\u03b1 as explicit results, as well as the corresponding R\u00e9nyi's D_\u03b1 analogues. \n    In particular, whenever the second measure is a product measure while the first one is Markovian, it is possible to prove the following.\n    \n    Let  and  be two probability measures on the space ^n such that \u226a, and assume that  is a product measure (i.e., =\u2297_i=1^n P_i). Assume also that  is a Markov measure induced by Q_1 and the kernels Q_i(\u00b7|\u00b7) with 1\u2264 i\u2264 n, i.e., (x^n) = Q_1(x_1)\u220f_i=2^n Q_i(x_i|x_i-1). Moreover, given a constant c, let X_0=c (almost surely) be an auxiliary random variable, then, \n    \n    H_\u03b1(Q) \u2264\u220f_i=1^n_X_i-1^1/\u03b2_i-1(H_\u03b1\u03b1_i^\u03b2_i-1/\u03b1_i(Q_i(\u00b7|X_i-1)P_X_i)),\n\n    where \u03b1_i\u2265 1 for i\u2265 0, \u03b2_0=1, \u03b1_n=1, and \u03b2_i = \u03b1_i/(\u03b1_i-1). \n    Moreover, selecting \u03b1_i\u2192 1^+ which implies \u03b2_i\u2192\u221e for every i\u2265 1, one recovers \n    \n    H_\u03b1() \u2264 H_\u03b1(Q_1P_1)\u00b7\u220f_i=2^n max_x_i-1\u2208 H_\u03b1(Q_i(\u00b7|x_i-1) P_i).\n\n    \n    \n        The proof follows from the steps undertaken in\u00a0eq:startTensoreq:endTensor along with the discussion immediately after, but replacing _X^n with \ud835\udcac. \n    \n    \n    Let us denote with h=d/d the density of  with respect to . Denote then with h_1 the marginal density h_1 = \u222b_^n-1 h d\n    \n    H_\u03b1() = (h^\u03b1)    = ((h_1h_2^n)^\u03b1) \n       = P_1(h_1^\u03b1 P^2\u2026 n((h_2^n)^\u03b1)) \n       \u2264 P_1((h_1^\u03b1)^\u03b2)^1/\u03b2 P_1((P^2\u2026 n((h_2^n)^\u03b1))^\u03b3)^1/\u03b3\n       = P_1((h_1^\u03b1)) _P_1P^2\u2026 n((h_2^n)^\u03b1),\n\n    where\u00a0<Ref> follows from Fubini's theorem as well as the fact that  is a product-measure and\u00a0<Ref> follows from H\u00f6lder's inequality with 1/\u03b2+1/\u03b3=1 and \u03b2,\u03b3 >1.\u00a0<Ref> follows from letting \u03b2\u2192 1 and, consequently, \u03b3\u2192\u221e.\n    The argument follows from repeating the same procedure in an iterated fashion and noticing that being  a Markovian product-measure, Q_2 (and the corresponding density with respect to P_2) will only depend on x_1 and x_2, Q_3 will only depend on x_2 and x_3 and so on. \n    \n    (Could be more formal).\u00a0<Ref>, which involves products of Hellinger integrals, can be re-written as a sum by considering R\u00e9nyi's divergences, due to the relationship connecting the two quantities (see\u00a0<Ref>). \n    \n    Under the same assumptions as in\u00a0<Ref>, one has that\n    \n    D_\u03b1() \u22641/\u03b1-1\u2211_i=1^n 1/\u03b2_i-1log_X_i-1(exp((\u03b1\u03b1_i-1)\u03b2_i-1/\u03b1_i(D_\u03b1\u03b1_i(Q_i(\u00b7|X_i-1)P_X_i))),\n\n     where \u03b1_i\u2265 1 for i\u2265 0, \u03b2_0=1, \u03b1_n=1 and \u03b2_i = \u03b1_i/(\u03b1_i-1). \n    Moreover, selecting \u03b1_i\u2192 1^+ which implies \u03b2_i\u2192\u221e for every i\u2265 1, one recovers \n        \n    D_\u03b1() \u2264 D_\u03b1(Q_1P_1) + \u2211_i=2^n max_x_i-1 D_\u03b1(Q_i(\u00b7|x_i-1) P_i).\n\n    \n    \n   \n  \n    This result allows us to control from above the Hellinger integral of two n-dimensional objects using the Hellinger integral of simpler 1-dimensional objects.\n    For instance, if the first measure represents the distribution induced by a time-homogeneous Markov chain, then all the kernels Q_i coincide and the expression becomes even easier to compute, as one can see in\u00a0<Ref>.\n    \n    A similar approach can also be employed to provide a result in cases where  is an arbitrary measure.\n     \n    Let  and  be two probability measures on the space ^n such that \u226a, and assume that  is a product-measure (i.e., =\u2297_i=1^n P_i). Then, \n    \n    H_\u03b1() \u2264 H_\u03b1(Q_1P_1)\u00b7\u220f_i=2^n max_x_1^i-1=x_1\u2026 x_i-1\u2208^i-1 H_\u03b1(Q_i(\u00b7|x_1^i-1) P_i).\n\n    Similarly, one has that \n     \n    D_\u03b1() \u2264 D_\u03b1(Q_1P_1) + \u2211_i=2^n max_x_1^i-1=x_1\u2026 x_i-1\u2208\ud835\udcb3^i-1 D_\u03b1(Q_i(\u00b7|x_1^i-1) P_i).\n\n    \n    The proof follows from the same argument of\u00a0<Ref>. The key difference is that, without making any additional assumptions on , the entire \u201cpast\u201d of the process needs to be considered. \n    This is why writing an expression similar to\u00a0<Ref> without Markovity would be rather cumbersome, and we restrict ourselves to the setting where the additional parameters are all considered to be such that  \u03b2_j\u2192\u221e. \n    \n    Finally, we show some lower bounds on Hellinger integrals and R\u00e9nyi's divergences. \n    \n        Let  and  be two probability measures on the space ^n such that \u226a, and assume that  is a product-measure (i.e., =\u2297_i=1^n P_i). Assume also that  is a Markov measure induced by Q_1 and the kernels Q_i(\u00b7|\u00b7) with 1\u2264 i\u2264 n, i.e., (x^n) = Q_1(x_1)\u220f_i=2^n Q_i(x_i|x_i-1). Moreover, given a constant c, let X_0=c (almost surely) be an auxiliary random variable, then, \n    \n    H_\u03b1(Q) \u2265\u220f_i=1^n_X_i-1^1/\u03b2_i-1(H_\u03b1\u03b1_i^\u03b2_i-1/\u03b1_i(Q_i(\u00b7|X_i-1)P_X_i)),\n\n    and\n    \n    D_\u03b1() \u22651/\u03b1-1\u2211_i=1^n 1/\u03b2_i-1log_X_i-1(exp((\n            \u03b1_i)(\u03b1\u03b1_i-1)\u03b2_i-1/\u03b1_i(D_\u03b1\u03b1_i(Q_i(\u00b7|X_i-1)P_X_i))),\n\n   where \u03b1_i\u2264 1 for i\u2265 1, \u03b2_0=1, \u03b1_n=1 and \u03b2_i = \u03b1_i/(\u03b1_i-1). \n    Moreover, selecting \u03b1_1\u21921^- which implies \u03b2_i\u2192-\u221e for every i\u2265 1, one recovers \n    \n    H_\u03b1() \u2265 H_\u03b1(Q_1P_1)\u00b7\u220f_i=2^n min_x_i-1\u2208 H_\u03b1(Q_i(\u00b7|x_i-1) P_i),\n\n    which, in the case of R\u00e9nyi's divergences, specialises to\n    \n    D_\u03b1() \u2265 D_\u03b1(Q_1P_1) + \u2211_i=2^n min_x_i-1\u2208\ud835\udcb3 D_\u03b1(Q_i(\u00b7|x_i-1) P_i).\n\n        \n    \n   \n   \n   \n    \n        The proof follows from similar arguments as  the proof of\u00a0<Ref>. The sole difference is that, instead of using H\u00f6lder's inequality at each step, one uses reverse H\u00f6lder's inequality. \n    \n\n\n\u00a7 EXPLICIT COMPARISON FOR A BINARY KERNEL\n\nThe setting is the following: let K be a time-homogeneous Markov chain  characterised by a doubly-stochastic transition matrix characterised by the vector of parameters \u03bb\u0305= (\u03bb_1,\u2026,\u03bb_m) (i.e., the rows and columns of K are permutations of \u03bb\u0305 with the constraints  \u2211_i K_i,j = \u2211_j K_i,j = 1). In this case, the Markov chain admits a stationary distribution \u03c0 which corresponds to the uniform distribution over the sample space. Hence, if one is considering an m-dimensional space, then \u03c0({x})=1/m for x\u2208{1,\u2026,m}.\n    In this case, if P_1 \u223c\u03c0, then P_i \u223c\u03c0 for every i\u2265 1. Moreover, one can prove that K=K^\u2190 and the bound of\u00a0<Ref> reduces to \n    _X^n({| f-_\u2297_n X_n(f)|\u2265 t ) \u2264 2^1/\u03b2exp(-1/\u03b2(2nt^2- (n-1)log(m \u2016\u03bb\u0305\u2016_\u2113^\u03b1^\u03b2))),\n\n    Henceforth, we will consider m=2 for simplicity and, thus, \u2016\u03bb\u0305\u2016_\u2113^\u03b1:=(\u2211_i=1^m\u03bb_i^\u03b1)^1/\u03b1 can be expressed as (\u03bb^\u03b1 + (1-\u03bb)^\u03b1)^1/\u03b1. Specialising\u00a0<Ref> to this setting, one obtains the following result.\n    \n    \n      Let n>1, and let X_1,\u2026,X_n be a sequence of random variables such that X_1\u223c\u03c0. For every \u03b1>1 and every function f such that\u00a0<Ref> holds true, one has that  \n      \n    \u2119(| f-_\u2297_i=1^n X_i(f)|\u2265 t )\u2264 2^1/\u03b2exp(-2nt^2/\u03b2 +n-1/\u03b2ln(2((1-\u03bb)^\u03b1+\u03bb^\u03b1)^1/\u03b1-1)) .\n\n      \n\n\n \u00a7.\u00a7 Comparison with\u00a0<cit.>\nThe bound obtained via the techniques of\u00a0<cit.> is\n      \n    \u2119(| f-_X^n(f)|\u2265 t)   \u2264\n              2exp(-2\u03bb^2 nt^2/((1-2\u03bb)^n-1)^2).\n\n      Let us denote \u03ba_\u03b1 := ((1-\u03bb)^\u03b1+\u03bb^\u03b1))^1/\u03b1-1 <1. Then, by direct comparison, it is possible to see that, whenever \n    t^2>((1-2\u03bb)^n-1)^2(1-1/n)ln(2\u03ba_\u03b1)/2((1-2\u03bb)^n-1)^2-\u03b2\u03bb^2):=t\u0305^2,\n \n    then the RHS of\u00a0(<ref>) decays faster than the RHS of\u00a0(<ref>). In particular, for a given \u03bb<1/2 and if  \u03b1>4/3, then\n    \n    lim_n\u2192+\u221e   ((1-2\u03bb)^n-1)^2(1-1/n)ln(2\u03ba_\u03b1)/2((1-2\u03bb)^n-1)^2-\u03b2\u03bb^2)\n       =ln(2 \u03ba_\u03b1)/2(1-\u03b2\u03bb^2)<2/4-\u03b2ln2.\n\n    t\u0305^2= (1+o_n(1))ln(2\u03ba_\u03b1)/2(1-\u03b2\u03bb^2) < (1+o_n(1))2ln2/4-\u03b2.\n\n\n\n\n\n    \n \n   Here, one can explicitly see the trade-off between the probability term and the Hellinger integral, described in\u00a0<Ref> and mediated by the choice of \u03b1. Taking the limit \u03b1\u2192\u221e in <Ref> leads to the following upper bound\n    \n    \u2119(| f-_\u2297_i=1^n X_i(f)|\u2265 t )\u2264 2exp(-2nt^2 + (n-1)ln(2(1-\u03bb))) .\n\n    In this setting, in order to improve over\u00a0<Ref>, one needs t^2>t\u0305^2, with\n    \n    t\u0305^2 = (1+o_n(1))ln(2(1-\u03bb))/2(1-\u03bb^2)< (1+o_n(1))2ln2/3.\n\n    Clearly, 1-\u03bb>\u03ba_\u03b1 for every \u03b1>1. Hence, on the one hand, <Ref> introduces a worse multiplicative constant (a larger \u03b1 implies a larger Hellinger integral, and we are considering the limit of \u03b1\u2192\u221e) and increases the minimum value of t one can consider for a given \u03bb<1/2 (<Ref> is increasing in \u03b1). On the other hand, it provides a faster exponential decay with n. In fact, as \u03b2\u2192 1, the RHS of (<ref>) scales as exp(-2nt^2(1+o_n(1))) for large enough t, which matches the behavior of\u00a0<Ref>. \n    \n\n \u00a7.\u00a7 Comparison with\u00a0<cit.>\n\n    In the setting considered above, <cit.> gives\n\n\n\n    \n    \u2119(| f-\u03c0^\u2297 n(f)|\u2265 t ) \u2264 2exp(-2\u03bb/1-\u03bbnt^2).\n\n\n    This means that, considering the decay provided by\u00a0<Ref> (which optimises the speed of decay for large enough t) whenever \n    t^2 > 1/2(1-\u03bb)/(1-\u03bb-\u03b2\u03bb)ln(2\n        \u03ba_\u03b1)(1+o_n(1)),\n\n    then\u00a0<Ref> decays faster than\u00a0<Ref>. \n    \n\n  \n   In particular, given \u03b2>1 one can choose \u03bb = 1/\u03b2+1<1/2 and render\u00a0<Ref> arbitrarily large. However, with the characterisation provided in\u00a0<Ref> one can see that in this case, one has that whenever \n    \n    t^2 > 1-\u03bb/2-4\u03bbln(2(1-\u03bb))(1+o_n(1)),\n\n    then\u00a0<Ref> leads to a faster decay than\u00a0<Ref>. \n    \n\n \u00a7.\u00a7 Comparison with\u00a0<cit.>\n \n    For the kernel considered in this appendix, <Ref> holds with C_n= (n-1)log(2(1-\u03bb)). Furthermore, for every i and x, x\u0302, we have that\n    TV(_X_i|X_i-1=x,_X_i|X_i-1=x\u0302)=(1-2\u03bb). Consequently, assuming t>1/(2\u03bb)\u221a(ln(1/_X^n(E))/n), <cit.> give\n    \n    _X^n(E^c_t)    \u2264exp(-2n(t(2\u03bb) -\u221a(ln(1/_X^n(E))/2n))^2) \n       \u2264exp(-8nt^2\u03bb^2+8t\u03bb\u221a(nln2/2)).\n\n    Comparing\u00a0<Ref> with C_n=(n-1)log(2(1-\u03bb)) with\u00a0<Ref>, one can see that, whenever\n     \n    t \u2265\u221a(2ln(2(1-\u03bb)))/(1-4\u03bb^2)(1+o_n(1)),\n\nthen\u00a0<Ref> improves over\u00a0<Ref>. \n\nA similar comparison can be drawn with respect to the tools in\u00a0<cit.> (in which the degree of dependence is measured differently), but it would lead to a worse bound than that expressed in\u00a0<Ref>.\n\n\n \u00a7.\u00a7 MCMC\n\nConsidering the same setting detailed in the previous subsections, one can more explicitly characterise the parameters determining\u00a0<Ref>. In particular, one has that \u03c0 = (1/2,1/2) and the spectral gap is equal to 1-2\u03bb. Consequently, if n_0=0 and one considers \u03b1\u2192\u221e,\u00a0<Ref> becomes:\n\n    \u2119(1/n\u2211_i=1^n f(X_n_0+i)-\u03c0(f) > t ) \u2264 2exp(-\u03bb/1-\u03bb\u00b72nt^2/(b-a)^2)\u00b7max{\u03bd({0}),\u03bd({1})},\n\nwhile\u00a0<Ref> boils down to the following:\n\n    \u2119(1/n\u2211_i=1^n f(X_n_0+i)-\u03c0(f) > t ) \u2264 2exp(-2nt^2/(b-a)^2)(2-2\u03bb)^n-1\u00b7max{\u03bd({0}),\u03bd({1})}.\n\nMaking a direct comparison one can see that if\n\n    t^2 \u2265((1-1/n)ln(2-2\u03bb))(b-a)^2/21-\u03bb/1-2\u03bb,\n\nthen\u00a0<Ref> provides a faster decay than\u00a0<Ref>.\n\n\n \u00a7.\u00a7 Comparison between SDPI for D_\u03b1\nand hypercontractivity\n\n\nIf \u03bc=(1/2,1/2) and K=BSC(\u03bb), then \u03bc K= \u03bc and one has, following Wyner's notation\u00a0<cit.>, the so-called Doubly-Symmetric Binary Source \u201cDSBS(\u03bb)\u201d. In this setting, the hyper-contractivity constant is given by <cit.> \n    \u03b3^\u22c6(\u03b1)=(1-2\u03bb)^2(\u03b1-1)-1 .\nMoreover, one can analytically see that, for every \u03bc,\n\n    D_\u03b1(K\u03bc K)/D_\u03b1(\u03b4_0\u03bc) < (1-2\u03bb)^(1+1/\u03b1)/(1-\u03bb)^(\u03b1-1)/\u03b1.\n\nAs \u03b1\u21921^+, the LHS of\u00a0<Ref> approaches a ratio between KL-divergences, while the RHS approaches (1-2\u03bb)^2, which equals \u03b7_KL(K)\u00a0<cit.>. Furthermore, taking the limit of \u03b1\u2192\u221e (which optimises the exponential rate of decay), the expression in\u00a0<Ref> provides an improvement over simply using DPI, while\u00a0<Ref> trivialises to \u03b3^\u22c6(+\u221e) = +\u221e. Hence, denoting with E={|f-_\u2297 X_i(f)|\u2265 t}, one has that, \nfor every \u03b1>1,\n\n    _X^n(E) \u2264 2^1/\u03b2exp(-2nt^2/\u03b2)\u00b7exp((1-2\u03bb)^2(\u03b1-1)-2/(1-2\u03bb)^2(\u03b1-1)-1(n-1)log2)   via <Ref>&<Ref>, \n    exp(1/\u03b2(1-2\u03bb)^(1+1/\u03b1)/(1-\u03bb)^(\u03b1-1)/\u03b1 (n-1)log2)    via <Ref>&<Ref>.\n\nOne can see that, for \u03b1 large enough, <Ref> improves upon\u00a0<Ref> for every \u03bb. In fact, taking \u03b1\u2192\u221e gives\n\n    _X^n(E) \u2264 2 exp(-2nt^2  )\u00b7exp((n-1)log2)   via <Ref>&<Ref>, \n    exp((1-2\u03bb)/(1-\u03bb) (n-1)log2)    via <Ref>&<Ref>.\n\n \n\n\n\u00a7 PROOF OF <REF>\n\n\n\n    For every n\u2265 0, we have \n    supp(S_n)=\u22c3_j=0^n { n-2j}.\n Moreover, given 0\u2264 j \u2264 n, \n\n    \u2119(S_n= n-2j) = nn-j2^-n.\n Furthermore, given x\u2208supp(S_n-1) and 0\u2264 j\u2264 n,\n    \n    \u2119_S_n|S_n-1=x(n-2j)= 1/21_{n-2j-x=1} + 1/21_{n-2j-x=-1}.\n\nTherefore, the following chain of inequalities holds:\n        \n    H_\u03b1 (P_S_n|S_n-1=xP_S_n)    = \u2211_j=0^n \u2119^\u03b1_S_n|S_n-1=x(n-2j) \u00b7\u2119_S_n^1-\u03b1(n-2j) \n       = 2^-\u03b1(\u2119_S_n^1-\u03b1(x+1)+\u2119_S_n^1-\u03b1(x-1))\n       = 2^-\u03b1((nn+x+1/2 2^-n)^1-\u03b1+(nn+x-1/2 2^-n)^1-\u03b1)\n       = 2^-\u03b1+n(\u03b1-1)((nn+x+1/2)^1-\u03b1+(nn+x-1/2)^1-\u03b1)\n       \u2264 2^-\u03b1+n(\u03b1-1)((n/n+x+1/2)^n+x+1/2(1-\u03b1)+(n/n+x-1/2)^n+x-1/2(1-\u03b1)) \n    \n           \n               = 2^-\u03b1+n(\u03b1-1)((n+x+1/2n)^n+x+1/2(\u03b1-1)+(n+x-1/2n)^n+x-1/2(\u03b1-1))\n       \u2264 2^-\u03b1+n(\u03b1-1)((n+x+1/2n)^n+x+1/2(\u03b1-1)+(n+x+1/2n)^n+x-1/2(\u03b1-1)) \n       = 2^-\u03b1+n(\u03b1-1)(n+x+1/2n)^n+x-1/2(\u03b1-1)\u00b7(1+(n+x+1/2n)^(\u03b1-1)) \n       \u2264 \n            2^-\u03b1+n(\u03b1-1)+1.\n      \n        Here, the inequality (<ref>) follows from the fact that nk\u2265(n/k)^k along with 1-\u03b1\u2264 0; the inequality (<ref>) follows from the fact that n+x+1/2n>0. \n        Moreover, it is easy to see that  n+x+1/2= n+x-1/2+1 and thus the factorisation in\u00a0<Ref> follows. To conclude, it suffice to notice that  n+x+1/2n\u2264 1.\n        Denoting  with 1/\u03b2= \u03b1-1/\u03b1, the computations just above imply that\n    \n    H_\u03b1^1/\u03b1(_S^n_\u2297_j=1^n S_j)    \u2264\u220f_i=2^n max_x\u2208suppS_i-1 H_\u03b1^1/\u03b1(_S_i|S_i-1=x_S_i) \n       \u2264\u220f_i=2^n 2^1/\u03b1-1 + i/\u03b2 =  \u220f_i=2^n 2^-1/\u03b2+ i/\u03b2 = 2^1/\u03b2\u2211_i=2^n (i-1) = 2^1/\u03b2\u2211_j=1^n-1 j = 2^n(n-1)/2\u03b2.\n\n\n\n    \n\n\u00a7 PROOF OF THE INEQUALITIES IN (<REF>) AND (<REF>)\n\n    \n    \nGiven the setting, denoting with y_n = \u2211_i=1^n-1p_ix_i, one has that\n\n    H_\u03b1(_X^n|X^n-1=x_1^n-1(1/2,1/2))    = (1/2)^1-\u03b1((1/2+\u2211_i=1^n-1p_ix_i)^\u03b1+(1/2-\u2211_i=1^n-1p_ix_i)^\u03b1) \n       \u2264(1/2)^1-\u03b1((1/2+\u2211_i=1^n-1p_ix_i)^\u230a\u03b1\u230b+(1/2-\u2211_i=1^n-1p_ix_i)^\u230a\u03b1\u230b) \n       = (1/2)^1-\u03b1(\u2211_k=0^\u230a\u03b1\u230b\u230a\u03b1\u230bk(1/2)^\u230a\u03b1\u230b-ky_n^k + \u2211_k=0^\u230a\u03b1\u230b\u230a\u03b1\u230bk(1/2)^\u230a\u03b1\u230b-k(-y_n)^k) \n       = (1/2)^1-\u03b1(\u2211_k=0^\u230a\u03b1\u230b\u230a\u03b1\u230bk(1/2)^\u230a\u03b1\u230b-k(y_n^k +(-y_n)^k))\n\n\n    = (1/2)^1-\u03b1(\u2211_j=0^\u230a\u03b1/2\u230b\u230a\u03b1\u230b2j(1/2)^\u230a\u03b1\u230b-2j2y_n^2j) \n       \u2264 2\u2211_j=0^\u230a\u03b1/2\u230b\u230a\u03b1\u230b2j (2y_n)^2j.\n\nMoreover, setting p_i = 2^-i-1 gives \n\n    max_x_1^n-1H_\u03b1(_X^n|X^n-1=x_1^n-1(1/2,1/2))    \u22642max_x_1^n-1\u2211_j=0^\u230a\u03b1/2\u230b\u230a\u03b1\u230b2j(2\u2211_i=1^n-1p_ix_i)^2j\n       =2 \u2211_j=0^\u230a\u03b1/2\u230b\u230a\u03b1\u230b2j(\u2211_i=1^n-12^-i)^2j\n       =2\u2211_j=0^\u230a\u03b1/2\u230b\u230a\u03b1\u230b2j(1-2^-n+1)^2j\n       \u2264 2\u2211_j=0^\u230a\u03b1/2\u230b\u230a\u03b1\u230b2j = 2^\u230a\u03b1\u230b\u2264 2^\u03b1.\n\n\n"}