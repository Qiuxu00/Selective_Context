{"entry_id": "http://arxiv.org/abs/2303.06674v1", "published": "20230312142824", "title": "Universal Instance Perception as Object Discovery and Retrieval", "authors": ["Bin Yan", "Yi Jiang", "Jiannan Wu", "Dong Wang", "Ping Luo", "Zehuan Yuan", "Huchuan Lu"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUniversal Instance Perception as Object Discovery and Retrieval\n    Bin Yan^1This work was performed while Bin Yan worked as an intern at\nByteDance. Email: mailto:yan_bin@mail.dlut.edu.cnyan_bin@mail.dlut.edu.cn. ^\u2020 Corresponding authors: mailto:jiangyi.enjoy@bytedance.comjiangyi.enjoy@bytedance.com, mailto:wdice@dlut.edu.cnwdice@dlut.edu.cn.,\nYi Jiang^2, \u2020,\nJiannan Wu^3, \nDong Wang^1, \u2020, \n\nPing Luo^3,\nZehuan Yuan^2,\nHuchuan Lu^1,4\n\n^1 School of Information and Communication Engineering, Dalian University of\nTechnology, China \n\n^2 ByteDance ^3 The University of Hong Kong ^4 Peng Cheng Laboratory\n\n    March 30, 2023\n===========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\n\n\n\n\nAll instance perception tasks aim at finding certain objects specified by some queries such as category names, language expressions, and target annotations, but this complete field has been split into multiple independent sub-tasks.\nIn this work, we present a universal instance perception model of the next generation, termed UNINEXT.\nUNINEXT reformulates diverse instance perception tasks into a unified object discovery and retrieval paradigm and can flexibly perceive different types of objects by simply changing the input prompts. This unified formulation brings the following benefits: (1) enormous data from different tasks and label vocabularies can be exploited for jointly training general instance-level representations, which is especially beneficial for tasks lacking in training data. (2)  the unified model is parameter-efficient and can save redundant computation when handling multiple tasks simultaneously.  \nUNINEXT shows superior performance on 20 challenging benchmarks from 10 instance-level tasks including classical image-level tasks (object detection and instance segmentation), vision-and-language tasks (referring expression comprehension and segmentation), and six video-level object tracking tasks. \nCode is available at https://github.com/MasterBin-IIAU/UNINEXThttps://github.com/MasterBin-IIAU/UNINEXT.\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\n\n\nObject-centric understanding is one of the most essential and challenging problems in computer vision. Over the years, the diversity of this field increases substantially. In this work, we mainly discuss 10 sub-tasks, distributed on the vertices of the cube shown in Figure\u00a0<ref>. As the most fundamental tasks, object detection\u00a0<cit.> and instance segmentation\u00a0<cit.> require finding all objects of specific categories by boxes and masks respectively. Extending inputs from static images to dynamic videos, Multiple Object Tracking (MOT)\u00a0<cit.>, Multi-Object Tracking and Segmentation (MOTS)\u00a0<cit.>, and Video Instance Segmentation (VIS)\u00a0<cit.> require finding all object trajectories of specific categories in videos. Except for category names, some tasks provide other reference information. For example, Referring Expression Comprehension (REC)\u00a0<cit.>, Referring Expression Segmentation (RES)\u00a0<cit.>, and Referring Video Object Segmentation (R-VOS)\u00a0<cit.> aim at finding objects matched with the given language expressions like \u201cThe fourth person from the left\u201d. Besides, Single Object Tracking (SOT)\u00a0<cit.> and Video Object Segmentation (VOS)\u00a0<cit.> take the target annotations (boxes or masks) given in the first frame as the reference, requiring to predict the trajectories of the tracked objects in the subsequent frames. Since all the above tasks aim to perceive instances of certain properties, we refer to them collectively as instance perception.\n\n\n\n\n\nAlthough bringing convenience to specific applications, such diverse task definitions split the whole field into fragmented pieces. As the result, most current instance perception methods are developed for only a single or a part of sub-tasks and trained on data from specific domains. Such fragmented design philosophy brings the following drawbacks: (1) Independent designs hinder models from learning and sharing generic knowledge between different tasks and domains, causing redundant parameters.  (2) The possibility of mutual collaboration between different tasks is overlooked. For example, object detection data enables models to recognize common objects, which can naturally improve the performance of REC and RES. (3) Restricted by fixed-size classifiers, traditional object detectors are hard to jointly train on multiple datasets with different label vocabularies\u00a0<cit.> and to dynamically change object categories to detect during inference\u00a0<cit.>. Since essentially all instance perception tasks aim at finding certain objects according to some queries, it leads to a natural question: could we design a unified model to solve all mainstream instance perception tasks once and for all?\n\nTo answer this question, we propose UNINEXT, a universal instance perception model of the next generation. We first reorganize 10 instance perception tasks into three types according to the different input prompts: (1) category names as prompts (Object Detection, Instance Segmentation, VIS, MOT, MOTS). (2) language expressions as prompts (REC, RES, R-VOS). (3) reference annotations as prompts (SOT, VOS). Then we propose a unified prompt-guided object discovery and retrieval formulation to solve all the above tasks. Specifically, UNINEXT first discovers N object proposals under the guidance of the prompts, then retrieves the final instances from the proposals according to the instance-prompt matching scores. Based on this new formulation, UNINEXT can flexibly perceive different instances by simply changing the input prompts. To deal with different prompt modalities, we adopt a prompt generation module, which consists of a reference text encoder and a reference visual encoder. Then an early fusion module is used to enhance the raw visual features of the current image and the prompt embeddings. This operation enables deep information exchange and provides highly discriminative representations for the later instance prediction step. Considering the flexible query-to-instance fashion, we choose a Transformer-based object detector\u00a0<cit.> as the instance decoder. Specifically, the decoder first generates N instance proposals, then the prompt is used to retrieve matched objects from these proposals. This flexible retrieval mechanism overcomes the disadvantages of traditional fixed-size classifiers and enables joint training on data from different tasks and domains.\n\nWith the unified model architecture, UNINEXT can learn strong generic representations on massive data from various tasks and solve 10 instance-level perception tasks using a single model with the same model parameters. Extensive experiments demonstrate that UNINEXT achieves superior performance on 20 challenging benchmarks. The contributions of our work can be summarized as follows. \n\n\t\n\n\t\n  * We propose a unified prompt-guided formulation for universal instance perception, reuniting previously fragmented instance-level sub-tasks into a whole.\n\t\n\n\t\n  * \n\tBenefiting from the flexible object discovery and retrieval paradigm, UNINEXT can train on different tasks and domains, in no need of task-specific heads.\n\t\n\t\n\n\t\n  * UNINEXT achieves superior performance on 20 challenging benchmarks from 10 instance perception tasks using a single model with the same model parameters.\n\n\n\n\n\n\u00a7 RELATED WORK\n\nInstance Perception. The goals and typical methods of 10 instance perception tasks are introduced as follows. \n\n\nRetrieval by Category Names. Object detection and instance segmentation aim at finding all objects of specific classes on the images in the format of boxes or masks. Early object detectors can be mainly divided into two-stage methods\u00a0<cit.> and one-stage methods\u00a0<cit.> according to whether to use RoI-level operations\u00a0<cit.>. Recently, Transformer-based detectors\u00a0<cit.> have drawn great attention for their conceptually simple and flexible frameworks. Besides, instance segmentation approaches can also be divided into detector-based\u00a0<cit.> and detector-free\u00a0<cit.> fashions according to whether box-level detectors are needed. Object detection and instance segmentation play critical roles and are foundations for all other instance perception tasks. \nFor example, MOT, MOTS, and VIS extend image-level detection and segmentation to videos, requiring finding all object trajectories of specific classes in videos. Mainstream algorithms\u00a0<cit.> of MOT and MOTS follow an online \"detection-then-association\" paradigm. However, due to the intrinsic difference in benchmarks of MOTS\u00a0<cit.> (high-resolution long videos) and VIS\u00a0<cit.> (low-resolution short videos), most recent VIS methods\u00a0<cit.> adopt an offline fashion. This strategy performs well on relatively simple VIS2019\u00a0<cit.>, but the performance drops drastically on challenging OVIS\u00a0<cit.> benchmark. Recently, IDOL\u00a0<cit.> bridges the performance gap between online fashion and its offline counterparts by discriminative instance embeddings, showing the potential of the online paradigm in unifying MOT, MOTS, and VIS. \n\nRetrieval by Language Expressions. REC, RES, and R-VOS aim at finding one specific target referred by a language expression using boxes or masks on the given images or videos. Similar to object detection, REC methods can be categorized into three paradigms: two-stage\u00a0<cit.>, one-stage\u00a0<cit.>, and Transformer-based\u00a0<cit.> ones. Different from REC, RES approaches\u00a0<cit.> focus more on designing diverse attention mechanisms to achieve vision-language alignment. Recently, SeqTR\u00a0<cit.> unifies REC and RES as a point prediction problem and obtains promising results. Finally, R-VOS can be seen as a natural extension of RES from images to videos. Current state-of-the-art methods\u00a0<cit.> are Transformer-based and process the whole video in an offline fashion. However, the offline paradigm hinders the applications in the real world such as long videos and ongoing videos (e.g. autonomous driving).\n\nRetrieval by Reference Annotations. SOT and VOS first specify tracked objects on the first frame of a video using boxes or masks, then require algorithms to predict the trajectories of the tracked objects in boxes or masks respectively. The core problems of these two tasks include (1) How to extract informative target features? (2) How to fuse the target information with representations of the current frame? For the first question, most SOT methods\u00a0<cit.> encode target information by passing a template to a siamese backbone. While VOS approaches\u00a0<cit.> usually pass multiple previous frames together with corresponding mask results to a memory encoder for extracting fine-grained target information. For the second question, correlations are widely adopted by early SOT algorithms\u00a0<cit.>. However, these simple linear operations may cause serious information loss. To alleviate this problem, later works\u00a0<cit.> resort to Transformer for more discriminative representations. Besides, feature fusion in VOS is almost dominated by space-time memory networks\u00a0<cit.>.\n\nUnified Vision Models. \nRecently, unified vision models\u00a0<cit.> have drawn great attention and achieved significant progress due to their strong generalizability and flexibility. Unified vision models attempt to solve multiple vision or multi-modal tasks by a single model. Existing works can be categorized into unified learning paradigms and unified model architectures.\n\n\n\nUnified Learning Paradigms. These works\u00a0<cit.> usually present a universal learning paradigm for covering as many tasks and modalities as possible. For example, MuST\u00a0<cit.> presents a multi-task self-training approach for 6 vision tasks. INTERN\u00a0<cit.> introduces a continuous learning scheme, showing strong generalization ability on 26 popular benchmarks. Unified-IO\u00a0<cit.> and OFA\u00a0<cit.> proposes a unified sequence-to-sequence framework that can handle a variety of vision, language, and multi-modal tasks. Although these works can perform many tasks, the commonality and inner relationship among different tasks are less explored and exploited.\n\n\n\n\n\n\nUnified Model Architectures. These works\u00a0<cit.> usually design a unified formulation or model architecture for a group of closely related tasks. For example, Mask R-CNN\u00a0<cit.> proposes a unified network to perform object detection and instance segmentation simultaneously. Mask2Former\u00a0<cit.> presents a universal architecture capable of handling panoptic, instance, and semantic segmentation. Pix2SeqV2\u00a0<cit.> designs a unified pixel-to-sequence interface for four vision tasks, namely object detection, instance segmentation, keypoint detection, and image captioning. GLIP\u00a0<cit.> cleverly reformulates object detection as phrase grounding by replacing classical classification with word-region alignment. This new formulation allows joint training on both detection and grounding data, showing strong transferability to various object-level recognition tasks. However, GLIP\u00a0<cit.> supports neither prompts in other modalities such as images & annotations nor video-level tracking tasks. In terms of object tracking, Unicorn\u00a0<cit.> proposes a unified solution for SOT, VOS, MOT, and MOTS, achieving superior performance on 8 benchmarks with the same model weights. However, it is still difficult for Unicorn to handle diverse label vocabularies\u00a0<cit.> during training and inference. In this work, we propose a universal prompt-guided architecture for 10 instance perception tasks, conquering the drawbacks of GLIP\u00a0<cit.> and Unicorn\u00a0<cit.> simultaneously. \n\n\n\n\n\u00a7 APPROACH\n\nBefore introducing detailed methods, we first categorize existing instance perception tasks into three classes. \n\n    \n\n    \n  * \n    Object detection, instance segmentation, MOT, MOTS, and VIS take category names as prompts to find all instances of specific classes. \n    \n\n    \n  * \n    REC, RES, and R-VOS exploit an expression as the prompt to localize a certain target. \n    \n\n    \n  * \n    SOT and VOS use the annotation given in the first frame as the prompt for predicting the trajectories of the tracked target.\n\n\nEssentially, all the above tasks aim to find objects specified by some prompts. This commonality motivates us to reformulate all instance perception tasks into a prompt-guided object discovery and retrieval problem and solve it by a unified model architecture and learning paradigm. As demonstrated in Figure\u00a0<ref>, UNINEXT consists of three main components: (1) prompt generation (2) image-prompt feature fusion (3) object discovery and retrieval. \n\n\n\n\n\n \u00a7.\u00a7 Prompt Generation\n\nFirst, a prompt generation module is adopted to transform the original diverse prompt inputs into a unified form. According to different modalities, we introduce the corresponding strategies in the next two paragraphs respectively. \n\nTo deal with language-related prompts, a language encoder\u00a0<cit.> Enc_L is adopted. To be specific, for category-guided tasks, we concatenate class names that appeared in the current dataset\u00a0<cit.> as the language expression. Take COCO\u00a0<cit.> as an example, the expression can be written as \u201cperson. bicycle. ... . toothbrush\". Then for both category-guided and expression-guided tasks, the language expression is passed into Enc_L, getting a prompt embedding F_p\u2208\u211d^L\u00d7d with a sequence length of L. \n\nFor the annotation-guided tasks, to extract fine-grained visual features and fully exploit the target annotations, an additional reference visual encoder Enc^ref_V is introduced. Specifically, first a template with 2^2 times the target box area is cropped centered on the target location on the reference frame. Then the template is resized to a fixed size of 256\u00d7256. To introduce more precise target information, an extra channel named the target prior is concatenated to the template image, forming a 4-channel input. In more detail, the value of the target prior is 1 on the target region otherwise 0. Then the template image together with the target prior is passed to the reference visual encoder Enc^ref_V, obtaining a hierarchical feature pyramid {C_3,C_4,C_5,C_6}. The corresponding spatial sizes are 32\u00d732, 16\u00d716, 8\u00d78, and 4\u00d74. To keep fine target information and get the prompt embedding in the same format as other tasks, a merging module is applied. Namely, all levels of features are first upsampled to 32\u00d732 then added, and flattened as the final prompt embedding F_p\u2208\u211d^1024\u00d7d.\n\nThe prompt generation process can be formulated as \n\n\n    F_p={   Enc^ref_L(expression)       expression-guided\n       Enc^ref_L(concat(categories))       category-guided\n       merge(Enc^ref_V([template, prior])       annotation-guided\n    .\n\n\n\n\n \u00a7.\u00a7 Image-Prompt Feature Fusion\n\nIn parallel with the prompt generation, the whole current image is passed through another visual encoder Enc_V, obtaining hierarchical visual features F_v. To enhance the original prompt embedding by the image contexts and to make the original visual features prompt-aware, an early fusion module is adopted. To be specific, first a bi-directional cross-attention module (Bi-XAtt) is used to retrieve information from different inputs, and then the retrieved representations are added to the original features. This process can be formulated as  \n\n    F_p2v, F_v2p = Bi-XAtt(F_v, F_p)\n       F^'_v = F_v + F_p2v; \n    F^'_p = F_p + F_v2p\n\nDifferent from GLIP\u00a0<cit.>, which adopts 6 vision-language fusion layers and 6 additional BERT layers for feature enhancement, our early fusion module is much more efficient.\n\n\n\n \u00a7.\u00a7 Object Discovery and Retrieval\n\n\nWith discriminative visual and prompt representations, the next crucial step is to transform input features into instances for various perception tasks. UNINEXT adopts the encoder-decoder architecture proposed by Deformable DETR\u00a0<cit.> for its flexible query-to-instance fashion. We introduce the detailed architectures as follows.\n\nThe Transformer encoder takes hierarchical prompt-aware visual features as the inputs. With the help of efficient Multi-scale Deformable Self-Attention\u00a0<cit.>, target information from different scales can be fully exchanged, bringing stronger instance features for the subsequent instance decoding. Besides, as performed in two-stage Deformable DETR\u00a0<cit.>, an auxiliary prediction head is appended at the end of the encoder, generating N initial reference points with the highest scores as the inputs of the decoder.\n\nThe Transformer decoder takes the enhanced multi-scale features, N reference points from the encoder, as well as N object queries as the inputs. As shown in previous works \u00a0<cit.>, object queries play a critical role in instance perception tasks. In this work, we attempt two query generation strategies: (1) static queries which do not change with images or prompts. (2) dynamic queries conditioned on the prompts. The first strategy can be easily implemented with . The second one can be performed by first pooling the enhanced prompt features F^'_v along the sequence dimension, getting a global representation, then repeating it by N times. The above two methods are compared in Sec\u00a0<ref> and we find that static queries usually perform better than dynamic queries. The potential reason could be that static queries contain richer information and possess better training stability than dynamic queries. With the help of the deformable attention, the object queries can efficiently retrieve prompt-aware visual features and learn strong instance embedding F_ins\u2208\u211d^N\u00d7d.\n\nAt the end of the decoder, a group of prediction heads is exploited to obtain the final instance predictions. Specifically, an instance head produces both boxes and masks of the targets. Besides, an embedding head\u00a0<cit.> is introduced for associating the current detected results with previous trajectories in MOT, MOTS, and VIS. Until now, we have mined N potential instance proposals, which are represented with gray masks in Figure\u00a0<ref>. However, not all proposals are what the prompts really refer to. Therefore, we need to further retrieve truly matched objects from these proposals according to the prompt embeddings as demonstrated in the right half of Figure\u00a0<ref>. Specifically, given the prompt embeddings F^'_p after early fusion, for category-guided tasks, we take the embedding of each category name as a weight matrix W\u2208\u211d^1\u00d7d. Besides, for expression-guided and annotation-guided tasks, the weight matrix W is obtained by aggregating the prompt embedding F^'_p using global average pooling (GAP) along the sequence dimension. \n\n\n\n    W={    F^'_p[i],  i\u2208{0,1,...,C-1}      category\n       1/L\u2211_i=0^LF_p^'(i,j)       expression/annotation\n    .\n\nFinally, the instance-prompt matching scores S can be computed as the matrix multiplication of the target features and the transposed weight matrix. S=F_insW^\u22a4. Following previous work\u00a0<cit.>, the matching scores can be supervised by Focal Loss\u00a0<cit.>. Different from previous fixed-size classifiers\u00a0<cit.>, the proposed retrieval head selects objects by the prompt-instance matching mechanism. This flexible design enables UNINEXT to jointly train on enormous datasets with diverse label vocabularies from different tasks, learning universal instance representations.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Training and Inference\n\n\nTraining. The whole training process consists of three consecutive stages: (1) general perception pretraining (2) image-level joint training (3) video-level joint training. \n\nIn the first stage, we pretrain UNINEXT on the large-scale object detection dataset Objects365\u00a0<cit.> for learning universal knowledge about objects. Since Objects365 does not have mask annotations, we introduce two auxiliary losses proposed by BoxInst\u00a0<cit.> for training the mask branch. The loss function can be formulated as \n\n    \u2112_stage1 = \u2112_retrieve + \u2112_box + \u2112^boxinst_mask\n\nThen based on the pretrained weights of the first stage, we finetune UNINEXT jointly on image datasets, namely COCO\u00a0<cit.> and the mixed dataset of RefCOCO\u00a0<cit.>, RefCOCO+\u00a0<cit.>, and RefCOCOg\u00a0<cit.>. With manually labeled mask annotations, the traditional loss functions like Dice Loss\u00a0<cit.> and Focal Loss\u00a0<cit.> can be used for the mask learning. After this step, UNINEXT can achieve superior performance on object detection, instance segmentation, REC, and RES. \n\n    \u2112_stage2 = \u2112_retrieve + \u2112_box + \u2112_mask\n\nFinally, we further finetune UNINEXT on video-level datasets for various downstream object tracking tasks and benchmarks. In this stage, the model is trained on two frames randomly chosen from the original videos. Besides, to avoid the model forgetting previously learned knowledge on image-level tasks, we also transform image-level datasets to pseudo videos for joint training with other video datasets. In summary, the training data in the third stage includes pseudo videos generated from COCO\u00a0<cit.>, RefCOCO/g/+\u00a0<cit.>, SOT&VOS datasets (GOT-10K\u00a0<cit.>, LaSOT\u00a0<cit.>, TrackingNet\u00a0<cit.>, and Youtube-VOS\u00a0<cit.>), MOT&VIS datasets (BDD100K\u00a0<cit.>, VIS19\u00a0<cit.>, OVIS\u00a0<cit.>), and R-VOS dataset Ref-Youtube-VOS\u00a0<cit.>. Meanwhile, a reference visual encoder for SOT&VOS and an extra embedding head for association are introduced and optimized in this period. \n\n    \u2112_stage3 = \u2112_retrieve + \u2112_box + \u2112_mask + \u2112_embed\n\nInference. For category-guided tasks, UNINEXT predicts instances of different categories and associates them with previous trajectories. The association proceeds in an online fashion and is purely based on the learned instance embedding following\u00a0<cit.>. For expression-guided and annotation-guided tasks, we directly pick the object with the highest matching score with the given prompt as the final result. Different from previous works\u00a0<cit.> restricted by the offline fashion or complex post-processing, our method is simple, online, and post-processing free. \n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n \u00a7.\u00a7 Implementation Details\n\nWe attempt three different backbones, ResNet-50\u00a0<cit.>, ConvNeXt-Large\u00a0<cit.>, and ViT-Huge\u00a0<cit.> as the visual encoder. We adopt BERT\u00a0<cit.> as the text encoder and its parameters are trained in the first and second training stages while being frozen in the last training stage. The Transformer encoder-decoder architecture follows \u00a0<cit.> with 6 encoder layers and 6 decoder layers. The number of object queries N is set to 900. The optimizer is AdamW\u00a0<cit.> with weight decay of 0.05. The model is trained on 32 and 16 A100 GPUs for Objects365 pretraining and other stages respectively. More details can be found in the appendix.\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Evaluations on 10 Tasks\n\n\nWe compare UNINEXT with task-specific counterparts in 20 datasets. In each benchmark, the best two results are indicated in bold and with underline. UNINEXT in all benchmarks uses the same model parameters.\n\n\n\n\n\nObject Detection and Instance Segmentation. We compare UNINEXT with state-of-the-art object detection and instance segmentation methods on COCO \ud835\ude9f\ud835\ude8a\ud835\ude952017 (5k images) and \ud835\ude9d\ud835\ude8e\ud835\ude9c\ud835\ude9d-\ud835\ude8d\ud835\ude8e\ud835\ude9f split (20k images) respectively. As shown in Table\u00a0<ref>, UNINEXT surpasses state-of-the-art query-based detector DN-Deformable DETR\u00a0<cit.> by 2.7 box AP. By replacing ResNet-50\u00a0<cit.> with stronger ConvNeXt-Large\u00a0<cit.> and ViT-Huge\u00a0<cit.> backbones, UNINEXT achieves a box AP of 58.1 and 60.6, surpassing competitive rivals Cascade Mask-RCNN\u00a0<cit.> and ViTDet-H\u00a0<cit.> by 3.3 and 1.9 respectively. Besides, the results of instance segmentation are shown in Table\u00a0<ref>. With the same ResNet-50 backbone, UNINEXT outperforms state-of-the-art QueryInst by 4.3 AP and 6.2 AP_L. When using ConvNeXt-Large as the backbone, UNINEXT achieves a mask AP of 49.6, surpassing Cascade Mask R-CNN\u00a0<cit.> by 2.0. With ViT-Huge as the backbone, UNINEXT achieves state-of-the-art mask AP of 51.8.\n\n\n\n\nREC and RES. RefCOCO\u00a0<cit.>, RefCOCO+\u00a0<cit.>, and RefCOCOg\u00a0<cit.> are three representative benchmarks for REC and RES proposed by different institutions. Following previous literature, we adopt Precision@0.5 and overall IoU (oIoU) as the evaluation metrics for REC and RES respectively and results are rounded to two decimal places. As shown in Table\u00a0<ref> and Table\u00a0<ref>, our method with ResNet-50 backbone surpasses all previous approaches on all splits. Furthermore, when using ConvNeXt-Large and ViT-Huge backbones, UNINEXT obtains new state-of-the-art results, exceeding the previous best method by a large margin. Especially on RES, UNINEXT-H outperforms LAVT\u00a0<cit.> by 10.85 on average.\n\n\n\n\n\n\n\n\n\nSOT. We compare UNINEXT with state-of-the-art SOT methods on four large-scale benchmarks: LaSOT\u00a0<cit.>, LaSOT-ext\u00a0<cit.>, TrackingNet\u00a0<cit.>, and TNL-2K\u00a0<cit.>. These benchmarks adopt the area under the success curve (AUC), normalized precision (P_Norm), and precision (P) as the evaluation metrics and include 280, 150, 511, and 700 videos in the test set respectively. As shown in Table\u00a0<ref>, UNINEXT achieves the best results in terms of AUC and P among all trackers with ResNet-50 backbone. Especially on TNL-2K, UNINEXT outperforms the second best method TransT\u00a0<cit.> by 5.3 AUC and 5.8 P respectively. Besides, UNINEXT with stronger backbones obtains the best AUC on all four benchmarks, exceeding Unicorn\u00a0<cit.> with the same backbone by 3.9 on LaSOT.\n\n\n\nVOS. The comparisons between UNINEXT with previous semi-supervised VOS methods are demonstrated in Table\u00a0<ref>. DAVIS-2017\u00a0<cit.> adopts region similarity , contour accuracy , and the averaged score  as the metrics. Similarly, Youtube-VOS 2018\u00a0<cit.> reports  and  for both seen and unseen categories, and the averaged overall score . UNINEXT achieves the best results among all non-memory-based methods, largely bridging the performance gap between non-memory-based approaches and memory-based ones. Furthermore, compared with traditional memory-based methods\u00a0<cit.>, UNINEXT does not rely on the intermediate mask predictions. This leads to constant memory consumption, enabling UNINEXT to handle long sequences of any length.  \n\n\n\nMOT. We compare UNINEXT with state-of-the-art MOT methods on BDD100K\u00a0<cit.>, which requires tracking 8 classes of instances in the autonomous driving scenario. Except for classical evaluation metrics Multiple-Object Tracking Accuracy (MOTA), Identity F1 Score (IDF1), and Identity Switches (IDS), BDD100K additionally introduces mMOTA, and mIDF1 to evaluate the average performance across 8 classes. As shown in Table\u00a0<ref>, UNINEXT surpasses Unicorn\u00a0<cit.> by 3.0 mMOTA and 2.7 mIDF1 respectively. \n\n\nMOTS. Similar to MOT, BDD100K MOTS Challenge\u00a0<cit.> evaluates the performance on multi-class tracking by mMOTSA, mMOTSP, mIDF1, and ID Sw. This benchmark contains 37 sequences with mask annotations in the validation set. As shown in Table\u00a0<ref>, UNINEXT achieves state-of-the-art performance, surpassing the previous best method Unicorn\u00a0<cit.> by 6.1 mMOTSA.\n\nVIS. We compare UNINEXT against state-of-the-art VIS methods on Youtube-VIS 2019\u00a0<cit.> and OVIS\u00a0<cit.> validation sets. Specifically, Youtube-VIS 2019 and OVIS have 40 and 25 object categories, containing 302 and 140 videos respectively in the validation set. Both benchmarks take AP as the main metric. As shown in Table\u00a0<ref>, when using the same ResNet-50 backbone,  \nUNINEXT obtains the best results on both datasets. Especially on more challenging OVIS, UNINEXT exceeds the previous best method IDOL\u00a0<cit.> by 3.8 AP. When using stronger ViT-Huge backbone, UNINEXT achieves state-of-the-art AP of 66.9 on Youtube-VIS 2019 and 49.0 on OVIS respectively, surpassing previous methods by a large margin. \n\n\n\n\n\n\n\nR-VOS. Ref-Youtube-VOS\u00a0<cit.> and Ref-DAVIS17\u00a0<cit.> are two popular R-VOS benchmarks, which are constructed by introducing language expressions for the objects in the original Youtube-VOS\u00a0<cit.> and DAVIS17\u00a0<cit.> datasets. As same as semi-supervised VOS, region similarity , contour accuracy , and the averaged score  are adopted as the metrics. As demonstrated in Table\u00a0<ref>, UNINEXT outperforms all previous R-VOS approaches by a large margin, when using the same ResNet-50 backbone. Especially on Ref-DAVIS17, UNINEXT exceeds previous best ReferFormer\u00a0<cit.> by 5.4 . Furthermore, when adopting stronger ViT-Huge backbone, UNINEXT achieves new state-of-the-art of 70.1 on Ref-Youtube-VOS and 72.5 on Ref-DAVIS17. Besides, different from offline RefFormer, UNINEXT works in a flexible online fashion, making it applicable to ongoing videos in the real world.\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Ablations and Other Analysis\n\n\n\nIn this section, we conduct component-wise analysis for better understanding our method. All models take ResNet-50 as the backbone. The methods are evaluated on five benchmarks (COCO\u00a0<cit.>, RefCOCO\u00a0<cit.>, Youtube-VOS\u00a0<cit.>, Ref-Youtube-VOS\u00a0<cit.>, and Youtube-VIS 2019\u00a0<cit.>) from five tasks (object detection, REC, VOS, R-VOS, and VIS). The results are shown in Table\u00a0<ref>. \n\nFusion. To study the effect of feature fusion between visual features and prompt embeddings, we implement a variant without any early fusion. In this version, prompt embeddings do not have an influence on proposal generation but are only used in the final object retrieval process. Experiments show that early fusion has the greatest impact on VOS, the performance on VOS drops drastically by 21.4  without feature fusion. This is mainly caused by the following reasons (1) Without the guidance of prompt embeddings, the network can hardly find rare referred targets like trees and sinks. (2) Without early fusion, the network cannot fully exploit fine mask annotations in the first frame, causing degradation of the mask quality. Besides, the removal of feature fusion also causes performance drop of 2.3 P@0.5 and 2.8 on REC and RVOS respectively, showing the importance of early fusion in expression-guided tasks. Finally, feature fusion has minimum influence on object detection and VIS. This can be understood because both two tasks aim to find all objects as completely as possible rather than locating one specific target referred by the prompt. \n\nQueries. We compare two different query generation strategies: static queries by  and dynamic queries conditioned on the prompt embeddings. Experiments show that dynamic queries perform slightly better than static queries on the first four tasks. However, static queries outperform dynamic ones by 2.8 AP on the VIS task, obtaining higher overall performance. A potential reason is that N different object queries can encode richer inner relationship among different targets than simply copying the pooled prompt by N times as queries. This is especially important for VIS because targets need to be associated according to their affinity in appearance and space.\n\n\nUnification. We also compare two different model design philosophies, one unified model or multiple task-specific models. Except for the unified model, we also retrain five task-specific models only on data from corresponding tasks. Experiments show that the unified model achieves significantly better performance than its task-specific counterparts on five tasks, demonstrating the superiority of the unified formulation and joint training on all instance perception tasks. \n\nFinally, the unified model can save tons of parameters, being much more parameter-efficient. \n\n\n\n\n\n\u00a7 CONCLUSIONS\n\nWe propose UNINEXT, a universal instance perception model of the next generation. For the first time, UNINEXT unifies 10 instance perception tasks with a prompt-guided object discovery and retrieval paradigm. Extensive experiments demonstrate that UNINEXT achieves superior performance on 20 challenging benchmarks with a single model with the same model parameters. We hope that UNINEXT can serve as a solid baseline for the research of instance perception in the future.\n\nAcknowledgement.\nWe would like to thank the reviewers for their insightful comments. The paper is supported in part by the National Key R&D Program of China under Grant No. 2018AAA0102001, 2022ZD0161000 and National Natural Science Foundation of China under grant No. 62293542, U1903215, 62022021 and the Fundamental Research Funds for the Central Universities No.DUT22ZD210.\n\n\nieee_fullname\n\n\n\n\n\n\n\n\u00a7 APPENDIX\n\n\n\n\nIn this appendix, we present more details about the training process and loss functions in \u00a0<ref> and \u00a0<ref>, network architecture in \u00a0<ref>, as well as more analysis and visualizations for better understanding in \u00a0<ref>.\n\n\n\n \u00a7.\u00a7 Training Process\n\n\nThe detailed hyperparameters during training are shown in Tab\u00a0<ref>. The whole training process consists of three stages. In each stage, the  learning rate scheduler is adopted. The learning rate drops by a factor of 10 after the given steps. For multi-dataset training, we follow the implementation of Detic\u00a0<cit.>, which randomly samples data from different tasks and then computes them on different GPUs in one iteration. Besides, the multi-scale training technique is used across all datasets in all stages. Take the pre-training on Objects365\u00a0<cit.> as an example, the original images are resized such that the shortest side is at least 480 and at most 800 pixels while the longest side is at most 1333. We use this as the default setting except on Youtube-VOS\u00a0<cit.>, Youtube-VIS-2019\u00a0<cit.>, and Ref-Youtube-VOS\u00a0<cit.>. A lower resolution with the shortest side ranging from 320 to 640 and the longest side not exceeding 768 is applied to these datasets\u00a0<cit.>, following previous works\u00a0<cit.>.\n\nSpecifically, in the first stage, the model is pretrained on Objects365\u00a0<cit.> for about 340K iterations (12 epochs) and the learning rate drops on the 11th epoch. In the second stage, we finetune UNINEXT on COCO\u00a0<cit.> and RefCOCO/g/+\u00a0<cit.> jointly for 12 epochs. In the third stage, UNINEXT is further finetuned for diverse video-level tasks. To guarantee balanced performance on various benchmarks, we set the data sampling ratios as (SOT&VOS):(MOT&MOTS):VIS:R-VOS = 1:1:1:1. For each task, 45K iterations are allocated, thus bringing 180K iterations in total for the third stage. Besides, to avoid forgetting previously learned knowledge on image-level tasks, we also generate pseudo videos from COCO\u00a0<cit.> and RefCOCO/g/+\u00a0<cit.> and mix them with training data of VIS\u00a0<cit.> and R-VOS\u00a0<cit.> respectively.\n\n\n\n \u00a7.\u00a7 Loss Functions\n\n\nWe present detailed loss functions described in Sec.\u00a0<ref> for better readability. First, \u2112_retrieve and \u2112_box are used across all three stages. Second, to learn mask representations from coarse boxes\u00a0<cit.> and fine mask annotations\u00a0<cit.>, UNINEXT uses \u2112^boxinst_mask in the first stage and \u2112_mask in the next two stages respectively. Finally, to associate instances on different frames\u00a0<cit.>, UNINEXT additionally adopts \u2112_embed in the last stage.\n\n\n\u2112_retrieve. Given the raw instance-prompt matching score s, the normalized matching probability p is computed as p=\u03c3(s), where \u03c3 is sigmoid function. Then \u2112_retrieve can be written as the form of Focal loss\u00a0<cit.>.\n flalpha\u2112_retrieve() = - (1 - )^\u03b3log().\npt= p    if matched\n 1 - p    otherwise.\n\u03b3 and \u03b1 are 2 and 0.25 respectively.\n\n\u2112_box. Following DETR-like methods\u00a0<cit.>, \u2112_box consists of two terms, GIoU Loss\u00a0<cit.> and \u2113_1 loss:\n loss_box\u2112_box(b,b\u0302) = \u03bb_giou\u2112_giou(b,b\u0302)+\u03bb_L_1\u2016b-b\u0302 \u2016.\nloss_giou\u2112_giou(b,b\u0302)=1-IoU(b,b\u0302)+A^c(b,b\u0302)-U(b,b\u0302)/A^c(b,b\u0302),\nwhere A^c(b,b\u0302) is the area of the smallest box containing b and b\u0302. U(b,b\u0302) is the area of the union of b and b\u0302.\n\n\u2112_mask. For datasets with mask annotations\u00a0<cit.>, Focal Loss\u00a0<cit.> and Dice Loss\u00a0<cit.> are adopted. \nloss_mask\u2112_mask(m,m\u0302) = \u03bb_focal\u2112_focal(m,m\u0302)+\u03bb_dice\u2112_dice(m,m\u0302).\nloss_dice\u2112_dice(m,m\u0302)=1-2mm\u0302+1/m\u0302+m+1,\nwhere m and m\u0302 are binary GT masks and predicted masks after sigmoid activation respectively.\n\n\u2112^boxinst_mask. For Objects365\u00a0<cit.> without mask annotations, UNINEXT uses Projection Loss and Pairwise Affinity Loss like BoxInst\u00a0<cit.>, which can learn mask prediction only based on box-level annotations.\nloss_boxinst\u2112^boxinst_mask(b,m\u0302) = \u2112_proj(b,m\u0302)+\u2112_pairwise(b,m\u0302).\nloss_proj\n\n\u2112_proj(b,m\u0302)=   \u2112_dice(proj_x(b),proj_x(m\u0302))+\n\n   \u2112_dice(proj_y(b),proj_y(m\u0302)).\n\n\nloss_pairwise\u2112_pairwise = -1/N\u2211_e \u2208E_in1_{S_e \u2265\u03c4}logP(y_e = 1).\ny_e\n     P(y_e = 1) = m\u0302_i, j\n     \u00b7m\u0302_k, l + (1 - m\u0302_i, j)\n     \n     \u00b7(1 - m\u0302_k, l).\nS_eS_e = S(c_i, j, c_l, k) = exp(-||c_i, j - c_l, k||/\u03b8),\nwhere y_e=1 means the two pixels have the same ground-truth label. S_e is the color similarity of the edge e. c_i,j and c_l,k are respectively the LAB color vectors of the two pixels (i, j) and (l, k) linked by the edge. \u03b8 is 2 in this work.\n\n\u2112_embed. UNINEXT uses contrastive loss\u00a0<cit.> to train discriminative embeddings for associating instances on different frames.\nloss_embed\u2112_embed = log[1+\u2211_\ud835\udc24^+\u2211_\ud835\udc24^-exp(\ud835\udc2f \u00b7\ud835\udc24^-  - \ud835\udc2f \u00b7\ud835\udc24^+) ],\nwhere k^+ and k^- are positive and negative feature embeddings from the reference frame. For each instance in the key frame, v is the feature embedding with the lowest cost.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Network Architecture\n\n\nTo transform the enhanced visual features F^'_v and prompt features F^'_p into the final instance predictions, an encoder-decoder Transformer architecture is adopted. Based on the original architecture in two-stage Deformable DETR\u00a0<cit.>, UNINEXT makes the following improvements:\n\n\n  * Introducing a mask head for segmentation. To predict high-quality masks, UNINEXT introduces a mask head\u00a0<cit.> based on dynamic convolutions. Specifically, first an MLP is used to transform instance embeddings into a group of parameters \u03c9. Then these parameters are used to perform three-layer 1\u00d71 convolutions with feature maps, obtaining masks of instances.\n\n  * Replacing one-to-one Hungarian matching with one-to-many SimOTA\u00a0<cit.>. Traditional Hungarian matching forces one GT to be only assigned to one query, leaving most of the queries negative. UNINEXT uses SimOTA\u00a0<cit.>, which enables multiple queries to be matched with one GT. This strategy can provide more positive samples and speed up convergence. During inference, UNINEXT uses NMS to remove duplicated predictions. \n\n  * Adding an IoU branch. UNINEXT adds an IoU branch to reflect the quality of the predicted boxes. During training, IoU does not affect the label assignment. During inference, the final scores are the geometric mean of the instance-prompt matching scores (after sigmoid) and the IoU scores.\n\n  * Adding some techniques in DINO\u00a0<cit.>. To further improve the performance, UNINEXT introduces some techniques\u00a0<cit.>, including contrastive DN, mixed query selection, and look forward twice. \n\n\n\n\n\n\n \u00a7.\u00a7 Analysis and Visualizations\n\n\n\nAnalysis. We compare UNINEXT with other competitive counterparts, which can handle multiple instance-level perception tasks. The opponents include Cascade Mask R-CNN\u00a0<cit.> for object detection and instance segmentation, SeqTR\u00a0<cit.> for REC and RES, VMT\u00a0<cit.> for MOTS and VIS, and Unicorn\u00a0<cit.> for SOT, VOS, MOT, and MOTS. As shown in Figure\u00a0<ref>, UNINEXT outperforms them and achieve state-of-the-art performance on all 10 tasks. \n\nRetrieval by Category Names. As shown in Figure\u00a0<ref>, UNINEXT can flexibly detect and segment objects of different categories by taking the corresponding category names as the prompts. For example, when taking \u201cdining table. wine glass. cake. knife\u201d as the prompts, UNINEXT would only perceive dining tables, wine glasses, cakes, and knives. Furthermore, benefiting from the flexible retrieval formulation, UNINEXT also has the potential for zero-shot (open-vocabulary) object detection. However, open-vocabulary object detection is beyond the scope of our paper and we leave it for future works.\n\nRetrieval by Language Expressions. We provide some visualizations for retrieval by language expressions in Figure\u00a0<ref>. UNINEXT can accurately locate the target referred by the given language expression when there are many similar distractors. This demonstrates that our method can not only perceive objects but also understand their relationships in positions (left, middle, right, etc) and sizes (taller, etc). \n\nRetrieval by Target Annotations. Our method supports annotations in formats of both boxes (SOT) and masks (VOS). Although there is only box-level annotation for SOT, we obtain the target prior by filling the region within the given box with 1 and leaving other regions as 0. As shown in Figure\u00a0<ref>, UNINEXT can precisely track and segment the targets in complex scenarios, given the annotation in the first frame.\n\n\n\n\n\n\n\n\n"}