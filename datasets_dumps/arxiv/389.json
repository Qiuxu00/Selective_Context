{"entry_id": "http://arxiv.org/abs/2303.06791v1", "published": "20230313003904", "title": "Beyond Single Items: Exploring User Preferences in Item Sets with the Conversational Playlist Curation Dataset", "authors": ["Arun Tejasvi Chaganty", "Megan Leszczynski", "Shu Zhang", "Ravi Ganti", "Krisztian Balog", "Filip Radlinski"], "primary_category": "cs.IR", "categories": ["cs.IR", "cs.CL"], "text": "\n\n\n\n\n\n\n\n\n\n\n  Google Research\n  \n \narunchaganty@google.com\n\n\nWork done while interning at Google Research.\n\n\n  Stanford University\n     \n\nmleszczy@cs.stanford.edu\n\n\n\n  Google Research\n    \n\nshzhang@google.com\n\n\n\n  Google Research\n    \n\ngmravi@google.com\n\n\n\n Google Research\n   \n \n krisztianb@google.com\n\n\n\n Google Research\n   \n \nfiliprad@google.com\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n \n\n \n \n \n \n \n\n\n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndefinitionDefinition\nassumptionAssumption\nhypothesisHypothesis\npropositionProposition\ntheoremTheorem\nlemmaLemma\ncorollaryCorollary\nalgAlgorithm\nexampleExample\n \n \n \n \n \n \n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsers in consumption domains, like music, are often able to more efficiently provide preferences over a set of items (e.g. a playlist or radio) than over single items (e.g. songs).\nUnfortunately, this is an underexplored area of research, with most existing recommendation systems limited to understanding preferences over single items.\nCurating an item set exponentiates the search space that recommender systems must consider (all subsets of items!):\nthis motivates conversational approaches\u2014where users explicitly state or refine their preferences and systems elicit preferences in natural language\u2014as an efficient way to understand user needs.\nWe call this task conversational item set curation and present a novel data collection methodology that efficiently collects realistic preferences about item sets in a conversational setting by observing both item-level and set-level feedback.\nWe apply this methodology to music recommendation to build the Conversational Playlist Curation Dataset (CPCD), where we\nshow that it leads raters to express preferences that would not be otherwise expressed.\nFinally, we propose a wide range of conversational retrieval models as baselines for this task and evaluate them on the dataset.\n\n\n\n\n\nBeyond Single Items: Exploring User Preferences in Item Sets with the Conversational Playlist Curation Dataset\n\n    Filip Radlinski\n    March 30, 2023\n================================================================================================================\n\n\n\nintroIntroduction\n\n\nIn recent years, conversational agents have become increasingly popular for a variety of tasks, including recommendation systems <cit.>. \nThe conversational paradigm is especially suited for recommendation tasks, because of the convenience of using natural language to express preferences and provide feedback\u00a0<cit.>.\nMost existing conversational recommendation methods focus on recommending individual items\u00a0<cit.>.\nHowever, there are also scenarios where the user aims to create a set of items from a larger collection\u2014creating a music playlist for a given context, like working out or for a long drive, is a common manifestation of this problem.\nIn this setting, the goal of the conversational agent is to aid the user in the selection of a set of items that the user has in mind.\nThe problem of recommending sets of items in a conversational setting, which we refer to as conversational item set curation, is still largely unexplored.\nWe use the term curation to highlight that this is a collaborative and iterative process that requires the conversational agent to elicit and understand the user's preferences in order to effectively aid them.\nIn this work, we propose a framing of this novel problem, create a representative dataset, and present and experimentally evaluate a set of baselines.\n\nThe problem of conversational item set curation differs from prior work on entity set retrieval\u00a0<cit.> in a number of important ways.\nFirst, what constitutes a relevant item for a set is highly subjective; different people would find different music \u201cenergetic\u201d or \u201crelaxing\u201d for a given context.  The conversational setting allows for natural feedback both on the item and on the set level.  \nSecond, the curation of the item set is more iterative and interactive than entity set retrieval, as the conversational agent must engage in a back-and-forth dialogue with the user in order to fully understand their preferences. This requires the development of more sophisticated dialog management techniques, including effective strategies for balancing exploration and exploitation as well as the ability to handle uncertainty and ambiguity in the user's responses. \nWe break down the problem of conversational item set curation into a set of more specific tasks, in order to provide a clear and specific focus for further research in this space.\nSpecifically, \n(1) conversational item retrieval is the task of returning relevant items from a collection based on the conversation history;\n(2) conversational item set refinement identifies which and how many retrieved items to use to update the item set; and\n(3) conversational elicitation/explanation generation is concerned with the generation of questions for eliciting user preferences either via direct questions or indirectly via user feedback provided on the explanations that accompany the system's recommendations.\n\nA prerequisite first step to allow research on the above tasks is the construction of a dataset with the desired characteristics.\nWe present a novel human-to-human data collection methodology that efficiently acquires realistic preferences about item sets that are incrementally constructed, observing both item-level and set-level feedback. \nSpecifically, we perform the data collection within the domain of music recommendation, although we expect it to be applicable to most recommendation domains, where item sets are natural units of consumption (e.g., compiling reading lists or healthy dinner recipes).\nWe implement a high-fidelity multi-modal conversational user interface, where users can listen to songs and leave both item- and set-level feedback.\nUsing this methodology, we build and publicly release the Conversational Playlist Creation Dataset (CPCD), comprising of over 900 dialogues\ncontaining with 4,800 utterances that express a wide range of preference statements and are paired with over 22,000 item ratings.\nSee example for an example conversation from the dataset. \n\nFinally, we use this dataset to experimentally compare a range of conversational retrieval models that are meant to serve as representative baselines. \nSpecifically, we focus on the first sub-task of conversational item retrieval and propose an evaluation methodology that adapts standard retrieval metrics to the multi-turn conversational setting.\nWe find that, while both sparse and dense retrieval methods are useful for this task, our best performing method is a dual encoder architecture\u00a0<cit.> pretrained using unsupervised contrastive learning\u00a0<cit.> and fine-tuned on the CPCD development set. \nThere remains significant headroom for improvement.\n\nIn summary, our key contributions are:\n\n\n    \n  * We introduce the task of conversational item set curation, a collaborative task where a user interacts with a system to curate a (small) set of items that meet the user's goal from a (large) collection.\n    \n  * We develop a data collection methodology for conversational item set curation.\n    \n  * We apply this methodology in the music domain to collect the Conversational Playlist Curation Dataset, which is made available at https://bit.ly/conversational-playlist-curationbit.ly/conversational-playlist-curation.\n    \n  * We present a wide range of conversational retrieval models as baseline approaches and evaluate them on this dataset.\n\n\n\n\n\nrelatedRelated work\n\nrelated-datasetsConversational Recommendation Datasets\nA key contribution of this work is a new conversational recommendation dataset, the Conversational Playlist Curation Dataset ().\nThere are several existing conversational recommendation datasets collected by pairing a \u201cuser\u201d and \u201cwizard\u201d to talk to each other:\n    in some settings, as with , the user has an open-ended dialog with the wizard\u00a0<cit.>;\n    in others users are asked to follow instructions (e.g., mention a specific preference) in each turn of the dialog\u00a0<cit.>.\nExisting datasets all focus on recommending a single item (typically a movie) to the user;\nin contrast, our work focuses on recommending a set of items (songs in particular).\nTo the best of our knowledge,  is the first such dataset.\nThis distinction leads to several important differences in methodology from prior work:\nfirst, we allow users and wizards to preview recommendations so that they can react accordingly;[\nWhile <cit.> allow users to preview recommended movies at the end of the task prior to rating them, they can't do so during the conversation.\n]\nsecond, we allow users to share explicit item ratings with wizards in addition to their responses, leading to more open-ended feedback that often includes soft attributes\u00a0<cit.>; \nfinally, we employ wizards with relevant domain expertise to respond with relevant recommendations.\nIn addition,  provides a larger set of candidate and relevant items, making it suitable to evaluate the recommendation component of conversational recommendation systems.\ndatasets summarizes some of the key differences of  with prior work.\n\nAn alternative to manually collecting data include synthetic data approaches for conversational recommendation systems (CRSs),\ne.g., \nusing item rating data\u00a0<cit.>,\ntextual review data\u00a0<cit.>,\nor user logs (e.g., watch history)\u00a0<cit.>.\n<cit.> and <cit.> generate dialogs using natural language templates, while <cit.> retrieve candidate utterances and then use human annotators to rewrite them to be more conversational.\n\n\n\n \u00a7.\u00a7 Conversational Information Seeking\n\nBeyond recommendation,\nconversational information seeking (CIS) systems help users find information through a sequence of interactions that can include search and question answering\u00a0<cit.>. \nSuch systems also seek to enable users to provide direct feedback to improve their results without needing to rely on as much historical interaction data as collaborative filtering-based RSs\u00a0<cit.>. \nAs a result, our work builds on existing work in modeling conversational search and conversational question answering\u00a0<cit.>.\n\n    \n\n\n \u00a7.\u00a7 Entity Retrieval\n\nFinally, item set recommendation is closely related to entity retrieval\u00a0<cit.>, with \nanalogies to entity list completion or example-augmented search\u00a0<cit.>.\nIn entity list completion, the input consists of a textual query and a small set of example entities;\nin conversational item set recommendation, the input consists of the conversation history (textual query) and the current item set (example entities).\nWe follow recent work in dense entity retrieval, and evaluate baselines that use a dual encoder to embed queries and items\u00a0<cit.>. \n\n\ntaskTask definition\n\nConversational item set curation is a collaborative task where a user interacts with a system to curate a set of items from a larger collection (cpcd-overview).\nWe use the term curate to emphasize the collaborative and iterative nature of the task:\nthe goal is to identify not just a single item that matches the user's preferences, but a collection of items that may span a range of interests, but must be ultimately cohesive with the theme or goal of the collection.\nWhile some themes may be narrow (\u201ccollection of Elvis Preseley's greatest hits\u201d), others may be broad (\u201crock music for a long drive\u201d).\n\nIn the following, we focus on the music domain where items are songs and the item set is a playlist.\nThe user begins the task by stating an overall theme or goal for their item set in their first utterance X_1, e.g. \u201cI'd like to create a playlist to listen to during Christmas with the family,\u201d\nand the system (possibly proxied by a human wizard) responds with an initial playlist Z_1 and an utterance E_1 that explains its choice and elicits more information from the user.\nIn following turns t, the user critiques their working playlist Z_t-1 in their response to the system X_t.\nThe system in turn uses the user feedback to retrieve additional songs Y_t and responds with an updated playlist Z_t and response E_t.\nNote that the system must decide how many results (if any) from Y_t it should include when updating the playlist Z_t.\nThis process continues for T turns until the user is satisfied with their playlist.\n\nConversational item set curation (CIsC) can be formalized as the task of going from the user goal X_1 to an item set that meets their goal Z_n.\nGiven a conversation history, H_t = (X_1, Z_1, E_1, \u2026, X_t), a conversational item set curation system must solve three sub-tasks: first, it must retrieve relevant items Y_t from the item corpus (conversational item retrieval), then it must update the item set Z_t (conversational item set refinement) and finally it must generate a response E_t (conversational elicitation / explanation generation).\nIn this paper, our primary focus will be on conversational item retrieval.\n\nNext, we describe the sub-tasks in further detail:\n\n\n\n  \nConversational Item Retrieval (CIRt)\nIn each turn, the system uses the conversation history H_t to retrieve and rank relevant items Y_t from the item collection .\nTo provide contextually relevant recommendations to the user, the system must understand refinements or critiques (\u201cGot anything from old Stevie Wonder?\u201d in cpcd-overview) and soft attributes (\u201cplaylist for a sunny day\u201d in example).\nFinally, we note that the scope of CIRt limited to preferences stated by the user thus far; eliciting further preferences is handled below.\nWe propose evaluating the output of CIRt, Y_t, against the target item set Z_T using standard information retrieval metrics such as MRR or Hits@k in eval.\n\n\n\n  \nConversational Item Set Refinement (CIsR)\nGiven a list of relevant recommendations from CIRt, the task of CIsR is to identify which and how many of these items should be used when updating the item set.\nAs reasonable heuristic would add the top k items retrieved by CIRt, but a better method could take into account the confidence and diversity of the retrieved items.\nAlternatively, users could be presented all the results from CIRt and be asked to perform item set refinement themselves.\nCIsR can be evaluated against Z_t using set-inclusion metrics like precision and recall.\nWe leave further modeling of this component to future work.\n\n\n\n  \nConversational Elicitation / Explanation Generation (CEEG)\nFinally, user requests are often underspecified;\nto provide better recommendations, systems can elicit preferences from users, e.g., by asking them for a genre or probing for why they liked what they like\u00a0<cit.>.\nIn addition, providing an explanation of its recommendation makes the system more scrutable to users and allows them to better steer the system\u00a0<cit.>.\nWhile the dataset proposed in this paper includes system responses that exhibit elicitation and explanations, we leave modeling this component to future work.\n\nmethodologyMethodology\n\n\n\nOur key contribution is a new dataset, , to benchmark conversational item set curation systems.\nSimilar to prior work, we use a human-to-human methodology to collect : \n    two human raters are recruited to play the role of a user and the system (\u201ca wizard\u201d).\nHowever, to faithfully simulate conversational item set recommendation in consumption domain such as music presents additional challenges:\nfirst, preferences are extremely personal\u2014\u201cpop music\u201d for one person can be wholly different for another;\nsecond, they are deeply contextual\u2014\u201cpop music\u201d for a party may be different from \u201cpop music\u201d for doing household chores;\nthird, they are often grounded in the audio content and are hard to understand using only text;\nand finally they require substantial domain expertise from wizards to provide relevant recommendations.\n\n\n    \n\n\n\nWe address these challenges as follows: we encourage users to personalize the data collection task by asking them to begin each conversation with a specific music-listening scenario in their own lives (e.g., \u201cwalking to work\u201d).\nThis allows users to relate to the task and fixes the context for the conversation and their listening preferences.\nNext, we implement a high-fidelity interface wherein users and wizards can listen to songs during the task.\nWe found that this was particularly helpful for wizards to better understand users' tastes.\nAdditionally, we implement a multi-modal interface where users can provide explicit item-level feedback in the form of likes and dislikes, allowing them to focus on broader set-level feedback in their replies.\nFinally, in pilot studies, we found that the task required wizards to not only be familiar with music in general, but also with the sub-genres pertinent to the user's interests: wizards with less relevant expertise took longer to perform the task, and had less engaging conversations that were driven entirely by the user.\nWe address this problem by asking users and wizards to each fill in a survey of their general genre-level preferences, and then matching users with wizards who were most familiar with the users' preferred genres. \n\n\n\n \u00a7.\u00a7 Interface design\n\nA high-fidelity multimodal interface was critical for this complex data collection task. \nThe user-facing interface displays the conversational history and the current playlist as recommended by the system (user-facing-interface).\nUsers can sample songs from their working playlist and provide explicit item-level feedback in the form of likes and dislikes.\nUsers end their turn by responding to the system via text,\n and are allowed to submit the task after completing at least 5 rounds of conversation and having rated at least 15 songs.\nBefore submitting the task, users complete a short survey where they rate\n    the completeness of their playlist,\n    their satisfaction with the playlist,\n    the understanding capabilities of the system,\n    its helpfulness, and\n    their overall experience with the system\n    (end-task-ratings).\n\nWizards are presented a similar interface with an additional element: a music search tool that wizards use to add songs to the user's playlist (wizard-facing-interface).\nThe music search uses the https://developers.google.com/youtube/v3/docs/search/listYouTube Search API.\nWe note that the API is limited in its capabilities and tends to return results with a high lexical overlap.\nTo preserve user privacy, the search results were not personalized.\nWizards were encouraged to search for related artists or recommendations via Google or YouTube Music.\n\n\n\n \u00a7.\u00a7 Rater Recruitment and Training\n\nWe recruited 10 wizards and 110 users who were fluent English speakers and regularly listened to music from a crowdsourcing vendor.\nTo control for cultural differences, we required both users and wizards to reside in the U.S.\nThe wizards were additionally required to have significant music expertise and have performed several music labeling tasks in the past.\n\nBoth users and wizards were provided training material in the form of slides.\nUsers were primed with the following description of the task:\n\n    In this study, you will create a personal music playlist for a purpose (e.g., \u201cwhile doing chores\u201d or \u201cwhen I\u2019m feeling down\u201d) by having a conversation with a system.\n\nThey were then asked to come up with several scenarios for each conversation in preparation for the task.\nUsers were informed that another person (the wizard) was facilitating the conversation and encouraged to be descriptive in their requests by telling the system what they liked or didn't like, and why.\nUsers were given instructions on how to use the interface, but were provided no further guidance on what to say.\nIn contrast, wizards went through multiple pilot rounds of training before beginning the task.\nDuring the pilot rounds, they were paired amongst themselves, and given feedback on how they should respond to users.\nDuring the task, wizards also assisted users with any interface issues (e.g., how to rate items).\n\nUsers and wizards were scheduled to interact for a fixed block of time (between 1 and 3 hours), and conducted multiple conversations during this interval.\nIn order to pair users and wizards, they each completed a short music preferences survey asking them to list where they listen to music (context-survey) and how familiar they are with various genres (genre-survey).\nFor each user, we ranked wizards based on their overlap with the genres the user expressed familiarity with and tried to pick the top-ranked available wizard for every user subject to scheduling constraints.\nBased on pilot feedback, we only matched wizards that were at least somewhat familiar with at least one genre that the user indicated a preference for.\nTo avoid priming their conversations, users and wizards were not informed of which genres they matched on.\n\n\n\n\n \u00a7.\u00a7 Post-processing\n\nAfter collecting the data, we took several manual and automated steps to process the data.\nFirst, we standardized all white space in the utterances.\nOften conversations would begin with the user and wizard checking for each other's presence (e.g., by saying \u201chi\u201d); these turns were merged with subsequent ones so that the first turn included the goal statement (e.g., \u201cHi, I'd love to create a playlist for classical songs\u201d).\nAfter about 20 minutes, wizards would nudge users to proceed to evaluation:\nthese utterances were removed along with any other turns at the end of the conversation where no songs were added.\nWe then manually reviewed the data to identify and remove any turns where users or wizards side-channeled debugging information (e.g., \u201cI don't see any items,\u201d \u201cplease rate items so I know what you like\u201d).\nFinally, we scraped and included metadata (i.e., titles, artists and album names) for each song mentioned in the dataset.\n\n\nanalysisDataset analysis\n\n\n\n\n\n\n\nWe now take a deeper look at the  dataset.\nIn total, after filtering and post-processing, the dataset contains 917 conversations that are randomly split into development and test sets; see\u00a0dataset-stats.\nexamples includes a few example conversations from the dataset. \nOn average, each conversation includes about 5.7 turns each for the user and wizard, and ends with a final playlist of about 19 songs.\nUsers rate about 4.8 songs in each turn, of which about 20% are negative ratings.\nIn total, the dataset includes over 100k unique songs that serve as the item corpus for retrieval tasks.\n\n\n\nNext, we analyze the distribution of preferences expressed by users and elicited by wizards. \nTo do so, we manually annotated 220 turns from 41 conversations.\nAfter exploratory analysis, we identified 6 key categories of preferences for users and and 4 for wizards;  preference-examples lists these categories with typical examples.\nWhile artist and genre based preferences make up a majority of the dataset, over 30% of preferences include soft attributes such as activities, sounds, moods, and more.\nAdditionally, many artist preferences use artists as a point of reference (e.g., \u201cMaybe something from 2pac and Ice Cube?\u201d).\nUsers express preferences in 80% of turns, while wizards elicit preferences 29% of the time.\nWe also observed that only about 5% of preferences in the dataset are negative preferences, e.g. \u201cThose might be too similar, though I do prefer them to not have any lyrics.\u201d\n\npreferences-by-turn shows how the relative frequency of different preferences change across turns.[\nWe only show the first 5 turns in preferences-by-turn as the number of data points sharply drops after 5 turns.\n]\nActivity and genre preferences are often expressed early in the conversation, and maintained as part of the conversation context in later turns.\nWizards often begin by eliciting the user for their preferences, and thus add fewer songs to the playlist in the first turn (per-turn-ratings).\nWe note that wizards often elicited users for artists and genres, which were the attributes that worked most reliably with the search tool; we believe this played a role in the wizards' choices.\n\nFinally, end-task-ratings summarizes the distribution of post-task user ratings.\nUsers were overall satisfied by the experience and \n    expressed that the system understood their preferences well and was very helpful in assisting them on the task.\nThey also expressed that they thought their playlist was mostly complete, and that they would listen to it again.\nThe post-task ratings were weakly correlated with the match scores, with Spearman rank correlation ranging from 0.11 for playlist satisfaction to 0.19 for system helpfulness (p < 0.05 in all cases), suggesting marginal benefit beyond ensuring wizards were at least somewhat familiar with genres the user was familiar with.\n\nbaselinesModeling Conversational Item Retrieval\nWe focus on modeling conversational item retrieval, and leave conversational item set refinement and conversational elicitation and explanation generation to future work.\nIn particular, we consider three canonical retrieval modeling approaches: sparse retrieval, dense retrieval, and query rewriting.\n\nsparse-modelSparse Retrieval: BM25\nBM25 is a commonly used sparse, bag-of-words ranking function that measures the relevance of a document or x to a query q.\nWe use BM25 in a sparse retrieval system by treating each item in the retrieval corpus as a candidate document represented in text as \u201c,\u201d and returning the top ranked items for each query.[\nWe used the hyperparameters k_1 = 1.5 and b=0.75 in our implementation.\n]\nWe include conversation history in queries by concatenating user queries from previous turns (omitting conversation history led to worse performance).\nFor consistency with the dense retrieval methods below, we use SentencePiece\u00a0<cit.> to tokenize both queries and documents.\n\nmodel-inputsDense Retrieval Methods\nWe consider a popular and efficient dense retrieval method: a dual encoder\u00a0<cit.>. \nDual encoders independently embed queries q and items x into normalized dense vectors using a neural network,\nand retrieve the most similar items for a query using cosine similarity:\n\u03c1(x; q) = f(q)^\u22a4 g(x),\nwhere f: \u2192^d and g: \u2192^d  are embedding functions for queries and items, respectively.\n\nAt turn t, we represent the using the conversation history H_t = (X_1, Z_1, E_1, \u2026, X_t) in text by concatenating user utterances with text representations of the top-k (k=3) songs added to the item set in each turn in reverse chronological order.\nFor example, the query at turn t is: \n\n    X_t  d(\u03b4 Z_t-1)  X_t-1\u2026\u00a0d(\u03b4 Z_1)  X_1,\n\nwhere d(\u03b4 Z_t) is a textual representation of the items (same as in BM25) added to the item set in turn t, and represents a separator token.\nWe do not include the previous system response (E_t) in the query.\n\n\n\n  \u00a7.\u00a7.\u00a7 Training and Inference\n\n\n\nFor training, we use a standard contrastive loss with in-batch negatives:\ngiven an example i with query q_i and target item set Z_i,\nwe randomly sample an item from x_i \u2208 Z_i to be a positive target,\nand take items from other item sets in the batch, x_j where j \u2260 i, to be negative targets.\nAdditionally, we augment the training data to improve robustness as follows: \n(1) we generate conversations of varying lengths by randomly truncating conversations to its first t turns, and\n(2) we generate historical item sets of varying lengths by randomly truncating them to their first k items.\n\n\n\n  \u00a7.\u00a7.\u00a7 Model implementation.\n\n\nWe use a shared encoder for f and g initialized either from a pre-trained T5 1.1-Base checkpoint\u00a0<cit.> or a T5 1.1-Base checkpoint that has been further pre-trained on retrieval tasks using the Contriever objective\u00a0<cit.>.\nWe then fine-tune the encoder on  for 1,000 steps using Adafactor\u00a0<cit.> with a batch size of 512 and a constant learning rate of 1e-3 (further fine-tuning led to overfitting).\nWe denote these methods as DE and Contriever respectively.\n\nAdditionally, to explore other relevant training data, we also fine-tuned a T5 1.1-Base checkpoint on a proprietary collection of 15,276 expertly curated playlists, , by using its playlist descriptions as queries and the playlists themselves as the target item sets.\nWe denote this system as .\n\nFor inference, we build an index of pre-computed item embeddings using each method's respective encoder.\nWe embed queries as in training, and use nearest neighbor search to return the top-k items for q_t.\n\nquery-rewritingQuery rewriting methods\nQuery rewriting or de-contextualization rewrites a query to incorporate conversation history, and has been widely used in state-of-the-art conversational search systems\u00a0<cit.>.\nFollowing prior work\u00a0<cit.>, we fine-tune a T5-based query rewriter on the CANARD dataset\u00a0<cit.> and use it to rewrite the queries in CPCD.\nWe adapt two of the above methods, BM25 and , to use a query rewriter by replacing the conversation history with rewritten queries; the resulting methods are named BM25+QR and +QR respectively.\n\n\nevalEvaluating Recommendation Performance\nWe now use  to evaluate the conversational item retrieval methods described in baselines.\n\nexp-setupExperimental Setup\n\n\n  \u00a7.\u00a7.\u00a7 Baselines.\n\nWe evaluate the following baseline methods: BM25, BM25+QR, , +QR, DE, and Contriever.\nAdditionally, we include a popularity baseline that returns the top 100 songs from the development set for every query in the test set.\n\n\n\n  \u00a7.\u00a7.\u00a7 Evaluation metrics.\n \n\nEvaluating recommendation systems is challenging because \nthere are often missing ratings\u00a0<cit.>: \nwe do not know whether an unrated song would be liked or disliked by a user. \nStandard ranking-based metrics treat items without ratings as not relevant\u2014even though they may actually be relevant\u2014and thus provide a lower bound on the recommendation performance.\nFollowing prior work\u00a0<cit.>, we compare systems using a standard ranking metric, Hits@k,\nwhich is 1 if and only if any of the top-k retrieved songs are in the target item set.\nUnless otherwise stated, we report a macro-averaged Hits@k, by averaging Hits@k across turns within a conversation and then across conversations.\nWe report Hits@k for multiple values of k (10, 20, 100):\n    while smaller values are more representative of realistic scenarios where users only consider a small set of items, <cit.> found that larger values of k have more discriminative power in offline experiments and can help mitigate the missing rating problem.\n\nEvaluating conversational recommendation systems poses an additional challenge: how to support evaluation across multiple turns.\nDifferent systems may recommend different items in previous turns, leading to divergent histories.\nTo fairly compare across models within a turn, we assume a shared \u201cgold\u201d history and target item set across all models. \nAdditionally, we remove any items in the target item set that also appear in the gold history.\nWhen there are additional songs liked in a turn, but not added to the history (e.g., when there is a limit to the number of songs in the history), we keep them in the target item set as leftovers\u00a0<cit.>, resulting in a larger target item set.\n\n\n\n \u00a7.\u00a7 Main Results\n \n\n\nbenchmark_results compares models on the CPCD test set.\nAs expected, the popularity baseline is among the worst evaluated, and its performance reflects the degree of overlap between the development and test sets: about 26% of songs liked in the development set were also liked in the test set and 38% of the top 100 songs in the development set also appear in top 100 songs of the test set.\nNext, the sparse retrieval baseline, BM25, does significantly better than the popularity baseline and is the second best method overall. \nIn particular, we find that BM25 scores highly on artist-based queries which are well-suited to sparse bag-of-words based methods. \nFurthermore, the search tool used by wizards appears to rely on lexical overlap, which may bias the results in favor of sparse methods.\nWe found that using query rewriting either did not improve or hurt performance: we attribute this to the rewriter not generalizing well to the music-seeking queries in CPCD; better query rewriting methods are a promising avenue for future work.\nFinally, the performance of the dense retrieval methods depended on both the fine-tuning dataset as well as the pretraining strategy:\n    while training on playlist descriptions (DE) outperforms the popularity baseline, it still lags far behind BM25,\n    and training solely on the  (DE) does even worse due to insufficient training data.\nIn contrast, training on  from a model pretrained for the retrieval task (Contriever) is the best method we evaluated and significantly outperforms BM25 (p < 0.05).\n\nconclusionConclusion\n\nWe introduced the task of conversational item set curation to capture recommendation scenarios, such as music consumption, where users are looking for a set of items instead of individual ones.\nIn these settings, we imagine users to collaborate with a system to iterative curate an item set through conversation.\nGiven a conversation history, we subdivided the task into conversational item retrieval (CIRt), conversational item set refinement (CISR) and conversational elicitation and explanation generation (CEEG). \nTo facilitate research in conversational item set curation and its subtasks, we developed an efficient data collection methodology and used it to create the Conversational Playlist Curation Dataset (CPCD).\nThe dataset contains rich preference statements from both users and wizards that are unique to the item set recommendation setting.\nFinally, we used the data to evaluate a wide range of conversational item retrieval methods.\nSpecifically, our evaluation focused on the CIRt subtask, where we showed that both sparse and dense retrieval methods are useful.\nFurther modeling\u2014e.g., pretraining paradigms that target conversational music recommendation or  explicitly modeling audio content\u2014is necessary to improve performance on this task.\nIn future work, we plan to explore using CPCD how to model and evaluate the remaining subtasks, CISR and CEEG, by utilizing the search actions and responses collected from wizards.\n\n\n\n\n\n\n\nACM-Reference-Format\n\n\n\n\n\n\n\n"}