{"entry_id": "http://arxiv.org/abs/2303.06657v1", "published": "20230312131305", "title": "Color Mismatches in Stereoscopic Video: Real-World Dataset and Deep Correction Method", "authors": ["Egor Chistov", "Nikita Alutis", "Maxim Velikanov", "Dmitriy Vatolin"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\nColor Mismatches in Stereoscopic Video: Real-World Dataset and Deep Correction Method\n\nThis study was supported by Russian Science Foundation under grant 22-21-00478, <https://rscf.ru/en/project/22-21-00478/>\n\n    \n    \n        Egor Chistov,1\n        Nikita Alutis,1\n        Maxim Velikanov,1\n        and\n        Dmitriy Vatolin12\n    \n    \n        Lomonosov Moscow State University1\n    \n    \n        MSU Institute for Artificial Intelligence2\n    \n    \n        \n    \n\n    \n===================================================================================================================================================================================================================================================================\n\n\n\n\n\nWe propose a real-world dataset of stereoscopic videos for color-mismatch correction. It includes real-world distortions achieved using a beam splitter. Our dataset is larger than any other for this task. We compared eight color-mismatch-correction methods on artificial and real-world datasets and showed that local methods are best suited to artificial distortions and that global methods are best suited to real-world distortions. Our efforts improved on the latest local neural-network method for color-mismatch correction in stereoscopic images, making it work faster and better on both artificial and real-world distortions.\n\n\n\n\ncolor mismatches, stereoscopic video, beam splitter, real-world video dataset, color transfer\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nThe left and right views of a stereoscopic image (stereopair) can have color mismatches for various reasons (e.g., illuminated camera filters, glare, and polarized light); Fig.\u00a0<ref> shows example. These mismatches can decrease the overall stereoscopic-video quality and may cause viewer discomfort and headaches\u00a0<cit.>.\n\nColor-mismatch correction is the task of transferring color from one view of a stereopair to corresponding areas in another where the colors differ incorrectly. A sufficiently large dataset and sampling of color-mismatch types are crucial to comparing color-mismatch-correction methods. Previous datasets for this task either are too small or lack real-world mismatch examples. Therefore, we prepared a dataset using a beam splitter and three cameras that each captured a different view simultaneously: a distorted left view, a ground-truth left view, and a right view. In addition to containing real-world distortions, our dataset is thus far the largest for this task.\n\nDespite the rapid development of deep learning, the latest neural-network method for correcting color mismatches\u00a0<cit.> is still inferior to conventional alternatives. We improved it by making it run faster and better on both artificial and real-world distortions.\n\nIn this paper, we propose a real-world dataset of stereoscopic videos for color-mismatch correction as well as a method for color-mismatch correction of stereoscopic images. We also compared eight color-mismatch-correction methods on an artificial dataset and our real-world dataset. The code and datasets are at <https://github.com/egorchistov/color-transfer/>.\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\nThis section provides an overview of stereoscopic datasets and methods for color-mismatch correction.\n\n\n\n \u00a7.\u00a7 Color-Mismatch-Correction Datasets\n\n\nFew datasets are available for color-mismatch correction. They are either too small or lack real-world mismatches.\n\nNiu et al.\u00a0<cit.> employed 2D videos with parallel camera motion to extract pseudo-stereopairs without color mismatches; they gathered 18 source stereopairs. Using Photoshop CS6, they applied saturation, brightness, contrast, hue, color-balance, and exposure operators\u2014each with three severity levels\u2014to one view of the stereopair.\n\nLavrushkin et al.\u00a0<cit.> proposed gathering frames from stereoscopic movies produced only via 3D rendering to obtain undistorted stereopairs. They employed 1,000 FullHD frames. After adding simplex noise to one view of the stereopair, they smoothed the result by applying a domain transform filter.\n\nGrogan et al.\u00a0<cit.> chose image pairs captured by one camera under different illumination conditions, camera settings, and color touch-up styles. They aligned reference images to match the target images. Their dataset contains 15 image pairs with real-world mismatches.\n\nThe work of Croci et al.\u00a0<cit.> used 1,035 stereopairs from several datasets: Flickr1024\u00a0<cit.>, InStereo2K\u00a0<cit.>, and the Ivy Lab database of stereoscopic 3D images\u00a0<cit.>. They applied various color operators\u2014brightness, color balance, contrast, exposure, hue, and saturation, each with six severity levels\u2014to one view of the stereopair using Photoshop 2021.\n\n\n\n \u00a7.\u00a7 Color-Mismatch-Correction Methods\n\n\nThe color-transfer problem is solvable globally or locally. Global methods\u00a0[Reinhard2001CGA, xiao2006color, Pitie2007IET] estimate a single color transformation for all pixels, and local methods\u00a0[pitie2007automated, Lavrushkin20183DTV, Grogan2019CVIU, Croci2021ICIP] do so for each pixel. Global methods do poorly on complex mismatches, and local methods produce inconsistent results in low-texture regions.\n\nReinhard et al.\u00a0<cit.> proposed approximating an image as a three-dimensional Gaussian distributed signal. For each channel in the CIELAB color space, they used a linear model to transfer color from a reference image to the target image. They chose CIELAB because it is uncorrelated and allowed them to independently manipulate all three color channels.\n\nXiao et al.\u00a0<cit.> computed a covariance matrix between color channels instead of treating them independently, because that matrix is the extension of standard deviation in correlated space. The authors decomposed the covariance matrix using the SVD algorithm. They described the transformation as a scale, rotation, and shift of pixel clusters.\n\nPiti\u00e9 et al.\u00a0<cit.> generalized all linear color-transfer approaches to the covariance-matrix fitting. They showed that the matrix decomposition this fitting implies can use the Cholesky decomposition, the square-root decomposition, or the solution to the Monge-Kantorovitch problem.\n\nPiti\u00e9 et al.\u00a0<cit.> proposed using an iterative-distribution fitting to transfer color. This method iteratively projects target and reference images on random one-dimensional axes and performs a probability-density transfer along those axes. The authors reduced grain-noise artifacts by minimizing a special cost function.\n\nLavrushkin et al.\u00a0<cit.> performed stereo matching with a modified cost function, making it better able to handle color mismatches between views. They applied a guided filter with a confidence-based variable-length kernel, using the left view as the target image and a warped right view as the guidance image.\n\nGrogan et al.\u00a0<cit.> proposed color transfer through estimation of the warping function that minimizes the divergence between two probability-density functions.\n\nCroci et al.\u00a0<cit.> employed a convolutional neural network for color-mismatch correction. First, the network extracts features from the input stereopair. It then feeds the extracted features into the parallax-attention mechanism\u00a0<cit.>, which performs stereo matching. Matched features pass through six residual blocks to yield the corrected stereoscopic view.\n\n\n\n\u00a7 PROPOSED DATASET\n\n\n\n\n\n\nWe propose a new real-world dataset of stereoscopic videos for evaluating color-mismatch-correction methods. We collected it using a beam splitter and three cameras that simultaneously capture three views of a scene: a distorted left view, the ground-truth left view, and a right view.\n\n\n\n \u00a7.\u00a7 Beam Splitter\n\n\nA beam splitter is an optical device that divides light into a transmitted beam and a reflected beam. Because some light is lost owing to absorption by the reflective coating, this device introduces real-world mismatches between stereopair views. Similar mismatches can appear in stereoscopic movies filmed with a beam-splitter rig.\n\nOur approach used a beam splitter to set a zero stereobase between the left camera and the left ground-truth camera, allowing us to create a distorted ground-truth data pair. A third camera captured the right view. Fig.\u00a0<ref> shows our setup.\n\nWe disabled optical stabilization and manually assigned all available camera settings, such as ISO, shutter speed, and color temperature. The cameras all had identical settings, so we obtained only beam-splitter distortions.\n\n\n\n \u00a7.\u00a7 Parallax Minimization\n\n\nFilming the left distorted view and left ground-truth view without parallax is crucial to achieving precise ground-truth data. Parallax is a difference in the apparent position of an object when viewed along two lines of sight. Postprocessing can precisely correct affine mismatches between views\u2014namely scale, rotation angle, and translation\u2014but it cannot correct parallax without using complex optical-flow, occlusion-estimation, and inpainting algorithms, which can reduce ground-truth-data quality.\n\nTo minimize parallax, we first visually aligned the lenses of the left distorted and left ground-truth cameras to obtain a visual zero stereobase. Then, using a photo from one camera and a video stream from another, we visualized the squared error between the views. This visualization allowed us to maximize the overlapping region by manually moving one camera. We achieved zero parallax through this method, but the manual alignment was time consuming.\n\n\n\n \u00a7.\u00a7 Postprocessing Pipeline\n\n\nOur research employed the postprocessing pipeline in Fig.\u00a0<ref>. Given the three videos from our cameras, we matched the horizontally flipped left distorted view to the left ground-truth view and rectified the stereopair using a homography transformation. To estimate the transformation parameters, we employed MAGSAC++\u00a0<cit.>. We selected SIFT\u00a0<cit.> and the Brute-Force matcher to match the left views, as well as the LoFTR matcher\u00a0<cit.> for rectification because it can better handle large parallax.\n\nThe technique of Piti\u00e9 et al.\u00a0<cit.> perfectly corrected the small loss in the transmitted beam. Without it, our approach would have been unable to obtain accurate ground-truth data. We performed temporal alignment using audio streams captured by the cameras: superimposing one stream on the other in the video editor allowed us to find the offset in frames.\n\n\n\n \u00a7.\u00a7 Scene Selection\n\n\n\n\n\n\nOur research involved scenes in which an object rests on a table with a white background, 1.5 meters from the camera (Fig.\u00a0<ref>). We filmed various objects including transparent glass, color patterns, and moving objects, choosing those that produced color mismatches. To ensure diversity, we filmed scenes under different lighting conditions and moving at different speeds. The lighting changed dynamically in one scene, and different scenes were shot with different set lighting.\n\nWe recorded several scenes in 4K resolution using three GoPro Hero 9 cameras. For the rest we recorded FullHD on two Legria HF G-10 cameras, one for the left ground-truth view and one for the right view, as well as a Panasonic HDC-SD800 for the left distorted view. The result was 24 scenes, each 50 frames long. We then cropped the region of interest to 960x720 pixels.\n\n\n\n\u00a7 PROPOSED METHOD\n\n\nColor-mismatch correction involves transferring color from one view of a stereopair to another in areas where colors incorrectly differ. Without limiting the task\u2019s generality, we propose a deep neural network that transfers color from the right view to the left view. The left corrected view should have a structure consistent with that of the left original view and colors consistent with those of the right view.\n\n\n\n \u00a7.\u00a7 Overview\n\n\nWe based our method on that of Croci et al.\u00a0<cit.>, borrowing ideas from Wang et al.\u00a0<cit.>. Our contribution is an effective multiscale network structure that works 2.6 times faster than Croci\u2019s neural-network-based method\u00a0<cit.> and, for artificial distortions, outperforms it by 3.7 dB on PSNR and, for real-world distortions, do so by 1.3 dB. Our method consists of three main modules: feature extraction, cascaded parallax attention, and transfer. Fig.\u00a0<ref> shows the overall architecture.\n\n\n\n \u00a7.\u00a7 Feature-Extraction Module\n\n\nThe left and right views feed into the feature-extraction module to yield their color and structural features at scales from 1 to 1/32. This module consists of six encoder blocks and three decoder blocks. Each is a residual block with batch normalization and weighted skip connection. For downsampling we used strided convolutions, and for upsampling we used strided transposed convolutions. For each scale we kept the channel count unchanged, as Fig.\u00a0<ref> shows.\n\n\n\n \u00a7.\u00a7 Cascaded Parallax-Attention Module\n\n\nFeatures from scales 1/16, 1/8, and 1/4 feed into the cascaded parallax-attention module (CasPAM) to obtain a multiscale warping-attention map. This module consists of three parallax-attention modules (PAMs) and three interpolation layers. The interpolation layers allow us to obtain a warping-attention map at these scales whenever directly calculating an attention map is memory inefficient. Each PAM consist of four parallax-attention blocks (PABs)\u00a0<cit.>. Every block updates the input features and converts them to a query and a key. After multiplying the query by the key, we added the result to the matching attention map. Our last step was to apply lower triangular softmax to this map and multiply the result by the right view\u2019s features to obtain warped right view\u2019s features.\n\n\n\n \u00a7.\u00a7 Transfer Module\n\n\nThe transfer module concatenates the valid mask, the left-view, and warped-right-view features at scales from 1/32 to 1. A valid mask indicates pixels that match in another view. Then, for each scale we added upsampled features from the previous scale to features from the current scale and passed the result through residual block without batch normalization. The residual block\u2019s output for scale 1 is the corrected left view.\n\n\n\n \u00a7.\u00a7 Loss Function\n\n\nWe trained our network using the L1, L2, and SSIM loss functions applied to the corrected left view and ground-truth left view. We trained our cascaded parallax-attention module without ground-truth attention maps using the photometric, smoothness, and cycle-consistency loss functions, as Wang et al.\u00a0<cit.> described.\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n \u00a7.\u00a7 Artificial Dataset\n\n\nTo compare our method with others we chose two datasets. The first dataset contained undistored stereopairs from Flickr1024\u00a0<cit.>, InStereo2K\u00a0<cit.>, and the Ivy Lab database of stereoscopic 3D images\u00a0<cit.>, following the method described by Croci et al.\u00a0<cit.>. To exclude stereopairs with color mismatches, we analyzed them automatically using the technique proposed by Grokholsky et al.\u00a0<cit.>. The final result was 1,035 undistorted stereopairs.\n\nInstead of employing Photoshop 2021 as Croci did, we applied augmentations to the left view of the stereopair. More specifically, our approach used the RandomBrightnessContrast, RandomGamma, and HueSaturationValue augmentations. We split the dataset into three parts: training (835 stereopairs), validation (100 stereopairs), and test (100 stereopairs).\n\n\n\n \u00a7.\u00a7 Real-World Dataset\n\n\nThe second dataset, described in Section\u00a0<ref>, contains the distortions we encountered while shooting stereoscopic videos using a beam splitter. We again divided the dataset into three parts: training (900 stereopairs), validation (150 stereopairs), and test (150 stereopairs).\n\n\n\n \u00a7.\u00a7 Training Procedure\n\n\nWe trained our network with the Adam optimizer over 100 epochs on the artificial dataset using random patches of size 512x256. The learning rate was 0.0001 and the batch size was 16. The network converged in 1.5 hours on a PC with an Intel Xeon Silver 4216 processor and Nvidia Titan RTX GPU with 24 GB of memory. Next, we fine-tuned all the neural-network-based methods on the real-world dataset over another 50 epochs at a learning rate of 0.0001. Our study found that in order to achieve higher scores on real-world distortions, one needs to fine-tune using artificial distortions with real-world ground truth stereopairs. When fine-tuning degraded the quality of the model, we used the untuned model.\n\n\n\n \u00a7.\u00a7 Quality Metrics\n\n\nOur evaluation used two full-reference quality metrics: PSNR (peak signal to noise ratio) and SSIM (structural simmilarity). PSNR is good for capturing changes in brightness, contrast, hue, and saturation, and SSIM is good for estimating content-dependent distortions. Also, we measured the time each method took to process one 512x512 frame. We repeated these measurements three times and in each case reported the shortest time in milliseconds.\n\n\n\n \u00a7.\u00a7 Ablation Study\n\n\n\n\nWe conducted an ablation study (Table\u00a0<ref>) on the validation part of the artificial dataset. Our conclusion is that the combination of the L1, L2, SSIM, and PAM loss functions is optimal and that all architectural components contribute to our network quality. Our study found that neural networks generalize much better to the unseen distortion type by using the valid mask. We also noticed that the batch-normalization layers in the transfer module yielded strong visible artifacts, so we disabled them in this module.\n\n\n\n \u00a7.\u00a7 Comparison Study\n\n\nWe compared the proposed local method (see Table\u00a0<ref> and Fig.\u00a0<ref>) with three global methods (Reinhard et al.\u00a0<cit.>, Xiao et al.\u00a0<cit.>, Piti\u00e9 et al.\u00a0<cit.>), one local method (Piti\u00e9 et al.\u00a0<cit.>), two correspondence-based local methods (Lavrushkin et al.\u00a0<cit.>, Grogan et al.\u00a0<cit.>), and one neural-network-based local method (Croci et al.\u00a0<cit.>).\n\nAs Table\u00a0<ref> shows, local methods perform better on artificial distortions and global ones perform better on real-world distortions. Local methods may do worse on the real-world dataset because of inconsistent results in low-texture regions; global methods may do worse on the artificial dataset because the artificial distortions are nonlinear.\n\nOur neural-network-based method achieves a 3.7 dB\u2013higher PSNR than Croci's neural-network-based method\u00a0<cit.> on the artificial dataset and a 1.3 dB-higher PSNR on the real-world dataset, along with a similar SSIM score, and is 2.6 times faster. That score is not, however, enough to outperform the conventional methods, which warranting further investigation.\n\n\n\n\u00a7 CONCLUSION\n\n\nFew datasets are available for evaluating color-mismatch-correction methods. Most of them are either too small or lack real-world mismatches. We created a new large-scale dataset of real-world stereoscopic videos to perform this task. We also showed that global and local methods produce different results on artificial and real-world datasets, and that neural-network-based methods fall short of conventional methods. We improved the latest neural-network method for correcting color mismatches and hope our contribution will enable development of methods that perform better on both artificial and real-world datasets.\n\n\n\nIEEEtran\n\n\n"}