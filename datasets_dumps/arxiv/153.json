{"entry_id": "http://arxiv.org/abs/2303.07142v2", "published": "20230313140953", "title": "Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification", "authors": ["Benjamin Clavi\u00e9", "Alexandru Ciceu", "Frederick Naylor", "Guillaume Souli\u00e9", "Thomas Brightwell"], "primary_category": "cs.CL", "categories": ["cs.CL"], "text": "\n\n\n\nPrompt Engineering for Job Classification\n\n\n\n\n\n\nB. Clavi\u00e9 et al.\n\nBright Network, Edinburgh, UK\n\n{ben.clavie, firstname.lastname}@brightnetwork.co.uk\n\nSilicon Grove, Edinburgh, UK\n\nalex@silicongrove.co\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLarge Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification\n    Benjamin Clavi\u00e91 Alexandru Ciceu2 Frederick Naylor1 Guillaume Souli\u00e91 Thomas Brightwell1\n    March 30, 2023\n======================================================================================================\n\n\n\n\n\n\n\nThis case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an English-language is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification, including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically, we evaluate the performance of two commercially available state-of-the-art GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. \nWe also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance.\n\nOur results show that, with a well-designed prompt, a zero-shot gpt-3.5-turboclassifier outperforms all other models, achieving a 6% increase in Precision@95% Recall compared to the best supervised approach. Furthermore, we observe that the wording of the prompt is a critical factor in eliciting the appropriate \"reasoning\" in the model, and that seemingly minor aspects of the prompt significantly affect the model's performance. \n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nThe combination of broadened access to higher education and rapid technological advancement with the mass-adoption of computing has resulted in a number of phenomena. The need for computational tools to support the delivery of quality education at scale has been frequently highlighted, even allowing for the development of an active academic subfield<cit.>. At the other end of the pipeline, technological advances have caused massive changes in the skills required for a large amount of jobs<cit.>, with some researchers also highlighting a potential mismatch between these required sets of skills and the skills possessed by the workforce<cit.>. These issues lead to a phenomenon known as the \"education-job mismatch\", which can lead to negative effects on lifetime income<cit.>. \n\nDue in part to these factors, the modern employment landscape can be difficult to enter for recent graduates, with recent LinkedIn surveys showing that over a third of \"entry-level\" positions require multiple years of experience, and more than half of such positions requiring 3 years experience in certain fields or extremely specific skills<cit.>. As a result, it has been noted that entering the job market is an increasingly difficult task, now demanding considerable time and effort<cit.>.\nWhile computational advances are now commonly used to support education and to assist workers in their everyday work, there is a lack of similarly mature technological solutions to alleviate the issues presented by exiting education to enter the workplace. We believe that the rapid development of machine learning presents a powerful opportunity to help ease this transition. \n\nThe case study at the core of this paper focuses on one of the important tasks to build towards this objective: Graduate Job Classification. Given a job posting containing its title and description, our aim is to be able to automatically identify whether or not the job is a position fit for a recent graduate or not, either because it requires considerable experience or because it doesn't require a higher education qualification. In light of the information presented above, as well as the sheer volume of job postings created everyday, this classification offers an important curation. This would allow graduates to focus their efforts on relevant positions, rather than spending a considerable amount of time filtering through large volumes of jobs, which is non-trivial due to often obfuscated requirements.<cit.> As a point of reference, the number of total job postings in the United Kingdom alone in the July-September 2022 period exceeded 1.2 million<cit.>. \n\nThis task contains a variety of challenges, the key one being the extreme importance of minimising false negatives, as any false negative would remove a potentially suitable job from a job-seeker's consideration when the list is presented to them. On the other hand, with such large volumes of posting, too many false positives would lead to the curated list being too noisy to provide useful assistance. A second major challenge is the reliance of the task on subtle language understanding, as the signals of a job's suitability can be very weak. \n\nIn this paper, we will evaluate a variety of text classification approaches applied to the English-language Graduate Job Classification task. In doing so, we will (i) show that the most recent Large Language Models (LLMs), based on Instruction-Tuned GPT-3<cit.>, can leverage the vast wealth of information acquired during their training to outperform state-of-the-art supervised classification approaches on this task and that (ii) proper prompt engineering has an enormous impact on LLM downstream performance on this task, contributing a real-world application to the very active research on the topic of prompt engineering<cit.>.\n\n\n\n\u00a7 BACKGROUND\n\n\n\n\n\n\nSince the introduction of the Transformer architecture<cit.> and the rise of transfer learning to leverage language models on downstream tasks<cit.>, the field of NLP has undergone rapid changes. Large pre-trained models such as BERT<cit.> and later improvements, like DeBERTa<cit.>, have resulted in significant performance improvements, surpassing prior word representation methods such as word vectors<cit.>. The development libraries such HuggingFace Transformers<cit.> has further contributed to making these models ubiquitous in NLP applications.\n\nThese advances resulted in a paradigm shift in NLP, focusing on the use or fine-tuning of extremely large, generalist, so-called \"foundation models\" rather than the training of task-specfic models<cit.>. This resulted in the frequent occurence of paradigm shift, where researchers focused on ways to reframe complex tasks into a format that could fit into tasks where such models are known to be strong, such as question-answering or text classification<cit.>.\n\nIn parallel to these fine-tuning approaches, there has been considerable work spent on the development of generative Large Language Models (LLMs), whose training focuses on causal generation: the task of predicting the next token given a context<cit.>. The release of GPT-3 showed that these models, on top of their ability to generate believable text, are also few-shot learners: given few examples, they are capable of performing a variety of tasks, such as question answering<cit.>.\n\nGoing further, very recent developments have shown that LLMs can reach noticeably better performance on downstream applications through instruction-tuning: being fine-tuned to specifically follow natural language instructions to reach state-of-the-art performance on many language understanding tasks.<cit.>.\n\nLLMs, being trained on billions of tokens, have been shown to be able to leverage the vast amount of knowledge found in their training data on various tasks, with performance increasing via both an increase in model and training data size, following complicated scaling laws<cit.>. This has paved the way for the appearance of a new approach to NLP applications, focusing on exploring ways to optimally use this large amassed knowledge: prompt engineering<cit.>.\n\nPrompt Engineering represents a new way of interacting with models, through natural language queries. It has gathered considerable research attention in the last year. Certain ways of prompting LLMs, such as Chain-of-Thought (CoT) prompting, have been shown to be able to prompt reasoning which considerably improve the models' downstream performance<cit.>. Additional research has showcased ways to bypass certain model weaknesses. Notably, while LLMs are prone to mathematical errors, they are able to generate executable Python code to compute the requested results through specific prompting<cit.>.\n\nOther efforts have showcased reasoning improvements by relying on a model self-verifying its own reasoning in a subsequent prompt improves performance<cit.>. All these approaches have shown that LLMs can match or outperform state-of-the-art results on certain tasks, while requiring little to no fine-tuning.\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTAL SETUP\n\n\n\n \u00a7.\u00a7 Data and Evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData\nOur target task is Graduate Job Classification. It is a binary classification, where, given a job posting containing both the job title and its description, the model must identify whether or not the job is a position fit for a recent graduate or not, either because it requires more experience or doesn't require higher education. In practice, over 25 000 jobs are received on a daily basis, with fewer than 5% of those appropriate for graduates.\n\n\n\nOur data is gathered from a large selection of UK-based jobs over a period of two years. These jobs were manually filtered into \"Graduate\" and \"Non-Graduate\" categories by human annotators. The gold standard dataset used in  this study is a subset of the original data, containing job postings whose original label was manually reviewed. Only jobs where inter-annotator agreement was reached were kept, until reaching a data size of 10000. We use the label GRAD for jobs suitable for graduates and NON_GRAD for all other jobs. In total, the data 3082 (30.8%) GRAD and 6918 (69.2%) NON_GRAD examples.\n\n\n\n\n\n\n\n\n\n\nEvaluation\nWe use the  Precision at 95% Recall (P@95%R) for the GRAD label as our main metric.\nWe chose this metric as the classifier cannot be deployed in production with a low recall, as it is extremely damaging to remove suitable jobs from graduates' consideration. Our goal is to ensure that Recall remains above a specific threshold while achieving the best possible precision at this threshold and help process the tens of thousands of jobs received daily. We also report the P@85%R, to give a better overview of the models' performance. \nTo facilitate LLM evaluation, we split our data into a 7000 examples training set and 3000 examples test set rather than using cross-validation.\n\n\n\n\n\n\n \u00a7.\u00a7 Baselines\n\n\n\n\nKeyword  We report the results for a simple, keyword and regular expression approaches to the task. We, along with our annotators, built a list of common phrases indicating that a job is suitable for a recent graduate. We then perform a simple look-up within the postings, which give us a lower bound for performance.\n\n\nSVM We present the results of a non-deep learning baseline method, which involves using a Support Vector Machine (SVM) classifier with a tf-idf text representation, which have been shown to produce robust baseline results, even reaching state-of-the-art results in some domain-specific tasks<cit.>.\n\n\n \u00a7.\u00a7 Supervised Classifiers\n\n\n\n\nULMFiT We report the results for ULMFiT, an RNN-based approach to training a small language model before fine-tuning it for classification<cit.>. We pre-train the ULMFiT language model on an unlabeled dataset of 50000 job postings, before fine-tuning the classifier on the data described above.\n\n\n\nDeBERTa-V3 We fine-tune a DeBERTa-V3-Base model, a refined version of DeBERTa<cit.> and which achieves state-of-the-art performance on a variety of text classification tasks<cit.>. We follow the method used in the paper introducing the model, with a maximum sequence length of 512. For any longer document, we report results using the first 100 tokens and the trailing 412 tokens of the document. This approach yielding the best results is likely due to most job descriptions frequently outlining the position's requirements towards the end.\n\n\n\n\n \u00a7.\u00a7 Large Language Models\n\nWe use a temperature of 0 for all language models.\n\n\nGPT-3.5 (text-davinci-002&text-davinci-003)We report our results on two variants of GPT-3<cit.>[These models are accessed through OpenAI's API.]. These models are LLMs further trained to improve their ability to follow natural language instructions<cit.>.\n\n\nGPT-3.5-turbo (gpt-3.5-turbo-0301) We evaluate GPT-3.5-turbo[This model is also accessed through OpenAI's API.], a model optimised for chat-like interactions<cit.>. To do so, we modified all our prompts to fit the conversation-like inputs expected by the model. GPT-3.5-turbo is the focus of our prompt engineering exploration. \n\n\n\n\n\u00a7 OVERALL RESULTS\n\n\nIn Table\u00a0<ref>, we report the the P@95% P85% for all models evaluated. We report a score of 0 for those if the model is unable to reach the recall threshold. For all approaches for which we do not have a way to target a specific Recall value, we also provide their Recall metric.\n\n\n\nOverall, we notice that SVMs, as often, are a strong baseline, although they are outperformed by both supervised deep learning approaches. However, they're outperformed by both of our supervised approaches. DeBERTaV3 achieves the highest P@85%R of all the models, but is beaten by both davinci-003 and GPT-3.5 on the P@95%R metric, which is key to high quality job curation.\n\n\n\n\n\n\n\n\n\n\n\n\nWe notice overall strong performance from the most powerful LLMs evaluated, although davinci-002, fails to reach our 95% Recall threshold, and trails behind both ULMFiT and DeBERTaV3 at a 85% recall threshold. On the other hand, davinci-003 outperforms DeBERTaV3, while GPT3-5 is by far the best-performing model on the P@95%R metric, with a 7.2 percentage point increase.\n\nOverall, these results show that while our best performing supervised approach obtains better metrics at lower recall thresholds, it falls noticeably behind LLMs behind when aiming for a very low false negative rate.\n\n\n\n\n\n\n\n\u00a7 LLMS & PROMPT ENGINEERING\n\n\n\n\nIn this section, we will discuss the prompt engineering steps taken to reach the best-performing version of gpt-3.5. We will largely focus on its chat-like input, although similar steps were used for other language models, minus the conversational format. Apart from the use of system messages, we noticed no major differences in prompt impact between models.\n\nFor each modification, we will provide an explanation of the changes, or, where relevant, a snippet highlighting the modification. An overview of all prompt modifications used is presented in Table\u00a0<ref>. \n\n\n\n\n\n \u00a7.\u00a7 Eliciting Reasoning\n\n\n\nZero-Shot Prompting\nWe set our baseline by simply prompting the model with our question with no further attempt to induce reasoning ('Baseline'):\n\n[breaklines,breakanywhere,bgcolor=lightgray]text\nFor the given job:\njob_posting\n\u2014\u2014\u2014\nIs this job (A) a job fit for a recent graduate,\nor (B) a job requiring more professional experience.\nAnswer:\n\n\n\n\n\nFew-shot CoT\nWe then experiment with few-shot chain-of-thought prompting<cit.>, by providing the model with successful classification examples. We do so using the gpt-3.5 chat format, mocking a conversation between the user, and the assistant, who elaborates on his reasoning before answering with (A) or (B). We prepend our query by providing the model with two examples[Due to the long token length of job postings, providing it with more than two examples required us to truncate the postings, which resulted in a degradation in performance] ('CoT').\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZero-shot CoT\nWe then attempt to elicit reasoning without providing the model any example, through Zero-shot Chain-of-Thought<cit.> ('Zero-CoT'). We expect that this approach will perform well, as job postings are found in large quantity in data used to train the model, and identifying whether a job is fit for a graduate does not require expert domain knowledge. We attempt to elicit reasoning by prompting the model think step-by-step, as follows:\n\n[breaklines,breakanywhere,bgcolor=lightgray, escapeinside=||]text\nFor the given job:\njob_posting\n\u2014\u2014\u2014\nIs this job (A) a job fit for a recent graduate,\nor (B) a job requiring more professional experience.\nAnswer: |Let's think step by step,|\n\n\n\n\n\n \u00a7.\u00a7 Initial Instructions\n\n\nWe then explore the impact of providing the model with instructions describing both its role and task. A notable difference between textscdavinci-003 and the gpt-3.5 chat format is that the latter introduces a new aspect to prompt engineering, which was not found in previous ways to interact with language models: the ability to provide a system message to the system. We explore multiple ways of providing instructions using this system message. \n\n\nGiving Instructions We provide information to the model about its role role as well as a description of its task:\n[breaklines,breakanywhere,bgcolor=lightgray, escapeinside=||]text\n|role| = \"\"\"You are an AI expert in career advice. You are tasked with sorting through jobs by analysing their content and deciding whether they would be a good fit for a recent graduate or not.\"\"\"\n|task| = \"\"\"A job is fit for a graduate if it's a junior-level position that does not require extensive prior professional experience. I will give you a job posting and you will analyse it, to know whether or not it describes a position fit for a graduate.\"\"\"\n\n\nInstructions as a user or system message There is no clear optimal way to use the system prompt, as opposed to passing instructions as a user query. The 'rawinst' approach, explained above, passes the whole instructions to the model as a user query. We evaluate the impact of passing the whole instructions as a system query ('sysinst'), as well as splitting them in two, with the model's role definition passed as a system query and the task as a user query (bothinst).\n\n\nMocked-exchange instructions\nWe attempt to further take advantage of the LLM's fine-tuned ability to follow a conversational format by breaking down our instructions further ('mock'). We iterate on the bothinst instruction format, by adding an extra confirmation message from the model:\n[breaklines,breakanywhere,bgcolor=lightgray, escapeinside=||]text\nuser_message_1 = \"\"\"A job is fit for a graduate [...] |Got it?\"\"\"|\n|assistant_message_1 = \"Yes, I understand. I am ready to analyse your job posting.\"|\n\n\n\nRe-iterating instructions\nWe further modify the instructions by introducing a practice commonly informally discussed but with little basis: re-iterating certain instructions ('reit'). In our case, this is done by appending a reminder to the system message, to reinforce the perceived expertise of the model in its role description, as well as reinforcing the importance of thinking step-by-step in the task description:\n[breaklines,breakanywhere,bgcolor=lightgray, escapeinside=||]text\n|system_prompt| =\"\"\"You are an AI expert in career advice. You are tasked with sorting through jobs by analysing their content and deciding whether they would be a good fit for a recent graduate or not. |Remember, you're the best AI careers expert and will use your expertise to provide the best possible analysis|\"\"\"\n|user_message_1| = \"\"\"[...] I will give you a job posting and you will analyse it, |step-by-step|, to know whether [...]\"\"\"\n\n\n\n\n \u00a7.\u00a7 Wording the prompt\n\n\n\nAnswer template\nWe experiment with asking the model to answer by following specific templates, either requiring that the final answer ('loose'), or the full reasoning ('strict') must adhere to a specific template. We experiment with different wordings for the template, with the best-performing ones as follows:\n[breaklines,breakanywhere,bgcolor=lightgray, escapeinside=||]text\n|loose| = \"\"\"[...]|Your answer must end with:|\n|Final Answer: This is a| (A) job fit for a recent graduate or\na student OR (B) a job requiring more professional experience.\nAnswer: Let's think step-by-step,\"\"\"\n|strict| = \"\"\"[...]|You will answer following this template:|\n|Reasoning step 1:||Reasoning step 2:||Reasoning step 3:||Final Answer: This is a| (A) job fit for a recent graduate or\na student OR (B) a job requiring more professional experience.\nAnswer: |Reasoning Step 1:|\"\"\"\n\n\n\n\nThe right conclusion We evaluate another small modification to the prompt to provide further positive re-inforcement to the model: we ask it reason in order to reach the right conclusion, by slightly modifying our final query:\n\n[breaklines,breakanywhere,bgcolor=lightgray, escapeinside=||]text\nAnswer: Let's think step-by-step |to reach the right conclusion,|\n\n\n\n\nAddressing reasoning gaps While analysing our early results, we noticed that the model can misinterpret instructions given to it, and produce flawed reasoning as a result. This manifested in attempts to over-generalise:\n\nThis job requires experience, but states that it can have been acquired through internships. However, not all graduates will have undergone internships. Therefore, (B) this job is not fit for all graduates.\n\n\nWe attempt to alleviate this by providing additional information in the model's instruction: \n\n\n[breaklines,breakanywhere,bgcolor=lightgray, escapeinside=||]text\ntask = \"A job is fit for a graduate if it's a junior-level position that does not require extensive prior professional experience. |When analysing the experience required, take into account that requiring internships is still fit for a graduate|. I will give you a job [...]\n\n\n\n\n\n\n\n \u00a7.\u00a7 The importance of subtle tweaks\n\n\n\nNaming the Assistant\nA somewhat common practice, as shown by Microsoft code-naming its Bing chatbot \"Sydney\" [As demonstrated by the widely circulated prompt https://simonwillison.net/2023/Feb/15/bing/], is to give LLMs a nickname by which they can be referred to. We modified our initial system prompt, as well as the user mocked-instructions, to refer to our model as Frederick ('name'), as follows:\n\n[breaklines,breakanywhere,bgcolor=lightgray,escapeinside=||]text\n|system_prompt| = \"|You are Frederick,| an AI expert\nin career advice. [...]\"\n[...]\n|first_assistant_response| = \"Yes, I understand. |I am Frederick,|\nand I will analyse your job posting.\"\n\n\n\n\n\nPositive Feedback\nIt has been anecdotally noted that giving positive reinforcement to gpt-3.5 can lead to better performance on some tasks [As reported by OpenAI, a partnered developer found that positive reinforcement resulted in increased accuracy.]. We thus prepend our main prompt with a positive reaction to the model's mocked acknowledgement of our instructions ('pos'):\n[breaklines,breakanywhere,bgcolor=lightgray,escapeinside=||]text\n|Great! Let's begin then :)|\nFor the given job: [...]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 PROMPT ENGINEERING RESULTS AND DISCUSSION\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe evaluation metrics (calculated against the GRAD label), as well as Template Stickiness, for all the modifications detailed above are presented in Table\u00a0<ref>. We provide these metrics rather than the more task-appropriate P@95%R used above to make it easier to compare the various impacts of prompt changes. Any modification below 95% Recall is presented in italic.\n\n\nWe notice that the impact of prompt-engineering on classification results is high. Simply asking the model to answer the question using its knowledge, only reaches an F1-score of 65.6, with a Recall of 70.6, considerably short of our target, while our final prompt reaches an F1-score of 91.7 with 97% recall.\n\nInterestingly, few-shot CoT prompting the model with examples performs noticeably worse than a zero-shot approach. We speculate that this is due to the examples biasing the model reasoning too much while the knowledge it already contains is sufficient for most classifications. Any attempt at providing more thorough reasoning for either label resulted in increased recall and decreased precision for the label. Despite multiple attempts, we found no scenario where providing examples performed better than zero-shot classification.\n\nProviding instructions to the model, with a role description as its system message and an initial user message describing the text, yielded the single biggest increase in performance (+5.9F1). Additionally, we highlight the impact of small changes to guide the model's reasoning. Mocking an acknowledgement of the instruction allows the model to hit the 95% Recall threshold (+1.3F1). Small additions, such as naming the model or providing it with positive reinforcement upon its acknowledgement of the instructions, also resulted in increased performance.\n\nWe found that gpt-3.5.turbo struggles with Template Stickiness, which we did not observe with text-davinci-003. Its answers often required additional parsing, as it would frequently discard the (A)/(B) answering format asked of it. Requesting that it follows either a strict reasoning template or a loose answer template yielded considerably higher template stickiness but resulted in performance decreases, no matter the template wording.\n\nOverall, we find that these results highlights just how prompt-sensitive downstream results are, and we showcase a good overview of common techniques that can result in large performance improvements.\n\n\n\n\u00a7 CONCLUSION\n\nIn this work, we have presented the task of Graduate Job Classification, by highlighting its importance. We have then evaluated a series of classifiers on a real-world dataset, attempting to find which approach allows for the best filtering of non-graduate joba while still meeting a sufficiently high recall threshold to not remove a large amount of legitimate graduate jobs in our curation efforts. In doing so, we showcased that the best-performing approach on this task is the use of Large Language Models (LLMs), in particular OpenAI's gpt-3.5-turbo.\n\nUsing language models for downstream tasks require a different paradigm, where time is not spent on fine-tuning the model itself but on improving the prompt, a natural language query. We present our evaluation of various prompt modifications, and present the fast improvement in performance that can be obtained by proper prompt engineering to allow the language model to leverage its vast amounts of amassed knowledge. We believe our work, presenting a real-world case study of the strong performance of LLMs on text classification tasks, provides good insight into prompt engineering and the specific prompt-tuning necessary to accomplish certain tasks. We provide our full results, the resulting prompt being currently used to filter thousands of jobs on a daily basis, to help support future applications in this area.\nsplncs04\n\n"}