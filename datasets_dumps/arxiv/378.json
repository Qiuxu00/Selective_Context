{"entry_id": "http://arxiv.org/abs/2303.06808v1", "published": "20230313014705", "title": "Boosting Source Code Learning with Data Augmentation: An Empirical Study", "authors": ["Zeming Dong", "Qiang Hu", "Yuejun Guo", "Zhenya Zhang", "Maxime Cordy", "Mike Papadakis", "Yves Le Traon", "Jianjun Zhao"], "primary_category": "cs.SE", "categories": ["cs.SE", "cs.AI"], "text": "\n\n\n\n\n\n\n\n\n\n  Kyushu University\n\n\n\nCorresponding author\n\n\n  University of Luxembourg\n\n\n\n\n\n  Luxembourg Institute of Science and Technology\n\n\n\n\n\n  Kyushu University\n\n\n\n\n\n\n\n\n\n\n\n\n\n  University of Luxembourg\n\n\n\n\n\n\n  Kyushu University\n\n\n\n\n\nThe next era of program understanding is being propelled by the use of machine learning to solve software problems. Recent studies have shown surprising results of source code learning, which applies deep neural networks (DNNs) to various critical software tasks, e.g., bug detection and clone detection. \nThis success can be greatly attributed to the utilization of massive high-quality training data, and in practice, data augmentation, which is a technique used to produce additional training data, has been widely adopted in various domains, such as computer vision. \n\nHowever, in source code learning, data augmentation has not been extensively studied, and existing practice is limited to simple syntax-preserved methods, such as code refactoring.\n\nEssentially, source code is often represented in two ways, namely, sequentially as text data and structurally as graph data, when it is used as training data in source code learning. \nInspired by these analogy relations, we take an early step to investigate whether data augmentation methods that are originally used for text and graphs are effective in improving the training quality of source code learning.\n\nTo that end, we first collect and categorize data augmentation methods in the literature. Second, we conduct a comprehensive empirical study on four critical tasks and 11 DNN architectures to explore the effectiveness of 12 data augmentation methods (including code refactoring and 11 other methods for text and graph data). \n\nOur results identify the data augmentation methods that can produce more accurate and robust models for source code learning, including those based on mixup (e.g., SenMixup for texts and Manifold-Mixup for graphs), and those that slightly break the syntax of source code (e.g., random swap and random deletion for texts).\n\n\n\n\n\n\n\n\n\n\n\n\n<ccs2012>\n <concept>\n  <concept_id>10010520.10010553.10010562</concept_id>\n  <concept_desc>Computer systems organization\u00a0Embedded systems</concept_desc>\n  <concept_significance>500</concept_significance>\n </concept>\n <concept>\n  <concept_id>10010520.10010575.10010755</concept_id>\n  <concept_desc>Computer systems organization\u00a0Redundancy</concept_desc>\n  <concept_significance>300</concept_significance>\n </concept>\n <concept>\n  <concept_id>10010520.10010553.10010554</concept_id>\n  <concept_desc>Computer systems organization\u00a0Robotics</concept_desc>\n  <concept_significance>100</concept_significance>\n </concept>\n <concept>\n  <concept_id>10003033.10003083.10003095</concept_id>\n  <concept_desc>Networks\u00a0Network reliability</concept_desc>\n  <concept_significance>100</concept_significance>\n </concept>\n</ccs2012>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoosting Source Code Learning with Data Augmentation: An Empirical Study\n    Jianjun Zhao\n    \n========================================================================\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\nIn recent years, there has been a growing trend in using machine learning for big code (ML4Code)\u00a0<cit.> due to the power of machine learning (especially deep learning) in mining patterns from large corpora of code. The promising results of ML4Code in multiple downstream code tasks, such as code clone detection\u00a0<cit.>, vulnerability detection\u00a0<cit.>, and problem classification\u00a0<cit.>, demonstrate its great potential in facilitating developers in practice. To produce programming language (PL) models with good performance, high-quality training data, and well-designed model architectures are crucial. While model architecture design can be informed by ideas from the natural language processing (NLP) field\u00a0<cit.>, there is no shortcut to preparing high-quality training data. Generally, the data must be collected, cleaned, and manually labeled before use. All these processes are challenging, and especially, data labeling is the most time-consuming and labor-intensive task. For example, labeling only four libraries of code can take 600 man-hours\u00a0<cit.>. As a result, it remains a challenging problem to prepare sufficient training data for PL models.\n\nIn the fields such as computer vision (CV), data augmentation has been a widely-used technique to solve the aforementioned issue of the lack of labeled training data. Essentially, data augmentation generates new training data by modifying existing labeled data under the premise that these new data preserve the original semantics. For example, when training an image classification model, instead of using the original training data only, a common practice is to utilize image transformation techniques\u00a0<cit.> to produce more diverse images. \nExisting studies\u00a0<cit.> have shown that data augmentation can effectively improve the accuracy and robustness of the models being trained.\n\n\n\nDespite the remarkable attention data augmentation has already gained in other fields, its value in source code learning is not yet clarified.\nMost existing studies in ML4Code still stick to the design of more powerful model architectures (e.g., GraphCodeBERT\u00a0<cit.>), more generic code representation techniques (e.g., token-based\u00a0<cit.> or tree-based representation\u00a0<cit.>), or learning strategies (e.g., using contrastive learning for code tasks\u00a0<cit.>). \nOnly a few works tend to study the problem of automatically enriching the code-related training data, such as\u00a0<cit.> in which a series of code refactoring methods are introduced for data augmentation. However, as reported in their works, the performance of those methods is very limited, and more effective data augmentation methods in source code learning are still in demand. \n\n\n\nContributions. \nIn source code learning tasks,  source code is often represented in two forms, namely, sequential data and structural data, when it plays the role of training data: the former is due to its analogy to texts, while the latter is realized via the abstract syntax tree (AST), a graph theoretical model that captures the relationship between different components of the code.\n\n\nInspired by these two representations, we empirically study the problem of whether existing data augmentation approaches in natural language processing (NLP) (that handles text data) and graph learning (that handles graph data) are effective to improve the training quality in source code learning.\n\nConcretely, we first survey and categorize existing data augmentation methods in the literature, and\n\n\nwe find 11 data augmentation methods from NLP and graph learning that are possible to be used in source code learning. Then, we adapt these methods to train code models and check their effectiveness in improving the accuracy and robustness of those models.\n\nWe design our large-scale study in order to answer the following research questions:\n\nRQ1: Can existing data augmentation methods produce accurate code models? The results show that data augmentation methods that linearly mix feature vectors in code embedding, e.g., SenMixup, can enhance the accuracy by up to 8.74%, compared to the training without using data augmentation. Remarkably, the methods adapted from NLP and graph learning are more effective than the code-specific data augmentation technique, namely, code refactoring. \n\nRQ2: Can existing data augmentation methods produce robust code models? \nThe results show that using data augmentation brings limited robustness improvement. Specifically, random swap, a method from NLP,  performs the best and can reduce the attack success rate by 5.67%. \n\nRQ3: How does data volume affect the effectiveness of data augmentation methods? The results demonstrate that when training data is scarce, incorporating data augmentation can help to improve both the accuracy and robustness. For example, using SenMixup can improve the accuracy of CodeBERT by up to 11.69%, and data augmentation methods that slightly break the syntax can enhance the robustness by up to 38.47%.  \n\nTo the best of our knowledge, this is the first work that adapts data augmentation methods from NLP and graph learning and empirically studies the effectiveness of incorporating data augmentation into training code models. Specifically, in this study, we found that data augmentation methods from NLP and graph learning can outperform code refactoring, which is dedicated to source code learning. \n\nMoreover, we found that even though some data augmentation methods can produce training data that slightly break the syntax of the source code, they are still useful in improving the quality of training in source code learning.\nWith these insightful findings, we pave the path to further research in boosting source code learning using data augmentation.\n\n\n\n\n\n\n\n\n\nPaper organization\nThe rest of this paper is organized as follows. Section\u00a0<ref> introduces the background of this work. Section\u00a0<ref> presents the adaptation of the data augmentation methods from NLP and graphs for source code learning. Section\u00a0<ref> presents the design of our empirical study. Section\u00a0<ref> details the experimental setup. Section\u00a0<ref> analyzes the results of our experiments. Section\u00a0<ref> discusses the main findings and limitations of our work. Section\u00a0<ref> summarizes related works, and the last section concludes this paper.\n\n\n\n\n\u00a7 BACKGROUND\n\n\n\n\n \u00a7.\u00a7 Source Code Learning\n\n\nIn a nutshell, source code learning consists in learning the information from source code and using the learned information to solve the downstream tasks, such as automated program repair\u00a0<cit.>, automated program synthesis\u00a0<cit.>, automated code comments generation\u00a0<cit.>, and code clone detection\u00a0<cit.>. \n\nAs mentioned in Section\u00a0<ref>, code representation is a crucial technique that converts source code into a DNN-readable format to learn the features\u00a0<cit.>. In this paper, we consider two widely-used code representations, namely sequential representation and structural representation, as follows:\n\n\n  * Sequential representation transforms the source code into a sequence of tokens (in a similar way to handling text data), where a token is the basic component of the code, such as a separator, an operator, a reserved word, a constant, and an identifier. \nIn this way, the original source code is processed to a number of tokens, e.g.,  \u201c\u201d is transformed to \u201c[]\u201d. Sequential representation keeps the context of the source code, which is useful for learning the syntactic information of source code. \n\n  * Structural representation transforms the code into a graph  that  captures the relationship of different components in the code while preserving its syntax elements. The graph can be a abstract syntax tree (AST), a control flow graph (CFG), and a data flow graph (DFG). By using structural representation, we can learn a model that perceives the semantic information of the code.\nIndeed, graph neural networks (GNNs), a DNN architecture that is recently popular in source code analysis, is based on the structural representation of source code. \n\n\n\n\n\n \u00a7.\u00a7 Data Augmentation in Source Code Learning\n\n\n\nDespite the great advantages of DNN, there are two main bottlenecks that prevent DNNs from achieving high performance, \n1) the lack of high-quality labeled training data and \n2) the different data distribution between training data and testing data. \nOne simple solution to these two problems is to increase the size and diversity of training data. Data augmentation\u00a0<cit.> is proposed to automatically produce additional synthetic training data by modifying existing data without further human effort. Generally, data augmentation involves a family of well-designed data transformation methods. For instance, in image processing, commonly-used data augmentation methods include re-scaling, zooming, random rotating, padding, and adding noise\u00a0<cit.>.\n\n\nRecently, software engineering researchers also considered data augmentation in source code learning\u00a0<cit.>, and the proposed methods are known as code refactoring. \n\n\nIn general, code refactoring, originally used for code simplification, involves a family of  techniques that rewrite the syntactic structure of source code while keeping the semantic information\u00a0<cit.>. \n\nCommonly-used code refactoring techniques include local variable renaming, duplication, dead store, etc. For instance, local variable renaming is a method that changes the names of a code element, including symbols, files, directories, packages, and modules. Technically, this method modifies the source code slightly but does not change the semantic behavior of the program.\n\n\n\n\n\n\n\n\n\nHowever, existing studies\u00a0<cit.> have shown that these simple strategies have limited advantages in improving the performance of code models. \nIn this study, inspired by the analogy of source code to texts and graphs (as mentioned in Section\u00a0<ref>), we empirically investigate whether data augmentation methods from NLP (that handles text data) and graph learning (that handles graph data) can effectively enrich the diversity of training data for source code learning. \n\n\n\n\n\n\u00a7 ADAPTING DATA AUGMENTATION METHODS FOR SOURCE CODE LEARNING\n\n\n\nAs introduced in Section\u00a0<ref>, source code can be represented \nin two ways, namely, as sequential data and as structural data. \n\nInspired by these two representations, we investigate the effectiveness of the data augmentation methods from NLP and those from graph learning in source code learning. \nSpecifically, we collect 7 data augmentation methods from NLP and 4 methods from graph learning, as shown in Fig.\u00a0<ref>. Moreover, \n[(i)]\n\n  * for methods from NLP, we make adaptations to them in order to handle source code, and the concrete adaptations are elaborated on in Section\u00a0<ref>;\n\n  * for methods from graph learning, we leverage abstract syntax tree (AST) and different code\nflows (e.g., control flow and data flow) to represent source code as graphs, and then apply the existing methods to achieve data augmentation, as explained in  Section\u00a0<ref>.  \n\n\n\n\n\n\n\n \u00a7.\u00a7 NLP  Data Augmentation Methods for Source Code Learning\n\n\nWe follow recent survey papers\u00a0<cit.> to collect the data augmentation methods for text data, and as a result, 7 methods are applicable to source code learning. These methods can be classified into three categories, namely,  paraphrasing, noising-based methods and sampling-based methods, which are described as follows:\n\n    \n  * Paraphrasing can express the same information as the original form and has been commonly used in NLP\u00a0<cit.>. In this paper, we select the  Back-translation\u00a0<cit.> method. \n    \n  * Noising-based methods slightly add noise to the original data but keep their semantic information\u00a0<cit.>. In this paper, we employ four types of noise injection methods, namely, synonym replacement, random insertion, random swap, and random deletion. \n    \n  * Sampling-based methods generate new synthetic data by linearly mixing the latent embeddings instead of directly operating on the raw text data. Unlike paraphrasing or noising-based methods, sampling-based methods are task-specific and require both data and their label formats\u00a0<cit.>. \n    In this paper, we select two advanced sampling-based data augmentation methods used in NLP, namely WordMixup and SenMixup\u00a0<cit.>. \n    \n\nIn the following, we elaborate on these 7 data augmentation methods and especially highlight the adaptations we have made in order to handle source code data.\n\n\nBack-translation (BT). \n\nThis method translates the original text into another language and then translates it back to the original one to generate additional source code data. \n\n\nIn source code learning, we implement BT by applying the English-French translation model\u00a0<cit.> bidirectionally for each statement in a program.\nFor example, in Fig.\u00a0<ref>, after BT, we replace the statement \u201c\u201d in the original code with \u201c\u201d and \u201c\u201d respectively. Note that these alterations may slightly break the syntax of the code data, nevertheless, they are rather minor, and the original relation between the features and the label is still preserved in the code data. \n\n\n\nSynonym Replacement (SR). In NLP, this method randomly selects n words from a sentence and then replaces the selected words with one of its randomly chosen synonyms. Different from BT, to further enrich the diversity, SR usually refrains from substituting strings that are semantically similar to the original text data. Specifically, in source code learning, we first randomly select n statements from a program. Then, each of the n words is replaced with one of its synonyms that is selected at random. In Fig.\u00a0<ref>, we randomly select one statement from a program and then replace it with another string that is generated from its synonyms chosen at random. Similar to the case of BT, this method also preserves the original relation between the features and the label in the code.\n \nRandom Insertion (RI). In NLP, this method randomly inserts a random synonym of a\nrandom word into a sentence to generate augmented text data. Different from RI used in text data, we first select a random synonym of a random word in the chosen statement, then randomly insert this selected synonym into a random position of this statement. Generally, this process is repeated n times. In Fig.\u00a0<ref>, we randomly insert the string that is generated from synonyms in a random position of the selected statement from the original code, i.e., \u201c\u201d. \nAgain, this method is able to preserve the original relation between the features and the label in the code.\n\nRandom Swap (RS). In NLP, this method randomly chooses two words in a sentence and then swaps their positions. Although the semantics of text data is, in general, sensitive to the order of words, within a limited level of word swapping, the text after RS is often still understandable to humans\u00a0<cit.>. Therefore, RS can be used to produce augmented text data. In source code learning, we randomly select two statements of a program and swap their positions, and this process is usually repeated n times. In Fig.\u00a0<ref>, we randomly select two statement \u201c\u201d and \u201c\u201d, and then swap their positions. \nDespite the minor alteration of the order of the selected statements, this method can still preserve the original relation between the features and the label in the code.\n\n\n\nRandom Deletion (RD). This method randomly removes some words in a sentence or some sentences in a document, with a probability p, to generate augmented text data. In source code learning, we randomly delete words in a randomly chosen statement. In Fig.\u00a0<ref>, we delete words in a statement with probability p=0.01. As a result, the operator \u201c\u201d is removed from the statement \u201c\u201d after RD program transformation. Similarly, this method is able to preserve the original relation between the features and the label in the code.\n\n \n\nIn the following, we introduce two data augmentation methods based on Mixup\u00a0<cit.>, a popular data augmentation approach in computer vision. \n\n\nSpecifically, Mixup synthesizes new image data and their labels by linearly mixing the image features and the labels of two selected images. It has inspired the development of many data augmentation methods in other fields, including WordMixup and SenMixup in NLP, which are introduced as follows.\n\n\nWordMixup and SenMixup. \nOriginally, WordMixup interpolates the samples in the word embedding space, and SenMixup interpolates the hidden states of sentence representations\u00a0<cit.>. We slightly modify WordMixup and SenMixup to adapt to source code learning. As shown in Eq.\u00a0(<ref>), there are two variants of Mixup in our study. The first one, denoted as WordMixup,   interpolates samples in the embedding space of statement representation, and the second one, denoted as SenMixup, conducts the interpolation after a linear transformation and before it is passed to a standard classifier that generates the predictive distribution over different labels. Given two pairs (x^i,y^i) and (x^j,y^j), where x^i and x^j represent the code data, and y^i and y^j are their corresponding labels, \nthe interpolated new data  are obtained via WordMixup and SenMixup, as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    x_\ud835\udc4a\ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc40\ud835\udc56\ud835\udc65^ij = \u03bb x^i + (1 - \u03bb) x^j          \n         x_\ud835\udc46\ud835\udc52\ud835\udc5b\ud835\udc40\ud835\udc56\ud835\udc65^ij = \u03bb f(x^i) + (1 - \u03bb) f(x^j) \n        y_\ud835\udc4a\ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc40\ud835\udc56\ud835\udc65^ij = \u03bb y^i + (1 - \u03bb) y^j          \n        y_\ud835\udc46\ud835\udc52\ud835\udc5b\ud835\udc40\ud835\udc56\ud835\udc65^ij = \u03bb y^i + (1 - \u03bb) y^j\n\nHere SenMixup follows a similar workflow with\u00a0<cit.>, and f(\u00b7) denotes a linear transformation method that is able to ensure that the input and the output have the same dimension. Moreover, x_\ud835\udc4a\ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc40\ud835\udc56\ud835\udc65^ij and x_\ud835\udc46\ud835\udc52\ud835\udc5b\ud835\udc40\ud835\udc56\ud835\udc65^ij represent the new synthetic training data obtained by WordMixup and SenMixup respectively, and y_\ud835\udc4a\ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc40\ud835\udc56\ud835\udc65^ij and y_\ud835\udc46\ud835\udc52\ud835\udc5b\ud835\udc40\ud835\udc56\ud835\udc65^ij are their labels.  The parameter \u03bb denotes the Mixup ratio, and according to\u00a0<cit.>, it is sampled from a Beta distribution with a shape parameter \u03b1(\u03bb\u223cBeta(\u03b1,\u03b1)). \n\n\nTo better understand how the linear interpolation method works, we use an example to show the details of linearly mixing two different programs (Program A and B from Python800 with label 0 and label 1, respectively) as depicted in Fig.\u00a0<ref>. First, we map a pair of programs into the vector space through CodeBERT\u00a0<cit.> and transform their labels into one-hot vectors with 800 classes (see Data A ((X_i, Y_i)) and Data B ((X_j, Y_j)) in Fig.\u00a0<ref>). Next, we linearly mix the code vectors and label vectors, respectively, of Data A and Data B as the augmented training data (X, Y) that could be used to train the model (see Data C in Fig.\u00a0<ref>). \n\n\n\n\n \u00a7.\u00a7 Graph Learning Data Augmentation Methods for Source Code Learning\n\n\nFollowing a recent survey\u00a0<cit.>, we collect four data augmentation methods from graph learning and apply them to source code learning.\nNote that this line of work is applicable to source code because of the structural representation (introduced in Section\u00a0<ref>) of source code, i.e., source code can be transformed to graph structures, such as abstract syntax tree (AST) and different code flows (e.g., control flow, data flow)\u00a0<cit.>, that can capture the relations between different components of the code. Based on the graph structure, we apply the methods from graph learning to augment additional training data for source code learning.\nThe collected data augmentation methods and their applications to source code learning are introduced as follows:\n\n\n\nDropNode.\n As a similar approach to Dropout\u00a0<cit.>, DropNode creates a large number of variants of one original graph by randomly removing a number of graph nodes along with the associated edges. As is well known, the original Dropout, which drops out units in a DNN during training, is a specific regularization approach to preventing DNNs from overfitting.\n \n In a similar way, DropNode is also used to alleviate the overfitting problem in the training of GNNs. \n In our study, for the purpose of data augmentation, we first transform the code data into graphs and then apply   DropNode to produce the variants of the original graphs as the additional training data. \n Note that our node-dropping operation is under the premise that the missing part of nodes should not affect the semantics of the original graph, as suggested by\u00a0<cit.>.\n\nDropEdge. In a similar way  to DropNode, DropEdge stochastically drops graph edges with a fixed probability to create variants of the original graph. DropEdge improves the generalization and alleviates over-smoothing and over-fitting problems of GNNs, especially for deeper GNNs\u00a0<cit.>.\n\nSubgraph. Unlike DropNode and DropEdge, \n\nthis method generates an augmented graph via sampling a connected subgraph\u00a0<cit.> from the original graph using a random walk. Similarly, this method is also required to keep part of the semantic information of the original graph.\n\nManifold-Mixup. As a variant of Mixup, Manifold-Mixup\u00a0<cit.> is designed for graph classification, by interpolating graph-level embedding. Technically, given two pairs (x_i^\ud835\udca2,y_i^\ud835\udca2) and (x_j^\ud835\udca2,y_j^\ud835\udca2) of graph data and their labels, the interpolated graph data pair (x_mix^\ud835\udca2,y_mix^\ud835\udca2) is calculated by: \n\n\n    x_mix^\ud835\udca2 = \u03bbx_i^\ud835\udca2 + (1 - \u03bb) \n    x_j^\ud835\udca2\n    \n     y_mix^\ud835\udca2 = \u03bb y_i^\ud835\udca2 + (1 - \u03bb) y_j^\ud835\udca2\n\n\n\n\n\n\n\n\n \n\n\n\n\u00a7 STUDY DESIGN\n\n \nIn order to assess the effectiveness of the data augmentation methods in Section\u00a0<ref> in source code learning, we design three research questions, as follows:\n\n\n[\u2219]\n\n  * RQ1: Can existing data augmentation methods produce accurate code models?\nAccuracy is the basic metric to evaluate the performance of a trained model, and  therefore, we first assess whether data augmentation methods from NLP and graph learning (as introduced in Section\u00a0<ref>) are able to improve the accuracy of the code models, based on the comparisons with two baseline approaches, namely, the training approach that does not use any data augmentation method, and the training approach that only uses the simple code refactoring method (as introduced in Section\u00a0<ref>). In this RQ, first, we prepare the original training data and randomly initialize code models, and then train these models using different data augmentation methods as listed in Fig.\u00a0<ref>. Specifically, we compare these methods in terms of two metrics, namely, 1) the convergence speed of the model and 2) the final accuracy of the model with fixed training epochs. \n\n\n  * RQ2: Can existing data augmentation methods produce robust code models?\n\nSince robustness\u00a0<cit.> is another important metric that evaluates the generalization ability to handle unseen data of the trained model, we obtain multiple trained code models and study their adversarial robustness in this RQ. According to the literature\u00a0<cit.>, data augmentation is helpful for improving the adversarial robustness of DNN models in other fields (e.g., computer vision).\n\nThus, we also explore whether this conclusion also holds for code models. Concretely, we employ two state-of-the-art \n adversarial attacks, namely, MHM\u00a0<cit.> and ALERT\u00a0<cit.>, to evaluate the robustness improvement of the code models that are trained using data augmentation. \n\n\n  * RQ3: How does data volume affect the effectiveness of data augmentation methods?\n\n\nFinally, since the initial purpose of data augmentation is to solve the problem of the lack of labeled data, it is necessary to further investigate the effectiveness of data augmentation methods in a more practical scenario, namely, the case when there is no sufficient training data. To do so, we reduce the size of the training set of each dataset and repeat the experiments in RQ1 and RQ2, and we then check whether data augmentation methods are still useful. \n\n\n\n\n\u00a7 EXPERIMENTAL SETUP\n\n\n\nOur study considers two mainstream programming languages (Java and Python), four crucial downstream tasks (problem classification, bug detection, authorship attribution, and clone detection), 11 DNN model architectures, and two pre-trained PL models (CodeBERT and GraphCodeBERT). Table\u00a0<ref> shows the details of datasets and models used in the experiments.\n\n\n\n \u00a7.\u00a7 Code Refactoring\n\nIn total, we collect 18 code refactoring methods from the existing literature. Pour et al.\u00a0<cit.> and Allamanis et al.\u00a0<cit.> cover 10 code refactoring operators, e.g. local variable renaming, if loop enhance, and argument adding. Wei et al.\u00a0<cit.> provides eight code refactoring methods, e.g. duplication, dead store, and unreachable loops/branches. In our experiment, for each code data, we randomly select one of these 18 code refactoring methods and apply it to the original code data to generate augmented code data.\n\n\n\n \u00a7.\u00a7 Source Code Learning Tasks and Datasets\n\nWe introduce the tasks and datasets that are often used in source code learning and adopted in our experiments.\n\n\n\n\n  * Problem classification is a typical source code learning task that classifies the target functions of source code. Given a series of problems with detailed descriptions and their corresponding candidate source code, the trained model will identify the problem that the code is trying to solve. Two recently released datasets, Java250 and Python800\u00a0<cit.>, are used in our empirical study for this task. Java250 is built for Java program classification, which has 250 classification problems, including 300 Java programs for each problem. Python800 is a dataset that is specially used for Python program classification tasks, containing 800 different problems with 300 solutions written by the Python program for each. \n\n\n  * Bug detection is to determine whether a piece of code contains bugs. Generally, detecting bugs may be thought of as a binary classification problem. It is challenging to prepare a dataset for bug detection since it necessitates a pair of codes with and without bugs, where the process is ultimately identified by human programmers. The common method to gather such pairs is to automatically crawl versions of code before and after commits from GitHub. However, human effort is required to check if the commit is actually fixing a bug or causing a new bug. Refactory\u00a0<cit.> and CodRep1\u00a0<cit.>, two open datasets designed for bug repair, are used in our study for this task. Specifically, Refactory includes 2,242 correct and 1,783 buggy Python programs which are written by real-world undergraduate students to finish 5 programming assignments. CodRep1 is a program repair dataset for Java, which includes 3,858 program pairs (buggy program and its fixed version) that are from real bug fixes. \n\n\n  * Authorship attribution task involves identifying the writer of a given code fragment by inferring the characteristics of programmers from their published source code, which is crucial for granting credit for a programmer's contribution and is also helpful for detecting plagiarism. We use the dataset from Google Code Jam (GCJ) provided by the work\u00a0<cit.>. \n\n\n  * Clone detection focuses on checking whether two codes are semantically identical or not, which helps prevent bug propagation and makes software maintenance easier. We use the broadly recognized clone detection benchmark dataset BigCloneBench\u00a0<cit.>, written by Java.\n\n\n\n\n \u00a7.\u00a7 Source Code Learning DNN Models\n\nThere are two paradigms for code learning, 1) using task-specific programming language (PL) models and 2) using pre-trained PL models. \n\n\n\n\n\n\n\n\n  * Code learning with task-specific PL models. This is a simple type of code learning where a code model is initialized randomly for a specific task and is trained using a task-related dataset from scratch. Generally, the trained models are lighter than the models using pre-trained PL models (e.g., 103 MB for BagofToken models vs. 487,737 MB for GraphCodeBERT models, as reported in our experiment) and can be deployed in machines with low computation resources. \n\n\n  * Code learning with pre-trained PL models. Different from task-specific models, pre-trained models are trained on a broad set of unlabeled data and can be used for a wide range of downstream tasks with minimal fine-tuning. Due to its large input volume, pre-trained models usually have better accuracy and higher generalization ability\u00a0<cit.>. First, pre-trained PL embedding models are trained using multi-language datasets, e.g., Java, C++, and Python. Then, given a dataset that targets a specific downstream task, such as code clone detection, we fine-tune the pre-trained model accordingly and produce the final model.\n\n\nWe prepare at least 8 types of DNN models for each dataset, including pre-trained PL models and models trained from scratch. \n\n\n  * Models trained from scratch. Seven types of models that need to be trained from scratch are studied, FNN (BagofToken)\u00a0<cit.>, CNN (SeqofToken)\u00a0<cit.>, Graph Convolutional Network (GCN)\u00a0<cit.>, Graph Isomorphism Network (GIN)\u00a0<cit.>, Graph Attention Networks (GATs)\u00a0<cit.>, Gated Graph Sequence Neural Networks (GGNNs)\u00a0<cit.>, and GraphSAGE (SAGE)\u00a0<cit.>. FNN (BagofToken) is a basic mode type that only contains dense layers. CNN (SeqofToken) consists of both dense layers and convolutional layers. GCN is a variant of convolution neural networks that are specially employed to deal with graph-structured data. GIN generalizes the Weisfeiler-Lehman WL test, which maximizes the discriminative capacity of GNNs. In addition, we also use GCN-Virtual and GIN-Virtual. They employ virtual nodes to enhance the aggregation phase, which involves adding an artificial node to each graph and connecting it in both directions to all graph nodes. GAT is one of the popular GNN architectures, which mainly applies the attention mechanism to the process of graph message passing. GGNN is a variant of GNNs, which updates the new hidden state with a Gated Recurrent Unit (GRU). SAGE is a framework to map each node of a graph into the low-dimensional vector space, which is designed for inductive representation learning on large graphs. Especially a code graph is an inter-procedural graph of program instructions where the edges contain information about data flow and control flow. Thus, in the phase of data augmentation on training data, we linearly mix the vector generated by GNNs rather than the actual code.\n\n  * Pre-trained models. Two pre-trained models, CodeBERT\u00a0<cit.> and GraphCodeBERT\u00a0<cit.>, are considered in our study. CodeBERT is a bimodal model trained by using data from multiple programming languages, such as C, C++, Java, and natural languages. It follows the same spirit as BERT and treats programs as sequences during pre-training. In order to consider the semantic-level structure of programs, GraphCodeBERT adds data-flow information to the training data that can produce a more precise code representation.  \n\n\n\n\n \u00a7.\u00a7 Evaluation Metrics\n\n\nIn our experiment, we evaluate the performance of trained DNN models from two perspectives, clean accuracy and model robustness. \n\n\n  * Accuracy is a basic metric that calculates the % of correctly classified data over the entire test data. Here, clean accuracy means the accuracy of models on the original test data. These data generally follow the same data distribution as the training data. As another important metric, robustness reflects the generalization ability of DNN models. The common way to measure the robustness of models is to use adversarial attack methods to attack models and check to what extent the model can defend against these attacks. \n\n  * Attack success rate (ASR) is used as the measurement of robustness, which calculates the % of successfully created adversarial examples. Only correctly predicted test samples are utilized to create adversarial examples while undertaking adversarial attacks. A higher ASR indicates that an attack method has strong performance; in turn, the robustness of the victim model is low. In this study, we adopt two different adversarial attack methods on source code that are widely used for pre-trained PL models, Metropolis-Hastings modifier (MHM) algorithm\u00a0<cit.> and naturalness\naware attack (ALERT)\u00a0<cit.>. Both attacks replace the name of local variables in the program and force the model to produce wrong predictions accordingly. MHM applies the Metropolis-Hastings sampling methodology to get the name of the replacement variable, and ALERT uses the masked language prediction function of pre-trained models to search for substitutes. \n\n\n\n\n \u00a7.\u00a7 Implementation Details\n\n\nThe implementation of all the data augmentation methods is based on pure Python and the Numpy package, which makes it easy to extend this study to support more techniques in the future. Moreover, we also provide a code refactoring generator, including 18 different code refactoring methods that support both Java and Python languages. The models, including BagOfToken and SeqOfToken, are built using TensorFlow2.3 and Keras2.4.3 frameworks. CodeBERT and GraphCodeBERT are built using PyTorch1.6.0. We set the training epoch to 50 for the above four models. All GNN models are implemented with the open-source library Pytorch Geometric (PyG)\u00a0<cit.>. In the phase of the DNN training, the epoch we set for all tasks is 100. For the optimizer, we use Adam <cit.> with the learning rate 10^-3 for all the models. For the Mixup ratio that is set in augmenting training data, \u03b1=0.1 is our default setting. To alleviate overfitting, we adopt early stopping with patience 20. To lessen the impact of randomness, we train each model five times and report the average results with standard deviation. We conduct all graph learning experiments on a server with 2 GPUs of NVIDIA RTX A6000. \n\n\n\n\n\n\u00a7 EVALUATION RESULTS\n\n\n\n\n\n \u00a7.\u00a7 RQ1: Can existing data augmentation methods produce accurate code models?\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, we check the final accuracy of each trained model to explore if these data augmentation methods can improve the performance of models compared to models without using data augmentation and with using code refactoring, respectively. Table\u00a0<ref> presents the test accuracy of BagofToken and SeqofToken models on original test data. First, for BagofToken, SenMixup achieves the best performance in four (out of five) datasets. More specifically, the results show that it outperforms No Aug by up to 4.09%, and on average, 2.27%, and Refactor by up to 6.63%, and on average, 3.82%. Surprisingly, in most cases (four out of five), the code refactoring method can not improve the accuracy of models compared to No Aug. Only in the clone detection task it brings a maximum 0.31% accuracy improvement. For SeqofToken models, again, SenMixup outperforms No Aug in four (out of five) datasets with accuracy improvements by up to 8.74% and 3.61% on average, and Refactor with accuracy improvements by up to 4.53% and 2.52% on average. The results recommend that when using traditional DNN models (e.g., FNN, CNN) to solve code classification tasks, SenMixup is a better choice for training data augmentation. \n\nTable\u00a0<ref> presents the accuracy of GNNs on six datasets. For GCN, Manifold-Mixup shows the best performance in four (out of six) datasets with a maximum of 1.60% accuracy improvement and a 0.94% improvement on average compared to No Aug. Besides, compared to Refactor, Manifold-Mixup also brings an accuracy improvement by up to 1.53% and 0.90% on average. For GAT, GGNN, and SAGE, Manifold-Mixup still has outstanding ability in accuracy improvement since 9 out of 12 cases support this finding, where Manifold-Mixup defeats all other data augmentation methods for GGNN in four datasets. However, for GIN, GCN-Virtual, and GIN-Virtual, DropEdge achieves the best performance in three (out of six) cases, whereas Manifold-Mixup only has one case with the best accuracy improvement. In conclusion, these results suggest that Manifold-Mixup is the first choice for augmenting source code datasets when using GNN models to learn source code features since Manifold-Mixup has the best accuracy improvement in 14 (out of 24) cases compared to other techniques and brings a maximum 1.60% accuracy improvement in our experiments.\n\nTable\u00a0<ref> presents the results of two pre-trained PL models. First of all, we observe that compared to the results of the above two types of models, SenMixup is not sufficient to improve the accuracy of pre-trained models (only a maximum of 0.77% accuracy improvement). By contrast, data augmentation methods that slightly break the syntax of source code, such as RS, are more effective and have a clear improvement (by up to 2.25%) compared to No Aug, and (by up to 3.01%) compared to Refactor. In GraphCodeBERT, where RS achieves the best accuracy improvement in four (out of six) datasets. \n\nThen, we check the convergence speed of models using different data augmentation methods. Fig.\u00a0<ref> and Fig.\u00a0<ref> depict the training logs of SeqofToken, GCN, and GraphCodeBERT models. From the results, we find that 1) models have similar convergence speed regardless of the used data augmentation methods, e.g., for Java250-SeqofToken, after 20 epochs, models have no significant accuracy improvements (except\u00a0BT). This finding reflects that the execution (computation budget) cost of model training is similar regardless of the used data augmentation method. 2) Similar to findings that come from analyzing the final accuracy of models, there are two methods that have clearly better performance than others, SenMixup and RS. This indicates that, with limited computation budgets, these two methods are also recommended for practical use.\n \n\n\n\n\nAnswer to RQ1: Data augmentation methods that linearly mix the code embedding, especially SenMixup, are effective in enhancing model performance with up to 8.74% accuracy improvement compared to the case without using data augmentation and up to 6.63% accuracy improvement compared to Refactor. Surprisingly, the data augmentation method that randomly swaps two statements and slightly breaks the syntax rules achieves the best results in pre-trained PL models with up to 2.25% accuracy improvement.\n\n\n\n\n \u00a7.\u00a7 RQ2: Can existing data augmentation methods produce robust code models?\n\n\n\n\n\nAdversarial robustness reflects how models handle the data with noise, which is an important characteristic that should be evaluated by the models. Table\u00a0<ref> presents the attack success rate of two state-of-the-art attack methods on our trained models. First, the results demonstrate that data augmentation can not always enhance the robustness of models. Compared to the No Aug models, only four out of eight models trained by using data augmentation have higher robustness. This phenomenon is consistent with the conclusion drawn by previous work\u00a0<cit.> that simply increasing the training data is not sufficient for improving the robustness of code models. Also importantly, in some cases, although data augmentation can help train a more robust model, the robustness improvement is insignificant, e.g., the greatest improvement is 5.98% (GraphCodeBERT-Refactor-BigCloneBench-ALERT).\n\nThen, we compare each data augmentation method. In CodeBERT, RS performs the best and has a relatively better robustness improvement in five (out of 12) cases, and reduces ASR by up to 5.10% under MHM attack and 3.69% under ALERT attack compared to No Aug. Besides, the second best one, RD reduces ASR by up to 5.67% in MHM and 2.66% in ALERT compared to No Aug. In GraphCodeBERT, RS still is the best choice for robustness improvement in five (out of 12) cases, which deduces ASR by up to 4.10% in MHM and 3.02% in ALERT compared to No Aug. Interestingly, based on pre-trained PL models, compared to Refactor, methods like RS that could sightly break the syntax of programs can produce more accurate and robust code models to solve downstream tasks. This finding can inspire future research that, when proposing new data augmentation methods, it is unnecessary to follow the program constraint to design the method. Finally, moving to tasks, based on the results, we recommend using RS to augment the training data when applying pre-trained PL models to source code learning because it has the best ability of robustness improvement in three (out of four) tasks.\n\n\n\n\n\nAnswer to RQ2: Data augmentation has limited benefits in improving the adversarial robustness of code models, i.e., there is no method that can always improve the robustness of models across different datasets and models compared to not using data augmentation. Among these methods, RS is the relatively best one that produces the most robust models in 10 (out of 26) cases with a maximum  ASR reduction of 5.67% compared to the case without using data augmentation. However, the syntax-preserved method Refactor only achieves the best in two cases. \n\n\n\n\n\n \u00a7.\u00a7 RQ3: How does data volume affect the effectiveness of data augmentation methods?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData augmentation methods are used to enrich the size and also the diversity of training data, especially in the scenario \u2013 lack of labeled training data. Therefore, it is necessary to explore whether data augmentation methods can be still effective in improving both the accuracy and robustness of DNN models when there is only a small size of training data. In this part, we keep only 10%, 5%, 3%, and 1% of training data (for the authorship attribution task, we keep 10% and 50% of training data since the size of original training data is relatively small, i.e., 528) and repeat the experiments conducted in Section\u00a0<ref> and Section\u00a0<ref>. We choose two mainstream pre-trained PL models for our study. \n\nAccuracy analysis. Table\u00a0<ref>, Table \u00a0<ref>, and the upper part of Table\u00a0<ref> present the results of clean accuracy. For problem classification tasks (Java250 and Python800), we can see that only in one case, CodeBERT (1%), data augmentation can improve the accuracy significantly (by up to 12.92% accuracy improvement). A similar phenomenon also happens in the clone detection task, we can see that only in GraphCodeBERT (5%), one data augmentation method (BT) has a huge positive impact on the performance of the model. In contrast, for bug detection and author attribution tasks, there is a method (SenMixup) that can always significantly improve the accuracy of models with a margin from 0.31% to 12.92%. In conclusion, SenMixup is still the recommended method that has the best results in 20 (out of 36) cases. Interestingly, two noising-based methods, SR and RI, that perform well when using the entire training data fail to produce accurate models after reducing the size of the training data. For example, in CodeBERT (1%), SR- and RI-produced models only have 37.06% and 27.03% accuracy, while the baseline method (No Aug) has 49.62%.\n\nConvergence speed analysis. Then, we check the convergence speed of models using different data augmentation methods. Fig.\u00a0<ref> represents the training logs of CodeBERT (10%) and GraphCodeBERT (10%) models. From the results, we can see that 1) when data augmentation methods are used, the convergence speed of models is easily affected by a drop in the data scale. For instance, compared to the results of model training using the entire dataset, after 10 epochs, GCJ-CodeBERT has more significant accuracy improvement with data augmentation methods, i.e., RS, SenMixup, and WordMixup. Surprisingly, for Refactory-CodeBERT, the model training using SenMixup and WordMixup brings almost 70.00% significant accuracy improvement at 5^th epoch compared to other data augmentation methods as well as No Aug. Besides, we can find all data augmentation methods effectively improve the performance of accuracy in BigCloneBench-CodeBERT compared to No Aug after 20 epochs. This finding reflects that the smaller the training data, the more effective the data augmentation. This can be beneficial for us to select more effective data augmentation methods to reduce the time (computation budget) cost of models when the training data is in a very limited situation. 2) Compared to other data augmentation methods, models training using linear interpolation methods including SenMixup and WordMixup require more epochs to reach convergence, e.g., for GCJ-GraphCodeBERT, most data augmentation methods bring limited accuracy improvement after 30 epochs (except SenMixup). This phenomenon is similar to the findings in\u00a0<cit.>, which reveals that data augmentation methods that strongly increase the complexity of training data require more epochs to converge.\n\nRobustness analysis. The lower part of Table\u00a0<ref>, Table\u00a0<ref>, and Table\u00a0<ref> depict the results of the attack success rate of MHM and ALERT attack on two pre-trained PL models. For the authorship attribution task (GCJ), we can find that although SenMixup is the best data augmentation method that improves the accuracy of pre-trained models, it brings limited improvements in terms of robustness (e.g. 0.55% ASR reduction in GraphCodeBERT (50%) under ALERT attack). In contrast, the data augmentation method that slightly alters the syntactic structure, especially RI, can effectively improve the performance of robustness. For instance, RI reduces ASR by up to 38.47% in CodeBERT (50%) under MHM attack as well as 28.63% in CodeBERT (50%) under ALERT attack compared to No Aug. For program classification tasks (Java250 and Python800), we observe that as the size of data decreases from 10% to 1%, the magnitude of robustness improvement brought by data augmentation methods becomes larger, e.g., a maximum reduction in ASR from 2.19% (CodeBERT (10%) ) to 17.21% (CodeBERT (1%)) under MHM attack. For the clone detection task, interestingly, most data augmentation methods that slightly change the code structure (except RS and RD) do not work on the robustness improvement at smaller data scales (CodeBERT (1%) and GraphCodeBERT (1%)). The bug detection task Refactory also has a similar phenomenon. In conclusion, we recommend SenMixup as the best data augmentation method used for improving the robustness of pre-trained PL models since it has the best results in 36 (out of 72) cases. Besides, the noising-based data augmentation method BT, which fails to produce the best robust models when using the entire training dataset, brings the greatest ASR reduction (i.e., 10.17% in GraphCodeBERT (10%) under ALERT attack) in the experiment using GraphCodeBERT with four different data scales.\n\n\n\n\n\nAnswer to RQ3: The selection of data augmentation methods becomes especially important when training data is scarce. Specifically, SenMixup is the best choice which outperforms No Aug by up to 11.69% in CodeBERT and 4.44% in GraphCodeBERT in terms of clean accuracy. However, the syntax-preserved method Refactor performs surprisingly worse, e.g., in 19 (out of 24) cases, Refactor harms the clean accuracy of models compared to no using data augmentation.  Considering the robustness, RI can produce a model with up to 38.47% higher robustness than models trained without data augmentation.\n\n\n\n\n\u00a7 DISCUSSION\n\n\n\n\n\n\n\n \u00a7.\u00a7 Is It Necessary to Use  Data Augmentation in Source Code Learning?\n\nFirst, the most important question is whether it is necessary to use data augmentation when preparing code models. From our empirical study, the answer is yes. \n\nIn the case of using a suitable data augmentation method, e.g., SenMixup, the trained models have higher accuracy (by up to 12.92%) and robustness (by up to 21.28%) than the models without using data augmentation. However, the results also demonstrate that when using pre-trained PL models, though data augmentation can still improve the performance of the model, the improvement is not significant compared to the case without pre-trained PL embeddings. The reason could be that, essentially, pre-training already plays the role of data augmentation that enhances the whole process of model training, and consequently, other data augmentation techniques become not as useful as they are in the cases without pre-training. An in-depth analysis of this phenomenon will be an interesting future research direction.\n\n\n\n\n \u00a7.\u00a7 Is It Necessary to Keep Syntax Rules in Data Augmentation for Source Code Learning?\n\n\nThe previous research has shown that, for natural language, although the semantics of text data is sensitive to their syntactic change, it could remain readable to humans\u00a0<cit.> and valid as additional training data if the change happens within a limited range that does not break the original relations between the text data and their labels. Indeed, that is why noising-based data augmentation methods, such as RI and RS (see Section\u00a0<ref>), are still very useful in NLP, as shown by the recent study\u00a0<cit.>.\n\nIn the context of source code learning, our experimental results suggest a similar conclusion, namely, even though some data augmentation methods can produce training data that slightly break the syntax of the source code, these data are still useful in improving the quality of training in source code learning. Indeed, as reported by our experiments, the pre-trained PL models using the RS method can achieve higher accuracy (by up to 17.54%) and higher robustness (by up to 14.55%) than the models using the baseline Refactor method. Moreover, this find shows the naturalness of source code and is consistent with the famous software naturalness hypothesis\u00a0<cit.>. \n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Threats to Validity\n\nThe internal threat to validity comes from the implementation of standard training and data augmentation methods. The code of model training for problem classification task (Java250, Python800) is taken from Project CodeNet\u00a0<cit.>, bug detection task (CodRep1, Refactory) adopted from\u00a0<cit.>, authorship attribution task (Google Code Jam) selected from the existing work\u00a0<cit.>, and clone detection task (BigCloneBench) chosen from\u00a0<cit.>. The implementation of SenMixup, WordMixup, and Manifold-Mixup comes from its original release\u00a0<cit.>, especially, we adapt those original mixup-based data augmentation to source code learning. The implementation of EDA and BT comes from\u00a0<cit.>, again we adapt them to code-related tasks. The implementation of Subgraph, DropNode, and DropEdge comes from its original release\u00a0<cit.>. The code refactoring methods for the Java language come from the existing works\u00a0<cit.>, and we adapt the implementation to the Python language. \n\nThe external threats to validity lie in the selected code-related tasks, datasets, DNNs, and data augmentation methods. We consider four different code-learning tasks, including problem classification, bug detection, authorship attribution, and clone detection in our study, a total of six datasets for the above tasks. Especially two popular programming languages in the software community (Java and Python) are included. Remarkably, we apply eleven types of DNN models, including two mainstream pre-trained PL models. For data augmentation methods from code, code refactoring methods cover the most common ones in the literature. Data augmentation methods of NLP and graph learning come from the most classic method, which is comprehensively adopted from the number of citations of papers and the authoritativeness of published journals or conferences.\n\nThe construct threats to validity mainly come from the parameters, randomness, and evaluation measures. Mixup-based data augmentation methods only contain the parameter \u03bb that controls the weight of mixing two feature vectors. We follow the original recommendation of Mixup. The parameters of data augmentation methods from NLP and graph learning also follow the original release. We repeat each experiment five times to reduce the influence of randomness, and the results are reported as an average and standard deviation. For evaluation measures, we consider both accuracy and robustness, and the latter is used to evaluate the generalization ability of DNNs.\n\n\n\n\u00a7 RELATED WORK\n\n\nWe review related work about data augmentation for source code learning and empirical study on source code learning.\n\n\n\n \u00a7.\u00a7 Data Augmentation for Source Code Learning\n\nData augmentation has achieved enormous success in the machine learning field\u00a0<cit.>. Inspired by its success, recently researchers devoted considerable effort to leveraging the data augmentation technique in big code tasks to improve the performance of code models in terms of accuracy and robustness. Adversarial training\u00a0<cit.>, which produces a set of adversarial examples to the training data, has been studied as the data augmentation method in code learning. Zhang et al.\u00a0<cit.> proposed a code data augmentation method that employs the metropolis-Hastings modifier (MHM) algorithm\u00a0<cit.> to improve the capability of deep comment generation models. Mi et al.\u00a0<cit.> generated the additional data from Auxiliary Classifier generative adversarial networks (GANs). Besides, as a program transformation method that is specially designed for code, code refactoring has been used as a mainstream code data augmentation method. Yu et al.\u00a0<cit.> designed program transformation rules for Java and evaluated the effectiveness of using these program transformations as code data augmentation in three big code-related tasks. Allamanis et al.\u00a0<cit.> used four simple code rewrite rules as code data augmentation methods for improving the generalization of the code model. \n \nCompared with the above works, our study is the first one that assesses the effectiveness of three types of data augmentation methods for code learning, namely, the methods for  code data, the methods for text data, and the methods for graph data.\n\n\n\n \u00a7.\u00a7 Empirical Studies on Source Code Learning\n\n\n\nRecently, many works conducted empirical studies to explore the topic of ML4Code.\nChirkova et al.\u00a0<cit.> conducted a thorough empirical study to evaluate the capabilities of using Transformer\u00a0<cit.> to solve three downstream tasks related to code learning, including code completion, function naming, and bug fixing. Siow et al.\u00a0<cit.> conducted an empirical study to evaluate existing program representation techniques. Zhang et al.\u00a0<cit.> empirically analyzed current testing and debugging practices for machine learning programs. They revealed that the interaction with the platform execution environments could easily cause machine learning program failures, moreover, current debugging is insufficient to locate the fault in machine learning code well. This work is useful for programmers to improve the quality of the source code of machine learning. Jebnoun et al.\u00a0<cit.> performed a comparative study to explore the distribution of code smells between machine learning and traditional applications. Yan et al.\u00a0<cit.> conducted a comprehensive empirical study on code search using machine techniques. Their empirical evaluation results revealed that machine learning techniques are more effective for queries on reusing code. More recently, Hu et al.\u00a0<cit.> empirically studied the distribution shift problem of code learning and defined five types of shift for code data. Steenhoek et al\u00a0<cit.> experimentally reproduced nine DNN models and two widely used vulnerability detection datasets to help understand deep learning-based models in source code learning. Mastropaolo et al.\u00a0<cit.> presented a comprehensive empirical study to evaluate the robustness of the code completion approach Github Copilot. Niu et al.\u00a0<cit.> performed a comparative study to analyze recently developed pre-trained PL models to advance the understanding of these pre-trained PL models used in source code learning.\n\nDifferent from the existing empirical studies, our work investigates data augmentation on source code learning that has rarely been studied to date. \n\n\n\n\u00a7 CONCLUSION\n\n\nOur empirical study highlights the importance of data augmentation in code learning and provides insights into the effectiveness of various augmentation methods for different downstream tasks and DNN models. Specifically, linear interpolation methods such as SenMixup and Manifold-Mixup are found to be effective in improving both the accuracy and robustness of most DNNs (except for pre-trained PL models), while methods e.g., random deletion (RD) and random swap (RS) that slightly break the syntax of source code were particularly effective for pre-trained PL models. Especially linear interpolation methods can handle the situation when the data scales are in very limited situation. Moreover, based on our findings from the three research questions, we open research directions for further improving the effectiveness of data augmentation in code-related tasks, with the ultimate goal of enhancing program understanding and accelerating software development.\n\n\n\n\nACM-Reference-Format\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}