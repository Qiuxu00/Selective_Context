{"entry_id": "http://arxiv.org/abs/2303.06681v1", "published": "20230312145422", "title": "Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction", "authors": ["Yiqun Lin", "Zhongjin Luo", "Wei Zhao", "Xiaomeng Li"], "primary_category": "eess.IV", "categories": ["eess.IV", "cs.CV"], "text": "\n\n\n\nDIF-Net\n\n\n\n\n\nY. Lin, Z. Luo, W. Zhao, and X. Li^\u22c6\n\n\nThe Hong Kong University of Science and Technology The Chinese University of Hong Kong, Shenzhen  Beihang University\n\n\nLearning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction\n    \nYiqun Lin1 Zhongjin Luo2 Wei Zhao3 Xiaomeng Li1Corresponding Authors: eexmli@ust.hk\n\n    March 30, 2023\n=========================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSparse-view cone-beam CT (CBCT) reconstruction is an important direction to reduce radiation dose and benefit clinical applications.\n\nPrevious voxel-based generation methods represent the CT as discrete voxels, resulting in high memory requirements and limited spatial resolution due to the use of 3D decoders. In this paper, we formulate the CT volume as a continuous intensity field and develop a novel  to perform high-quality CBCT reconstruction from extremely sparse (\u226410) projection views at an ultrafast speed.\n\nThe intensity field of a CT can be regarded as a continuous function of 3D spatial points. Therefore, the reconstruction can be reformulated as regressing the intensity value of an arbitrary 3D point from given sparse projections.\n\nSpecifically, for a point,  extracts its view-specific features from different 2D projection views. These features are subsequently aggregated by a fusion module for intensity estimation. Notably, thousands of points can be processed in parallel to improve efficiency during training and testing.\n\nIn practice, we collect a knee CBCT dataset to train and evaluate . Extensive experiments show that our approach can reconstruct CBCT with high image quality and high spatial resolution from extremely sparse views within 1.6 seconds, significantly outperforming state-of-the-art methods.\n\nOur code will be available at https://github.com/lyqun/DIF-Net.\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nCone-beam computed tomography (CBCT) is a common 3D imaging technique used to examine the internal structure of an object with high spatial resolution and fast scanning speed\u00a0<cit.>. During CBCT scanning, the scanner rotates around the object and emits cone-shaped beams, obtaining 2D projections in the detection panel to reconstruct 3D volume.\n\nIn recent years, beyond dentistry, CBCT has been widely used to acquire images of the human knee joint for applications such as total knee arthroplasty and postoperative pain management\u00a0<cit.>.\n\nTo maintain image quality, CBCT typically requires hundreds of projections involving high radiation doses from X-rays, which could be a concern in clinical practice.\n\nSparse-view reconstruction is one of the ways to reduce radiation dose by reducing the number of scanning views (10\u00d7 fewer). In this paper, we study a more challenging problem, extremely sparse-view CBCT reconstruction, aiming to reconstruct a high-quality CT volume from fewer than 10 projection views.\n\n\n\nCompared to conventional CT (, parallel beam, fan beam), CBCT reconstructs a 3D volume from 2D projections instead of a 2D slice from 1D projections, as comparison shown in Figure\u00a0<ref>, resulting in a significant increase in spatial dimensionality and computational complexity. Therefore, although sparse-view conventional CT reconstruction\u00a0<cit.> has been developed for many years, these methods cannot be trivially extended to CBCT.\n\nCBCT reconstruction can be divided into dense-view (\u2265100), sparse-view (20\u223c50), extremely sparse-view (\u226410), and single/orthogonal-view reconstructions depending on the number of projection views required. \n\nA typical example of dense-view reconstruction is FDK\u00a0<cit.>, which is a filtered-backprojection (FBP) algorithm that accumulates intensities by backprojecting from 2D views, but requires hundreds of views to avoid streaking artifacts. \n\nTo reduce required projection views, ART\u00a0<cit.> and its extensions (, SART\u00a0<cit.>, VW-ART\u00a0<cit.>) formulate reconstruction as an iterative minimization process, which is useful when projections are limited. Nevertheless, such methods often take a long computational time to converge and cope poorly with extremely sparse projections; see results of SART in Table\u00a0<ref>.\n\nWith the development of deep learning techniques and computing devices, learning-based approaches are proposed for CBCT sparse-view reconstruction. Anish \u00a0<cit.> propose to reconstruct a coarse CT with FDK and use 2D CNNs to denoise each slice. However, the algorithm has not been validated on medical datasets, and the performance is still limited as FDK introduces extensive streaking artifacts with sparse views.\n\nRecently, neural rendering techniques\u00a0<cit.> have been introduced to reconstruct CBCT volume by parameterizing the attenuation coefficient field as an implicit neural representation field (NeRF), but they require a long time for reconstruction and do not perform well with extremely sparse views; see results of NAF in Table\u00a0<ref>. \n\nFor single/orthogonal-view reconstruction, voxel-based approaches\u00a0<cit.> are proposed to build 2D-to-3D generation networks that consist of 2D encoders and 3D decoders with large training parameters, leading to high memory requirements and limited spatial resolution. These methods are special designs with the networks\u00a0<cit.> or patient-specific training data\u00a0<cit.>, which are difficult to extend to general sparse-view reconstruction.\n\nIn this work, our goal is to reconstruct a CBCT of high image quality and high spatial resolution from extremely sparse (\u226410) 2D projections, which is an important yet challenging and unstudied problem in sparse-view CBCT reconstruction. \n\nUnlike previous voxel-based methods that represent the CT as discrete voxels, we formulate the CT volume as a continuous intensity field, which can be regarded as a continuous function g(\u00b7) of 3D spatial points. The property of a point p in this field represents its intensity value v, , v = g(p). Therefore, the reconstruction problem can be reformulated as regressing the intensity value of an arbitrary 3D point from a stack of 2D projections \u2110, , v = g(\u2110, p). \n\nBased on the above formulation, we develop a novel reconstruction framework, namely  (Deep Intensity Field Network). \n\nSpecifically,  first extracts feature maps from K given 2D projections. Given a 3D point, we project the point onto the 2D imaging panel of each view_i by corresponding imaging parameters (distance, angle, ect.) and query its view-specific features from the feature map of view_i. Then, K view-specific features from different views are aggregated by a cross-view fusion module for intensity regression. \n\nBy introducing the continuous intensity field, it becomes possible to train  with a set of sparsely sampled points to reduce memory requirement, and reconstruct the CT volume with any desired resolution during testing.\n\nCompared to NeRF-based methods\u00a0<cit.>,  can reconstruct CT of high resolution in a very short time since only inference is required for a new test sample (no retraining). Furthermore, our method performs much better than NeRF-based methods with extremely limited views due to the prior knowledge learned from training data.\n\nTo summarize, the main contributions of this work include\n\n1.) we are the first to introduce the continuous intensity field for supervised CBCT reconstruction;\n\n2.) we propose a novel reconstruction framework  that reconstructs CBCT with high image quality (PSNR: 29.3 dB, SSIM: 0.92) and high spatial resolution (\u2265256^3) from extremely sparse (\u226410) views within 1.6 seconds; \n\n3.) we conduct extensive experiments to validate the effectiveness of the proposed sparse-view CBCT reconstruction method on a clinical knee CBCT dataset.\n\n\n\n\u00a7 METHOD\n\n\n\n\n \u00a7.\u00a7 Intensity Field\n\n\nWe formulate the CT volume as a continuous intensity field, where the property of a 3D point p \u2208\u211d^3 in this field represents its intensity value v \u2208\u211d. The intensity field can be defined as a continuous function g: \u211d^3 \u2192\u211d, such that v = g(p). Hence, the reconstruction problem can be reformulated as regressing the intensity value of an arbitrary point p in the 3D space from K projections \u2110 = {I_1, I_2, \u2026, I_K}, , v = g(\u2110, p). \n\nBased on the above formulation, we propose a novel reconstruction framework, namely , to perform efficient sparse-view CBCT reconstruction, as the overview shown in Figrue\u00a0<ref>.\n\n\n\n\n\n\n\n \u00a7.\u00a7 : Deep Intensity Field Network\n\n first extracts feature maps {F_1, F_2, \u2026, F_K}\u2282\u211d^C\u00d7 H \u00d7 W from projections \u2110 using a shared 2D encoder, where C is the number of feature channels and H/W are height/width. In practice, we choose U-Net\u00a0<cit.> as the 2D encoder because of its good feature extraction ability and popular applications in medical image analysis\u00a0<cit.>.\n\nThen, given a 3D point,  gathers its view-specific features queried from feature maps of different views for intensity regression.\n\n\n View-specific feature querying.\nConsidering a point p \u2208\u211d^3 in the 3D space, for a projection view_i with scanning angle \u03b1_i and other imaging parameters \u03b2 (distance, spacing, etc.), we project p to the 2D imaging panel of view_i and obtain its 2D projection coordinates p'_i = \u03c6(p, \u03b1_i, \u03b2) \u2208\u211d^2, where \u03c6(\u00b7) is the projection function. Projection coordinates p'_i are used for querying view-specific features f_i \u2208\u211d^C from the 2D feature map F_i of view_i:\n\n\n    f_i = \u03c0(F_i, p'_i) = \u03c0(F_i, \u03c6(p, \u03b1_i, \u03b2)),\n\n\nwhere \u03c0(\u00b7) is bilinar interpolation. Similar to perspective projection, the CBCT projection function \u03c6(\u00b7) can be formulated as\n\n\n    \u03c6(p, \u03b1_i, \u03b2) = H(A(\u03b2)R(\u03b1_i)\n    [ p; 1 ]),\n\n\nwhere R(\u03b1_i) \u2208\u211d^4\u00d74 is a rotation matrix that transforms point p from the world coordinate system to the scanner coordinate system of view_i, A(\u03b2) \u2208\u211d^3\u00d74 is a projection matrix that projects the point onto the 2D imaging panel of view_i, and H: \u211d^3 \u2192\u211d^2 is the homogeneous division that maps the homogeneous coordinates of p'_i to its Cartesian coordinates. Due to page limitations, the detailed formulation of \u03c6(\u00b7) is given in the supplementary material. \n\n\n\n Cross-view feature fusion & intensity regression.\nGiven K projection views, K view-specific features of the point p are queried from different views to form a feature list F(p) = {f_1, f_2, \u2026, f_K}\u2282\u211d^C. Then, the cross-view feature fusion \u03b4(\u00b7) is introduced to gather features from F(p) and generate a 1D vector f\u0305 = \u03b4(F(p)) \u2208\u211d^C to represent the semantic features of p. In general, F(p) is an unordered feature set, which means that \u03b4(\u00b7) should be a set function and can be implemented with a pooling layer (, max/avg pooling). In our experiments, the projection angles of the training and test samples are the same, uniformly sampled from 0^\u2218 to 180^\u2218 (half rotation). Therefore, F(p) can be regarded as an ordered list (K\u00d7 C tensor), and \u03b4(\u00b7) can be implemented by a 2-layer MLP (K\u2192\u230aK/2\u230b\u2192 1) for feature aggregation. We will compare different implementations of \u03b4(\u00b7) in the ablation study. Finally, a 4-layer MLP (C\u2192 2C \u2192\u230aC/2\u230b\u2192\u230aC/8\u230b\u2192 1) is applied to f\u0305 for the regression of intensity value v\u2208\u211d.\n\n\n\n\n\n\n \u00a7.\u00a7 Network Training\n\n\nAssume that the shape and spacing of the original CT volume are H\u00d7 W \u00d7 D and  (s_h, s_w, s_d) mm, respectively. During training, different from previous voxel-based methods that regard the entire 3D CT image as the supervision target, we randomly sample a set of N points {p_1, p_2, \u2026, p_N} with coordinates ranging from (0, 0, 0) to (s_hH, s_wW, s_dD) in the world coordinate system (unit: mm) as the input. Then  will estimate their intensity values \ud835\udcb1 = {v_1, v_2, \u2026, v_N} from given projections \u2110. For supervision, ground-truth intensity values \ud835\udcb1\u0302 = {v\u0302_1, v\u0302_2, \u2026, v\u0302_N} can be obtained from the ground-truth CT image based on the coordinates of points by trilinear interpolation. We choose mean-square-error (MSE) as the objective function, and the training loss can be formulated as\n\n\n    \u2112(\ud835\udcb1, \ud835\udcb1\u0302) = 1/N\u2211_i=1^N (v_i - v\u0302_i)^2.\n\n\nBecause background points (62%, , air) occupy more space than foreground points (38%, , bones, organs), uniform sampling will bring imbalanced prediction of intensities. We set an intensity threshold 10^-5 to identify foreground and background areas by binary classification and sample N/2 points from each area for training.\n\n\n\n\n \u00a7.\u00a7 Volume Reconstruction\n\n\nDuring inference, a regular and dense point set to cover all CT voxels is sampled, , to uniformly sample H\u00d7 W \u00d7 D points from (0, 0, 0) to (s_hH, s_wW, s_dD). Then the network will take 2D projections and points as the input and generate intensity values of sampled points to form the target CT volume. \n\nUnlike previous voxel-based methods that are limited to generating fixed-resolution CT volumes, our method enables scalable output resolutions by introducing the representation of continuous intensity field. \n\nFor example, we can uniformly sample \u230aH/s\u230b\u00d7\u230aW/s\u230b\u00d7\u230aD/s\u230b points to generate a coarse CT image but with a faster reconstruction speed, or sample \u230a sH\u230b\u00d7\u230a sW\u230b\u00d7\u230a sD\u230b points to generate a CT image with higher resolution, where s > 1 is the scaling ratio.\n\n\n\n\u00a7 EXPERIMENTS\n\n\nWe conduct extensive experiments on a collected knee CBCT dataset to show the effectiveness of our proposed method on sparse-view CBCT reconstruction. Compared to previous works, our  can reconstruct a CT volume with high image quality and high spatial resolution from extremely sparse (\u2264 10) projections at an ultrafast speed.\n\n\n\n \u00a7.\u00a7 Experimental Settings\n\n\n Dataset and preprocessing. \nWe collect a knee CBCT dataset consisting of 614 CT scans. Of these, 464 are used for training, 50 for validation, and 100 for testing. We resample, interpolate, and crop (or pad) CT scans to have isotropic voxel spacing of (0.8, 0.8, 0.8) mm and shape of 256\u00d7 256 \u00d7 256. 2D projections are generated by digitally reconstructed radiographs (DRRs) at a resolution of 256\u00d7 256. Projection angles are uniformly selected in the range of 180^\u2218.\n\n\n Implementation. \nWe implement  using PyTorch with a single NVIDIA RTX 3090 GPU. The network parameters are optimized using stochastic gradient descent (SGD) with a momentum of 0.98 and an initial learning rate of 0.01. The learning rate is decreased by a factor of 0.001^1/400\u2248 0.9829 per epoch, and we train the model for 400 epochs with a batch size of 4. For each CT scan, N = 10,000 points are sampled as the input during one training iteration. For the full model, we employ U-Net\u00a0<cit.> with C = 128 output feature channels as the 2D encoder, and cross-view feature fusion is implemented with MLP.\n\n\n Baseline methods. We choose four publicly available methods as our baselines, including traditional methods FDK\u00a0<cit.> and SART\u00a0<cit.>, voxel-based method PatRecon\u00a0<cit.>, and NeRF-based method NAF\u00a0<cit.>. In addition to PatRecon, we do not compare other voxel-based methods such as X2CT-GAN\u00a0<cit.> and MFCT-GAN\u00a0<cit.> because their networks are specifically designed for reconstruction from 2-view X-rays and cannot be extended to multi-view reconstruction due to extremely high memory requirements.\n\n\n Evaluation metrics. \nWe follow previous works\u00a0<cit.> to evaluate the reconstructed CT volumes with two quantitative metrics, namely peak signal-to-noise ratio (PSNR) and structural similarity (SSIM)\u00a0<cit.>. Higher PSNR/SSIM values represent superior reconstruction quality.\n\n\n\n \u00a7.\u00a7 Results\n \nPerformance. As shown in Table\u00a0<ref>, we compare  with four previous methods\u00a0<cit.> under the setting of reconstruction with different output resolutions (, 128^3, 256^3) and from different numbers of projection views (, 6, 8, and 10). Experiments show that our proposed  can reconstruct CBCT with high image quality even using only 6 projection views, which significantly outperforms previous works in terms of PSNR and SSIM values. More importantly,  can be directly applied to reconstruct CT images with different output resolutions without the need for model retraining or modification. \n\nAs visual results shown in Figure\u00a0<ref>, PatRecon\u00a0<cit.> produces meaningless results because it is designed for patient-specific reconstruction and should be trained on several CT scans of the same patient, which may not be appropriate for general CT datasets; FDK\u00a0<cit.> produces results with many streaking artifacts due to lack of sufficient projection views; SART\u00a0<cit.> and NAF\u00a0<cit.> produce results with good shape contours but lack detailed internal information; our proposed  can reconstruct high-quality CT with better shape contour, clearer internal information, and fewer artifacts. More visual comparisons of the number of input views are given in the supplementary material.\n\n\n\n\n\n\n\n\n Reconstruction efficiency. As shown in Table\u00a0<ref>, FDK\u00a0<cit.> requires the least time for reconstruction, but has the worst image quality; SART\u00a0<cit.> and NAF\u00a0<cit.> require a lot of time for optimization or training; PatRecon\u00a0<cit.> has a large number of model parameters and requires high computational memory due to its voxel-based design. Our  can reconstruct high-quality CT within 1.6 seconds, much faster than previous methods. In addition, , which benefits from the intensity field representation, has fewer training parameters and requires less computational memory, enabling high-resolution reconstruction.\n\n\n Ablation study. Table\u00a0<ref> and <ref> show the ablative analysis of cross-view fusion strategy and the number of training points N. Experiments demonstrate that \n\n1.) MLP performs best, but max pooling is also effective and would be a general solution when the view angles are not consistent across training/test data, as discussed in Section\u00a0<ref>; \n\n2.) fewer points (, 5,000) may destabilize the loss and gradient during training, leading to performance degradation; 10,000 points are enough to achieve the best performance, and training with 10,000 points is much sparser than voxel-based methods that train with the entire CT volume (, 256^3 or 128^3). \n\nWe have tried to use a different encoder like pre-trained ResNet18\u00a0<cit.> with more model parameters than U-Net\u00a0<cit.>. However, ResNet18 does not bring any improvement (PSNR/SSIM: 29.2/0.92), which means that U-Net is powerful enough for feature extraction in this task.\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nIn this work, we formulate the CT volume as a continuous intensity field and present a novel  for ultrafast CBCT reconstruction from extremely sparse (\u226410) projection views.\n\n aims to estimate the intensity value of an arbitrary point in 3D space from input projections, which means 3D CNNs are not required for feature decoding, thereby reducing memory requirement and computational cost. \n\nExperiments show that  can perform efficient and high-quality CT reconstruction, significantly outperforming previous state-of-the-art methods.\n\nMore importantly,  is a general sparse-view reconstruction framework, which can be trained on a large-scale dataset containing various body parts with different projection views and imaging parameters to achieve better generalization ability. This will be left as our future work.\n\n\n\n*\nsplncs04\n\n\n"}