{"entry_id": "http://arxiv.org/abs/2303.07030v1", "published": "20230313114548", "title": "$\\nabla$SD: Differentiable Programming for Sparse Tensors", "authors": ["Amir Shaikhha", "Mathieu Huot", "Shideh Hashemian"], "primary_category": "cs.PL", "categories": ["cs.PL", "cs.LG", "cs.MS"], "text": "\n\n\n\n\n\n\n\n\n\n\n\namir.shaikhha@ed.ac.uk\n\n  University of Edinburgh            \n  United Kingdom                    \n\n\nmathieu.huot@stx.ox.ac.uk\n\n  University of Oxford            \n  United Kingdom                    \n\n\nshideh.hashemian@aut.ac.ir\n\n  Amirkabir University of Technology            \n  Iran                    \n\n\n\n\n\n\n\n\n\n\n\n\n\nSparse tensors are prevalent in many data-intensive applications, yet existing differentiable programming frameworks are tailored towards dense tensors. \nThis presents a significant challenge for efficiently computing gradients through sparse tensor operations, as their irregular sparsity patterns can result in substantial memory and computational overheads. \nIn this work, we introduce a novel framework that enables the efficient and automatic differentiation of sparse tensors, addressing this fundamental issue. Our experiments demonstrate the effectiveness of the proposed framework in terms of performance and scalability, outperforming state-of-the-art frameworks across a range of synthetic and real-world datasets. Our approach offers a promising direction for enabling efficient and scalable differentiable programming with sparse tensors, which has significant implications for numerous applications in machine learning, natural language processing, and scientific computing.\n\n\n\n\n: Differentiable Programming for Sparse Tensors\n    Shideh Hashemian\n    March 30, 2023\n===============================================\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nSparse tensors are essential in many scientific and engineering applications, such as natural language processing, computer vision, and graph analytics. Unlike dense tensors, which store all of their elements regardless of their value, sparse tensors only store non-zero values, resulting in significant memory savings and computational efficiency. Sparse tensors also enable efficient representation and manipulation of high-dimensional data structures, which are often encountered in modern machine learning and scientific computing, such as sparse tensors representing the frequency of words in a document or corpus in natural language processing,  adjacency matrices of large and sparse graphs in network/relational analysis, or sparse user-item interaction matrices for collaborative filtering in recommender systems.\nThis has inspired recent interest in developing better support for sparse tensors\u00a0<cit.>.\n\nAutomatic differentiation (AD) is a fundamental technique in modern machine learning and scientific computing that enables efficient computation of the gradient of a function. This is crucial for optimization, parameter estimation, and many other applications in which gradient-based optimization methods are employed. While AD tools for dense tensors are abundant and well-established, the lack of efficient AD tools for sparse tensors hinders the wider adoption of these techniques and represents a major research challenge in this field. Libraries such as TensorFlow, PyTorch, and JAX provide efficient and scalable implementations of gradient computation for dense tensor operations, but their support for sparse operations is limited\u00a0<cit.>. As a result, there have been various efforts to manually provide differentiation for particular sparse tensor kernels\u00a0<cit.>.\n\nAD for sparse tensor algebra is more challenging than for dense tensor algebra for several reasons. Firstly, the structure of sparse tensors is more complex than that of dense tensors, and their sparsity patterns are often irregular and vary across different operations. This can make it challenging to efficiently propagate gradients through the computation graph and to identify which elements of the sparse tensor are relevant for the gradient computation. Secondly, sparse tensor operations often require the use of specialized data structures and algorithms, such as compressed sparse row (CSR) or compressed sparse column (CSC) formats, which are not natively supported by most AD-enabled frameworks or have incomplete AD rules.\n\nThis paper presents , a novel differentiable programming framework that supports the automatic differentiation of arbitrary sparse computations. To the best of our knowledge,  is the first framework that provides this capability.\nAs opposed to the existing frameworks that offer AD support for a limited number of sparse kernels\u00a0<cit.>,  allows the AD of an arbitrary sparse computation expressible in tensor algebra. \n\nThe key insight is to perform differentiation over a logical representation of a sparse tensor. \nThis means that there is a clear separation of concerns between the semantics of differentiation over a program on the one hand, and optimizations and data layout representations on the other (cf. Figure\u00a0<ref>). To see the simple example of the dot product of two vectors, see Example 1 (cont.) for its logical form before differentiation in <ref>, after differentiation in <ref>, its physical coordinate format (COO) representation in <ref>, and in <ref> for the generated code.\n\nThe physical representation of sparse tensors (e.g., COO/CSR/CSC) involves multiple arrays storing a compressed representation of the matrix (cf. Figure\u00a0<ref>). The computations over such representations involve imperative nested loops over these arrays. However, our logical representation uses a nested dictionary, where sparse computations are expressed functionally as nested summations over them. The logical representation can be later fused with a concrete physical storage format (cf. Section\u00a0<ref>).\n\nIn more detail, the contributions of this paper are as follows:\n\n\n\n\n\n\n\n    \n  * We present , the first framework with systematic support for the automatic differentiation of sparse tensors.  is based on \u00a0<cit.>, a recently introduced intermediate language that can express sparse tensor workloads by separating the tensor computations from the storage specifications (Section\u00a0<ref>). \n    \n  * We introduce a novel tensorized forward-mode AD that computes the gradients in a batch (Section\u00a0<ref>). Our automatic differentiation transform is over the logical part of the language, which we call Logical , without worrying about the physical storage formats. \n    \n  * The differentiated program is then optimized by leveraging the following AD-agnostic transformations: (1) sparsity-aware rewrite rules, (2) composing with the physical storage formats, and (3) algebraic rewrite rules applied in a cost-based manner using equality saturation\u00a0<cit.> (Section\u00a0<ref>). In addition,  performs low-level transformations for removing unnecessary intermediate tensors appearing in nested loops before generating low-level C++ code.\n    \n  * We show the correctness of our approach (Section\u00a0<ref>). That is, AD is a well-typed transformation, computes derivatives  of programs, and our optimizations are sound with respect to our denotational semantics.\n    \n  * We experimentally validate the effectiveness of  in comparison with the state-of-the-art frameworks (Section\u00a0<ref>). We demonstrate that  scales the gradient computation to large matrices with many zero elements over both real-world and synthetic datasets.\n\n\n\n\n\u00a7 BACKGROUND\n\n\n\n\n \u00a7.\u00a7 Automatic Differentiation\n\n\n\nAutomatic differentiation (AD) is a powerful and widely used technique in machine learning and scientific computing that enables efficient computation of the gradient of a function. The gradient is a crucial quantity in many optimization, parameter estimation, and machine learning algorithms, and its computation is often a bottleneck in the training process. AD provides a computationally efficient and accurate way of computing the gradient by breaking down a function into a series of elementary operations and applying the chain rule to compute the derivatives of each operation. The result is an exact gradient that is computed with a similar computational cost as the original function, with no need for approximate methods such as finite differences, or manual derivation.\n\nForward Mode\nOne method of computing the gradient of a function is the forward-mode AD which involves computing the derivatives of each operation in the forward direction through the computational graph. The method starts with the input variables and propagates the values and their derivatives through the graph, one operation at a time, until the output variable is reached. At each operation, the derivative of the output variable with respect to each input variable is computed using the chain rule, and these derivatives are stored in a computational graph that can be used to compute the gradient of the function.\n\nExample 1 - Vector Dot Product Consider the following function:\n\n    f([x_1, x_2], [y_1, y_2]) = x_1y_1 + x_2y_2\n\n\nwhich takes two pairs of input variables x_1, x_2 and y_1, y_2 and computes the dot product of the vectors x = [x_1, x_2] and y = [y_1, y_2]. To compute the gradient of f with respect to the inputs using forward-mode AD, we start by converting the program into ANF\u00a0<cit.>:\n\n\nf([x_1, x_2], [y_1, y_2])= \n     t_1 = x_1y_1 \n\n     t_2 = x_2y_2 \n\n     t_3 = t_1 + t_2 \n\n    t_3 \n\n\n\nThe forward-mode AD lifts every variable to a dual number by associating a tangent variable v' to each input and intermediate variable v. Then, each intermediate tangent variable is computed by following the chain rule. In the previous example, the function f is transformed as follows:\n\n\nf'([x_1, x_2], [y_1, y_2], [x_1', x_2'], [y_1', y_2'])=  \n      t_1 = x_1y_1 \n\n     t_1' = x_1'y_1 + x_1y_1' \n\n     t_2 = x_2y_2 \n\n     t_2' = x_2'y_2 + x_2y_2' \n\n     t_3 = t_1 + t_2 \n\n     t_3' = t_1' + t_2' \n\n    t_3' \n\n\n\nTo compute the partial derivative of f with respect to each input we need to set the corresponding tangent variable to 1 and the other input tangent variables to 0. For example, the gradient of f with respect to the first vector is computed by the following partial derivative computations:\n\n\nf'([a_1,a_2],[b_1,b_2],[1,0],[0,0]) \u2192^*     b_1 =     \u2202 f/\u2202 x_1(a_1,a_2,b_1,b_2) \n\nf'([a_1,a_2],[b_1,b_2],[0,1],[0,0]) \u2192^*     b_2 =     \u2202 f/\u2202 x_2(a_1,a_2,b_1,b_2) \n\n\n\nReverse-mode AD\nForward mode AD is computationally expensive for the derivative computation of scalar-valued functions with tensor inputs, which among other use cases appear in training machine learning models by optimizing an objective function. This is due to the fact that when differentiating a program representing a function \u211d^n\u2192\u211d, as is often the case in these contexts, one needs n runs of the program transformed by forward-mode to obtain the whole gradient.\nThe reverse-mode technique, which computes the gradient of such functions in one run, is then more appropriate, and is massively used in deep learning frameworks\u00a0<cit.>. There has been recent interest on bridging the gap between theoretical correctness guarantees and more practical, efficient implementations with complexity guarantees <cit.>.\n\nExample 1 (cont.) Consider a generalization of the previous function, where \u00b7 denotes the dot product of two vectors:\n \n    f(V_1, V_2) = V_1 \u00b7 V_2\n\n\nIf each input vector has m elements, then the cost of forward-mode AD is O(m^2) as it requires m forward passes, each costing O(m). However, reverse-mode AD can compute the gradient by one forward pass to compute the primal values and one reverse pass to compute the gradient values, resulting in an O(m) overall complexity.  \n\nVector Forward-mode AD\nInspired by the use cases that require the computation of the full Jacobian matrix\u00a0<cit.>, there have been efforts on batch computations of forward-mode AD\u00a0<cit.>. It has been experimentally demonstrated that by leveraging rewrite rules, one can even recover the asymptotic performance of reverse-mode AD on vectorized forward-mode AD\u00a0<cit.>. \n\n\n\n\n\n \u00a7.\u00a7 Sparse tensors and semi-ring dictionaries\n\n\n\nSparse Tensors Sparse tensors are a type of data structure that is commonly used to represent high-dimensional data that have a majority of zero values. Sparse tensors have a compact representation that only stores the non-zero values and their corresponding indices, which makes them more memory-efficient than dense tensors for large-scale data. Sparse tensors are used in many domains, including natural language processing, computer vision, and scientific computing. For example, in natural language processing, sparse tensors can be used to represent text data as a bag-of-words or term frequency-inverse document frequency (TF-IDF) matrix, where the rows correspond to documents and the columns correspond to words (Figure\u00a0<ref>).\n\nSparse tensors can be manipulated using a variety of specialized algorithms and data structures, such as compressed sparse row (CSR) and compressed sparse column (CSC) formats, which enable efficient matrix-vector multiplication and other operations. However, the irregular sparsity patterns of sparse tensors pose significant challenges for automatic differentiation.\n\nExample 1 (cont.)\nIn the previous example, if the majority of the elements of the input vectors of sizxe m are zeros (the number of non-zero elements, denoted by nnz, is such that nnz<<m), one can use the CSR representation shown in Figure\u00a0<ref>. In this representation, the array !pos! is a compressed representation of rows, whereas !idx! and !val! show the columns and values of non-zero elements. For example, !pos(0)=0,pos(1)=7! depicts that the indices 0 to 6 of !idx!/!val! correspond to the column/value of the elements in the row=0 of the matrix, and 7 to 14 for the positions of the row=1. However, existing linear algebra frameworks do not support gradients over this representation. Thus, rather than computing the gradient in O(nnz), they compute it over the dense representation in O(m).\n\n\n\nSemi-ring Dictionaries Semi-ring dictionaries are data structures that subsume sets, multisets, and dense/sparse tensors\u00a0<cit.>. A semi-ring is a set with two binary operations that satisfy certain axioms, such as associativity, distributivity, and commutativity. For example, the set of non-negative integers with addition and multiplication forms a semi-ring, and the set of Booleans with logical \u2228 and \u2227 forms another semi-ring. Semi-ring dictionaries are designed to represent sparse tensors as key-value pairs. In sparse vectors, the keys correspond to the vector indices and the values correspond to the non-zero elements. In sparse matrices, the keys correspond to the row indices and the values correspond to the sparse vector associated with that row. The semi-ring operations are then defined in terms of the corresponding operations on the values, such as addition or multiplication. The multiplication operator for semi-ring dictionaries has a semantics of tensor outer product, as can be observed in the next example.\n\nExample 2 - Scalar-Vector Product Consider the scalar-vector product between a scalar value !s! and a vector value !V! represented using a semi-ring dictionary. The equivalent semi-ring dictionary representation is !s * V!, where !*! has the semantics of tensor outer product.\n\n SDQL\u00a0<cit.> is a functional language for querying against semi-ring dictionaries. SDQL is expressive enough to capture database queries and linear algebra expressions; this makes it appropriate as an intermediate language for hybrid database and machine learning workloads. SDQL provides the following constructs for manipulating semi-ring dictionaries: \n \n    \n  * !dict(k)! accesses the value associated with the key !k! in !dict!, and if the key does not exist, it returns the addition identity of semi-ring (0 in the case of real and natural numbers).\n    \n  * !k -> v! constructs a dictionary with a single key-value pair of !k! and !v!.\n    \n  * !sum(<k,v> in dict) f(k,v)! folds over each key-value pair of !dict! and computes the summation of !f(k,v)! starting from the addition identity of the corresponding semi-ring.\n\n\nExample 1 (cont.) The equivalent expression for V1 \u00b7 V2 can be one of the following two:\n\n\n\nsum(<i, a> in V1) a * V2(i)\n     \n\nsum(<i, a> in V2) V1(i) * a\n\n\n\nThe preferred choice depends on the number of non-zero elements of !V1! and !V2!. If !V1! (resp. !V2!) has fewer non-zero elements, the left (resp. right) variant is more efficient. Otherwise, if both have the same number of non-zero elements (e.g., both are dense), both variants have the same performance.\n\n\n\n \u00a0<cit.> is a dialect of SDQL tailored for sparse tensor processing; it restricts SDQL to the types required for sparse tensors while extending it with constructs required for different sparse storage formats (e.g., CSR, CSC). The following constructs are central to :\n\n \n    \n  * !(st:en)! specifies a dense array holding the range of numbers from !st! to !en! (excluding).\n    \n  * !arr(st:en)! specifies the sub-array of !arr! ranging from !st! to !en! (excluding).\n    \n  * Additional annotations for guiding rewrite rules such\u00a0<cit.>.\n\n\n\n\n\n\n\n\n\n\u00a7 LANGUAGES\n\n\n\nIn this section, we give an overview of the languages used in .\nWe divide  in two. The smaller fragment, Logical , on which AD will be performed, and the full fragment, Physical , which augments the logical subset of the language with the constructs for expressing the different sparse storage formats. The grammar and most important typing rules of these languages are shown in Figure\u00a0<ref>.\n\n\n\n\nLogical  Initially, the program is expressed in a subset of  that is sufficient for expressing tensor programs at the logical level, i.e., without worrying about the storage format. \nThus, at this stage, we do not require the support for dense arrays. Furthermore, there is no need for expressing tuples. Nevertheless, for convenience, we use tupled let-binding as a syntactic sugar for multiple let bindings.\n\nLogical  is expressive enough to capture Einstein summations\u00a0<cit.>. For example, the matrix-matrix product for two matrices !M1! and !M2! (both represented as nested dictionaries) is expressed as:\n\n\nsum(<i,row> in M1)  i ->\n  sum(<j, v1> in row)\n    sum(<k, v2> in M2(j))  k ->\n      v1 * v2  \n\n\nHowever, the expressiveness goes beyond Einstein summations. As an example, !map! of function !f! over the values of a tensor !e! is expressed as !sum(<k,v> in e)  k -> f(v) !. \n\n\n\n\n\n\n\nPhysical  The storage specifications require dense-array-related constructs such as range (!st:end!) and subarray (!arr(st:end)!). Thus, after combining the differentiated program with the storage specification, we need to include these constructs for the Physical . We go back to this intermediate language in Section\u00a0<ref>. The next section focuses primarily on Logical  and the differentiation rules over its constructs.\n\n\n\n\n\n\n\n\u00a7 DIFFERENTIATION\n\n\nIn this section, we present the differentiation transformations applied to Logical  expressions. First, for exposition purposes, we present a variant of traditional forward-mode AD (FAD).\nThen, we show a tensorized FAD that not only subsumes the traditional FAD, but also computes gradients more efficiently.\nFinally, we show the high-level API exposed to the programmers.\n\n\n\n \u00a7.\u00a7 Scalar Forward-Mode Transformation\n\nTraditional FAD uses dual numbers to compute the tangent (derivative) component along with the actual (original) computation. We refer to it as scalar FAD because for each scalar expression, it stores a scalar tangent component.\n\nSimilar to other FAD frameworks,  precedes the differentiation transformation with an ANF conversion\u00a0<cit.>.\nThis allows for sharing sub-expressions and avoids duplication of computations for non-unary constructs such as multiplication.\nLogical  does not allow function definitions nor higher-order functions; all functions need to be inlined\u00a0<cit.>.\n\nScalar Constructs Figure\u00a0<ref> shows the forward-mode transformation rules.\nAs opposed to existing functional AD systems,  does not use explicit pair construction and projection for dealing with dual numbers.\nInstead, the  construct only computes the tangent part of differentiation and refers to the expressions in the ANF transformed program for primal components (cf. the rule for let binding).\nThis avoids the need to extend the target language of differentiation with pairing constructs. Furthermore, this makes the differentiation rules simpler. For every unary real operation !op!, we assume that the language has a unary real operation !op'! representing its derivative.\nFinally, the differentiation for all discrete types (!int! and !bool!) is !0!.\n\n\n\nExample 1 (cont.) Consider again the case of the dot-product of two unrolled vectors of size two initially used in Section\u00a0<ref>. Applying differentiation over the ANF transformed program in  is as follows:\n\n\n\n\u2131\n   \n\nlet t1 = x1*y1 in\nlet t2 = x2*y2 in\nlet t3 = t1+t2 in\nt3\n    \n\n\n\n\nAfter applying the rules in Figure\u00a0<ref>, we obtain the following program:\n\n\nlet <t1, t1'> = <x1*y1, x1*y1' + x1'*y1> in\nlet <t2, t2'> = <x2*y2, x2*y2' + x2'*y2> in\nlet <t3, t3'> = <t1+t2, t1'+t2'> in\nt3'\n\n\nNote that the let-binding constructs are syntactic sugar; there is no pair created (cf. Figure\u00a0<ref>).\n\nTensor Constructs By choosing not to incorporate pairs in the language, we have eliminated the option of differentiating vectors as vectors of pairs (i.e., arrays of structs). Instead, an expression of type !tensor n! is differentiated as an expression with the same type. One of the interesting tensor-based differentiation rules is our rule for summation, where we need to access the corresponding element from the differentiated range, as we can observe in the following example.\n\nExample 1 (cont.) Let us go back to the dot-product for two vectors !V1! and !V2!. The differentiation transformation is expressed as follows:\n\n\n    \n\nsum(<i,a> in V2) V1(i) * a\n    \n\n\n\nApplying differentiation rules results in the following program:\n\n\nsum(<i,a> in V2) \n  let <i',a'> = <0,V2'(i)>\n  V1(i) * a' + V1'(i) * a\n\n\nIn order to compute the gradient of this function with respect to one of its vector inputs, say !V1!, we need to repeatedly set !V1'! into a one-hot vector that is !1! at index !j! and !0! everywhere else, and set !V2'! to be the zero vector.\nThis requires multiple rounds of running the forward-mode AD for different one-hot vectors, which is computationally expensive. Previous research\u00a0<cit.> has shown how this can be optimized by wrapping the vector construction around the forward-mode AD and applying loop optimizations. \n\nExample 2 (cont.) Consider the case of scalar-vector product, represented as !s * V! in . \nApplying the scalar FAD rules on this program results in:\n\n\ns * V' + s' * V\n\n\nIn the case of differentiation with respect to !V!, similar to the dot-product example, one needs to repeatedly pass all different one-hot vectors as !V'!.\nHowever, the differentiation with respect to !s! can be done by setting !s'! to 1 and !V'! to !zero[tensor 1]!.\n\nNext, we show an alternative differentiation transformation that enables native tensorized forward-mode AD.\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Tensorized Forward-Mode Transformation\n\n\n\nAs tensors are first-class citizens in , one can directly express the differentiation with respect to tensor variables of type \u03c4, represented by . This means that the derivative of an expression of type !tensor n! with respect to a tensor of type !tensor m!, will be a !tensor (n + m)!, which is the same as the outer tensor product type (cf. Figure\u00a0<ref>, the rules for types).\n\nFigure\u00a0<ref> shows the rules for tensorized FAD.\nThey generalize the rules for scalar forward-mode AD, which one recovers by setting \u03c4 to be !real!. The key differences are in the rules for constant reals and multiplication. Rather than just returning a real-valued !0!, tensorized FAD returns the zero value of type \u03c4 represented as !zero[\u03c4]!.\nFor multiplication, if \u03c4 is a tensor type with a non-zero order, the first term still computes the multiplication of !e1! and the differentiation of !e2!. However, the second term requires re-arranging the indices of the tensors. This complication can be avoided by only allowing for the multiplication of real numbers in the input program. This is achieved by applying multiplication normalization (cf. Section\u00a0<ref>).\n\n\n\n\nExample 1 (cont.) In our running example, tensorized FAD transformation is represented as\n!sum(<i,a> in V2) V1(i) * a!. \n\nBy applying the rules in Figure\u00a0<ref> we have:\n\n\nsum(<i,a> in V2) \n  let <i',a'> = <0,V2'(i)>\n  V1(i) * a' + V1'(i) * a\n\n\nAs one can see, this program looks identical to the version generated by scalar FAD. The difference is in the type of !V1'! and !V2'!. In scalar FAD, their type is the same as !V1! and !V2!, i.e., !tensor 1!. \nIn tensorized FAD, their type is !tensor 2!. Thus, the multiplications in the last expression are now scalar-vector multiplications.\n\nAs opposed to scalar FAD, we need to assemble the zero and one-hot vectors of all iterations together; instead of passing them vector by vector, we pass them as an entire matrix in which each row represents one of the one-hot vectors. The definition of !onehot[tensor 1]! in Figure\u00a0<ref> specifies how one can build such a matrix for variable !x!.\n\nExample 3 Consider the case of computing the trace of the matrix !M!. The tensorized FAD over this expression in  is represented as \n !sum(<i,r> in M) r(i)!.\n\n\n\n\n\n\n\n\n\nFor each row !r! of matrix !M! at index !i!, we compute the summation of diagonal elements specified by !r(i)!. After our tensorized FAD transformation, we obtain the following program:\n\n\nsum(<i,r> in M) \n  let <i',r'> = <0,M'(i)>\n  r'(i)\n\n\nTo compute the one-hot input for a matrix, we need to generalize the case of a vector. In the case of differentiating with respect to a vector (!tensor 1!), we were passing a matrix (!tensor 2!) for the one-hot inputs. Here, in the case of differentiating w.r.t. a  matrix (!tensor 2!), we need to pass an order-4 tensor (!tensor 4!). The definition of !onehot[tensor 2]! can also be found in Figure\u00a0<ref>.\n\n\n\n\n\n"}