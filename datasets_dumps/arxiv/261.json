{"entry_id": "http://arxiv.org/abs/2303.06994v1", "published": "20230313104959", "title": "Synthesizing Realistic Image Restoration Training Pairs: A Diffusion Approach", "authors": ["Tao Yang", "Peiran Ren", "Xuansong xie", "Lei Zhang"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSynthesizing Realistic Image Restoration Training Pairs: A Diffusion Approach\n    \nTao Yang[1], Peiran Ren[1], Xuansong Xie[1], and Lei Zhang[2]\n\n[1]DAMO Academy, Alibaba Group\n\n[2]Department of Computing, The Hong Kong Polytechnic University\n\nyangtao9009@gmail.com, peiran_r@sohu.com, xingtong.xxs@taobao.com, cslzhang@comp.polyu.edu.hk\n\n\n\n    March 30, 2023\n======================================================================================================================================================================================================================================================================\n\n\nempty\n\n\n\n\n   In supervised image restoration tasks, one key issue is how to obtain the aligned high-quality (HQ) and low-quality (LQ) training image pairs. Unfortunately, such HQ-LQ training pairs are hard to capture in practice, and hard to synthesize due to the complex unknown degradation in the wild. While several sophisticated degradation models have been manually designed to synthesize LQ images from their HQ counterparts, the distribution gap between the synthesized and real-world LQ images remains large. We propose a new approach to synthesizing realistic image restoration training pairs using the emerging denoising diffusion probabilistic model (DDPM). \n   First, we train a DDPM, which could convert a noisy input into the desired LQ image, with a large amount of collected LQ images, which define the target data distribution. Then, for a given HQ image, we synthesize an initial LQ image by using an off-the-shelf degradation model, and iteratively add proper Gaussian noises to it. Finally, we denoise the noisy LQ image using the pre-trained DDPM to obtain the final LQ image, which falls into the target distribution of real-world LQ images. Thanks to the strong capability of DDPM in distribution approximation, the synthesized HQ-LQ image pairs can be used to train robust models for real-world image restoration tasks, such as blind face image restoration and blind image super-resolution. Experiments demonstrated the superiority of our proposed approach to existing degradation models. Code and data will be released.\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nDeep neural networks (DNNs) <cit.> have been successfully used in a variety of computer vision tasks, including image classification <cit.>, object detection <cit.>, segmentation <cit.>, as well as image restoration <cit.>. In supervised learning, the amount and quality of labeled training data will largely affect the practical performance of trained DNN models. This problem becomes more crucial in real-world image restoration tasks <cit.>, where the aligned high-quality (HQ) and low-quality (LQ) training image pairs are very difficult to capture in practice. \nEarly works mainly resort to using bicubic downsampling or some simple degradation models <cit.> to synthesize LQ images from their HQ counterparts, which however can only cover a very small set of degradation types. The real-world LQ images can suffer from many factors, including but not limited to low resolution, blur, noise, compression, ., which are too complex to be explicitly modeled. As a result, the DNN models trained on synthesized HQ-LQ training pairs can hardly perform well on real-world LQ images. \n\nTo alleviate the above difficulties, some works have been proposed to first predict the degradation parameters <cit.> and then restore the HQ image with them in a non-blind way. They work well on some specific non-blind deblurring  <cit.>, denoising <cit.> and JPEG artifacts removal <cit.> tasks. However, the degradation of real-world LQ images are often unknown and cannot be pre-defined, making it is hard, if not possible, to predict accurate degradation parameters. Some researchers attempted to collect real-world LQ-HQ pairs <cit.> by using long-short camera focal lengths, which only work in applications where similar photographing devices are used. Recently, a couple of handcrafted degradation models <cit.> have been proposed to model complex real-world degradations. Zhang <cit.> randomly shuffled blur, downsampling and noise degradations to form a complex combination. Wang <cit.> developed a high-order degradation model. While these methods can simulate a much larger scope of degradation types and have shown impressive progress in handling LQ images in the wild, the distribution gap between the synthesized and real-world LQ images remains large <cit.>. \n\nWith the rapid advancement of deep generative networks <cit.>, methods have been developed to learn how to synthesize LQ images from their HQ counterparts. Lugmayr <cit.> learned a domain distribution network using unpaired data and then built HQ-LQ pairs with it. Similarly, Fritsche <cit.> synthesized LQ images by using DSGAN to introduce natural image characteristics. Luo <cit.> proposed a probabilistic degradation model (PDM) to describe different degradations. Most recently, Li <cit.> developed the ReDegNet to model real-world degradations using paired face images and transfer them to produce LQ natural images. However, this method largely relies on the HQ faces generated by blind face restoration models <cit.>, which limits its applications.\n\nIn this work, we revisit the problem of HQ-LQ image pair synthesis, which is critical to the many image restoration tasks such as blind face restoration and real-world image super-resolution. Our idea is to seamlessly integrate the advantages of handcrafted degradation models and deep generative networks. We first train a generator, , the emerging denoising diffusion probabilistic model (DDPM) <cit.>, by using a large amount of real-world LQ images collected from the Internet. The trained DDPM can be used to generate realistic LQ images. When building HQ-LQ image pairs, we first adopt a handcrafted degradation model to synthesize initial LQ images from the input HQ images, which may fall into the distribution of real-world LQ images. To reduce the distribution gap, the initially synthesized LQ images are added with Gaussian noise and then denoised by the pre-trained DDPM. After a few steps, the distribution of synthesized LQ images will become closer and closer to the distribution of real-world LQ images. Finally, a set of realistic HQ-LQ training pairs can be synthesized, and they can be used to train robust DNN models for image restoration tasks. \n\nIn summary, in this work we present a novel diffusion approach to synthesizing realistic training pairs for image restoration, targeting at shortening the distribution gap between synthetic and real-world LQ data. The synthesized HQ-LQ training pairs can be used for various downstream real-world image restoration tasks, as validated in our experiments of blind image restoration and blind image super-resolution. To the best of our knowledge, this is the first image degradation modeling method based on diffusion models. Codes will be made publicly available.\n\n\n\n\u00a7 RELATED WORK\n\n\n\n \u00a7.\u00a7 Degradation Modeling\n\nImage degradation modeling is of great importance for many downstream tasks such as blind face restoration (BFR) <cit.> and blind image super-resolution (BISR) <cit.>. \nBicubic downsampling has been popularly adopted in the research of single image super-resolution (SISR) <cit.>. Though it provides a common platform for comparison of SISR algorithms, models trained with this degradation strategy are of little avail, especially for real-world applications. Some works turned to use the classical degradation models <cit.>, which take the commonly observed noise, blur, and downsampling degradations into consideration. Unfortunately, they are far from enough to describe the complex unknown degradations in real-world LQ images. \n\nZhang <cit.> proposed a random shuffling strategy to construct more complex degradations of blur, downsampling and noise. Concurrently, Wang <cit.> designed a high-order degradation model with several repeated degradation processes using the classical degradation model. These two methods managed to simulate diverse degradation combinations. \nConsidering that real-world degradations are too complex to be explicitly modeled, some researchers attempted to learn a network to implicitly approximate the degradation process <cit.>. Nonetheless, it is challenging to learn such models due to the high illness of the problem. \n\nBlind Face Restoration.\nFace image restoration has attracted a lot of attentions <cit.>. Since facial images have specific structures, it is feasible to restore a clear face image from real-world degraded observations <cit.>. Many BFR methods have been proposed by resorting to exemplar images <cit.>, 3D facial priors <cit.>, and facial component dictionaries <cit.>. Recently, generative face prior has been widely used and shown powerful capability in BFR tasks <cit.>. It has been demonstrated that many LQ face images in the wild can be robustly restored. Researchers have also found that a learned discrete codebook prior can better reduce the uncertainty and ambiguity of face restoration mapping <cit.>. \n\nBlind Image Super-resolution.\nWhile significant progress has been achieved for non-blind SISR <cit.>, blind image super-resolution (BISR) remains very difficult. Bell-Kligler <cit.> introduced KernelGAN to estimate the blur kernel and then restore images based on it, which is prone to errors of estimated kernels. Following works jointly predicted the blur kernel and the HQ image. Gu <cit.> suggested an iterative correction scheme to achieve better results. \nThose methods focused on blur kernel estimation, while images in the wild suffer from much more complex degradations other than blur. Cai <cit.> and Wei et al. <cit.> respectively established an SISR dataset with paired LQ-HQ images collected by zooming camera lens. However, the model can be hardly generalized to other photographing devices. Some works exploited DNNs to learn the degradation process with unpaired data <cit.>. Lugmayr et al. <cit.> employed a cycle consistency loss to learn a distribution mapping network. Fritsche et al. <cit.> proposed DSGAN to generate LQ images. Luo <cit.> modeled the degradation as a random variable and learned its distribution. Li et al. <cit.> transferred the real-world degradations learned from face images to natural images. Although those methods have shown impressive results in some cases, their overall generalization performance in the wild remains limited. \n\n\n\n \u00a7.\u00a7 Deep Generative Network\n\nThe generative adversarial networks (GANs) <cit.> have demonstrated much more powerful capability to synthesize HQ images than likelihood-based methods such as variational autoencoders (VAE) <cit.>, autoregressive models <cit.> and flows <cit.>. \nRecently, diffusion probabilistic models (DPMs) have emerged and shown promising performance in tasks of image generation <cit.>, image inpainting <cit.>, image-to-image translation <cit.>, text-to-image generation <cit.>, text-to-video generation <cit.>, etc. In particular, high quality image synthesis results were presented in <cit.>, which was extended and improved in <cit.>. Despite the great success, DPMs require hundreds of steps to simulate a Markov chain. Song et al. <cit.> proposed a denoising diffusion implicit model (DDIM) to accelerate the sampling speed. Very recently, DPMs have shown impressive results in text-to-image/video generation <cit.>. \n\n\n\n\n\n\u00a7 PROPOSED METHOD\n\n\n\n \u00a7.\u00a7 Problem Formulation and Framework\n\n\nLet's denote by \ud835\udcb4 the space of original HQ images, by \ud835\udcb3 the space of synthetic LQ images, and by \u211b the space of real-world LQ images. Generally speaking, the spaces \ud835\udcb3 and \u211b are partially overlapped because  synthetic LQ images can have similar statistics to the real-world LQ images. How large the overlapped subspace between \ud835\udcb3 and \u211b will be depends on the employed degradation model, denoted by d. For example, Zhang et al. <cit.> and Wang et al. <cit.> handcrafted sophisticated models to synthesize LQ data from their corresponding HQ images. Some works resorted to learning a mapping network from \ud835\udcb4 to \u211b by unsupervised learning <cit.> or transferring degradations <cit.>. Nonetheless, the distributions gap between synthesized and real-world LQ images remains large. \n\n\nDifferent from these previous efforts <cit.>, we aim to integrate the advantages of handcrafted degradation models and deep generative networks, more specifically, the denoising diffusion probabilistic models (DDPM) <cit.>, to synthesize realistic training pairs. As shown in Fig.\u00a0<ref>, given an HQ image y\u2208\ud835\udcb4, we first adopt a state-of-the-art degradation model <cit.> d to generate an initial LQ image x\u2208\ud835\udcb3, , x=d(y). Usually, x shares some similar statistics and appearances to images in \u211b, while it still has certain distance to space \u211b. To make x closer to \u211b, we train a DDPM, which defines a Markov chain of diffusion steps to add random noise to data and then reverses the diffusion process to construct desired data samples from noise, to further transform x into r\u2208\u211b. \n\nThe diffusion process will gradually convert the data of a complex distribution into the data of a simpler prior distribution \ud835\udcab, , the isotropic Gaussian distribution. In other words, the distributions of images in \ud835\udcb3 and \u211b would become closer and closer during the diffusion process, and approach to the same isotropic Gaussian distribution finally. As depicted in Fig.\u00a0<ref>, we denote by \ud835\udcb3_t and \u211b_t respectively the latent distributions of \ud835\udcb3 and \u211b after diffusing t steps. One can see that the domain gap between \ud835\udcb3_t and \u211b_t becomes more and more indistinguishable. \n\nBased on the above consideration, we first diffuse x by t steps to generate z, , z=q(x), which is more likely to fall into space \u211b_t. We then reverse z to obtain the final LQ image r, , r=p(z), using DDPM pre-trained on real-world LQ images. The whole HQ to LQ image synthesis process is illustrated by the solid red line in Fig.\u00a0<ref>. Due to the powerful distribution approximation capability of DDPM, it is anticipated that the final LQ image r will fall into the space \u211b of real-world LQ images. Consequently, aligned HQ-LQ image pairs, , (y,r), can be obtained and used to train more robust image restoration models than those trained with (y,x). \n\n\n\n\n\n\n\n \u00a7.\u00a7 DDPM Model Training\n\n\nDegraded Image Dataset. Our method employs a DDPM to convert the HQ image into the LQ one, which is expected to meet the distribution of real-world LQ images. Therefore, a large-scale real-world LQ image dataset is needed to train the DDPM model first. Since most of the publicly available datasets are composed of HQ images <cit.> or synthetic LQ ones <cit.>, we build a large-scale degraded image dataset (DID) from scratch, which consists of 70,000 degraded face images and 30,000 degraded natural images. Some example images are shown in Fig.\u00a0<ref>. \n\nWe first collected a large number of images from the Internet, where each image contains at least one face. Then the facial portions are automatically detected, cropped and aligned <cit.>. Every face image is resized to have a resolution of 512\u00d7 512. We carefully prune the data to exclude occasional statues, paintings, . In addition, we tend to not harvest too many faces in a single group of photos in order to diversify the degradation types. Finally, 70,000 real-world degraded faces are collected. \n\nTo tackle more general image restoration problems such as blind image super-resolution, we crawled another set of 30,000 real-world natural images with diverse sizes and contents from the Internet. Specifically, we use more than 200 keywords, which cover a large scope of categories, including human, animal, landscape, indoor scene, ., to search via Google Images. Different from face images, we keep the crawled images unchanged. \n\nDenoising Diffusion Probabilistic Models.\nGiven a data point from a real data distribution x_0\u223c q(x), it is interesting to learn a model distribution p(x) that could approximate q(x). To achieve this goal, DDPM <cit.> defines a forward diffusion process q, which produces a sequence of latents {x_1,x_2,...,x_T} by adding Gaussian noise in T steps. The step sizes are controlled by a variance schedule \u03b2_t\u2208(0,1), where t\u2208{1,2,...,T}. There is:\n\n    q(x_t|x_t-1)   :=\ud835\udca9(x_t:\u221a(1-\u03b2_tx_t-1),\u03b2_t\ud835\udc08).\n\nIn particular, the above equation allows us to sample a latent at an arbitrary step t directly, conditioned on the input x_0. Let \u03b1_t=1-\u03b2_t and \u03b1\u0305_t=\u220f_i=1^t\u03b1_i, we can write the marginal distribution as follows:\n\n    q(x_t|x_0)=\ud835\udca9(x_t;\u221a(\u03b1\u0305_t)x_0,(1-\u03b1\u0305_t)\ud835\udc08).\n\nWith the help of reparameterization techniques <cit.>, x_t can be formulated as follows:\n\n    x_t=\u221a(\u03b1\u0305_t)x_0+\u221a(1-\u03b1\u0305_t)\u03f5, \u00a0where\u00a0\u03f5\u223c\ud835\udca9(0, \ud835\udc08).\n\nSince \u03b1_t\u2208(0,1), x_T is equivalent to an isotropic Gaussian distribution when T\u2192\u221e. Usually, T is set to 1,000. \n\nIf the exact reverse distribution q(x_t-1|x_t) is known, we will be able to recreate the original samples from a Gaussian noise input. Unfortunately, q(x_t-1|x_t) depends on the entire data distribution. We need to learn a model p_\u03b8 to approximate it. It is worth noting that if \u03b2_t is small enough, q(x_t-1|x_t) will also be Gaussian, ,\n\n    p(x_t-1|x_t)   :=\ud835\udca9(x_t-1;\u03bc_\u03b8(x_t,t),\u03a3_\u03b8(x_t,t)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Training.\nWith the collected DID, we train a DDPM to approximate its distribution. As illustrated on the left side of Fig.\u00a0<ref>, during each optimization iteration, we diffuse a randomly sampled LQ image x_0 from DID for t steps, where t is randomly chosen from {0,1,,...,T-1}. According to Eqn.\u00a0<ref>, we can easily compute x_t with a randomly generated Gaussian noise \u03f5. In order to reverse the diffusing process, DDPM is designed to predict the noise \u03f5 on top of x_t and t. To this end, we can parameterize \u03f5_\u03b8(x_t,t) with a simplified loss function:\n\n    L_simple=E_t,x_0,\u03f5[||\u03f5-\u03f5_\u03b8(x_t,t)||].\n\nDifferent from Ho <cit.>, we use L_1 instead of L_2 loss here because L_1 loss is usually more resistant to overfitting and encourages less predictive features. Particularly, for the task of BISR, we randomly crop patches of resolution 256\u00d7 256 from the original image as input x_0.\n\n\n\n \u00a7.\u00a7 HQ-LQ Image Pair Synthesis\n\n\nWith the DDPM model trained in Sec. <ref>, we are able to synthesize realistic LQ images from the HQ ones, as illustrated on the right side of Fig.\u00a0<ref>. We first apply a handcrafted degradation model to the input HQ image to obtain an initial LQ image, and then iteratively add proper Gaussian noises to the initially synthesized LQ image for t steps, obtaining a noisy LQ image x_t. Note that x_t can be directly calculated according to Eqn.\u00a0<ref> without time-consuming iterations. This is because mathematically, the merge of two Gaussian noises will result in another Gaussian noise. \n\nWe then employ the pre-trained DDPM to predict the Gaussian noise \u03f5 added to x_t. With the predicted noise, we obtain a predicted x_0, which incorporates x_t to produce x_t-1. We iteratively perform the above operations for t times and finally obtain a synthesized LQ image. Generally speaking, the quality of the output LQ image depends on the setting of t. Since the DDPM model is trained to generate samples that meet the target distribution of real-world LQ images, our synthesized LQ images are more realistic than the manually synthesized ones, as we discussed in Sec.\u00a0<ref>. \n\nIn the aforementioned synthesizing process, one can see that the handcrafted degradation model d and the diffusion step t are important factors to the final results. In our experiments, we adopt the degradation model proposed by Wang <cit.> and randomly sample t from [0, 500] and [0, 250] for BFR and BISR, respectively. \n\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n \u00a7.\u00a7 Experimental Setup\n\n\nTo comprehensively evaluate the effectiveness of the proposed HQ-LQ training pair synthesis method, we perform experiments on both synthetic data (for quantitative evaluation) and real-world data (for qualitative evaluation). For each of the BFR and BISR tasks, we employ several open-sourced models and re-train them on our training pairs, and then apply them to the synthetic test data for objective evaluation. On the real-world test data, we invite seventeen human subjects to perform user-study and compare the visual quality of the restored HQ images.    \n\nTest data.\nFor the task of BFR, we use the first 5,000 HQ face images in the CelebA-HQ dataset <cit.> to simulate LQ images, while for the task of BISR, we use the 100 HQ images in the DIV2K validation dataset  <cit.> to simulate LQ images. \nApart from the synthetic test data, we also select 100 real-world LQ face images and 100 LQ natural images (excluded from DDPM pre-training) from the DID to qualitatively evaluate the performance of different BFR and BISR models in the wild.\n\nEvaluation metrics. As for the quantitative evaluation, the Fr\u00e9chet Inception Distances (FID) <cit.>, the Learned Perceptual Image Patch Similarity (LPIPS) <cit.>, the Peak Signal-to-Noise Ratio (PSNR) and the Structural Similarity Index Measure (SSIM) <cit.> indices are used to measure the distance between the model output and ground-truth. \n\nTraining details.\nWhen training a DDPM, we adopt the Adam optimizer <cit.> with a batch size of 16. The learning rate is fixed as 8\u00d7 10^-5. In particular, we adopt the exponential moving average (EMA) method with a decay coefficient of 0.995 during optimization to ensure the training stability. The model is updated for 700K iterations.\n\nIn regarding to the training of downstreaming task (, BFR and BISR) models, we simply keep the training configurations in their original papers <cit.> unchanged, except when we train these models by using the HQ-LQ training pairs synthesized by our method.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Selections of Initial Handcrafted Degradation and Diffusion Step\n\nIn order to find out the effects of handcrafted degradation model d and diffusion step t on the synthesized HQ-LQ pairs, we conduct ablation studies by applying four representative handcrafted degradation models, , bicubic downsampling (denoted by Bicubic), the classical degradation model (denoted by Classical, see <cit.>), the one proposed by Zhang <cit.> and the one proposed by Wang <cit.>, to our method. \n\n\n\n\n\n\nWe use the first 5,000 face images in FFHQ <cit.> as the HQ inputs, and generate four groups of LQ counterparts by our method with the four handcrafted degradation models. We calculate the FID <cit.> between each group of generated LQ images and the real degraded faces in our collected DID to evaluate the distribution distances.\n\n\nHandcrafted Degradation Model.\nThe curves of FID versus diffusion step t are plotted on the top of Fig. <ref>. We see that when t=0 (, without applying the pre-trained DDPM), Bicubic has the worst FID, while Zhang <cit.> and Wang <cit.> are among the best. This indicates that the degradation models proposed by Zhang et al. <cit.> and Wang et al. <cit.> can better simulate real-world degraded data than bicubic downsampling and the classical degradation model. It is worth mentioning that all the four models can produce realistic LQ images (, low FID value) with a large enough step number t; however, a good degradation model should be able to generate LQ images that cover a large scope of LQ space. The models developed by Zhang <cit.> and Wang <cit.> demonstrate their superiority to bicubic downsampling and the classical degradation model. We choose the model proposed by Wang <cit.> due to its efficient implementation. \n\nDiffusion Step. A qualified HQ-LQ image pair should share the same content and structure in general. We calculate the PSNR of generated HQ-LQ pairs and plot the curve on the bottom of Fig. <ref>.\nOne can see that all curves converge with the increase of diffusion step t. This is because the distributions of LQ images and real-world degraded images are getting closer during the diffusion process, and they approach to the same isotropic Gaussian distribution. In other words, with a large enough diffusion step t, we can produce LQ images with lower FID values. However, a large t will destroy the structure of synthesized LQs compared with their HQ counterparts. As shown in Fig.\u00a0<ref>, the PSNR decreases when t grows. By our experiments, when the PSNR of an HQ-LQ pair is smaller than 24, the general structure of them will become inconsistent. Fig.\u00a0<ref> shows some synthesized LQ images by our method with different steps t. One can see that the structures of synthesized LQ images become inconsistent with the HQ ones when t>500 / t>250 for face/natural images.\nWe therefore randomly sample t from [0, 500] for the training pair synthesis of BFR, and sample t from [0, 250] for BISR.  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Experiments on Synthetic Test Images\n\n\nBlind Face Restoration.\nWe validate the effectiveness of our approach by using four representative BFR models, , PSFRGAN <cit.>, HiFaceGAN <cit.>, GFPGAN <cit.> and GPEN <cit.>, whose training codes are publicly available so that we can re-train them by using the HQ-LQ image pairs synthesized by our method with their original training settings. We denote by PSFRGAN+, HiFaceGAN+, GFPGAN+ and GPEN+ the re-trained models on our training data. By applying those original and re-trained models on the synthetic LQ face images, we can calculate the FID, LPIPS, PSNR and SSIM indices by comparing the reconstructed HQ images with the ground-truth HQ images. The results are listed in Table\u00a0<ref>. One can see that for each BFR method, the model trained on our synthesized training pairs can achieve better FID/LPIPS/PSNR/SSIM indices than its original counterpart in most cases. \n\nFig.\u00a0<ref> compares the BFR results on some degraded face images by original BFR models and their re-trained counterparts. It can be seen that GFPGAN <cit.> and GPEN <cit.> can produce reasonable face reconstructions while failing to tackle faces that are severely compressed or with large viewpoint variation. After re-training GFPGAN and GPEN with HQ-LQ training pairs synthesized by the proposed method, GFPGAN+ and GPEN+ succeed in restoring clearer faces with more vivid details in comparison with their original counterparts. \n\nMore visual comparison results can be found in Sec.\u00a0<ref>. \n\nBlind Image Super-Resolution.\nBISR aims to reconstruct HQ images with perceptually realistic details from the input LQ image with unknown degradation. We employ two categories of state-of-the-art algorithms in this experiment. The first category is GAN-based methods trained on unpaired real data, including ESRGAN-FS <cit.>, DASR <cit.>, Wang <cit.>, PDM-SRGAN <cit.>, F2N-ESRGAN <cit.>. All methods are re-trained on our DID. The second category is models trained with handcrafted HQ-LQ image pairs, including RealSR <cit.>, BSRGAN <cit.>, Real-ESRGAN <cit.>. All these three methods use RRDB <cit.> as the backbone with similar losses. The key difference among them lies in the training data. Therefore, we re-train a RRDB-based model <cit.> by using our synthesized training pairs with the training settings of Real-ESRGAN <cit.>, resulting in the RRDB+ model.  The quantitative evaluation results on the test data are presented in Table\u00a0<ref>. One can see that RRDB+ outperforms the competing methods in most of the metrics. In particular, compared with the original Real-ESRGAN model, our re-trained  RRDB+ achieves similar LPIPS index but significantly better FID, PSNR and SSIM indices. \n\nFig.\u00a0<ref> presents the visual comparison of competing BISR methods on several synthesized LQ natural images. One can see that our method can better reconstruct realistic details and preserve the general structures, demonstrating the effectiveness of our HQ-LQ training pair synthesis methods. Comparing to Real-ESRGAN <cit.>, RRDB+ has clear advantages in inhibiting artifacts. \n\n\n\n \u00a7.\u00a7 Experiments on Images in the Wild\n\nSince our method targets at synthesizing realistic image restoration training pairs, it is necessary to conduct experiments on images in the wild. We select 100 face and 100 natural images from DID for evaluation. \n\nBlind Face Restoration.\nWe apply the same four BFR methods as in Sec.\u00a0<ref> to the 100 real-world LQ face images. Fig.\u00a0<ref> shows the BFR results on two images. Due to the limit of space, we only show the results of GFPGAN <cit.>, GPEN <cit.> and GPEN+ here. More results can be found in Sec.\u00a0<ref>. \nAs can be seen, GPEN+ can better handle real-world degradations and produce more realistic results than GPEN and GFPGAN, mainly due to the more realistic image training pairs.\n\nTo better evaluate the advantage of the proposed method, we conduct a user study as subjective assessment. For each of the four groups of BFR methods, , PSFRGAN vs. PSFRGAN+, HiFaceGAN vs. HiFaceGAN+, GFPGAN vs. GFPGAN+, GPEN vs. GPEN+, the BFR results are presented in pairs to 17 volunteers in a random order. The volunteers are asked to pick the better BFR image from each pair according to their perceptual quality. The statistics are presented in Fig.\u00a0<ref>. One can see that in each group, the model trained with our synthesized data is more preferred.\n\nBlind Image Super-Resolution.\nFig.\u00a0<ref> shows the BISR results of ESRGAN-FS <cit.>, DASR <cit.>, BSRGAN <cit.>, Real-ESRGAN <cit.>, PDM-SRGAN <cit.>, F2N-ESRGAN <cit.>, and our RRDB+. It can be seen that the competing methods yield dirty outputs, , the second and third columns of Fig.\u00a0<ref>, or tend to produce over-smoothed results, , the frog in the first row of Fig.\u00a0<ref>, or fail to reconstruct photo-realistic details, , the eye in the second row of Fig.\u00a0<ref>. With the help of the realistic HQ-LQ training pairs generated by our method, RRDB+ is capable of better handling complex degradations in the wild. More visual results can be found in Sec.\u00a0<ref>. \n\nSince the commonly used quantitative metrics do not correlate well with human visual perception to image quality, we conduct a user study as subjective assessment. The BISR results of BSRGAN <cit.>, Real-ESRGAN <cit.>, PDM-SRGAN <cit.>, F2N-ESRGAN <cit.> and RRDB+ on 100 natural images from DID are presented to 17 volunteers in random order. The volunteers are asked to rank the five BISR outputs of each input image according to their perceptual quality. As presented in Fig.\u00a0<ref>, our method RRDB+ receives the most rank-1 votes and the least rank-5 votes.\n\n\n\n\n\n\n\u00a7 CONCLUSION AND DISCUSSION\n\nWe proposed, for the first time to our best knowledge, to train a DDPM to synthesize realistic image restoration training pairs. With a collected LQ image dataset, a DDPM was first trained to approximate its distribution. The pre-trained DDPM was then employed to convert the initially degraded image from its HQ counterpart into the desired LQ image, which fell into the distribution of real-world LQ images. With the synthesized realistic HQ-LQ image pairs, we re-trained the many state-of-the-art BFR and BISR models, and the re-trained models demonstrated much better realistic image restoration performance than their original counterparts both quantitatively and qualitatively.\n\nIt should be noted that in our experiments, we collected a large scale degraded face dataset and a natural image dataset, respectively, as the target distributions to train the DDPM and synthesize HQ-LQ training pairs. In practice, the users can build their own LQ image dataset according to their needs, train the corresponding DDPM models and synthesize training pairs for different image restoration tasks. \n\n\nieee_fullname\n\n\n\n\n\n\n\n\u00a7 APPENDICES\n\n\n\n\n \u00a7.\u00a7 More BFR Results on Synthetic Faces\n\n\n\nThis section shows more visual results of competing BFR methods on synthetic face images. As in the main paper, we compare the BFR results on degraded face images by the original BFR models, , GFPGAN <cit.> and GPEN <cit.>, and their re-trained counterparts on our training pairs, , GFPGAN+ and GPEN+. The visual comparisons are presented in Fig.\u00a0<ref>. One can see that GFPGAN+ and GPEN+ generate superior results with finer details to their original counterparts. \n\n\n\n \u00a7.\u00a7 More BFR Results on Faces in the Wild\n\n\nThis section shows more visual results of competing BFR methods on face images in the wild. As in the main paper, we compare our method with PSFRGAN <cit.>, HiFaceGAN <cit.>, GFPGAN <cit.> and GPEN <cit.>. The visual comparisons are presented in Fig.\u00a0<ref>. It can be seen that our method can produce more realistic results.\n\n\n\n \u00a7.\u00a7 More BISR Results on Images in the Wild\n\n\nIn this section, we show more visual comparison between our method with state-of-the-art BISR methods, including DASR <cit.>, PDM-SRGAN <cit.>, BSRGAN <cit.> and Real-ESRGAN <cit.>, in Fig.\u00a0<ref>, from which we can see the superior performance of our method on BISR tasks. \n\n\n\n \u00a7.\u00a7 Comparison between GAN-based methods and our diffusion method on LQ image synthesis\n\nIn order to demonstrate the superior performance of our proposed diffusion-based method in synthesizing LQ images to existing GAN-based methods <cit.>, we compute the FID values to evaluate the distribution distances between synthesized LQs and real LQs in DID. The quantitative results are presented in Table\u00a0<ref>. It can be seen that our diffusion-based method outperforms existing GAN-based methods significantly. This is because GAN-based methods learn a direct mapping from HQs to LQs, which is hard to train due to the large distribution gap and unstable adversarial training, while our method employs an initial LQ image as the starting point, and use a pre-trained diffusion model to stably generate the final LQ image. During the reverse process of DDPM, the images are naturally constrained in the LQ space. In contrast, GAN-based methods only constrain the final outputs, and hence they are less stable in LQ image synthesis.\n\n\n\n\n\n\n\n\n\n\n"}