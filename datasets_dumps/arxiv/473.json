{"entry_id": "http://arxiv.org/abs/2303.11423v1", "published": "20230312134945", "title": "Heart Murmur and Abnormal PCG Detection via Wavelet Scattering Transform & a 1D-CNN", "authors": ["Ahmed Patwa", "Muhammad Mahboob Ur Rahman", "Tareq Y. Al-Naffouri"], "primary_category": "eess.SP", "categories": ["eess.SP", "cs.LG"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeart Murmur and Abnormal PCG Detection via Wavelet Scattering Transform & a 1D-CNN \n    Ahmed\u00a0Patwa,\u00a0Member,\u00a0IEEE,\n        M.\u00a0Mahboob\u00a0Ur\u00a0Rahman,\u00a0Senior Member,\u00a0IEEE,\n        and\u00a0Tareq\u00a0Al-Naffouri,\u00a0Senior Member,\u00a0IEEE\nAuthors are with the Department\nof Electrical Engineering, KAUST, KSA.\n    March 30, 2023\n===========================================================================================================================================================================================================\n\n\n\n\n\n\n\n\nThis work leverages deep learning (DL) techniques in order to do automatic and accurate heart murmur detection from phonocardiogram (PCG) recordings.  Two public PCG datasets (CirCor Digiscope 2022 dataset and PCG 2016 dataset) from Physionet online database are utilized to train and test three custom neural networks (NN): a 1D convolutional neural network (CNN), a long short-term memory (LSTM) recurrent neural network (RNN), and a convolutional RNN (C-RNN). Under our proposed method, we first do pre-processing on both datasets in order to prepare the data for the NNs. Key pre-processing steps include the following: denoising, segmentation, re-labeling of noise-only segments, data normalization, and time-frequency analysis of the PCG segments using wavelet scattering transform. We note that the segmentation step results in a number of noise-only segments; thus, we re-label the noise-only segments. Further, we do exhaustive search to find optimal segment length (which turns out to be 4 sec). \n\nTo evaluate the performance of the three NNs we have implemented, we conduct four experiments, first three using PCG 2022 dataset, and fourth using PCG 2016 dataset. It turns out that our custom 1D-CNN outperforms other two NNs (LSTM-RNN and C-RNN) as well as the state-of-the-art. \n\nSpecifically, for experiment E1 (murmur detection using original PCG 2022 dataset), our 1D-CNN model achieves an accuracy of 82.28%, weighted accuracy of 83.81%, F1-score of 65.79%, and and area under receive operating charactertic (AUROC) curve of 90.79%. \nFor experiment E2 (mumur detection using PCG 2022 dataset with unknown class removed), our 1D-CNN model achieves an accuracy of 87.05%, F1-score of 87.72%, and AUROC of 94.4%. \nFor experiment E3 (murmur detection using PCG 2022 dataset with re-labeling of segments), our 1D-CNN model achieves an accuracy of 82.86%, weighted accuracy of 86.30%, F1-score of 81.87%, and AUROC of 93.45%. \nFor experiment E4 (abnormal PCG detection using PCG 2016 dataset), our 1D-CNN model achieves an accuracy of 96.30%, F1-score of 96.29% and AUROC of 98.17%.\n\n\n\n\n\n\n\n\nphonocardiogram, heart murmur, valvular heart disease, physionet, convolutional neural network, recurrent neural network, wavelet scattering.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCardiac auscultation provides valuable insights into the mechanical activity of heart, and remains the gold standard for diagnosis of a wide range of cardiovascular diseases. Phonocardiogram (PCG)\u2014the systematic recording of heart sounds by means of a digital stethoscope, is a clinically significant method to study the pathology of the four heart valves (i.e., the mitral valve, tricuspid valve, aortic valve, and pulmonary valve). This is because the PCG could record abnormal sounds, known as heart murmurs, made by heart valves as they open and close during cardiac cycle. Heart murmurs, when present, often indicate the presence of a heart valve disease, e.g., mitral regurgitation, mitral stenosis, congenital heart disease, septal defects, patent ductus arteriosus in newborns, defective cardiac valves, rheumatic heart disease, etc <cit.>. Thus, early murmur detection, classification, and grading analysis in an automated fashion to help diagnose a valvular heart disease at an early stage is the need of the hour <cit.>. \n\nPCG signal acquisition is done from following four chest locations where the valves can be best heard: Aortic area, Pulmonic area, Tricuspid area, and Mitral area <cit.>. For a normal sinus cardiac cycle, the PCG signal captures two fundamental heart sounds, first is called S1 sound while second is called S2 sounds. S1 signifies the start of isovolumetric ventricular contraction with the closing of the mitral and tricuspid valves amid rapid increase in pressure within the ventricles. S2, on the other hand, implies the start of diastole with the closing of the aortic and pulmonic valves. In addition to S1 and S2 sounds, the mechanical activity of the heart may occasionally give rise other sounds which include: third heart sound (S3), the fourth heart sound (S4), systolic ejection click (EC), mid-systolic click (MC), diastolic sound or opening snap (OS), and heart murmurs (due to turbulent flow of blood due to malfunctioning of heart valves) <cit.>.   \n\nPCG signal analysis for abnormal heart beat detection and for murmur detection has traditionally been done using classical signal processing techniques <cit.>. More recently, there has been enormous interest in utilizing tools from machine learning <cit.>, deep learning <cit.>, and transfer learning <cit.> in order to do heart sounds classification (see the review articles <cit.>,<cit.>, and references therein for more in-depth discussion of the related work). \n\nContributions.\nInline with the recent surge of interest in utilizing deep learning methods for PCG signal analysis, this work utilizes two public PCG datasets (PCG 2022 dataset and PCG 2016 dataset) from Physionet database <cit.> to train and test three custom neural networks (NN), i.e., 1D-CNN, LSTM-RNN, C-RNN for heart murmur and abnormal PCG detection. We first pre-process the data in order to split each PCG signal into smaller segments using a custom-designed GUI framework. We systematically identify the noise-only segments and re-label them, and do exhaustive search to determine optimal segment size. We conduct four experiments to evaluate the performance of our NN models on following four datasets: 1) PCG 2022 dataset as is, 2) PCG 2022 dataset with unknown class removed, 3) PCG 2022 dataset with re-labeled segments, and 4) PCG 2016 dataset. We note that our 1D-CNN model outperforms the other two NN models (i.e., LSTM-RNN and C-RNN), as well as the state-of-the-art methods for murmur detection.\n\nOutline.\nThe rest of this paper is organized as follows. Section II describes selected related work on murmur detection and heart valve disease classification. Section III outlines essential details of the two public datasets used, pre-processing done, and time-frequency domain methods for feature extraction. Section IV presents three deep learning classifiers for murmur detection and abnormal PCG detection. Section V discusses performance results for all three classifiers for both datasets. Section VI concludes the paper.\n\n\n\n\u00a7 RELATED WORK\n\n\nBroadly speaking, the relevant literature on PCG signal analysis has attempted to solve following major problems, to date: i) identification of fundamental heart sounds (S1 and S2) <cit.>, ii) abnormal PCG detection (by identifying sounds other than the normal lub-dub sound) <cit.>, iii) heart sound classification (normal sounds, murmurs, extra clicks, artifacts) <cit.>, iv) heart murmur detection <cit.>, v) heart valve disease classification (by means of heart murmur classification) <cit.>, and vi) PCG signal denoising methods <cit.>. Lately, there is a work which attempts to do automatic murmur grading analysis <cit.> at the patient-level, and has the potential to do disease staging and progression analysis. \n\nIn terms of public datasets, apart from PCG 2022 dataset <cit.> and PCG 2016 dataset <cit.> that are available on Physionet database, there are a few other public datasets too. These include: EPHNOGRAM <cit.>, PASCAL, Michigan heart sound and murmur database (MHSDB) provided by the University of Michigan health system, and cardiac auscultation of heart murmurs database provided by eGeneral Medical Inc. <cit.>. On a side note, fetal PCG dataset provides 26 recordings of fetal PCG collected from expecting mothers with singleton pregnancy during their last trimester <cit.>. \n\nSince this work focuses mainly on automatic heart murmur and abnormal PCG detection, selected work on these two problems is discussed as follows. \n<cit.> utilizes PCG 2016 dataset, applies multiple time-frequency analysis methods, e.g., discrete wavelet transform, mel-frequency cepstral coefficients etc., along with a feedforward neural network in order to do abnormal PCG detection, achieving 97.1% accuracy.\n<cit.> computes short time Fourier transform of the PCG signals without doing any segmentation, implements a custom CNN, and achieves an accuracy of 95.4%, and 96.8%, on PCG 2016 dataset and PASCAL dataset, respectively. \n<cit.> utilizes two PCG datasets to evaluate their custom CNN model, as well as two pre-trained models (i.e., VGGNet-16, ResNet-50) in order to do heart valve disease (HVD) classification. They claim to achieve an overall accuracy of 99% to classify the following HVD: aortic stenosis, mitral stenosis, aortic regurgitation, mitral regurgitation, and mitral valve prolapse.\n<cit.> utilized a private dataset (with data collected at PKU Muhammadiyah Yogyakarta Hospital, Indonesia), and implemented a custom CNN model in order to HVD classification for the following diseases: angina pectoris, congestive heart failure, and hypertensive heart disease. They reported an accuracy of 85% for their HVD classification problem. \n<cit.> aimed at intelligent diagnosis of murmurs in pediatric patients with congenital heart disease. They applied segmentation method to extract the first and second heart sounds from the PCG signals, and used them as input to their feedforward neural network classifier, resulting in HVD detection accuracy of 93%. \n<cit.> utilizes the PASCAL dataset and aims to classify PCG signals into three classes (normal, systolic murmur, diastolic murmur). To this end, they extract various time-domain features, frequency-domain features, and statistical features, and pass them to three classifiers: k-NN, fuzzy k-NN and a feedforward neural network. They report a classification accuracy of 99.6%, that is achieved by the fuzzy k-NN. \n<cit.> utilizes the PCG 2016 dataset, extracts time-domain, frequency-domain, and time-frequency domain features from the PCG signals, and implements an ensemble of 20 feedforward neural networks in order to do abnormal PCG detection, achieving an overall accuracy of 91.5%.\nFinally, <cit.> utilizes the PCG 2016 dataset, extracts time-domain, frequency-domain and time-frequency domain features, does feature selection and implements a two-layer feedforward neural network for abnormal PCG detection, resulting in an accuracy of 85.2%. \n\n\n\n\n\n\u00a7 DATASETS, PRE-PROCESSING & FEATURE EXTRACTION \n\n\nThis work utilized two PCG datasets: 1) CirCor Digiscope PCG dataset from PhysioNet 2022 challenge (called PCG 2022 dataset, in this work) <cit.>, 2) PCG dataset from PhysioNet 2016 challenge (called PCG 2016 dataset, in this work). Below, we describe the key details of the two datasets, followed by the key pre-processing steps performed on each dataset, followed by feature extraction by means of various time-frequency analysis methods.\n\n\n\n\n \u00a7.\u00a7 Datasets\n\n1) CirCor Digiscope PCG dataset from PhysioNet 2022 challenge:\n\nThis dataset is a collection of PCG signals collected from pediatric subjects during the two mass screening campaigns in Brazil. The dataset consists of 3,163 audio files, collected from 942 patients, sampled at 4,000 Hz. The length of these recordings vary between 5 seconds to 65 seconds. These PCG recordings have been taken from one or more of 4 possible locations on the chest, namely: AV, MV, PV, TV (see Fig. 1). Additionally, some samples have been taken at another location, labeled as Phc. Moreover, some subjects had multiple recordings for the same chest location. Murmur labels are given both patient-wise, or chest-location-wise. This implies that we could get to know whether a patient has murmur or not, as well as if the dataset annotator detected murmur in each recording separately. In this work, we go by chest-location-wise murmur detection. \n\n\n\n\n\n\nIn addition to the audio PCG data, following meta-data is also available: age, weight, and gender of the pediatric subject. Furthermore, additional information about heart murmurs such as the location of a murmur, strength of a murmur is also given. Having said that, this work utilizes the CirCor Digiscope 2022 dataset for murmur detection, i.e., we solve a classification problem with following three classes: 1) murmur present, 2) murmur absent, and 3) unknown (noisy, unknown to the annotator).\n\n2) PCG dataset from PhysioNet/CinC 2016 challenge:\n\nThe PhysioNet/CinC challenge 2016 dataset consists of a total of 3,240 heart sound recordings, sampled at 2,000 Hz. The length of these recordings vary between 5 seconds to 120 seconds. All the data was recorded from a single precordial location on the chest. As for the labels, each recording has been annotated as normal (taken from a healthy subject) or abnormal (taken from subjects with confirmed cardiac diagnosis). Furthermore, each recording has been labeled as either high-quality or low quality. Moreover, some of the data belonging to abnormal class has been further annotated with the exact diagnosis that the subject suffers from. There are a total of 2,575 recordings for the normal class, while there are a total of 665 recordings for the abnormal class. Thus, the data is highly imbalanced given the number of examples for the normal and abnormal class, and this effect is more pronounced when it comes to disease labels (within the abnormal class). \n\nWe utilize PCG 2016 dataset to solve the classification problem with two classes: 1) PCG normal, 2) PCG abnormal. One important difference between this dataset and the CirCor Digiscope dataset is that this dataset assigns normal labels even to the low-quality recordings. Thus, these recordings could still be used for the classification problem (PCG normal vs. PCG abnormal), without adding a third class for low-quality recordings.  \n\nTable <ref> summarizes the key statistics of the two datasets.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Data Pre-Processing\n\nDenoising:\nBoth datasets have noise overlapping with heartbeats. Some prominent examples of noise in the two datasets include: noise due to stethoscope movement, baby crying, intense breathing, and people talking. Since the spectrums of voice and PCG overlap, complete elimination of noise is not possible. However, out-of-band (OOB) noise rejection is still possible. To this end, a low-pass filter with a cut-off frequency of 300 Hz (chosen by experimentation) has been applied to each PCG signal to remove the OOB noise.\n\nSegmentation:\nSince recordings are not of the same length, each recording has been divided into N-second long segments[The optimal segment length turns out to be N=4, as will be discussed in Section V in more detail.], to increase the number of training examples for the deep learning model.\nThus, for PCG 2016 dataset, we end up with 12,827 PCG segments for the normal class and 3,922 PCG segments for the abnormal class. \nTo deal with the problem of highly imbalanced dataset, a balanced subset is constructed by including all the PCG segments from the abnormal class, while randomly selecting PCG segments from the normal class. This way, each of the two classes contain a total of 7,844 audio files (segments).  \nThe same process is repeated for the CirCor Digiscope 2022 dataset, where a total of 16,522 segments were extracted. We then utilized 6,571 files to construct as a more-balanced dataset as follows: 3,579 files from murmur absent class, 2,593 files from murmur present class, and 399 samples from the unknown class.\n\n\nRe-labeling of noise-only segments: \nThe splitting of a PCG time-series into multiple smaller segments results in a new problem (i.e., how to assign labels to each segment). Note that a label (murmur present, murmur absent, unknown) was assigned to each recording as a whole by the annotator in the original dataset. However, when dividing a PCG signal into smaller segments, a situation arises where some segments are bad segments (i.e., they contain only noise, with no heartbeat at all!). If left untreated, feeding such noise-only segments to the deep learning model could have a negative impact on its performance. That is, the model will consider one such segment as representing a murmur. But in reality, the segment contains pure noise, and thus, should ideally be labeled as an unknown sample. Thus, to deal with this problem, a systematic method for assessment and re-labeling of each segment of the CirCor 2022 dataset is needed. \n\nGraphical user interface for segment assessment and potential re-labeling:\nTo this end, we developed a simple graphical user interface (GUI) in Python, in order to assess the quality of the default label for each segment in order to re-label all the noise-only segments, in a semi-automated fashion (see Fig. <ref>). Specifically, the GUI of Fig. <ref> shows the 2D time-frequency domain representation of each segment, as well as plays the audio. This allows us to systematically determine the presence or absence of heart beat in each segment, for all PCG files. Note that the re-labeling process made sure not to alter the integrity of the dataset. Specifically, the only allowed change was to re-label (murmur present) to (unknown), or (murmur absent) to (unknown) indicating that a certain segment of a whole recording doesn't actually represent a heartbeat signal, but rather contains only noise. In other words, no (murmur present) was changed to (murmur absent) or the other way around. Similarly, no (unknown) was changed to (murmur present) or (murmur absent). This exercise helped us identify the bad segments, which in turn helped us improve the training accuracy of our proposed DL classifiers. The resulting new distribution of the three classes (murmur present, murmur absent, and unknown) is summarized in Table II. \n\n\n\n\n\n\n\n\n\n\nData normalization:\nFor each data vector (segment), its mean \u03bc and standard deviation \u03c3 is computed. Then, i-th element x_i of each data vector is normalized by means of following operation: x_i,n=x_i-\u03bc/\u03c3. The normalized data vector has zero mean and unit variance.\n\n\n\n \u00a7.\u00a7 Time-Frequency Analysis for Feature Extraction\n\nIn order to study the feasibility of the two classification problems at hand, and in order to help our neural networks do automatic feature extraction, we transform the 1D audio PCG signals into their equivalent 2D representation using various time-frequency domain methods, e.g., short-time Fourier transform, mel-spectrograms, discrete and continuous wavelet transforms, and wavelet scattering transform (WST). We note that all but WST are not effective in differentiating between the three classes for the CirCor Digiscope 2022 dataset (the same holds for the PCG 2016 dataset). This is because WST is known to be effective for feature extraction, even for very small datasets. Therefore, we stick to WST for the rest of this work[Wavelet scattering is a technique that applies mathematical operations similar to those applied in convolutional layers. Specifically, WST utilizes wavelets as fixed convolutional filter, followed by a non-linearity (modulus operator), followed by averaging in order to find a low-variance representation of the data. ].\n\nFig. <ref> provides a visual summary of two possible situations (murmur present, murmur absent) at each of the four heart valves. Specifically, Fig. <ref> provides the audio PCG time-series as well as the wavelet scattering function for all possible scenarios. We conclude from Fig. <ref> that wavelet scattering function indeed is effective in discriminating between the three classes for the CirCor Digiscope 2022 dataset. \n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 HEART MURMUR & ABNORMAL PCG DETECTION\n\n\n\n\n \u00a7.\u00a7 Neural Network Architectures\n\nFor heart murmur detection and abnormal PCG detection, we implemented and tested the following neural network (NN) architectures: convolutional neural network (CNN)\u2013both 1D and 2D, recurrent neural network (RNN)[The motivation behind using the RNN (LSTM and GRU) is to exploit its capability to learn temporal dependencies between the samples of a time-series (audio PCG signals, in this work).], combination of CNN and RNN, and residual network (ResNet). Eventually, it was the 1D-CNN model that resulted in maximum classification accuracy. Therefore, Fig. <ref> provides a detailed block diagram of the custom 1D-CNN model, with important parameters for each layer specified. One could see that the best-performer 1D-CNN model is relatively shallow, with 4 convolution layers (each followed by a pooling layer, followed by a batch normalization layer), a flattening layer, and 4 dense (fully connected) layers. Each (convolutional and dense) layer utilized RELU activation function, except the last layer which utilized softmax activation function. \n\nFor the sake of completeness, Figs. <ref>, <ref> present the block diagrams of the two other neural network models implemented in this work: a C-RNN model, and a single-layer LSTM-RNN model (with tanh activation function). Finally, Fig. <ref> provides the overall pictorial summary of our approach for heart murmur and abnormal PCG detection.\n\nNote that we re-train our 1D-CNN model from scratch in order to do abnormal PCG detection on the PCG 2016 dataset later[We investigated transfer learning technique by reusing the 1D-CNN model trained on PCG 2022 dataset by testing it on PCG 2022 dataset, but without much success. This points to the fundamental different nature (distribution) of the two datasets.].\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Training and Testing of NNs & Hyper-parameters\n\n\nWe implemented the custom 1D-CNN and LSTM-RNN models in Python using the PyTorch framework (in Microsoft visual studio environment), on a desktop PC with I5 7600 intel CPU and 1060ti Nvidia graphics card. The total available data for both datasets was divided into training data and test data using an 80-20 split, while making sure that test data is totally unique (i.e., the test data is not seen by the models during the training stage). For backpropagation purpose, the cross-entropy loss function was used. Table <ref> summarizes the important hyper-parameters for the 1D-CNN model and the LSTM-RNN model when trained on both datasets.\n\n\n\nWe conducted a total of four experiments by training the custom 1D-CNN and LSTM-RNN on the two datasets, in order to do: \n\n    \n  * E1) murmur detection using the original PCG 2022 dataset with 3 classes (murmur present, murmur absent, unknown).\n    \n  * E2) murmur detection using a subset of the PCG 2022 dataset with 2 classes (murmur present, murmur absent) with unknown class excluded. \n    \n  * E3) murmur detection with 3 classes using the cleaned PCG 2022 dataset after re-labeling of noise-only segments, as discussed in previous section. \n    \n  * E4) abnormal PCG detection using the PCG 2016 dataset. \n\n\n\nFig. <ref> shows that the loss as well as the accuracy settles in about 150 epochs during both the training and the validation phase, for experiment E1. Fig. <ref> shows that the loss and accuracy settle much quickly (in about 10 epochs), for experiment E2 (due to the removal of the unknown class with noisy samples). The loss and accuracy plots for experiments E3 and E4 show a similar trend, and thus, are omitted for the sake of brevity.\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n\n\nWe first describe the performance metrics used to evaluate the 1D-CNN and other classifiers, followed by a discussion of the key results for the three experiments E1, E2, E3 on murmur detection using the three variants of the PCG 2022 dataset, followed by a discussion of selected results for the fourth experiment E4 on abnormal PCG detection using PCG 2016 dataset. \n\n\n\n \u00a7.\u00a7 Performance Metrics\n\n\nWe use accuracy and F1-score as the main performance evaluation metrics. Since accuracy is most meaningful for a balanced dataset, we minimized the class imbalance by reducing the size of the murmur present class in both datasets. Additionally, weighted accuracy is also used as a performance metric for comparison with other contestants of the Physionet challenge 2022.\n\n\n\n  \u00a7.\u00a7.\u00a7 Accuracy\n\nThe accuracy of a dataset is the ratio of number of correct predictions S_c by the model and total number of samples S_t: A = S_c/S_t\u00d7 100. \n\n\n\n  \u00a7.\u00a7.\u00a7 Weighted Accuracy\n\nThe weighted accuracy is a metric that gives more weight to the patients with murmur, and is defined as:\n\nA_w = 5m_pp + 3m_uu + m_aa/5(m_pp + m_up + m_ap) + 3(m_pu + m_uu + m_au) + (m_pa + m_ua + m_aa)\nwhere m_xx is defined as in Table <ref>.\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 F1-Score\n\nF1-score is a statistical measure that depends upon two factors, precision and recall. Precision is obtained by dividing total number of correctly classified elements, i.e. True Positives by total positively classified elements, i.e. True Positives (TP) + False Positives (FP). Thus, P = TP/TP + FP. Recall is obtained by dividing total number of positively classified samples by the total number of samples that should had been marked as positive, i.e., True Positives and True Positives + False Negatives (FN) respectively. Thus, R = TP/TP + FN.\nWith precision and recall in hand, F1-score is calculated as: F1 = 2 * P*R/P+R, where P is precision and R is recall.\n\n\n\n\n\n \u00a7.\u00a7 Results: Murmur detection using PCG 2022 dataset\n\n\nPerformance of 1D-CNN:\nFirst, we report the results obtained by the best-performing 1D-CNN model. For experiment E1 that does murmur detection using PCG 2022 dataset with three classes, we obtained a maximum accuracy of 82.28%, 83.81% weighted accuracy, and 90% area under the receiver operating characteristic curve (AUROC) (see the confusion matrix in Fig. <ref> (a)). On the other hand, the experiment E2 that does murmur detection using PCG 2022 dataset with two classes (with unknown class with low-quality audios removed), resulted in an improved accuracy of 87.66%, elevated F1-score of 0.87, and higher AUROC of 92.28% (see the confusion matrix in Fig. <ref> (b)). Finally, for experiment E3 that does murmur detection using PCG 2022 dataset with three classes (after re-labeling), the weighted accuracy improved to 86% and AUROC rose to 93% (see Fig. <ref> (c)). This resulted in a more balanced confusion matrix for experiment E3 as before the re-labeling process, only 17% of the unknown class samples were correctly detected in experiment E1, but now it increases to 80% after the re-labeling process in experiment E3. This could also be verified by comparing the F1-scores for experiments E1 and E3. \n\nSelection of optimal segment size:\nWe repeated our experiment E1 (where we utilize the original PCG 2022 dataset as is) with different segment sizes (i.e., 1 sec, 3 sec, 4 sec, and 5s), and assessed the performance of our 1D-CNN model with the aim to find an optimal segment size (see Table <ref>). We utilize F1-score as the core performance metric to select the optimal segment size (this is because the accuracy is not a reliable performance metric for the imbalanced datasets). Furthermore, we observed that overlapping segments cause extreme over-fitting; therefore, we ended up selecting non-overlapping segments of duration 4 seconds. \n\n\n\n\n\n\n\n\nPerformance comparison with LSTM-RNN and CRNN:\nTable <ref> lists the performance obtained by the other two NN models (i.e., LSTM-RNN and CRNN) that we implemented and tested. We note that our 1D-CNN model outperforms both LSTM-RNN and CRNN models in terms of accuracy, weighted accuracy, precision, recall, F1-score and AUROC, for experiment E3 (which does murmur detection on the cleaned dataset).\n\nPerformance comparison with the state-of-the-art:\nTable <ref> provides a thorough comparison of the performance achieved by the top three contestants in the Physionet 2022 challenge, with the performance achieved by our NN models: 1D-CNN, LSTM-RNN, and CRNN. Table <ref> demonstrates that our 1D-CNN model outperforms all the three top contestants in terms of accuracy, weighted accuracy, F1-score and AUROC, for both experiments E1 (which does murmur detection on the original dataset) and E3 (which does murmur detection on the cleaned dataset). \n\n\n\n \n  \n   \n    \n\n\n\n\n\n \n  \n   \n    \n     \n      \n       \n        \n        \n        \n        \n        \n        \n        \n\n    \n    \n\n\n\n\n\n \u00a7.\u00a7 Results: Abnormal PCG detection using PCG 2016 dataset\n\n\nAs mentioned in the previous section, we re-train our 1D-CNN model from scratch on PCG 2016 dataset in order to differentiate between a normal PCG signal and an abnormal PCG signal. The hyper-parameters of the 1D-CNN that was fine-tuned for PCG 2016 dataset could be found in Table <ref>. Specifically, for the abnormal PCG detection problem, the model achieved 96.30% accuracy, and 98.17% AUROC (see Fig. <ref> and Table <ref>).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nThis work utilized two public datasets, i.e., CirCor Digiscope 2022 dataset and PCG 2016 datasets (from Physionet online database) in order to train three deep learning classifiers, i.e., 1D-CNN, LSTM-RNN, and C-RNN to do heart murmur detection as well as abnormal PCG detection. We observed that our 1D-CNN outperformed the other two NNs as well as the state-of-the-art, with an accuracy of 82.86%, weighted accuracy of 86.30%, and F1-score of 81.87% (for experiment E3 which does murmur detection after re-labeling of noise-only segments). \n\nWe note that the CirCor Digiscope 2022 dataset also contains other valuable information about the murmurs, e.g., murmur grading, timing, quality, pitch, shape etc. Thus, design of novel machine/deep learning algorithms which could do automatic and accurate murmur grading analysis with little data is an open problem of great interest. Furthermore, study of generative methods which could reliably generate synthetic PCG data in order to help train the data hungry deep learning methods is another interesting but challenging open problem (due to non-stationary nature of PCG signals with murmurs).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIEEEtran\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}