{"entry_id": "http://arxiv.org/abs/2303.06778v1", "published": "20230312233818", "title": "Sublevel Set Approximation in The Hausdorff and Volume Metric with Application to Path Planning and Obstacle Avoidance", "authors": ["Morgan Jones"], "primary_category": "math.OC", "categories": ["math.OC"], "text": "\n\n\n\nStudy of Multiuser Scheduling with Enhanced Greedy Techniques for Multicell and Cell-Free Massive MIMO Networks \n\n    Saeed Mashdour^\u22c6, Rodrigo C. de Lamare ^\u22c6,\u2020 and Jo\u00e3o P. S. H. Lima ^ \n  ^\u22c6 Centre for Telecommunications Studies, Pontifical Catholic University of Rio de Janeiro, Brazil \n\n^\u2020 Department of Electronic Engineering, University of York, United Kingdom \n\n^ CPqD, Campinas, Brazil \n\nsmashdour@gmail.com, delamare@cetuc.puc-rio.br, jsales@cpqd.com.br  \n\n\nThis work was supported by CNPq and CPqD.\n    \n==========================================================================================================================================================================================================================================================================================================================================================================================================\n\nplain\nplain\n\n\n\n\n\n\n\nUnder what circumstances does the \u201ccloseness\" of two functions  imply the \u201ccloseness\" of their respective sublevel sets? In this paper, we answer this question by showing that if a sequence of functions converges strictly from above/below to a function, V, in the L^\u221e (or L^1) norm then these functions yield a sequence sublevel sets that converge to the sublevel set of V with respect to the Hausdorff metric (or volume metric). Based on these theoretical results we propose Sum-of-Squares (SOS) numerical schemes for the  optimal outer/inner polynomial sublevel set approximation of various sets, including intersections and unions of semialgebraic sets, Minkowski sums, Pontryagin differences and discrete points. We present several numerical examples demonstrating the usefulness of our proposed algorithm \nincluding approximating sets of discrete points to solve machine learning binary classification problems and  approximating Minkowski sums to construct C-spaces for computing optimal collision-free paths for Dubin's car.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n \nThe notion of a set is a fundamental object of the modern mathematical toolkit, simply defined as a collection of elements. Sets are ubiquitous throughout control theory and are the language we use to represent frequently encountered concepts such as uncertainty\u00a0<cit.>, regions of attraction\u00a0<cit.>, reachable states\u00a0<cit.>, attractors\u00a0<cit.>, admissible inputs\u00a0<cit.>, feasible states\u00a0<cit.>, etc. Even for simple, low-dimensional problems, such sets can contain vast complexities and be numerically challenging to manipulate and analyze. Take the Lorenz attractor, for example. As modern engineering systems become increasingly complex, we can expect that this problem will be exacerbated as the sets we encounter become ever more unwieldy. In this context, we present several fundamental results for the approximation of sets, along with associated implementable numerical schemes based on Sum-of-Square (SOS) programming.\n\n\n\nFor some given set X \u2282^n, the goal of this paper is to find an outer or inner set approximation of X. Specifically, we would like to compute another set Y \u2282^n such that X \u2286 Y (outer approximation) or Y \u2286 X (inner approximation) and Y \u2248 X, where the approximation is defined with respect to some set metric. \n\nPrevious attempts at solving this set approximation problem were based on the fact that the volume of an ellipsoid, {x \u2208^n: x^\u22a4 A x <0 }, is proportional to (A^-1).  Then an equivalent optimization problem can be constructed for computing outer ellipsoid approximations by minimizing the convex objective function -log(A)\u00a0<cit.>. This approach of determinant maximization has been heuristically generalized to the problem of computing outer SOS polynomial sublevel approximations\u00a0<cit.>. Alternative heuristic matrix trace maximization schemes have also been proposed\u00a0<cit.>. More recently, in the special case of using star convex semialgebraic sets, an heuristic approach based on sublevel set scaling has been proposed in\u00a0<cit.>, demonstrating impressive numerical set approximation results based on SOS programming. An approach based on L^1 maximization, similar to some of the schemes proposed in this paper, has been proposed in\u00a0<cit.>. However, the work of \u00a0<cit.> only considers the specific problem of approximating Pontryagin differences and is still heuristic in the sense that convergence of the proposed numerical schemes is not shown. The only previous work that proposes a non-heuristic numerical method for set approximation is\u00a0<cit.>. In\u00a0<cit.> sets defined by quantifiers are approximated in the volume metric to arbitrary accuracy.  In this paper we extend\u00a0<cit.> to the volume metric approximation of more general sets, represented as sublevel sets of integrable functions. Moreover, we propose a new fundamental result for the approximation sets in the Hausdorff metric. Other works that have dealt with the problem of set approximation in the Hausdorff metric include the excellent work of\u00a0<cit.> that considered the problem of mollifying feasible constraints and helped to inspire the proof of the Hausdorff set approximations in this paper.\n\n\n\nThe main contributions of this paper is to show that when X \u2282^n can be written as a sublevel set of some function V the following holds:\n\n\t\n  * Approximating V in the L^\u221e norm from above by some function J yields a sublevel set Y={x \u2208\u039b : J(x)<\u03b3} that provides an arbitrarily accurate inner approximation of X in the Hausdorff metric.\n\t\t\n  * Approximating V in the L^1 norm from above by some function J yields a sublevel set Y={x \u2208\u039b : J(x)<\u03b3} that provides an arbitrarily accurate inner approximation of X in the volume metric.\n\nThe above sublevel set approximation results provide us with guiding principles for the design of numerical procedures for set approximation. Firstly, given a set, X, we must write X as a sublevel set of some function V. Fortunately, this requirement is not difficult to satisfy for many of the sets encountered in control theory and in this work we find V is explicitly for intersections and unions of semialgebraic sets, Minkowski sums, Pontryagin differences and discrete points. Secondly, given a function V we must approximate V from above. That is we must find a function J that minimizes ||V-J|| such that V(x) \u2264 J(x), where ||\u00b7|| is the L^1 or L^\u221e norm. By introducing auxiliary variables we show that this optimization problem can be lifted to a convex optimization problem and solved by tightening the inequality constraints to SOS constraints. Further to this we establish that we can approximate these sets in the Volume/Hausdorff metric by a polynomial sublevel set arbitrarily well by solving the resulting SOS optimization problems with sufficiently large polynomial degrees.\n\n\n\n\n\n \u00a7.\u00a7 Application to Path Planning and Obstacle Avoidance\n\nThe path planning of autonomous systems is the computational problem of finding the sequence of inputs that moves a given object from an initial condition to a target set while avoiding obstacles. Computing optimal paths is a well researched subject, with a broad range of practical uses ranging from the navigation of UAVs\u00a0<cit.> to the precise movements of robotic manipulators\u00a0<cit.>.\n\n Many popular algorithms for solving the path planning problem, such as A^* or Dijkstra, do not inherently account for the size and shape of the object. A common method for overcoming this problem is to transform the workspace of the problem to a Configuration-space (C-space), where in C-space the object is represented by a single point and obstacles are enlarged to account for the loss of size and shape of the object. The enlarged obstacles in C-space are given by the Minkowski sum of the sets representing the object and obstacles. In special convex cases the Minkowski sum can be found as a closed form solution\u00a0<cit.>, however,  unfortunately there is no analytical expression for the Minkowski sum of two general sets.\n\nIn the absence of an analytical expression for Minkowski sums we rely on numerical approximations. Numerical schemes have previously been proposed for the case of ellipsoid objects and convex objects\u00a0<cit.> and also for more general sets using SOS and log heuristics\u00a0<cit.>. In this paper we apply our proposed SOS based numerical scheme  for set approximation, with convergence guarantees, to the problem of approximating Minkowski sums in order to construct C-spaces. In this C-space we then apply the methods proposed in\u00a0<cit.> to compute the optimal input sequence that drives a Dubin's car to a target set in a minimum number of steps while avoiding obstacles.\n\n\n\n\n\n\u00a7 NOTATION\n \n\n\n\n\n\n\nFor A \u2282^n we denote the indicator function by 1_A : ^n \u2192 that is defined as 1_A(x) =     1  if  x \u2208 A\n    0  otherwise. For B \u2286^n,  \u03bc(B):=\u222b_^n1_B (x) dx is the Lebesgue measure of B. We denote the Hausdorff metric (given in Eq.\u00a0(<ref>)) by D_H and the volume metric (given in Eq.\u00a0(<ref>)) by D_V. We denote the power set of ^n, the set of all subsets of ^n, as P(^n)={X:X\u2282^n }. For two sets A,B \u2208^n we denote A/B= {x \u2208 A: x \u2209 B}. For x \u2208^n we denote ||x||_p= ( \u2211_i =1^n x_i^p )^1/p. For \u03b7>0 and y \u2208^n we denote the set B_\u03b7(y)= {x\u2208^n : ||x-y||_2< \u03b7}. We say f: \u03a9\u2192 is such that f \u2208 L^1(\u03a9,) if ||f||_L^1(\u03a9,):=\u222b_\u03a9 |f(x)| dx < \u221e. We define the L^\u221e norm as ||f||_L^\u221e(\u03a9,):=sup_x \u2208\u03a9 |f(x)|. We denote the space of polynomials p: ^n \u2192 by [x] and polynomials with degree at most d \u2208 by _d[x]. We say p \u2208_2d[x] is Sum-of-Squares (SOS) if there exists p_i \u2208_d[x] such that p(x) = \u2211_i=1^k (p_i(x))^2. We denote \u2211_SOS^d to be the set of SOS polynomials of at most degree d \u2208 and the set of all SOS polynomials as \u2211_SOS. For two sets A,B \u2282^n we denote the Minkowski sum by A \u2295 B (defined in Eq.\u00a0(<ref>)) and Pontryagin difference by A \u2296 B (defined in Eq.\u00a0(<ref>)). If M is a subspace of a vector space X we denote equivalence relation \u223c_M for x,y \u2208 X by x \u223c_M y if x-y \u2208 M. We denote quotient space by X  M:={{y \u2208 X: y \u223c_M x }: x \u2208 X}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 SUBLEVEL SET APPROXIMATION\n \nFor a given set X \u2282^n the goal of this paper is to solve the following problem,\n\n    Y^* \u2208inf_Y \u2208 C D_S(X,Y),\n\nwhere D_S: P(^n) \u00d7 P(^n) \u2192 [0, \u221e) is some set metric providing a notion of how close set Y^* is to X and C \u2282 P(^n) is the set of feasible sets.\n\nThe decision variables in Opt.\u00a0(<ref>) are sets, which are uncountable objects. This poses a challenge as it is difficult to search and hence optimize over sets effectively. To make such set optimization problems tractable, we need to find ways to parameterize our decision variables. The approach we take to overcome this problem is to consider problems where both our target set, X, and our decision variable, Y, are sublevel sets of functions, taking the following forms X={x \u2208\u039b: V(x)<\u03b3} and Y={x \u2208\u039b: J(x)< \u03b3}, where \u039b\u2282^n and \u03b3\u2208. Then, rather than attempting to directly solve the \u201cgeometric\" set optimiziation problem in Eq\u00a0(<ref>), we consider the following associated \u201calgebriac\" optimization problem, \n\n    J^* \u2208inf_J \u2208 F D_F(V,J),\n\n where D_F: L^1(^n , ) \u00d7 L^1(^n , ) \u2192 [0,\u221e) is some function metric providing a notion of the distance between the functions V and J and F is some finite dimensional function space.\n\nThe decision variables of Opt.\u00a0(<ref>) are functions and hence can be easily optimized over by parametrizing the decision variables using basis functions. We next turn our attention to the question of when does solving Opt.\u00a0(<ref>) yield a close solution to Opt.\u00a0(<ref>)? More specifically, if X={x \u2208\u039b: V(x)<  \u03b3}, Y={x \u2208\u039b: J(x)<  \u03b3} and J \u2248 V when is it true that X \u2248 Y? In the following subsections we answer this question by showing that, for a given function, V:^n \u2192, a compact set, \u039b\u2282^n, and a sequence of functions, {J_d}_d \u2208, the following hold:\n\n\t\n  * If J_d(x) \u2265 V(x) for all x \u2208\u039b and J_d \u2192 V as d \u2192\u221e in the L^\u221e(\u039b,^n) norm then for any \u03b3>0 we have that {x \u2208\u039b: J_d(x)< \u03b3}\u2192{x \u2208\u039b: V(x)< \u03b3} with respect to the Hausdorff metric (defined in Eq.\u00a0(<ref>)).\n\t\n  * If V(x) \u2264 J_d(x) for all  x \u2208\u039b  and J_d \u2192 V as d \u2192\u221e in the L^1(\u039b,^n) norm then for any \u03b3>0 we have that {x \u2208\u039b: J_d(x) < \u03b3}\u2192{x \u2208\u039b: V(x) < \u03b3} in the volume metric (defined in Eq.\u00a0(<ref>)). \n\n\n\n\n \u00a7.\u00a7 Sublevel Set Approximation In The Hausdorff Metric\n\nFor sets A,B \u2282^n, we denote the Hausdorff metric as D_H(A,B), defining\n\n    D_H(A,B):=max{ H(A,B), H(B,A)  } ,\n\nwhere H(A,B):=sup_x \u2208 A D(x,B) and D(x,B):=inf_y \u2208 B{x-y_2}.\n\n\nWe now show that uniform convergence of functions yields sublevel approximation in the Hausdorff metric.\n \n\tConsider a compact set \u039b\u2282^n, a function V : \u039b\u2192, and a family of functions {J_d }_d \u2208 that satisfies the following properties:\n\n\t\n  * For any d \u2208 we have V(x) \u2264 J_d(x) for all x \u2208\u039b.\n\t\n  * lim_d \u2192\u221esup_x \u2208\u039b |V(x) -J_d(x)| =0.\n\nThen for all \u03b3\u2208 we have that,\n\n    lim_d \u2192\u221e\tD_H ({x \u2208\u039b: J_d(x)< \u03b3},{x \u2208\u039b: V(x)< \u03b3})=0.\n\n\n Throughout this proof we use the following notation X_d:={x \u2208\u039b: J_d(x)< \u03b3} and X^*:={x \u2208\u039b: V(x)< \u03b3}. \n\t\nNow, recall from Eq.\u00a0(<ref>) that the Hausdorff metric for two sets  X_d, X^* \u2282^n is defined as the maximum of two terms, D_H(X_d,X^*):=max{ H(X_d,X^*), H(X^*,X_d)  }, where H(X_d,X^*):=sup_x \u2208 X_d D(x,X^*) and D(x,X^*):=inf_y \u2208 X^*{x-y_2}. This naturally leads us to split the remainder of the proof into two parts. In Part\u00a01 of the proof we show that the first term is such that H(X_d,X^*)=0 for all d \u2208. In Part\u00a02 of the proof we show that for all >0 there exists N \u2208 such that for all d>N the second term is such that H(X^*,X_d)<. Then Parts\u00a01 and\u00a02 of the proof can be used together to show Eq.\u00a0(<ref>), completing the proof.\n\t\n\tPart 1 of proof: In this part of the proof we show H( X_d,X^*)=0 for all d \u2208. \n\t\n\tSince V(x) \u2264 J_d(x) for all x \u2208\u039b it follows that X_d \u2286 X^* for any d \u2208. Thus for all d \u2208 and x \u2208  X_d we have that D(x,X^*)=0. Therefore it clearly follows H(X_d,X^*)=sup_x \u2208 X_d D(x,X^*)=0 for all d \u2208.\n\t\n\tPart 2 of proof: In this part of the proof we show that for all >0 there exists N \u2208 such that for all d>N we have H(X^*, X_d)<. For contradiction suppose the negation, that there exists \u03b4>0 such that\n\t\n    H(X^*, X_d) > \u03b4 for all  d \u2208.\n\n\tThen for each d \u2208 there exists x_d \u2208 X^* such that \n\t\n    D(x_d,X_d) \u2265\u03b4 for all  d \u2208.\n\n\n\t\n\t Now, X_d \u2286\u039b and \u039b is compact. Therefore the sequence {x_d}_d \u2208\u2286\u039b is bounded. Thus, by the Bolzano Weierstrass Theorem (Thm.\u00a0<ref> found in Appendix\u00a0<ref>), there exists a convergent subsequence {y_n}_n \u2208\u2286{x_d}_d \u2208. Let us denote the limit point of {y_n}_n \u2208 by y^* \u2208\u039b. \n\t \n\t Since {x_d}_d \u2208 satisfies Eq.\u00a0(<ref>) and  {y_n}_n \u2208\u2286{x_d}_d \u2208 it follows\n\t \n    D(y_n,X_n) \u2265\u03b4 for all  n \u2208.\n\n\t \n\t \n\t Moreover, because x_d \u2208 X^* for all d \u2208 and {y_n}_n \u2208\u2286{x_d}_d \u2208 it follows y_n \u2208 X^* for all n \u2208. Hence, _n:=\u03b3 - V(y_n)  >0 for all n \u2208. \n\t \n\t On the other hand, since lim_d \u2192\u221esup_x \u2208\u039b |V(x) -J_d(x)| =0 it follows for each n \u2208 there exists N_n \u2208 such that\n\t \n    J_d(y_n) - V(y_n)< _n  for all  d> N_n.\n\n\t Since _n := \u03b3 - V(y_n)  >0, it follows by Eq.\u00a0(<ref>) that\n\t \t \n    J_d(y_n) < \u03b3 for all  d> N_n,\n\n\t which implies y_n \u2208 X_d for all d> N_n. \n\t\n\t \n\t Now, since y_n \u2192 y^* there exists N \u2208 such that ||y_n - y^*||_2< \u03b4/4 for all n >N. Fixing n >N and selecting d>max{N,N_n} (as in Eq.\u00a0(<ref>) so y_n \u2208 X_d for d>N_n) we have that\n\t \n    D(y_d, X_d)= inf_x \u2208 X_d ||y_d - x ||_2 \n           \u2264 ||y_d - y_n||_2 \u2264 ||y_d - y^*||_2 + ||y_n - y^*||_2 \u2264\u03b4/2,\n\n\t contradicting Eq.\u00a0(<ref>). Thus it follows that for all >0 there exists N \u2208 such that for all d>N we have H(X^*, X_d)<. \n \n \n \tThe conditions that V(x) \u2264 J_d(x) and lim_d \u2192\u221esup_x \u2208\u039b |V(x) -J_d(x)| =0 in Thm.\u00a0<ref> cannot be relaxed. In Section\u00a0<ref> we have presented two counterexamples showing that if either of these conditions are relaxed then it is not necessary true that the sublevel sets of J_d converge to the sublevel set of V in the Hausdorff metric.\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n \u00a7.\u00a7 Sublevel Set Approximation in The Volume Metric\n \n\nFor Lebesgue measurable sets A,B \u2282^n, we denote the volume metric as D_V(A,B), where\n\n    D_V(A,B):=\u03bc( (A/B) \u222a (B/A) ),\n\nrecalling from Sec.\u00a0<ref> that we denote \u03bc(A) as the Lebesgue measure of the set A \u2282^n. \n\n\n \n\tConsider a Lebesgue measurable set \u039b\u2282^n, a function V \u2208 L^1(\u039b, ), and a family of functions {J_d \u2208 L^1(\u039b, ): d \u2208} that satisfies the following properties:\n\t\n\t\t\n  * For any d \u2208 we have V(x) \u2264 J_d(x) for all x \u2208\u039b.\n\t\t\n  * lim_d \u2192\u221e ||V -J_d||_L^1(\u039b, ) =0.\n\t\n\tThen for all \u03b3\u2208 we have that\n\t\n    lim_d \u2192\u221e\tD_V ({x \u2208\u039b: J_d(x) < \u03b3} , {x \u2208\u039b: V(x) < \u03b3}) =0.\n \n\n\n\t\tLet us denote \u1e7c(x)=-V(x) and J\u0303_d(x)= - J_d(x). It follows that J\u0303_d(x) \u2264\u1e7c(x) for all x \u2208\u039b and lim_d \u2192\u221e ||\u1e7c -J\u0303_d||_L^1(\u039b, ) =0. Therefore, by Prop.\u00a0<ref> (found in the appendix) it follows that for any \u03b3\u2208 we have that,\n\t\n    lim_d \u2192\u221e\tD_V ({x \u2208\u039b : \u1e7c(x) \u2264\u03b3}, {x \u2208\u039b : J\u0303_d(x) \u2264\u03b3}) =0.\n\n\t\n\tNow, \u039b={x \u2208\u039b : V(x) < \u03b3}\u222a{x \u2208\u039b : V(x) \u2265\u03b3} = {x \u2208\u039b : V(x) < \u03b3}\u222a{x \u2208\u039b : \u1e7c(x) \u2264 -\u03b3}. Therefore\n\t\n    {x \u2208\u039b : V(x) < \u03b3} = \u039b / {x \u2208\u039b : \u1e7c(x) \u2264 -\u03b3},\n\n\tand by a similar argument\n\t\n    {x \u2208\u039b : J_d(x) < \u03b3} = \u039b / {x \u2208\u039b : J\u0303_d(x) \u2264 -\u03b3}.\n\n\tThus, by Lem.\u00a0<ref> (found in the appendix) and since {x \u2208\u039b : J\u0303_d(x) \u2264\u03b3}\u2286\u039b, we have that\n\t\n    D_V ({x \u2208\u039b : V(x) < \u03b3}, {x \u2208\u039b : J_d(x) < \u03b3})\n        = D_V ( \u039b/ {x \u2208\u039b : \u1e7c(x) < -\u03b3}, \u039b/ {x \u2208\u039b : J\u0303_d(x) < -\u03b3}) \n        = D_V ({x \u2208\u039b : \u1e7c(x) \u2264 -\u03b3}, {x \u2208\u039b : J\u0303_d(x) \u2264 -\u03b3}).\n\n\tNow by Eqs\u00a0(<ref>) and\u00a0(<ref>) it follows that Eq.\u00a0(<ref>) holds.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\n\t\n\n\n\n\n\n\n\u00a7 SUBLEVEL SET APPROXIMATION USING SOS PROGRAMMING\n \n Given  some set X \u2282^n we would like to approximate, Theorems\u00a0<ref> and\u00a0<ref> illuminate the following steps we must take:\n\n\t\n  * Write the set X as a sublevel set of some function V.\n\t\n  * Approximate the function V by a uniformly bounding function in the L^\u221e norm or L^1 norm (depending whether or not the set approximation is required to be with respect to the Hausdorff or volume metric).\n\nStep 1 is always viable since by Prop.\u00a0<ref> (found in the appendix), it follows that for any compact set X \u2282^n there always exists a smooth function V such that X={x \u2208^n: V(x) \u2264 0}. However, for numerical implementation it will be necessary to have an analytical expression of such a function V. Later, in the following subsections, we will present several analytical expressions of V for various classes of sets including intersections and unions of semialgebraic sets, Minkowski sums, Pontryagin differences and discrete points. Before proceeding to these subsections we next briefly discuss the general approach we take to solving Step 2 in our strategy, approximating V by a uniformly bounding function in either the L^\u221e or L^1 norm.\n\n\n  \nAn optimization problem for L^\u221e approximation\n\nFor a given function V \u2208 C^\u221e(^n,) let us consider the problem of approximating V  uniformly from above in the L^\u221e norm, \n\n    J_d^* \u2208   inf_J_d \u2208_d[x]sup_x \u2208\u039b |V(x)-J_d(x)|\n       such that  V(x) \u2264 J_d(x)  for all  x \u2208\u039b.\n\n\n\nUnfortunately, the objective function of Opt.\u00a0(<ref>) , sup_x \u2208\u039b |V(x)-J_d(x)|, is not differentiable. This lack of differentiability causes problems later as our numerical implementation uses polynomial optimization to solve this problem. Fortunately, it is possible to lift the problem to a convex problem by introducing extra decision variables in the following way,\n\n    J_d^* \u2208   inf_J_d \u2208_d[x],\n    \tP_d \u2208_d[x], \u03b3\u2208\u03b3\n       such that  P_d(x) \u2264 V(x) \u2264 J_d(x)  for all  x \u2208\u039b,  \n        J_d(x)-P_d(x)< \u03b3 for all  x \u2208\u039b.\n\nNow, any solution, J_d^*, to Opt.\u00a0(<ref>) is feasible to Opt.\u00a0(<ref>) and minimizes the objective of Opt.\u00a0(<ref>) to a value of less than or equal to \u03b3. \n\n\n  \nAn optimization problem for L^1 approximation For a given function V \u2208 C^\u221e(^n,) let us consider the problem of approximating V  uniformly from above in the L^1 norm, \n\n    J_d^* \u2208   sup_J_d \u2208_d[x]\u222b_\u039b |J_d(x)-V(x)| dx \n       such that  V(x)  \u2264 J_d(x)   for all  x \u2208\u039b.\n\n\nUnfortunately, we have a similar problem as we did with Opt.\u00a0(<ref>), that the objective function of Opt.\u00a0(<ref>) is not differentiable and hence Opt.\u00a0(<ref>) is not readily solvable using polynomial optimization.  However, since the constraints of Opt.\u00a0(<ref>) enforce V(x) \u2264 J_d(x) it follows that \u222b_\u039b |J_d(x)-V(x)| dx= \u222b_\u039b J_d(x) dx- \u222b_\u039b V(x) dx. Since \u222b_\u039b V(x) dx is a constant it is equivalent to minimize \u222b_\u039b J_d(x) dx as it is to minimize \u222b_\u039b |J_d(x)-V(x)| dx. Hence, rather than solving Opt.\u00a0(<ref>) we solve, \n\n    J_d^* \u2208   inf_J_d \u2208_d[x]\u222b_\u039b J_d(x) dx \n       such that  V(x) \u2264 J_d(x)    for all  x \u2208\u039b.\n\nOpt.\u00a0(<ref>) only yields inner sublevel set approximations of {x \u2208\u039b: V(x)< \u03b3} since V(x) \u2264 J_d(x) implies {x \u2208\u039b: J_d(x) \u2264\u03b3}\u2286{x \u2208\u039b: V(x) \u2264\u03b3}. In contrast Opt.\u00a0(<ref>) yields both inner and outer sublevel set approximations since {x \u2208\u039b: J_d(x) \u2264\u03b3}\u2286{x \u2208\u039b: V(x) \u2264\u03b3}\u2286{x \u2208\u039b: P_d(x) \u2264\u03b3}. It is also useful to consider a similar optimization problem to Opt.\u00a0(<ref>) that yields outer sublevel set approximations\n\n    J_d^* \u2208   sup_J_d \u2208_d[x]\u222b_\u039b J(x) dx \n       such that  J_d(x) \u2264 V(x)    for all  x \u2208\u039b.\n\n\n\n  \nSOS implementation of L^\u221e and L^1 approximation\nTo solve Opts\u00a0(<ref>)\u00a0(<ref>) and\u00a0(<ref>) we tighten the problem by replacing all inequality constraints to SOS constraints. This tightening results in a SOS optimization problem that yields a single polynomial sublevel set approximation of several types of sets. Unfortunately, it is non-trivial to tighten Opts\u00a0(<ref>)\u00a0(<ref>) and\u00a0(<ref>) because, as we will see, for many set approximation problems the associated V is non-polynomial and hence we cannot directly constrain V(x) \u2264 J_d(x) or P_d(x) \u2264 V(x) using SOS. In the subsequent subsections we consider the following cases,\n\n\t\n  * V(x)=max_1 \u2264 i \u2264 m g_i(x) associated with intersections of semialgebraic sets (see Lemma\u00a0<ref>).\n\t\n  * V(x):=min_1 \u2264 i \u2264 m g_i(x) associated with intersections unions of semialgebraic sets (see Lemma\u00a0<ref>).\n\t\n  * V(x):=inf_w  \u2208{z \u2208\u039b: g_2(z) \u2264 0 } g_1(x-w) associated with intersections Minkowski sums (see Lemma\u00a0<ref>).\n\t\n  * V(x):=sup_w  \u2208{z \u2208\u039b: g_2(z) \u2264 0 } g_1(x+w) associated with intersections Pontryagin differences (see Lemma\u00a0<ref>).\n\t\n  * V(x):=1-1_{x_i}_i=1^N(x) associated with discrete points (see Lemma\u00a0<ref>).\n\nFor brevity we next only discuss the case of using SOS to constrain V(x) \u2264 J_d(x) or P_d(x) \u2264 V(x) when V(x)=max_1 \u2264 i \u2264 m g_i(x) and g_i \u2208[x] for each 1 \u2264 i \u2264 m. Other forms of V can also be constrained using a similar approach since, with the exception of discrete points, each V involves max/min operators. \n\nFor V(x)=max_1 \u2264 i \u2264 m g_i(x) it is straightforward to constrain V(x) \u2264 J_d(x) by enforcing g_i(x) \u2264 J_d(x) for 1 \u2264 i \u2264 m which tightens to the SOS constraint J_d-g_i \u2208\u2211_SOS for 1 \u2264 i \u2264 m. On the other hand it is slightly more difficult to constrain P_d(x) \u2264 V(x). This is because if we set P_d(x) \u2264 g_i(x) for all 1 \u2264 i \u2264 m then P_d(x) \u2264min_1 \u2264 i \u2264 m g_i(x) <max_1 \u2264 i \u2264 m g_i(x)=V(x) implying that P_d cannot be made arbitrarily close to V since |V(x)-P_d(x)| \u2265max_1 \u2264 i \u2264 m g_i(x)-min_1 \u2264 i \u2264 m g_i(x)>0.  To enforce P_d(x) \u2264 V(x) in a non-conservative manner we enforce\n\n    P_d(x) \u2264 g_i(x)  for  x \u2208{y\u2208\u039b: g_i(y)\u2265 g_j(y)  for all  i  j  }.\n\n This constraint is now a polynomial inequality over a semialgebraic set and therefore can be readily tightened to an SOS constraint.\n\n\n \u00a7.\u00a7 Approximation of Semialgebraic Sets\n \nIn this subsection we solve the problem of approximating semialgebraic sets, sets of the form X={x \u2208^n: g_i(x) < 0  for all  1 \u2264 i \u2264 m }, where g_i \u2208[x] for 1 \u2264 i \u2264 m. Throughout this section we will assume X \u2282^n is a compact set and hence WLOG we assume \n \n    X={x \u2208\u039b: g_i(x) < 0  for all  1 \u2264 i \u2264 m },\n\n where \u039b\u2282^n is some sufficiently large compact set and g_i \u2208[x] for 1 \u2264 i \u2264 m. \n \n In the following Lemma we show that semialgebraic sets can be written as a sublevel set of a single function.\n \n\tConsider X={x \u2208\u039b: g_i(x) < 0  for all  1 \u2264 i \u2264 m } then \n\n\t\n    X={x \u2208\u039b: V(x)<0 },\n\nwhere V(x):=max_1 \u2264 i \u2264 m g_i(x).\n\n\n\n\n\n\n\nSuppose y \u2208 X then g_i(y) < 0  for all  1 \u2264 i \u2264 m and hence V(y)=max_1 \u2264 i \u2264 m g_i(y)<0. Hence, y \u2208{x \u2208\u039b: V(x)<0 } implying that X \u2286{x \u2208\u039b: V(x)<0 }. On the other hand suppose y \u2208{x \u2208\u039b: V(x)<0 }. Then V(y)=max_1 \u2264 i \u2264 m g_i(y)<0 implying g_i(y) < 0  for all  1 \u2264 i \u2264 m and hence y \u2208 X. Therefore {x \u2208\u039b: V(x)<0 }\u2286  X. Hence {x \u2208\u039b: V(x)<0 }  =\t  X. \n\n\nIt is now clear from Lemma\u00a0<ref> and Theorems\u00a0<ref> and\u00a0<ref> that in order to approximate the set in Eq.\u00a0(<ref>) we must solve Opt.\u00a0(<ref>) or Opt.\u00a0(<ref>) for V(x)=max_1 \u2264 i \u2264 m g_i(x). \n\nWLOG we assume \u039b\u2282^n is a ball with sufficiently large radius, that is \u039b={x \u2208^n: ||x||_2<r}. We now propose the following SOS tightening of Opt.\u00a0(<ref>) for V(x):=max_1 \u2264 i \u2264 m g_i(x),\n\n    (J_d^*,P_d^*,\u03b3_d^*) \u2208inf_J_d \u2208_d[x],\n    \tP_d \u2208_d[x], \u03b3\u2208\u03b3     such that \n    \n        (g_i(x)   -  P_d(x))   -   s_i,1(x)(r^2   -   ||x||_2^2) \n        - \u2211_j=1^m   s_i,j,2(x)(g_i(x)   -  g_j(x)) \u2208\u2211_SOS^d   for all  1 \u2264 i \u2264 m, \n    \n        J_d(x)-g_i(x) - s_i,3(x)(r^2 - ||x||_2^2)  \u2208\u2211_SOS^d   for all  1 \u2264 i \u2264 m,  \n       \u03b3 -( J_d(x)-P_d(x)) -s_4(x)(r^2 - ||x||_2^2)  \u2208\u2211_SOS^d,\n        s_i,1(x)  \u2208\u2211_SOS^d,  s_i,j,2(x)  \u2208\u2211_SOS^d,   s_i,3(x)   \u2208\u2211_SOS^d,\n         s_4(x)   \u2208\u2211_SOS^d   for all  i,j   \u2208{1,...,m}.\n\n\n\nIn a similar way to how we tightened Opt.\u00a0(<ref>) to get Opt.\u00a0(<ref>) we next tighten Opt.\u00a0(<ref>) for V(x):=max_1 \u2264 i \u2264 m g_i(x) to get,\n\n    J_d^* \u2208   inf_J_d \u2208_d[x]\u222b_\u039b J_d(x) dx      such that \n        J_d(x) - g_i(x) - s_i(x)(r^2 - ||x||_2^2) \u2208\u2211_SOS^d    for  1 \u2264 i \u2264 m, \n         s_i(x) \u2208\u2211_SOS^d  for  1 \u2264 i \u2264 m.\n\n\n\n\n \n\n\n\n \u00a7.\u00a7 Approximation of Unions of Semialgebraic Sets\n \nIn this subsection we solve the problem of approximating the union of semialgebraic sets, \n\n    X= \u222a_i=1^m_1{x \u2208\u039b: g_i,j(x) < 0  for all  1 \u2264 j \u2264 m_2 },\n\n where g_i,j\u2208[x] for 1 \u2264 i \u2264 m_1 and 1 \u2264 j \u2264 m_2. For simplicity we will only consider the case m_2=1. \n \n\n \n \n \n \n \n  In the following Lemma we show that unions of semialgebraic sets can be written as a sublevel set of a single function.\n  \n\tConsider X=\u222a_i=1^m {x \u2208\u039b: g_i(x) < 0 } then \n\t\n    X={x \u2208\u039b: V(x)<0 },\n\n\twhere V(x):=min_1 \u2264 i \u2264 m g_i(x).\n\n\n\n\n\n\n\n\tFollows by a similar argument as Lem.\u00a0<ref>.\n\nTo approximate sets of the form given in Eq.\u00a0(<ref>) we now propose the following SOS tightening of Opt.\u00a0(<ref>) for V(x):=min_1 \u2264 i \u2264 m g_i(x), \n\n    (J_d^*,P_d^*,\u03b3_d^*) \u2208inf_J_d \u2208_d[x],\n    \t\tP_d \u2208_d[x], \u03b3\u2208\u03b3     such that \n        (g_i(x)   -  P_d(x))   -   s_i,1(x)(r^2   -   ||x||_2^2)   \u2208\u2211_SOS^d  for all  1 \u2264 i \u2264 m, \n        (J_d(x)-g_i(x)) - s_i,2(x)(r^2 - ||x||_2^2)  \n        - \u2211_j=1^m   s_i,j,3(x)(g_j(x)   -  g_i(x)) \u2208\u2211_SOS^d   for all  1 \u2264 i \u2264 m,  \n       \u03b3 -( J_d(x)-P_d(x)) -s_4(x)(r^2 - ||x||_2^2)  \u2208\u2211_SOS^d,\n        s_i,1(x)  \u2208\u2211_SOS^d,  s_i,2(x)   \u2208\u2211_SOS^d,  s_i,j,3(x)  \u2208\u2211_SOS^d,   ,\n        s_4(x) \u2208\u2211_SOS^d   for all  i,j   \u2208{1,...,m}.\n\n\n\nWe next propose the following SOS tightening of Opt.\u00a0(<ref>) for V(x):=min_1 \u2264 i \u2264 m g_i(x),\n\n    J_d^* \u2208   sup_J_d \u2208_d[x]\u222b_\u039b J_d(x) dx      such that \n        g_i(x) - J_d(x) - s_i(x)(r^2 - ||x||_2^2) \u2208\u2211_SOS^d    for  1 \u2264 i \u2264 m, \n         s_i (x)\u2208\u2211_SOS^d  for  1 \u2264 i \u2264 m.\n\n\n Note, solving Opt.\u00a0(<ref>) results in both a lower (P_d) and upper (J_d) approximation of V(x):=min_1 \u2264 i \u2264 m g_i(x), whereas, solving Opt.\u00a0(<ref>) only results in a lower approximation. Unfortunately, Theorems\u00a0<ref> and\u00a0<ref> only apply for upper function approximations and hence we are unable to use these theorems to show Opt.\u00a0(<ref>) yields an arbitrary accurate sublevel set approximation of the set given in Eq.\u00a0(<ref>). We could construct a similar SOS problem to Opt.\u00a0(<ref>) that results in a upper approximation in a non-conservative way using a similar argument as in Eq.\u00a0(<ref>) but this would result in more SOS decision variables. Alternatively, we will later show that Opt.\u00a0(<ref>) yields a sublevel approximation to the non-strict/relaxed version the set given of Eq.\u00a0(<ref>). That is, Opt.\u00a0(<ref>) can be used to approximate the following set,\n \n    X= \u222a_i=1^m{x \u2208\u039b: g_i(x) \u2264 0},\n\n where g_i \u2208[x] for 1 \u2264 i \u2264 m.\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Approximation of Minkowski Sums\n \n\n\nGiven two sets A,B \u2282^n their Minkowski sum is defined as\n\n    A \u2295 B = {a+b \u2208^n : a \u2208 A  and  b \u2208 B }.\n\n\nWe next consider the problem of approximating the set X=A \u2295 B, where A,B \u2282^n can be written as sublevel sets. That is we consider the problem of approximating the following,\n\n    X={x \u2208\u039b: g_1(x) \u2264 0 }\u2295{x \u2208\u039b: g_2(x) \u2264 0 },\n\nwhere g_1,g_2 \u2208 C^\u221e. Note that this is non-restrictive since Prop.\u00a0<ref> shows any compact set can be written as a sublevel set and Lemmas\u00a0<ref> and\u00a0<ref> give this sublevel set in an analytical form for intersections or unions of semialgebraic sets. \n\nIn the following lemma we show sets satisfying Eq.\u00a0(<ref>) can be written as the sublevel set of a single function. \n \n\tConsider X:={x \u2208\u039b: g_1(x) \u2264 0 }\u2295{x \u2208\u039b: g_2(x) \u2264 0 }, where \u039b\u2282^n is a compact set and g_1,g_2 \u2208 C(\u039b,), then \n\t\n    X={x \u2208\u039b: V(x) \u2264 0 },\n\n\twhere V(x):=inf_w  \u2208{z \u2208\u039b: g_2(z) \u2264 0 } g_1(x-w).\n\n\n\n\n\n\n\n\tSuppose x \u2208 X then there exists z_1 \u2208{x \u2208\u039b: g_1(x) \u2264 0 } and z_2 \u2208{x \u2208\u039b: g_2(x) \u2264 0 } such that x =z_1+z_2. Hence,\n\t\n    V(x)= inf_w  \u2208{z \u2208\u039b: g_2(z) \u2264 0 } g_1(x-w) \u2264 g_1(x-z_2)= g_1(z_1) \u2264 0,\n\nsince z_1 \u2208{x \u2208\u039b: g_1(x) \u2264 0 }. Therefore, x \u2208{y \u2208\u039b: V(y) \u2264 0 } implying X \u2286{x \u2208\u039b: V(x) \u2264 0 }.\n\nOn the other hand suppose x \u2208{y \u2208\u039b: V(y) \u2264 0 }. Note that by Lem.\u00a0<ref> (found in the appendix) it follows that {z \u2208\u039b: g_2(z) \u2264 0 } is compact. Now, by the extreme value theorem, continuous functions attain their extreme values over compact sets implying that there exists z_2 \u2208inf_w  \u2208{z \u2208\u039b: g_2(z) \u2264 0 } g_1(x-w) such that g_1(x-z_2)=V(x). Let z_1=x-z_2. Because V(x) \u2264 0 it follows that g_1(z_1)= g_1(x-z_2)=V(x) \u2264 0. Hence, z_1 \u2208{x \u2208\u039b: g_1(x)  \u2264 0 }. Therefore it follows that x \u2208 X and hence X \u2282{y \u2208\u039b: V(y) \u2264 0 }. Thus X={x \u2208\u039b: V(x)  \u2264 0 }.\n\nFor simplicity we next consider the special case of approximating A \u2295 B where A is a union of sublevel sets and B is a semialgebraic set. That is we consider the problem of approximating the following set,\n\n    X:=   \u222a_i=1^m_1{x \u2208\u039b: g_1,i(x) \u2264 0 }\n               \u2295{x \u2208\u039b: g_2,j(x) \u2264 0  for  1 \u2264 j \u2264 m_2 },\n\nwhere g_1,i, g_2,j\u2208[x] for all 1 \u2264 i \u2264 m_1 and 1 \u2264 j \u2264 m_2. Other cases where one or both of A or B is a union or intersection of semialgebraic sets can be handled by a similar methodology. Now, by similar arguments to Lemmas\u00a0<ref> and\u00a0<ref> and the use of Lemma\u00a0<ref> it is clear that X, given in Eq.\u00a0(<ref>), is such that X={x \u2208\u039b: V(x) \u2264 0 } where\n\n    V(x)=inf_w  \u2208{z \u2208\u039b: max_1 \u2264 j \u2264 m_2 g_2,j(z) \u2264 0 }min_1 \u2264 i \u2264 m_1 g_1,i(x-w).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\nUnlike in Subsections\u00a0<ref> and\u00a0<ref>, in this subsection we do not provide a SOS tightening of Opt.\u00a0(<ref>) due to the complexity of using SOS to enforce the constraint V(x)\u2264 J_d(x) when V is given in Eq.\u00a0(<ref>). Indeed, we could increase the problems dimension and enforce,\n\n    J_d(x) - g_1,i(x-w) - s_i,0(x,u,w)(g_1,i(x-u) - g_1,i(x-w)) \n        -\u2211_j=1^m_2 s_i,j,1(x,w,u)(g_1,j(x-w) -g_1,i(x-w))  \n        + \u2211_j=1^m_2 s_i,j,2(x,w,u)g_2,j(w)+ \u2211_j=1^m_2 s_i,j,3(x,w,u)g_2,j(u) \n        - s_i,4(x,w,u)(r^2 - ||(x,u,w)||_2^2) \u2208\u2211_SOS for  1 \u2264 i \u2264 m_1,\n        s_i,0,s_i,j,1, s_i,j,2, s_i,j,3, s_i,4\u2208\u2211_SOS for  1 \u2264 i \u2264 m_1 & 1 \u2264 j\u2264 m_2\n \n\nHowever, this approach results in a very large SOS optimization problem. Fortunately, it is relatively easy to enforce the constraint J_d(x) \u2264 V(x). This is the only constraint that is required to be tightened in Opt.\u00a0(<ref>). Thus, to approximate sets of the form given in Eq.\u00a0(<ref>) we now propose the following SOS tightening of Opt.\u00a0(<ref>) for V(x):=inf_w  \u2208{z \u2208\u039b: max_1 \u2264 j \u2264 m_2 g_2,j(z) \u2264 0 }min_1 \u2264 i \u2264 m_1 g_1,i(x-w),\n\n    J_d^* \u2208   sup_J_d \u2208_d[x]\u222b_\u039b J_d(x) dx      such that \n        g_1,i(x-w) - J_d(x) - s_i,1(x,w)(r^2 - ||(x,w)||_2^2)\n            +\u2211_j=1^m_2 s_i,j,2(x,w)g_2,j(w)  \u2208\u2211_SOS^d    for  1 \u2264 i \u2264 m_1, \n         s_i,1(x) \u2208\u2211_SOS^d  for  1 \u2264 i \u2264 m_1, \n        s_i,j,2(x) \u2208\u2211_SOS^d  for  1 \u2264 i \u2264 m_1  and  1 \u2264 j \u2264 m_2.\n\n\n\n\n \u00a7.\u00a7 Approximation of Pontryagin Differences\n \n\n\n\tGiven two sets A,B \u2282^n their Pontryagin difference is defined as\n\t\n    A \u2296 B = {a \u2208 A: a+b \u2208 A  for all  b \u2208 B   }.\n\n\nWe next show that the Pontryagin difference of two sublevel sets can be written as a sublevel set of a single function.\n \n\tConsider X:={x \u2208\u039b: g_1(x) < 0 }\u2296{x \u2208\u039b: g_2(x) \u2264 0 }, where \u039b\u2282^n is a compact set and g_1,g_2 \u2208 C(\u039b,), then \n\t\n    X={x \u2208\u039b: V(x) < 0 },\n\n\twhere V(x):=sup_w  \u2208{z \u2208\u039b: g_2(z) \u2264 0 } g_1(x+w).\n\t\n\t\n\t\n\t\t\n\t\t\n\n\n\tWe first show Eq.\u00a0(<ref>). Suppose x \u2208 X then it must follow that g_1(x+w)<0 for all w \u2208{z \u2208\u039b: g_2(z) \u2264 0 }. Note that by Lem.\u00a0<ref> (found in the appendix) it follows that {z \u2208\u039b: g_2(z) \u2264 0 } is compact. Now, by the extreme value theorem continuous functions attain their extreme values over compact sets implying that there exists w^*  \u2208{z \u2208\u039b: g_2(z) \u2264 0 } such that g_1(x+w^*)=sup_w  \u2208{z \u2208\u039b: g_2(z) \u2264 0 } g_1(x+w). Hence V(x)=sup_w  \u2208{z \u2208\u039b: g_2(z) \u2264 0 } g_1(x+w)=g_1(x+w^*)<0 implying that x \u2208{z \u2208\u039b: V(z) < 0 } and hence X \u2286{x \u2208\u039b: V(x) < 0 }.\n\t\n\tOn the other hand if x \u2208{z \u2208\u039b: V(z) < 0 } it follows that sup_w  \u2208{z \u2208\u039b: g_2(z) \u2264 0 } g_1(x+w)<0. Hence, g_1(x+w) \u2264sup_w  \u2208{z \u2208\u039b: g_2(z) \u2264 0 } g_1(x+w) < 0 for all w  \u2208{z \u2208\u039b: g_2(z) \u2264 0 } implying x+w \u2208{x \u2208\u039b: g_1(x) <\t 0 } for all w  \u2208{z \u2208\u039b: g_2(z) \u2264 0 } . Thus it follows that x \u2208 X.\n\t\n\t\n\n\nFor simplicity we consider the special case of approximating A \u2296 B where both A and B are semialgebraic sets. Other cases where one or both of A or B is a union or intersection of semialgebraic sets can be handled by a similar methodology. That is we consider the problem of approximating the following set,\n\n    X:=   {x \u2208\u039b: g_1,i(x) < 0  for  1 \u2264 i \u2264 m_1 }\n               \u2296{x \u2208\u039b: g_2,j(x) \u2264 0  for  1 \u2264 j \u2264 m_2 },\n \nwhere g_1,i, g_2,j\u2208[x] for all 1 \u2264 i \u2264 m_1 and 1 \u2264 j \u2264 m_2. Other cases where one or both of A or B is a union or intersection of semialgebraic sets can be handled by a similar methodology. Now, by Lemmas\u00a0<ref> and\u00a0<ref> it is clear that X, given in Eq.\u00a0(<ref>), is such that X={x \u2208\u039b: V(x) < 0 } where\n\n    V(x)=sup_w  \u2208{z \u2208\u039b: max_1 \u2264 j \u2264 m_2 g_2,j(z) \u2264 0 }max_1 \u2264 i \u2264 m_1 g_1,i(x+w).\n\nSimilarly to Subsection\u00a0<ref>, we do not provide a tightening of the Opt.\u00a0(<ref>) due to the complexity of using SOS to enforce the constraint J_d(x) \u2264 V(x) when V is given in Eq.\u00a0(<ref>) \n\nTo approximate sets given in Eq.\u00a0(<ref>) we now propose the following SOS tightening of Opt.\u00a0(<ref>) for V(x):=sup_w  \u2208{z \u2208\u039b: max_1 \u2264 j \u2264 m_2 g_2,j(z) \u2264 0 }max_1 \u2264 i \u2264 m_1 g_1,i(x+w),\n\t\n    J_d^* \u2208   inf_J_d \u2208_d[x]\u222b_\u039b J_d(x) dx      such that \n        J_d(x)- g_1,i(x+w)  - s_i,1(x,w)(r^2 - ||(x,w)||_2^2)\n            +\u2211_j=1^m_2 s_i,j,2(x,w)g_2,j(w)  \u2208\u2211_SOS^d    for  1 \u2264 i \u2264 m_1, \n         s_i,1\u2208\u2211_SOS^d  for  1 \u2264 i \u2264 m_1, \n        s_i,j,2\u2208\u2211_SOS^d  for  1 \u2264 i \u2264 m_1  and  1 \u2264 j \u2264 m_2.\n\n\n\t\n\n\n\n \u00a7.\u00a7 Approximation of Discrete Points\n \nIn this section we consider the problem of approximating a set of discrete points, {x_i}_i=1^N \u2282^n. We first note that it is possible to write a set of discrete points as a sublevel set of a polynomial without any computation by considering the function V(x)= \u220f_i=1^N (x_i - x)^2. However, this polynomial function has a very large degree, double the number of data points. We next  show that sets of discrete points can be written as a sublevel set of a simpler function. This will allow us to derive an SOS optimization problem to approximate this simple function by a polynomial with relatively small degree, yielding a low degree polynomial sublevel approximation of {x_i}_i=1^N \u2282^n.\n \nConsider X={x_i}_i=1^N \u2282\u039b. Then\n\n    X={x \u2208\u039b: V(x) \u2264 0 },\n\nwhere V(x)=1-1_{x_i}_i=1^N(x).\n\n\n\tFollows trivially.\n\n\n\nWe do not provide an SOS tightening for Opt.\u00a0(<ref>) when V(x)=1-1_{x_i}_i=1^N(x) because in order for us to prove that the resulting polynomial sublevel set approximation can be arbitrarily small with respect to the Hausdorff metric we rely on Thm.\u00a0<ref>. Thm.\u00a0<ref> requires that our polynomial approximations of V converge in the L^\u221e norm. Unfortunately, this V is not continuous so it is not possible to approximate it uniformly using smooth functions like polynomials. Fortunately, V is an integrable function so it is possible to approximate it by a polynomial in the L^1 norm using the following SOS tightening of Opt.\u00a0(<ref>),\n\n    J_d^* \u2208   sup_J_d \u2208_d[x]\u222b_\u039b J_d(x) dx      such that \n        J_d(x_i) <0   for  1 \u2264 i \u2264 N, \n         1-J_d(x) - s_0(x)(r^2 -||x||_2^2) \u2208\u2211_SOS^d,    s_0(x) \u2208\u2211_SOS^d.\n\n\n\n\n\n\n\n\u00a7 CONVERGENCE OF OUR PROPOSED SOS PROGRAMS\n \nIn the previous sections we proposed several SOS optimization problems to approximate various functions, V, whose sublevel sets yield several important classes of sets (intersections and unions of semialgebraic sets, Minkowski sums, Pontryagin differences and discrete points). In this section we use Theorems\u00a0<ref> and\u00a0<ref> to prove that the sequences of solutions to our SOS problems produce sequences of sublevel sets that each converge to the associated target set in the Hausdorff or volume metric. We begin this section with showing convergence in the Hausdorff metric. \n\n \n\tIt holds that,\n\t\t\n    lim_d \u2192\u221e\tD_H ({x \u2208\u039b: J_d^*(x)<0} , X ) =0,\n               {x \u2208\u039b: J_d^*(x)<0}\u2286 X,\n\nwhere \u039b:={x \u2208^n: ||x||_2 \u2264 r} and the set X and the sequence of functions {J_d^*}_d \u2208\u2282[x] satisfy either of the following:\n\t\n\t\t\n  * X is a semialgebraic set, given by Eq.\u00a0(<ref>), and {J_d^*}_d \u2208\u2282[x] solve the family of d-degree SOS optimization problems given in Eq.\u00a0(<ref>).\n\t\t\n  * X is a union of semialgebraic sets given by Eq.\u00a0(<ref>) and {J_d^*}_d \u2208\u2282[x] solve the family of d-degree SOS optimization problems given in Eq.\u00a0(<ref>).\n\t\n\n\n\n\n\n\n\n\n\n\n\tConvergence to semialgebraic sets:\tWe first prove the first case where X is a semialgebraic set, given by Eq.\u00a0(<ref>), and {J_d^*}_d \u2208\u2282[x] solve the family of d-degree SOS optimization problems given in Eq.\u00a0(<ref>).\n\t\nNow, it follows that if we show that\n\t\n    J_d^*(x) \u2265 V(x)  for all  x \u2208\u039b,\n    lim_d \u2192\u221e||J_d^*-V||_L^\u221e(\u039b,)=0,\n\n\twhere V(x):=max_1 \u2264 i \u2264 m g_i(x), then Eqs\u00a0(<ref>) and\u00a0(<ref>) hold and the proof is complete. This is because Lem.\u00a0<ref> shows that X={x \u2208\u039b: V(x)<0} and therefore if Eq.\u00a0(<ref>) holds then clearly Eq.\u00a0(<ref>) must hold. Moreover, if Eqs\u00a0(<ref>) and\u00a0(<ref>) hold then by Theorem\u00a0<ref> it must follow that lim_d \u2192\u221e\tD_H ({x \u2208\u039b: J_d^*(x)<0} , {x \u2208\u039b: V(x)<0} ). It is then clear Eq.\u00a0(<ref>) holds.\n\t\n\tWe first show Eq.\u00a0(<ref>). Because J_d^* solves Opt.\u00a0(<ref>) it follows that it must be feasible to Opt.\u00a0(<ref>) and hence satisfies each constraint. In particular there must exist SOS variables s_2,i\u2208\u2211_SOS^d such that \n\t\n    J_d^*(x) - g_i(x) -s_2,i(x)(r^2 - ||x||_2^2) \u2208\u2211_SOS^d  for  1 \u2264 i \u2264 m.\n\n\tHence, it follows by Eq.\u00a0(<ref>) and the positivity of SOS polynomials that J_d^*(x) \u2265 g_i(x) for all x \u2208\u039b and 1\u2264 i \u2264 m. Therefore J_d^*(x) \u2265max_1 \u2264 i \u2264 m g_i(x)=V(x) and thus Eq.\u00a0(<ref>) holds. \n\t\n\tWe next show that Eq.\u00a0(<ref>) holds. Let >0, by Cor.\u00a0<ref>, found in Appendix\u00a0<ref>, there exists H_1,H_2 \u2208[x] such that, \n\t\n    sup_ x \u2208\u039b |V(x) - H_j(x) | < /4 for  j \u2208{1,2}, \n        H_1(x) > g_i(x)  for all  x \u2208 B_r(0)  and  1 \u2264 i \u2264 m,\n        H_2(x) < g_i(x)  for all  x \u2208 B_r(0) \u2229Y\u0305_\u0305i\u0305 and  1 \u2264 i \u2264 m,\n\n\twhere  Y\u0305_\u0305i\u0305 :={y \u2208\u039b: g_i(y) \u2265 g_j(y)  for  1 \u2264 j \u2264 m  }. \n\t\n\tSince H_1(x)-g_i(x) >0 for all x \u2208 B_r(0) and 1 \u2264 i \u2264 m and also since B_r(0) is compact semialgebraic set it follows by Thm.\u00a0<ref> that there exists SOS polynomials {s_2,i}_i \u2208{1,..,m}\u2282\u2211_SOS and s_0,1\t\u2208\u2211_SOS such that,\n\t\n    H_1(x)-g_i(x)-s_2,i(x)(r^2 - ||x||_2^2)=s_0,1\t(x).\n\n\tMoreover, by a similar argument, since g_i(x)- H_2(x) >0 for all x \u2208  B_r(0) \u2229Y\u0305_\u0305i\u0305 and 1 \u2264 i \u2264 m and also since B_r(0)\u2229Y\u0305_\u0305i\u0305 is compact semialgebraic set it follows by Thm.\u00a0<ref> that there exists SOS polynomials {s_1,i,j}_(i,j) \u2208{1,..,m}\u00d7{1,..,m}\u2282\u2211_SOS, {s_1,i}_i \u2208{1,..,m}\u2282\u2211_SOS  and s_0,2\t\u2208\u2211_SOS such that,\n\t\n    g_i(x)-H_2(x) -s_1,i(x)     (r^2 - ||x||_2^2) \n        -\u2211_j=1^m s_1,i,j(x)(g_i(x)-g_j(x))     =s_0,2(x).\n\n\tNow clearly, H_2(x)< max_1 \u2264 i \u2264 m g_i(x) =V(x)< H_1(x) for all x \u2208  B_r(0) implying\n\t\n    \u03b3_H -(H_2(x) - H_1(x)) \u2265 (H_2(x) - H_1(x)) \n        = (H_2(x)-V(x))+ (V(x) - H_1(x))>0  for all  x \u2208 B_r(0),\n\n\twhere \u03b3_H :=2 sup_x \u2208 B_r(0){H_2(x) - H_1(x)}. Therefore by Thm.\u00a0<ref> there exists SOS polynomials s_3\u2208\u2211_SOS and and s_0,3\t\u2208\u2211_SOS such that,\n\t\n    \u03b3_H -(H_2(x) - H_1(x)) -s_3(x)(r^2 - ||x||_2^2) =s_0,3(x).\n\n\tFurthermore, by Eq.\u00a0(<ref>) it follows that\n\t\n    \u03b3_H\u2264  2sup_x \u2208 B_r(0)|H_2(x)-V(x)| +   2sup_x \u2208 B_r(0)|V(x)-H_1(x)| < .\n\n\t\n\tNow, let d_0:=max{ deg(H_1),deg(H_2), max_i,j \u2208{1,...,m}{s_1,i,j}, max_i \u2208{1,...,m}{s_2,i}, s_3, max_i \u2208{1,2,3}{s_0,i}}. It then follows by Eqs\u00a0(<ref>), (<ref>) and\u00a0(<ref>) that (H_2,H_1,\u03b3_H) is a feasible solution to Opt.\u00a0(<ref>) for degree d \u2265 d_0. Hence, the value of the objective resulting from the optimal solutions, (J_d^*,P_d^*, \u03b3_d^*) for d \u2265 d_0, will be less than or equal to the value of the objective function resulting from the feasible solution (H_2,H_1,\u03b3_H). Therefore by Eq.\u00a0(<ref>) we have that,\n\t\n    \u03b3^*_d\u2264\u03b3_H< for all  d \u2265 d_0.\n\n\t\n\tNow, since (J_d^*,P_d^*, \u03b3_d^*) is the optimal solution to Opt.\u00a0(<ref>) it satisfies all of the constraints of Opt.\u00a0(<ref>). Since SOS polynomials are non-negative it then follows\n\t\n    J^*_d(x)   \u2265 g_i(x)  for all  x \u2208 B_r(0)  and  1 \u2264 i \u2264 m, \n    \n    \t\tP^*_d(x)    \u2264 g_i(x)  for all  x \u2208 B_r(0) \u2229Y\u0305_\u0305i\u0305 and  1 \u2264 i \u2264 m,\n    \n    \t\tJ^*_d(x)- P^*_d(x)    \u2264\u03b3_d^*  for all  x \u2208 B_r(0),\n\n\timplying P^*_d(x) \u2264 V(x) \u2264 J^*_d(x) for all B_r(0) and hence \n\t\n    |J^*_d(x) - V(x)|     = J^*_d(x) - V(x) \u2264 J^*_d(x) - P^*_d(x)\n       \u2264\u03b3_d^*  for all  x \u2208 B_r(0).\n\n\tTherefore, by Eqs\u00a0(<ref>) and\u00a0(<ref>) it follows that\n\t\n    sup_x \u2208 B_r(0)\t|J^*_d(x) - V(x)|\u2264\u03b3_d^*<  for  d \u2265 d_0.\n\n\tNow, >0 was arbitrarily chosen and thus Eq.\u00a0(<ref>) shows Eq.\u00a0(<ref>), completing the proof.\n\t\n\t\tConvergence to unions of semialgebraic sets: Consider the case when X is given by Eq.\u00a0(<ref>) and {J_d^*}_d \u2208\u2282[x] is given in Eq.\u00a0(<ref>). Then  Eqs\u00a0(<ref>) and\u00a0(<ref>) hold by a similar argument to the proof of convergence to to semialgebraic sets, where instead of showing lim_d \u2192\u221e||J_d^*-max_1 \u2264 i \u2264 m g_i||_L^\u221e(\u039b,)=0 (as in Eq.\u00a0(<ref>)) we show lim_d \u2192\u221e||J_d^*-min_1 \u2264 i \u2264 m g_i||_L^\u221e(\u039b,)=0.\n\n\nNote, the solutions, {J_d}_d \u2208, to Opts\u00a0(<ref>) and\u00a0(<ref>) provide inner sublevel set approximations as shown in Eq.\u00a0(<ref>). However, it is still possible to construct an outer sublevel set approximation using P_d^* since in both cases P_d^*(x) \u2264 V(x) and lim_d \u2192\u221e ||P_d- V||_L^\u221e(\u039b,)=0. Unfortunately, as shown in Counterexample\u00a0<ref> (found in the appendix), it is not necessarily true that {x \u2208\u039b: P^*_d(x)<\u03b3}\u2192{x \u2208\u039b: V(x)<\u03b3} in the Hausdorff metric. Fortunately, it is fairly straight forward to show {x \u2208\u039b: P^*_d(x) \u2264\u03b3}\u2192{x \u2208\u039b: V(x) \u2264\u03b3} in the volume metric using Prop.\u00a0<ref>.\n\n\n\nWe next use Theorem\u00a0<ref> to show that the SOS programs we proposed in Section\u00a0<ref>, that approximate in the L^1 norm from above, yield polynomial sublevel sets that converge in the volume metric to semialgebraic sets or Pontryagin differences.\n \n\tIt holds that,\n\t\n    lim_d \u2192\u221e\tD_V ({x \u2208\u039b: J_d^*(x) < 0} , X ) =0, \n               {x \u2208\u039b: J_d^*(x)<0}\u2286 X,\n\n\twhere \u039b:={x \u2208^n: ||x||_2 \u2264 r} and the set X and the sequence of solutions {J_d^*}_d \u2208\u2282[x] satisfy either of the following:\n\t\n\t\t\n  * X is a semialgebraic set, given by Eq.\u00a0(<ref>), and {J_d^*}_d \u2208\u2282[x] is given in Eq.\u00a0(<ref>).\n\t\t\n  * X is a Pontryagin difference, given by Eq.\u00a0(<ref>), and {J_d^*}_d \u2208\u2282[x] is given in Eq.\u00a0(<ref>).\n\t\n\n\nConvergence to semialgebraic sets:\tWe first show Eqs\u00a0(<ref>) and\u00a0(<ref>) hols in the case X is a semialgebraic set, given by Eq.\u00a0(<ref>), and {J_d^*}_d \u2208\u2282[x] is given in Eq.\u00a0(<ref>).\n\n\n Now, it follows that if we show that\n\t\n    J_d^*(x) \u2265 V(x)  for all  x \u2208\u039b,\n    lim_d \u2192\u221e||J_d^*-V||_L^1(\u039b,)=0,\n\n\twhere V(x):=max_1 \u2264 i \u2264 m g_i(x), then Eqs\u00a0(<ref>) and\u00a0(<ref>) hold and the proof is complete. This is because by Lem.\u00a0<ref> we have that X={x \u2208\u039b: V(x)<0}. Then if Eqs\u00a0(<ref>) and\u00a0(<ref>) hold then clearly Eq.\u00a0(<ref>) is satisfied and, by Thm.\u00a0<ref>, it must follow that lim_d \u2192\u221e\tD_V({x \u2208\u039b: J_d^*(x)<0} , {x \u2208\u039b: V(x)<0} ). It is then clear Eq.\u00a0(<ref>) holds.\n\t\n\tWe first show Eq.\u00a0(<ref>). Because J_d^* solves Opt.\u00a0(<ref>) it follows that it must be feasible to Opt.\u00a0(<ref>) and hence satisfies each constraint. In particular there must exist SOS variables s_i\u2208\u2211_SOS^d such that \n\t\n    J_d^*(x) - g_i(x) -s_i(x)(r^2 - ||x||_2^2) \u2208\u2211_SOS^d  for  1 \u2264 i \u2264 m.\n\n\tHence, it follows by Eq.\u00a0(<ref>) and the positivity of SOS polynomials that J_d^*(x) \u2265 g_i(x) for all x \u2208\u039b and 1\u2264 i \u2264 m. Therefore J_d^*(x) \u2265max_1 \u2264 i \u2264 m g_i(x)=V(x) and thus Eq.\u00a0(<ref>) holds. \n\t\n\tWe next show Eq.\u00a0(<ref>). Consider the sequence of functions, {G_d^*}_d \u2208\u2282[x], that solve the family of d-degree SOS optimization problems given in Eq.\u00a0(<ref>). This sequence of functions is also feasible to Opt.\u00a0(<ref>). Therefore the value of the objective function of Opt.\u00a0(<ref>) resulting from G_d^* will be greater than the optimal solution J_d^*. That is\n\t\n    \u222b_\u039b J_d^*(x) dx \u2264\u222b_\u039b G_d^*(x) dx.\n\n\tMoreover, by Eq.\u00a0(<ref>), in the proof of Thm.\u00a0<ref>, we have that \n\t\n    lim_d \u2192\u221e||G_d^*-V||_L^\u221e(B_r(0),)=0.\n\n\tHence,\n\t\n    lim_d \u2192\u221e\u222b_\u039b |V(x) - J_d^*(x)| dx = lim_d \u2192\u221e\u222b_\u039b V(x) - J_d^*(x) dx\n       \u2264lim_d \u2192\u221e\u222b_\u039b V(x) - G_d^*(x) dx \u2264lim_d \u2192\u221e\u03bc(\u039b) ||V-G_d||_L^1(B_r(0),)=0.\n\n\tTherefore, Eq.\u00a0(<ref>) is shown completing the proof.\n\t\n\t\n\t\t\t\n\t\t\t\t\tConvergence to Pontryagin differences: \tConsider the case where X is given by Eq.\u00a0(<ref>), and {J_d^*}_d \u2208\u2282[x] is given in Eq.\u00a0(<ref>). Note, in Eq.\u00a0(<ref>) that it was shown that X={x \u2208\u039b: V(x) < 0 }, where \n    V(x):=sup_w  \u2208{z \u2208\u039b: max_1 \u2264 i \u2264 m_2 g_2,i(z) \u2264 0 }max_1 \u2264 i \u2264 m_1 g_1,i(x-w).\n\n\t\t\t\t\t\n\t\t\t\t\tThe rest of the proof follows by a similar argument to the convergence to semialgebraic sets proof, where we use Cor.\u00a0<ref> (found in the appendix) to approximate V uniformly from above by a polynomial. We then show that this polynomial approximation is feasible to Opt.\u00a0(<ref>) for sufficiently large d \u2208 using Theorem\u00a0<ref> (found in the appendix). Because this feasible solution to Opt.\u00a0(<ref>) is arbitrarily close to V, it can then be shown that the true solution of Opt.\u00a0(<ref>) approximates V from above with arbitrary precision with respect to the L^1 norm. Hence, Eq.\u00a0(<ref>) follows by Thm.\u00a0<ref>. It is also clear that Eq.\u00a0(<ref>) since J_d^*(x) \u2265 V(x).\n\n\nThm.\u00a0<ref> shows that the SOS programs we proposed in Section\u00a0<ref>, that approximate in the L^1 norm from above, converge to various sets defined by strict sublevel sets. We next show that the SOS programs we proposed in Section\u00a0<ref>, that approximate in the L^1 norm from below, yield polynomial sublevel sets that converge in the volume metric to unions of semialgebraic sets or Minkowski sums or discrete points defined by  non-strict sublevel sets.\n\nIt holds that,\n\n    lim_d \u2192\u221e\tD_V ({x \u2208\u039b: J_d^*(x)  \u2264 0} , X ) =0,\n                \t X \u2286{x \u2208\u039b: J_d^*(x) \u2264 0},\n\nwhere \u039b:={x \u2208^n: ||x||_2 \u2264 r} and the set X and the sequence of solutions {J_d^*}_d \u2208\u2282[x] satisfy either of the following:\n\n\t\n  * X is a union of semialgebraic sets, given by Eq.\u00a0(<ref>), and {J_d^*}_d \u2208\u2282[x] is given in Eq.\u00a0(<ref>).\n\t\n  * X is a Minkowski sum, given by Eq.\u00a0(<ref>), and {J_d^*}_d \u2208\u2282[x] is given in Eq.\u00a0(<ref>).\n\t\n  * X is a set of discrete points, given by X={x_i}_i=1^N\u2282^n, and {J_d^*}_d \u2208\u2282[x] is given in Eq.\u00a0(<ref>).\n\n\n\n\t\tConvergence to unions of semialgebraic sets:  Consider the case when X is given by Eq.\u00a0(<ref>) and {J_d^*}_d \u2208\u2282[x] is given in Eq.\u00a0(<ref>).\tBy a similar argument to the proof of Thm.\u00a0<ref> that showed Eqs\u00a0(<ref>) and\u00a0(<ref>) hold, it is possible to use Prop.\u00a0<ref> to show that\n\t\n    V(x) \u2265 J_d^*(x)   for all  x \u2208\u039b,\n    lim_d \u2192\u221e||J_d^*-V||_L^1(\u039b,)=0,\n\n\tfor V(x):=min_1 \u2264 i \u2264 m g_i(x). Then by Prop.\u00a0<ref> it follows that lim_d \u2192\u221e D_V({x \u2208\u039b: V(x) \u2264 0}, {x \u2208\u039b: J_d(x) \u2264 0 } )=0. By a similar argument to Lem.\u00a0<ref> it follows that X={x \u2208\u039b: V(x) \u2264 0}, where X is given by Eq.\u00a0(<ref>).  Therefore it is clear Eq.\u00a0(<ref>) holds. Moreover, since V(x) \u2265 J_d^*(x) it is clear that Eq.\u00a0(<ref>) holds.\n\t\n\tConvergence to Minkowski sums: Consider the case where X is given by Eq.\u00a0(<ref>), and {J_d^*}_d \u2208\u2282[x] is given in Eq.\u00a0(<ref>). Note in Eq.\u00a0(<ref>) that it was shown that X={x \u2208\u039b: V(x) \u2264 0 }, where \n    V(x):=inf_w  \u2208{z \u2208\u039b: max_1 \u2264 i \u2264 m_2 g_2,i(z) \u2264 0 }min_1 \u2264 i \u2264 m_1 g_1,i(x-w).\n\n\t\n\tThe rest of the proof follows by a similar argument to the convergence to unions of semialgebraic sets proof, where we use Lem.\u00a0<ref> (found in the appendix) to approximate V uniformly from below by a polynomial. We then show this polynomial approximation is feasible to Opt.\u00a0(<ref>) for sufficiently large d \u2208 using Theorem\u00a0<ref> (found in the appendix). Because this feasible solution to Opt.\u00a0(<ref>) is arbitrarily close to V, it can then be shown that the true solution of Opt.\u00a0(<ref>) approximates V from below with arbitrary precision with respect to the L^1 norm. Hence, Eq.\u00a0(<ref>) follows by Prop.\u00a0<ref>. Moreover, since V(x) \u2265 J_d^*(x) it is clear that Eq.\u00a0(<ref>) holds.\n\t\n\t\tConvergence to Discrete Points: Consider the case where X={x_i}_i=1^N, and {J_d^*}_d \u2208\u2282[x] is given in Eq.\u00a0(<ref>). Note in Lem.\u00a0<ref> it was shown that X={x \u2208\u039b: V(x) \u2264 0 }, where \n    V(x):=1 - 1_{x_i}_i=1^N(x).\n\n\t\t\n\t\tThe rest of this proof again follows by a similar argument to proof of convergence to unions of semialgebraic sets or Minkowski sums. This time we use Prop.\u00a0<ref> to approximate V uniformly from below by a polynomial. We then show that this polynomial is feasible to Opt.\u00a0(<ref>) for sufficiently large enough degree using Theorem\u00a0<ref>. Because this feasible solution to Opt.\u00a0(<ref>) is arbitrarily close to V, it can then be shown that the true solution of Opt.\u00a0(<ref>) approximates V from below with arbitrary precision with respect to the L^1 norm. Hence, Eq.\u00a0(<ref>) follows by Prop.\u00a0<ref>. Moreover, since V(x) \u2265 J_d^*(x) it is clear that Eq.\u00a0(<ref>) holds.\n\n\n\n\n\n\n\n\n\n\n\u00a7 NUMERICAL EXAMPLES\n\nIn this section we use the SOS programs we proposed in Sec.\u00a0<ref> to approximate various sets. We first approximate unions of semialgebraic sets in the Hausdorff and volume metric. We then approximate Minkowski sums and Pontryagin differences in the volume metric. We finish the section with three numerical examples that have practical motivations, the approximation of Region of Attractions (ROAs) and attractor sets of nonlinear systems via the outer bounding sets of discrete points, and the Minkowski sum of obstacles and vehicle shape sets to construct a C-space in which collision free path planning can be computed in. All SOS programs are solved using Yalmip\u00a0<cit.> with SDP solver Mosek\u00a0<cit.>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Approximation of unions of semialgebraic sets] \n\tConsider the union of semialgebraic sets\n\t\n    \u222a_i=1^3 X_i= \u222a_i=1^3 {x \u2208^2: g_i(x)<0 }\n    where \n    \n    \tg_1(x)     = x_1^2 + x_2^2 -0.075 , \n    \n    \tg_2(x)     = (x_1-0.15)^2 + (x_2-0.15)^2 -0.025, \n    \n    \tg_3(x)     = (x_1+0.25)^2 + (x_2 + 0.25)^2  -0.001.\n\n\n\n\tIn Fig.\u00a0<ref> we have plotted \u222a_i=1^3 X_i as the green region along with several approximations of the form {x\u2208^n: P(x)<0 }. For our Hausdorff approximation P=P_d and P_d is found by solving SOS Opt.\u00a0(<ref>). For our volume approximation P=J_d and J_d is found by solving SOS Opt.\u00a0(<ref>). Fig.\u00a0<ref> shows several outer approximations for d=2,10 & 18. As expected from the convergence proofs given in Sec.\u00a0<ref>, in both cases we see as the degree increases our set approximation improves. Interestingly the degree 18 Hausdorff approximation seems to give a better representation of the topology than the degree 18 volume approximation by showing some separation between X_2 and X_3. \n\t\n\t\n\n\n[Approximation of Minkowski sums and Pontryagin differences]  In\u00a0<cit.> the Minkowski sum of the following sets was heuristically approximated,\n\t\n    X_1     ={x \u2208^2:x_1^2+x_2^2 - 0.25^2<0  }\n    \n    \t\tX_2     = {x \u2208^2: x_1-0.5<0, -0.5-x_1<0,\n           -x_2-0.5<0, x_2-x_1-0.5<0,x_1+x_2-0.5<0  }\n\n\n\n\n\n\n\nIn Fig\u00a0<ref> we have plotted both the sets X_1 and X_2 as the gold and purple regions respectively (note that there is an axis scale change between Sub-figures\u00a0<ref> and\u00a0<ref>).\n\nIn Fig.\u00a0<ref> we have plotted X_1 \u2295 X_2 as the green region, which was found by discretizing both X_1 and X_2 and adding each element together. We have also plotted our outer approximations of X_1 \u2295 X_2 that take the form {x\u2208^2: J_d(x) \u2264 0} where J_d is found by solving SOS Opt.\u00a0(<ref>) for d=2,6 & 12. \n\nIn Fig.\u00a0<ref> we have plotted X_1 \u2296 X_2 as the green region, which was found by discretizing both X_1 and X_2. We have also plotted our inner approximations of X_1 \u2296 X_2 that take the form {x\u2208^2: J_d(x) < 0} where J_d is found by solving SOS Opt.\u00a0(<ref>) for d=8,10 & 16.  As expected from the convergence proofs given in Sec.\u00a0<ref>, in both cases we see as the degree increases our set approximation improves.\n\n\n\n\n[Approximation of ROAs] \n\n\n\nThe ROA is defined as the set of initial conditions for which a systems trajectories tend to an equilibrium point. We next consider the problem of approximating the ROA of the single machine infinite bus system given by the following nonlinear Ordinary Differential Equation (ODE):\n\n    [ \u1e8b_1(t); \u1e8b_2(t) ]=[                                   x_2(t); (1/2H)(P_m-P_e sin(x_1(t)+\u03b4_ep)-Dx_2(t)) ],\n\n\twhere H=0.0106, X_t=0.28, P_m=1, E_s=1.21, V=1, P_e=(E_s V)/(P_m X_t), D=0.03 and \u03b4_ep=sin^-1(1/P_e).\n\t\n\tUsing a similar method to\u00a0<cit.> we simulate ODE\u00a0(<ref>) for various initial conditions, x(0)=x_0 \u2208^2. Using these simulations we construct a labelled data set where each data point represents an initial condition that is either an element of the ROA or not. To approximate the ROA we must then solve this machine learning binary classification problem by computing the decision boundary of our labelled data set. We solve this problem by only considering the stable data points, {x_i}_i=1^N. We then use our proposed SOS algorithm to compute an outer approximation of {x_i}_i=1^N. In Fig.\u00a0<ref> we have plotted  our estimation of the ROA as the red region, that is of the form {x\u2208^2: J_d(x)  \u2264 0} where J_d is found by solving SOS Opt.\u00a0(<ref>) for d=14.\n\t\n\n\n[Approximation of attractor sets]  The Lorenz system can be modelled as the following nonlinear ODE,\n\n    [ \u1e8b_1(t); \u1e8b_2(t); \u1e8b_3(t) ]=[              \u03c3( x_2(t)-x_1(t) ); \u03c1 x_1(t) -x_2(t) -x_1(t) x_3(t);        x_1(t) x_2(t) - \u03b2 x_3(t) ],\n\n\twhere (\u03c3, \u03c1,\u03b2)=(10,28,8/3). It is well known that the Lorenz system exhibits a global attractor set in which all trajectories converge towards. The problem of approximating the Lorenz attractor from data can be posed as a machine learning classification problem\u00a0<cit.>. One way to approach this classification problem is by collecting discrete points, {x_i}_i=1^N, of terminal points of trajectories simulated for large amounts of time. Assuming our simulation time is sufficiently large, each of the discrete points, {x_i}_i=1^N, will be inside the attractor set. In Fig.\u00a0<ref> we have plotted our approximation of {x_i}_i=1^N as the red region, that is of the form {x\u2208^3: J_d(x)  \u2264 0} where J_d is found by solving SOS Opt.\u00a0(<ref>) for d=15.\n\t\n\t\n\n\n[Approximation of C-space for collision free path planning]  \n\t\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\tDubin's car is the name given to the following discrete time system,\n\t\n    x(t+1)=[ x_1(t) + \u03bdcos(x_3(t)); x_2(t) + \u03bdsin(x_3(t)); x_3(t) + \u03bd/Ltan(u(t)) ],\n\n\twhere (x_1(t), x_2(t)) \u2208^2 is the position of the car at time t \u2208, x_3(t) \u2208 denotes the\n\tangle that the car is pointing towards, u(t) \u2208 is the steering angle input, \u03bd\u2208 is the fixed speed of the car, and L>0 is a parameter that determines the turning radius of the car.\n\t\n\tIn Fig.\u00a0<ref> Dubin's car at various time stages is described by the set coloured purple and is given by\n\t\n    X_1={x \u2208^2: x_1^2 + x_2^2 -0.1^2<0 }.\n\n\tFurthermore, several obstacles are described by golden coloured sublevel sets X_2:=\u2229_i=1^6 {x \u2208^2: g_i(x)<0 }, where\n\t\n    g_star(x) =0.1-2.5 x_1^2 x_2^2-0.05(x_1+x_2)^2,\n        g_1(x)  = g_star(5x+[0.2, -0.2]^\u22a4 ) \n        g_2(x)  = [                         0.75+x_1;                             -x_1;                        -x_2+0.85; -((0.85-0.7)/(0.75))x_1-0.85+x_2 ]\n        g_3(x) = g_2([  cos((\u03c0+1)/2) -sin((\u03c0+1)/2);  sin((\u03c0+1)/2)  cos((\u03c0+1)/2) ]^-1 x ) \n        g_4(x)  = [       x_1;  0.25-x_1; 0.85+ x_2;  -0.8-x_2 ],   g_5(x)  = [  x_1-0.1;  0.5-x_1; 0.4+ x_2; -0.2-x_2 ] , \n         g_6(x)  = [  x_1-0.4; 0.75-x_1; 0.4+ x_2;  0.8-x_2 ].\n\nNote, some of the sets that describe our obstacles were taken from the previous works of\u00a0<cit.> and\u00a0<cit.>.\n\n\nIn Fig.\u00a0<ref> we have plotted our outer sublevel set approximation of X_1 \u2295 X_2 as the red region. This approximation was obtained by solving Opt.\u00a0(<ref>) for d=12. Based on this approximation of  X_1 \u2295 X_2 we then applied the Dynamic Programming (DP) algorithm proposed in\u00a0<cit.> to compute the optimal path collision free path. That is we derived a sequence of inputs u(0), u(1), \u2026,  u(T) that drives the system described in Eq.\u00a0(<ref>) from an initial condition, x(0)=x_0, to the target set, given by the blue square, in the minimum number of steps while avoiding the enlarged obstacles in the C-space, given by our approximation of X_1 \u2295 X_2. As shown in Fig.\u00a0<ref>, solving the path planning problem in C-space for obstacles X_1 \u2295 X_2, ensures that there is no collisions in the workspace when the shape of Dubin's car is accounted for. \n\t\n\t\n\t\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nWe have established a link between the L^\u221e and L^1 function metrics and the Hausdorff and volume set metrics, respectively, allowing us to construct SOS programs for accurately approximating sets encountered throughout control theory. Specifically, we have shown that if functions are close in the L^\u221e norm and one uniformly bounds the other, their sublevel sets are close in the Hausdorff metric. Likewise, if we change the function metric to the L^1 norm, the respective sublevel sets are close in the volume metric. By applying our methodology to approximating sets of discrete points, we have proposed a new machine learning binary classification algorithm that accurately finds decision boundaries for problems with low-dimensional, error-free, and dense data sets. We have applied this new classification algorithm to the problem of approximating ROAs or attractor sets of nonlinear ODEs. Furthermore, our set approximation approach allows us to numerically approximate Minkowski sums, which can be used to compute optimal collision-free paths.\n\n\n[\n    < g r a p h i c s >\n]Morgan Jones received the Mmath degree in\n\tmathematics from The University of Oxford, England in 2016 and PhD degree from Arizona State University, USA in 2021.\n\tSince 2022 he has been a lecturer in the department of Automatic Control and Systems Engineering at the University of Sheffield. His research primarily\n\tfocuses on the estimation of reachable sets, attractors and regions of attraction for nonlinear ODEs. Furthermore,\n\the has an interest in extensions of the dynamic programing framework to non-separable cost functions.\n\n\n\nieeetr\n\n\n\n\n\u00a7 APPENDIX\n\n\n\n\n \u00a7.\u00a7 The Volume Metric\n \nRecall from Section\u00a0<ref> that\n\n    D_V(A,B):=\u03bc( (A/B) \u222a (B/A) ),\n where \u03bc(A) is the Lebesgue measure of A \u2282^n.\n\n \n\tD: X \u00d7 X \u2192 is a metric if the following is satisfied for all x,y \u2208 X,\n\t\n\t\n\t\n\t\t\n\t\t\t\n  * D(x,y) \u2265 0,\n\t\t\t\n  * D(x,y)=0 iff x=y,\n\t\t\t\n  * D(x,y)=D(y,x),\n\t\t\t\n  * D(x,z) \u2264 D(x,y) + D(y,z).\n\t\t\n\t\t\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\n\n\nThe sublevel approximation results presented in this appendix are required in the proof of Theorem\u00a0<ref>. \n\n\n\n\n \n\tConsider the quotient space,\n\t\t\n    X:=  B {X \u2282^n : X \u2205, \u03bc(X) =0 },\n  where B is the set of all Lebesgue measurable sets. Then D_V: X \u00d7 X \u2192 is a metric.\n\n\n\n\n\n\n \n\tConsider Lebesgue measurable sets A,B \u2282^n. Suppose A and B have finite Lebesgue measure and B \u2286 A, then\n\t\n    D_V(A,B)    =\u03bc(A/B)= \u03bc(A)- \u03bc (B).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\tConsider a Lebesgue measurable set \u039b\u2282^n, a function V \u2208 L^1(\u039b, ), and a family of functions {J_d \u2208 L^1(\u039b, ): d \u2208} that satisfies the following properties:\n\t\n\t\t\n  * For any d \u2208 we have J_d(x) \u2264 V(x) for all x \u2208\u039b.\n\t\t\n  * lim_d \u2192\u221e ||V -J_d||_L^1(\u039b, ) =0.\n\t\n\tThen for all \u03b3\u2208\n\t\n    lim_d \u2192\u221e\tD_V ({x \u2208\u039b : V(x) \u2264\u03b3}, {x \u2208\u039b : J_d(x) \u2264\u03b3}) =0.\n\n\n\n\n\n \u00a7.\u00a7 Counterexamples: When Close Functions Have Distant Sublevel Sets\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe first show that if we slightly change the conditions of Thm.\u00a0<ref> to have J_d(x)\u2264 V(x) (rather than V(x) \u2264 J_d(x)) then we can no longer establish that the sublevel sets of V and J_d will be close.\n \nWe show there exists \u03b3\u2208, \u039b\u2282, V \u2208 L^1(\u039b,) and {J_d}_d \u2208\u2282 L^1(\u039b,) such that J_d(x)\u2264 V(x) for all x \u2208\u039b and lim_d \u2192\u221e  ||V(x) - J_d(x)||_L^\u221e(\u039b,) dx=0 but\n\t\n    lim_d \u2192\u221e D_H({ x \u2208\u039b : V(x) < \u03b3}, {x \u2208\u039b : J_d(x) < \u03b3})   0.\n\n\tLet \u039b:=[-10,10], \u03b3:=1, V(x):=1-1_[1,2](x) and J_d(x)= 1-1_[1,2](x) - 1/d. Clearly, lim_d \u2192\u221e  ||V(x) - J_d(x)||_L^\u221e(\u039b,) dx=lim_d \u2192\u221e1/d=0. However,\n\t\n    { x \u2208\u039b : V(x) < 1 }   = [1,2].\n    { x \u2208\u039b : J_d(x) < 1 }   = \u039b.\n\n\nNote, Counterexample\u00a0<ref> does not contradict Prop.\u00a0<ref> that deals with the same case where J_d lower bounds V. This is because Prop.\u00a0<ref> shows that the non-strict sublevel sets are close in the volume metric. Indeed, { x \u2208\u039b : V(x) \u2264  1 }= { x \u2208\u039b : J_d(x) \u2264  1 } so there is no contradiction in this case.\n\n\nWe consider what happens if we change the other condition of Thm.\u00a0<ref> where instead of having ||J_d - V||_L^\u221e(\u039b,)\u2192 0 we only have ||J_d - V||_L^1(\u039b,)\u2192 0.\n \n\tWe show there exists \u03b3\u2208, \u039b\u2282, V \u2208 L^1(\u039b,) and {J_d}_d \u2208\u2282 L^1(\u039b,) such that V(x) \u2264 J_d(x) for all x \u2208\u039b and lim_d \u2192\u221e  ||V(x) - J_d(x)||_L^1(\u039b,) =0 but\n\t\n    lim_d \u2192\u221e D_H({ x \u2208\u039b : V(x) < \u03b3}, {x \u2208\u039b : J_d(x) < \u03b3})   0.\n\n\tLet \u039b:=[-10,10], \u03b3:=1, V(x): =1_[1,2](x)-1 and J_d(x):= 1_[1,2](x)-1 + 1_[0,1/d](x). Clearly, lim_d \u2192\u221e  ||V(x) - J_d(x)||_L^\u221e(\u039b,) =lim_d \u2192\u221esup_x \u2208\u039b1_[0,1/d](x) =1  0 and lim_d \u2192\u221e  ||V(x) - J_d(x)||_L^1(\u039b,)= lim_d \u2192\u221e\u222b_\u039b1_[0,1/d](x) dx = lim_d \u2192\u221e\u222b_0^1/d 1dx =0. However,\n\t\n    { x \u2208\u039b : V(x) < 1 }   = [1,2]  .\n    { x \u2208\u039b : J_d(x) < 1 }   = [0,1/d] \u222a[1,2].\n\nHence \n\n    lim_d \u2192\u221e D_H({ x \u2208\u039b : V(x) < \u03b3}, {x \u2208\u039b : J_d(x) < \u03b3})\n        = lim_d \u2192\u221e D_H( [1,2], [0,1/d] \u222a[1,2])=1  0\n\n\nAlthough Counterexample\u00a0<ref> shows that if functions are close in the L1 norm then their sublevel sets may not be close in the Hausdorff metric this does not contradict Theorem\u00a0<ref>, that shows that these sublevel sets must be close in the volume metric. This holds true in the case of Counterexample\u00a0<ref> since \n\n    lim_d \u2192\u221e D_V({ x \u2208\u039b : V(x) < \u03b3}, {x \u2208\u039b : J_d(x) < \u03b3})\n        = lim_d \u2192\u221e D_V( [1,2], [0,1/d] \u222a[1,2])=  lim_d \u2192\u221e\u03bc([0,1/d])=   0\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Polynomial Approximation\n \nIn Sec.\u00a0<ref> we characterized several sets (intersections and unions of semialgebraic sets, Minkowski sums, Pontryagin differences and discrete points) by sublevel sets of various functions. We now show that we can approximate these functions arbitrarily well by polynomials that are also feasible to our associated SOS optimization problems. In order to approximate these functions we use the Weierstrass approximation theorem.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\tLet E \u2282^n be an open set and f \u2208 C^1(E, ). For any compact set K \u2286 E and >0 there exists  g \u2208[x] such that\n\t\n    sup_x \u2208 K| f(x) -  g(x)| < .\n\n\t\n\t\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe next show that there exists a polynomial that is feasible to Opt.\u00a0(<ref>) and that arbitrarily approximates the function, V(x):=min_1 \u2264 i \u2264 m g_i(x), whose sublevel set characterizes the set given in Eq.\u00a0(<ref>).\n \n\tConsider a compact set \u039b\u2282^n, functions g_i \u2208 LocLip(\u039b,) for 1 \u2264 i \u2264 m, a scalar r>0, and V(x):= min_1 \u2264 i \u2264 m{ g_i(x) }. Then for any >0 there exists H_1,H_2 \u2208[x] such that\n\t\n    sup_ x \u2208\u039b |V(x) - H_j(x) | <  for  j \u2208{1,2}, \n        H_1(x) < g_i(x)  for all  x \u2208 B_r(0)  and  1 \u2264 i \u2264 m,\n        H_2 (x)> g_i(x)  for all  x \u2208 B_r(0)  \u2229 Y_i  and  1 \u2264 i \u2264 m,\n\n\twhere Y_i  :={y \u2208\u039b: g_i(y) \u2264 g_j(y)  for  1 \u2264 j \u2264 m  }.\n\n\n\n\n\n\n\n\n\n\n\n We first show the existence of H_1 \u2208[x] that satisfies Eq.\u00a0(<ref>). Since \u039b\u2282^n and B_r(0) are compact sets it follows that there exists R>0 such that \u039b\u222a B_r(0) \u2282 B_R(0). Let >0. Since V is continuous by Lem.\u00a0<ref> it follows by Thm.\u00a0<ref> that there exists P \u2208[x] such that sup_ x \u2208 B_R(0) |V(x) - P(x) | </4. Let H_1(x):=P(x) - /4. Then\n\t\n    |V(x) - H_1(x) |     = |V(x) - P(x) + /2| < |V(x) - P(x)| + /4\n        < /2  for all  x \u2208 B_R(0),\n\n\tand hence sup_ x \u2208\u039b |V(x) - H_1(x) | \u2264/2 <. \n\t\n\tMoreover, since |V(x) - P(x)|</4 for all x \u2208 B_R(0) we have that P(x)<V(x) + /4 for all x \u2208 B_R(0) . Hence H_1(x)<V(x) + /4-/4= V(x)= min_1 \u2264 j \u2264 m{ g_j(x) }\u2264 g_i(x) for all 1 \u2264 i \u2264 m and x \u2208 B_r(0) \u2282 B_R(0).\n\t\n\tWe next show the existence of H_2 \u2208[x] that satisfies Eq.\u00a0(<ref>). This time let H_2(x):=P(x) +/4. Then\n\t\n    |V(x) - H_2(x) |     = |V(x) - P(x) - /2| < |V(x) - P(x)| + /4\n        < /2  for all  x \u2208 B_R(0),\n\n\tand hence sup_ x \u2208\u039b |V(x) - H_2(x) | <. \n\t\n\tMoreover, since |V(x) - P(x)|</4 for all x \u2208 B_R(0) we have that P(x) > V(x) -  /4 for all x \u2208 B_R(0). Hence H_2(x)>V(x) - /4 + /4=V(x) for all x \u2208 B_R(0). Now, when x \u2208 Y_i we have V(x)=g_i(x). Therefore, H_2(x)>V(x)=g_i(x) for all x \u2208 Y_i \u2229 B_r(0).\n\t\n\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe next show that there exists a polynomial that is feasible to Opt.\u00a0(<ref>) and that arbitrarily approximates the function, V(x):=max_1 \u2264 i \u2264 m g_i(x), whose sublevel set characterizes the set given in Eq.\u00a0(<ref>).\n \n\tConsider a compact set \u039b\u2282^n, g_i \u2208 LocLip(\u039b,) for 1 \u2264 i \u2264 m and V(x):= max_1 \u2264 i \u2264 m{ g_i(x) }. Then for any >0 there exists H_1,H_2 \u2208[x] such that\n\t\n    sup_ x \u2208\u039b |V(x) - H_j(x) | <  for  j \u2208{1,2}, \n        H_1(x) > g_i(x)  for all  x \u2208 B_r(0)  and  1 \u2264 i \u2264 m,\n        H_2(x) < g_i(x)  for all  x \u2208 B_r(0) \u2229Y\u0305_\u0305i\u0305 and  1 \u2264 i \u2264 m,\n\nwhere Y\u0305_\u0305i\u0305 :={y \u2208\u039b: g_i(y) \u2265 g_j(y)  for  1 \u2264 j \u2264 m  }.\n\n\nNote that max_1 \u2264 i \u2264 m{ g_i(x) }= -min_1 \u2264 i \u2264 m{ -g_i(x) }. Let \u1e7c(x):=min_1 \u2264 i \u2264 m{ -g_i(x) }= -V(x). By Prop.\u00a0<ref> there exists H\u0303_\u03031\u0303, H\u0303_\u03032\u0303\u2208[x] such that \n\t\n    sup_ x \u2208\u039b |\u1e7c(x) - H\u0303_\u0303j\u0303(x) | <  for  j \u2208{1,2}, \n       H\u0303_\u03031\u0303(x) < -g_i(x)  for all  x \u2208 B_r(0)  and  1 \u2264 i \u2264 m, \n       H\u0303_\u03032\u0303(x) > -g_i(x)   for all  x \u2208 B_r(0) \u2229\u1ef8_\u0303\u0129 and  1 \u2264 i \u2264 m,\n\nwhere \u1ef8_\u0303\u0129 :={y \u2208\u039b: -g_i(y) \u2264- g_j(y)  for  1 \u2264 j \u2264 m  } = {y \u2208\u039b: g_i(y) \u2265 g_j(y)  for  1 \u2264 j \u2264 m  } = Y\u0305_\u0305i\u0305.\n\nLet H_1(x):=-H\u0303_\u03031\u0303(x) and H_2(x):=-H\u0303_\u03032\u0303(x). Clearly H_1 and H_2 satisfies Eq.\u00a0(<ref>), completing the proof.\n\n\nWe next show that there exists a polynomial that is feasible to Opt.\u00a0(<ref>) and that arbitrarily approximates the function, V(x):=inf_w  \u2208{z \u2208\u039b: g_2,i(z) \u2264 0  for  1 \u2264 i \u2264 m_2 }min_1 \u2264 i \u2264 m_1 g_1,i(x-w), whose sublevel set characterizes the set given in Eq.\u00a0(<ref>).\n\n \n\tConsider a compact set \u039b\u2282^n, g_i \u2208 LocLip(\u039b,) for 1 \u2264 i \u2264 m and V(x):= inf_w  \u2208{z \u2208\u039b: g_2,i(z) \u2264 0  for  1 \u2264 i \u2264 m_2 }min_1 \u2264 i \u2264 m_1 g_1,i(x-w). Then for any >0 there exists H \u2208[x] such that\n\n    sup_ x \u2208\u039b |V(x) - H(x) | < , \n        H(x) < g_1,i(x-w)   for all  x \u2208 B_r(0), \n            w \u2208{z \u2208\u039b: g_2,j(z) \u2264 0  for  1 \u2264 j \u2264 m_2 } and  1 \u2264 i \u2264 m_1.\n\n\n\n\tSince \u039b\u2282^n and B_r(0) are compact sets there exists R>0 such that \u039b\u222a B_r(0) \u2282 B_R(0). Now, by Lem.\u00a0<ref> it follows that V is a continuous function. Hence, by Thm.\u00a0<ref>, for any >0 there exists a polynomial P \u2208[x] such that\n\t\n    sup_ x \u2208 B_R(0) |V(x) - P(x) | < /4.\n\nLet us consider H(x):=P(x) -  /4. Then\n\n    |V(x) - H(x)|  \u2264 \t|V(x) - P(x)|  + /4 < /2 for  x \u2208 B_R(0).\n\nHence, \tsup_ x \u2208\u039b |V(x) - H(x) | <.\n\nAlso note that since |V(x) - P(x)| < /4 for all x \u2208 B_R(0) it follows that P(x)< V(x) + /4 and hence H(x)= P(x)- /4< V(x) for all x \u2208 B_r(0) \u2282 B_R(0). Therefore\n\n    P(x)    < V(x)= inf_u  \u2208{z \u2208\u039b: g_2,j(z) \u2264 0  for  1 \u2264 j \u2264 m_2 }min_1 \u2264 j \u2264 m_1 g_1,j(x-u)\n       \u2264inf_u  \u2208{z \u2208\u039b: g_2,j(z) \u2264 0  for  1 \u2264 j \u2264 m_2 }  g_1,i(x-u) \n       \u2264 g_1,i(x-w)  for all  x \u2208 B_r(0), 1 \u2264 i \u2264 m_1,   and \n                 w \u2208{z \u2208\u039b: g_2,j(z) \u2264 0  for  1 \u2264 j \u2264 m_2 }.\n\nThus we have shown Eq.\u00a0(<ref>) completing the proof.\n\n\nWe next show that there exists a polynomial that is feasible to Opt.\u00a0(<ref>) and that arbitrarily approximates the function, V(x):=sup_w  \u2208{z \u2208\u039b: g_2,i(z) \u2264 0  for  1 \u2264 i \u2264 m_2 }max_1 \u2264 i \u2264 m_1 g_1,i(x+w), whose sublevel set characterizes the set given in Eq.\u00a0(<ref>).\n\n \n\tConsider a compact set \u039b\u2282^n, g_i \u2208 LocLip(\u039b,) for 1 \u2264 i \u2264 m and V(x):= sup_w  \u2208{z \u2208\u039b: g_2,i(z) \u2264 0  for  1 \u2264 i \u2264 m_2 }max_1 \u2264 i \u2264 m_1 g_1,i(x+w). Then for any >0 there exists H \u2208[x] such that\n\t\n    sup_ x \u2208\u039b |V(x) - H(x) | < , \n        H(x) > g_1,i(x+w)   for all  x \u2208 B_r(0), \n            w \u2208{z \u2208\u039b: g_2,j(z) \u2264 0  for  1 \u2264 j \u2264 m_2 } and  1 \u2264 i \u2264 m_1.\n\n\n\n\tFollows by a similar argument to Lem.\u00a0<ref>.\n\n\nWe next show that there exists a polynomial that is feasible to Opt.\u00a0(<ref>) and that arbitrarily approximates the function, V(x):=1-1_{x_i}_i=1^N(x), whose sublevel set characterizes the set given by X={x_i}_i=1^N. \n\n \nConsider a compact set \u039b\u2282^n and some discrete points {x_i}_i=1^N \u2282\u039b. Then, for any >0 there exists a polynomial H \u2208[x] such that\n\n    ||H(x) - V(x)||_L^1(\u039b,)< , \n    \n    \tH(x_i)<0  for all  i \u2208{1,...,N}, \n    \n    \tH(x) < 1  for all  x \u2208\u039b,\n\nwhere V(x)=1-1_{x_i}_i=1^N(x). \n\n\n\tWe first show that there exists a smooth function that satisfies Eq.\u00a0(<ref>). We then approximate this function by a polynomial.\n\t\n\tLet F(x):=1-\u2211_i=1^N \u03b7( x-x_i/\u03b4), where \u03b7\u2208 C^\u221e(^n, [0,\u221e))  is the bump function given by\n\t\n    \u03b7(x) =  2e exp( 1/||x||_2^2 -1)  for  ||x||_2<1\n    \n    \t\t\t0  otherwise.\n\n\nFor more information on bump functions see\u00a0<cit.>.\n\nLet C:=\u222b_^n\u03b7(x) dx < \u221e and 0<\u03b4< ( /2 NC)^1/n, then\n\n    ||V(x) - F(x)||_L^1(\u039b,)\u2264\u2211_i=1^N \u222b_^n\u03b7( x-x_i/\u03b4) dx\n        =\u03b4^n \u2211_i=1^N \u222b_^n\u03b7(x) dx=\u03b4^n NC</2.\n\nMoreover,\n\n    F(x_i)=1 - \u03b7(0) - \u2211_j u\u03b7( x_i-x_j/\u03b4) \u2264 1 - \u03b7(0)= -1<0.\n\nFurthermore, since \u03b7(x) \u2265 0 for all x \u2208^n it is clear that\n\n    F(x) = 1-\u2211_i=1^N \u03b7( x-x_i/\u03b4) \u2264 1  for all  x \u2208^n.\n\n\nSince \u039b is a compact set there exists R>0 such that \u039b\u2282 B_R(0). Now, \u03b7\u2208 C^\u221e(^n, [0,\u221e)) and therefore F \u2208 C^\u221e(^n, ). Hence, by Thm.\u00a0<ref>, there exists a polynomial P \u2208[x] such that\n\n    sup_ x \u2208 B_R(0) |F(x) - P(x) | < /4 (1 + \u03bc(\u039b)).\n\nLet us consider H(x):=P(x) -  /4(1 + \u03bc(\u039b)). Then\n\n    |F(x) - H(x)|     \u2264 \t|F(x) - P(x)|  + /4(1 + \u03bc(\u039b))\n        < /2(1 + \u03bc(\u039b)) for  x \u2208 B_R(0).\n\nEqs\u00a0(<ref>) and\u00a0(<ref>) imply that\n\n    || V- H||_L^1(\u039b, )   \u2264 || V- F||_L^1(\u039b, )  + || F- H||_L^1(\u039b, )\n       \u2264/2 + \u03bc(\u039b)  ||F- H||_L^\u221e(\u039b, )\n        < .\n\n\nMoreover, by Eq.\u00a0(<ref>) we have P(x) < F(x) +/4(1 + \u03bc(\u039b)) for x \u2208 B_R(0) and hence H(x)=P(x) - /4(1 + \u03bc(\u039b))< F(x). Therefore by Eqs\u00a0(<ref>) and\u00a0(<ref>) it follows that \n\n    H(x_i)    <F(x_i)<0  for all  i \u2208{1,...,N}, \n    \n    \tH(x)     <F(x)< 1  for all  x \u2208\u039b.\n\n\n\n\n\n \u00a7.\u00a7  Miscellaneous Results\n \n \n\tConsider a sequence {x_n}_n \u2208\u2282^n that is bounded, that is there exists M>0 such that x_n < M for all n \u2208. Then there exists a convergent subsequence {y_n}_n \u2208\u2282{x_n}_n \u2208. \n\n\n \n\tConsider the semialgebriac set X = {x \u2208^n: g_i(x) \u2265 0  for  i=1,...,k}. Further suppose {x  \u2208^n : g_i(x) \u2265 0 } is compact for some i \u2208{1,..,k}. If the polynomial f: ^n \u2192 satisfies f(x)>0 for all x \u2208 X, then there exists SOS polynomials {s_i}_i \u2208{1,..,m}\u2282\u2211_SOS such that,\n\t\n\n    f - \u2211_i=1^m s_ig_i \u2208\u2211_SOS.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nConsider some compact set X \u2282^n and polynomial functions {g_i}_i=1^m \u2208[x]. Suppose\n\t\n    V_1(x)     := inf_w \u2208 Xmin_1 \u2264 i \u2264 mg_i(x-w),    \n    \tV_2(x)  := sup_w \u2208 Xmax_1 \u2264 i \u2264 m g_i(x+w) ,\n\n\tthen V_1 and V_2 are continuous functions.\n\n\n\n\n\n\n\n \nFor each compact set X \u2282^n there exists a bounded function p\u2208 C^\u221e(^n,) such that \n\n    X=\t{x \u2208^n: p(x) \u2264 0}.\n\n\n\n \n\tIf f:^n \u2192 is a continuous function then {x \u2208^n: f(x) \u2264\u03b3}, where \u03b3\u2208, is a closed set. Furthermore, if \u039b\u2282^n is compact then {x \u2208\u039b: f(x) \u2264\u03b3} is a compact set. \n\n\n\tConsider a converging subsequence {x_k}_k=1^\u221e\u2282{x \u2208\u039b: f(x) \u2264\u03b3} such that x_k \u2192 x^*. By continuity we have f(x_k) \u2192 f(x^*). Since f(x_k) \u2264\u03b3 for all k it follows f(x^*) \u2264\u03b3. Hence, x^* \u2208{x \u2208\u039b: f(x) \u2264\u03b3} implying that {x \u2208\u039b: f(x) \u2264\u03b3} is closed. Now, {x \u2208\u039b: f(x) \u2264\u03b3}\u2282\u039b and \u039b is bounded. Since {x \u2208\u039b: f(x) \u2264\u03b3} is closed and bounded it follows that it is compact.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}