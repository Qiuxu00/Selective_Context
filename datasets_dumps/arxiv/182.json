{"entry_id": "http://arxiv.org/abs/2303.07100v1", "published": "20230313134009", "title": "A Feature-based Approach for the Recognition of Image Quality Degradation in Automotive Applications", "authors": ["Florian Bauer"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\nA Feature-based Approach for the Recognition of Image Quality Degradation in Automotive Applications \nCo-funded by Bayerisches Staatsministerium f\u00fcr Wirtschaft, Landesentwicklung und Energie (stmwi.bayern.de).\n\n    \nFlorian Bauer\nb-plus technologies GmbH\n\nDeggendorf, Germany \n\nflorian.bauer@b-plus.com\n\n    March 30, 2023\n==================================================================================================================================================================================================================\n\n\n\n\nCameras play a crucial role in modern driver assistance systems and are an essential part of the sensor technology for automated driving.\nThe quality of images captured by in-vehicle cameras highly influences the performance of visual perception systems.\nThis paper presents a feature-based algorithm to detect certain effects that can degrade image quality in automotive applications.\nThe algorithm is based on an intelligent selection of significant features. \nDue to the small number of features, the algorithm performs well even with small data sets.\nExperiments with different data sets show that the algorithm can detect soiling adhering to camera lenses and classify different types of image degradation.\n\n\n\nfeature-based, image quality, degradation, soiling, fisheye camera\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\nThe functionality of in-vehicle visual perception systems relies on the quality of the images captured by the cameras.\nThe performance of visual perception algorithms is known to degrade significantly in adverse conditions.\nAdverse weather conditions can be rain, snow, or fog. Further adverse conditions can be caused by poor lighting, or simply soil, ice, dust, or water adhering to the camera's lens.\n\n\nUnlike cameras behind the windshield, fisheye cameras are typically mounted at a low position on a vehicle and are directly exposed to the environment. \nThus, the lenses of these cameras tend to get soiled, e.g., by splashes of mud or water or by raindrops forming on the lens <cit.>. \nBecause of their extensive field of view, which typically covers areas with varying lighting conditions, images from fisheye cameras are also susceptible to local overexposure or underexposure. Soiling and incorrect exposure can degrade the quality of images significantly.\n\n\nAlgorithms that can detect image quality degradation are essential to higher levels of vehicle automation. They allow the system to issue warnings about malfunction or even to disable affected functions <cit.>.\nHowever, these algorithms may also be applied in the development process of the automation system. \nPoor image quality may cause unexpected, challenging, or dangerous situations, often referred to as corner cases <cit.>.\nIt is essential to detect such situations and include them in the data that is used for the development of machine learning algorithms.\n\n\nFig.\u00a0<ref> shows a selection of images (converted to grayscale) captured by four surround-view fisheye cameras on a test vehicle, along with some common effects that can degrade image quality.\n\n\n\nThe work presented here focuses on the recognition of image quality degradation in automotive applications, caused by soiling adhering to the lens (raindrops, dust, mud, ice), noise, blur, underexposure, or glare.\nThe key contribution of this paper is the development of a feature-based approach. \nThis approach is based on a set of features that can be computed from a single grayscale image.\nThe features are derived from the statistical moments of the distributions of quantities that result from the application of different filter operations on the grayscale image. \n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\nThe recognition and detection of reduced image quality have been studied in various fields of engineering. \nFor instance, in image quality assessment (IQA) the appearance of an image to a human viewer is most important <cit.> and a score may be used to quantify quality. Whereas for images used in visual perception, the \u201cquality\u201d refers to the amount of useful information the image contains, e.g., the number of identifiable objects <cit.>.\nDepending on the application, different terminology has been introduced to define image quality and its degradation.\u00a0 \nThis section briefly introduces this terminology along with some previously developed methods to evaluate and detect image quality degradation.\n\n\n\nIn IQA, the algorithms presented in <cit.> use scene statistics of \u201clocally normalized luminance coefficients\u201d to quantify possible losses of \u201cnaturalness\u201d in the image due to the presence of \u201cdistortions\u201d.\u00a0\nHere, the term distortion refers to artificial blur, noise, or the result of extensive compression that affects image quality.\nBased on the statistics, a representative feature vector is computed, which is used to evaluate the perceptual quality of an image. \n\nIn visual perception applications, adverse weather conditions (e.g., fog, mist, rain, and snow) are known to degrade the performance of many image- and video-based algorithms.\u00a0\nOften, adverse weather conditions not only reduce the visible range of on-board cameras and cause a loss of contrast, but also cause soiling on the camera's lens.\u00a0\nIn the past, the effect of rain in particular has been studied in this context.\u00a0\n\nThe detection of raindrops has for instance been studied in <cit.>. The survey paper <cit.> summarizes methods for the detection of raindrops adhering to a vehicle\u2019s windshield.\n\n\nHowever, soiling on lenses of in-vehicle cameras may not only be caused by raindrops. \nIn <cit.> a method to detect \u201cimage artifacts\u201d that arise from raindrops, dirt, and scratches on the camera lens is discussed. The method is based on a pixel-wise correlation between several frames captured on moving systems.\n\n\nRecent approaches use Convolutional Neural Networks (CNNs) to detect \u201csoiling\u201d on camera lenses.\nDifferent networks that can detect \u201copaque\u201d and \u201ctransparent\u201d soiling on image-, tile-, or pixel-level are presented in <cit.>.\nAlthough the networks are \u201celatively small-sized\u201d data sets with many thousands of annotated images are used for training (e.g., WoodScape <cit.>).\u00a0\n\n\nBefore the rise of deep learning, feature-based approaches were commonly used in computer vision applications.\nThese approaches typically use the statistics of handcrafted features and a classical machine learning model to perform the final classification <cit.>.\u00a0\nE.g., the well-known method in <cit.> proposes features based on the \u201chistogram of gradients\u201d (HoG) that enables high accuracy for human detection.\nIn <cit.>, a method is presented that uses features derived from histograms of different image properties (brightness, contrast, sharpness, hue, and saturation). The histograms are computed for multiple regions of interest in the image and the features are compiled into one large vector. A support vector machine (SVM) classifier is applied to estimate the weather situation (clear weather, light rain, heavy rain) based on the vector.\u00a0\n\n\n\n\u00a7 FEATURE EXTRACTION\n\n\n\nThis paper presents a feature-based algorithm that can recognize common effects of image quality degradation in automotive applications. The proposed set of features can be derived from a single grayscale image. \nAn SVM is used to recognize different classes of degradation effects based on the features.\nFig.\u00a0<ref> gives an overview of the algorithm. \n\nFirst, different filters are applied to the grayscale image to determine local quantities for blur, sharpness, and others. Next, features based on the statistical distribution of these quantities are computed. \nA machine learning algorithm is applied to recognize different classes of image quality degradation.\n\nAlthough color images may contain additional valuable information, the proposed algorithm uses grayscale images only. This allows the algorithm to be applied to a wider range of cameras given that some of the color filter arrays used in the automotive industry do not provide color information (e.g. RCCC). \n\n\n\n\n \u00a7.\u00a7 Image filtering\n\n\nFirst, a series of filter operations is applied to a grayscale image.\nFig.\u00a0<ref> shows a grayscale image captured by a fisheye camera with lens affected by soiling and the five quantities that are computed based on it.\n\nLet I(i,j) with 0 \u2264 I(i,j) \u2264 1 denote the intensity of each pixel in the initial grayscale image, where i = 1, 2, \u2026 H, and j = 1, 2, \u2026 W are the pixel indices, and H and W are the height and width of the image, respectively. \n\n\n\n  \u00a7.\u00a7.\u00a7 Local Mean Subtracted Field\n\n\nTo compute the local mean subtracted field (I(i,j) - \u03bc(i,j)) the local mean field \u03bc(i,j) is required, which is defined by\n\n\n    \u03bc(i,j) = \u2211_k=-K^K \u2211_l=-L^L w_k,l I(i+k, j+l)    .\n \nIn this equation, w is a circular Gaussian weighting function sampled out to three standard deviations and scaled to unit volume, as suggested in <cit.>. K and L determine the size of the filter. For K = L = 3 this results in a filter of 7 pixels by 7 pixels.\nThe operation defined by (<ref>) returns the weighted sum of pixel values of the grayscale image within a small neighborhood of a pixel and is commonly used to blur images.\nThe local mean field \u03bc is subtracted pixel-wise from the grayscale image to obtain the local mean subtracted field (I - \u03bc).\n\n\n\n  \u00a7.\u00a7.\u00a7 Local Contrast Field\n\n\nThis quantity is defined as:\n\n    \u03c3 (i,j) = (\u2211_k=-K^K \u2211_l=-L^L w_k,l (I(i+k, j+l) - \u03bc(i,j))^2)^1/2  .\n\nIt gives the local variance of an image and can be used as a measure for local image sharpness <cit.>. \n\n\n\n  \u00a7.\u00a7.\u00a7 Gradient field\n\n\nThe edges in an image are known to be important for visual perception such as object detection but can also be affected by different types of image degradation. Edges in images can be identified by various measures, like the magnitude of the first-order gradients in the horizontal and vertical directions (Sobel), or the Laplacian of an image.\nExperiments with different filters show that the Laplacian operator yields the most distinctive features for soiled images.\n\n\n\n\nIn Cartesian coordinates, the Laplacian operator gives the sum of the second-order derivatives in horizontal and vertical directions of an image and can be approximated by a discrete filter.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Mean Subtracted Contrast Normalized (MSCN) Coefficients\n\n\nThis quantity is inspired by image quality assessment (IQA) algorithms <cit.>.\nIt is defined as:\n\n    \u00ce(i,j) =  I(i,j) - \u03bc(i,j) /\u03c3(i,j) + e    ,\n\nwhere e is a constant that prevents division by zero.\n\nThese coefficients have been observed to follow a certain statistical distribution for images that contain little or no distortion. The presence of image distortion (noise, blur, artifacts from compression) alters the distribution of the coefficients significantly.\n\n\n\n  \u00a7.\u00a7.\u00a7 Pair-Wise Product of MSCN coefficients\n\n\nSimilarly, the products of pairs of MSCN coefficients provide valuable information on image distortion <cit.>. The products \u00ce_H(i,j) along the horizontal direction can be computed from\n\n    \u00ce_H(i,j) = \u00ce(i,j)  \u00ce(i+1,j)    .\n\n\n\n All the quantities defined in this section can be computed very efficiently and are assumed to provide valuable information to identify image degradation. \n \n For instance, the local mean subtracted field (I - \u03bc) provides a local measure of blurriness in the image. In Fig.\u00a0<ref> b it can be seen that the mean subtracted field exhibits small magnitudes (color-coded by gray) in the parts of the image that are soiled.\n \n\n\n \u00a7.\u00a7 Computation of Statistical features\n\n\n\nExamining the quantities in Fig.\u00a0<ref> showed that their distributions (histograms) exhibit characteristic shapes corresponding to certain effects of image degradation.\nThus, it can be hypothesized that the statistical properties of these distributions can be used to recognize certain types of image quality degradation.\n\nHowever, the number of features directly influences the complexity of the classification problem. A large number of features may increase the accuracy, but will also increase computation time and the size of the required training set in particular. \nThus, a small number of highly significant features should be selected. \n\nThe proposed features are the first- and second-order moments (mean and variance) of the distributions of the quantities in Fig.\u00a0<ref>. \nThey are assumed to be highly significant, because for several distribution functions the shape parameters can be estimated based on these two statistical properties. \nThese properties are also often used for functions that do not have a closed form to determine the estimates <cit.>.\nBecause the distributions may be asymmetric, the moments are calculated for the non-negative and negative values separately.\n\n    \u03bc_pos = 1/H W\u2211_i=1^H\u2211_j=1^W x(i, j)   \u2200   x(i,j) \u2265 0\n\n\n    V_pos = 1/H W\u2211_i=1^H\u2211_j=1^W (x(i, j) - \u03bc_pos)^2   \u2200   x(i,j) \u2265 0\n\nEquations\u00a0(<ref>) and (<ref>) give the features \u03bc_pos and V_pos based on the non-negative values x \u2265 0. For features \u03bc_neg and V_neg the negative values x < 0 are used, resulting in four features per distribution. \nOnly two features are computed for the distributions I(i,j) and \u03c3(i,j) because they merely have non-negative values.\nIn total, this results in a feature vector \ud835\udc1f with 20 scalar elements.\n\n\n\n \u00a7.\u00a7 Classification\n\n\nThe feature vector \ud835\udc1f is used as input for a classifier that maps from feature space into classes. \nFor this task, supervised machine learning algorithms such as Decision Trees, Neural Networks, or Support Vector Machines can be used.\nDue to its robustness, computational efficiency, and applicability to small and imbalanced data sets, an SVM with an RBF (Radial Basis Function) kernel is used. The algorithm is implemented in Python based on the libraries contained in Scikit-learn <cit.>.\n\n\n\n\u00a7 EXPERIMENTS AND RESULTS\n\n\n\n\n\n \u00a7.\u00a7 Data Sets\n\n\nTable\u00a0<ref> provides information on each of the four data sets used for the experiments in this section.\n \n All data sets contain at least images from two classes: clean and soiled. Data Set 1 has additional images according to four more effects of image degradation.\n The images in Data Set 1 were sampled from video recordings captured by four surround-view fisheye cameras which are mounted on the front and rear of a test vehicle as well as on both mirrors. Sample images can be seen in Fig.\u00a0<ref>.\n The images in Data Set 2 were extracted from a collection of videos that were taken during every day driving situations by a rear view camera in a modern production car. The images clearly exhibit some artifacts originating from the lossy compression that was applied to the video data.\n Data Set 3 is the subset of the WoodScape data set that is publicly available <cit.>.\n Data set 4 is a combination of data sets 1, 2, and 3. The images are selected in a way that the three data sets and two classes are roughly balanced.\n \n\n\n\n \u00a7.\u00a7 Recognition of soiled images\n\n\nThe proposed algorithm is applied to the four data sets to classify the input images as either clean or soiled (binary classification). \nThe data set is split into training (75%) and test data (25%).\nThe features are mean removed and scaled to unit variance first.\nAn SVM classifier with an RBF kernel is trained on each of the four data sets. \nThe regularization parameter C and the kernel coefficient \u03b3 (see <cit.>) are adjusted for each of the data sets individually. \n \nTable\u00a0<ref> summarizes the performed experiments and accuracies achieved on the test data.\n\nThe accuracy is defined as the ratio of the number of correctly predicted test images to the total number of test images.\n\nOne of the major advantages of the proposed, feature-based algorithm is its low complexity, because the classification of the images is based on merely 20 features. \nEven for training sets with only 500 images, accuracies \u2265 95% can be achieved. \n\n\n\n \u00a7.\u00a7 Accuracy Compared to CNN\n\n\nTo compare the feature-based approach to a CNN (see <cit.>), the same experiments as described in <cit.> are performed. \nIn these experiments, different subsets of the WoodScape data set are used: (I) training and testing on images captured by front (FV) and rear (RV) cameras only, (II) training on FV and RV cameras and testing on all cameras, and (III) training and testing on all cameras. The results are shown in Table\u00a0<ref>.\n\nThe accuracy achieved by the CNN is given in the bottom row. It is computed based on the raw confusion matrices presented in <cit.>.\nThe results of experiment (II) provide insight into how well the algorithm generalizes across images taken by cameras mounted on different positions (different perspectives, e.g. Fig.\u00a0<ref>, a and b).\nAlthough the number of images available for the training of the algorithm is significantly smaller, the feature-based approach outperforms the CNN in this task.\n\n\n\n \u00a7.\u00a7 Classification of Degradation Effects\n\n\nIn this experiment the proposed algorithm is applied to the entire Data Set 1 (multi-class classification). \nThe results presented in Fig.\u00a0<ref> show that the feature-based algorithm is able to recognize more than just soiled images.\n\n Other degradation effects can be classified even more accurately. The algorithm achieves an overall accuracy of 96.52%.\n\n\n\n\u00a7 CONCLUSION\n\n\n\nThis paper presents a feature-based algorithm that is based on an intelligent selection of features that are significant for the detection of image quality degradation. \nFrom experiments with different image data sets it is found that the algorithm can reliably detect soiling on camera lenses. \nA comparison with another approach (based on Deep Learning) shows that it generalizes better across images captured from different camera positions.\nThe algorithm is also capable of classifying different effects of image quality degradation, such as soiling, blur, glare, noise, or underexposure.\nThe major advantages of the presented algorithm are that small data sets suffice to train the classifier and that it can be computed very efficiently.\n\n\n\nIEEEtran\n\n\n\n"}