{"entry_id": "http://arxiv.org/abs/2303.06636v1", "published": "20230312112424", "title": "Strong Converses for Memoryless Bi-Static ISAC", "authors": ["Mehrasa Ahmadipour", "Michele Wigger", "Shlomo Shamai"], "primary_category": "cs.IT", "categories": ["cs.IT", "math.IT"], "text": "\t\n\t=0\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\t\n\n\n\t\n\t[4]\n\tStrong  Converses for  Memoryless Bi-Static ISAC\n    \n\t\tMehrasa Ahmadipour1, Mich\u00e8le Wigger2, Shlomo Shamai3 \n\t\t\n\t1\n\t\tUMPA, ENS de Lyon\n\t<mehrasa.ahmadipour@ens-lyon.fr>\n\t\t2 LTCI Telecom Paris, IP Paris, 91120 Palaiseau, France, Emails:\n\t\t\t<michele.wigger@telecom-paris.fr>\n\t\t3 Technion, Israel ,  Email: sshlomo@ee.technion.ac.il\n\t\t\n\t\n    \n==============================================================================================================================================================================================================================================================================================\n\n\nThe paper characterizes  the fundamental limits of  integrated sensing and communication (ISAC) systems with a bi-static radar, where the  radar receiver is located close to the transmitter  and  estimates or detects the state based on the transmitter's channel inputs and the backscattered signals. Two models are considered. In the first model, the memoryless state sequence is distributed according to a fixed distribution and the goal of the radar receiver is to reconstruct this state-sequence with  smallest possible distortion. In the second model, the memoryless state is distributed either according to P_S or to Q_S and the radar's goal is to detect this underlying distribution so that the missed-detection error probability has maximum exponential decay-rate  (maximum Stein exponent). Similarly to previous results, our fundamental limits show that the tradeoff between  sensing and communication  solely stems from the empirical statistics of the transmitted codewords which influences both  performances. The main technical  contribution are two strong converse proofs that hold for all probabilities of communication error \u03f5 and excess-distortion probability or false-alarm probability \u03b4 summing to less than 1, \u03f5+\u03b4 < 1. These proofs are based on  two parallel change-of-measure arguments on the sets of typical sequences, one change-of-measure to obtain the desired bound on the communication rate, and the second to bound the sensing performance.\n\n\n\tIntegrated sensing and communication, strong converse, change of measure arguments.\n\t\n\n\n\u00a7 INTRODUCTION\n\nSensing is a promising new feature in the upcoming 6G  <cit.> and  WIFI standards <cit.>. In  particular,  huge technological efforts are  being made to integrate radar systems with communication systems. Such integrated systems are especially appealing for autonomous driving applications or autonomous manufacturing sites (as part of the Industry 4.0). In integrated sensing and communication systems (ISAC), the idea is to use the backscattered signals from communication for radar applications  to sense the environment, detect hazardous events, or infer properties of other terminals (e.g., velocities or directions of other cars). \n\n\n\tWhile ISAC has inspired  a  plethora of works in the signal processing and communications community, see  for example <cit.> and references therein, only few works were reported from the  information-theoretic community <cit.>. The results in <cit.> and the present manuscript all  focus on the system model in Figure\u00a0<ref> consisting of a transmitter (Tx) sending a message to a receiver (Rx) over a state-dependent discrete memoryless channel (SDMC). A bistatic radar close to the Tx receives the backscattered signal modeled through generalized feedback. Due to the proximity to the Tx, this radar receiver also knows the Tx's  channel inputs and compares them to its feedback outputs. \n\t\n\t\n\tThe works in <cit.> determined the fundamental performance limits of a  detection-version of the model in Figure\u00a0<ref>.  Specifically in these works, the state sequence {S_t} is assumed constant over time, taking on one of multiple possible values depending on a underlying hypothesis, and the radar receiver aims to guess this hypothesis. Sensing performance is measured in terms of exponential decay-rate of the probability of error, either the minimum exponential decay-rate over all hypotheses  <cit.>  or the set of decay-rates that are simultaneously achievable under the different hypotheses <cit.>. The work <cit.> also studies a  mono-static version of this problem, assuming that Tx coincides with the  radar receiver and thus can use the generalized-feedback signals also for communication purposes. For this mono-static radar scenario however only a coding scheme but no converse is presented. The problem is known to be hard as it relates to the challenging close-loop controlled sensing problem <cit.>. \n\t\n\tThe first information-theoretic work  <cit.> on ISAC determined the fundamental limits of the  rate-distortion version of the ISAC problem in Figure\u00a0<ref>. assuming that the Tx can use the feedback signals also for coding (i.e., under the close-loop coding assumption).   Extensions to network scenarios and  to scenarios with secrecy constraints were subsequently presented in <cit.>.\n\nIn this paper, we consider both the rate-distortion version and the hypothesis testing  versions of the model in Figure\u00a0<ref>. In our first model, the state sequence {S_t}_t\u2265 1 is independent and identically  distributed (i.i.d.) according to a given distribution P_S and the radar wishes to  reconstruct this state sequence with smallest possible distortion. In our second model, the state-sequence {S_t} depends on a binary hypothesis \u210b\u2208{0,1}. If \u210b=0, then {S_t} is i.i.d. according to a distribution P_S or if \u210b=1, it is i.i.d.  according to a distribution Q_S. We measure sensing performance in terms of Stein's exponent, i.e., in terms of  the maximum exponential decay-rate of the missed-detection error probability (detecting P_S  instead of Q_S) under a permissible threshold on the false-alarm probability (detecting Q_S  instead of P_S). \n\t\n\t\t\n\n\n\tFor both our models, we determine the fundamental limits of  communication rates and  distortion/missed-detection error exponents that are simultaneously achievable. \n\tSimilarly to previous works <cit.>  our  limits  exhibit a tradeoff between the  sensing and communication performances, which however solely stems from the empirical statistics of the codewords used for communication.\n\n\t\n\tThe direct parts of our proofs follow immediately from existing works. Our contributions are the proofs of the converse results. In fact, we present strong converse proofs  that  hold whenever the maximum allowed probability of communication error \u03f5 and the maximum allowed distortion-excess probability or false-alarm probability \u03b4 satisfy \u03f5+\u03b4 < 1. The converse proofs are extensions of the channel coding strong converse proof in <cit.> to incorporate also the sensing bounds. Interestingly, the same change-of-measure as in <cit.> can be used to obtain the desired bound on the rate of communication. Different  changes-of-measure are used to obtain the desired bounds on the sensing performances.\n\t\n\tStrong converse proofs based on change-of-measure arguments go back to  Gu and Effros <cit.> and can be also found in various other works, e.g., <cit.>. The proof method was formalized and first applied to channel coding by Tyagi and Watanabe <cit.>. Recent works <cit.> slightly modified and simplified the technique in <cit.> by restricting the new measures to sequences on typical or conditionally-typical sets. This feature allows to circumvent resorting to variational characterizations for the multi-letter and single-letter problems as proposed in <cit.>. Notice that the works <cit.> also showed the utility of the proposed converse proof method for scenarios with expectation constraints, in which case the fundamental limits depend on the permissible error probabilities. \t\n\t\n\t\n\n\t\n\n\n\n\n\nNotation:\nUpper-case letters are used for random quantities and lower-case letters for deterministic realizations. Calligraphic font is used for sets.   All random variables are assumed finite and discrete. We  abbreviate   the n-tuples (X_1,\u2026, X_n) and (x_1,\u2026, x_n) as X^n and x^n and the n-t tuples  (X_t+1,\u2026, X_n) and (x_t+1,\u2026, x_n) as X_t+1^n and x_t+1^n. We further abbreviate independent and identically distributed as i.i.d. and probability mass function as pmf.\n\n  Entropy, conditional entropy, and mutual information functionals are written as H(\u00b7), H(\u00b7|\u00b7), and I(\u00b7;\u00b7), where the arguments of these functionals are random variables and whenever their probability mass function (pmf)  is not clear from the context, we add it as a subscript  to these functionals. The Kullback-Leibler divergence between two pmfs is denoted by  D(\u00b7\u00b7).  We shall use  \ud835\udcaf_\u03bc^(n)(P_XY)  to indicate the jointly strongly-typical set with respect to the pmf P_XY on the product alphabet \ud835\udcb3\u00d7\ud835\udcb4 and parameter \u03bc as defined in  <cit.>. Specifically, denoting by n_x^n,y^n(a,b) the number of occurrences of the pair (a,b) in sequences (x^n,y^n): \n  \n    n_x^n,y^n(a,b) =  | {t (x_t,y_t)=(a,b)  }| ,\n\n a pair  (x^n,y^n) lies in \ud835\udcaf_\u03bc^(n)(P_XY) if \n\n    |  n_x^n,y^n(a,b)/n   - P_XY(a,b) | \u2264\u03bc,      \u2200 (a,b)\u2208\ud835\udcb3\u00d7\ud835\udcb4,\n\nand n_x^n,y^n(a,b)=0 whenever  P_XY(a,b)=0.\n   The conditionally strongly-typical set with respect to a conditional pmf P_Y|X from \ud835\udcb3 to \ud835\udcb4,  parameter \u03bc>0, and  sequence x^n\u2208\ud835\udcb3^n is denoted \ud835\udcaf_\u03bc^(n)(P_Y|X, x^n) <cit.>. It contains all sequences y^n\u2208\ud835\udcb4^n satisfying \n \n    |  n_x^n,y^n(a,b)/n   -  n_x^n(a)/n P_Y|X(b|a) | \u2264\u03bc,      \u2200 (a,b)\u2208\ud835\udcb3\u00d7\ud835\udcb4,\n\nand  n_x^n,y^n(a,b)=0 whenever P_Y|X(b|a)=0.  Here n_x^n(a) denotes the number of occurrences of symbol a in x^n. In this paper, we denote the joint type of (x^n,y^n) by \u03c0_x^ny^n, i.e., \n\n    \u03c0_x^ny^n(a,b)\u225cn_x^n,y^n(a,b)/n.\n\n Accordingly, the marginal type of x^n is written as \u03c0_x^n.\n \n \n\n\n \n\n\n\n\n\n\n\n\n\u00a7 MEMORYLESS STATE AND AVERAGE DISTORTION AS A SENSING MEASURE\n\n\nConsider the bistatic radar receiver model over a memoryless  channel in Fig.\u00a0<ref>. A transmitter (Tx) that wishes to communicate a random message M to a receiver (Rx) over a state-dependent channel. The message M is uniformly distributed over the set {1,\u2026, 2^nR} with R>0 and n>0  denoting the  rate and  blocklength of communication, respectively. The channel from the Tx to the Rx depends on a  state-sequence S^n=(S_1,\u2026, S_n) which is i.i.d. according to a given pmf P_S.\n\nFor a given blocklength n, the Tx thus produces the n-length sequence of  channel inputs  \n\n    X^n = \u03d5^(n) (M)\n\nfor some choice of the   encoding function \u03d5^(n){1,\u2026, 2^nR}\u2192\ud835\udcb3^n.\n\nBased on X^n and S^n the channel produces the sequences Y^n observed at the Rx and the backscattered signal Z^n. The channel is assumed memoryless and described by the stationary  transition law P_YZ|XS implying that the pair (Y_t,Z_t) is produced according to the channel law P_YZ|XS based on the time-t symbols (X_t,S_t).\n\nThe Rx attempts to guess message M based on the sequence of channel outputs Y^n:\n\n    M\u0302 = g^(n)(Y^n)\n\nusing a decoding function of the form g^(n)\ud835\udcb4^n \u2192{1,\u2026, 2^nR}. \n\nPerformance of  communication  is measured in terms of average error probability \n\n    p^(n)(error) :=  [ M\u0302\u2260 M ]\n\n\n\n\n\nThe radar receiver produces as a guess a reconstruction of the state sequence \n\n    \u015c^n = h^(n)(X^n,Z^n),\n\nbased on the inputs and backscattered signals. Radar sensing performance is measured as average expected distortion\n\n    dist^(n)( \u015c^n , S^n)\u225c1/n\u2211_i=1^n   d (\u015c_i, S_i),\n\nfor a given and bounded distortion function d(\u00b7,\u00b7). \n\n\nIn this context we have the following definition and result. \n\n\nA rate-distortion pair (R,D) is  (\u03f5,\u03b4)-achievable over the state-dependent  channel (\ud835\udcb3,\ud835\udcb4, P_Y|XS) with state-distribution P_S, if there exists a sequence of encoding, decoding, and estimation functions {(\u03d5^(n), g^(n), h^(n))} such that  the average  probability of error satisfies\n\n    _n\u2192\u221e  p^(n)(error) \u2264\u03f5\n\nand the excess distortion probability\n\n    _n\u2192\u221e[ dist^(n)( \u015c^n, S^n) > D]\u2264\u03b4.\n\n\n\n\nFor any \u03f5+\u03b4 <1, a rate-distortion pair (R,D) is (\u03f5,\u03b4)-achievable, if and only if, there exists a pmf P_X \n satisfying\n \n    R = I_P_XP_SP_Y|XS(X;Y)\n\nand \n \n    D \u2265\ud835\udd3c_P_XP_SP_Z|XS[  d(\u015d(X, Z),  S) ]\n\nwhere \n\n    \u015d(x,z) :=min_\u015d\u2208\ud835\udcae\u0302\u2211_s P_S|XZ(s|x,z) d( \u015d, s)\n\nand \n    P_S|XZ(s|x,z) := P_S(s) P_Z|XS(z|x,s)/\u2211_s' P_S(s') P_Z|XS(z|x,s')\n\n\n \n\n\nThe limiting case \u03f5, \u03b4\u2193 0 of the theorem was already proved in <cit.>. Achievability of the theorem  follows thus directly from  this previous result. The converse is proved in the following \nsubsection, also using the next lemma, which is from <cit.>.\n\n\n\nWithout loss in optimality, one can restrict to the per-symbol estimator\n\n    h^(n)(x^n,z^n)= (\u015d(x_1,z_1), \u2026, \u015d(x_n,z_n) ).\n\n\n\n\t\n\n \u00a7.\u00a7 Strong Converse Proof\n\n\n\nFix a sequence of  encoding and decoding functions {(\u03d5^(n), g^(n))}_n=1^\u221e and consider the  optimal estimator h^(n) in Lemma\u00a0<ref>. Assume that   (<ref>)  and (<ref>) are satisfied. For readability, we will also write x^n(\u00b7) for the function \u03d5^(n)(\u00b7). \n\nChoose a sequence of small positive numbers  {\u03bc_n }_n=1^\u221e satisfying\n\n    lim_n\u2192\u221e\u03bc_n      =    0 \n    lim_n\u2192\u221e(n \u00b7\u03bc_n^2  )^-1    =    0.\n \n\n\n\nExpurgation:\n\nFix \u03b7\u2208 (0,1-\u03f5-\u03b4] and let  \u2133\u0303 be the set of messages m that satisfy the following two conditions:\n\n\trCl \n\t\t[M\u0302 \u2260M|M=m ]    \u2264     1-\u03b7 \n\n\t\t[ dist^(n)( \u015c^n, S^n) > D |M=m ]    \u2264    1-\u03b7. \n\t\n\nSince the set of messages  not satisfying  (<ref>) is at most of size \n\n    \u03f5/(1-\u03b7) 2^nR,\n\nand similarly also the set of \nmessages  not satisfying  (<ref>) is of size at most  \u03b4/(1-\u03b7)2^nR, we can deduce that the set \u2133\u0303 (which is the complement of the union of these two sets) is of size at least \n\n    (1-  \u03f5+\u03b4/1-\u03b7)2^nR=(1-\u03b7 - \u03f5-\u03b4)/(1-\u03b7)2^nR.\n\nDefine the random variable M\u0303 to be uniform over the set \u2133\u0303 and let \n\n    X\u0303^n=x^n(M\u0303),\n\nand thus \n\n    |\u2133\u0303|/2^nR\u2265(1-  \u03f5+\u03b4/1-\u03b7)= : \u03b3.\n\n\nLet T  be a uniform random variable over {1,\u2026, n}, independent of all other random variables and notice that \nrCl \n\tP_X\u0303_T(x)    =    1/n \u2211_t=1^n P_X\u0303_t(x) \n\n\t    =     1/n \u2211_t=1^n \ud835\udd3c[1{ X\u0303_t(M\u0303)=x}]\n\n\t   =      \ud835\udd3c[\u03c0_x^n(M\u0303)(x) ]. \n\nLet now {n_i} be an increasing subsequence of blocklengths so that the probability vector P_X\u0303_T converges and denote the convergence point by P_X:\nrCl\n\tlim_n_i \u2192\u221e  1/|\u2133\u0303| \u2211_m\u2208M\u0303    \u03c0_x^n_i(m)(x) \n    =:     P_X(x), \n\t    \u2200x \u2208\ud835\udcb3.  \nIn the remainder of this proof, we restrict attention to this subsequence of blocklengths {n_i}.\n\n\n\nProof of Channel Coding Bound:\nWe first prove the converse bound for channel coding. To this end, consider the two conditions\n\n\trCl \n\t\tg^(n)( y^n)    =    m  \n\n\n\t\t\n\t\t| \u03c0_s^n,x^n(m),y^n(a,b,c) - P_S(a) \u03c0_x^n(m)(b) P_Y|XS(c|a,b) |    \u2264    \u03bc_n , \n\n\t\n\nand define for  each message m \u2208\u2133\u0303 the set \nrCl \n\t\ud835\udc9f_\ud835\udc9e,m    :=     {  (s^n,y^n)    (<ref>)   and   (<ref>) }. \n\t\n\t\n\nIntroduce the   new random variables (S^n_\ud835\udc9e,Y^n_\ud835\udc9e) of  joint conditional pmf\nrCl \n\t\n\t\tP_S^n_\ud835\udc9e Y^n_\ud835\udc9e|M\u0303 (s^n,y^n|m)  \n\n\t   =      P_S^\u2297n(s^n) \u00b7P_Y|XS^\u2297n(y^n |x^n(m), s^n)/  \u0394_\ud835\udc9e,m \u00b71 { (s^n,y^n) \u2208\ud835\udc9f_\ud835\udc9e,m },\n\n\nfor \nrCl\n\t\u0394_\ud835\udc9e,m :=  \u2211_s^n,y^n P_S^\u2297n(s^n) \u00b7P_Y|XS^\u2297n(y^n |x^n(m), s^n)    \n\n\t        \u00b71 { (s^n,y^n) \u2208\ud835\udc9f_\ud835\udc9e,m }.\n\n\n\n\n\nBy  <cit.> and Conditions (<ref>) and (<ref>), we have:\nrCl\n\t\n\t\n\t\u0394_\ud835\udc9e,m\u2265\u03b7-|\ud835\udcae||\ud835\udcb3||\ud835\udcb4|/4\u03bc_n^2 n,   \u2200m \u2208\u2133\u0303.\n \nMoreover, for \nM\u0303=m:\nrCl\n\t\tP_Y^n_\ud835\udc9e|M\u0303=m(y^n) \n\n\t   =      \u2211_s^n  P_S^\u2297n(s^n) \u00b7P_Y|XS^\u2297n(y^n |x^n(m), s^n)/  \u0394_\ud835\udc9e,m \u00b71 { (s^n,y^n) \u2208\ud835\udc9f_\ud835\udc9e,m } \n\n\n\t   \u2264   \u2211_s^n  P_S^\u2297n(s^n) \u00b7P_Y|XS^\u2297n(y^n |x^n(m), s^n)/  \u0394_\ud835\udc9e,m \n\n\t    =      P_Y|X^\u2297n(y^n|x^n(m))/\u0394_\ud835\udc9e,m. \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinue to notice that:\nrCl\n\tR    =    1/n H(M\u0303)    - 1/n log\u03b3\n\n\t\n\t    (a)=    1/n  I( M\u0303 ; Y_\ud835\udc9e^n)-  1/n log\u03b3\n\n\t    =   1/n  H(Y^n_\ud835\udc9e) -1/n H(Y_\ud835\udc9e^n |M\u0303 )- 1/n log\u03b3\n\n\t    \t\u2264     1/n  \u2211_i=1^n H(Y_\ud835\udc9e,i) \t-\t1/n H(Y_\ud835\udc9e^n|M\u0303) - 1/n log\u03b3\n\n\t   =    H(Y_\ud835\udc9e,T |T)- 1/n H(Y_\ud835\udc9e^n|M\u0303) - 1/n log\u03b3\n\n\t   \u2264    H(Y_\ud835\udc9e,T)- 1/n H(Y_\ud835\udc9e^n|M\u0303)   -1/n log\u03b3,\n\nwhere we defined the random variable T to be uniform over {1,\u2026,n} independent of the other random variables.  Here, (a) holds because M\u0303=g(Y^n_\ud835\udc9e) by Condition\u00a0(<ref>).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice next that \nrCl\n\tP_X\u0303_TS_\ud835\udc9e,\ud835\udcafY_\ud835\udc9e,T(x,s,y)   \n\n\t   =   1/n  \u2211_t=1^n P_X\u0303_tS_\ud835\udc9e,tY_\ud835\udc9e,t(x,s,y) \n\n\t   =   1/n  \u2211_t=1^n \ud835\udd3c[ 1{X\u0303_t,S_\ud835\udc9e,t,Y_\ud835\udc9e,t)=(x,s,y)}] \n\n\t   =    [  \u03c0_x^n(M\u0303)S_\ud835\udc9e^nY_\ud835\udc9e^n(x,s,y)]  \n\t\n\t\n\t\n\nHowever, by Condition\u00a0(<ref>) \nfor any triple (x,s,y) with positive P_S(s) P_Y|XS(y|x,s) the following inequality is satisfied with probability 1:\nrCl\n\t|\u03c0_x^n(m)S^n_\ud835\udc9eY_\ud835\udc9e^n(x,s,y) -  \u03c0_x^n(m)(x) P_Y|XS(y|x,s)P_S(s) | \n\n\t        \u2264\u03bc_n.  \n\n\n\n\n\n\nNotice that  by (<ref>) and (<ref>) and \nsince \u03bc_n_i\u2192 0 as n_i\u2192\u221e:rCl\n\tlim_i \u2192\u221e P_ X\u0303_TS_\ud835\udc9e,TY_\ud835\udc9e,T(x,s,y)= P_X(x) P_S(s)P_Y|XS(y|x,s),  \nwhich by continuity of the entropy functional implies \n\n    lim_n_i\u2192\u221e  H(Y_\ud835\udc9e,T) =H_P_XP_SP_Y|XS(Y).\n\n\n\nNext, by  definition and by (<ref>):\nrCl\n\t1/n_i H(Y^n_i_\ud835\udc9e |M\u0303=m)  \n\n\t    =    - 1/n_i  \u2211_y^n \u2208\ud835\udc9f_\ud835\udc9e,m P_Y^n_i_\ud835\udc9e|M\u0303=m(y^n_i) logP_Y_\ud835\udc9e^n_i|M\u0303=m(y^n_i) \n\n\t    \u2265   - 1/n_i  \u2211_y^n_i \u2208\ud835\udc9f_\ud835\udc9e,m P_Y^n_i_\ud835\udc9e|M\u0303=m(y^n_i) logP_Y|X^\u2297n(y^n_i|x^n_i(m))/ \u0394_\ud835\udc9e,m \n \n\n\t    =    -1/n_i \u2211_t=1^n_i  \u2211_y^n_i \u2208\ud835\udc9f_\ud835\udc9e,m P_Y^n_i_\ud835\udc9e|M\u0303=m(y^n_i) logP_Y|X(y_t|x_t(m)) \n\n\t        + 1/n_i log\u0394_\ud835\udc9e,m, \n\n\t    =    -1/n_i \u2211_t=1^n_i  \u2211_y_t\u2208\ud835\udcb4 P_Y_\ud835\udc9e,t|M\u0303=m(y_t) logP_Y|X(y_t|x_t(m)) \n\n\t       + 1/n_i log\u0394_\ud835\udc9e,m, \n\n\t    =    -1/n_i \u2211_t=1^n_i  \u2211_y\u2208\ud835\udcb4  [   1{ Y_\ud835\udc9e,t=y } |M\u0303=m] logP_Y|X(y|x_t(m)) \n\n\t      + 1/n_i log\u0394_\ud835\udc9e,m,\n\n\t    =    -   \u2211_x\u2208\ud835\udcb3 \u2211_y\u2208\ud835\udcb4 [1/n_i    \u2211_t=1^n_i  1{x_t(m)=x,Y_\ud835\udc9e,t=y } |M\u0303=m] \n\n\t       \u00b7logP_Y|X(y|x)  \n\n\t       + 1/n_i log\u0394_\ud835\udc9e,m, \n\n\t\n\t    =    - \u2211_x\u2208\ud835\udcb3 \u2211_y\u2208\ud835\udcb4 \u2211_s\u2208\ud835\udcae  [ \u03c0_x^n_i(m)S^n_i_\ud835\udc9e Y_\ud835\udc9e^n_i(x,s,y)|M\u0303=m]\n\n\t       \u00b7logP_Y|X(y|x)  \n\n\t      + 1/n_i log\u0394_\ud835\udc9e,m,\n\t\n\t\n\t\n\nwhere P_Y|X(y|x)=\u2211_s\u2208\ud835\udcae P_Y|XS(y|x,s) P_S(s).\nAveraging over all messages m\u2208\u2133\u0303, we obtain:\nrCl\n\t1/n_iH(Y^n_i_\ud835\udc9e |M\u0303) \n\n\t   \u2265    - \u2211_x\u2208\ud835\udcb3   \u2211_y\u2208\ud835\udcb4  \u2211_s\u2208\ud835\udcae   [ \u03c0_x^n_i(M\u0303)S^n_i_\ud835\udc9eY_\ud835\udc9e^n_i(x,s,y)  ] \u00b7logP_Y|X(y|x) \n\n\t       +   [  1/n_i log\u0394_\ud835\udc9e,M\u0303].\n\n\n\nBy (<ref>) the term  E[  1/n_ilog\u0394_\ud835\udc9e,M\u0303] vanishes for increasing blocklengths, and thus using the definition of  P_X in (<ref>), \nfone can follow the same bounding steps as leading to (<ref>) to obtain: \nrCl\n\tlim_i\u2192\u221e 1/n_i H(\u1ef8^n_i |M\u0303)  \n\n\t   =    - \u2211_x\u2208\ud835\udcb3 P_X(x) \u2211_y \u2208\ud835\udcb4 \u2211_s\u2208\ud835\udcae P_S(s) P_Y|XS(y|x,s) logP_Y|X(y|x) \n\n\t   =   H_P_XP_SP_Y|XS(Y|X).\n\n\nCombining (<ref>) with (<ref>) and (<ref>), and since 1/n_ilog\u03b3\u2192 0  as n\u2192\u221e, we can conclude that \nrCl\n\tR    \u2264    H_P_XP_SP_Y|XS(Y) - H_P_XP_SP_Y|XS(Y|X) \n\n\t   =     I_P_XP_SP_Y|XS(X;Y).\n\n\n\n\n\n\nProof of Distortion Bound:\nConsider the two conditions\n\n\trCl \n\t\tdist^(n)(h^(n)(x^n(m),z^n) , s^n)     \u2264    D \n  \n\n\t\t| \u03c0_s^n,x^n(m),z^n(a,b,c) - P_S(a) \u03c0_x^n(m)(b) P_Z|XS(c|a,b) |    \u2264    \u03bc_n , \n\n\t\n\nand define for  each message m \u2208\u2133\u0303 the set \nrCl \n\t\ud835\udc9f_\ud835\udcae,m    :=     {  (s^n,z^n)    (<ref>)   and   (<ref>) }. \n\t\n\t\n\n\nRecall the definition X\u0303^n=x^n(M\u0303) and the limit in (<ref>).  \nDefine  the   new random variables (S^n_\ud835\udcae, Z^n_\ud835\udcae) of  joint conditional pmf\nrCl \n\t\n\t\tP_S^n_\ud835\udcaeZ^n_\ud835\udcae|M\u0303 (s^n,z^n|m)  \n\n\t   =    P_S^\u2297n(s^n) \u00b7P_Z|XS^\u2297n( z^n |x^n(m), s^n)/  \u0394_\ud835\udcae,m  \u00b71 { (s^n,z^n) \u2208\ud835\udc9f_\ud835\udcae,m }, \n\n\nfor \nrCl\n\t\u0394_\ud835\udcae,m := \u2211_s^n,z^n P_S^\u2297n(s^n) \u00b7P_Z|XS^\u2297n(z^n |x^n(m), s^n)    \n\n\t        \u00b71 { (s^n,z^n) \u2208\ud835\udc9f_\ud835\udcae,m }.\nNotice that by  <cit.> and Conditions (<ref>) and  (<ref>), we have:\nrCl\n\t\n\t\n\t\u0394_\ud835\udcae,m\u2265\u03b7-|\ud835\udcae||\ud835\udcb3||\ud835\udcb5|/4\u03bc_n^2 n,     \u2200m \u2208\u2133\u0303.  \n\n\nFollowing similar steps to (<ref>)\u2013(<ref>), by (<ref>) and definition (<ref>),  we can conclude that \nrCl\n\tlim_n_i \u2192\u221e P_ X\u0303_TS_\ud835\udcae,TZ_\ud835\udcae,T(x,s,z) = P_X(x) P_S(s)P_Z|XS(z|x,s). \n\n\t\n\n\nBy Condition\u00a0(<ref>), we have  with probability 1: \nrCl\n\tD     \u2265   1/n  \u2211_t=1^n d ( \u015d(X\u0303_t, Z_\ud835\udcae,t)  ,   S_\ud835\udcae,t).\n\nTherefore, \nfor any blocklength n_i:\nrCl\n\tD     \u2265   1/n_i  \u2211_j=1^n_i  \ud835\udd3c[ d ( \u015d(X\u0303_j, Z_\ud835\udcae,j)  ,   S_\ud835\udcae,j)]  \n\n\t    =     \ud835\udd3c[ d ( \u015d(X\u0303_T, Z_\ud835\udcae,T)  ,   S_\ud835\udcae,T) ],\n\nand by (<ref>) in the limit as n_i\u2192\u221e:\nrCl\n\tD     \u2265    \ud835\udd3c_P_XP_SP_Z|XS[ d ( \u015d(X, S)  ,    S) ].\n\nThis concludes the proof of the converse.\n\n\n\n\n\n\n\n\u00a7 STEIN'S EXPONENT AS A  SENSING MEASURE\n\nIn this section we assume that the state-sequence S^n depends on a binary hypothesis \u210b\u2208{0,1}. Under the null hypothesis \u210b=0 it is i.i.d. according to the pmf P_S and under the alternative hypothesis \u210b=1 it is  i.i.d. according to the pmf Q_S. The radar receiver  attempts to guess the underlying hypothesis based on the inputs and backscattered signals, so it produces a guess of the form \n    \u210b\u0302 = h^(n)(X^n,Z^n) \u2208{0,1}.\n\n \n\n\nRadar sensing performance is measured in terms of Stein's exponent. That means, it is required that the type-I error probability \n\n    \u03b1_n:=[  \u210b\u0302=1|\u210b=0]\n stays below a given threshold, while the type-II error probability \n\n    \u03b2_n :=[  \u210b\u0302=0|\u210b=1]\n\nshould decay exponentially fast to 0 with largest possible exponent. \n\n\n\n\n\nA rate-exponent pair (R,E) is  (\u03f5,\u03b4)-achievable over the state-dependent  DMC (\ud835\udcb3,\ud835\udcb4, P_Y|XS) with state-distribution P_S, if there exists a sequence of encoding, decoding, and estimation functions {(\u03d5^(n), g^(n), h^(n))} such that for each blocklength n the average  probability of error satisfies \n\n    _n\u2192\u221e p^(n)(error) \u2264\u03f5,     \u210b\u2208{0,1},\n\nwhile the detection error probabilities satisfy:  \n\n    _n\u2192\u221e\u03b1_n\u2264\u03b4,\n\nand \n\n    -_n\u2192\u221e1/nlog\u03b2_n\u2265 E.\n\n\n\n\nFor any \u03f5, \u03b4\u2265 0 satisfying \u03f5+\u03b4 <1, a rate-exponent pair (R,E) is (\u03f5,\u03b4)-achievable, if and only if, there exists a pmf P_X \n satisfying\n \n    R \u2264min{ I_P_XP_SP_Y|XS(X;Y),   I_P_XQ_SP_Y|XS(X;Y)},\n\nand \n \n    E \u2264\ud835\udd3c_P_X[D( P_Z|X Q_Z|X) ]\n\nwhere P_Z|X and Q_Z|X denote the conditional marginals of P_SP_Z|XS and  Q_SP_Z|XS, respectively.\n\n\n \n\nAchievability follows by standard random coding for a compound channel and by applying a Neyman-Pearson test at the radar receiver. The converse is proved in Appendix\u00a0<ref>.\n\n\n\nThe works in <cit.>  consider degenerate state-distributions where P_S and Q_S are deterministic distributions. In this case, our Theorem simplifies as follows.[Recall that  <cit.>  required exponential decrease both for the type-I and type-II error probabilities \u03b1_n and \u03b2_n. Their result is thus not a special case of ours.] \n\n\nAssume degenerate state-distributions P_S(s_0)=1 and Q_S(s_1)=1 for two distinct symbols s_0,s_1\u2208\ud835\udcae. Then, for any \u03f5, \u03b4\u2265 0 satisfying \u03f5+\u03b4 <1, a rate-exponent pair (R,E) is (\u03f5,\u03b4)-achievable, if and only if, there exists a pmf P_X \n satisfying\n \n    R \u2264min{ I_P_XP_Y|X^(s_0)(X;Y),    I_P_XP_Y|X^(s_1)(X;Y)},\n\nand \n \n    E \u2264\ud835\udd3c_P_X[D( P_Z|XS(\u00b7|X,s_0) P_Z|XS(\u00b7|X,s_1) ] ,\n\nwhere P_Y|X^(s)(y|x)\u225c P_Y|XS(y| x,s) for any triple (x, s, y).\n\n\n\n\n\u00a7 CONCLUSION AND FUTURE DIRECTIONS\n\nIn this paper we established the strong converse for two ISAC problems with bi-static radar whenever \u03f5+\u03b4 < 1. Interesting future research directions include extensions to mono-static radar systems where the transmitter can apply closed-loop encodings depending also on past generalized feedback systems or  systems with memory. Analyzing other sensing criteria is also of interest, such as the minimum exponential decay-rate over all hypotheses or the estimation error when the distribution of the state-sequence depends on a single continuous-valued parameter.\nThe setup where only part of the state or a noisy version of the state is to be estimated is also of interest, for instance, in scenarios in  state-dependent fading channels where one \n\thas no interest in estimating the fading. Notice that this setup is included in the rate-distortion model through an appropriate definition of the distortion measure. On a related note, our model also includes as special case the setups where the receiver has perfect or imperfect channel-state information by including this state-information as part of the output. \n\t\n\nieeetran\n\n\n\n\n\n\n\u00a7 STRONG CONVERSE PROOF\n\n\n\nFix a sequence of  encoding,  decoding, and estimation functions {(\u03d5^(n), g^(n), h^(n))}_n=1^\u221e. Assume that   (<ref>)  and (<ref>) are satisfied. For readability, we will also write x^n(\u00b7) for the function \u03d5^(n)(\u00b7). \nChoose a sequence of small positive numbers  {\u03bc_n } satisfying\n\n    lim_n\u2192\u221e\u03bc_n      =    0 \n    lim_n\u2192\u221e(n \u00b7\u03bc_n^2  )^-1    =    0.\n \n\n\nExpurgation: Fix \u03b7\u2208 (0,1-\u03f5-\u03b4] and let  \u2133\u0303 be the set of messages m that satisfy the following two conditions:\n\nrCl \n[M\u0302 \u2260M|M=m ]    \u2264   1-\u03b7\n\n\n[  \u210b\u0302=1|\u210b=0 , M=m ]    \u2264    1-\u03b7.\n \n \n The size of the set  \u2133\u0303 satisfies \n\n    |\u2133\u0303|/2^nR\u2265(1-  \u03f5+\u03b4/1-\u03b7)= : \u03b3.\n\n Define the random variable M\u0303 to be uniform over the set \u2133\u0303 and X\u0303^n=x^n(M\u0303).\n \n Let T be uniform over {1,\u2026, n} independent of all other quantities  and notice that \nrCl \nP_X\u0303_T(x)   =      \ud835\udd3c[\u03c0_x^n(M\u0303)(x) ]. \n \n\nConsider an increasing subsequence of blocklengths n_i so that the probabilities P_X\u0303_T(x) converge and denote the convergence point by P_X: \n \n    lim_n_i \u2192\u221e P_X\u0303_T= P_X(x),      x \u2208\ud835\udcb3.\n \n \n\n\nProof of Channel Coding Bound:  Considering the sequence of blocklengths {n_i}_i=1^\u221e and following  steps (<ref>)\u2013(<ref>) in the previous Section\u00a0<ref>, once with the pmf P_S and  once with the pmf Q_S, one can show that \n\n    R \u2264min{ I_P_XP_SP_Y|XS(X;Y),    I_P_XQ_SP_Y|XS(X;Y)},\n\nwhere P_X denotes the pmf in (<ref>). \n\n\n\nProof of Stein's Exponent:\nConsider conditions \n\nrCl \nh^(n)(x^n(m),z^n)  )     =     0\n  \n\n | \u03c0_s^n,x^n(m),z^n(a,b,c) - P_S(a) \u03c0_x^n(m)(b) P_Z|XS(c|a,b) |    \u2264    \u03bc_n , \n\n \n \nand define for  each message m \u2208\u2133\u0303 the set \nrCl \n\ud835\udc9f_\ud835\udcae,m    :=     {  (s^n,z^n)    (<ref>)   and   (<ref>) }. \n\n\n\nDefine  the   new random variables (Y^n_\ud835\udcae, Z^n_\ud835\udcae) of  joint conditional pmf\nrCl \n\nP_S^n_\ud835\udcae  Z^n_\ud835\udcae|M\u0303 (s^n, z^n|m)  \n\n   =    P_S^\u2297n(s^n) \u00b7P_Z|XS^\u2297n( z^n |x^n(m), s^n)/  \u0394_\ud835\udcae,m  \u00b71 { (s^n,z^n) \u2208\ud835\udc9f_\ud835\udcae,m },\n\n\nfor \nrCl\n\u0394_\ud835\udcae,m := \u2211_s^n,z^n P_S^\u2297n(s^n) \u00b7P_Z|XS^\u2297n( z^n |x^n(m), s^n)    \n\n        \u00b71 { (s^n,z^n) \u2208\ud835\udc9f_\ud835\udcae,m }.\nBy  <cit.> and Conditions (<ref>) and  (<ref>), we have:\nrCl\n\n\n\u0394_\ud835\udcae,m\u2265\u03b7-|\ud835\udcae||\ud835\udcb3||\ud835\udcb5|/4\u03bc_n^2 n,     \u2200m \u2208\u2133\u0303.  \n\n\nFollowing similar steps to (<ref>)\u2013(<ref>), we can conclude that \nrCl\nlim_n_i \u2192\u221e P_ X\u0303_TS_\ud835\udcae,TZ_\ud835\udcae,T(x,s,z)     \n\n     =     P_X(x) P_S(s)P_Z|XS(z|x,s). \n\n\nWe next notice the inequalities:\nrCl\n\u03b2_n     =     [ \u210b\u0302=0 |\u210b=1] \n\n   =      1/2^nR \u2211_m=1^2^nR [ \u210b\u0302=0 |\u210b=1, M=m] \n\n\n   \u2265    |\u2133\u0303|/2^nR \u00b71/|\u2133\u0303| \u2211_m\u2208\u2133\u0303 [ \u210b\u0302=0 |\u210b=1, M\u0303=m] \n\n    \u2265      \u03b3\u00b7\ud835\udd3c_M\u0303[ [ \u210b\u0302=0 |\u210b=1, M\u0303=m] ],\n\nand therefore, \nrCl\n- 1/n log\u03b2_n  \n\n    \u2264    - 1/n log\ud835\udd3c_M\u0303[ [ \u210b\u0302=0 |\u210b=1, M\u0303=m] ] - 1/n log\u03b3,\n\n\nwhere notice that the term 1/nlog\u03b3 vanishes asymptotically for infinite blocklengths. \n\nDefining \nrCl Q_Z|X(z|x)   :=    \u2211_s Q_S(s) P_Z|SX(z|x,w)\n\nP_Z|X(z|x)   :=    \u2211_s P_S(s) P_Z|SX(z|x,w),\n\n we can further obtain: \nrCl\n - 1/n log\ud835\udd3c_M\u0303[ [ \u210b\u0302=0 |\u210b=1, M\u0303=m] ]\n\n    =    - 1/n logP_X\u0303^n Q_Z|X^\u2297n(\u210b\u0302=0) \n\n    =    1/n D( P_X\u0303^nZ_\ud835\udcae^n  (\u210b\u0302)     P_X\u0303^n Q_Z|X^\u2297n (\u210b\u0302) ) \n\n    \u2264   1/n D( P_X\u0303^nZ_\ud835\udcae^n     P_X\u0303^n Q_Z|X^\u2297n ) \n\n\n\n\n    =    1/n \u2211_m \u2208\u2133\u0303  1/|\u2133\u0303| \u2211_z^n P_ Z_\ud835\udcae^n|X\u0303^n(z^n|x^n(m)) \n\n        \u00b7log P_ Z_\ud835\udcae^n|X\u0303^n(z^n|x^n(m))/ Q_Z|X^\u2297n(z^n|x^n(m) )  \n\n     \u2264   1/n \u2211_m \u2208\u2133\u0303  1/|\u2133\u0303| \u2211_z^n P_ Z_\ud835\udcae^n|X\u0303^n(z^n|x^n(m))     \n\n        \u00b7log P_ Z|X^\u2297n (z^n|x^n(m))/ Q_Z|X^\u2297n(z^n|x^n(m) )    \n\n        - 1/n \u2211_m \u2208\u2133\u0303  1/|\u2133\u0303|  log\u0394_\ud835\udcae,m\n\n      =    1/n \u2211_x^nP_X\u0303^n Z_\ud835\udcae^n(x^n,z^n) log P_ Z|X^\u2297n (z^n|x^n)/ Q_Z|X^\u2297n(z^n|x^n ) \n\n         - 1/n \ud835\udd3c[  log\u0394_\ud835\udcae,M\u0303 ] \n\n       =    1/n \u2211_t=1^n \u2211_x_t,z_t P_X\u0303_t Z_\ud835\udcae,t (x_t,z_t) log P_ Z|X (z_t|x_t)/ Q_Z|X(z_t|x_t )  \n\n         - 1/n \ud835\udd3c[  log\u0394_\ud835\udcae,M\u0303 ] \n\n        =     \u2211_x,z P_X\u0303_T Z_\ud835\udcae,T (x,z) log P_ Z|X (z|x)/ Q_Z|X(z|x)  \n\n           - 1/n \ud835\udd3c[  log\u0394_\ud835\udcae,M\u0303 ].\n\n\nCombining (<ref>), (<ref>), and (<ref>), and considering the subsequence of blocklengths {n_i}, we conclude  that \n\n    _i\u2192\u221e - 1/nlog\u03b2_n_i\u2264\ud835\udd3c_P_X[ D(P_Z|X Q_Z|X)  ],\n\nwhere we used that 1/n\ud835\udd3c[  log\u0394_\ud835\udcae,M\u0303] \u2192 0 as n_i\u2192\u221e by (<ref>).\nThis concludes the proof of the converse.\n\n\n\n"}