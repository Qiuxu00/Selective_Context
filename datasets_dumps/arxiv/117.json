{"entry_id": "http://arxiv.org/abs/2303.07196v1", "published": "20230313153419", "title": "A Comprehensive Empirical Evaluation of Existing Word Embedding Approaches", "authors": ["Obaidullah Zaland", "Muhammad Abulaish", "Mohd. Fazil"], "primary_category": "cs.CL", "categories": ["cs.CL", "cs.NE"], "text": "\n\n\n\n\n\n\n\nVector-based word representations help countless Natural Language Processing (NLP) tasks capture both semantic and syntactic regularities of the language. In this paper, we present the characteristics of existing word embedding approaches and analyze them with regards to many classification tasks. We categorize the methods into two main groups - Traditional approaches mostly use matrix factorization to produce word representations, and they are not able to capture the semantic and syntactic regularities of the language very well. Neural-Network based approaches, on the other hand, can capture sophisticated regularities of the language and preserve the word relationships in the generated word representations. We report experimental results on multiple classification tasks and highlight the scenarios where one approach performs better than the rest.  \n\n\n\n\n\n\nSocial network analysis of Japanese manga: similarities to real-world social networks and trends over decades\n    Naoki Masuda\n    March 30, 2023\n=============================================================================================================\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nDense real-valued word vector representations have been essential to NLP tasks, such as question answering <cit.>, semantic role labeling <cit.>, textual entailment <cit.> and machine translation <cit.>. These word representations, also called word embeddings, encode semantic and syntactic characteristics of the words, so these embeddings can act as input features to downstream tasks like sentiment analysis <cit.>, rumor detection <cit.>, and fake news detection <cit.>.\n\nSince the early days of natural language understanding, an exhaustive amount of research has been put into obtaining these word representations from a corpus of unlabeled text. Researchers have used statistical models <cit.>, neural network language models <cit.>, log-bilinear models <cit.>, and context-based neural models <cit.>, among others, to construct these word representations. Nevertheless, the question of which approach to use in a specific scenario remains unanswered. \nTo compare word embedding methods, we can use intrinsic or extrinsic evaluators<cit.>. Intrinsic evaluators measure the quality of word embeddings directly using semantic and syntactic relationships among them<cit.>, while extrinsic evaluators measure the quality of these word embeddings on downstream tasks using them as input features. \n\nWords with closer meaning to each other are called semantically related terms, and the embeddings produced for such words shall lie closer in the produced embedding space. For example, words like bicycle, cycle, and bike are similar and semantically related to each other. Syntactically related words are those words which are bind by the syntax of a natural language such as English. For example, brief and briefly are syntactically related to each other, where briefly is the adverb for the adjective brief. Also, big and bigger are syntactically related, where the latter is the comparative form of the former.\n\n\nMost studies investigate word embedding models concerning their intrinsic characteristics. Although intrinsic evaluation of these models shed light on the semantic and syntactic similarities among these words, they do not explicitly depict the scenarios where one model is superior or inferior to other models. Moreover, intrinsic evaluators require additional resources in the form of pre-defined queries for subjective tests<cit.>. These queries can be word couplets such as adjectives and their comparative forms, or countries and their capitals, and are also called query inventories<cit.>. \n\nExtrinsic evaluation of these methods, on the other hand,  maybe computationally expensive and time taking, but it provides much more insight into the quality of the word representations that these methods extract. To evaluate the actual quality of these methods, we should compare these approaches with regards to their real word use cases. Small factors like the window size, data pre-processing, skewness of dataset, and dimension of the word embeddings, sometimes have an impact on the result of downstream tasks (e.g., classification) <cit.>. \n\nHowever, researchers have investigated the relationship between intrinsic and extrinsic evaluators; they have not been comprehensive<cit.>. Besides, these comparative studies have not evaluated all the existing models, as most studies only focused on examining LSA, word2vec and GloVe <cit.>. Furthermore, as per our knowledge, there has been no extensive extrinsic evaluation which involves all the state of the art models (GloVe, Word2vec, ELMo, BERT and fasttext). \n\nAn excellent word representation method should be able to take care of some essential points.\n\n    \n  * The most frequent words such as the or and should not affect the quality of the word vectors.\n    \n  * Rare words should have quality word representations.\n    \n  * Multiple word embeddings for multiple word senses.\n\nApart from the above qualities, intrinsic to the word embeddings, the features extracted by these models should work well in different scenarios; for example, if we are dealing with sentiment analysis, the features should work fine with both balanced and unbalanced data. In this paper, we use pre-trained vectors, trained using the dominant word representation algorithms for different scenarios of classification tasks. We also train word embeddings on these algorithms from scratch to measure the effect of pre-training with respect to each algorithm in various tasks. Besides, we study the impact of model parameters (window size and embedding dimensions) on the output of the downstream tasks. The effect of external parameters on the output of these models is also studied. These external parameters are the degree of formality of corpora used for pre-training, amount of text in the corpus used for pre-training, and pre-processing. \n\nWe organize the rest of the paper in the following sections. In section 2, we introduce existing word representation models, including traditional and neural models. We present the properties of word embedding models and compare the existing models according to these properties in section 3. The results have been reported in section 4. We have carried out exhaustive comparisons on both pre-trained and trained word embeddings. In the end, we conclude our findings in section 5. \n\n\n\n\u00a7 STATE OF THE ART WORD EMBEDDING APPROACHES\n\n\nWord embeddings act as a backbone for the downstream natural language processing tasks. Hence numerous approaches have been proposed to train these embeddings over the years. For clarity, we classify these models into two categories - traditional models and neural network models. \n\n\n\n \u00a7.\u00a7 Traditional models\n\n\nTraditional models construct the word representations from the statistical information present in the corpus on the basis of the idea of distributional semantics. They use term frequencies, term-term co-occurrence frequencies <cit.>, and term-document frequencies <cit.> as the basis for these vector representations. One Hot Encoding, the simplest model, uses a vector of size |V|, where |V| is the size of the vocabulary, to represent each word. The word vectors produced, have a value 1, at a specific position for that word and 0 everywhere else. For example, if V = {have, a, great, day}, then the word have can be represented as {1,0,0,0}, the word a can be represented as {0,1,0,0}, and so on.  \n\nLatent Semantic Analysis (LSA) <cit.> is the most influential models in this category. LSA utilizes the statistical information present in the corpus to build a term-document frequency matrix X. LSA then uses Singular Value Decomposition (SVD) for finding a low-rank approximation to the matrix to construct the word vector representations. SVD decomposes the co-occurrence matrix X into three matrices, V, V^T and \u03a3. \n\n    X = V\u03a3 V^T\n\nWhile \u03a3 is a diagonal matrix comprising the singular values of the matrix, V and V^T are orthogonal matrices, comprising of left and right singular vectors. For obtaining a lower-rank approximation of rank j, for the matrix X, we select the j largest singular values alongside the right and left singular vectors from V and V^T corresponding to those singular values. \n\n    X_j = V_j \u03a3_j V_j^T\n\n\nHyperspace Analogue to Language (HAL)<cit.>, on the other hand, uses term-term frequencies to construct the co-occurrence matrix X. HAL passes a \"window\" over the text, and words within the window are termed to co-occur with a strength inversely proportionate to the number of terms between them in the window. Words occurring to the right and left of a word are recorded separately. As a result, a matrix with n rows and 2n columns, n being the size of the vocabulary, is formed. As the vocabulary grows, the co-occurrence becomes enormous, and so does the word embedding size. Hence, to reduce the dimensionality of word embeddings, columns with higher variance are selected out of 2n columns. Columns related to the most frequent words such as the have higher frequencies while providing little information about the text. These high-frequency or high variance columns contribute disproportionately to the distance between produced word representation vectors. \n\nCorrelated Occurrence Analogue to Lexical Semantic (COALS)<cit.> uses a normalization procedure to minimize the influence of most common words on the quality of produced word embeddings. COALS removes different columns for left and right contexts and adds a single column for each word, making the co-occurrence matrix symmetric. Moreover, COALS uses conditional rate, that is, whether word a co-occurs more or less with word b than in general, as matrix entries instead of raw term-term co-occurrence count. Pearson's correlation coefficient can be used to calculate the conditional rate between word pairs. After formulating the co-occurrence matrix in this manner, negative entries are removed from the matrix, and the positive values get replaced by their square roots. The authors explain that the negatively correlated terms may not relate to each other semantically and haven't been used in the text corresponding to the same topic. They argue that removing the negatively correlated word columns results in less information loss than removing low-variance word frequency columns.\n\nA variety of traditional models use the corpus statistics in different manners to form the co-occurrence matrix. Moreover, apart from SVD, other transformation methods such as Hellinger Principal Composition Analysis (HPCA) <cit.> have been used to learn these word representations. Although traditional models are easy to understand, they are computationally expensive and become infeasible, to work with on large datasets<cit.>, because the co-occurrence matrix becomes huge and impractical to operate on. \n\n\n\n \u00a7.\u00a7 Neural Models\n\n\nUnlike traditional models, neural models have evolved extensively with time. The first generation of these models was Neural Network Language Models (NNLM). Although initial Neural Network Language Models <cit.> solved the curse of dimensionality problem present in statistical models, noticeable gains came after the introduction of Recurrent Neural Network Language Models (RNN-LM) <cit.>. Language modeling predicts the word sequence w_1w_2...w_T probability in a natural language text.\n\n    P(w_1^T) = \u220f_t=1^T P(w_t|w_1^t-1)\n\n\nThe Feed-Forward Neural Network Language Models (FNNLM) accomplishes the same task, but instead of considering all the history words, it adopts the n-gram based idea and considers only n-1 words before the current word. In FNNLM, \n\n\n    \u220f_t=1^T P(w_t|w_1^t-1) \u2248\u220f_t=1^T P(w_t|w_t-n+1^t-1)\n\n\n\nRecurrent Neural Network Language Models (RNNLM) are a little different, as they have an internal state space. This internal state space can work as a memory, which stores information related to all the sequences in history and gets passed to the next sequence enabling the model to deal with the uncertain length of sequences. \n\nThe main concern with these language models was that they used 1-of-V encoding, also called one-hot encoding (OHE). In OHE, every word has a vector representation of size V (vocabulary size). The vocabulary size grows very fast and can reach millions of words. Representing every word with a dimension so large makes the model slow and inefficient. Furthermore, the words not seen in the training set can not be represented. These deficiencies led to the introduction of dense word representation and models like <cit.> and <cit.>. A distributed representation, also termed word embedding is a real-valued representation of a word, with a lower dimension than size V. Each dimension in this embedding embodies a latent characteristic of the word <cit.>. In subsequent sub-sections, we will introduce the most widespread models used to obtain these word embeddings. \n\n\n\n  \u00a7.\u00a7.\u00a7 Word2vec\n\n\nNNLM got complicated over time, and later it was found that simple shallow neural network models like word2vec can work better than these overly-complicated language models. Mikolov introduced two variations of window-based neural models in 2013 - Continuous Bag Of Words (CBOW) and skipgram<cit.>. \n\nCBOW predicts the center word using its context, so with every iteration, it tries to maximize the following probability for every word in the corpus.\n\n    \u2211_-c\u2264j\u2264c,j0logp(w_i|w_i+j)\n\nHere, w_i is the center word, while w_i+j is the context word present at a distance j from the center word. \nOn the other hand, the skipgram model predicts the context words using a center word. It maximizes the following probability with each iteration. \n\n    \u2211_-c\u2264j\u2264c,j0logp(w_i+j|w_i)\n\nTo calculate the probability function between words, both these models use softmax function. \n\n\n    p(w_c|w_o) = exp(v_w_c^'T  v_w_o)/\u2211_w=1^W exp(v_w^'T v_w_o)\n\n\nHere v_w and v_w^' are input and output vector representations of the word w. In both these models, we have two distinct representations for each word. For example, if we are using the skip-gram model, the input representation will be used when the word is a center word, and we will use the output representation when the word is a context word. In the end, we can either concatenate both input and output representations or take their average to construct the final word embedding. \n\nTo make the model better, Mikolov introduced a few tricks in his second paper. First of all, the softmax function is inefficient, as it takes sum over all the words in the vocabulary. To do this, for each token in our corpus, the model becomes extensively slow. Negative sampling solves the above issue. Instead of going the whole vocabulary, we try to maximize the center word's similarity with the words in the context window (positive examples) and randomly select k other words from the vocabulary and minimize the similarity between the center word and these k negative examples. \n\n    p(w_c|w_o) = log\u03c3(v_w_c^'T v_w_o) + \u2211_i=1^k[log\u03c3(-v_w_i^'T v_w_o)]\n\n\nThe negative samples are drawn from power distribution, and the word2vec paper suggests that the value of k can be 5-20 and 2-5 for small and large training datasets.  The second improvement was in the form of subsampling of frequent words. The authors of word2vec also note that some words (e.g., \"in\", \"the\" and \"or\") occur more frequently than other words while providing less or no information. To tackle the issue, the authors used a subsampling technique where each word w_i was discarded with some probability P(w_i).\n\n    P(w_i) = 1 - \u221a(t/f(w_i))\n\nHere f(w_i) is the frequency of word w_i in the corpus and t is a chosen threshold. \n\nWhile CBOW did not work very well with semantic tasks, skipgram handled both semantic and syntactic tasks quite well. In addition to these tasks, word2vec introduced a new task of word analogies. This task examined if the produced word embeddings are able to retain the relationship between words. For example, prince is to princess as king is to queen should be retained in word embeddings as vec(prince) - vec(princess) = vec(king) - vec(queen).\n\nRegardless, both these models produced state-of-the-art performance in numerous tasks with a single projection layer and a simple architecture. The problem with this model was that, as it was a window-based model, it failed to use the global statistics of the corpora to a great extent. Furthermore, the word representations were not context-dependent (there was only one vector for different word senses). Global Vectors (GloVe), introduced in 2014, solves the first problem.\n\n\n\n  \u00a7.\u00a7.\u00a7 Global Vectors (GloVe)\n\n\nGloVe<cit.>, in many ways, resembles the traditional models. Like LSA, it creates the co-occurrence matrix from the text corpus but with two main differences. First, it creates a term-term co-occurrence matrix, as opposed to the term-document co-occurrence matrix in LSA, and second, instead of considering the co-occurrence count in the document, it considers the co-occurrence within a specific range or a window.  Thus, it benefits from both the global statistics of the corpus and the local information of the window. This property of GloVe makes it a suitable model for both word similarity and word analogy tasks. \n\nGloVe also introduces a new least square loss function.\n\n    J = \u2211_p,q=1^Vf(X_pq)(w_p^Tw_q + b_p + b_q - logX_pq)^2\n\nHere V is the vocabulary size, w_p is the vector for center or center word, w_q is the vector for context word, b_p is the target word bias, b_q is the context word bias, X_pq is the number of times w_p occurs with w_q and f(x) is a weighting function. The definition for the weighting function is:\n\n    f(x) = \n    {[ (x/x_max)^\u03b1   x < x_max;           1             ].\n\n\u03b1 and x_max are hyperparameters, fixed to  0.75 and 100 respectively in the paper. This specific choice of weighting function has specific desiring properties such as:\n\n\n    \n  * f(0) = 0, as the co-occurrence becomes large, it becomes sparse and most of its entities become zero. Hence, f should be continuous and f(0) should be defined, f(0) = 0.\n    \n  * It should not overweight large co-occurrences and hence it should be relatively small for large numbers. \n\n\nThe choice of a proper loss function and utilization of both global and local information made GloVe a more suitable algorithm to model both rare and frequent words. Nevertheless, the algorithm was unable to produce embeddings unseen words, and the context of the word was still not considered. The word book had a single word representation as opposed to having different word representations for the noun book and the verb book. Fasttext<cit.>, introduced in 2017, was able to solve the first issue. \n\n\n\n  \u00a7.\u00a7.\u00a7 Fasttext\n\nTo model rare and previously unseen words effectively can be challenging. Besides, having a separate vector representation for each verb form can be inefficient, especially for morphologically rich languages. Fasttext solved these issues by improving the basic skipgram model and incorporating sub-word information into it. In fasttext, each word is represented as a sum of its character n-grams, taking into account the words' morphology. While other models such as <cit.> also learned morphological regularities of the language, they did not use the subword information to a large extent. An extension of the fasttext model <cit.> also solves the problem of misspelled words, as it replaces the misspelled word with the nearest correct word, although for our comparisons in this paper, we will take into consideration the base fasttext model. \n\nAs fasttext improves on the skipgram model, their objective functions are very similar. Fasttext starts with the skipgram objective function and improves upon it by including the subword information. The skipgram with negative sampling objective function is, \n\n    \u2211_t=1^T[  \u2211_c \u2208 C_t\u2113(v_w_t^T v^'_w_c) + \n        \u2211_i=1^k\u2113(-v_w_t^T v_w_i^')]\n\nv_w and v_w^' are the same input and output word vectors while \u2113(x) is the logistic loss function. \n\nFasttext introduces the subword information into the function, and instead of using the target word vector in the equation, it uses its n-grams' representations. Consider the word \"jargon\" and n=3, where n is the n-gram length; we will have the following n-grams:\n \n<ja, jar, arg, rgo, gon, on>\nand the special sequence\n\n<jargon>\n\nThe word representation is derived by summing the representations of all of its character n-grams. For example, if \u2133\u2282{1,....,M} is a word's character n-grams set and z_m is the vector representation for n-gram m then,\n\n    v_w = \u2211_m \u2208\u2133_wz_m\n\nSimilarly, now the objective function of fasttext becomes,\n\n    \u2211_t=1^T[  \u2211_c \u2208 C_t\u2113(\n        \u2211_m \u2208\u2133_w(z_m^T v^'_w_c)) + \n        \u2211_i=1^k\u2113(-\n        \u2211_m \u2208\u2133_w(z_m^T v_w_i^'))]\n\nIn simple words, this function maximizes the similarity between the n-gram representations of the center word with the context word representations, while minimizing the similarity between the n-gram representations of the center word with k negative samples, for each word t in the corpus T.  \n\nFasttext improved significantly on syntactic tasks, especially for morphologically rich languages like Italian, while the performance for semantic tasks remained the same. Neverthless, representing words using their n-grams, helped in better representing rare words and words that were not seen during training, but these representations were still context-independent. \n\n\n\n  \u00a7.\u00a7.\u00a7 Embeddings from Language Models(ELMo)\n\nIn natural languages, the linguistic context of a word defines the word's meaning. The same word may have different meanings when used in different contexts, called polysemy. Methods, such as <cit.>, have previously proposed models to overcome the problem of polysemy in learning word representations, by learning multiple embeddings for each word. Nevertheless, these models had shallow architectures and required predefined word sense classes. ELMo is a deep contextualized model for learning word embeddings from the unlabeled text. ELMo representations are deep and contextualized, the reason being that they are a function of a bidirectional model's internal layers, and they are depend on the context of the word. ELMo word embeddings are a function of the entire sentence. \n\nA forward language model takes the tokens (t_1,...,t_i-1) and models the probability of t_i to predict the likelihood of the token sequence,\n\n    p(t_1,t_2,...,t_N) = \u220f_i=1^N p(t_i | t_1,...,t_i-1)\n\nThe model takes a context-independent token embedding and run in through L layers of forward Long Short Term Memory networks (LSTMs). Every layer j outputs a context dependent representation of the token t_i, h_i,j^LM<cit.>. The LSTM output for the last layer can be used for predicting the next token t_i+1. A backward language model, on the other hand, uses the future tokens (t_i+1, t_i+2,...,t_N) to model the likelihood of the current token (t_i)\n\n    p(t_1,t_2,...,t_N) = \u220f_i=1^N p(t_i | t_i+1, t_i+2,...,t_N)\n\nThe same way as above, L layers of backward LSTMs is implemented, and each layer j computes a context dependent hidden representation of the token t_i, h_i,j^LM.\n\nA biLM couples a forward language model and a backward language model, and ELMo simultaneously maximizes the log-likelihood of the biLM:\n\n    \u2211_i=1^N(logp(t_i |t_1,...,t_i-1; \u0398_x, \u0398_LSTM,\u0398_s)\n    \n    + logp(t_i | t_1,...,t_i-1;\u0398_x,\u0398_LSTM,\u0398_s))\n\n\u0398_x and \u0398_s are parameters for token representation and the softmax layer. \u0398_LSTM and \u0398_LSTM are the parameters for forward and backward LSTMs. \n\nThe model computes 2L+1 parameters, a token representation x_i, and a hidden representation h_i,j^LM for each layer of the forward and backward LSTMs, for each token t_i. If x_i^LM = h_i,0^LM and h_i,j^LM = [h_i,j^LM; h_i,j^LM], for each biLSTM layer, then \n\n    R_i = {\n             h_i,j^LM | j = 0,...,L\n            }\n\nIn the simplest case, ELMo will select the top layer, E(R_i) = h_i,L^LM, as in <cit.>, and more generally, it will estimate a weighting of all biLM layers for the job in hand.\n\nELMo addressed many issues present in the natural language understanding, including polysemy. It had a deep architecture as opposed to the previous word representation models like word2vec and GloVe, and the produced representations were context-aware. After ELMo, many similar deep pre-trained models, for example: Open AI GPT<cit.>, Open AI GPT2<cit.>, and BERT<cit.>, were introduced. \n\n\n\n\n\n    \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Bidirectional Encoder Representations from Transformers (BERT)\n\nBERT is a multi-layer bidirectional Transformer encoder <cit.> based language representation model. It uses a Masked Language Model (MLM), for encoding both left and right contexts into token representation. MLM masks some of the input tokens randomly and then predict the vocabulary ID for masked tokens. Implementing BERT involves two steps: pre-training and fine-tuning. In pretraining phase, the model is trained on a large corpus of unlabeled text. During the fine-tuning phase, the parameters that were learned in the pretraining phase are initialized and subsequently fine-tuned for a particular downstream task. \n\nBERT handles various downstream tasks due to its flexible architecture. It can represent a single sentence or two sentences paired together (Sentence A, Sentence B). To represent two sentences, BERT adds a unique token, [SEP]. Additionally, every sequence starts with a particular classification token [CLS]. We use the final state of [CLS] token as a sequence representation for classification tasks. A token's input representation is obtained by summing the corresponding token embedding, segmentation embedding, and positional embedding. For token embeddings, BERT uses WordPiece embedding <cit.>, segmentation embedding of the token denotes whether a token belongs to sentence A or sentence B, and positional embedding contains the token's position in the input sequence. BERT proved very useful in language modeling and outperformed other models in almost every language modeling task. \n\n\n\n    \n\n\n\n\n\u00a7 PROPERTIES OF EXISTING WORD EMBEDDING APPROACHES\n\n\nWe start evaluating the existing word representation approaches by comparing their intrinsic properties before external evaluations. These properties affect the performance of these approaches on extrinsic tasks directly or indirectly. We start from the basic word representation approach (e.g., One-hot encoding) and take into consideration the existing state of the art word embedding models. We study these models concerning properties such as the density of the word representations, polysemy, context-awareness, fine-tuning, resource intensity, and representing out of vocabulary (OOV) words. \n \nThe density of the word representations, generated by a word embedding model, makes the model memory efficient. It also enables the model to encode latent features corresponding to the words, instead of encoding plain statistical information. While initial models (e.g., one-hot encoding) generates sparse representations, other models (e.g., LSA) generates sparse representations and then projects them to lower dimensions. Modern approaches, on the other hand, starts with learning dense low dimensional word vectors. Generating low-dimensional word vectors enable the word embedding models to encode hidden properties of the words, hence preserving the semantic and syntactic regularities present in the corpus. For example, word2vec maximizes log probability between words that lie closer to each other. hence, words with the same context tend to have similar or closer word representations. \n\nPolysemy enables the word embedding models to learn different word vectors for different word senses or even the same sense. A word may provide different meanings when used in different contexts. A word embedding model should be able to keep track word context to be able to generate word vectors for polysemous words. Traditional models (e.g., LSA and one-hot encoding) provides a single static word representation for each word, irrespective of its sense. GloVe, word2vec, and fasttext also provide a similar static word representation not taking the polysemy into the account. ELMo and BERT, on the other hand, provide distinct word representations for each word sense. This is possible, as these models keep track of the context of the word. Context-dependent word representations further improve the performance of ELMo and BERT on downstream tasks, as the meaning of the word affects its representations. Consider a word book. It may be used as a noun or a verb. In the former case, the representation of the word should be closer to words like notebook and pen, while in the latter case the representation of the word should be closer to other words such as ticket. Hence, multiple context-dependent representations enable models like BERT and ELMo to encode and leverage the meaning of certain words effectively. \n\nFine-tuning allows the model to tune its parameters for the task they are used. Word embedding models generate word representations, which can be used as features for underlying classification tasks. While some models (e.g., GloVe and word2vec) generates fixed word vectors, other models (e.g., BERT and ELMo) allow fine-tuning to generate more appropriate word representations for the task at hand.  \n\nOOV words are those words that haven't been seen by the model during training. The model can be able to produce a word representation for an unseen word if it understands the morphology of the words in some manner. While traditional models, word2vec and GloVe do not utilize the sub-word information and acts on each word as a single indivisible entity, modern approaches such as fasttext, ELMo and BERT can benefit from the sub-word information. Fasttext perceives every word as a combination of n-grams, which enables fasttext to represent OOV words. The n-grams required to build the word may previously exist, and the representation of the word is the sum of all its n-grams. ELMo can handle OOV word representation as it uses character embeddings to computer word-level embeddings. BERT uses WordPiece algorithm to divide a word into pieces and represent each piece with an embedding.  \n\nWhile most of the focus goes on how different word embedding approaches perform in various tasks, it is necessary to know the resource intensity of these approaches. Sometimes, a model consumes much more time and resources compared to a different model, while their performance on a task may not differ significantly. The comparison of different word representation models concerning their intrinsic properties is presented in Table <ref>.  \n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n\n\n\n \u00a7.\u00a7 Datasets\n\nWe evaluate the models mentioned above concerning 4 different classification tasks, spam detection, radical language detection, abusive language detection, and distinguishing abusive and hateful language. The spam detection and radical language detection datasets both contain 200K (100K negative tweets and 100K positive tweets) each, the abusive language detection dataset contains 25K tweets (12.5K negative tweets and 12.5K positive tweets), and the abusive language vs. hateful language dataset contains 5K tweets (2.5K abusive language tweets and 2.5K hateful language tweets). The datasets details have been given in table <ref>. \n\nWe have conducted different experiments in multiple settings for the spam detection and radical language detection datasets. For example, we have evaluated the models with different subsets of these datasets (25K, 50K, 100K, 150K, and 200K tweets). We have also compared the models using different ratios of the positive and negative classes to examine the impact of data skewness on each model. \n\n\n\n\n \u00a7.\u00a7 Experimental model setup\n\nOur experimental model consists of an input embedding layer, two bi-LSTM layers consisting of 32 and 64 units, respectively, a 128-unit dense layer, and a sigmoid output layer. \n\nWe used the tensorflow framework <cit.> to train our models. We used the gensim library <cit.> to train word2vec and fasttext vectors on our corpora. We also used a Tensorflow implementation of GloVe to train our vectors, instead of using the original code written in C. For ELMO and BERT, we used tensorflow-hub to retrieve the context dependent pre-trained vectors. We used BERT-base (L=12, H=768) implementation of the BERT model in our experiments.\n\n\n\n \u00a7.\u00a7 Evaluating Trained Word Embeddings \n\nWhile training word vectors on our corpora, there are many hyper-parameters such as window size and vector dimension. Choosing the correct value for a hyper-parameter can affect the computed word vectors drastically. We have trained word vectors for word2vec (skipgram and CBOW), GloVe, and fasttext on our corpora. Afterward, we have used these computed word vectors for our classification tasks and report the results. We have not trained vectors for BERT and ELMo from scratch, as training these vectors from scratch is inefficient compared to pre-trained vectors considering the smaller size of corpus size. Additionally, they are resource and time exhaustive, and as we will see in the next section, it is better to use their pre-trained vectors.  \n\n\n\n  \u00a7.\u00a7.\u00a7 Window Size: \n Window size refers to the number of words to both sides of the center word that we consider when we train our word embeddings. In skipgram, CBOW and fasttext, the window size is used to maximize the log probability between the center and the context words (words present in the window), while in GloVe, we use the window size for constructing the word co-occurrence matrix. We have selected three different window sizes and reported the result for each dataset in table <ref>.\n\nWhile GloVe performs better with large window sizes consistently, other models tend to have an up and down. Although the difference between the performance on different window sizes for all the models is not significant, we can safely say that the models can capture more information with large window sizes. We should keep in mind that large window sizes require more computational and memory resources. Also, with much larger window sizes, the model may capture irrelevant information. For example, a word may not depend or be relevant to another word that occurred 20 words later.\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Embedding Dimension\n \nWe have also trained word vectors of various dimensions, to examine its effect on intrinsic classification tasks. While for most of these tasks, the performance is similar for high dimension and low dimension vectors, we witness that for tasks with more data (e.g., Spam-200K), the higher dimension vectors perform better. As high dimension vectors can encode more data and represent words more sparsely, this results in a better performance for most of the extrinsic tasks. On the contrary, if the data is less, using smaller dimension vectors can encode the information in a better in some scenarios (e.g., Abusive vs. hateful). \n\nUltimately, higher dimension vectors may perform similar, if not better, for almost all tasks, but they also require more memory and time to train. The F1 scores for multiple datasets have been presented in table <ref>.\n\n\n    \n\n\n\n\n\n\n\n \u00a7.\u00a7 Evaluating Pre-trained Word Embeddings\n\nWe have also used the pre-trained vectors for extrinsic classification tasks and reported the results. In addition to word2vec, GloVe, and fasttext, we have also included BERT and ELMo in the evaluation in this section. Pre-trained word vectors for word2vec, GloVe, and fasttext are static, as they are context-independent, while BERT and ELMo provide context-dependent word vectors and shall be extracted for each sentence as we train our classification model. We will first compare these models' performance on certain classification tasks and subsequently examine the effect of some factors on these models. For our experiment purpose, we have used BERT-base, which 768 dimension vectors, ELMo has 1024 dimension vectors, and all other models have 300 dimensions word representations.\n\nTable 5 shows the F1 score on various classification tasks for all the models. We can see that all models perform very well on relatively easy datasets (e.g., radical language and abusive language), while many models struggle on others. Differentiating abusive tweets from hate tweets is an arduous task, as both contain similar language structure, even then BERT improves the F! score for the dataset by almost 5% compared to any other model.\n\n\n\n  \u00a7.\u00a7.\u00a7 Dataset Size \n In this segment, we evaluate the effect of the data size on pre-trained word embeddings' performance. We select multiple subsets of the datasets and evaluate the models, examining their classification F1 score. While it may be evident that more data will result in better performance, there are situations where a massive amount of data may not be available. It is also essential to know the magnitude of the effect of classification data increment on the model performance.  \n\nAll models have been pre-trained with billions of words. While BERT and ELMo possess the task-specific fine-tuning ability, other models do not. The effect of these two features, fine-tuning and context-dependency, is noticeable, as these two models perform extra-ordinary well on the extrinsic classification tasks. Furthermore, while BERT barely suffers from smaller classification datasets, the loss for other models is relatively high. Word2vec suffers the most, as its F1 score decreases by more than 6 percent for spam classification when the dataset contains 25K tweets instead of 200K tweets. \n\nBoth datasets have a very different text structure, which is further proved by the classification results. While Radical tweets are well structured and more formally written, all models perform extraordinarily well while classifying it. Furthermore, radical tweets contain specific words that can rarely be found in regular tweets, which further differentiates them from regular tweets' structure. On the other hand, spam tweets are more similar to normal tweets, and most of these spam tweets contain slang language and abbreviations for which there are no representations in many models. \n\nAnother point which further helps BERT is the mechanism in which BERT divides a word into multiple pieces using the WordPiece algorithm at its heart. This partition helps BERT to provide context-dependent vectors for those words, which otherwise may by OOV words. Table <ref> depicts the results.\n\n\n    \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Unbalanced Data \n Deep learning models usually tend to overfit towards the more general class. In this section, we compare all word embedding models based on their performance on unbalanced data. We see that, although all models perform worse on skewed data, BERT and ELMo keep their performance above par on the most complex tasks. Once again, we observe that with the easier dataset, the influence of data skewness is negligible, while with the complex task (e.g., spam classification), the impact is more prominent. The classification F1 score for various datasets are presented in table <ref>.   \n\n\n    \n\n\n  \u00a7.\u00a7.\u00a7 Classification Data Pre-processing \n \nWhile pre-processing is an integral part of training deep learning classification models, we would like to examine the effect of fully and partially cleaning the classification data on classification results. \n\nIn the first case, we clean the data completely. We remove URLs, hashtags, symbols, mentions, and stopwords from the tweets. We also lemmatize the words. While in the latter case, we only remove URLs and hashtags from the data and do not lemmatize the words.\n\n\n\n    \n\n\n\nThe partially cleaned data yields better results in all cases. This indicates the stopwords and actual verb form on the classification result. We also observe that the boost in performance for fasttext, ELMo, and BERT while partially cleaning the data, is higher. The reason behind this boost of performance is the use of sub-word information in these models, which allows them to represent OOV words and use the morphology of the words. The results are shown in table <ref>\n\n\n\n \u00a7.\u00a7 Multi-class Classification\n\nUntil now, we classified tweets into two classes; positive and negative. In some scenarios, we need to classify our data into multiple categories or classes. Here, we evaluate the pre-trained word embeddings' performance in multi-class classification. \n\nWe build a new dataset, consisting of five classes, and 2500 tweets for every class. We also make changes to our classification model, changing the sigmoid output layer to a softmax output layer with five units, to accommodate multi-class classification.  The complete comparison is presented in Table <ref>.\n\nWe present the results for both fully cleaned and partially cleaned data. While with fully cleaned data, there is a slight difference between BERT and GloVe, the difference is much more noticeable with partially cleaned data. This further emphasizes our previous point that fully cleaning the data results in information loss. \n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\n\n\n \u00a7.\u00a7 Pretrained Word Embeddings vs. Trained Word Embeddings\n\nWe evaluated context-independent trained word embedding models. We also compared all major pre-trained word embedding models on multiple extrinsic classification tasks. Now, we will compare pre-trained word embeddings generated by these context-independent models with the word embeddings that we have trained on our corpora. We have used similar possible hyper-parameter values. We use 300 dimension vectors, with a context size of 5, and for word2vec, we use the skipgram model with a negative sampling value of 15. \n\nFor most of the tasks, the word representation that we have trained from scratch outperforms the pre-trained word embeddings. This may be because the corpus used for training these word embeddings may not be relevant to the task in hand. Another reason can be that the corpus contain some words, for which the embedding is not availabe in the pre-trained embeddings. Although if we have a small classification dataset (e.g., abusive vs. hateful), it would be better to use pre-trained vectors, as small text corpus will not generate quality word representations. The data factor becomes more evident once we examine different subsets of the spam dataset. While the performance difference between the pre-trained and trained vectors is more for the smaller subset (30K tweets), it becomes decreases considerably for the bigger subsets (100K and 200K tweets). See table 10 for details. \n\n\n\n\n\u00a7 CONCLUSION\n\nIn this paper, we evaluated the existing word embedding algorithms on extrinsic classification tasks. We also described the working of each of these models and provided an insight into how these models encode the relations between words. We explained the desired properties of a good word embedding approach and discussed the presence and absence of these properties in certain models. We also illustrated how these properties affect the word embeddings produced by word embedding models. The impact of certain parameters such as., window size, embedding dimension, was also illustrated on the word embeddings' quality. We also compared pre-trained word embeddings, trained word embeddings, and their impact on classification tasks. Moreover, we also obtained an insight into which algorithm performs better when used for multi-class classification. \n\nAlthough it is difficult to study all the word embedding algorithms and their properties in a single paper, we covered the most important approaches, selected from different categories. To make our comparisons simpler, we divided the models into two groups, traditional models and neural models, and observed that neural word embedding models provide numerous advantages over traditional approaches. \n\nWe observed that while in almost every task, BERT overperformed the other word embedding approaches, in certain classification tasks (e.g., Abusive vs. Hateful), the difference was negligible. Hence, taking into consideration its resource extensiveness of BERT, simpler models (e.g., word2vec and GloVe) provided better results. ELMo, on the other hand, performed similarly to BERT, in most of the tasks, as it uses task-specific parameter tuning. \n\nWe also observed that the structure of underlying classification data plays a vital role. While some datasets (e.g., Radical dataset), may be very easy to classify, as their text combinations are very different, in other datasets (e.g., Abusive vs. hateful), both the classes follow almost the same text pattern, which makes it hard to differentiate. Before selecting a word embedding model, we shall analyze the underlying classification data and examine the text structure of different text classes. When the classes follow different text patterns, the simple models (e.g., word2vec and GloVe) perform similar to complex models (e.g., BERT). \n\nData pre-processing, although being an integral part of a classification task, can impact the result negatively, if done extensively. We observed that the original morphology of the text provides crucial information, and can be lost if the pre-processing is done carelessly. The above point further emphasizes the importance of using sub-word information. Approaches that consider words as indivisible entities can face OOV words and hence an increasing number of unknown word vectors. All these unknown word vectors result in loss of information. Approaches that use sub-word or character-level information, on the other hand, can handle OOV words, and represent the actual morphology of the words effectively. Hence models like BERT, ELMo and fasttext, perform better than their predecessors.\n\n\n"}