{"entry_id": "http://arxiv.org/abs/2303.06993v1", "published": "20230313104925", "title": "Actor-Critic learning for mean-field control in continuous time", "authors": ["Noufel Frikha", "Maximilien Germain", "Mathieu Lauri\u00e8re", "Huy\u00ean Pham", "Xuanye Song"], "primary_category": "stat.ML", "categories": ["stat.ML", "math.OC"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nequationsection\n\n\n\n\n\n \n\n\n \n \n \n\n\n\n\n\n\n\n\n\nshowonlyrefs\n\n\n\n\n\nfit,matrix,chains,positioning,decorations.pathreplacing,arrows\n\n\nshowonlyrefs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nletterpaper, \nleft=   30 mm,\nright=  20 mm,\ntop=    45 mm,\nbottom= 45 mm,\n\n\n \n^\u22ba\n\n\u25b8\n\n \n\n\n\n\n#1#2#1#22mu#1#2\n\n\n\n\n\n\u2211\n\u220f\n\u222b\n/\ninf\nsup\nlim\nlim inf\nlim sup\nmax\nmin\nClp\n1\u03311\n\n\n\n\n\n\nA\n\n\ud835\udd38\n\ud835\udd40\n\u2115\n\u211d\n\u0141\ud835\udd43\n\ud835\udd44\n\u2124\n\ud835\udd3c\n\ud835\udd3d\n\ud835\udd3e\n\u2119\n\u211a\n\ud835\udd3b\n\ud835\udd4b\n\ud835\udd4f\n\ud835\udd4a\n\ud835\udd4c\n\n\n\ud835\udc0f_\nM\n\u03f5 \n\u03c0 \nR \np \n\u03c0 \nR \nF \nf\u0303 \nd \nW \nV \n\u03bd \nA \nu \n\u00e2 \n\u03b1 \n \n\ud835\udd1e \n\ud835\udd23 \na \nb \nf \nd \n\u1e0d \n\n\nU\nM\nf\u0302\nf\u0302\nD\n\n_#1#1ess sup  \n\n_#1#1argmin  \n_#1#1argmax  \n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nK\nL\nP\nR\n\u03a0\nM\nN\nO\nS\nT\nU\nV\nW\nX\nY\nZ\n\n\n\n\n\u03b5\n\n\n \n\n\n#1\u2202#1\u2202t\n#1\u2202#1\u2202s\n#1\u2202^2 #1\u2202s^2\n#1\u2202#1\u2202y\n#1\u2202^2 #1\u2202y^2\n#1\u2202^2 #1\u2202s \u2202y\n#1\u2202#1\u2202k\n#1\u2202#1\u2202p\n#1\u2202^2 #1\u2202k^2\n#1\u2202^2 #1\u2202p^2\n#1\u2202^2 #1\u2202k \u2202y\n#1\u2202^2 #1\u2202k \u2202p\n#1\u2202^2 #1\u2202y \u2202p\n\n#1(<ref>)\n\n\nN\nX\nP\nQ\n\np\nx\ns\nc\na\nf\ng\nq\n\u03b1\n\u03c0\n\n\u03be\n\n\n\n\n\u0131I\nT\n\nNT\nS\n\n\n\nX\nC\ne\nx\ny\nb\nn\ns\nw\nN\n\u03c3\n\ng\ni\np\nc\nr\nv\nf\n\u0393\n\nm\nD\n\n\nm \n\n\ud835\udd1e \na \nb \n\ud835\udd1f\n\ud835\udd32 \nu \n\u1e0d \n\ud835\udd30 \np \ns \n\u03c3 \n\u03c3 \ny \n\ud835\udd36 \nz \n\ud835\udd37 \n\ud835\udd16 \nS \n\n\u03c0 \n\u03c0 \nJ \n\ud835\udd0d \nG \n\ud835\udd0a \nV \n\n\n\n\n\n \n\n    #1\n        0.0 -0.055 -0.08 0.07 #1#1#1#1#1#1#1#1#1#1#1#1TheoremTheorem[section]\n        DefinitionDefinition[section]\n        PropositionProposition[section]\n        AssumptionAssumption[section]\n        LemmaLemma[section]\n        CorollaryCorollary[section]\n        RemarkRemark[section]\n        ExampleExample[section]\n        equationsectionActor-Critic learning  for mean-field control in continuous time\n            Noufel FRIKHA[CES, UMR 8174,  Universit\u00e9 Paris 1 Panth\u00e9on Sorbonne, mailto: Noufel.Frikha at univ-paris1.fr Noufel.Frikha at univ-paris1.fr]   Maximilien GERMAIN[LPSM,  Universit\u00e9 Paris Cit\u00e9, mailto: maximilien.germain at gmail.commaximilien.germain at gmail.com]   Mathieu LAURIERE[NYU Shanghai, mailto: mathieu(dot)lauriere at nyu.edumathieu.lauriere at nyu.edu]\n           Huy\u00ean PHAM[ LPSM,  Universit\u00e9 Paris Cit\u00e9,   mailto:pham at lpsm.parispham at lpsm.paris; This author  is supported by  the BNP-PAR Chair \u201cFutures of Quantitative Finance\", \n        and by FiME, Laboratoire de Finance des March\u00e9s de l'Energie, and the \u201cFinance and Sustainable Development\u201d EDF - CACIB Chair]   Xuanye SONG[LPSM,  Universit\u00e9 Paris Cit\u00e9,   mailto:pham at lpsm.parisxsong at lpsm.paris;]\n            March 30, 2023\n        ============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n        \n        \n        We study policy gradient for mean-field control in continuous time in a  reinforcement learning setting. By considering randomised policies with entropy regularisation, we derive a gradient expectation representation of the value function, which is amenable to actor-critic type  algorithms, \n        where the value functions and the policies are learnt alternately based on observation samples of the state  and model-free estimation of the population state distribution, either by offline or online learning.  \n        In the linear-quadratic mean-field framework, we obtain an exact parametrisation of the actor and critic functions defined on the Wasserstein space. Finally, we illustrate the results of our algorithms with some numerical experiments on  concrete examples. \n        \n         Keywords: Mean-field control, reinforcement learning, policy gradient, linear-quadratic, actor-critic algorithms.   \n        \n        \n        \n        \n        \u00a7 INTRODUCTION\n         Mean-field control (MFC in short),  also called  McKean-Vlasov  (MKV in short) control problem  is concerned with the study of  large population models  of interacting agents who are cooperative and act for collective welfare according to a center of decision (or social planner).  \n        It has attracted a growing interest over the last years with the emergence of mean-field game, and there is now a large literature on the theory and its various applications in  economics/finance, population dynamics, social sciences and herd behavior. We refer to the seminal two-volume monograph <cit.>-<cit.> for a detailed treatment of the topic. \n        \n        \n        Mean-field control problems lead to infinite dimensional problems in the Wasserstein space of probability measures, and analytical solutions are rarely available. It is then crucial to design efficient numerical schemes for solving such problems, and in the past few years, several works have proposed numerical methods in a model-based setting based \n        either on forward-backward SDE characterisation of MKV from Pontryagin maximum principle, or Master Bellman equation from dynamic programming, and often relying on suitable class of neural networks, see e.g.  <cit.>,  <cit.>,  <cit.>, <cit.>, <cit.>, <cit.>. \n        \n        The question of learning solutions to MFC in a model-free setting, i.e. when the environment (model coefficients) is unknown, has recently attracted attention, see <cit.>, <cit.>,  <cit.>, and this is precisely the purpose of  Reinforcement learning (RL):  learn optimal control  by trial and error, i.e., repeatedly try \n        policy, observe the state, receive and evaluate the reward, and improve the policy.  There are two main approaches in RL: (i) Q-learning based on dynamic programming, and (ii)  Policy gradient  based on parametrisation of policies, and a key feature in RL is the  exploration of the unknown \n        environment  to  broaden search space, which can be achieved  via randomised policies. RL is a very active branch of machine learning and we refer to the second edition of the monograph <cit.> for an overview of this field. \n        \n        Most algorithms in RL are  limited to discrete-time frameworks for  Markov decision processes (MDP) or mean-field MDP, and the study of RL in continuous time has been recently initiated in <cit.>, <cit.>, <cit.> for controlled diffusion processes.  \n        In line with these works, we provide in this paper a theoretical treatment of policy gradient methods for MFC  in continuous time and state/action space by relying on stochastic calculus that has been recently developed for MKV equations.  Our main theoretical result is to obtain a policy gradient representation for value function with \n        randomised parametric policies and entropy regularisers for encouraging exploration. Based on this representation, we design model-free actor critic algorithms involving either the whole trajectories of the state (off-line learning), or the current and next state (online learning).  In the mean-field context, a key issue is to handle the population state distribution, which is an input of the policy (actor) and value function (critic), and instead of assuming that we have at disposal a simulator of the state distribution as in <cit.>, we estimate it in a model-free manner as in <cit.>, which is more suitable for real-world applications. We next study the linear quadratic (LQ) case for which we derive explicit solutions, and this can be used for proposing an exact parametrisation of the critic and actor functions that is incorporated in  stochastic gradient  when updating the policies and value functions.  The explicit solutions in the LQ setting  are served as benchmarks for the numerical results of our algorithms in two examples. \n        \n        \n        The rest of the paper is organized as follows. In Section <ref>, we formulate the mean-field control problem in continuous-time with randomised policies and entropy regularisers, and state the partial differential equation (PDE) characterisation of the value function in the Wasserstein space.  We develop in Section <ref> policy gradient methods by establishing a policy gradient representation, and its implication for actor-critic algorithms.  Section <ref> is devoted to the linear-quadratic setting, and we present in Section <ref> numerical results on two examples to illustrate the accuracy of our algorithms.  Finally, proofs of the policy gradient theorem are detailed in Appendix \n        <ref>, while the derivation of the explicit solution in the LQ case is shown in Appendix <ref>. \n        \n        \n        \n         Notations.\n        \n        \n        \n        \n        \n        The scalar product between two vectors x and y is denoted by x \u00b7 y, and | \u00b7 | is  the Euclidian norm.  \n        Given two matrices M=(M_ij) and N=(N_ij), we denote by M:N=Tr(M N)=\u2211_i,j M_ij N_ij its inner product, and by |M| the Frobenius norm of M. \n        Here  is the transpose matrice operator. \n        Let =(_i_1i_2i_3)\u2208^d_1\u00d7 d_2\u00d7 d_3 be  a tensor of order 3. For p=1,2,3, the p-mode product of   with a vector b=(b_i)\u2208^d_p, is denoted by \u2219_p b, and it is a tensor of order 2, i.e. a matrix \n        \n        defined elementwise as \n        ( \u2219_1 b)_i_2i_3    =  \u2211_i_1=1^d_1 M_i_1i_2i_3 b_i_1,  ( \u2219_2 b)_i_1i_3   =  \u2211_i_2=1^d_2  M_i_1i_2i_3 b_i_2,  ( \u2219_3 b)_i_1i_2   =  \u2211_i_3=1^d_3 M_i_1i_2i_3 b_i_3.  \n         \n        The p-mode product of a 3-th order tensor M\u2208^d_1\u00d7 d_2\u00d7 d_3 with a matrix B=(B_ij)\u2208^d_p\u00d7 d, also denoted by M\u2219_p B,  is a 3-th order \n         tensor \n         \n         defined elementwise as\n        (M\u2219_1 B )_\u2113 i_2i_3     = \u2211_i_1 =1^d_1 M_i_1i_2i_3 B_i_1\u2113,   (M\u2219_2 B )_i_1\u2113 i_3  = \u2211_i_2 =1^d_2 M_i_1i_2i_3 B_i_2\u2113\n        ( \u2219_3 B)_i_1i_2\u2113    =  \u2211_i_3=1^d_3 M_i_1i_2i_3 B_i_3\u2113. \n        \n        Finally, the tensor contraction (or partial trace) of a 3-th order tensor M\u2208^d_1\u00d7 d_2\u00d7 d_3 whose dimensions d_p and d_q are equal is denoted as Tr_p,qM. This tensor contraction is a tensor of order 1, i.e. a vector, defined elementwise as \n        (Tr_1,2M)_i_3 =  \u2211_\u2113=1^d_1 M_\u2113\u2113 i_3,  (Tr_1,3M)_i_2 =  \u2211_\u2113=1^d_1 M_\u2113 i_2 \u2113,  (Tr_2,3M)_i_1 =  \u2211_\u2113=1^d_2 M_i_1 \u2113\u2113.\n        \n         \n        \n        \n        \n        \n        \n        \u00a7 EXPLORATORY FORMULATION OF MEAN-FIELD CONTROL\n         \n        \n        \n        Let us  consider a mean-field control problem where the ^d-valued controlled state  process X=X^\u03b1  is governed by the dynamics\n        \n            X\u0323_s    =    b(X_s,_X_s,\u03b1_s) \u1e63  + \u03c3(X_s,_X_s,\u03b1_s) \u1e88_s,      s \u2265 0,\n         \n        with W a standard p-dimensional Brownian motion on a  probability space (\u03a9,,) equipped with the filtration =(_t)_t\u2265 0 generated by W, and augmented with a \u03c3-algebra  rich enough to support a uniformly distributed random variable \n        independent of W. The control \u03b1=(\u03b1_t)_t  is an -progressively measurable  process with \u03b1_t representing the action of the agent at time t, and valued in the action space A\u2282^q.  Here, _X_t denotes the marginal law of X_t, \n        _2(^d) is the Wasserstein space of probability measures \u03bc with a finite second order moment, i.e., M_2(\u03bc):=(\u222b |x|^2 \u03bc(x\u0323))^1 2<\u221e,  \n        equipped with the Wasserstein distance _2, and the coefficient b  (resp. \u03c3) is a measurable  function from ^d\u00d7_2(^d)\u00d7 A into ^d (resp. ^d\u00d7 p).  \n        \n        \n        Throughout the paper, we make the standard Lipschitz assumptions on the coefficients b and \u03c3 to ensure the existence and uniqueness of a strong solution  \n        to the stochastic differential equation (SDE in short) (<ref>) given any initial condition \u03be with law \u03bc\u2208_2(^d). \n        \n        \n        \n        \n        \n        The objective of a mean-field  control problem on finite horizon T<\u221e, is\n        \n        to minimize over the control \u03b1 an expected total cost of the form\n        \n            [ \u222b_0^T e^-\u03b2 s  f(X_s,_X_s,\u03b1_s) \u1e63 + e^-\u03b2 T g(X_T,_X_T) ].\n         \n        Here f is a running cost function defined on ^d\u00d7_2(^d)\u00d7 A, while g is a terminal cost function on ^d\u00d7_2(^d), and \u03b2\u2208_+ is a given discount factor. In a model-based setting, i.e., when the coefficients b, \u03c3, and the functions f, g are known, the solution to MFC control problem can be characterised by a forward backward SDE arising from the maximum principle (see <cit.>,  or by a Master  Bellman equation arsing from dynamic programming principle (see <cit.>). Moreover,  the optimisation over -progressively measurable process \u03b1 (open-loop control), \n        or feedback (also called closed-loop) controls \u03b1, i.e., in the form \u03b1_t=\u03c0(t,X_t,_X_t), 0\u2264 t\u2264 T,  for some deterministic policy \u03c0, i.e., a  measurable function \u03c0:[0,T]\u00d7^d\u00d7_2(^d)\u2192A,  yields the same value function. \n         \n         \n         In a model-free reinforcement learning (RL)  setting, when the coefficients are unknown, the agent can  only rely on observation samples of state and reward in order to learn the optimal strategy. This is achieved by  trial and error where the agent tries a policy, receive and evaluate the reward and then improve performance \n         by repeating this procedure. A critical issue in reinforcement learning when the environment is unknown, is  exploration in order to broaden search space, and a key and now common idea is to use  randomised (or stochastic) policies: in a mean-field setting,  this is defined by a probability  transition kernel from \n         [0,T]\u00d7^d\u00d7_2(^d) into A, i.e., a measurable function \u03c0:(t,x,\u03bc)\u2208[0,T]\u00d7^d\u00d7_2(^d)\u21a6\u03c0(.|t,x,\u03bc)\u2208(A), the set of probability measures on A. \n         We then say that the process \u03b1=(\u03b1_t)_t  is a randomised feedback control generated from a stochastic  policy \u03c0, denoted by \u03b1\u223c\u03c0, if at each time t, the action \u03b1_t is sampled from the probability distribution \u03c0(.|t,X_t,_X_t). Note that the sampling is drawn at each time \n         from the \u03c3-algebra  rich enough to support a uniformly distributed random variable independent of W. More precisely,  it is defined as follows: given a probability transition kernel  \u03c0, one can associate a measurable function \u03d5_\u03c0:[0,T]\u00d7^d\u00d7_2(^d)\u00d7 [0,1]\u2192A such that \n         the law of \u03d5_\u03c0(t,x,\u03bc,U) is \u03c0(.|t,x,\u03bc) where U is an uniform random variable on [0,1].  We would then naturally define the control process by \u03b1_t=\u03d5_\u03c0(t,X_t,_X_t,U_t), 0\u2264 t\u2264 T,  \n         for a collection of -measurable i.i.d. uniform random variables (U_t)_t,  but this raises some measurability issues as \n         (t,\u03c9)\u21a6U_t(\u03c9) is not jointly measurable in the usual product space ([0,T]\u00d7\u03a9,_[0,T]\u2297,\u1e6d\u2297).  To cope these issues, one can use the notion of Fubini extension, see  <cit.>. We consider an atomless  probability space ([0,T],,\u03c1) extending the usual Lebesgue measure interval space \n         ([0,T],_[0,T],\u1e6d), and a rich Fubini extension ([0,T]\u00d7\u03a9,\u22a0,\u03c1\u22a0) of the product space  ([0,T]\u00d7\u03a9,\u2297,\u03c1\u2297). Then, from Theorem 1 in <cit.>, there exists a \u22a0-measurable map :[0,T]\u00d7\u03a9\u2192[0,1] such that the random variables \n         U_t=(t,.) are essentially pairwise independent, and uniformly distributed on [0,1]. Denote by  the filtration generated by (W,), and consider the controlled process governed by \n         \n            X\u0323_s    =    b(X_s,_X_s,\u03b1_s) \u1e63 + \u03c3(X_s,_X_s,\u03b1_s) \u1e88_s,\n        \n         where \u03b1_t=\u03d5_\u03c0(t,X_t,_X_t,U_t)\u223c\u03c0(.|t,X_t,_X_t), 0\u2264 t\u2264 T, is -progressively measurable.  Here, to alleviate notations, we write \u03c1(\u1e6d)\u2261\u1e6d. \n        \n        \n        \n        Moreover, in order to encourage exploration of randomised policies, we shall substract entropy regularisers to the cost term, as adopted in the recent works by <cit.>, <cit.>, by considering the Shannon differential entropy defined as\n        (\u03c0(.|t,x,\u03bc))     :=     - \u222b_A log p(t,x,\u03bc,a) \u03c0(\u1ea1|t,x,\u03bc), \n          \n        by assuming that \u03c0(.|t,x,\u03bc) admits a density p(t,x,\u03bc,.) with respect to some measure \u03bd on A. \n        The goal of the social planner is now to minimise over randomised policies \u03c0 the cost \n        \n            J(\u03c0)    =  _\u03b1\u223c\u03c0[ \u222b_0^T e^-\u03b2 s[  f(X_s,_X_s,\u03b1_s) -  \u03bb( \u03c0(.|s,X_s,_X_s) )  ] \u1e63   +   e^-\u03b2 T g(X_T,_X_T) ],\n        \n        where \u03bb\u22650 is a  temperature parameter on exploration.  Here, the notation in _\u03b1\u223c\u03c0[.]  means that the expectation operator is taken when the randomised feedback control \u03b1 is generated from the stochastic policy \u03c0, and X=X^\u03b1 is driven by the dynamics (<ref>). \n        \n        \n        \n        Let us now introduce the dynamic Markovian version of the above mean-field problem. \n        \n        Given a stochastic policy \u03c0, an initial time-state-distribution triple (t,x,\u03bc)\u2208[0,T]\u00d7^d\u00d7_2(^d), and \u03be\u2208L^2(_t;^d) (the set of square-integrable _t-measurable random variables valued in ^d) with distribution law \u03bc (\u03be\u223c\u03bc),  we consider the decoupled  state processes {X_s^t,\u03be,t\u2264 s\u2264 T} and {X_s^t,x,\u03be,t\u2264 s\u2264 T} given by \n        \n            X_s^t,\u03be   = \u03be + \u222b_t^s b(X_r^t,\u03be,_X_r^t,\u03be,\u03b1_r)  \u1e5b + \u222b_t^s \u03c3(X_r^t,\u03be,_X_r^t,\u03be,\u03b1_r)  \u1e88_r,    \n            \n            X_s^t,x, \u03bc   = x + \u222b_t^s b(X_r^t,x, \u03bc,_X_r^t,\u03be,\u03b1_r) \u1e5b + \u222b_t^s \u03c3(X_r^t,x, \u03bc,_X_r^t,\u03be,\u03b1_r)  \u1e88_r,    t \u2264 s \u2264 T,\n        \n        where \u03b1 is a randomised feedback control generated from \u03c0, i.e.,  \u03b1_s is sampled at each time s from \u03c0(.|s,X_s^t,x, \u03bc,_X_s^t,\u03be) (here, to alleviate notations, we omit the dependence  of X^t,\u03be and X^t,x, \u03bc in \u03b1\u223c\u03c0). \n        We make the standard Lipschitz regularity assumptions on the coefficients b and \u03c3 to ensure the existence and uniqueness of a strong solution   to  (<ref>) given any initial condition t, \u03be, x.  \n        By weak uniqueness, it follows that the law of the process (X_s^t, \u03be)_s\u2208 [t, T] given by the unique solution to the first SDE in (<ref>) only depends upon \u03be through its law \u03bc. It thus makes sense to consider (_X_s^t,\u03be)_s\u2208 [t, T] as a function of \u03bc without specifying the choice of the random variable \u03be that has \u03bc as distribution. In particular, for any 0\u2264 t\u2264 s\u2264 T, the random variable X_s^t,x,\u03bc depends on \n        \u03be only through its law \u03bc. As a consequence, we can define the cost value function of the stochastic policy \u03c0 as the function defined on [0,T]\u00d7^d\u00d7_2(^d) by \n        \n            V^\u03c0(t,x,\u03bc)    =  _\u03b1\u223c\u03c0[ \u222b_t^T e^-\u03b2 (s-t)[  f(X_s^t,x, \u03bc, _X_s^t,\u03be,\u03b1_s) -  \u03bb( \u03c0(.|s,X_s^t,x,\u03bc,_X_s^t,\u03be) ) ]  \u1e63\n                +    e^-\u03b2 (T-t) g(X_T^t,x, \u03bc,_X_T^t,\u03be) ].\n         \n        Since X_s^t, \u03be, \u03be = X_s^t, \u03be a.s., the initial cost  value in (<ref>) when starting from some initial random state \u03be\u2208L^2(;^d) with law \u03bc is equal to  J(\u03c0)=_\u03be\u223c\u03bc[V^\u03c0(0,\u03be,\u03bc)]. \n         \n         \n         \n         We complete this section by characterizing the cost value function V^\u03c0, for a given stochastic policy \u03c0, in terms of a linear parabolic partial differential equation (PDE) of mean-field type stated in the strip [0,T] \u00d7\u211d^d \u00d7\ud835\udcab_2(\u211d^d). We first introduce the coefficients associated to the dynamics and the value function, given a stochastic policy \u03c0, namely\n          \n         b_\u03c0(t,x,\u03bc)   =  \u222b_A b(x,\u03bc, a)   \u03c0(\u1ea1|t,x,\u03bc),       \u03a3_\u03c0(t,x,\u03bc)   =  \u222b_A (\u03c3\u03c3)(x,\u03bc,a) \u03c0(\u1ea1|t, x,\u03bc), \n        \n         f_\u03c0(t,x,\u03bc)   =  \u222b_A f(x, \u03bc, a) \u03c0(\u1ea1|t,x,\u03bc),         E_\u03c0(t,x,\u03bc)   =    - \u222b_A log p(t,x,\u03bc,a) \u03c0(\u1ea1|t,x,\u03bc),\n         \n         and let \u03c3_\u03c0 := \u03a3_\u03c0^1/2.\n         \n         \n         \n        \n        \n         \n         \n        Before presenting the regularity assumptions, we introduce some notations regarding the Wasserstein derivative (also called L-derivative) of a real-valued smooth map U defined on _2(^d). We follow the common practice of denoting by \u2202_\u03bc U(\u03bc)(v) \u2208^d the Wasserstein derivative of U with respect \u03bc evaluated at (\u03bc, v) \u2208_2(^d) \u00d7^d. Its ith coordinate is denoted by \u2202^i_\u03bc U(\u03bc)(v). We will also work with higher order derivatives. For a positive integer n, a multi-index \u03bb of {1, \u22ef, d}, a n-tuple of multi-indices \u03b3=(\u03b3_1, \u22ef, \u03b3_n) of {1, \u22ef, d} and v = (v_1, \u22ef, v_n)\u2208 (\u211d^d)^n, we denote by \u2202_\u03bc^\u03bbU(\u03bc)(v) the derivative \u2202^\u03bb_n_\u03bc[\u22ef [\u2202^\u03bb_1_\u03bc U(\u03bc)](v_1)\u22ef](v_n). If v\u21a6\u2202_\u03bc^\u03bbU(\u03bc)(v) is smooth, we write  \u2202^\u03b3_v\u2202_\u03bc^\u03bbU(\u03bc)(v) for the derivative \u2202^\u03b3_n_v_n\u22ef\u2202^\u03b3_1_v_1\u2202_\u03bc^\u03bbU(\u03bc)(v).\n        \n        We will often deal with maps that depend on additional time and space variables. In particular, we will work with the two spaces \ud835\udc9e^2, 2(^d \u00d7_2(^d)) and \ud835\udc9e^1, 2, 2([0,T] \u00d7^d \u00d7_2(^d)) and refer the reader to <cit.> Chapter 5 for more details. \n        \n        Having these notations at hand, we make the following regularity assumptions on the coefficients b_\u03c0 , \u03c3_\u03c0, the cost functions f_\u03c0, g and the Shannon differential entropy E_\u03c0. Below, \u03c0:[0,T] \u00d7\u211d^d \u00d7\ud835\udcab_2(\u211d^d) \u2192\ud835\udcab(A) is a fixed stochastic policy.\n        \n         \n         \n         \n          (i) For any h \u2208{ b^i_\u03c0, \u03c3_\u03c0^i, j, i=1, \u22ef, d, j = 1, \u22ef, p}, the following derivatives\n        \n            \u2202_x h(t, x, \u03bc),  \u2202^2_x h(t, x, \u03bc),  \u2202_\u03bc h(t, x, \u03bc)(v),  \u2202_v [\u2202_\u03bc h(t, x, \u03bc)](v),\n         exist for any (t, x, v, \u03bc) \u2208 [0,T] \u00d7 (^d)^2 \u00d7_2(^d), are bounded and locally Lipschitz continuous with respect to x, \u03bc, v uniformly in t\u2208 [0,T]. Moreover, h(t, .) is at most of linear growth, uniformly in t\u2208 [0,T], namely, there exists C<\u221e such that for all t, x, \u03bc\n            |h(t, x, \u03bc)| \u2264 C (1+ |x| + M_2(\u03bc)).\n        \n          (ii) For any t\u2208 [0,T], f_\u03c0 (t, .),   E_\u03c0(t, .),   g \u2208\ud835\udc9e^2,2(\u211d^d \u00d7_2(\u211d^d)).\n         \n          (iii) There exists some constant C< \u221e, such that for any (t, x, v,\u03bc)\u2208[0,T]\u00d7(^d)^2\u00d7_2(^d), \n         \n            |f_\u03c0 (t, x, \u03bc)| + |E_\u03c0(t, x, \u03bc)| + |g(x, \u03bc)| \u2264 C (1 + |x|^2 + M_2(\u03bc)^q),\n        \n            | \u2202_x f_\u03c0 (t, x, \u03bc)| + |\u2202_x E_\u03c0(t, x, \u03bc)| + |\u2202_x g(x, \u03bc)| \u2264 C (1+ |x| + M_2(\u03bc)^q),\n        \n            | \u2202_\u03bc f_\u03c0 (t, x, \u03bc)(v) | + |\u2202_\u03bc E_\u03c0(t, x, \u03bc)(v)| + |\u2202_\u03bc g(x, \u03bc)(v)| \u2264 C (1+ |x| + |v| + M_2(\u03bc)^q),\n        \n            | \u2202_v [\u2202_\u03bc f_\u03c0 (t, x, \u03bc)](v) |+      | \u2202^2_x f_\u03c0 (t, x, \u03bc)| +  |\u2202_v [\u2202_\u03bc E_\u03c0(t, x, \u03bc)](v)| + |\u2202^2_x E_\u03c0(t, x, \u03bc)| \n                + |\u2202_v [\u2202_\u03bc g(x, \u03bc)](v)|  + |\u2202^2_x g(x, \u03bc)| \u2264 C (1+ M_2(\u03bc)^q),\n         for some q\u2265 0.\n         \n         It is readily seen from the integral form of b_\u03c0, \u03a3_\u03c0, f_\u03c0, E_\u03c0 that if for any a \u2208 A, the functions (x, \u03bc) \u21a6 b(x, \u03bc, a), \u03c3(x, \u03bc, a),   f(x, \u03bc, a) and the density (x, \u03bc) \u21a6 p(t, x, \u03bc, a) of the probability measure \u03c0(\u1ea1|t,x,\u03bc) are smooth with derivatives satisfying some adequate estimates then Assumption 2.1 is satisfied. In particular, this will be the case when the coefficients b, \u03c3 are linear functions and f together with g are quadratic functions of the variables of x, \u222b_^d z \u03bc(\u1e93) and a and if p is a Gaussian density with a smooth mean and a time-dependent covariance-matrix as in the linear quadratic framework, see Section <ref>. \n         \n         \n         \n        \n         \n        \n         \n         \n         \n        We now have the following PDE characterisation of the cost value function V^\u03c0.\n        \n         \n         \n          \n         Under Assumption <ref>, the function V^\u03c0 defined by (<ref>) belongs to C^1,2, 2([0,T]\u00d7^d\u00d7_2(^d)) and satisfies the following linear parabolic PDE\n         \n            _\u03c0[V^\u03c0](t,x,\u03bc) + (f_\u03c0 -  \u03bb E_\u03c0)(t,x,\u03bc) = 0,    (t, x, \u03bc) \u2208 [0,T)\u00d7^d\u00d7_2(^d),\n         with the terminal condition V^\u03c0(T,x,\u03bc)=g(x,\u03bc), where \n        \n         _\u03c0 is the operator defined by \n         \n         _\u03c0[\u03c6](t,x,\u03bc) \n            =    - \u03b2\u03c6(t, x, \u03bc) + \u2202_t\u03c6(t,x,\u03bc) \n        \n         +  b_\u03c0(t,x,\u03bc) \u00b7 D_x\u03c6(t,x,\u03bc) +  1/2\u03a3_\u03c0(t,x,\u03bc) : D_x^2 \u03c6(t,x,\u03bc)  \n                +   _\u03be\u223c\u03bc[   b_\u03c0(t,\u03be,\u03bc) \u00b7\u2202_\u03bc\u03c6(t,x,\u03bc)(\u03be) + 1/2\u03a3_\u03c0(t,\u03be,\u03bc) : \u2202_\u03c5\u2202_\u03bc\u03c6(t,x,\u03bc)(\u03be)    ].\n         \n        \n        \n        \n        \n         \n         In particular, the above result indicates that provided the coefficients b_\u03c0, \u03a3_\u03c0, the functions f_\u03c0, E_\u03c0 and the terminal condition g are smooth with derivatives satisfying some appropriate estimates, the solution V^\u03c0 to the Kolmogorov PDE (<ref>) is smooth. In this sense, it preserves the regularity of the terminal condition.\n         \n         However, one can weaken the regularity assumption on the terminal condition (and actually of the coefficients themselves) by benefiting from the smoothness of the underlying fundamental solution (or the transition density of the associated stochastic process) under some additional non-degeneracy assumption. We refer e.g. to <cit.>, <cit.>, <cit.> in the uniformly elliptic diffusion setting and to <cit.> in the case of non-degenerate stable driven SDE.\n           Proof.  See Appendix <ref>\n        \n        \u00a7 POLICY GRADIENT METHOD\n        \n        \n        \n        \n        We now consider a parametric family of randomised policies \u03c0_\u03b8, with densities p_\u03b8, \u03b8\u2208\u0398, \u0398 being a non-empty open subset of ^D, for some positive integer D, \n        and denote by (\u03b8)=J(\u03c0_\u03b8) the associated cost function, viewed as a function of the parameters \u03b8, recalling that J is defined by (<ref>). The principle of  \n        policy gradient method is to minimize  over \u03b8 the function (\u03b8) by stochastic gradient descent  algorithm. In our RL setting, we aim to derive a probabilistic representation of the gradient function  \u2207_\u03b8(\u03b8) that does not involve model coefficients \n        b,\u03c3, but only observation samples of state X_t, state distribution _X_t, and rewards f_t:=f(X_t,_X_t,\u03b1_t), g_T:=g(X_T,_X_T) when taking decision \u03b1\u223c\u03c0_\u03b8. \n        \n        \n        \n        \n         \u00a7.\u00a7 Policy gradient representation\n        \n         \n         \n        \n        We make the following assumptions on the parametric family of randomised policy and coefficients.\n        \n        \n        \n          (i) For any h \u2208{ b^i_\u03c0_\u03b8, \u03c3_\u03c0_\u03b8^i, j, f_\u03c0_\u03b8, E_\u03c0_\u03b8, g, i=1, \u22ef, d, j = 1, \u22ef, p}, any multi-indices \u03b1, \u03b2, \u03bb of {1, \u22ef, d} such that 0\u2264 |\u03b1| \u2264 2, 0\u2264 |\u03b2| \u2264 1, \u03bb being of length n, 0\u2264 n \u2264 2, any n-tuple of multi-indices \u03b3=(\u03b3_1, \u22ef, \u03b3_n) with 0\u2264 |\u03b3_1| + \u22ef + |\u03b3_n|\u2264 2, denoting by h_\u03b8(t, x, \u03bc) the value of h at (\u03b8, t, x, \u03bc), the following derivatives \n            \u2202^\u03b2_\u03b8\u2202_x^\u03b1\u2202^\u03b3_v\u2202^\u03bb_\u03bc h_\u03b8(t, x, \u03bc)(v),  \u2202_x^\u03b1\u2202^\u03b2_\u03b8\u2202^\u03b3_v\u2202^\u03bb_\u03bc h_\u03b8(t, x, \u03bc)(v),  \u2202^\u03b1_x\u2202^\u03b3_v\u2202^\u03b2_\u03b8\u2202^\u03bb_\u03bc h_\u03b8(t, x, \u03bc)(v),  \u2202^\u03b1_x\u2202^\u03b3_v\u2202^\u03bb_\u03bc\u2202^\u03b2_\u03b8  h_\u03b8(t, x, \u03bc)(v), \n               \u2202^\u03b3_v\u2202^\u03b1_x\u2202^\u03bb_\u03bc\u2202^\u03b2_\u03b8  h_\u03b8(t, x, \u03bc)(v), \u2202^\u03b3_v\u2202^\u03bb_\u03bc\u2202^\u03b1_x\u2202^\u03b2_\u03b8  h_\u03b8(t, x, \u03bc)(v),  \u2202^\u03b3_v\u2202^\u03bb_\u03bc\u2202^\u03b2_\u03b8\u2202^\u03b1_x h_\u03b8(t, x, \u03bc)(v), \u2202^\u03b3_v\u2202^\u03b2_\u03b8\u2202^\u03bb_\u03bc\u2202^\u03b1_x h_\u03b8(t, x, \u03bc)(v), \n               \u2202^\u03b2_\u03b8\u2202^\u03b3_v\u2202^\u03bb_\u03bc\u2202^\u03b1_x h_\u03b8(t, x, \u03bc)(v), \u2202^\u03b3_v\u2202^\u03b2_\u03b8\u2202^\u03b1_x\u2202^\u03bb_\u03bc   h_\u03b8(t, x, \u03bc)(v),\n             \u2202^\u03b3_v\u2202^\u03b1_x\u2202^\u03b2_\u03b8\u2202^\u03bb_\u03bc   h_\u03b8(t, x, \u03bc)(v), \u2202^\u03b2_\u03b8\u2202^\u03b1_x\u2202^\u03b3_v\u2202^\u03bb_\u03bc   h_\u03b8(t, x, \u03bc)(v),\n         exist for any (t,\u03b8, x, v, \u03bc) \u2208  [0,T] \u00d7\u0398\u00d7 (\u211d^d)^n+1\u00d7_2(\u211d^d) and are locally Lipschitz continuous with respect to \u03b8, x, \u03bc, v uniformly in t\u2208 [0,T][Hence, according to Clairaut's theorem, these partial derivatives are equal.]. Moreover, if h= b^i_\u03c0_\u03b8 or \u03c3_\u03c0_\u03b8^i, j, the aforementioned derivatives of order greater or equal to one are bounded.\n        \n          (ii)  The estimates of Assumption <ref>(iii)  are satisfied for the family of policies {\u03c0_\u03b8, \u03b8\u2208\u0398}, locally uniformly in \u03b8, i.e. for any \u03b8\u2208\ud835\udca6, \ud835\udca6 being any compact subset of \u0398. Additionally, there exists some constant C< \u221e, such that for any h\u2208{f_\u03c0_\u03b8, E_\u03c0_\u03b8}, any (t, \u03bc, x) \u2208 [0,T] \u00d7_2(\u211d^d) \u00d7\u211d^d, any v= (v_1, v_2) \u2208 (\u211d^d)^2, any \u03b8\u2208\ud835\udca6, \ud835\udca6 being any compact subset of \u0398, any multi-index \u03bb, |\u03bb| =2, any multi-index \u03bb=(\u03bb_1, \u03bb_2) of {1, \u22ef, d}, any couple of multi-indices \u03b3=(\u03b3_1, \u03b3_2)\n            | \u2202_\u03b8 h_\u03b8 (t, x, \u03bc)|  \u2264 C (1 + |x|^2 + M_2(\u03bc)^q),\n        \n            | \u2202_\u03b8\u2202_x h_\u03b8 (t, x, \u03bc)|  + |\u2202_x g(x, \u03bc)| \u2264 C (1+ |x| + M_2(\u03bc)^q),\n        \n            | \u2202_\u03b8\u2202_\u03bc h_\u03b8 (t, x, \u03bc)(v_1) | + | \u2202_\u03bc\u2202_x h_\u03b8 (t, x, \u03bc)(v_1) | + | \u2202_\u03bc\u2202_x g ( x, \u03bc)(v_1) |\u2264 C (1+ |x| + |v_1| + M_2(\u03bc)^q),\n        \n            | \u2202_\u03b8\u2202_v_1\u2202_\u03bc h_\u03b8 (t, x, \u03bc)(v_1) |+   | \u2202_\u03b8\u2202^2_x h_\u03b8 (t, x, \u03bc)| +    | \u2202_\u03bc\u2202^2_x h_\u03b8 (t, x, \u03bc)(v_1)| + | \u2202_v^\u03b3\u2202^\u03bb_\u03bc h_\u03b8(t, x, \u03bc)(v)|  \n                + |\u2202_v_1\u2202_\u03bc g(x, \u03bc)(v_1)|  + |\u2202^2_x g(x, \u03bc)| + | \u2202_v^\u03b3\u2202^\u03bb_\u03bc g(t, x, \u03bc)(v)|\u2264 C (1+ M_2(\u03bc)^q),\n         for some q\u2265 0.\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        As shown in Appendix <ref>, Assumption <ref> guarantees that the derivatives (t, \u03b8, x, \u03bc, v)\u21a6\u2202_\u03b8\u2202_t _\u03b8(t, x, \u03bc), \n        \u2202_\u03b8_\u03b8(t, x, \u03bc), \u2202_\u03b8\u2202_x_\u03b8(t, x, \u03bc), \u2202_\u03b8\u2202_\u03bc_\u03b8(t, x, \u03bc)(v),  \u2202_\u03b8\u2202^2_x_\u03b8(t, x, \u03bc), \u2202_\u03b8\u2202_v\u2202_\u03bc_\u03b8(t, x, \u03bc)(v), where _\u03b8(t, x, \u03bc):=V^\u03c0_\u03b8(t, x, \u03bc) defined by (<ref>) with \u03c0=\u03c0_\u03b8, exist, are continuous and satisfy suitable growth conditions. \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        We then  let \u2207_\u03b8(\u03b8)=[_\u03b8(0, \u03be, \u03bc)] where _\u03b8(t, x, \u03bc) := \u2207_\u03b8_\u03b8(t, x, \u03bc).  The main result of this section provides a probabilistic representation of the gradient function _\u03b8. \n        \n         \n        Suppose that Assumption <ref> holds. Assume moreover that for any t, x, \u03bc, a, the map \u0398\u220b\u03b8\u21a6 p_\u03b8(t, x, \u03bc, a) is differentiable with a derivative satisfying the following estimates: for some constant C<\u221e and some q\u22650, for any (t,x, \u03bc) \u2208 [0,T] \u00d7^d \u00d7_\n        2(^d) and any compact subset \ud835\udca6\u2282\u0398.\n        \n            \u222b_Asup_\u03b8\u2208\ud835\udca6   { | \u2207_\u03b8 p_\u03b8(t, x, \u03bc, a)| ( |b(x, \u03bc, a)| + |(\u03c3\u03c3)^(x, \u03bc, a)| \n                   + |f(x, \u03bc, a)| + |log(p_\u03b8(t, x, \u03bc, a))|) }\u03bd(\u1ea1) < \u221e,\n         and\n        \n            \u222b_A     | \u2207_\u03b8log(p_\u03b8(t, x, \u03bc, a))|^2 |\u03c3(x, \u03bc, a)|^2   p_\u03b8(t, x, \u03bc, a)  \u03bd(\u1ea1) \u2264 C (1+ |x|^q + M_2(\u03bc)^q).\n        \n        \n        Then, it holds \n        \n            _\u03b8(t,x,\u03bc)     =  _\u03b1\u223c\u03c0_\u03b8[ \u222b_t^T e^-\u03b2(s-t)\u2207_\u03b8log p_\u03b8(s,X_s^t,x,\u03bc,_X_s^t,\u03be,\u03b1_s) {_\u03b8(s,X_s^t,x,\u03bc,_X_s^t,\u03be) \n                 +  [  f(X_s^t,x,\u03bc,_X_s^t,\u03be,\u03b1_s) +  \u03bblog p_\u03b8(s,X_s^t,x,\u03bc,_X_s^t,\u03be,\u03b1_s)  - \u03b2_\u03b8(s,X_s^t,x,\u03bc,_X_s^t,\u03be)  ] \u1e63}\n                +  \u222b_t^T  e^-\u03b2(s-t)_\u03b8[_\u03b8](s,X_s^t, x,\u03bc,_X_s^t,\u03be)   \u1e63],\n         \n        for any (t,x,\u03bc, \u03b8)\u2208[0,T]\u00d7^d\u00d7_2(^d) \u00d7\u0398 and \u03be\u223c\u03bc, where _\u03b8 is the operator defined by \n        \n            _\u03b8[\u03c6](t,x,\u03bc)    =  _\u03be\u223c\u03bc[ \u2207_\u03b8  b_\u03b8(t,\u03be,\u03bc) \u2202_\u03bc\u03c6(t,x,\u03bc)(\u03be) \n                  + 1/2 tr_1,2( \u2207_\u03b8\u03a3_\u03b8(t,\u03be,\u03bc) \u2219_1 \u2202_\u03c5\u2202_\u03bc\u03c6(t,x,\u03bc)(\u03be))   ],\n        \n         and we set b_\u03b8(t,x,\u03bc)=\u222b_A b(x,\u03bc,a)  \u03c0_\u03b8(\u1ea1|t,x,\u03bc), \u03a3_\u03b8(t,x,\u03bc)   =  \u222b_A (\u03c3\u03c3)(x,\u03bc,a)  \u03c0_\u03b8(\u1ea1|t,x,\u03bc). Here \u2207_\u03b8\u03a3_\u03b8=(\u2202\u03a3_\u03b8^ij/\u2202\u03b8_k)_i,j,k\u2208^d\u00d7 d\u00d7 D is a tensor of order 3, and we used the product tensor notations \u2219_1  recalled in the introduction.  \n        [On the martingale property of the policy gradient] \n        \n        Notice that for any function B on [0,T]\u00d7^d\u00d7_2(^d) (called baseline function), we have for all 0\u2264 t\u2264 s\u2264 T, \n        _\u03b1\u223c\u03c0_\u03b8[ \u2207_\u03b8log p_\u03b8(s,X_s^t,x,\u03be,_X_s^t,\u03be,\u03b1_s) B(s,X_s^t,x,\u03be,_X_s^t,\u03be) ]    =    0. \n        \n        Indeed, \n              _\u03b1\u223c\u03c0_\u03b8[ \u2207_\u03b8log p_\u03b8(s,X_s^t,x,\u03be,_X_s^t,\u03be,\u03b1_s) B(s,X_s^t,x,\u03be) | X_s^t,x,\u03be]  \n           =    B(s,X_s^t,x,\u03be,_X_s^t,\u03be) \u222b_A \u2207_\u03b8log p_\u03b8(s,X_s^t,x,\u03be,_X_s^t,\u03be,a) \u03c0_\u03b8(\u1ea1| s,X_s^t,x,\u03be,_X_s^t,\u03be)   \n           =    B(s,X_s^t,x,\u03be,_X_s^t,\u03be) \u2207_\u03b8\u222b_A \u03c0_\u03b8(\u1ea1| s,X_s^t,x,\u03be,_X_s^t,\u03be)   =   0. \n        \n        The representation in Theorem <ref> also means that the process \n        \n            { e^-\u03b2(s-t)_\u03b8(s,X_s^t,x,\u03bc,_X_s^t,\u03be) + \u222b_t^s  e^-\u03b2(r-t)\u2207_\u03b8log p_\u03b8(r,X_r^t,x,\u03bc,_X_r^t,\u03be,\u03b1_r) {_\u03b8(r,X_r^t,x,\u03bc,_X_r^t,\u03be)  \n                 +  [  f(X_r^t,x,\u03bc,_X_r^t,\u03be,\u03b1_r) +  \u03bblog p_\u03b8(r,X_r^t,x,\u03bc,_X_r^t,\u03be,\u03b1_r) - \u03b2_\u03b8(r,X_r^t,x,\u03bc,_X_r^t,\u03be)   ] \u1e5b}\n                 + \u222b_t^s e^-\u03b2(r-t)_\u03b8[_\u03b8](r,X_r^t,x,\u03bc,_X_r^t,\u03be)   \u1e5b,   t \u2264 s \u2264 T}\n         \n        is a martingale, for any given \u03b1\u223c\u03c0_\u03b8. \n         Proof. See Appendix <ref>\n        \n        \n        \n        \n        \n        In the next section, we show how the probabilistic representation formula of the gradient function _\u03b8 provided by Theorem <ref> can be used to design two actor-critic algorithms for learning optimal cost function and randomised policy by relying on samples of the actions, states and state distributions. \n        \n        \n         \n        \n        \n        \n        \n        \n        \n        \n         \u00a7.\u00a7 Actor-critic Algorithms\n        \n        \n         \n        Actor-critic (AC) methods combine policy gradient (PG) and performance evaluation (PE).  Compared to most existing works on RL for mean-field problems, mainly based on Q-learning (see e.g. <cit.>, <cit.>, <cit.>)\n        we do not assume that the agent (the social planner) has at disposal a simulator for the state distribution, but instead will estimate the distribution of the population from the observation of the state of the representative player and by updating the distribution along repeated episodes. More precisely, for each episode i=1,2,\u2026,N,  from the observation of the state X_t_k^i of a representative player i  at time t_k, we update the state distribution according to\n        \n            \u03bc_t_k^i    =    (1-\u03c1^i_S) \u03bc_t_k^i-1 + \u03c1^i_S \u03b4_X_t_k^i,\n         \n        where (\u03c1^i_S)_i is a sequence of learning parameters in (0,1), e.g. \u03c1^i_S=1/i. It is expected from the propagation of chaos,  that when the number of episodes  N goes to infinity, \u03bc_t_k^N converge to the limiting distribution _X_t_k of the population. \n        Notice that a similar estimation procedure was  recently proposed in <cit.> in the context of a  MFC control problem in discrete time with finite state and action spaces over an infinite horizon.  \n        \n        \n        In addition to the family of randomised policies (t,x,\u03bc)\u21a6\u03c0_\u03b8(\u1ea1|t,x,\u03bc)=p_\u03b8(t,x,\u03bc)\u03bd(\u1ea1), with parameter \u03b8, we are given a family of  functions (t,x,\u03bc)\u21a6^\u03b7(t,x,\u03bc) on [0,T]\u00d7^d\u00d7_2(^d), with parameter \u03b7, aiming to approximate the optimal cost value function. \n        AC algorithm is then updating alternately the two parameters to find the optimal pair (\u03b8^*,\u03b7^*), hence determining the approximate optimal randomised policy and the associated cost value function.  On the one hand, \n        the loss function in the PE step  for learning ^\u03b7, for fixed policy \u03c0_\u03b8,  is based on the martingale formulation of the process\n        { e^-\u03b2 t^\u03b7(t,X_t^x,\u03bc,_X_t^\u03be) + \u222b_0^t  e^-\u03b2 r[  f(X_r^x, \u03bc,_X_r^\u03be,\u03b1_r)  +  \u03bblog p_\u03b8(r,X_r^x,\u03bc,_X_r^\u03be,\u03b1_r)  ] \u1e5b,   0 \u2264 t \u2264 T}, \n        \n        and on the other hand, the objective (here a cost) function in the PG step  for learning \u03c0_\u03b8, for fixed ^\u03b7, is based on the martingale formulation of the process\n        \n            { e^-\u03b2 t_\u03b8(t,X_t^x,\u03bc,_X_t^\u03be) + \u222b_0^t  e^-\u03b2 r\u2207_\u03b8log p_\u03b8(r,X_r^x,\u03bc,_X_r^\u03be,\u03b1_r) [ ^\u03b7(r,X_r^x,\u03bc,_X_r^\u03be)  \n                 +  (  f(X_r^x,\u03bc,_X_r^\u03be,\u03b1_r)  +  \u03bblog p_\u03b8(r,X_r^x,\u03bc,_X_r^\u03be,\u03b1_r) - \u03b2^\u03b7(r,X_r^x,\u03bc,_X_r^\u03be) ) \u1e5b] \n                +  _\u03b8[^\u03b7](r,X_r^x,\u03bc,_X_r^\u03be)   \u1e5b,   0 \u2264 t \u2264 T}.\n         \n        Here, we denote X^x,\u03bc=X^0,x,\u03bc  (resp. X^\u03be=X^0,\u03be) when the initial time of the flow is t=0.  \n        We emphasise that these loss functions are minimised by training samples of the state trajectories X_t^x_0,\u03be, actions \u03b1\u223c\u03c0_\u03b8,  estimation \u03bc_t of _X_t^\u03be according to (<ref>), and observation of the associated running and terminal costs. \n        \n        \n        \n        We  first develop  AC algorithms in the offline setting where all  state trajectories are sampled. In this case, given \u03b8, the proposed loss function for the PE step is\n        \n        L^PE(\u03b7)    =   _\u03b1\u223c\u03c0_\u03b8[ \u222b_0^T | e^-\u03b2(T-t) g(X_T,_X_T) \n                + \u222b_t^T e^-\u03b2(r-t)[ f(X_r,_X_r,\u03b1_r) +  \u03bblog p_\u03b8(r,X_r^,_X_r^,\u03b1_r)] \u1e5b - ^\u03b7(t,X_t,_X_t) |^2 \u1e6d],\n         which leads, after  time discretisation of [0,T] on the grid {t_k = k \u0394 t, k=0,\u2026,n},  \n        and by applying stochastic gradient descent (SGD) with learning rate \u03c1_E,  to the following update rule: \n        \u03b7   \u2190   \u03b7 + \u03c1_E \u2211_k=0^n-1( e^-\u03b2(n-k)\u0394 t g_t_n   + \u2211_\u2113=k^n-1  e^-\u03b2(\u2113-k)\u0394 t[ f_t_\u2113 +  \u03bblog p_\u03b8(t_\u2113,X_t_\u2113,\u03bc_t_\u2113,\u03b1_t_\u2113)] \u0394 t \n                -  ^\u03b7(t_k,X_t_k,\u03bc_t_k) ) \u2207_\u03b7^\u03b7(t_k,X_t_k,\u03bc_t_k) \u0394 t, \n        \n        where we set   f_t_\u2113=f(X_t_\u2113,_X_t_\u2113,\u03b1_t_\u2113), as the output running cost at time t_\u2113, for an input state X_t_l, action \u03b1_t_\u2113,   \u2113=0,\u2026,n-1,   \n        and g_T=g(X_T,_X_T) the terminal cost for an input X_T.  \n        Given \u03b7, the learning in the PG step relies on the gradient representation (<ref>), and (after time discretisation) leads to the update rule\n        \u03b8   \u2190   \u03b8 -  \u03c1_G \u011c_\u03b8, \n          \u011c_\u03b8    =    \u2211_k=0^n-1 e^-\u03b2 t_k\u2207_\u03b8log p_\u03b8(t_k,X_t_k,\u03bc_t_k,\u03b1_t_k) [ ^\u03b7(t_k+1,X_t_k+1,\u03bc_t_k+1) - ^\u03b7(t_k,X_t_k,\u03bc_t_k) \n               +  ( f_t_k    +  \u03bblog p_\u03b8(t_k,X_t_k,\u03bc_t_k,\u03b1_t_k)    - \u03b2^\u03b7(t_k,X_t_k,\u03bc_t_k)  ) \u0394 t ]   +  _\u03b8[^\u03b7](t_k,X_t_k,\u03bc_t_k)   \u0394 t.  \n        \n        The pseudo-code is described in Algorithm <ref>. \n        \n        \n        [H] \n        \n         Input data: Number of episodes N, number of mesh time-grid n (\u2194 time step \u0394 t=T/n), learning rates  \u03c1_S^i, \u03c1_E^i, \u03c1_G^i for the state distribution, PE and PG estimation, and function of the number of episodes i. \n        Parameter \u03bb for entropy regularisation. \n        Functional forms ^\u03b7 of cost value function,  p_\u03b8 of density policies. \n        \n         Initialisation: \u03bc_t_k: state distribution on ^d, for k=0,\u2026,N,  parameters \u03b7, \u03b8. \n        \n        each episode i=1,\u2026,NInitialise state X_0\u223c\u03bc_0\n        k=0,\u2026,n-1Update state distribution: \u03bc_t_k\u2190(1-\u03c1_S^i) \u03bc_t_k + \u03c1_S^i \u03b4_X_t_k\n        \n        Generate action \u03b1_t_k\u223c\u03c0_\u03b8(.|t_k,X_t_k,\u03bc_t_k)\n        \n        Observe (e.g. by environment simulator) state X_t_k+1 and cost f_t_k\n        \n        If k=n-1, update terminal state distribution: \u03bc_t_n\u2190(1-\u03c1_S) \u03bc_t_n + \u03c1_S \u03b4_X_t_n, and observe terminal cost g_t_n\n        k\u2190k+1Compute\n        \u0394_\u03b7   =   \u2211_k=0^n-1( e^-\u03b2(n-k)\u0394 t  g_t_n   + \u2211_\u2113=k^n-1 e^-\u03b2(\u2113-k)\u0394 t[ f_t_\u2113  +  \u03bblog p_\u03b8(t_\u2113,X_t_\u2113,\u03bc_t_\u2113,\u03b1_t_\u2113)] \u0394 t \n                -  ^\u03b7(t_k,X_t_k,\u03bc_t_k) ) \u2207_\u03b7^\u03b7(t_k,X_t_k,\u03bc_t_k) \u0394 t \n        \u011c_\u03b8    =    \u2211_k=0^n-1 e^-\u03b2 t_k\u2207_\u03b8log p_\u03b8(t_k,X_t_k,\u03bc_t_k,\u03b1_t_k) [ ^\u03b7(t_k+1,X_t_k+1,\u03bc_t_k+1) - ^\u03b7(t_k,X_t_k,\u03bc_t_k) \n               +  ( f_t_k   +  \u03bblog p_\u03b8(t_k,X_t_k,\u03bc_t_k,\u03b1_t_k) - \u03b2^\u03b7(t_k,X_t_k,\u03bc_t_k)  ) \u0394 t ]   +  _\u03b8[^\u03b7](t_k,X_t_k,\u03bc_t_k)   \u0394 t.  \n        \n        Critic Update: \u03b7\u2190\u03b7 + \u03c1_E^i \u0394 _\u03b7;  Actor Update: \u03b8\u2190\u03b8  -   \u03c1_G^i \u011c_\u03b8 Return: ^\u03b7, \u03c0_\u03b8Offline actor-critic mean-field algorithm  \n        \n        \n        \n        \n        We next develop AC algorithm for online setting where only past sample trajectory is available, and so the parameters (\u03b8,\u03b7) are updated in real-time incrementally. In this case, given a policy \u03c0_\u03b8, we consider  at each time step t_k, k=0,\u2026,n-1, a loss function for  PE  \n        given by \n        \n        L_t_k^PE(\u03b7)    =   _\u03b1\u223c\u03c0_\u03b8[ |  ^\u03b7(t_k+1,X_t_k+1,_X_t_k+1)  - ^\u03b7(t_k,X_t_k,_X_t_k) \n               +  ( f(X_t_k,_X_t_k,\u03b1_t_k)   +   \u03bblog p_\u03b8(t_k,X_t_k,_X_t_k,\u03b1_t_k)  - \u03b2^\u03b7(t_k,X_t_k,_X_t_k)) \u0394 t |^2 ].\n        \n        Concerning PG, we note that when \u03b8 is an optimal parameter, we should have _\u03b8=0.  Therefore, from the martingale condition in (<ref>), this suggests to find \u03b8 such that at any time  t_k, k=0,\u2026,n-1_\u03b1\u223c\u03c0_\u03b8{\u2207_\u03b8log p_\u03b8(t_k,X_t_k,_X_t_k,\u03b1_t_k)  [ ^\u03b7(t_k+1,X_t_k+1,_X_t_k+1)  - ^\u03b7(t_k,X_t_k,_X_t_k)        \n        \n          +  ( f(X_t_k,_X_t_k,\u03b1_t_k)   +  \u03bblog p_\u03b8(t_k,X_t_k,_X_t_k,\u03b1_t_k) - \u03b2^\u03b7(t_k,X_t_k,_X_t_k)  ) \u0394 t   ] + _\u03b8[^\u03b7](t_k,X_t_k,\u03bc_t_k)   \u0394 t }      \n          \n           =  0.       \n        The pseudo-code is described in Algorithm <ref>. \n        \n        \n        \n        [H] \n        \n         Input data: Number of episodes N, number of mesh time-grid n (\u2194 time step \u0394 t=T/n), learning rates  \u03c1_S^i, \u03c1_E^i, \u03c1_G^i for the state distribution, PE and PG estimation, and function of the number of episodes i.  Parameter \u03bb for entropy regularisation. \n        Functional forms ^\u03b7 of cost value function,  p_\u03b8 of density policies. \n        \n         Initialisation: \u03bc_t_k: state distribution on ^d, for k=0,\u2026,n,  parameters \u03b7, \u03b8. \n        \n        each episode i=1,\u2026,NInitialise state X_0\u223c\u03bc_0\n        k=0,\u2026,n-1Update state distribution: \u03bc_t_k\u2190(1-\u03c1_S^i) \u03bc_t_k + \u03c1_S^i \u03b4_X_t_k\n        \n        Generate action \u03b1_t_k\u223c\u03c0_\u03b8(.|t_k,X_t_k,\u03bc_t_k)\n        \n        Observe (e.g. by environment simulator) state X_t_k+1 and cost  f_t_k\n        \n        If k=n-1, update terminal state distribution: \u03bc_t_k+1\u2190(1-\u03c1_S^i) \u03bc_t_k+1 + \u03c1_S^i \u03b4_X_t_k+1, and observe terminal cost g_t_k+1\n        \n        \n        Compute\n        \u03b4_\u03b7   =   ^\u03b7(t_k+1,X_t_k+1,\u03bc_t_k+1) - ^\u03b7(t_k,X_t_k,\u03bc_t_k)   \n                   +  (   f_t_k  +  \u03bblog p_\u03b8(t_k,X_t_k,\u03bc_t_k,\u03b1_t_k)  - \u03b2^\u03b7(t_k,X_t_k,\u03bc_t_k)  ) \u0394 t     \n        \u0394_\u03b7   =   \u03b4_\u03b7\u2207_\u03b7^\u03b7(t_k,X_t_k,\u03bc_t_k) \n        \u0394_\u03b8   =   \u03b4_\u03b7\u2207_\u03b8log p_\u03b8(t_k,X_t_k,\u03bc_t_k,\u03b1_t_k) +   _\u03b8[^\u03b7](t_k,X_t_k,\u03bc_t_k)  \u0394 t, \n        \n        with the constraint that when k=n-1,  ^\u03b7(t_k+1,X_t_k+1,\u03bc_t_k+1)=g_t_k+1.\n        \n        Critic Update: \u03b7\u2190\u03b7 + \u03c1_E^i \u0394 _\u03b7;  Actor Update: \u03b8\u2190\u03b8 -  \u03c1_G^i  \u0394_\u03b8\n        k\u2190k+1 Return: ^\u03b7, \u03c0_\u03b8Online actor-critic mean-field algorithm  [About the choice of actor and critic parametric functions]  \n        In the Actor-critic algorithms, we have to specify a parametric family of  randomised policies \u03c0_\u03b8, and  a parametric family of critic functions ^\u03b7. In general, for critic functions, one can consider cylindrical neural network functions in the form\n        \n            ^\u03b7(t,x,\u03bc)    = \u03a8(t,x, <\u03c6,\u03bc>),    (t,x,\u03bc) \u2208 [0,T]\u00d7^d\u00d7_2(^d),\n         \n        where \u03a8 is a feedforward neural network from [0,T]\u00d7^d\u00d7^k into , and \u03c6 is another feedforward neural network from ^d into ^k (called latent space), and we use the notation \n        <\u03d5,\u03bc>:=\u222b\u03d5(x)\u03bc(x\u0323).  The set of parameters \u03b7 is the union of the parameter sets for the two  neural networks \u03a8 and \u03c6. \n        This choice is motivated by the density property of the set of cylindrical functions, i.e. functions in the form  (<ref>) with continuous functions \u03a8 and \u03c6, with respect to continuous functions on [0,T]\u00d7^d\u00d7_2(^d) as  shown  in <cit.>, \n        and the universal approximation property  of feedforward neural networks on finite-dimensional space, see <cit.>. \n        \n        Concerning the policies, notice that when the temperature parameter  for exploration \u03bb is zero, the optimal policy is of pure (non randomised)  feedback form as a function of (t,x,\u03bc). When \n        \u03bb>0, the optimal policy is in general truly randomised, and the larger is \u03bb, the larger is the exploration in the sense that the variance of the randomised policy increases.  We can  then take for the parametric family of randomised policies,  for example Gaussian distributions: \n        \n        \u03c0_\u03b8(.|t,x,\u03bc)     =   ( (t,x,\u03bc); \u03d1(\u03bb) ), \n        \n        where  is a cylindrical neural network function on [0,T]\u00d7^d\u00d7_2(^d) valued in A\u2282^m, and \u03d1(.) is a given symmetric matrix-valued function, nondecreasing w.r.t. \u03bb, with \u03d1(\u03bb) positive-definite for \u03bb>0, and \u03d1(0)=0. \n        \n        In some particular mean-field models, we may know  a priori the structural form of the optimal value function and optimal randomised policy, and this suggests alternately some specific form for the parametric family of actor and critic functions. This is typically the case of the linear quadratic model, as presented in the next section. \n        \n        \n        \n        The above actor-critic algorithms involve the computation of the term _\u03b8[^\u03b7] at each time t_k, and along the observed state X_t_k and estimated state distribution \u03bc_t_k. This additional term, compared to the actor-critic algorithms designed in <cit.>  for standard stochastic control without mean-field interaction, involves the operator _\u03b8 defined in (<ref>).  In the separable form case, namely when the coefficients of the mean-field process are in the form\n        \n        b(x,\u03bc,a)   =  (x,\u03bc) +  C(a),        (\u03c3\u03c3)(x,\u03bc,a)   =  \u03a3(x,\u03bc) + F(a), \n        \n        where C and F are known functions from A into ^d, resp. ^d\u00d7 d, we notice that \n        \u2207_\u03b8 b_\u03b8(t,x,\u03bc)     =    \u2207_\u03b8 C_\u03b8(t,x,\u03bc),       C_\u03b8(t,x,\u03bc)   :=  \u222b C(a)   \u03c0_\u03b8(\u1ea1|t,x,\u03bc), \n        \n        \u2207_\u03b8\u03a3_\u03b8(t,x,\u03bc)     =    \u2207_\u03b8 F_\u03b8(t,x,\u03bc),         F_\u03b8(t,x,\u03bc)   :=  \u222b  F(a)   \u03c0_\u03b8(\u1ea1|t,x,\u03bc), \n        \n        \n        are known functions,  and consequently  also the function _\u03b8[^\u03b7].  Another important case where the term _\u03b8[^\u03b7] is a known computable function is given in the linear quadratic framework as presented in the next section. \n        \n        \n        \n        \n        \u00a7 THE LINEAR QUADRATIC CASE\n         \n        \n        \n        We focus on  the important class of MFC control problem with linear state dynamics and quadratic reward, namely \n        \n            b(x,\u03bc,a)   =    B x + B\u0305\u03bc\u0305+  C a,    \u03c3(x,\u03bc,a)   =  \u03b3 + D x + D\u0305\u03bc\u0305+ Fa,  \n            \n            f(x,\u03bc,a)   =   x Qx + \u03bc\u0305Q\u0305\u03bc\u0305+ a N a  + 2a Ix + 2aI\u0305\u03bc\u0305+  2M.x + 2H.a, \n            \n            g(x,\u03bc)   =   x P x + \u03bc\u0305P\u0305\u03bc\u0305+ 2L.x,\n         \n        for (x,\u03bc,a)\u2208^d\u00d7_2(^d)\u00d7^m, where we denote by \u03bc\u0305=\u222b x \u03bc(x\u0323),  B, B\u0305, D, D\u0305 are constant matrices in ^d\u00d7 d, C, F are constant matrices in ^d\u00d7 m, \u03b3 is a constant in ^d, \n        N is a  symmetric matrix in _+^m, I, I\u0305\u2208^m\u00d7 d,  Q, Q\u0305, P, P\u0305 are  symmetric matrices in  ^d,  with Q\u22650, P\u22650, \n        \n        \n        \n        \n        M, L\u2208^d, H\u2208^m. \n        \n        In this case,  the optimal value function to this LQ MFC problem with entropy regularisation when minimizing over randomised controls a functional cost as in (<ref>),   is given by \n        \n            v(t,x,\u03bc)    =   (x - \u03bc\u0305) K(t) (x-\u03bc\u0305) + \u03bc\u0305\u039b(t)\u03bc\u0305+ 2 Y(t).x + R(t),\n         \n        where K (valued in ^d), \u039b (valued in ^d), Y valued in ^d, and R valued in , are solutions to a system of ordinary differential equations on [0,T] given in (<ref>). \n        Moreover,  the optimal randomised control is of feedback form with  Gaussian distribution:\n        \u03c0^*(.|t,x,\u03bc)     =     ( - S(t)^-1(  U(t) x + (W(t) - U(t))\u03bc\u0305+ O(t) ); \u03bb/2 S(t)^-1), \n        \n        where \n        \n        S(t)   =   N + F K(t) F,        O(t)   =    H + C Y(t)  + F K(t) \u03b3\n        \n        U(t)   =   I + C K(t) + F K(t) D,         W(t)   =   I + I\u0305 + C \u039b(t) + F K(t) (D + D\u0305). \n        \n        This is an extension of  the mean-field LQ control without entropy and control randomization, and the proof that adapts arguments in <cit.> is reported in Appendix <ref>. \n        \n        \n        K\u0307 + KB + B K + D K D + Q        \n        \n        -   (I+ F KD + C K)(N + F K F)^-1(I+ F KD + C K)    =     0,       t \u2208 [0,T],  \n        \n        K(T)    =     P,  \n        \u039b\u0307+ \u039b(B+B\u0305) + (B+B\u0305)\u039b + (D+D\u0305)\u039b (D+D\u0305) + Q + Q\u0305      \n        \n        -    (I+ I\u0305+ F K(D+D\u0305) + C\u039b)(N + F K F)^-1 (I + I\u0305 + F K(D+D\u0305) + C\u039b)    =    0,    t \u2208 [0,T], \n        \u039b(T)    =    P + P\u0305,  \n        \u1e8e + (B + B\u0305) Y + (D+D\u0305) K \u03b3 + M       \n        \n         -    ( I+ I\u0305+  F K(D+D\u0305) + C\u039b)(N + F KF)^-1(H + F K\u03b3 + C Y)     =    0,    t \u2208 [0,T], \n        \n        Y(T)    =    L,  \n        \u1e58 + \u03b3 K \u03b3  - (H + F K\u03b3 + C Y)(N + F KF)^-1(H + F K\u03b3 + C Y)       \n        \n        -   \u03bb m/2log(2\u03c0)-\u03bb/2log|\u03bb/2det(N+F K(t) F)|   =    0,      t \u2208 [0,T],  \n        \n        R(T)    =    0. \n        \n        Moreover, the optimal randomised is in feedback form: \u03c0_t^*(\u1ea1)=\u03c0\u0302(\u1ea1 | t,X_t^*,_X_t^*) with Gaussian distribution\n        \u03c0^*(.| t,x,\u03bc)    =    (  - S_t^-1(U_t  x + (V_t-U_t) \u03bc\u0305+ O_t)  ;  \u03bb/2 S_t^-1),\n        \n        where \n        \n        S_t  =    N+F K(t) F,         O_t  =    H+ F K(t)\u03b3 +C Y(t),  \n        \n        U_t  =    I+F K(t)D +C K(t),        V_t  =  I+I\u0305+F K(t)(D+D\u0305) +C\u039b(t). \n        \n        \n         \n         \n         \n        \n        In a RL setting, the coefficients of the LQ model (<ref>)\n        are unknown, thus  K, \u039b, Y, and R cannot be solved from the system of ODEs, and S, O, U, and W are also unknown.  We shall then employ our RL algorithms to solve the LQ problem in a model-free setting. \n        In view of the above structure of the optimal value function and randomised policy, we parametrise the cost  value function by \n        \n            ^\u03b7(t,x,\u03bc)    =   (x - \u03bc\u0305) K^\u03b7(t) (x-\u03bc\u0305) + \u03bc\u0305\u039b^\u03b7(t)\u03bc\u0305+ 2 Y^\u03b7(t).x + R^\u03b7(t),\n         \n        for some parametric functions K^\u03b7, \u039b^\u03b7, Y^\u03b7, R^\u03b7 on [0,T], with parameters \u03b7\u2208^p. On the other hand, we parametrise the randomised policies by \n        \n            \u03c0_\u03b8(.|t,x,\u03bc)    =  (  \u03d5_1^\u03b8(t) x + \u03d5_2^\u03b8(t)\u03bc\u0305+ \u03d5_3^\u03b8(t); \u03a3^\u03b8(t) ),\n         \n        for some parametric functions \u03d5_1^\u03b8,\u03d5_2^\u03b8,\u03d5_3^\u03b8,\u03a3^\u03b8 on [0,T], with parameter \u03b8\u2208^q. \n        \n        The parametric functions  K^\u03b7, \u039b^\u03b7, Y^\u03b7, R^\u03b7, and  \u03d5_1^\u03b8,\u03d5_2^\u03b8,\u03d5_3^\u03b8,\u03a3^\u03b8, could be  in general neural networks on [0,T], but depending on the examples, we could take more specific forms, as discussed in the next section. \n        \n        \n        For parametrisation of the cost  value function and randomised policies as in (<ref>), (<ref>), we  see that \n        \u2202_\u03bc^\u03b7(t,x,\u03bc)(x')    =   \n         -2 K^\u03b7(t)(x-\u03bc\u0305) + 2 \u039b\u03b7\u03bc\u0305,   \u2202_x'\u2202_\u03bc^\u03b7(t,x,\u03bc)(x') = 0,  \n        \u2207_\u03b8 b_\u03b8(t,x,\u03bc)     =      C \u2207_\u03b8\u03d5_1^\u03b8(t) \u2219_2 x + C \u2207_\u03b8\u03d5_2^\u03b8(t) \u2219_2  \u03bc\u0305+ C \u2207_\u03b8\u03d5_3^\u03b8(t)\n        \n        and then  \n        _\u03b8[^\u03b7](t,x,\u03bc)    =    2  [ (\u2207_\u03b8\u03d5_1^\u03b8(t) + \u2207_\u03b8\u03d5_2^\u03b8(t)) \u2219_2  \u03bc\u0305+ \u2207_\u03b8\u03d5_3^\u03b8(t) ] C( - K^\u03b7(t)(x-\u03bc\u0305) +  \u039b^\u03b7\u03bc\u0305),\n        \n        which only involves, up to the knowledge of C, known functions of (t,x,\u03bc). Notice also that when \u03d5_1^\u03b8=-\u03d5_2^\u03b8, and \u03d5_3^\u03b8\u22610 (see below  the example of mean-field systemic risk), then _\u03b8[^\u03b7]\u22610. \n        \n        \n        \n        \n        \u00a7 NUMERICAL EXAMPLES\n        \n        \n         \u00a7.\u00a7 Example 1: mean-field systemic risk\n        \n        \n        \n         We consider a mean-field model of systemic risk introduced in <cit.>. This fits into a LQ MFC with \n         B\u0305 = -  B > 0,   C=1,  \u03b3 > 0,   D = D\u0305 = F = 0 \n        \n        I = - I\u0305 >0,    Q + Q\u0305 = 0,    N = 1/2,   M=H=L=0,   P + P\u0305 = 0,\n         \n        and Q\u22652I^2. We also take X_0 \u223c(0,1). In this case, the solution to the system of ODEs (<ref>)  yields the analytic expression: \n        \n        K(t)    =    - 1/2[ B\u0305 + 2I - \u221a(\u0394)\u221a(\u0394)sinh(\u221a(\u0394)(T-t))  + (B\u0305 + 2I  + 2P) cosh (\u221a(\u0394)(T-t))/\u221a(\u0394)cosh(\u221a(\u0394)(T-t))  + \n        (B\u0305 + 2I + 2P) sinh (\u221a(\u0394)(T-t))], \n        \n        R(t)    =   \u03b3^2/2ln[ cosh (\u221a(\u0394)(T-t)) + B\u0305 + 2I  + 2P/\u221a(\u0394)sinh (\u221a(\u0394)(T-t)) ] - \u03b3^2/2 (B\u0305 + 2I)(T-t) \n               -\u03bb (T-t)/2log(2\u03c0\u03bb)\n        \n        with \u221a(\u0394)=\u221a((B\u0305+2I)^2  + 2Q -  4I^2),  and \u039b=Y=0, while the optimal randomised policy is given by\n        \u03c0\u0302(.|t,x,\u03bc)    =   ( \u03d5(t)(x - \u03bc\u0305) ; \u03bb),    \u03d5(t)   =    -2(K(t) + I). \n        \n         \n        \n        In view of these expressions, we shall use  critic function as\n        ^\u03b7(t,x,\u03bc)    =    K^\u03b7(t)  (x - \u03bc\u0305)^2  + R^\u03b7(t), \n        \n        for some parametric functions K^\u03b7 and R^\u03b7 on [0,T] with parameters \u03b7, and actor functions as \n        \u03c0_\u03b8(.|t,x,\u03bc)    =   (  \u03d5^\u03b8(t) (x -\u03bc\u0305); \u03bb),  \n          log p_\u03b8(t,x,\u03bc,a)     =     - 1/2log(2\u03c0\u03bb) - \n        |a - \u03d5^\u03b8(t)(x-\u03bc\u0305)|^2/2\u03bb,\n        \n        for some parametric function \u03d5^\u03b8 on [0,T] with parameter \u03b8.   As shown in Section <ref>, we notice that _\u03b8[^\u03b7]=0. \n        \n        \n        We shall test with two choices of parametric functions: \n        \n          *  Exact parametrisation:\n            K^\u03b7(t)   =     - 1/2[ \u03b7_3  - \u03b7_1 sinh(\u03b7_1(T-t))  + \u03b7_2  cosh (\u03b7_1(T-t))/cosh(\u03b7_1(T-t))  + \n            \u03b7_2  sinh (\u03b7_1(T-t))], \n            \n            R^\u03b7(t)   =  \u03b7_4 ln[ cosh (\u03b7_1(T-t)) + \u03b7_2  sinh (\u03b7_1(T-t)) ] -  \u03b7_3 \u03b7_4(T-t) -\u03bb (T-t)/2log(2\u03c0\u03bb) \n            \u03d5^\u03b8(t)    =  \u03b8_3  - \u03b8_1 sinh(\u03b8_1(T-t))  + \u03b8_2  cosh (\u03b8_1(T-t))/cosh(\u03b8_1(T-t))  + \n            \u03b8_2  sinh (\u03b8_1(T-t)),\n        \n        with parameters \u03b7=(\u03b7_1,\u03b7_2,\u03b7_3,\u03b7_4)\u2208_+^4, and \u03b8=(\u03b8_1,\u03b8_2,\u03b8_3)\u2208_+^3, so that the optimal solution in the model-based case corresponds to \n        \u03b7_1^*=\u221a(\u0394), \u03b7_2^*=B\u0305 + 2I  + 2P, \u03b7_3^*=B\u0305 + 2I, \u03b7_4^*=\u03b3^2/2, and \u03b8_1^*=\u221a(\u0394), \u03b8_2^*=B\u0305 + 2I  + 2P, \u03b8_3^*=B\u0305. \n        \n          *  Neural networks:  for K^\u03b7, R^\u03b7 and \u03d5^\u03b8, with time input. \n        \n        Compute for i=1,\u2026,4, \n        \u2202 K^\u03b7/\u2202\u03b7_i  =  ,   \u2202 R^\u03b7/\u2202\u03b7_i  =  \u2202_\u03b7_1 K^\u03b7(t)    =    -1/2[\u03b7_1(T-t)( [sinh(\u03b7_1(T-t))+\u03b7_2cosh(\u03b7_1(T-t))/cosh(\u03b7_1(T-t))+\u03b7_2sinh(\u03b7_1(T-t))]^2-1) \n               -  sinh(\u03b7_1(T-t))+\u03b7_2cosh(\u03b7_1(T-t))/cosh(\u03b7_1(T-t))+\u03b7_2sinh(\u03b7_1(T-t))], \n        \u2202_\u03b7_2K^\u03b7 (t)    =   \u03b7_1/2 [ cosh(\u03b7_1(T-t))+\u03b7_2sinh(\u03b7_1(T-t)) ]^2,   \u2202_\u03b7_3K^\u03b7 (t) =  -1/2,  \u2202_\u03b7_4K^\u03b7 (t) = 0, \n        \u2202_\u03b7_1R^\u03b7(t)    =   \u03b7_4 (T-t) sinh(\u03b7_1(T-t))+\u03b7_2cosh(\u03b7_1(T-t))/cosh(\u03b7_1(T-t))+\u03b7_2sinh(\u03b7_1(T-t))\n        \u2202_\u03b7_2R^\u03b7(t)    =   \u03b7_4sinh(\u03b7_1(T-t))/cosh(\u03b7_1(T-t))+\u03b7_2sinh(\u03b7_1(T-t)),   \u2202_\u03b7_3R^\u03b7(t) = -\u03b7_4(T-t), \n        \u2202_\u03b7_4R^\u03b7(t)    =   ln[ cosh (\u03b7_1(T-t)) + \u03b7_2  sinh (\u03b7_1(T-t)) ] -\u03b7_3(T-t),  \n        \n        and so  \u2202^\u03b7/\u2202\u03b7_i=\u2202 K^\u03b7/\u2202\u03b7_i(x-\u03bc\u0305)^2 + \u2202 R^\u03b7/\u2202\u03b7_i. \n        \n        \n        For the randomised policies, we use the following parametrisation\n        \u03c0^\u03b8(.|t,x,\u03bc)    =   (  \u03d5^\u03b8(t) (x -\u03bc\u0305); \u03bb),  \n          log p_\u03b8(t,x,\u03bc,a)     =     - 1/2log(2\u03c0\u03bb) - \n        |a - \u03d5^\u03b8(t)(x-\u03bc\u0305)|^2/2\u03bb. \n        \n        with parameter \u03b8=(\u03b8_1,\u03b8_2,\u03b8_3)\u2208_+^3, and \n        \u03d5^\u03b8(t)    =   \u03b8_3  - \u03b8_1 sinh(\u03b8_1(T-t))  + \u03b8_2  cosh (\u03b8_1(T-t))/cosh(\u03b8_1(T-t))  + \n        \u03b8_2  sinh (\u03b8_1(T-t)).\n        Compute \u2202\u03d5^\u03b8/\u2202\u03b8_i, and then \u2202log p_\u03b8/\u2202\u03b8_i  for i=1,\u2026,3.\n        \u2202_\u03b8_1\u03d5^\u03b8 (t)    =   \u03b8_1(T-t) [ (sinh(\u03b8_1(T-t))+\u03b8_2cosh(\u03b8_1(T-t))/cosh(\u03b8_1(T-t))+\u03b8_2sinh(\u03b8_1(T-t)))^2-1] \n                -  sinh(\u03b8_1(T-t))+\u03b8_2cosh(\u03b8_1(T-t))/cosh(\u03b8_1(T-t))+\u03b8_2sinh(\u03b8_1(T-t))\u2202_\u03b8_2\u03d5^\u03b8 (t)    =    -\u03b8_1/(cosh(\u03b8_1(T-t))+\u03b8_2sinh(\u03b8_1(T-t)))^2,  \u2202_\u03b8_3\u03d5^\u03b8 (t) = 1\n        \n        and\n        \u2207_\u03b8log p_\u03b8(t,x,\u03bc,a)    =   (a-\u03d5^\u03b8(t)(x-\u03bc\u0305))(x-\u03bc\u0305)/\u03bb\u2207_\u03b8\u03d5^\u03b8(t).\n        \n        \n         \n         \n        \n        We implement our actor-critic algorithms with a simulator of X for coefficients equal to\n        \n        T = 1,  \u03b3 = 1,  B\u0305 = - B = 0.6,    I=0.4,   P=Q=1,  \n        \n        \n        \n        \n         The simulator for  X is  based on the real mean-field model:\n        X\u0323_t    =   ( B\u0305([X_t] -  X_t) + \u03b1_t ) \u1e6d + \u03b3\u1e88_t. \n        \n        Since \u03b1\u223c\u03c0^\u03b8, we note that [\u03b1_t]=\u03d5^\u03b8(t)([X_t] - [\u03bc\u0305_t])=0.  We deduce that under such \u03b1, [X_t]=0, hence [X_t]=[X_0].  From the above mean-field dynamics of X, we deduce that \n        \n        X_t_k+1 - [X_0]    =     e^-B\u0305\u0394 t (X_t_k - [X_0]) +   \u03b1_t_k(   1 - e^-B\u0305\u0394 t/B\u0305) + \u03b3\u222b_t_k^t_k+1 e^-B\u0305(t_k+1 - s)\u1e88_s \n           \u2243    e^B\u0394 t (X_t_k - [X_0]) +   \u03b1_t_k(   1 - e^-B\u0305\u0394 t/B\u0305) + \u03b3 e^-B\u0305\u0394 t\u0394 W_t_k. \n        \n        The cost is simulated according to\n        \n        f_t_k   =    Q(X_t_k - [X_0])^2 + 1/2\u03b1_t_k^2 + 2 \u03b1_t_k I (X_t_k - [X_0]),    g_T   =   P (X_T - [X_0])^2. \n        \n        \n        \n        \n        We first present the numerical results of our offline Algorithm <ref> when using the exact parametrisation  (<ref>).  The derivatives  w.r.t. to  \u03b7 of K^\u03b7, R^\u03b7, hence of ^\u03b7, as well as the derivative w.r.t. \u03b8 of log p_\u03b8 have explicit analytic expressions that are implemented in \n        the updating rule of the actor-critic algorithm. \n        \n        Here we used the following parameters:  the learning rates (\u03c1_S, \u03c1_E, \u03c1_G) were taken as \u03c1_S=0.2 constant, \u03c1_E = (0.2,0.2,0.1,0.5) until iteration 500, then (2.0,1.0,2.0,1.0) until iteration 1500 and then (0.5,0.5,0.5,0.5), whereas \u03c1_G = (0.1,0.1,0.1) until iteration 1500, then (0.05,0.05,0.05); \n        \n        \n        \n        \n        \n         \n        Here we used the following parameters: \n        \u03bc_t_k was initialized at 0; the number of episodes was N=2100; the time horizon was T=1 and the time step \u0394 t= 0.02. The values of the model parameters were as described above.\n        The learning rates (\u03c1_S, \u03c1_E, \u03c1_G) and \u03bb were taken as \u03c1_S=0.2 constant, and at iteration i, \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n            \u03c1_E(i) = \n                \n                        (0.01,0.1,0.01,0.2)  if  i \u2264 500\n                        \n            \n                        (0.1,0.1,0.1,0.1)  if  500 < i \u2264 21000\n                  \u03c1_G(i) = \n                \n                        (0.03,0.05,0.03)  if  i \u2264 7000\n                        \n            \n                        (0.01,0.01,0.01)  if  7000 < i \u2264 10000\n                        \n            \n                        (0.005,0.01,0.005)   if  10000 < i \u2264 14000\n                        \n                        \n                        \n            \n                        (0.002,0.002,0.002)    if  17000 < i \u2264 21000\n        \n        and\n        \n            \u03bb(i) = \n                \n                    0.1  if  i \u2264 8000, \n            \n                    0.01  if  8000 < i \u2264 14000, \n            \n                    0.001  if  14000 < i   \u2264 21000\n        \n        Moreover, after i=14000 iterations, we also increase the size of the minibatch from 20 to 40. \n        In Table <ref>, we give the learnt parameters for the critic and actor functions, to be compared wih \n        the exact value of the  parameters.  \n        \n        \n        \n        \n         In Figure\u00a0<ref>, we see that, even though the parameters \u03b7 and \u03b8 (shown with full lines) are slightly different from the true optimal values (shown in dashed lines), the functions K, R and \u03d5 are matched almost perfectly. \n        \n        \n        \n         \n        We also display one realization of the control and of the cost. These are based on evaluating the control and the cumulative cost along one trajectory of the state. We first simulate 10^4 realizations of a Brownian motion. Based on this, we generate trajectories for one 10^4 population of agents using the learnt control and one population of 10^4 agents using the optimal control. For the population that uses the learnt control, the control is given by the mean of the actor, namely, \u03d5^\u03b8(t) (x -\u03bc\u0305). In the dynamics, the cost and the control, the mean field term is replaced by the empirical mean of the corresponding population at the current time. We can see that the trajectories of control (resp. cost) are very similar. \n        \n        \n         \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n         \n        \n        \n         Next, we present in Figure <ref> and  Figure <ref>  the numerical results of our online Algorithm <ref> when using neural networks. In this case, the derivatives  w.r.t. to  \u03b7 of K^\u03b7, R^\u03b7, hence of ^\u03b7, as well as the derivative w.r.t. \u03b8 of log p_\u03b8 \n         are computed by automatic differentiation. \n         We use neural networks with 3 hidden layers, 10 neurons per layer and tanh activation functions. We take n=30, N=15000 iterations, batch size 500 (10000 for the law estimation in the simulator), constant learning rates 10^-3, except \u03c9_S = 1.  \n          We change \u03bb along episodes: \u03bb=0.1 for the first 3334 ones, then 0.01 for the next 3333 ones, then 0.001 until the end.\n         \n         \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n          \u00a7.\u00a7.\u00a7 Programmation\n        \n        Under the LQ MFC conditions above, we define \n        \n            J(\u03b8)   =\ud835\udd3c_\u03b1\u223c\u03c0^\u03b8[\u222b_0^T(f(X_s,_X_s,\u03b1_s)+\u03bblog (p_s^\u03b8(\u03b1_s)))ds+g(X_T,_X_T)]\n               =\ud835\udd3c_\u03b1\u223c\u03c0^\u03b8[\u222b_0^T (QX_s^2+Q\u0305X\u0305_s^2+\u03b1_s^2/2+2I\u03b1_s X_s+2I\u0305\u03b1_sX\u0305_s+\u03bblog(p_s^\u03b8(\u03b1_s)))ds+PX_T^2+P\u0305X\u0305_T^2]\n               =\ud835\udd3c_\u03b1\u223c\u03c0^\u03b8[\u222b_0^T (Q(X_s^2-X\u0305_s^2)+\u03b1_s^2/2+2I\u03b1_s (X_s-X\u0305_s)+\u03bblog(p_s^\u03b8(\u03b1_s)))ds+P(X_T^2-X\u0305_T^2)]\n        \n        where the dynamic of the controlled process {X_t}:\n            dX_s=(B(X_s-X\u0305_s)+\u03b1_s)ds+\u03b3 dW_s\n        \n        then \n        \n            dX\u0305_s=\u03b1\u0305_sds\n        \n        and p_t^\u03b8(a) is the density function of \u03c0^\u03b8(\u00b7|t,X_t,_X_t)=\ud835\udca9(\u03d5^\u03b8(t)(X_t-X\u0305_t);\u03bb) defined above.\n        Hence\n        \n            J(\u03b8)=[\u222b_0^T (Q+(\u03d5^\u03b8(s))^2/2+2I\u03d5^\u03b8(s))(X_s-X\u0305_s)^2ds+P(X_T-X\u0305_T)^2-\u03bb T/2log(2\u03c0\u03bb)]\n        \n            =\u222b_0^T (Q+(\u03d5^\u03b8(s))^2/2+2I\u03d5^\u03b8(s))Var(X_s)ds+PVar(X_T)-\u03bb T/2log(2\u03c0\u03bb)\n        \n        Cause \n        \n            d(X_s-X\u0305_s)=(B(X_s-X\u0305_s)+(\u03b1_s-\u03b1\u0305_s))ds+\u03b3 dW_s\n        \n        we have\n        \n            Var(X_s)=[(X_s-X\u0305_s)^2]=\u03b3^2 s\n        \n        Therefore\n        \n            J(\u03b8)=\u222b_0^T\u03b3^2(Q+(\u03d5^\u03b8(s))^2/2+2I\u03d5^\u03b8(s))sds+P\u03b3^2T-\u03bb T/2log(2\u03c0\u03bb)\n        \n        with \n        \n            \u2202_\u03b8_iJ(\u03b8)=\u222b_0^T \u03b3^2(\u03d5^\u03b8(s)\u2202_\u03b8_i\u03d5^\u03b8(s)+2I\u2202\n            _\u03b8_i\u03d5^\u03b8(s))sds\n        \n        \n        Then, we compute the gradient of {\u03d5^\u03b8}:\n            \u2202_\u03b8_1\u03d5^\u03b8 (t)=\u03b8_1(T-t)((sinh(\u03b8_1(T-t))+\u03b8_2cosh(\u03b8_1(T-t))/cosh(\u03b8_1(T-t))+\u03b8_2sinh(\u03b8_1(T-t)))^2-1)-sinh(\u03b8_1(T-t))+\u03b8_2cosh(\u03b8_1(T-t))/cosh(\u03b8_1(T-t))+\u03b8_2sinh(\u03b8_1(T-t))\n        \n            \u2202_\u03b8_2\u03d5^\u03b8 (t)=-\u03b8_1/(cosh(\u03b8_1(T-t))+\u03b8_2sinh(\u03b8_1(T-t)))^2\n        \n            \u2202_\u03b8_3\u03d5^\u03b8 (t)=1\n        \n        \n        \n        \n        \n        Finally, we test  in Table <ref> the learnt policies \n        from the exact and NN parametrisation by computing the associated initial expected social costs.  \n        We simulate 10 populations, each consisting of 10^4 agents. All the agents use the control function with the parameters learnt by the algorithm. For the dynamics, the cost and the control, the mean field term is replaced by the empirical mean of the corresponding population at the present time step. For each population, we compute the social cost. We then average over the 10 populations in order to get a Monte Carlo estimate of the social cost. We report in the table the value of this average social cost, the standard deviation over the 10 populations, and the relative error between the average social cost and the optimal cost computed by the formula K^\u03b7^*_0  Var(X_0) +R^\u03b7^*_0 with the optimal parameter \u03b7^*. \n        \n        \n        \n        \n         \n        \n        \n        \n        \n        \n        \n         \u00a7.\u00a7 Example 2: mean-variance problem\n        \n        \n        \n        We consider the celebrated Markowitz portfolio selection problem: minimize over the amount \u03b1  invested  in a stock driven by a Black-Scholes model with rate of return C, and constant volatility F>0, the \n        risk of the terminal wealth X_T measured by the mean-variance criterion: \n        \n        P  Var(X_T) - [X_T],\n        \n        where P>0 is the risk aversion of the investor. This  is a particular case of  the LQ MFC with \n        \n        B = B\u0305 = 0,  \u03b3 = D = D\u0305 = 0,   \n        \n        Q=Q\u0305=N=I=I\u0305=M=H =0,    P + P\u0305 = 0,   L=-1/2. \n        \n        \n        In this case, the solution to the system of ODEs (<ref>)  gives the analytic expression: \n        \n        K(t)    =    P exp( - C^2/F^2(T-t) ),    Y(t)   =   -1/2, \n        \n        R(t)     =    1/4P[ 1 - exp(  C^2/F^2(T-t) )  ] -\u03bb(T-t)/2log(\u03c0\u03bb/PF^2)-\u03bb C^2/4F^2(T-t)^2,  \n        \u039b=0, while the optimal randomised  policy is given by \n        \n            \u03c0\u0302(.|t,x,\u03bc)     =  ( \u03d5(t) (x  - \u03bc\u0305) + \u03d5_3(t) ;  \u03bb\u03d1(t)  ) \n                 \u03d5(t)   =    - C/F^2,   \u03d5_3^\u03b8(t)   =  C/2F^2 P e^C^2/F^2(T-t),   \u03d1(t)   =  1/2F^2 P  e^C^2/F^2(T-t).\n         \n        \n        In  a model-free setting,  the volatility F>0 of the stock price is unknown, while the risk aversion parameter P>0 is usually known and  fixed by the investor, and  we assume here that the rate of return C>0 is known. \n        In view of the above expressions, we shall use  critic function as\n        ^\u03b7(t,x,\u03bc)    =    K^\u03b7(t)  (x - \u03bc\u0305)^2  + R^\u03b7(t), \n        \n        for some parametric functions K^\u03b7 and R^\u03b7 on [0,T] with parameters \u03b7, and actor functions as \n        \u03c0_\u03b8(.|t,x,\u03bc)    =   (  \u03d5^\u03b8(t) (x -\u03bc\u0305) + \u03d5_3^\u03b8; \u03bb\u03d1^\u03b8(t)  ),  \n          log p_\u03b8(t,x,\u03bc,a)     =     - 1/2log(2\u03c0\u03bb\u03d1^\u03b8(t)) - \n        |a - \u03d5^\u03b8(t)(x-\u03bc\u0305)- \u03d5_3^\u03b8(t) |^2/2\u03bb,\n        \n        for some parametric functions \u03d5^\u03b8, \u03d5_3^\u03b8, and \u03d1^\u03b8>0 on [0,T] with parameter \u03b8.  Given such  family of parametric actor/critic functions, we have \n         _\u03b8[^\u03b7](t,x,\u03bc)      =    \n        -  2  C K^\u03b7(t)(x-\u03bc\u0305)  \u2207_\u03b8\u03d5_3^\u03b8(t). \n         \n        We shall test with two choices of parametric functions: \n        \n          *  Exact parametrisation:\n            K^\u03b7(t)    =     P e^-\u03b7(T-t), \n            \n            R^\u03b7(t)   =  1/4P[  1 - e^\u03b7(T-t)]-\u03bb(T-t)/2log(\u03c0\u03b7\u03bb/C^2P)-\u03bb\u03b7/4(T-t)^2 \n            \u03d5^\u03b8(t)   =      -C \u03b8,   \u03d5_3^\u03b8(t)   =  C\u03b8/2P  e^C^2\u03b8(T-t),  \u03d1^\u03b8(t)   =  \u03b8/2P e^C^2\u03b8(T-t),\n         \n        with parameters \u03b7\u2208(0,\u221e), \u03b8\u2208, so that the optimal solution in the model-based case corresponds to \n        \u03b7^*=C^2/F^2,  and \u03b8^*=1/F^2. \n        \n          *  Neural networks:  for K^\u03b7, R^\u03b7,  \u03d5^\u03b8, \u03d5_3^\u03b8 and \u03d1^\u03b8  with time input. \n        \n        \n        This suggests to use a parametrisation of the performance value function as \n        ^\u03b7(t,x,\u03bc)    =    K^\u03b7(t)(x- \u03bc\u0305)^2   - x +  R^\u03b7(t),\n        \n        with \n        \n        \u03b7\u2208(0,\u221e) and \n        \n        K^\u03b7(t)    =    P e^-\u03b7(T-t),    R^\u03b7(t)   =  1/4P[ - e^\u03b7(T-t) + 1]-\u03bb(T-t)/2log(\u03c0\u03b7\u03bb/C^2P)-\u03bb\u03b7/4(T-t)^2,  \n        \n        \n        and a parametrisation for the randomised policies as \n        \u03c0^\u03b8(.|t,x,\u03bc)    =   (  - C \u03b8 (x -\u03bc\u0305-  1/2P  e^C^2 \u03b8(T-t) );  \u03bb\u03b8/2P   e^C^2\u03b8(T-t)), \n        \n         log p_\u03b8(t,x,\u03bc,a)    =    - 1/2log(2\u03c0\u03bb\u03b8/2P) - 1/2 C^2  \u03b8(T-t)  \n                 -  | a + C \u03b8 (x -\u03bc\u0305-  1/2P  e^C^2 \u03b8(T-t) ) |^2/\u03bb\u03b8 P  e^- C^2 \u03b8(T-t), \n        \n        \n        \n        with parameter \u03b8\u2208(0,\u221e). \n        \n        This corresponds to \u03d5_1^\u03b8=-\u03d5_2^\u03b8 (=-C\u03b8), \n        \u03d5_3^\u03b8(t)    =   C\u03b8/2P  e^C^2\u03b8(T-t),\n        \n        \n        \n        \n        \n         \n         \n         \n         \n          \n        \n         Remark:  When using neural networks approximation, we take\n         \u03c0^\u03b8(.|t,x,\u03bc)    =   ( \u03d5^\u03b8(t) (x-\u03bc\u0305) + \u03d5_3^\u03b8(t); \u03bb\u03a3^\u03b8(t) ),   \n         log p_\u03b8(t,x,\u03bc,a)    =    - 1/2log(2\u03c0\u03bb\u03a3^\u03b8(t) ) -   | a -  \u03d5^\u03b8(t) (x -\u03bc\u0305) -  \u03d5_3^\u03b8(t)  |^2/2\u03bb\u03a3^\u03b8(t).\n         \n         We also have \n         _\u03b8[^\u03b7](t,x,\u03bc)      =    \n        -  2  C K_\u03b7(t)(x-\u03bc\u0305)  \u2207_\u03b8\u03d5_3^\u03b8(t). \n         \n        Compare the learned NN \u03d5^\u03b8, \u03d5_3^\u03b8 and \u03a3^\u03b8 with the true values:\n         \u03d5^\u03b8  =   - C/F^2,   \u03d5_3^\u03b8(t)   =  C/2PF^2exp(  C^2/F^2(T-t) ),   \u03a3^\u03b8(t)   =  1/2PF^2exp(  C^2/F^2(T-t) ),\n         Compute \u2207_\u03b7 K^\u03b7, \u2207 R^\u03b7, then \u2207_\u03b7^\u03b7, \u2207_\u03b8log p_\u03b8, and \n        implement and compare to the results in <cit.>, <cit.>.  \n        \n        \n        \n        \u2207_\u03b7 K^\u03b7(t) = -P(T-t)e^-\u03b7(T-t),  \u2207_\u03b7 R^\u03b7(t) =  - T-t/4Pe^\u03b7(T-t)-\u03bb(T-t)/2\u03b7-\u03bb/4(T-t)^2\n        \n        and so \n        \n            \u2207_\u03b7^\u03b7(t)    = \u2207_\u03b7 K^\u03b7(t)(x-\u03bc\u0305)^2+\u2207_\u03b7 R^\u03b7(t)\n                =  -P(T-t)e^-\u03b7(T-t)(x-\u03bc\u0305)^2  - T-t/4Pe^\u03b7(T-t)-\u03bb(T-t)/2\u03b7-\u03bb/4(T-t)^2\n        \n        Moreover, \n        \u2207_\u03b8log p_\u03b8(t,x,\u03bc,a)    =     -1/2\u03b8-1/2C^2(T-t)-2(a+CF_\u03b8(t))C\u03b8\u2207_\u03b8 F_\u03b8(t) -(a+CF_\u03b8(t))^2/\u03bb\u03b8^2Pe^-C^2\u03b8(T-t)\n                   +  (a+CF_\u03b8(t))^2/\u03bb\u03b8PC^2(T-t)e^-C^2\u03b8(T-t)\n        where \n        \n        F_\u03b8(t)    =   \u03b8(x-\u03bc\u0305-1/2Pe^C^2\u03b8(T-t)),   \u2207_\u03b8 F_\u03b8(t)   =   x-\u03bc\u0305-1+\u03b8 C^2(T-t)/2Pe^C^2\u03b8(T-t).\n        \n        \n          \n         We implement our algorithms with a simulator of X for different values of (C,F).  The simulator of X is based on the time discretization of the wealth process\n        \n        \n        \n        \n            X_t_k+1   =    X_t_k + C \u03b1_t_k\u0394 t + F \u03b1_t_k\u0394 W_t_k,    k=0,\u2026,n-1,\n        \n        with  \u03b1\u223c\u03c0_\u03b8. Since  [\u03b1_t]=\u03d5_3^\u03b8(t), we have \n        _\u03b1\u223c\u03c0_\u03b8[X_T]    =    X_0  + C \u2211_k=0^n-1\u03d5_3^\u03b8(t_k) \u0394 t,  \n        \n        and the terminal cost is computed with \n        \n        g_T     =     P(X_T - _\u03b1\u223c\u03c0_\u03b8[X_T])^2 - X_T.\n        \n        The Sharpe ratio associated to a strategy \u03b1\u223c\u03c0_\u03b8 is given by \n        \n        S^\u03b8   =   _\u03b1\u223c\u03c0_\u03b8[X_T] - X_0/\u221a( Var_\u03b1\u223c\u03c0_\u03b8(X_T))  =   C \u2211_k=0^n-1\u03d5_3^\u03b8(t_k) \u0394 t/\u221a( Var_\u03b1\u223c\u03c0_\u03b8(X_T)). \n        \n        The variance of the wealth  can be estimated by Monte-Carlo sampling X_T when \u03b1\u223c\u03c0_\u03b8, or  can be explicitly computed as follows:\n        \n        by setting Y_t=X_t - _\u03b1\u223c\u03c0_\u03b8[X_t], we get by It\u00f4's formula: \n        _\u03b1\u223c\u03c0_\u03b8[Y_t^2]    =   ( 2 C _\u03b1\u223c\u03c0_\u03b8[Y_t\u03b1_t] + F^2 [\u03b1_t^2] ) \u1e6d.  \n        \n        Since \u03b1_t\u223c\u03c0^\u03b8(.|t,X_t,_\u03b1\u223c\u03c0_\u03b8[X_t])=( \u03d5^\u03b8(t) Y_t  + \u03d5_3^\u03b8(t); \u03bb\u03d1^\u03b8(t) ), we have\n        [\u03b1_t^2]    =   \u03bb\u03d1^\u03b8(t) +  |\u03d5_3^\u03b8(t)|^2 + |\u03d5^\u03b8(t)|^2 [Y_t^2],     \n        _\u03b1\u223c\u03c0_\u03b8[Y_t \u03b1_t]    =   [Y_t ( \u03d5^\u03b8(t) Y_t  + \u03d5_3^\u03b8(t) ) ]   =  \u03d5^\u03b8(t) [Y_t^2], \n        \n        and so \n        _\u03b1\u223c\u03c0_\u03b8[Y_t^2]    =   [   \u03d5^\u03b8(t)(2 C + F^2 \u03d5^\u03b8(t) )  [Y_t^2] + F^2(  \u03bb\u03d1^\u03b8(t) +  |\u03d5_3^\u03b8(t)|^2 ) ] \u1e6d.  \n        \n        We deduce that\n        \n            Var_\u03b1\u223c\u03c0_\u03b8(X_T)    =    e^\u222b_0^T \u03d5^\u03b8(t)(2 C + F^2 \u03d5^\u03b8(t) ) \u1e6d\u222b_0^T  e^- \u222b_0^t \u03d5^\u03b8(s)(2 C + F^2 \u03d5^\u03b8(s)) \u1e63  F^2(  \u03bb\u03d1^\u03b8(t) +  |\u03d5_3^\u03b8(t)|^2 ) \u1e6d.\n         \n        We shall compare   the Sharpe ratio when following the learned  strategy \u03b1\u223c\u03c0_\u03b8, and the optimal one which is given by \n        \n        S^*    =   exp(C^2/F^2T)  -1/\u221a(2 \u03bb P T +  exp(C^2/F^2T)  -1 ). \n        \n        \n        \n        \n        We first  present the numerical results of our offline Algorithm <ref> when using the exact parametrisation (<ref>). \n        The derivatives  w.r.t. to  \u03b7 of K^\u03b7, R^\u03b7, hence of ^\u03b7, as well as the derivative w.r.t. \u03b8 of log p_\u03b8, and _\u03b8[^\u03b7]  have explicit analytic expressions that are implemented in \n        the updating rule of the actor-critic algorithm.  Moreover, the Sharpe ratio when \u03b1\u223c\u03c0_\u03b8 is analytically expressed as \n        \n        S^\u03b8   =   e^C^2\u03b8 T - 1 /2P/\u221a(F^2\u03bb(1-e^C^2\u03b8 T(F^2\u03b8 -1))/2C^2P(1-F^2\u03b8)+e^C^2 F^2|\u03b8|^2 T-1/4P^2 ) . \n        \n        \n        \n        Here we used the following parameters:  the learning rates (\u03c1_S, \u03c1_E, \u03c1_G) were taken as (0.3,0.5,0.5) until iteration 13500, and then (\u03c1_E, \u03c1_G) were decreased to (0.1,0.1); moreover, to avoid instabilities we have truncated the norm of the updates to 0.1 and 0.5 respectively; \u03bc_t_k was initialized at 1; the number of episodes was N=2500; the time horizon was T=0.2 and the time step \u0394 t= 0.01. The values of the model parameters were as described above with C=0.3, F=0.1, P=1. In Figure\u00a0<ref>, we see that, even though the parameters \u03b7 and \u03b8 (shown with full lines) are slightly different from the true optimal values (shown in dashed lines), the functions K, R, \u03d5 and \u03d5_3 are matched almost perfectly.\n        \n        \n        Mathieu: TODO: present the table. Vary \u03bb.   \n        \n        \n        \n        \n        Next, we present in Figure <ref> and Figure <ref> the numerical results of our online Algorithm <ref> when using neural networks. In this case, the derivatives  w.r.t. to  \u03b7 of K^\u03b7, R^\u03b7, hence of ^\u03b7, as well as the derivative w.r.t. \u03b8 of log p_\u03b8 \n         are computed by automatic differentiation. \n         We use neural networks with 3 hidden layers, with 10 neurons per layer and tanh activation functions. n=30, N=10000 iterations, batch size 300 (10000 for the law estimation in the simulator), constant learning rates 10^-3, except \u03c9_S = 1. \n        We change \u03bb along episodes: \u03bb=0.1 for the first 3334 ones, then 0.01 for the first 3333 ones, then 0.001 until the end.\n         \n         \n        \n         \u00a7.\u00a7 Example 2: optimal trading\n        \n        \n        \n        We consider an optimal trading problem where the inventory is governed by \n        X\u0323_t    =   \u03b1_t \u1e6d + \u03b3\u1e88_t, \n        \n        and we aim to minimize over randomised trading rate \u03b1\u223c\u03c0 the cost functional\n        [ \u222b_0^T \u03b1_t^2 + 2 H \u03b1_t  - \u03bb(\u03c0_t) \u1e6d + P  Var(X_T) ]. \n        \n        where \u03b3>0, H>0 is the transaction price per trading,  P>0 is a risk aversion parameter, and \u03bb>0 is the temperature parameter.  \n        This model fits into the LQ framework, and the solution to the system of ODEs (<ref>) is given by \n        \n        \n        \n        \n        \n        K(t)     =  P/1 + P(T-t),           R(t)   =  \u03b3^2 log (1 + P(T-t)) - (H^2 + \u03bb/2log( \u03c0\u03bb) ) (T-t),\n        \u039b=Y\u22610, \n        while the optimal randomised policy is given by \n        \u03c0\u0302(\u2022|t,x,\u03bc)    \u223c   (  - K(t) (x - \u03bc\u0305)  - H; \u03bb/2). \n        \n        \n        \n        In a RL setting, the coefficients \u03c3, H and P are unknown, and we use critic function as\n        ^\u03b7(t,x,\u03bc)    =    K^\u03b7(t)  (x - \u03bc\u0305)^2  + R^\u03b7(t), \n        \n        for some parametric functions K^\u03b7 and R^\u03b7 on [0,T] with parameters \u03b7, and actor functions as \n        \u03c0_\u03b8(.|t,x,\u03bc)    =   (  \u03d5^\u03b8(t) (x -\u03bc\u0305) + \u03d5_3^\u03b8(t); \u03bb/2),  \n          log p_\u03b8(t,x,\u03bc,a)     =     - 1/2log(\u03c0\u03bb) - \n        |a - \u03d5^\u03b8(t)(x-\u03bc\u0305)- \u03d5_3^\u03b8(t) |^2/\u03bb,\n        \n        for some parametric functions \u03d5^\u03b8, \u03d5_3^\u03b8  on [0,T] with parameter \u03b8.  Given such  family of parametric actor/critic functions, we have \n         _\u03b8[^\u03b7](t,x,\u03bc)      =    \n        -  2  C K^\u03b7(t)(x-\u03bc\u0305)  \u2207_\u03b8\u03d5_3^\u03b8(t). \n         \n        We shall test with two choices of parametric functions: \n        \n          *  Exact parametrisation:\n            K^\u03b7(t)    =  \u03b7_1/1 + \u03b7_1(T-t)\n            \n            R^\u03b7(t)   =  \u03b7_2  log (1 + \u03b7_1(T-t)) - ( \u03b7_3  + \u03bb/2log( \u03c0\u03bb) ) (T-t)\n            \u03d5^\u03b8(t)   =      - \u03b8_1/1 + \u03b8_1(T-t),   \u03d5_3^\u03b8(t)   =    -  \u03b8_2,\n         \n        with parameters \u03b7=(\u03b7_1,\u03b7_2,\u03b7_3)\u2208(0,\u221e)^3, \u03b8=(\u03b8_1,\u03b8_2)\u2208(0,\u221e)^2, so that the optimal solution in the model-based case corresponds to \n        (\u03b7_1^*,\u03b7_2^*,\u03b7_3^*)=(P,\u03b3^2,H^2),  and (\u03b8_1^*,\u03b8_2^*)=(P,H).    \n        \n          *  Neural networks:  for K^\u03b7, R^\u03b7,  \u03d5^\u03b8, and \u03d5_3^\u03b8  with time input.  Actually, we take for \u03d5_3^\u03b8 a constant function. \n        \n        \n         \n         \n        \n        \n        \n        \n        We first  present the numerical results of our offline Algorithm <ref> when using the exact parametrisation (<ref>). \n        The derivatives  w.r.t. to  \u03b7 of K^\u03b7, R^\u03b7, hence of ^\u03b7, as well as the derivative w.r.t. \u03b8 of log p_\u03b8, and _\u03b8[^\u03b7]  have explicit analytic expressions that are implemented in \n        the updating rule of the actor-critic algorithm. \n        Here we used the following parameters: the learning rates (\u03c1_S, \u03c1_E, \u03c1_G) and \u03bb were taken as \u03c1_S=0.2 constant,\n         and at iteration i,\n         \n            \u03c1_E(i) = \n                \n                        (0.05,0.05,0.05)  if  i \u2264 8000\n                        \n            \n                        (0.05,0.05,0.01)  if  8000 < i \u2264 20000\n                  \u03c1_G(i) = \n                \n                        (0.005,0.005)  if  i \u2264 8000\n                        \n            \n                        (0.001,0.001)  if  8000 < i \u2264 13000\n                        \n            \n                        (0.0005,0.0005)  if  8000 < i \u2264 13000\n        \n        and\n        \n            \u03bb(i) = \n                \n                    0.1  if  i \u2264 8000, \n            \n                    0.01  if  8000 < i \u2264 13000 \n            \n                    0.001  if  13000 < i \u2264 20000\n        \u03bc_t_k was initialized at 0; the number of episodes was N=20000; the time horizon was T=1 and the time step \u0394 t= 0.02. The values of the model parameters are: P=3, H=2, \u03b3=1, and X_0 \u223c(1,1).\n        \n        \n        In Table <ref>, we give the learnt parameters for the critic and actor function to be compared with the exact values, when using the learnt policy with learnt empirical distribution from the algorithm.  \n        \n        \n        \n        \n        \n         \n        In Figure\u00a0<ref>, we see that the parameters and, hence, the functions K, R and \u03d5 are matched almost perfectly. We also display one realization of the control and of the cost. These are based on evaluating the control and the cumulative cost along one trajectory of the state. We first simulate 10^4 realizations of a Brownian motion. Based on this, we generate trajectories for one 10^4 population of agents using the learnt control and one population of 10^4 agents using the optimal control. For the population that uses the learnt control, the control is given by the mean of the actor, namely, \u03d5^\u03b8(t) (x -\u03bc\u0305) + \u03d5_3^\u03b8(t). In the dynamics, the cost and the control, the mean field term is replaced by the empirical mean of the the corresponding population at the current time. We can see that the trajectories of control (resp. cost) are very similar. \n        \n        \n        \n        \n        \n        Next, we present in Figure <ref> and Figure <ref> the numerical results of our online Algorithm <ref> when using neural networks. In this case, the derivatives  w.r.t. to  \u03b7 of K^\u03b7, R^\u03b7, hence of ^\u03b7, as well as the derivative w.r.t. \u03b8 of log p_\u03b8 \n         are computed by automatic differentiation. \n        We use neural networks with 3 hidden layers,  10 neurons per layer and tanh activation functions. We take n=30, N=15000 iterations, batch size 300 (10000 for the law estimation in the simulator), constant learning rates 10^-3, except \u03c9_S = 1. \n        Again, we change \u03bb along episodes: \u03bb=0.1 for the second 3334 ones, then 0.01 for the next 3333 ones, then 0.001 until the end.\n        \n        \n         \n        \n        \n        Finally, we test in Table <ref> the learnt policies from the exact and NN parametrisation by computing the associated initial expected social costs. \n        We simulate 10 populations, each consisting of 10^4 agents. All the agents use the control function with the parameters learnt by the algorithm. For the dynamics, the cost and the control, the mean field term is replaced by the empirical mean of the corresponding population at the present time step. For each population, we compute the social cost. We then average over the 10 populations in order to get a Monte Carlo estimate of the social cost. We report in the table the value of this average social cost, the standard deviation over the 10 populations, and the relative error between the average social cost and the optimal cost computed by the formula K^\u03b7^*_0  Var(X_0) +R^\u03b7^*_0 with the optimal parameter \u03b7^*. \n        \n        \n        \n        \n        \n         \n         \n        \n        \n        \n        \n        \n        \n        \n         \u00a7.\u00a7 A min/max linear quadratic mean-field control problem\n        \n        \n        We consider a mean-field model in which the dynamics is linear and the running cost is quadratic in the position, the control and the expectation of the position. The terminal cost  encourages to be close to one of two targets. This type of model is inspired by the min-LQG problem of\u00a0<cit.>. More precisely, the controlled McKean-Vlasov dynamics  on  is given by \n        \n            X\u0323_t    =  [  B X_t + B\u0305[X_t] +   \u03b1_t ]   \u1e6d +  \u03b3 \u1e88_t , \n               X_0 \u223c\u03bc_0,\n        \n        for some constants B,B\u0305\u2208, and a control \u03b1 valued in A=, \n        \n        \n        \n         \n        \n        \n        \n        \n        while  the running and terminal costs are given by \n        \n            f(x,\u03bc,a)    =  1/2( Q x^2 + Q\u0305 (x -  \u03bc\u0305)^2 + N a^2 ), \n                      g(x)   =  min{  |x - \u03b6_1 |^2, | x - \u03b6_2 |^2 },\n         \n        for some non-negative constants Q, Q\u0305, N, and two real numbers \u03b6_1 and \u03b6_2.  \n        \n        \n        For this model, there is no explicit solution, and we even don't know the structural form of the optimal value function and optimal randomised policy.  In a RL setting, and as suggested in Remark <ref>, we shall use critic functions in the cylindrical  form\n        ^\u03b7(t,x,\u03bc)    =   \u03a8_\u03b7(t,x,<\u03c6_\u03b7,\u03bc>),    (t,x,\u03bc) \u2208 [0,T]\u00d7\u00d7_2(), \n        \n        with feedforward neural networks functions \u03a8_\u03b7(t,x,y) from  [0,T]\u00d7\u00d7^k into , and \u03c6_\u03b7(x) from   into ^k, \n        and actor functions as \n        \u03c0_\u03b8(.|t,x,\u03bc)    =   (   _\u03b8(t,x,\u03bc) ; \u03bb),  \n          log p_\u03b8(t,x,\u03bc,a)     =     - 1/2log(2\u03c0\u03bb) - \n        |a - _\u03b8(t,x,\u03bc) |^2/2\u03bb,\n        \n        with cylindrical neural network functions _\u03b8 from [0,T]\u00d7\u00d7_2() into . In this case, we have \n        \u2207_\u03b8 b_\u03b8(t,x,\u03bc)   =  \u2207_\u03b8_\u03b8(t,x,\u03bc),        \u2207_\u03b8\u03a3_\u03b8(t,x,\u03bc)   =   0, \n        \u2202_\u03bc^\u03b7(t,x,\u03bc)(.)    =     D_y \u03a8_\u03b7(t,x,<\u03c6_\u03b7,\u03bc>) \u00b7 D_x \u03c6_\u03b7(.),        \n        and so \n        _\u03b8[^\u03b7](t,x,\u03bc)    =   _\u03be\u223c\u03bc[ \u2207_\u03b8_\u03b8(t,\u03be,\u03bc)   D_y \u03a8_\u03b7(t,x,<\u03c6_\u03b7,\u03bc>) \u00b7 D_x \u03c6_\u03b7(\u03be) ]. \n        Implement such _\u03b8[^\u03b7](t,x,\u03bc) in the algorithm,  and simulate X with the following parameters as in our previous DeepSet paper:\n         \n         B = B\u0305 = 0,    Q = 0,  Q\u0305 = N = 1,    T = 0.5,\n         X_0\u223c(x_0,\u03d1_0^2) with \n         \n          * \u03b3=0.3, x_0=1, \u03d1_0=0.2\n          * \u03b3=0.3, x_0=0.625, \u03d1_0=\u221a(0.2)\n          * \u03b3=0.5, x_0=0.625, \u03d1_0=\u221a(0.2)Faire les m\u00eame graphes que pour les deux pr\u00e9c\u00e9dents exemples\n        \n        \u00a7 SOME EXTENSIONS AND VARIATIONS\n        \n        \n         \u00a7.\u00a7 Infinite horizon\n        \n        \n        \n        Given a (stationary) randomised policy  \u03c0(.|x,\u03bc) with density a\u2208A\u21a6p(x,\u03bc,a) on A, the performance value function to the MFC on infinite horizon with entropy regularisation  is formulated as \n        \n        J(x,\u03bc;\u03c0)    =   _\u03b1\u223c\u03c0[ \u222b_0^\u221e e^-\u03b2 t(  f(X_t^x,\u03be,_X_t^\u03be,\u03b1_t) - \u03bb p(X_t^x,\u03be,_X_t^\u03be,\u03b1_t) )  \u1e6d], \n        \n        where \u03b2>0 is a discount factor. \n        \n        We are then  given a family of randomised policies (x,\u03bc)\u21a6\u03c0_\u03b8(\u1ea1|x,\u03bc)=p_\u03b8(x,\u03bc)\u03bd(\u1ea1), with parameter \u03b8,  a family of  functions (x,\u03bc)\u21a6^\u03b7(x,\u03bc) on ^d\u00d7_2(^d), with parameter \u03b7, aiming to approximate the optimal performance value function. \n        The AC algorithm is then updating alternatively the two parameters to find the optimal pair (\u03b8^*,\u03b7^*):  on the one hand,  for fixed policy \u03c0_\u03b8,  we rely  on the martingale formulation of the process\n        { e^-\u03b2 t^\u03b7(X_t^x_0,\u03be,_X_t^\u03be) + \u222b_0^t e^-\u03b2 r[  f(X_r^x_0,\u03be,_X_r^\u03be,\u03b1_r)  - \u03bblog p_\u03b8(X_r^x_0,\u03be,_X_r^\u03be,\u03b1_r)  ] \u1e5b,   t \u2265 0 }, \n        \n        and on the other hand, for fixed ^\u03b7,  on the martingale formulation of the process\n        \n            {\u222b_0^t  e^-\u03b2 r\u2207_\u03b8log p_\u03b8(X_r^x_0,\u03be,_X_r^\u03be,\u03b1_r) [ ^\u03b7(X_r^x_0,\u03be,_X_r^\u03be) - \u03b2^\u03b7(X_r^x_0,\u03be,_X_r^\u03be)  \n                 +  (  f(X_r^x_0,\u03be,_X_r^\u03be,\u03b1_r) -\u03bb - \u03bblog p_\u03b8(X_r^x_0,\u03be,_X_r^\u03be,\u03b1_r)  ) \u1e5b],   0 \u2264 t \u2264 T},\n         \n        for the parameter \u03b8 which vanishes the gradient of the performance value function. \n        \n        \n        \n        \n        \n        This suggests the following pseudo-code for online AC: \n        \n        \n        [H] \n        \n         Input data: Mesh time size \u0394 t, number of time steps n, Number of episodes N, learning rate \u03c1_S^i, \u03c1_E^i, \u03c1_G^i for the state distribution, PE and PG estimation, and function of the number of episodes i. \n        Parameter \u03bb for entropy regularisation. \n        Functional forms ^\u03b7 of performance value function,  p_\u03b8 of density policies. \n        \n         Initialisation: \u03bc_t_k: state distribution on ^d, for k=0,\u2026,n,  parameters \u03b7, \u03b8. \n        \n        each episode i=1,\u2026,NInitialise state X_0\u223c\u03bc_0\n        k=0,\u2026,n-1Update state distribution: \u03bc_t_k\u2190(1-\u03c1_S^i) \u03bc_t_k + \u03c1_S^i \u03b4_X_t_k\n        \n        Generate action \u03b1_t_k\u223c\u03c0_\u03b8(.|X_t_k,\u03bc_t_k)\n        \n        Observe (e.g. by environment simulator) state X_t_k+1 and reward f_t_k\n         \n        Compute\n        \u03b4_\u03b7   =   ^\u03b7(X_t_k+1,\u03bc_t_k+1)  - ^\u03b7(X_t_k,\u03bc_t_k) - \u03b2^\u03b7(X_t_k,\u03bc_t_k)  \u0394 t  \n                   +    f_t_k\u0394 t  - \u03bblog p_\u03b8(X_t_k,\u03bc_t_k,\u03b1_t_k) \u0394 t  \n        \u0394_\u03b7   =   \u03b4_\u03b7\u2207_\u03b7^\u03b7(X_t_k,\u03bc_t_k) \n        \u0394_\u03b8   =     e^-\u03b2 t_k\u2207_\u03b8log p_\u03b8(X_t_k,\u03bc_t_k,\u03b1_t_k) [ \u03b4_\u03b7 - \u03bb\u0394 t ],  \n        \n         \n        Critic Update: \u03b7\u2190\u03b7 + \u03c1_E^i \u0394 _\u03b7\n        \n        Actor Update: \u03b8\u2190\u03b8 + \u03c1_G^i \u0394_\u03b8\n        k\u2190k+1 Return: ^\u03b7, \u03c0_\u03b8Online actor-critic mean-field algorithm (infinite horizon)  \n        \n        \n          \n        Linear Quadratic case. In the LQ case on infinite horizon,  the optimal value function take the form \n        \n            v(x,\u03bc)    =   (x - \u03bc\u0305) K (x-\u03bc\u0305) + \u03bc\u0305\u039b\u03bc\u0305+ 2 Y.x + R,\n         \n        for some constants  K in ^d, \u039b in ^d, Y in ^d, and R in , solution to a system of algebraic Riccati equations, namely\n        \n        -\u03b2 K  + KB + B K + D K D + Q        \n        \n        -   (I+ F KD + C K)(N + F K F)^-1(I+ F KD + C K)    =     0,    \n        \n        - \u03b2\u039b + \u039b(B+B\u0305) + (B+B\u0305)\u039b + (D+D\u0305)\u039b (D+D\u0305) + Q + Q\u0305      \n        \n        -    (I+ I\u0305+ F K(D+D\u0305) + C\u039b)(N + F K F)^-1 (I + I\u0305 + F K(D+D\u0305) + C\u039b)    =    0,   \n        \n        - \u03b2   Y + (B + B\u0305) Y + (D+D\u0305) K \u03b3 + M       \n        \n         -    ( I+ I\u0305+  F K(D+D\u0305) + C\u039b)(N + F KF)^-1(H + F K\u03b3 + C Y)     =    0,   \n        \n         - \u03b2   R + \u03b3 K \u03b3  - (H + F K\u03b3 + C Y)(N + F KF)^-1(H + F K\u03b3 + C Y)       \n        \n         -   \u03bb m/2log(2\u03c0)-\u03bb/2log|\u03bb/2det(N+F K F)|   =    0,    \n        \n        while  the optimal randomised policy  is a gaussian distribution given by \n        \u03c0\u0302(.| t,x,\u03bc)    =    (  - S^-1(U  x + (V -U) \u03bc\u0305+ O )  ;  \u03bb/2 S^-1),\n        \n        where \n        \n        S  =    N+F K  F,           O   =     H+ F K \u03b3 +C Y,    \n        \n        U   =     I+F KD +C K ,           V   =    I+I\u0305+F K(D+D\u0305) +C\u039b.\n        \n        \n        \n        It is then natural to use a parametrisation of the performance value function as \n        ^\u03b7(x,\u03bc)    =    (x - \u03bc\u0305)\u03b7_1 (x-\u03bc\u0305) + \u03bc\u0305\u03b7_2\u03bc\u0305+  \u03b7_3.x + \u03b7_4, \n        \n        with parameter  \u03b7=(\u03b7_1,\u03b7_2,\u03b7_3,\u03b7_4)\u2208^d\u00d7^d\u00d7^d\u00d7, and a parametrisation of the randomised policies as\n        \u03c0^\u03b8(.|x,\u03bc)    =   (  \u03b8_1x +  \u03b8_2\u03bc\u0305+ \u03b8_3; \u03b8_4 ),\n        \n        with parameter \u03b8=(\u03b8_1,\u03b8_2,\u03b8_3,\u03b8_4)\u2208^m\u00d7 d\u00d7^m\u00d7 d\u00d7^m\u00d7_+^m. \n        \n        \n        \n        \n         \u00a7.\u00a7 Common noise\n        \n        \n        \n        The controlled dynamics is driven by \n        \n            X\u0323_s    =    b(X_s,^0_X_s,\u03b1_s) \u1e63  + \u03c3(X_s,^0_X_s,\u03b1_s) \u1e88_s + \u03c3_0(X_s,^0_X_s,\u03b1_s) \u1e88^0_s ,\n         \n        where there is an additional q-dimensional Brownian motion W^0 called common noise, \u03c3_0 is a measurable function from ^d\u00d7_2(^d)\u00d7 A into ^d\u00d7 q, and _X_t^0 denotes the conditional law of X_t given  W^0. \n        Given a parametric randomised policy \u03c0_\u03b8(.|t,x,\u03bc)  with density a\u2208A\u21a6p_\u03b8(t,x,\u03bc,a) on A, the associated performance value function to the MFC with common noise  on finite horizon with entropy regularisation  is formulated as \n        \n        J_\u03b8(t,x,\u03bc)    =   _\u03b1\u223c\u03c0_\u03b8[ \u222b_t^T  e^-\u03b2(s-t)(  f(X_s^t,x,\u03be,^0_X_s^t,\u03be,\u03b1_s) - \u03bb p_\u03b8(s,X_s^t,x,\u03be,^0_X_s^t,\u03be,\u03b1_s) )  \u1e63\n                +   e^-\u03b2(T-t)  g(X_T^t,x,\u03be,^0_X_T^t,\u03be)  ].  \n        \n        \n        \n        By means of It\u00f4's formula for conditional flow of probability measures, we show that the function J_\u03b8 is solution  to the PDE on [0,T]\u00d7^d\u00d7_2(^d)\n            \u222b_A { - \u03b2 J_\u03b8(t,x,\u03bc) +   ^a[J_\u03b8](t,x,\u03bc) + f(x,\u03bc,a) -\u03bblog p_\u03b8(t,x,\u03bc,a) }\u03c0_\u03b8(\u1ea1|t,x,\u03bc)   =    0,\n         \n         with terminal condition: J_\u03b8(T,x,\u03bc)=g(x,\u03bc),  and where  ^a is the operator defined  by\n        ^a [\u03c6](t,x,\u03bc)    =   \u03c6(t,x,\u03bc) +  b(x,\u03bc,a).D_x\u03c6(t,x,\u03bc) +  1/2 tr((\u03c3\u03c3+\u03c3_0\u03c3_0)(x,\u03bc,a) D_x^2 \u03c6(t,x,\u03bc)) \n                +   _\u03be\u223c\u03bc[   b(\u03be,\u03bc,a).\u2202_\u03bc\u03c6(t,x,\u03bc)(\u03be) + 1/2 tr((\u03c3\u03c3+\u03c3_0\u03c3_0)(\u03be,\u03bc,a) \u2202_x\u2202_\u03bc\u03c6(t,x,\u03bc)(\u03be))   ] \n               +  _(\u03be,\u03be')\u223c\u03bc\u2297\u03bc[  1/2 tr( \u03c3_0(\u03be,\u03bc,a)\u03c3_0(\u03be',\u03bc,a) \u2202^2_\u03bc\u03c6(t,x,\u03bc)(\u03be,\u03be'))   ].  \n        \n        \n        \n        \n        By following similar arguments as in paragraph <ref>, we obtain the gradient representation of the performance value function:\n        \n            \u2207_\u03b8 J_\u03b8(t,x,\u03bc)     =  _\u03b1\u223c\u03c0_\u03b8[ \u222b_t^T e^-\u03b2(s-t)\u2207_\u03b8log p_\u03b8(s,X_s^t,x,\u03be,^0_X_s^t,\u03be,\u03b1_s) {J\u0323_\u03b8(s,X_s^t,x,\u03be,^0_X_s^t,\u03be) \n                - \u03b2  J_\u03b8(s,X_s^t,x,\u03be,^0_X_s^t,\u03be)  +  [  f(X_s^t,x,\u03be,^0_X_s^t,\u03be,\u03b1_s) -\u03bb(1+ log p_\u03b8(s,X_s^t,x,\u03be,^0_X_s^t,\u03be,\u03b1_s) ) ] \u1e63}.\n         \n        Therefore, the AC algorithms in the common noise case have the same pseudo-codes as in the no common noise case (up to the discount factor \u03b2 that we have added here).  \n        \n        \n         \n        \n        \u00a7 LINEAR QUADRATIC MEAN-FIELD CONTROL WITH ENTROPY REGULARISATION\n        \n        \n        \n        We extend these results to the case of randomised controls, \u03b1_t\u223c\u03c0_t(\u1ea1)=p_t(a) \u1ea1, with -adapted (A)-valued process (\u03c0_t) with densities p_t, and with entropy regularisation.\n        \n            J(\u03c0)=_\u03b1\u223c\u03c0[\u222b_0^T (f(X_t,\u2119_X_t,\u03b1_t)+\u03bblog(p_t(\u03b1_t)))dt+g(X_T,\u2119_X_T)]\n         \n        where the dynamics of the controlled process is given by\n        \n            dX^\u03c0_t=b(X^\u03c0_t,\u2119_X^\u03c0_t,\u03b1_t)dt+\u03c3(X^\u03c0_t,\u2119_X^\u03c0_t,\u03b1_t)dW_t\n         with \u03b1_t\u223c\u03c0_t(da)=p_t(a)da and b,\u03c3,f,g defined above.\n        \n        In order to simplify the notations, we denote X_t=X^\u03c0_t and X\u0305_t=\ud835\udd3c[X^\u03c0_t]\n        \n        We minimize over the randomised controls:\n        \n            sup_\u03c0J(\u03c0)\n        \n        To specify it,\n        \n            J(\u03c0)=_\u03b1\u223c\u03c0[\u222b_0^T (X_t Q X_t+X\u0305_tQ\u0305X\u0305_t+\u03b1_t N\u03b1_t+2\u03b1_t I X_t+2\u03b1_tI\u0305X\u0305_t+2M X_t+2H\u03b1_t+\u03bblog(p(\u03b1_t)))dt\n        \n            +X_T P X_T+X\u0305_TP\u0305X\u0305_T+2L X_T]\n         \n        where\n        \n            dX_t=(BX_t+B\u0305X\u0305_t+C\u03b1_t)dt+(\u03b3+DX_t+D\u0305X\u0305_t+F\u03b1_t)dW_t\n        \n        Let {\ud835\udcb1^\u03c0_t,t\u2208[0,T],\u03c0\u2208\ud835\udcab(^m)} be a family of \ud835\udd3d-adapted processes in the form \ud835\udcb1^\u03c0_t=v(t,X^\u03c0_t,\u2119_X^\u03c0_t) for some \ud835\udd3d-adapted random field {v(t,x,\u03bc),t\u2208[0,T],x\u2208\u211d^d,\u03bc\u2208\ud835\udcab_2(\u211d^d)} satisfying\n        \n            v(t,x,\u03bc)\u2264 C(\u03c7_t+| x|^2+|\u03bc\u0305|^2),t\u2208[0,T],x\u2208\u211d^d,\u03bc\u2208\ud835\udcab_2(\u211d^d)\n        \n        for some positive constant C and nonnegative process \u03c7 with sup_t\u2208[0,T][|\u03c7_t|]<+\u221e and such that\n        \n          (i)v(T,x,\u03bc)=g(x,\u03bc),x\u2208\u211d^d,\u03bc\u2208\ud835\udcab_2(\u211d^d)\n          (ii)the map t\u2208[0,T]\u21a6_\u03b1\u223c\u03c0[\ud835\udcae^\u03c0_t], with \ud835\udcae^\u03c0_t=\ud835\udcb1^\u03c0_t+\u222b_0^t (X_s Q X_s+X\u0305_sQ\u0305X\u0305_s+\u03b1_s N \u03b1_s  + 2\u03b1_s I X_s + 2\u03b1_sI\u0305X\u0305_s +  2M X_s + 2H\u03b1_s+\u03bblog(p_s(\u03b1_s)))ds is nondecreasing for all \u03c0\u2208\ud835\udcab(^m) with \u03c0_t(da)=p_t(a)da\n          (iii)the map t\u2208[0,T]\u21a6_\u03b1\u223c\u03c0[\ud835\udcae^\u03c0_t] is constant for some \u03c0^*\u2208\ud835\udcab(^m) with \u03c0^*_t(da)=p^*_t(a)da\n        Then, \u03c0^* is an optimal randomised control and _\u03b1\u223c\u03c0^*[v(0,X_0,\u2119_X_0)] is the value of the randomised LQMKV control problem:\n        \n            V_0=_\u03b1\u223c\u03c0^*[v(0,X_0,\u2119_X_0)]=J(\u03c0^*)\n        Step 1 We infer a candidate for the random field {v(t,x,\u03bc),t\u2208[0,T],x\u2208\u211d^d,\u03bc\u2208\ud835\udcab_2(\u211d^d)} in the form \n        \n            v(t,x,\u03bc)=(x-\u03bc\u0305) K(t)(x-\u03bc\u0305)+\u03bc\u0305\u039b(t)\u03bc\u0305+2Y(t) x+R(t)\n        Step 2 For \u03c0\u2208\ud835\udcab(^m) and t\u2208[0,T], with \ud835\udcae^\u03c0_t defined above, we have\n        \n            d_\u03b1\u223c\u03c0[\ud835\udcae^\u03c0_t]=_\u03b1\u223c\u03c0[\ud835\udc9f^\u03c0_t]dt\n         for some \ud835\udd3d-adapted processes \ud835\udc9f^\u03c0 with \n        \n            _\u03b1\u223c\u03c0[\ud835\udc9f^\u03c0_t]=_\u03b1\u223c\u03c0[ X_t Q X_t+X\u0305_tQ\u0305X\u0305_t+\u03b1_t N \u03b1_t + 2\u03b1_t I X_t + 2\u03b1_tI\u0305X\u0305_t +  2M X_t + 2H\u03b1_t+\u03bblog(p_t(\u03b1_t))]\n        \n            +d/dt\ud835\udd3c_\u03b1\u223c\u03c0[v(t,X_t,\u2119_X_t)]\n        Step 3  To find an optimal control \u03c0^* such that _\u03b1\u223c\u03c0^*[\ud835\udc9f^\u03c0^*_t]=0 and this implies that [\ud835\udc9f^\u03c0_t]\u2265 0, \u2200\u03c0\u2208\ud835\udcab(\ud835\udc9c) and then the property (ii).\n        \n        In fact, defining \n            w(t,x,x\u0305)=(x-x\u0305) K(t)(x-x\u0305)+x\u0305\u039b(t)x\u0305+2Y(t) x+R(t)\n         we have\n        \n            v(t,X_t,\u2119_X_t)=w(t,X_t,X\u0305_t)=(X_t-X\u0305_t) K(t)(X_t-X\u0305_t)+X\u0305_t\u039b(t)X\u0305_t+2Y(t) X_t+R(t)\n        \n        Using the It\u00f4's formula, we have\n        \n            dv(t,X_t,\u2119_X_t)=[(X_t-X\u0305_t)K\u0307(t)(X_t-X\u0305_t)+X\u0305_t\u039b\u0307(t)X\u0305_t+2\u1e8e(t) X_t+\u1e58(t)]dt+\n        \n            ((X_t-X\u0305_t) K(t)+Y(t))dX_t+(dX_t)(K(t)(X_t-X\u0305_t) K(t)+Y(t))\n        \n            +(-(X_t-X\u0305_t) K(t)+X\u0305_t\u039b(t))dX\u0305_t+(dX\u0305_t) (-K(t)(X_t-X\u0305_t)+\u039b(t)X\u0305_t)+Tr(K(t)d\u27e8 X,X\u27e9_t)\n        \n            =[(X_t-X\u0305_t)K\u0307(t)(X_t-X\u0305_t)+X\u0305_t\u039b\u0307(t)X\u0305_t+2\u1e8e(t) X_t+\u1e58(t)]dt+\n        \n            ((X_t-X\u0305_t) K(t)+Y(t))dX_t+(dX_t)(K(t)(X_t-X\u0305_t) K(t)+Y(t))\n        \n            +(-(X_t-X\u0305_t) K(t)+X\u0305_t\u039b(t))dX\u0305_t+(dX\u0305_t) (-K(t)(X_t-X\u0305_t)+\u039b(t)X\u0305_t)\n        \n            +(\u03b3+DX_t+D\u0305X\u0305_t+F\u03b1_t) K(t)(\u03b3+DX_t+D\u0305X\u0305_t+F\u03b1_t)dt\n        \n        Since\n        \n            dX_t=(BX_t+B\u0305X\u0305_t+C\u03b1_t)dt+(\u03b3+DX_t+D\u0305X\u0305_t+F\u03b1_t)dW_t\n        \n        and denoting \u03b1\u0305_t=\ud835\udd3c[\u03b1_t]\n        we get\n        \n            dX\u0305_t=((B+B\u0305)X\u0305_t+C\u03b1\u0305_t)dt\n        \n        Noticing that \n            \ud835\udd3c_\u03b1\u223c\u03c0[(-(X_t-X\u0305_t) K(t)+X\u0305_t\u039b(t))dX\u0305_t]=_\u03b1\u223c\u03c0[X\u0305_t\u039b(t)((B+B\u0305)X\u0305_t+C\u03b1\u0305_t)]dt\n        \n            =_\u03b1\u223c\u03c0[X\u0305_t\u039b(t)((B+B\u0305)X\u0305_t+C\u03b1_t)]dt\n        \n        Hence\n        \n            d/dt_\u03b1\u223c\u03c0[v(t,X_t,\u2119_X_t)]=_\u03b1\u223c\u03c0[(X_t-X\u0305_t)K\u0307(t)(X_t-X\u0305_t)+X\u0305_t\u039b\u0307(t)X\u0305_t+2\u1e8e(t) X_t+\u1e58(t)\n        \n            +((X_t-X\u0305_t) K(t)+Y(t))(BX_t+B\u0305X\u0305_t+C\u03b1_t)+(BX_t+B\u0305X\u0305_t+C\u03b1_t)(K(t)(X_t-X\u0305_t)+Y(t))\n        \n            +X\u0305_t\u039b(t)((B+B\u0305)X\u0305_t+C\u03b1_t)+((B+B\u0305)X\u0305_t+C\u03b1_t)\u039b(t)X\u0305_t\n        \n            +(\u03b3+DX_t+D\u0305X\u0305_t+F\u03b1_t) K(t)(\u03b3+DX_t+D\u0305X\u0305_t+F\u03b1_t)]\n        \n        Hence\n        \n            \ud835\udd3c_\u03b1\u223c\u03c0[\ud835\udc9f^\u03c0_t]=_\u03b1\u223c\u03c0[(X_t-X\u0305_t)(Q+K\u0307(t)+K(t)B+BK(t)+D K(t) D)(X_t-X\u0305_t)\n        \n            +X\u0305_t(Q+Q\u0305+\u039b\u0307(t)+\u039b(t)(B+B\u0305)+(B+B\u0305)\u039b(t)+(D+D\u0305) K(t)(D+D\u0305))X\u0305_t\n        \n            +2(M+\u1e8e(t)+(B+B\u0305) Y(t)+(D+D\u0305) K(t)\u03b3) X_t+\u1e58(t)+\u03b3 K(t)\u03b3\n        \n            +\u03b1_t (N+F K(t) F) \u03b1_t+2\u03b1_t(I+C K(t)+F K(t)D)X_t+2\u03b1_t(I\u0305-C K(t)+C\u039b(t)+F K(t)D\u0305)X\u0305_t\n        \n            +2( H+C Y(t)+F K(t)\u03b3)\u03b1_t+\u03bblog(p_t(\u03b1_t))]\n        \n            =[(X_t-X\u0305_t)(Q+K\u0307(t)+2K(t)B+D K(t) D)(X_t-X\u0305_t)\n        \n            +X\u0305_t(Q+Q\u0305+\u039b\u0307(t)+2\u039b(t)(B+B\u0305)+(D+D\u0305) K(t)(D+D\u0305))X\u0305_t\n        \n            +2(M+\u1e8e(t)+(B+B\u0305) Y(t)+(D+D\u0305) K(t)\u03b3) X_t+\u1e58(t)+\u03b3 K(t)\u03b3\n        \n            +\u222b_^m(a (N+F K(t) F) a+2a(I+C K(t)+F K(t)D)X_t+2a(I\u0305-C K(t)+C\u039b(t)+F K(t)D\u0305)X\u0305_t\n        \n            +2( H+C Y(t)+F K(t)\u03b3) a+\u03bblog(p_t(a)))p_t(a)da]\n        \n        Denoting\n        \n            S_t=N+F K(t) F\n        \n            U_t=I+C K(t)+F K(t)D\n        \n            V_t=I+I\u0305+C\u039b(t)+F K(t)(D+D\u0305)\n        \n            O_t=H+C Y(t)+F K(t)\u03b3\n        \n        the optimal control \u03c0^*_t(da)=p_t^*(a)da minimizes \n            \u222b_^m(a S_t a+2aU_tX_t+2a(V_t-U_t)X\u0305_t+2O_t a+\u03bblog(p_t(a)))p_t(a)da\n        \u03bb>0 For the minimization problem \n        \n            inf_p>0,\u222b_^mp(a)da=1\u222b_^m(f(a)+\u03bblog(p(a)))p(a)da\n         the solution is \n        \n            p^*(a)=exp(-1/\u03bbf(a))/\u222b_^mexp(-1/\u03bbf(a))da\n        Proof\n        In order to solve the optimization problem with constraints, we consider the Lagrange method with a multiplier \u03b2, that is\n        \n            inf_p>0,\u03b2\u222b_^m(f(a)+\u03bblog(p(a)))p(a)da-\u03b2(\u222b_^mp(a)da-1)=inf_p>0,\u03b2\u222b_^m(f(a)+\u03bblog(p(a))-\u03b2)p(a)da+\u03b2\n        \n        For inf_p>0\u222b_^m(f(a)+\u03bblog(p(a))-\u03b2)p(a)da we minimize it point-wisely.\n         Considering \n            F(p;\u03b2)=(f(a)+\u03bblog p-\u03b2)p\n        \n            \u2202/\u2202 pF(p;\u03b2)=f(a)+\u03bblog p^*-\u03b2+\u03bb\n        \n        and \n            \u2202^2/\u2202^2 pF(p;\u03b2)=\u03bb/p>0\n        \n        Hence the solution (p^*,\u03b2^*) must satisfy for all a\u2208^m,\n        \n            \u2202/\u2202 pF(p^*(a);\u03b2^*)=f(a)+\u03bblog p^*(a)-\u03b2^*+\u03bb=0\n            \u222b_^mp^*(a)da=1\n        \n        Finally we obtain \n        \n            p^*(a)=exp(-1/\u03bbf(a))/\u222b_^mexp(-1/\u03bbf(a))da\n        \n        \n        According to this lemma, the optimal control is given by\n        \n            p_t^*(a)=exp(-1/\u03bb(a S_t a+2a U_tX_t+2a(V_t-U_t)X\u0305_t+2O_t a))/\u222b_^mexp(-1/\u03bb(a S_t a+2a U_tX_t+2a(V_t-U_t)X\u0305_t+2O_t a))da\n        \n        it is the gaussian distrubution with\n        \n            \u03c0_t^*=\ud835\udca9(\u00b7|-S_t^-1(U_tX_t+(V_t-U_t)X\u0305_t+O_t),\u03bb/2S_t^-1)\n        \n        Hence\n        \n            \u222b_^m(a S_t a+2a U_tX_t+2a(V_t-U_t)X\u0305_t+2O_t a+\u03bblog(p_t^*(a)))p_t^*(a)da\n        \n            =-(U_tX_t+(V_t-U_t)X\u0305_t+O_t) S_t^-1(U_tX_t+(V_t-U_t)X\u0305_t+O_t)-\u03bb m/2log(2\u03c0)-\u03bb/2log|\u03bb/2det(S_t)|\n        \n        and\n        \n            \ud835\udd3c[\u222b_^m(a S_t a+2a U_tX_t+2a(V_t-U_t)X\u0305_t+2O_t a+\u03bblog(p_t^*(a)))p_t^*(a)da]\n        \n            =-\ud835\udd3c[(X_t-X\u0305_t) U_t S_t^-1U_t(X_t-X\u0305_t)+X\u0305_t V_t S_t^-1 V_tX\u0305_t+2O_t S_t^-1V_t X_t+O_t S_t^-1O_t]-\u03bb m/2log(2\u03c0)-\u03bb/2log|\u03bb/2det(S_t)|\n        \n        then {K(t)},{\u039b(t)},{Y(t)},{R(t)} satisfy the following equations:\n        \n            K\u0307(t)+Q+K(t)B+B K(t)+D K(t) D\n               -(I+C K(t)+F K(t) D) (N+F K(t) F)^-1(I+C K(t)+F K(t) D)=0\n               K(T)=P\n               \u039b\u0307(t)+Q+Q\u0305+\u039b(t)(B+B\u0305)+(B+B\u0305)\u039b(t)+(D+D\u0305) K(t)(D+D\u0305)\n               -(I+I\u0305+C\u039b(t)+F K(t)(D+D\u0305)(N+F K(t) F)^-1(I+I\u0305+C\u039b(t)+F K(t)(D+D\u0305)=0\n               \u039b(T)=P+P\u0305\n               \u1e8e(t)+M+(B+B\u0305) Y(t)+(D+D\u0305) K(t)\u03b3\n               -(H+C Y(t)+F K(t)\u03b3)(N+F K(t) F)^-1(I+I\u0305+C\u039b(t)+F K(t)(D+D\u0305)=0\n               Y(T)=L\n               \u1e58(t)+\u03b3 K(t)\u03b3-(H+C Y(t)+F K(t)\u03b3)(N+F K(t) F)^-1(H+C Y(t)+F K(t)\u03b3)\n               -\u03bb m/2log(2\u03c0)-\u03bb/2log|\u03bb/2det(N+F K(t) F)|=0\n               R(T)=0\n         \n        \n        \u00a7 PROOFS OF SOME REPRESENTATION RESULTS\n        \n        \n         \u00a7.\u00a7 Proof of Proposition <ref>\n        Step 1: For a fixed policy \u03c0, we introduce the non-linear McKean-Vlasov SDE with dynamics\n        \n            X\u0303^t, \u03be_s = \u03be + \u222b_t^s  b_\u03c0(r,X\u0303^t, \u03be_r,_X\u0303^t, \u03be_r)  \u1e5b + \u222b_t^s \u03c3_\u03c0(r,X\u0303^t, \u03be_r,_X\u0303^t, \u03be_r) \u1e88_r,\n         recalling that \u03c3_\u03c0=\u03a3_\u03c0^1/2, as well as its associated decoupled SDE with dynamics\n        \n            X\u0303^t, x, \u03bc_s = x + \u222b_t^s  b_\u03c0(r,X\u0303^t, x, \u03bc_r,_X\u0303^t, \u03be_r)  \u1e5b + \u222b_t^s \u03c3_\u03c0(r,X\u0303^t, x, \u03bc_r,_X\u0303^t, \u03be_r) \u1e88_r.\n        \n        Under Assumption <ref>(i), the coefficients b_\u03c0 and \u03c3_\u03c0 are Lipschitz-continuous and with at most linear growth with respect to the variable x and \u03bc locally uniformly in time. Hence, the SDEs (<ref>)-(<ref>) admit a unique strong solution.\n        \n        Denoting by  the probability measure on \ud835\udc9e([0, \u221e), ^d) (the space of continuous functions defined on [0,\u221e) taking values in ^d) induced by the unique solution to the SDE (<ref>) and by (t) its marginal at time t, its infinitesimal generator is given by\n        \n            \u2112\u0303^\u03c0_t\u03c6(x)     = \u2211_i=1^d \u222b_A b_i( x, (t), a)  \u03c0(\u1ea1| t, x, (t)) \u2202_x_i\u03c6(x) \n                  +  1/2\u2211_i,j=1^d \u222b_A (\u03c3\u03c3)_i, j(x, (t), a)   \u03c0(\u1ea1| t, x, (t)) \u2202^2_x_i, x_j\u03c6(x).\n        \n        Now, coming back to the dynamics of the McKean-Vlasov SDE (<ref>), we importantly point out that since at each time s, the action \u03b1_s is sampled from the probability distribution \u03c0(.|s, X^t, \u03be_s, _X_s^t, \u03be) independently of W, the infinitesimal generator at time t of (<ref>) is exactly given by \u2112\u0303^\u03c0_t. Hence, it follows from the uniqueness of the martingale problem associated to \u2112\u0303^\u03c0 that X^t, \u03be and X\u0303^t, \u03be have the same law [This was formally shown by law of large numbers in <cit.> in the standard diffusion case.].\n        \n        We thus conclude that V^\u03c0 can be written as \n        \n            V^\u03c0(t, x,\u03bc)    =  _[ \u222b_t^T e^-\u03b2 (s-t)  (f_\u03c0-\u03bb E_\u03c0)(s,X\u0303_s^t,x,\u03bc,_X\u0303_s^t,\u03be)   \u1e63\n            +    e^-\u03b2 (T-t) g(X\u0303_T^t, x, \u03bc,_X\u0303_T^t,\u03be) ].\n        \n        Step 2: We know, see e.g. <cit.> or <cit.>, that Assumption <ref>(i) guarantees the existence of  a modification of X\u0303^t, x, \u03bc such that:\n         \n          *  The map x\u21a6X\u0303_s^t, x, \u03be is \u2119-a.s. twice continuously differentiable, \n         \n          *   for any x\u2208\u211d^d,  0\u2264 t\u2264 s, and any p\u22651, the map  \ud835\udcab_2(\u211d^d) \u220b\u03bc\u21a6X\u0303_s^t, x, \u03bc\u2208 L^p(\u2119) is differentiable and the map \u211d^d \u220b v \u21a6\u2202_\u03bcX\u0303_s^t, x, \u03bc(v) \u2208 L^p(\u2119) is differentiable,\n         \n          *  for any p\u22651, the derivatives (t, x, \u03bc, v) \u21a6\u2202_x X\u0303_s^t, x, \u03bc, \u2202_x^2 X\u0303_s^t, x, \u03bc, \u2202_\u03bcX\u0303_s^t, x, \u03bc(v), \u2202_v [\u2202_\u03bcX\u0303_s^t, x, \u03bc](v) \u2208 L^p(\u2119) are continuous.\n         \n        Moreover, the following estimates hold for n=1,2 and any p\u22651\n            sup_0\u2264 t \u2264 s \u2264 T, (x,v) \u2208 (\u211d^d)^2, \u03bc\u2208_2(\u211d^d){\u2202_x^n X\u0303_s^t, x, \u03bc_L^p(\u2119) + \u2202_\u03bcX\u0303_s^t, x, \u03bc(v) _L^p(\u2119) +  \u2202_v \u2202_\u03bcX\u0303_s^t, x, \u03bc(v) _L^p(\u2119)} < \u221e.\n        \n        \n        We thus deduce that the functions x\u21a6 f_\u03c0(s,X\u0303_s^t,x,\u03bc,_X\u0303_s^t,\u03be),    E_\u03c0(s,X\u0303_s^t, x,\u03bc,_X\u0303_s^t,\u03be),   g(X\u0303_T^t, x, \u03bc,_X\u0303_T^t,\u03be) are \u2119-a.s. twice continuously differentiable with derivatives that belong to L^p(\u2119), for any p\u22651, uniformly in x, \u03bc and t\u2208 [0,s]. The dominated convergence theorem eventually guarantees that x\u21a6 V^\u03c0 (t, x, \u03bc) is twice continuously differentiable with\n        \n            \u2202_x_i V^\u03c0(t, x, \u03bc)    =  _[ \u222b_t^T e^-\u03b2 (s-t)\u2211_k=1^d  \u2202_x_k (f_\u03c0-\u03bb E_\u03c0)(s,X\u0303_s^t, x,\u03bc,_X\u0303_s^t,\u03be)   \u2202_x_i (X\u0303_s^t, x, \u03bc)^k  \u1e63\n                +    e^-\u03b2 (T-t)\u2211_k=1^d \u2202_x_k g(X\u0303_T^t, x, \u03bc,_X\u0303_T^t,\u03be)  \u2202_x_i (X\u0303_T^t, x, \u03bc)^k ],\n         and\n        \n            \u2202^2_x_i, x_j V^\u03c0(t, x, \u03bc)    =  _[ \u222b_t^T e^-\u03b2 (s-t)\u2211_k, \u2113=1^d \u2202^2_x_k, x_\u2113 (f_\u03c0-\u03bb E_\u03c0)(s,X\u0303_s^t, x,\u03bc,_X\u0303_s^t,\u03be)   \u2202_x_i (X\u0303_s^t, x, \u03bc)^k  \u2202_x_j (X\u0303_s^t, x, \u03bc)^\u2113\u1e63\n                 +    e^-\u03b2 (T-t)\u2211_k, \u2113=1^d \u2202^2_x_k, x_\u2113 g(X\u0303_T^t, x, \u03bc,_X\u0303_T^t,\u03be)  \u2202_x_i (X\u0303_T^t, x, \u03bc)^k  \u2202_x_j (X\u0303_T^t, x, \u03bc)^\u2113\n                + \u222b_t^T e^-\u03b2 (s-t)\u2211_k=1^d  \u2202_x_k (f_\u03c0-\u03bb E_\u03c0)(s,X\u0303_s^t, x,\u03bc,_X\u0303_s^t,\u03be)   \u2202^2_x_i, x_j (X\u0303_s^t, x, \u03bc)^k   \u1e63\n                  +    e^-\u03b2 (T-t)\u2211_k=1^d \u2202_x_k g(X\u0303_T^t, x, \u03bc,_X\u0303_T^t,\u03be)  \u2202^2_x_i, x_j (X\u0303_T^t, x, \u03bc)^k  ].\n        \n        It follows from the above expression and again the dominated convergence theorem that (t, x, \u03bc) \u21a6\u2202_x_i V^\u03c0(t, x, \u03bc), \u2202^2_x_i, x_j V^\u03c0(t, x, \u03bc) are continuous.\n        \n        Similarly, note that under the current assumption, the functions \u03bc\u21a6 h(t,X\u0303_s^t, x, \u03bc,_X\u0303_s^t,\u03be), g(X\u0303_T^t, x, \u03bc,_X\u0303_T^t,\u03be), where h \u2208{   f_\u03c0, E_\u03c0}, are L-differentiable with derivatives satisfying\n        \n            \u2202^i_\u03bc [h(s,X\u0303_s^t, x,\u03bc,_X\u0303_s^t,\u03be)](v)     = \u2211_k=1^d \u2202_x_k h(s,X\u0303_s^t, x,\u03bc,_X\u0303_s^t,\u03be) \u2202^i_\u03bc [(X\u0303_s^t, x,\u03bc)^k](v) \n                  +\ud835\udd3c[ \u2211_k=1^d \u2202^k_\u03bc h(s, X\u0303_s^t, x,\u03bc, _X\u0303_s^t,\u03be)(X_s^t, v, \u03bc) \u2202_x_i (X_s^t, v, \u03bc)^k]  \n                  + \u222b_\u211d^d\ud835\udd3c[\u2211_k=1^d \u2202^k_\u03bc h(s,X\u0303_s^t, x,\u03bc, _X\u0303_s^t,\u03be)(X_s^t, x', \u03bc) \u2202^i_\u03bc [(X_s^t, x', \u03bc)^k](v) ]  \u03bc(x\u0323'), \n            \u2202^i_\u03bc [g(X\u0303_T^t, x, \u03bc,_X\u0303_T^t,\u03be)](v)     =  \u2211_k=1^d \u2202_x_k g(X\u0303_T^t, x,\u03bc,_X\u0303_T^t,\u03be) \u2202^i_\u03bc [(X\u0303_T^t, x,\u03bc)^k](v)\n                  + \ud835\udd3c[ \u2211_k=1^d \u2202^k_\u03bc g(X\u0303_T^t, x,\u03bc, _X\u0303_T^t,\u03be)(X_T^t, v, \u03bc) \u2202_x_i (X_T^t, v, \u03bc)^k ]  \n                  + \u222b_\u211d^d\ud835\udd3c[\u2211_k=1^d \u2202^k_\u03bc g(X\u0303_T^t, x,\u03bc, _X\u0303_T^t,\u03be)(X_T^t, x', \u03bc) \u2202^i_\u03bc [(X_T^t, x', \u03bc)^k] (v) ]  \u03bc(x\u0323'),\n         where (X_s^t, x, \u03bc)_s\u2208 [t, T] stands for a copy of (X\u0303^t, x, \u03bc_s)_s \u2208 [t, T] defined on a copy (\u03a9, \u2131, \u2119) of the original probability space (\u03a9, \u2131, \u2119). Under Assumption <ref>, it follows from the above identities that (t, x, \u03bc, v) \u21a6\u2202_\u03bc [h(s,X\u0303_s^t, x,\u03bc,_X\u0303_s^t,\u03be)](v) ,  \u2202_\u03bc [g(X\u0303_T^t, x, \u03bc,_X\u0303_T^t,\u03be)](v) \u2208 L^p(\u2119), p\u22651, are continuous and satisfy\n        \n            |\u2202_\u03bc [h(s,X\u0303_s^t, x, \u03bc, _X\u0303_s^t,\u03be)](v) |   \u2264 K (1+ |X\u0303_s^t, x, \u03bc| + |v| + M_2( _X\u0303_s^t,\u03be)^q) (1+ |\u2202_\u03bcX\u0303^t, x, \u03bc_s(v)| ) \n               \u2264 K (1+ |X\u0303_s^t, x,\u03bc| + |v| + M_2(\u03bc)^q) (1+ |\u2202_\u03bcX\u0303^t, x, \u03bc_s(v)| ),\n         and\n        \n            | \u2202_\u03bc [g(X\u0303_T^t, x, \u03bc,_X\u0303_T^t,\u03be)](v) |    \u2264 K (1 + |X\u0303_T^t, x,\u03bc| + |v| + M_2( _X\u0303_T^t,\u03be)^q) (1+ |\u2202_\u03bcX\u0303^t, x, \u03bc_T(v)| )\n               \u2264 K (1+ |X\u0303_T^t, x,\u03bc| + |v| + M_2(\u03bc)^q) (1+ |\u2202_\u03bcX\u0303^t, x, \u03bc_T(v)| ),\n         where we used the fact that M_2( _X\u0303_s^t,\u03be) \u2264 K(1+M_2(\u03bc)), for any s\u2208 [t, T], for the last inequality. Similarly, it follows from (<ref>) and the dominated convergence theorem that v\u21a6\u2202_\u03bc [h(t,X\u0303_s^t, x, \u03bc,_X\u0303_s^t,\u03be)](v), \u2202_\u03bc [g(X\u0303_T^t, x, \u03bc,_X\u0303_T^t,\u03be)](v) are continuously differentiable with derivatives being continuous with respect to their entries and satisfying\n        \n            | \u2202_v \u2202_\u03bc [h(s,X\u0303_s^t, x,\u03bc, _X\u0303_s^t,\u03be)](v) |   \u2264  K (1+ |X\u0303_s^t, x, \u03bc| + |v| + M_2(\u03bc)^q),\n            \n            | \u2202_v \u2202_\u03bc [g(X\u0303_T^t, x, \u03bc,_X\u0303_T^t,\u03be)](v) |    \u2264 K (1+ |X\u0303_T^t, x, \u03bc| + |v| + M_2(\u03bc)^q).\n        \n        \n        Coming back to (<ref>) and using the above estimates together with the dominated convergence theorem allows to conclude that \u03bc\u21a6 V^\u03c0(t, x, \u03bc) is L-differentiable and that v\u21a6\u2202_\u03bc V^\u03c0(t, x, \u03bc)(v) is differentiable. Moreover, both derivatives \u2202_\u03bc V^\u03c0(t, x, \u03bc)(v), \u2202_v \u2202_\u03bc V^\u03c0(t, x, \u03bc)(v) are continuous with respect to their entries and satisfy\n        \n            sup_t \u2208 [0,T]{ | \u2202_\u03bc V^\u03c0(t, x, \u03bc)(v)| + |\u2202_v \u2202_\u03bc V^\u03c0(t, x, \u03bc)(v)| }\u2264 K (1+ |x| +|v| + M_2(\u03bc)^q).\n        \n        We thus conclude that V^\u03c0\u2208\ud835\udc9e^0, 2, 2([0,T] \u00d7\u211d^d \u00d7_2(\u211d^d)).\n        \n        Step 3:  Let us now prove that (t, x, \u03bc) \u21a6 V^\u03c0(t, x, \u03bc) \u2208\ud835\udc9e^1, 2, 2([0,T] \u00d7\u211d^d \u00d7_2(\u211d^d)). From the Markov property satisfied by the SDE (<ref>), stemming from its strong well-posedness, for any 0\u2264 h \u2264 t, the following relation is satisfied\n        \n            V^\u03c0(t-h, x, \u03bc)     =  e^-\u03b2 h\ud835\udd3c[ \u222b_t-h^t e^-\u03b2 (s-t)   (f_\u03c0-\u03bb E_\u03c0)(s,X\u0303_s^t-h, x, \u03bc,_X\u0303_s^t-h, \u03be)   \u1e63] \n                  + e^-\u03b2 h\ud835\udd3c[ V^\u03c0(t, X\u0303_t^t-h, x, \u03bc, \u2119_X\u0303_t^t-h, \u03be)].\n        \n        Now, combining the fact that V^\u03c0(t, .) \u2208\ud835\udc9e^2, 2(\u211d^d \u00d7_2(\u211d^d)) with (<ref>) guarantees that one may apply It\u00f4's rule, see e.g. Proposition 5.102 <cit.>. We thus obtain\n        \n            h^-1 (V^\u03c0(t-h, x, \u03bc)      -V^\u03c0(t, x, \u03bc)) \n                =  e^-\u03b2 h h^-1\u222b_t-h^t e^-\u03b2 (s-t)\ud835\udd3c[ (f_\u03c0-\u03bb E_\u03c0)(s,X\u0303_s^t-h, x,\u03bc,_X\u0303_s^t-h, \u03be)  ] \u1e63\n                  + e^-\u03b2 h h^-1\ud835\udd3c[ V^\u03c0(t, X\u0303_t^t-h, x, \u03bc, \u2119_X\u0303_t^t-h, \u03be) - V^\u03c0(t, x, \u03bc)]\n                  + h^-1 (e^-\u03b2 h - 1) V^\u03c0(t, x, \u03bc)\n                = e^-\u03b2 h h^-1\u222b_t-h^t e^-\u03b2 (s-t)\ud835\udd3c[ (f_\u03c0-\u03bb E_\u03c0)(s,X\u0303_s^t-h, x,\u03bc,_X\u0303_s^t-h, \u03be)    ] \u1e63\n                  + e^-\u03b2 h h^-1\u222b_t-h^t\ud835\udd3c[[V^\u03c0](t, X\u0303_s^t-h, x, \u03bc, \u2119_X\u0303_s^t-h, \u03be)] \u1e63\n                  + h^-1 (e^-\u03b2 h - 1) V^\u03c0(t, x, \u03bc),\n        \n        where\n         \n         _\u03c0[\u03c6](t, x,\u03bc) \n            =      b_\u03c0(t, x, \u03bc) \u00b7 D_x\u03c6(t, x, \u03bc) +  1/2\u03a3_\u03c0(t, x, \u03bc) : D_x^2 \u03c6(t, x, \u03bc)  \n                +   _\u03be\u223c\u03bc[   b_\u03c0(t,\u03be,\u03bc) \u00b7\u2202_\u03bc\u03c6(t, x, \u03bc)(\u03be) + 1/2\u03a3_\u03c0(t, \u03be, \u03bc) : \u2202_\u03c5\u2202_\u03bc\u03c6(t, x, \u03bc)(\u03be)    ].\n         \n        Letting h\u2193 0 in (<ref>), from the continuity and quadratic growth of f_\u03c0, E_\u03c0 as well as the continuity of [V^\u03c0](t,.), we deduce that t\u21a6 V^\u03c0(t, x, \u03bc) is left-differentiable on (0,T). Still from the continuity of  f_\u03c0, E_\u03c0 and [V^\u03c0], we eventually conclude that it is differentiable on [0,T) with a derivative satisfying\n        \n            \u2202_t V^\u03c0(t, x, \u03bc) - \u03b2 V^\u03c0(t, x, \u03bc) +  _\u03c0[V^\u03c0](t, x,\u03bc) +  (f_\u03c0-\u03bb E_\u03c0)(t, x, \u03bc) = 0.\n        \n        The proof is now complete.\n        \n        \n        \n        \n        \n        \n         \u00a7.\u00a7 Differentiability of the parametric critic function\n         \n        \n        \n        \n        \n        Under the standard assumption that the coefficients b_\u03c0_\u03b8(t, .),  \u03c3_\u03c0_\u03b8(t, .) are Lipschitz-continuous on \u211d^d \u00d7_2(\u211d^d) uniformly in t\u2208 [0,T] and \u03b8\u2208\u0398, the system of SDEs (<ref>) admits a unique strong solution when \u03b1\u223c\u03c0_\u03b8. We will denote by (X_s^t, \u03be(\u03b8), X_s^t, x, \u03be(\u03b8)) the solution taken at time s. We will also use the more compact notation\n         \n            X_s^t,\u03be(\u03b8)    = \u03be + \u222b_t^s \u2211_j=0^p g^j_\u03b8(r, X_r^t, \u03be(\u03b8), _X_r^t, \u03be(\u03b8))  \u1e88^j_r    \n            \n            X_s^t, x,\u03bc(\u03b8)    = x + \u222b_t^s \u2211_j=0^p g^j_\u03b8(r, X_r^t, x, \u03bc(\u03b8), _X_r^t, \u03be(\u03b8))  \u1e88^j_r,    t \u2264 s \u2264 T,\n         with g^0_\u03b8(t, x, \u03bc) = b_\u03c0_\u03b8(t, x, \u03bc), g^j_\u03b8(t, x, \u03bc) = \u03c3^., j_\u03c0_\u03b8(t, x, \u03bc), \u1e88_r = ( \u1e88_r^0, \u22ef, \u1e88_r^p) with \u1e88^0_r = \u1e5b.\n        \n        \n        \n        Under Assumption <ref>, the derivatives (t, \u03b8, x, \u03bc, v) \u21a6\u2202_\u03b8\u2202_xX\u0303_s^t, x, \u03bc(\u03b8), \u2202_x\u2202_\u03b8X\u0303_s^t, x, \u03bc(\u03b8), \u2202_\u03b8\u2202^2_xX\u0303_s^t, x, \u03bc(\u03b8), \u2202^2_x\u2202_\u03b8X\u0303_s^t, x, \u03bc(\u03b8), \u2202_\u03b8 [\u2202_\u03bcX\u0303_s^t, x, \u03bc(\u03b8)](v), \u2202_\u03bc\u2202_\u03b8X\u0303_s^t, x, \u03bc(\u03b8)(v), \u2202_\u03b8\u2202_v[\u2202_\u03bcX\u0303_s^t, x, \u03bc(\u03b8)](v), \n        \u2202_v [\u2202_\u03bc\u2202_\u03b8X\u0303_s^t, x, \u03bc(\u03b8)](v)\u2208L^p(\u2119) exist and are locally Lipschitz continuous for all p\u22651. \n        \n        \n        \n        \n        \n        \n        \n        \n        \n         Proof. The proof of the existence and continuity of the derivatives of the flow X_s^t, x, \u03be(\u03b8) with respect to the parameters x, \u03bc, v and \u03b8 is rather standard but quite mechanical and actually follows similar lines of reasonings as those employed for the proof of Theorem 3.2 in <cit.>. We thus omit it.  \n        \n        \n        \n        With the same notations as Lemma <ref>, under Assumption <ref>, taking h_\u03b8 = f_\u03c0_\u03b8, E_\u03c0_\u03b8 or g(x, \u03bc), we deduce from the above result that the derivatives (t, \u03b8, x, \u03bc, v) \u21a6\u2202_\u03b8\u2202_x [h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8))], \u2202_x\u2202_\u03b8 [h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8))], \u2202_\u03b8\u2202_\u03bc  [h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8))](v), \u2202_\u03bc\u2202_\u03b8  [h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8))](v), \u2202_\u03b8\u2202_v\u2202_\u03bc [h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8))](v),  \u2202_v\u2202_\u03bc\u2202_\u03b8 [h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8))](v)\u2208 L^p(\u2119), for any p \u22651 and any 0\u2264 t\u2264 s \u2264 T, exist and are continuous. For instance, standard computations give \n            \u2202_\u03b8_l\u2202_x_i    [h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8))] \n                = \u2211_j=1^d \u2202_\u03b8_l\u2202_x_j h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8)) \u2202_x_i (X\u0303_s^t, x, \u03bc(\u03b8))^j\n                  + \u2211_j,k=1^d \u2202^2_x_j, x_k h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8)) \u2202_x_i (X\u0303_s^t, x, \u03be(\u03b8))^j  \u2202_\u03b8_l (X\u0303_s^t, x, \u03bc(\u03b8))^k \n                  +  \u2211_j, k=1^d \ud835\udd3c[ [\u2202_\u03bc\u2202_x_j h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8))]_k(X_s^t, \u03bc(\u03b8)) \u2202_\u03b8_l (X_s^t, \u03be(\u03b8))^k ] \u2202_x_i (X\u0303_s^t, x, \u03bc(\u03b8))^j\n                  + \u2211_j=1^d \u2202_x_j h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8)) \u2202_\u03b8_l\u2202_x_i (X\u0303_s^t, x, \u03bc(\u03b8))^j\n         and \n          \n            \u2202_\u03b8_l\u2202^i_\u03bc    [h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8))](v) \n                = \u2211_j = 1^d \u2202_\u03b8_l\u2202_x_j h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8)) \u2202^i_\u03bc (X\u0303_s^t, x, \u03bc(\u03b8))^j\n                  + \u2211_j,k =1^d \u2202^2_x_j, x_k h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8)) \u2202^i_\u03bc (X\u0303_s^t, x, \u03bc(\u03b8))^j  \u2202_\u03b8_l (X\u0303_s^t, x, \u03bc(\u03b8))^k \n                  + \u2211_j,k =1^d \ud835\udd3c[ [\u2202_\u03bc\u2202_x_j h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8))]_k(X_s^t, \u03be(\u03b8)) \u2202_\u03b8_l (X_s^t, \u03be(\u03b8))^k ] \u2202^i_\u03bc (X\u0303_s^t, x, \u03bc(\u03b8))^j\n                   + \u2211_j=1^d \u2202_x_j h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8)) \u2202_\u03b8_l\u2202^i_\u03bc (X\u0303_s^t, x, \u03bc(\u03b8))^j\n                  + \u2211_j=1^d \ud835\udd3c[ \u2202_\u03b8_l\u2202^j_\u03bc h_\u03b8(s, X\u0303_s^t, x,\u03bc(\u03b8), _X\u0303_s^t,\u03be(\u03b8))(X_s^t, v, \u03bc(\u03b8)) \u2202_v_i (X_s^t, v, \u03bc(\u03b8))^j ]  \n                  + \u2211_j, k=1^d \ud835\udd3c[ \u2202_x_k\u2202^j_\u03bc h_\u03b8(s, X\u0303_s^t, x,\u03bc(\u03b8), _X\u0303_s^t,\u03be(\u03b8))(X_s^t, v, \u03bc(\u03b8)) \u2202_v_i (X_s^t, v, \u03bc(\u03b8))^j ] \u2202_\u03b8_l (X\u0303_s^t, x, \u03bc)^k \n                  + \u2211_j, k=1^d   \ud835\udd3c\ud835\udd3c\u030c[ \u2202^k_\u03bc\u2202^j_\u03bc h_\u03b8(s,X\u0303_s^t, x,\u03bc(\u03b8), _X\u0303_s^t,\u03be(\u03b8))(X_s^t, v, \u03bc(\u03b8), X\u030c_s^t, \u03be(\u03b8)) \u2202_v_i (X_s^t, v, \u03bc(\u03b8))^j\u2202_\u03b8_l (X\u030c_s^t, \u03be(\u03b8))^k ]  \n                  + \u2211_j, k =1^d   \ud835\udd3c[ \u2202_v_k\u2202^j_\u03bc h_\u03b8(s, X\u0303_s^t, x,\u03bc(\u03b8), _X\u0303_s^t,\u03be(\u03b8))(X_s^t, v, \u03bc(\u03b8)) \u2202_v_i (X_s^t, v, \u03bc(\u03b8))^j\u2202_\u03b8_l (X_s^t, v, \u03bc(\u03b8))^k ]  \n                  + \u2211_j=1^d \ud835\udd3c[  \u2202^j_\u03bc h_\u03b8(s, X\u0303_s^t, x,\u03bc(\u03b8), _X\u0303_s^t,\u03be(\u03b8))(X_s^t, v, \u03bc(\u03b8)) \u2202_\u03b8_l\u2202_v_i (X_s^t, v, \u03bc(\u03b8))^j ]\n                  + \u2211_j=1^d \u222b_\u211d^d \ud835\udd3c[\u2202_\u03b8_l\u2202^j_\u03bc h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8))(X_s^t, x', \u03bc(\u03b8)) \u2202^i_\u03bc (X_s^t, x', \u03bc(\u03b8))^j(v) ]  \u03bc(x\u0323') \n                  +  \u2211_j, k=1^d \u222b_\u211d^d \ud835\udd3c[\u2202_x_k\u2202^j_\u03bc h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8))(X_s^t, x', \u03bc(\u03b8)) \u2202_\u03b8_l (X\u0303_s^t, x, \u03bc(\u03b8))^k\u2202^i_\u03bc (X_s^t, x', \u03bc(\u03b8))^j(v) ]  \u03bc(x\u0323')\n                  + \u2211_j, k=1^d \u222b_\u211d^d \ud835\udd3c\ud835\udd3c\u030c[ \u2202^k_\u03bc\u2202^j_\u03bc h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8))(X_s^t, x', \u03bc(\u03b8), X\u030c_s^t, \u03be(\u03b8)) \u2202^i_\u03bc (X_s^t, x', \u03bc(\u03b8))^j(v) \u2202_\u03b8_l (X\u030c_s^t, \u03be(\u03b8))^k]  \u03bc(x\u0323')\n                  + \u2211_j, k=1^d \u222b_\u211d^d \ud835\udd3c[ \u2202_v_k\u2202^j_\u03bc h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8))(X_s^t, x', \u03bc(\u03b8)) \u2202_\u03b8_l (X_s^t, x', \u03bc(\u03b8))^k \u2202^i_\u03bc (X_s^t, x', \u03bc(\u03b8))^j(v) ]  \u03bc(x\u0323')\n                  + \u2211_j=1^d \u222b_\u211d^d \ud835\udd3c[  \u2202^j_\u03bc h_\u03b8(s, X\u0303_s^t, x, \u03bc(\u03b8), _X\u0303_s^t, \u03be(\u03b8))(X_s^t, x', \u03bc(\u03b8)) \u2202_\u03b8_l\u2202^i_\u03bc (X_s^t, x', \u03bc(\u03b8))^j(v)]  \u03bc(x\u0323').\n        \n        \n        In the above identity, X\u030c^t,  \u03be_s(\u03b8) stands for a random variable independent of (X\u0303^t, x, \u03bc_s, X_s^t, v, \u03bc, X_s^t, x', \u03bc) with the same law as X\u0303_s^t, \u03be.\n         \n        Then, starting from the expression of _\u03b8 in (<ref>) (with \u03c0=\u03c0_\u03b8), the dominated convergence theorem guarantees that the derivatives (t, \u03b8, x, \u03bc, v) \u21a6\u2202_\u03b8\u2202_x_\u03b8(t, x, \u03bc), \u2202_x\u2202_\u03b8_\u03b8(t, x, \u03bc), \u2202_\u03b8\u2202^2_x_\u03b8(t, x, \u03bc), \u2202^2_x\u2202_\u03b8_\u03b8(t, x, \u03bc), \u2202_\u03b8\u2202_\u03bc_\u03b8(t, x, \u03bc)(v), \u2202_\u03bc\u2202_\u03b8_\u03b8(t, x, \u03bc)(v), \u2202_\u03b8\u2202_v\u2202_\u03bc_\u03b8(t, x, \u03bc)(v), \u2202_v\u2202_\u03bc\u2202_\u03b8_\u03b8(t, x, \u03bc)(v) exist and are locally Lipschitz continuous. Hence, from Clairaut's theorem, we deduce that \u2202_\u03b8\u2202_x_\u03b8(t, x, \u03bc) = \u2202_x\u2202_\u03b8_\u03b8(t, x, \u03bc), \u2202_\u03b8\u2202^2_x_\u03b8(t, x, \u03bc) = \u2202^2_x\u2202_\u03b8_\u03b8(t, x, \u03bc), \u2202_\u03b8\u2202_\u03bc_\u03b8(t, x, \u03bc)(v) = \u2202_\u03bc\u2202_\u03b8_\u03b8(t, x, \u03bc)(v) and \u2202_\u03b8\u2202_v\u2202_\u03bc_\u03b8(t, x, \u03bc)(v) = \u2202_v\u2202_\u03bc\u2202_\u03b8_\u03b8(t, x, \u03bc)(v) for all t, x, \u03bc, \u03b8, v.\n        \n        Moreover, from Assumption <ref> and Lemma <ref>, there exist q and C such that for any t, x, \u03bc, v and any \u03b8\u2208\ud835\udca6, \ud835\udca6 being a compact subset of \u0398\n            |\u2202_\u03b8_\u03b8(t, x, \u03bc)| \u2264 C(1 + |x|^2 + M_2(\u03bc)^q),\n        \n            |\u2202_\u03b8\u2202_x_\u03b8(t, x, \u03bc)| + |\u2202_\u03b8\u2202_\u03bc_\u03b8(t, x, \u03bc)(v)|  \u2264 C ( 1+ |x| + |v|+ M_2(\u03bc)^q),\n         and\n        \n            |\u2202_\u03b8\u2202^2_x_\u03b8(t, x, \u03bc)| + |\u2202_\u03b8\u2202_v\u2202_\u03bc_\u03b8(t, x, \u03bc)(v)|  \u2264 C ( 1+ |v| + M_2(\u03bc)^q).\n        \n        \n        \n        Now, differentiating with respect to \u03b8 both sides of (<ref>), we deduce that \u03b8\u21a6\u2202_t _\u03b8(t, x, \u03bc) is differentiable with a derivative \u2202_\u03b8\u2202_t _\u03b8(t, x, \u03bc) being continuous with respect to t, x, \u03bc, \u03b8. Also, taking \u03c0=\u03c0_\u03b8 and differentiating with respect to \u03b8 both sides of the identity of (<ref>) (using Lemma <ref> together with the estimates (<ref>), (<ref>), (<ref>) and the dominated convergence theorem to differentiate the right-hand side therein) and then passing to the limit as h\u2193 0, we get that t\u21a6\u2202_\u03b8_\u03b8(t, x, \u03bc) is differentiable with a derivative \u2202_t \u2202_\u03b8_\u03b8(t, x, \u03bc) being continuous with respect to t, x, \u03bc, \u03b8.  We thus conclude that the two derivatives \u2202_\u03b8\u2202_t _\u03b8(t, x, \u03bc) and \u2202_t \u2202_\u03b8_\u03b8(t, x, \u03bc) coincide for all t, x, \u03bc, \u03b8.\n        \n        \n        \n        \n        \n         \u00a7.\u00a7 Proof of Theorem <ref>\n        Step 1: We start from the PDE characterisation of _\u03b8 in Proposition <ref> that we write as \n        \n            \u222b_A  {_\u03b8^a[_\u03b8](t,x,\u03bc) + f(x,\u03bc,a) +  \u03bblog p_\u03b8(t,x,\u03bc,a) } \u03c0_\u03b8(\u1ea1|t,x,\u03bc)    =    0,\n         \n        where \n        _\u03b8^a[\u03c6](t,x,\u03bc)     =    - \u03b2\u03c6(t,x,\u03bc)  + \u2202_t\u03c6(t,x,\u03bc)  +  b(x,\u03bc,a) \u00b7 D_x\u03c6(t,x,\u03bc) +  1/2\u03c3\u03c3(x,\u03bc,a) : D_x^2 \u03c6(t,x,\u03bc)  \n                +   _\u03be\u223c\u03bc[   b_\u03b8(t,\u03be,\u03bc) \u00b7\u2202_\u03bc\u03c6(t,x,\u03bc)(\u03be) + 1/2\u03a3_\u03b8(t,\u03be,\u03bc) : \u2202_\u03c5\u2202_\u03bc\u03c6(t,x,\u03bc)(\u03be)    ],\n          recalling that b_\u03b8(t,x,\u03bc)=\u222b_A b(x,\u03bc,a)  \u03c0_\u03b8(\u1ea1|t,x,\u03bc), \u03a3_\u03b8(t,x,\u03bc)   =  \u222b_A (\u03c3\u03c3)(x,\u03bc,a)  \u03c0_\u03b8(\u1ea1|t,x,\u03bc).  \n        \n        For any fixed t, x, \u03bc, we now differentiate w.r.t. \u03b8\u2208\u0398 both sides of (<ref>) to get a new system of linear PDEs satisfied by _\u03b8.\n        In particular, using the identity\n        \n            \u2207_\u03b8[_\u03b8^a[_\u03b8](t,x,\u03bc)] = _\u03b8^a[_\u03b8](t,x,\u03bc) +   _\u03b8[_\u03b8](t,x,\u03bc),\n         together with (<ref>) and the dominated convergence theorem, we get\n        \n            \u222b_A {_\u03b8^a[_\u03b8](t,x,\u03bc)  +   _\u03b8[_\u03b8](t,x,\u03bc) \n            \n             +   [ _\u03b8^a[_\u03b8](t,x,\u03bc) + f(x,\u03bc,a) +  \u03bblog p_\u03b8(t,x,\u03bc,a) ]  \u2207_\u03b8log p_\u03b8(t,x,\u03bc,a)  }\u03c0_\u03b8(\u1ea1|t,x,\u03bc)    =   0,\n         with terminal condition _\u03b8(T,x,\u03bc)=0.\n        \n        \n        \n        \n        \n        \n        \n        Note that we have used the fact that \n        \u222b_A \u2207_\u03b8log p_\u03b8(t,x,\u03bc,a)  \u03c0_\u03b8(\u1ea1|t,x,\u03bc)     =   \u2207_\u03b8\u222b_A \u03c0_\u03b8(\u1ea1|t,x,\u03bc)    =   0,  \n        \n        and  the above PDE is a system of D equations, where  _\u03b8^a[_\u03b8] denotes the operator applied to each component of the ^D-valued function _\u03b8. \n        Step 2: Denote by \n        \n            F\u0303_\u03b8(t,x,\u03bc,a)    =  {_\u03b8^a[_\u03b8](t,x, \u03bc) + f(x,\u03bc,a) + \u03bblog p_\u03b8(t,x,\u03bc,a) }\u2207_\u03b8log p_\u03b8(t,x,\u03bc,a) \n                    +  _\u03b8[_\u03b8](t,x,\u03bc),\n        \n        and\n        f\u0303_\u03c0_\u03b8(t,x,\u03bc)    =   \u222b_AF\u0303_\u03b8(t,x,\u03bc,a)  \u03c0_\u03b8(\u1ea1|t,x,\u03bc),\n         so that the linear PDE (<ref>) satisfied by _\u03b8 now writes \n        _\u03c0_\u03b8[_\u03b8](t,x,\u03bc)  +  f\u0303_\u03c0_\u03b8(t,x,\u03bc)    =    0,\n        \n         with terminal condition _\u03b8(T,x,\u03bc)=0. Observe that the above PDE is similar to (<ref>). In order to obtain the announced probabilistic representation formula, we first apply the chain rule formula on the strip [t, T] \u00d7^d\u00d7_2(^d), see e.g. Proposition 5.102 in <cit.>, to (e^-\u03b2 s_\u03b8(s, X_s^t,x, \u03bc,_X_s^t, \u03be))_s \u2208 [t, T] using the estimates (<ref>) and (<ref>). We thus obtain\n        \n            d(e^-\u03b2 s_\u03b8(s, X_s^t,x, \u03bc,_X_s^t, \u03be))     = - e^-\u03b2 sf\u0303_\u03c0_\u03b8(s, X_s^t,x,\u03bc,_X_s^t,\u03be)  \u1e63\n                + e^-\u03b2 s\u2202_x _\u03b8(s,  X_s^t, x, \u03bc,_X_s^t, \u03be)\u03c3_\u03c0_\u03b8(X_s^t, x, \u03bc, _X_s^t,\u03be) \u1e88_s.\n        \n        \n        Observe that (<ref>) together with the fact that for any \u03b8\u2208^D, |\u03c3_\u03c0_\u03b8(x, \u03bc)| \u2264 C(1+|x|+M_2(\u03bc)), for some constant C, directly yields that the stochastic integral is a square integrable martingale. Hence, integrating from t to T both sides of the above and using the facts that _\u03b8(T,x,\u03bc)=0 and \u2119_X_s^t, \u03be = \u2119_X_s^t, \u03be, \u2119_X_s^t, x, \u03bc = \u2119_X_s^t, x, \u03bc , we eventually deduce\n        \n            _\u03b8(t,x,\u03bc)    =  _\u03b1\u223c\u03c0_\u03b8[ \u222b_t^T  e^-\u03b2(s-t)F\u0303_\u03b8(s, X_s^t,x,\u03bc,_ X_s^t,\u03be, \u03b1_s)   \u1e63].\n        Step 3: On the other hand,  applying again the chain rule formula to _\u03b8(s,X_s^t,x,\u03bc,_X_s^t,\u03be), when  \u03b1\u223c\u03c0_\u03b8, see e.g. Proposition 5.102 in <cit.>, we have \n        \n        _\u03b8(s, X_s^t,x,\u03bc, _ X_s^t,\u03be) \n        \n           =   \n        ( ^\u03b1_s_\u03b8[_\u03b8](s, X_s^t, x, \u03bc, _ X^t, \u03be_s)  + \u03b2_\u03b8(s, X_s^t,x,\u03bc,_ X_s^t,\u03be) )  \u1e63\n                  +  \n        D_x  _\u03b8(s, X_s^t,x,\u03bc,_ X_s^t,\u03be)\u03c3( X_s^t,x,\u03bc,_ X_s^t,\u03be, \u03b1_s)  \u1e88_s,    t \u2264 s \u2264 T,  \n        \n        and thus by definition of F\u0303_\u03b8\n            \u222b_t^T  e^-\u03b2(s-t)F\u0303_\u03b8(s, X_s^t,x,\u03bc,_ X_s^t,\u03be, \u03b1_s)   \u1e63\n                =  \u222b_t^T e^-\u03b2(s-t)\u2207_\u03b8log(p_\u03b8(s,  X_s^t, x, \u03bc, _ X_s^t, \u03be, \u03b1_s)) (_\u03b8(s,  X_s^t, x, \u03bc, _X_s^t, \u03be) - \u03b2_\u03b8(s,  X_s^t, x, \u03bc, _ X_s^t, \u03be) \n                  + _\u03b8[_\u03b8](s,X_s^t, x, \u03bc, _X_s^t, \u03be) )  \u1e63\n                + \u222b_t^T  e^-\u03b2(s-t)\u2207_\u03b8log(p_\u03b8(s,  X_s^t, x, \u03bc, _ X_s^t, \u03be, \u03b1_s)) ( f(X_s^t,x,\u03bc,_ X_s^t,\u03be, \u03b1_s) + \u03bblog(p_\u03b8 (s,  X_s^t, x, \u03bc, _ X_s^t, \u03be, \u03b1_s) )   )  \u1e63\n                - \u222b_t^T e^-\u03b2(s-t)\u2207_\u03b8log(p_\u03b8(s,  X_s^t, x, \u03bc, _ X_s^t, \u03be, \u03b1_s))    D_x  _\u03b8(s, X_s^t,x,\u03bc,_ X_s^t,\u03be)\u03c3( X_s^t,x,\u03bc,_ X_s^t,\u03be, \u03b1_s)    \u1e88_s.\n        \n        \n        Note that (<ref>) as well as the bound |D_x V_\u03b8(s, x, \u03bc)|\u2264 C(1+|x|+|\u03bc|^q), for some q\u22650, directly deduced from the identity (<ref>) and Assumption <ref>, guarantees that the stochastic integral appearing in the right-hand side of the above identity is a square integrable martingale. Hence, taking expectation in both sides of the above identity eventually yields\n        \n        \n            G_\u03b8(t,x,\u03bc)     :=  _\u03b1\u223c\u03c0_\u03b8[ \u222b_t^T  e^-\u03b2(s-t)\u2207_\u03b8log p_\u03b8(s,X_s^t,x,\u03bc,_X_s^t,\u03be,\u03b1_s) {_\u03b8(s,X_s^t,x,\u03bc,_X_s^t,\u03be) \n                 +  [  f(X_s^t,x,\u03bc,_X_s^t,\u03be,\u03b1_s) +  \u03bblog p_\u03b8(s,X_s^t,x,\u03bc,_X_s^t,\u03be,\u03b1_s)  - \u03b2_\u03b8(s,X_s^t,x,\u03bc,_X_s^t,\u03be) ] \u1e63}\n                +  \u222b_t^T e^-\u03b2(s-t)_\u03b8[_\u03b8](s,X_s^t,x,\u03bc,_X_s^t,\u03be)   \u1e63].\n        \n        \n        \n        \n         \n        \n        \n        This proves the announced probabilistic representation formula for G_\u03b8. \n        \n        \n        \u00a7 LINEAR QUADRATIC MEAN-FIELD CONTROL WITH RANDOMISED CONTROLS AND ENTROPY REGULARISATION\n         \n        \n        \n        A  stochastic policy is  a probability  transition kernel from [0,T]\u00d7^d\u00d7_2(^d) into A=^m, i.e., a  measurable function \u03c0:(t,x,\u03bc)\u2208[0,T]\u00d7^d\u00d7_2(^d)\u21a6\u03c0(.|t,x,\u03bc)\u2208(^m).  We denote by \u03a0 the set of stochastic policies \n        \u03c0 with densities p  with respect to the Lebesgue measure on ^m: \u03c0( \u1ea1 |t,x,\u03bc)=p(t,x,\u03bc,a)\u1ea1.  \n        We say that the process \u03b1=(\u03b1_t)_t  is a randomised feedback control generated from a stochastic  policy \u03c0\u2208\u03a0, denoted by \u03b1\u223c\u03c0, if at each time t, the action \u03b1_t is sampled (according to the \u03c3-algebra ) \n        from the probability distribution \u03c0(.|t,X_t,_X_t). The dynamics X=X^\u03b1 follows a linear mean-field dynamics with coefficients b(x,\u03bc,a)=b\u0305(x,\u03bc\u0305,a),   \u03c3(x,\u03bc,a)=\u03c3\u0305(x,\u03bc\u0305,a) in the form\n        b\u0305(x,x\u0305,a)   =    B x + B\u0305x\u0305 +  C a,    \u03c3\u0305(x,x\u0305,a)   =  \u03b3 + D x + D\u0305x\u0305 + Fa, \n        \n        for (x,\u03bc,x\u0305,a)\u2208^d\u00d7_2(^d)\u00d7^d\u00d7^m, where we denote by \u03bc\u0305=\u222b x \u03bc(x\u0323),  B, B\u0305, D, D\u0305 are constant matrices in ^d\u00d7 d, C, F are constant matrices in ^d\u00d7 m, \u03b3 is a constant in ^d.  \n        \n        Given a stochastic policy \u03c0\u2208\u03a0, we consider the functional cost V^\u03c0 with entropy regulariser defined in (<ref>) with quadratic functions f(x,\u03bc,a)=f\u0305(x,\u03bc\u0305,a) and g(x,\u03bc)=g\u0305(x,\u03bc\u0305):\n        f\u0305(x,x\u0305,a)   =   x Qx + x\u0305Q\u0305x\u0305  + a N a  + 2a Ix + 2aI\u0305x\u0305 +  2M.x + 2H.a, \n        g\u0305(x,x\u0305)   =   x P x + x\u0305P\u0305x\u0305 + 2L.x,\n        \n        where N is a  symmetric matrix in _+^m, I, I\u0305\u2208^m\u00d7 d,  Q, Q\u0305, P, P\u0305 are  symmetric matrices in  ^d,  \n        \n        \n        \n        \n        \n        M, L\u2208^d, H\u2208^m, assumed to satisfy the conditions: \n        \n        \n         (H1) (i) There exists \u03b4>0 s.t. \n        \n        N  \u2265 \u03b4 I_m,    P    \u2265  0,    Q  - I N^-1I  \u2265  0.  \n        \n        or (ii)  n=m=1, I=0, F\u22600, Q\u22650,  P>0. \n        \n        \n         (H2) (i) There exists \u03b4>0 s.t. \n        \n        N  \u2265 \u03b4 I_m,    P + P\u0305 \u2265  0,    (Q + Q\u0305)  -  (I + I\u0305) N^-1 (I + I\u0305)  \u2265  0.  \n        \n        or (ii)  I+I\u0305=0, F\u22600,  Q + Q\u0305\u22650,  P+P\u0305\u22650, P>0. \n        \n        \n        \n        \n        \n        \n        \n        The solution to the LQ mean-field control problem with entropy regulariser is then given by the following theorem:\n        \n        \n        \n        Let Assumptions  (H1)- (H2) hold. Then, the value function is equal to \n        \n        v(t,x,\u03bc)   :=  inf_\u03c0\u2208\u03a0 V^\u03c0(t,x,\u03bc)    =     (x-\u03bc\u0305) K(t)(x-\u03bc\u0305)+\u03bc\u0305\u039b(t)\u03bc\u0305+2Y(t) x+R(t),\n        \n        for (t,x,\u03bc)\u2208[0,T]\u00d7^d\u00d7_2(^d), where  the quadruple (K,\u039b,Y,R) valued in (_+^d,_+^d,^d,) is solution on [0,T] to the system of Riccati equations:\n        \n            K\u0307(t) - \u03b2 K(t) +Q+K(t)B+B K(t)+D K(t) D \n                       -(I+C K(t)+F K(t) D) (N+F K(t) F)^-1(I+C K(t)+F K(t) D)      =   0,  \n            \n                       \u039b\u0307(t) -\u03b2\u039b(t) + Q\u0302 +\u039b(t)B\u0302 +B\u0302\u039b(t)+D\u0302 K(t)D\u0302\n                - (\u00ce +C\u039b(t)+F K(t)D\u0302)(N+F K(t) F)^-1(\u00ce +C\u039b(t)+F K(t)D\u0302)   =   0  \n            \n                       \u1e8e(t) -\u03b2 Y(t) +M+B\u0302 Y(t)+ D\u0302 K(t)\u03b3\n                -(\u00ce +C\u039b(t)+F K(t) D\u0302)(N+F K(t) F)^-1(H+C Y(t)+F K(t)\u03b3)    =   0\n            \n            \n                       \u1e58(t)  -\u03b2 R(t)  +\u03b3 K(t)\u03b3  +  \u03bb m/2log(2\u03c0)-\u03bb/2log|\u03bb/2 det(N+F K(t) F)|\n                -(H+C Y(t)+F K(t)\u03b3)(N+F K(t) F)^-1(H+C Y(t)+F K(t)\u03b3)    =   0\n        \n        with the terminal condition (K(T),\u039b(T),Y(T),R(T))=(P,P\u0302,L,0), where we set \u00ce:=I+I\u0305, B\u0302:=B+B\u0305, D\u0302:=D+D\u0305,  \n        Q\u0302:=Q + Q\u0305, P\u0302:=P+P\u0305.  \n        \n        Moreover, the optimal stochastic policy follows a Gaussian distribution: \n        \n            \u03c0^*(.|t,x,\u03bc)     =   ( - S(t)^-1(  U(t) x + (\u00db(t) - U(t))\u03bc\u0305+ O(t) ); \u03bb/2 S(t)^-1),\n         \n        where we set \n        \n        S(t)   :=   N + F K(t) F,        O(t)   :=    H + C Y(t)  + F K(t) \u03b3\n        \n        U(t)   :=   I + C K(t) + F K(t) D,        \u00db(t)   :=  \u00ce  + C \u039b(t) + F K(t) D\u0302. \n        \n        Conditions  (H1) and  (H2) ensure the existence and uniqueness of a solution (K,\u039b) to the matrix Riccati equation in (<ref>) satisfying K\u22650, \u039b\u22650 (hence S(t)^-1 is well-defined). Given (K,\u039b), the equations for \n        (Y,R)  are simply linear ODEs. \n        \n        \n        Let ^r be the set of _2(^m)-valued -adapted processes \u03c0=(\u03c0_t)_t with densities p=(p_t)_t w.r.t. to the Lebesque measure: \u03c0_t(\u1ea1)=p_t(a) \u1ea1.   \n        We say that an ^m-valued  process \u03b1=(\u03b1_t)_t  is a randomised (open-loop) control generated from \u03c0\u2208^r, denoted by \u03b1\u223c\u03c0, if at each time t, the decision \u03b1_t is sampled from the probability distribution \u03c0_t.  \n        \n        We then consider the problem of minimizing over randomised controls \u03b1\u223c\u03c0\u2208^r the cost functional with entropy regulariser \n        \n        J(\u03c0)    =   _\u03b1\u223c\u03c0[ \u222b_0^T e^-\u03b2 t(  f(X_t,_X_t,\u03b1_t)  + \u03bb\u222b_^mlog p_t(a) \u1ea1)  \u1e6d +  e^-\u03b2 T g(X_T,_X_T) ], \n        \n        where the process X=X^\u03b1 follows a linear mean-field dynamics, and f, g are quadratic cost functions, namely: \n        \n            b(x,\u03bc,a)   =    B x + B\u0305\u03bc\u0305+  C a,    \u03c3(x,\u03bc,a)   =  \u03b3 + D x + D\u0305\u03bc\u0305+ Fa,  \n            \n            f(x,\u03bc,a)   =   x Qx + \u03bc\u0305Q\u0305\u03bc\u0305+ a N a  + 2a Ix + 2aI\u0305\u03bc\u0305+  2M.x + 2H.a, \n            \n            g(x,\u03bc)   =   x P x + \u03bc\u0305P\u0305\u03bc\u0305+ 2L.x,\n         \n        for (x,\u03bc,a)\u2208^d\u00d7_2(^d)\u00d7^m, where we denote by \u03bc\u0305=\u222b x \u03bc(x\u0323),  B, B\u0305, D, D\u0305 are constant matrices in ^d\u00d7 d, C, F are constant matrices in ^d\u00d7 m, \u03b3 is a constant in ^d, \n        N is a  symmetric matrix in _+^m, I, I\u0305\u2208^m\u00d7 d,  Q, Q\u0305, P, P\u0305 are  symmetric matrices in  ^d,  with Q\u22650, P\u22650, \n        \n        \n        \n        \n        M, L\u2208^d, H\u2208^m. \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n         Proof of Theorem <ref>.  We adapt the arguments in  <cit.> to our case with randomised controls and entropy regulariser.  \n        \n         Step 1. Let us consider the function defined on [0,T]\u00d7^d\u00d7_2(^d) by w(t,x,\u03bc)=w\u0305(t,x,\u03bc\u0305), where w\u0305 is defined on [0,T]\u00d7^d\u00d7^d by \n        w\u0305(t,x,x\u0305)    =    (x-x\u0305) K(t)(x-x\u0305)+x\u0305\u039b(t)x\u0305+2Y(t) x+R(t),\n        \n        for some functions (to be determined later)  K, \u039b, Y and R on [0,T], and valued on _+^d, _+^d, ^d, and .  Fix (t_0,x_0,\u03bc_0)\u2208[0,T]\u00d7^d\u00d7_2(^d), and \u03be_0\u2208L^2(_t_0;^d)\u223c\u03bc_0.   Given \u03c0\u2208\u03a0 with density p,  and a randomised control \u03b1\u223c\u03c0, we consider the process \n        _t^\u03b1   :=     e^-\u03b2(t-t_0)w\u0305(t,X_t^t_0,x_0,\u03bc_0,X\u0305_t^t_0,\u03bc_0) + \u222b_t_0^t e^-\u03b2(s-t_0)[  f\u0305(X_s^t_0,x_0,\u03bc_0,X\u0305_s^t_0,\u03bc_0,\u03b1_s)   \n               +  \u03bb\u222b_^m(log_t(a) ) _t(a) \u1ea1\n        ]  \u1e63,    \n        \n        for t_0\u2264 t \u2264 T, where we set _t(a)=p(t,X_t^t_0,x_0,\u03bc_0,_X_t^t_0,\u03be_0,a), and \n        X\u0305_t^t_0,\u03bc_0:=_\u03b1\u223c\u03c0[X_t^t_0,\u03be_0] which follows the dynamics:\n        \n        d X\u0305_t    =   ( B\u0302X\u0305_t  + C \u03b1\u0305_t) \u1e6d, \n        \n        with \u03b1\u0305_t:=_\u03b1\u223c\u03c0[\u03b1_t]. \n        \n         Step 2.  We apply It\u00f4's formula to _t^\u03b1 for \u03b1\u223c\u03c0, and take the expectation to get\n        \n            _\u03b1\u223c\u03c0[_t^\u03b1]    =    e^-\u03b2(t-t_0)_\u03b1\u223c\u03c0[_t^\u03b1] \u1e6d,\n         \n        with \n        _t^\u03b1   =     - \u03b2w\u0305(t,X_t,X\u0305_t) + /\u1e6d_\u03b1\u223c\u03c0[w\u0305(t,X_t,X\u0305_t)] + f\u0305(X_t,X\u0305_t,\u03b1_t)   + \u03bb\u222b_^m (log_t(a)) _t(a) \u1ea1,\n        \n        where we omit the dependence on t_0,x_0,\u03bc_0 of X and X\u0305 to alleviate notations.  By applying It\u00f4's formula to w\u0305(t,X_t,X\u0305_t), recalling the quadratic forms of w\u0305, f\u0305, and using the linear dynamics of X and X\u0305, we obtain similarly as in <cit.> (after careful but straightforward computations): \n        \n            _\u03b1\u223c\u03c0[_t^\u03b1]    =     _\u03b1\u223c\u03c0[  \n            (X_t-X\u0305_t)(K\u0307(t) - \u03b2 K(t) + Q  +K(t)B+B K(t)+D K(t) D)(X_t-X\u0305_t)  \n                 +  X\u0305_t( \u039b\u0307(t) - \u03b2\u039b(t) + Q\u0302 +  \u039b(t) B\u0302 + B\u0302\u039b(t)+ D\u0302 K(t)D\u0302)X\u0305_t  \n                 +   2(\u1e8e(t) - \u03b2 Y(t) + M  + B\u0302 Y(t)+D\u0302 K(t)\u03b3) X_t +  \u1e58(t) - \u03b2 R(t) +\u03b3 K(t)\u03b3\n                 +  \u03b1_t S(t) \u03b1_t +  2\u03b1_t(  U(t) (X_t - X\u0305_t)  +   \u00db(t) X\u0305_t + O(t) )  +  \u03bb\u222b_^m (log_t(a)) _t(a) \u1ea1] \n              =     _\u03b1\u223c\u03c0[  \n            (X_t-X\u0305_t)(K\u0307(t) - \u03b2 K(t) + Q  +K(t)B+B K(t)+D K(t) D)(X_t-X\u0305_t)  \n                 +  X\u0305_t( \u039b\u0307(t) - \u03b2\u039b(t) + Q\u0302 +  \u039b(t) B\u0302 + B\u0302\u039b(t)+ D\u0302 K(t)D\u0302)X\u0305_t  \n                 +   2(\u1e8e(t) - \u03b2 Y(t) + M  + B\u0302 Y(t)+D\u0302 K(t)\u03b3) X_t +  \u1e58(t) - \u03b2 R(t) +\u03b3 K(t)\u03b3\n               \n              +  \u222b_^m [ \u03d5_t(a) +  \u03bblog_t(a) ] _t(a) \u1ea1],\n         \n        where we used in the last equality the fact that \u03b1\u223c\u03c0, and set  \u03d5_t(a):=a S(t) a  +  2 a \u03c7_t with \n        \u03c7_t:=U(t) (X_t - X\u0305_t)  +   \u00db(t) X\u0305_t + O(t). \n        \n        \n        \n         Step 3.  Let \u03d5 be a quadratic function on ^m: \u03d5(a)=a S a + 2 a\u03c7  for some positive-definite matrix S\u2208_+^m, and \u03c7\u2208^m, and denote by  \n        _2(^m) the set of square integrable density functions on ^m, i.e., the set of nonnegative measurable functions  on ^m s.t. \u222b_^m(a) \u1ea1=1, and \u222b_^m |a|^2 (a) \u1ea1<\u221e.  Let us consider the cost functional on _2(^m) defined by\n        \n        C_\u03d5()     :=    \u222b_^m [ \u03d5(a) + \u03bblog(a) ] (a) \u1ea1. \n        \n        Then, the minimizer of C_\u03d5 is achieved with ^*\u2208_2(^m) given by \n        \n            ^*(a)    =  exp(-1/\u03bb\u03d5(a) )/\u222b_^mexp(-1/\u03bb\u03d5(a) ) \u1ea1,    a \u2208^m.\n         \n        Indeed, by considering the Lagrangian function associated to  this  minimization problem \n        \n        L_\u03d5(,\u03bd)    =    C_\u03d5() - \u03bd( \u222b_^m(a)\u1ea1 - 1 )   =  \u222b_^m[ \u03d5(a) + \u03bblog(a) - \u03bd] (a) \u1ea1   +   \u03bd, \n        \n        for (,\u03bd)\u2208_2(^m)\u00d7, we see that the minimization over  is obtained pointwisely, i.e. inside the integral over a\u2208^m, hence leading to the first-order equations: \n        \n            \u03d5(a) + \u03bblog^*(a) - \u03bd^* +  \u03bb  =    0,    a \u2208^m,  \n            \u222b_a \u2208^m^*(a) \u1ea1  =   1.\n         \n        This yields the expression of ^* in (<ref>), which is  actually the density of  a Gaussian distribution \n        \n            \u03c0^*    =  ( - S^-1\u03c7; \u03bb/2 S^-1).\n         \n        The infimum of C_\u03d5 is then equal to \n        \n            inf_\u2208_2(^m) C_\u03d5()    =    C_\u03d5(^*)   =    - \u03c7 S^-1\u03c7  - \u03bb m/2log(2\u03c0)-\u03bb/2log|\u03bb/2det(S)|.\n        \n         Step 4.  Notice that under  (H1), the matrix  S(t)=N+F K(t) F is positive-definite for K\u22650,  and _t(.)\u2208_2(^m) a.s. for t\u2208[t_0,T]. \n        From (<ref>) and (<ref>), we then have for all \u03c0\u2208\u03a0, \n        \n            _\u03b1\u223c\u03c0[_t^\u03b1]  \n              =     _\u03b1\u223c\u03c0[  \n            (X_t-X\u0305_t)(K\u0307(t) - \u03b2 K(t) + Q  +K(t)B+B K(t)+D K(t) D)(X_t-X\u0305_t)  \n                 +  X\u0305_t( \u039b\u0307(t) - \u03b2\u039b(t) + Q\u0302 +  \u039b(t) B\u0302 + B\u0302\u039b(t)+ D\u0302 K(t)D\u0302)X\u0305_t  \n                 +   2(\u1e8e(t) - \u03b2 Y(t) + M  + B\u0302 Y(t)+D\u0302 K(t)\u03b3) X_t +  \u1e58(t) - \u03b2 R(t) +\u03b3 K(t)\u03b3\n                 +   C_\u03d5_t(_t) ] \n             \u2265    _\u03b1\u223c\u03c0[  \n            (X_t-X\u0305_t)(K\u0307(t) - \u03b2 K(t) + Q  +K(t)B+B K(t)+D K(t) D   - U(t) S(t)^-1 U(t) )(X_t-X\u0305_t)  \n                 +  X\u0305_t( \u039b\u0307(t) - \u03b2\u039b(t) + Q\u0302 +  \u039b(t) B\u0302 + B\u0302\u039b(t)+ D\u0302 K(t)D\u0302  - \u00db(t) S(t)^-1\u00db(t)  )X\u0305_t  \n                 +   2(\u1e8e(t) - \u03b2 Y(t) + M  + B\u0302 Y(t)+D\u0302 K(t)\u03b3  - O(t) S(t)^-1\u00db(t) ) X_t \n                 +  \u1e58(t) - \u03b2 R(t) +\u03b3 K(t)\u03b3 - O(t) S(t)^-1O(t) - \u03bb m/2log(2\u03c0)-\u03bb/2log|\u03bb/2det(S(t))|.\n         \n        Therefore, by taking (K,\u039b,Y,R) solution to (<ref>), we  see that the r.h.s. of (<ref>) vanishes, which means that for all \u03c0\u2208\u03a0, _\u03b1\u223c\u03c0[_t^\u03b1]\u22650.  Moreover, from (<ref>), the equality in (<ref>) holds true for the choice of \u03c0^*\u2208\u03a0 as defined in \n        (<ref>), and thus\n        inf_\u03c0\u2208\u03a0_\u03b1\u223c\u03c0[_t^\u03b1]    =   _\u03b1\u223c\u03c0^*[_t^\u03b1]   =   0,     t \u2208 [t_0,T]. \n         \n        From (<ref>), this means that the function t\u21a6_\u03b1\u223c\u03c0[_t^\u03b1] is nondecreasing on [t_0,T] for any \u03c0\u2208\u03a0, and constant on [t_0,T] for \u03c0=\u03c0^*. By definition of ^\u03b1, V^\u03c0,  and noting that w\u0305(T,x,x\u0305)=g\u0305(x,x\u0305) from the terminal condition on (K,\u039b,Y,R),  \n        it follows  that \n        \n            w(t_0,x_0,\u03bc_0)   =  w\u0305(t_0,x_0,\u03bc\u0305_0)   =  _\u03b1\u223c\u03c0[S_t_0^\u03b1]     \u2264 _\u03b1\u223c\u03c0[S_T^\u03b1]   =   V^\u03c0(t_0,x_0,\u03bc_0),\n         \n        for any \u03c0\u2208\u03a0, with equality   in (<ref>) for \u03c0=\u03c0^*.  We conclude that \n        inf_\u03c0\u2208\u03a0 V^\u03c0(t_0,x_0,\u03bc_0)     =     V^\u03c0^* (t_0,x_0,\u03bc_0)   =    w(t_0,x_0,\u03bc_0) \n           =    (x_0-\u03bc\u0305_\u03050\u0305) K(t_0)(x_0-\u03bc\u0305_\u03050\u0305)+\u03bc\u0305_\u03050\u0305\u039b(t_0)\u03bc\u0305_\u03050\u0305+2Y(t_0) x_0+R(t_0). \n        plain\n"}