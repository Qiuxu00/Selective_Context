{"entry_id": "http://arxiv.org/abs/2303.07141v1", "published": "20230313140940", "title": "An Improved Baseline Framework for Pose Estimation Challenge at ECCV 2022 Visual Perception for Navigation in Human Environments Workshop", "authors": ["Jiajun Fu", "Yonghao Dang", "Ruoqi Yin", "Shaojie Zhang", "Feng Zhou", "Wending Zhao", "Jianqin Yin"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\nheadings\n16SubNumber***  \n\n \n\n\n\n\n\n\nBeijing University of Posts and Telecommunications, Beijing, China\n\n\nAn Improved Baseline Framework for Pose Estimation Challenge at ECCV 2022 Visual Perception for Navigation in Human Environments Workshop\n    Jiajun Fu, Yonghao Dang, Ruoqi Yin, Shaojie Zhang, Feng Zhou, Wending Zhao, Jianqin YinCorresponding Author\n    March 30, 2023\n=========================================================================================================================================\n\n\n\n\nThis technical report describes our first-place solution to the pose estimation challenge at ECCV 2022 Visual Perception for Navigation in Human Environments Workshop. In this challenge, we aim to estimate human poses from in-the-wild stitched panoramic images. Our method is built based on Faster R-CNN <cit.> for human detection, and HRNet <cit.> for human pose estimation. We describe technical details for the JRDB-Pose dataset, together with some experimental results. In the competition, we achieved 0.303 OSPA_IOU and 64.047% AP_0.5 on the test set of JRDB-Pose.\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nPose estimation plays an important role in computer vision, which serves as a fundamental task for high-level human action analysis, such as action recognition <cit.>, action assessment <cit.>, and human-robot interaction <cit.>. In recent years, there has been a growing interest in developing pose estimation models that can accurately detect and estimate human poses in wild scenarios. Although many large-scale datasets <cit.> have been introduced in recent years, no dataset primarily targets robotic perception tasks in social navigation environments, and thus rarely reflects the specific challenges found in human-robot interaction and robot navigation in crowded human environments. The JRDB-Pose dataset <cit.> aims at bridging this gap and sets up a new benchmark for pose estimation.\n\nThis technical report focuses on adopting image-based pose estimation, where bottom-up and top-down methods are two commonly used approaches. In bottom-up methods, the pose of individual body parts is first estimated and then grouped to form a complete pose. On the other hand, top-down methods first detect the entire body and then estimate the pose of each body part. A main strength of top-down methods is their ability to incorporate high-level information about the body, such as prior knowledge about human anatomy and shape. This makes them more robust to occlusion and deformation of body parts, which can be a challenge for bottom-up methods. Moreover, panoramic images contain lots of people with different scales. Compared with bottom-up methods, top-down methods are more robust to multi-scale targets. Therefore, we adopt a classical top-down method-HRNet <cit.> for human pose estimation. Details are given in the remainder of this report.\n\n\n\n\n\u00a7 METHODS\n\n\nWe directly adopted Faster R-CNN for human detection and HRNet for pose estimation[For detailed network architectures, please check the original papers <cit.>.]. These two networks rely heavily on pre-trained models. For the HRNet, a standard initialization setting is to load pre-trained weights from the COCO dataset <cit.>. However, the annotated keypoint locations in the COCO dataset are different from the ones in the JRDB-Pose dataset. To handle this keypoint mismatch, we modified the weights of the last convolution (from the feature space to the keypoint space). For a keypoint in the JRDB-Pose dataset, we find the nearest counterpart(s) in the COCO dataset and copy the last convolution weights for this counterpart. If there are multiple counterparts, we utilize the average weights. We name this nearest-match-based weight initialization. The detailed correspondences are shown in Table <ref>.\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n \u00a7.\u00a7 Implementation Details\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Dataset.\n For 2022's challenge of human pose estimation, the JRDB-Pose dataset <cit.> provides 56,000 stitched panoramic images and 600,000 bounding box annotations, making JRDB-Pose one of the most extensive publicly available datasets that provide ground truth human body pose annotations. Moreover, these annotations come from in-the-wild videos and incorporate heavily occluded poses, which makes JRDB-Pose challenging and an accurate representation of the real-world environment. For the needs of the challenge, we select training images in the \"Bytes-coffee\", \"Huang\", \"Huang-hall\", \"Huang-lane\", and \"Jordan-hall\" as the validation set. The rest images are used as the training set. Please refer to the JRDB-Pose paper <cit.> for the detailed calculation of evaluation metrics.\n\n\n\n  \u00a7.\u00a7.\u00a7 Training.\n We trained the detection model and pose estimation model, separately. Each model is trained in an end-to-end manner.\n\nFor the detection model, we trained the Faster R-CNN <cit.> using ResNet-50 <cit.> as the backbone. It adopted SGD with a mini-batch of four on four 3090 GPUs and trained it for 50 epochs with a base learning rate of 0.02, which is decreased by a factor of 10 at epochs 30 and 45. We performed a linear warm-up <cit.> during the first 1500 iterations. MMDetection <cit.> was used for implementation. Besides, we directly utilized the stitched panoramic images and extracted ground truth bounding boxes based on corresponding poses. We performed a random left/right shift and removed bounding boxes across the left and right sides of the image. \n\nFor the pose estimation model, we trained the HRNet-w48 <cit.> using the pre-trained model on COCO <cit.> as initialization weights. It adopted SGD with a mini-batch of 64 on two 3090 GPUs and trained it for 210 epochs with a base learning rate of 0.0005, which is decreased by a factor of 10 at epochs 170 and 200. We performed a linear warm-up <cit.> during the first 500 iterations. MMPose <cit.> was used for implementation. Besides, we cropped the input images based on corresponding poses and resized the images into 384 \u00d7 288. \n\n\n\n  \u00a7.\u00a7.\u00a7 Inference.\n During testing, we first extracted bounding boxes from the detector model and performed pose estimation. Specifically, we directly input the stitched panoramic images to the Faster R-CNN model. Afterward, we utilized non-maximal suppression (NMS) to filter overlapping detections. The rest detections were resized into 384 \u00d7 288 and fed into HRNet-w48 to generate pose estimation results. For each model, we utilize the checkpoint with the best performance on the validation set.\n\n\n\n\n\n \u00a7.\u00a7 Results.\n\n\nWe compare our model with the state-of-the-art methods on the JRDB-Pose test set. Table <ref> shows that our framework outperforms existing methods by a large margin. We show some visualization results in Figure <ref>.\n\n\n\n\n\n \u00a7.\u00a7 Ablation Studies\n\n\nIn this section, we show the effectiveness of the weight initialization and the human detector.\n\nFor the weight initialization, we evaluated the adopted initialization in the validation set. Specifically, we tested three initialization and the results are presented in Table <ref>. The \u201cImageNet\" indicates initializing image backbone weights from ImageNet <cit.>. This initialization performed the worst since the decoders in HRNet were initialized randomly and get no prior keypoint information. The \u201cCOCO + Random\" represents COCO weight initialization except for the last convolution, whose weights were initialized randomly. The performance is slightly worse than our initialization method (\u201cCOCO + Match\").\n\nFor the human detector, we tested bounding boxes from different approaches on the test set. The results are shown in Table <ref>. The \u201cBaseline\" indicates pose estimation results from the official bounding boxes. There are many redundant boxes and some bounding boxes are small to cover enough image cues. With the NMS and implemented human detector, the performance improves a lot on the test set.\n\n\n\n[t]0.48\ncaptypetable\nImpact of weight initialization\n\n\n\nMethod     AP_0.5\u2191 \n\nImageNet     92.892% \n\nCOCO + Random     96.482% \n\nCOCO + Match     97.968% \n\n\n\n[t]0.48\ncaptypetable\nImpact of human detector\n\n\n\nName     OSPA_IOU\u2193     AP_0.5\u2191 \n\nBaseline     0.406     34.186% \n\nOurs     0.303     64.047% \n\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\nWe demonstrate our solution to the pose estimation challenge in this technical report. Despite simple, it has achieved promising results and has the potential for further improvements.\n\nsplncs\n\n"}