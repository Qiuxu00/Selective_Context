{"entry_id": "http://arxiv.org/abs/2303.06710v2", "published": "20230312172254", "title": "Decision Making for Human-in-the-loop Robotic Agents via Uncertainty-Aware Reinforcement Learning", "authors": ["Siddharth Singi", "Zhanpeng He", "Alvin Pan", "Sandip Patel", "Gunnar A. Sigurdsson", "Robinson Piramuthu", "Shuran Song", "Matei Ciocarlie"], "primary_category": "cs.RO", "categories": ["cs.RO", "cs.AI", "cs.LG"], "text": "\n\n[\n    [\n    March 30, 2023\n==================\n\nempty\nempty\n\n\n\n In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly autonomously in solving a task, but can request help from an external expert when needed. However, knowing when to request such assistance is critical: too few requests can lead to the robot making mistakes, but too many requests can overload the expert. In this paper, we present a Reinforcement Learning based approach to this problem,  where a semi-autonomous agent asks for external assistance when it has low confidence in the eventual success of the task. The confidence level is computed by estimating the variance of the return from the current state. We show that this estimate can be iteratively improved during training using a Bellman-like recursion. On discrete navigation problems with both fully- and partially-observable state information, we show that our method makes effective use of a limited budget of expert calls at run-time, despite having no access to the expert at training time. \n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\nDeep Reinforcement Learning (DRL) has shown great progress in learning decision-making for complex robotic skills <cit.> using experiences collected by a robotic agent exploring an environment and receiving reward signals. Traditional RL agents  use a policy learned during training in order to act autonomously at deployment. However, even a well-trained agent can encounter situations when deployed that are hard to make decisions for, for reasons such as partial state observability, uncertain dynamics, changes in state distributions between training and testing, etc.\n\nThe Human-in-the-Loop (HitL) paradigm has been developed in robotics precisely for situations where an agent can act autonomously most of the time, but would still benefit from receiving assistance from an available (tele-)operator, usually assumed to be a human expert. This paradigm is particularly powerful if the agent itself makes the decision of when to request assistance, thus freeing the operator from having to monitor task progress. However, this approach gives rise to a critical decision-making problem on the agent's part: when to request help? Too few\nsuch requests can lead to the robot making mistakes, but too many\nrequests will overload the expert and lose the benefit of semi-autonomous operation.\n\nIn this paper, we propose a DRL-based method for a HitL agent to make the critical determination of when to request expert assistance. We posit that the best moment to request such assistance is when the agent is highly uncertain in the successful outcome of the task. From an RL perspective, we relate this uncertainty to the variance of the return from the current state, as perceived by the agent. Numerous RL algorithms provide methods for the agent to estimate the expected return from a given state. We show that similar methods can be used to estimate the variance of return as well during the training process. At deployment time, the agent can then request expert assistance when its estimate of the return variance from the current state falls below a given threshold. We dub our method HULA, for Human-in-the-loop Uncertainty-aware Learning Agent, and illustrate its operation in Fig.\u00a0<ref>.\n\n\n\nCritically, HULA does not need to make any calls to the expert during training. This stands in contrast to a standard RL approach, where an agent could learn how to use an expert simply by making numerous such calls at train time. Nevertheless, our method is able to make effective use of a limited budget of expert calls in order to improve task performance. We summarize our key contributions as follows:\n\n    \n  * To the best of our knowledge, we are the first to propose a method for an HitL RL agent to learn how to effectively budget its interactions with a human expert at deployment time, without needing any expert calls during training.\n    \n  * We show that the variance of return, which can be estimated during training using Bellman-like equations, is an effective measure for agent uncertainty and can be used at deployment to determine when to request assistance.\n    \n  * Our experiments on discrete navigation problems with both fully- and partially-observable state information show that our method is as effective or better in managing its budget of expert calls compared to a standard learning approach that also makes thousands of expert calls during training.\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\nOur work extends standard reinforcement learning algorithms to human-in-the-loop policy that solves robotics tasks with the help of humans. Researchers have investigated how to learn such a policy. For example, Arakawa propose DQN-TAMER <cit.>, which is a method that incorporates a human observer model by real-time human feedback during training. \nExpected Local Improvement (ELI) <cit.> trains a state selector that suggests states requiring to query new actions from human experts.\nPAINT <cit.> learns a classifier to identify irreversible states and query the expert in case entering such states cannot be avoided.\nHug-DRL <cit.> leverages a  control transfer mechanism between humans and automation that corrects the agent\u2019s unreasonable actions during training by human intervention, where it has demonstrated significant potential in autonomous driving applications.\nWhile these methods exploit human intervention during training, our work relies on the internal uncertainty of an agent and only uses an expert in test time. \n\n\nexplicitly estimates the uncertainty of an RL agent and use it to make decisions about requesting an expert's assistance. \nPrior work has explored representing the uncertainty of outcomes during RL training <cit.>.\nFor instance, Kahn use the variance of the dynamics model to represent the uncertainty of collision to avoid damage from high-speed collisions \n<cit.>. Ensemble Quantile Networks (EQN) <cit.> estimates both aleatoric and epistemic uncertainties via the combination of quantile networks(IQN) <cit.> and randomized prior function(RPF) <cit.>.\nThese works use the uncertainty of an agent to perform task-specific decision-making, while our work differs in using it to collaborate with a human.\nMoreover, our work represents an agent's uncertainty using the variance of returns, which is a direct indicator of task performance.\n\nThe closest work to ours is RCMP, which learns a HitL policy that queries the expert when the epistemic uncertainty estimated by the variance of multiple value functions is high <cit.>. However, RCMP requires expert queries during training, whereas our work learns an uncertainty model based on the variance of return and does not query an expert in train time.\n\n\n\n\u00a7 METHOD\n\n\n\nConsider a problem for an autonomous agent formulated as a standard Markov decision process (MDP). A general MDP is defined as a tuple of (S, A, r, p), where S represents the state space, A represents the action space, r(s, a) is a reward function that evaluates immediate reward of action a \u2208 A in a state s \u2208 S, and p represent a transition distribution p(s_t+1| s_t, a_t). The goal of solving an MDP is to learn a policy \u03c0(a|s) that maximizes its expected return \ud835\udd3c_\u03c0[R], where R=\u2211\u03b3^tr(s_t, a_t) and \u03b3 is a discount factor.\n\nGoing beyond the standard MDP formulation, we now assume the availability of an expert that can give instructions to the robotic agent. When queried from a given state s_t, this expert can  directly provide an action a_exp(s_t) for the robot to execute. We assume that the expert always provides high-quality advice, i.e. a policy of always following expert-provided actions from all states will achieve a satisfactorily high return on the MDP problem above. However, such a policy would be impractical: we assume that the expert has limited bandwidth or availability, thus it is desirable for the agent to balance the goal of achieving high returns with the goal of not overloading the expert with requests. Such a scenario is typical in HitL robotics applications.\n\nOur method aims to determine when the agent should request expert assistance while making the most of a limited number of such calls available during deployment. Furthermore, we will like to limit (or eliminate) the number of calls made to the expert during its training phase.\n\nTo achieve this goal, our method leverages the uncertainty of the outcomes of the agent during exploration. Specifically, we use the variance of the return from a given state as a measure of uncertainty. Intuitively, a state with high variance of the return is one from which the agent, acting alone, could achieve a range of outcomes, ranging from very poor to very effective. We posit that these are the best states in which to request assistance. In contrast, states where the variance is low are those in which the agent is certain of the outcome of the task, and there are fewer ways in which external assistance can be of help.\n\nMost RL algorithms work by estimating the expected return from a given state; this estimate is continuously refined during training time. We modify this training procedure also to maintain and refine an estimate of _\u03c0(R), the variance of the returns under the learned policy, as an indicator of the agent's uncertainty. At test time, our agent can use this estimate of variance to decide when to request help. We detail these processes next.\n\n\n\n \u00a7.\u00a7 Estimation for the variance of return\n\n\nThe expected return from a given state is encapsulated by the state value functions, which are explicitly relied on by most RL algorithms. However, while most RL algorithms are not concerned with the variance of the return, we would like to compute and maintain similar estimates for it during training. By definition, the variance of the return is defined by: \n\n    (R) = \ud835\udd3c[R^2] - \ud835\udd3c[R]^2\n\n\n\nThe expected return of taking action a_t in state s_t is given by the Q-function under policy \u03c0:\n\n    \ud835\udd3c[R] = Q^\u03c0(s_t, a_t)\n\nSince the transition function is not available, it is usually approximated via Bellman update:\n\n\n    Q^\u03c0(s_t, a_t) \u2190 (1 - \u03b1)    Q^\u03c0(s_t, a_t) + \u03b1(r(s_t, a_t) \n       + \u03b3max_a_t+1\u2208 AQ(a_t+1, s_t+1))\n\n\nThe second moment of return \ud835\udd3c[R^2], which we call M^\u03c0(s_t, a_t), can be approximated using a sampling-based method by:\n\n    M^\u03c0   (s_t, a_t) = \ud835\udd3c[R^2] \n       = \ud835\udd3c[[r(s_t, a_t) + \u03b3\u2211_k=t+1^k=Nr(s_k, a_k)]^2] \n       = \ud835\udd3c[r^2(s_t, a_t) + 2\u03b3\u2211_k=t^k=Nr(s_k, a_k) + (\u2211_k=t+1^k=Nr(s_k, a_k))^2] \n       = r^2(s_t, a_t) + 2\u03b3\u2211_s_t+1\u2208 S p(s_t+1|s_t, a_t) Q^\u03c0(s_t+1, a_t+1) \n       + \u03b3^2\u2211_s_t+1\u2208 S p(s_t+1|s_t, a_t) M^\u03c0(s_t+1, a_t+1)\n\nSince the transition function is not available for a model-free agent, we can estimate M^\u03c0 using a Bellman-like formula with sampled data:\n\n    M^\u03c0(s_t, a_t)    \u2190 (1 - \u03b1)M^\u03c0(s_t, a_t) + \u03b1 M'(s_t, a_t) where \n        M'(s_t, a_t)    = r^2(s_t, a_t) + 2\u03b3 Q^\u03c0(s_t+1, a_t+1)  \n       + \u03b3^2 M(s_t+1, a_t+1)\n\nHere, a_t+1 = _a_t+1\u2208 AQ(s_t+1, a_t+1), since we assume a greedy agent that will take the actions that lead to max expected return during deployment.\n\nFinally, combining M and Q, we can approximate the variance of returns in a state by:\n\n    (R) = M(s_t, a_t) - Q^2(s_t, a_t)\n\n\nHence, the variance of the return given a state and action pair only depends on the Q function as well as the M (second function), which in turn depends on Q and can be updated via a Bellman-like recursion. \n\n\n\n \u00a7.\u00a7 HULA: Complete method\n\n\n\n\n\nWe are now ready to integrate the variance estimation into a complete learning algorithm. We use algorithms from the Q-learning family, as these already build an explicit estimate of the Q function at train time, and also use a greedy policy w.r.t. Q at run-time. \n\nDuring training, in addition to the function estimator for the Q function (referred to as Q_\u03b8_1), we build and update an estimator for M (referred to as Q_\u03b8_2). This estimator can be of the same type as used for Q: for tabular Q-learning, we can use a table, while for Deep Q-Networks (DQN) <cit.> we use a deep neural network. At every iteration, we use Eqs. (<ref>-<ref>) to update the estimator for M. In the case of tabular Q-learning, Eq. (<ref>) can be used directly, while in the case of DQN it can be transformed in a loss function analogous to the one used for Q. The complete procedure is shown in Alg.\u00a0<ref>.\n\nFinally, during deployment, the agent can use the trained estimators for the Q and M functions to compute the variance of the return for the current state. If this variance exceeds a threshold, the agent will request assistance and execute the action provided by the expert. Otherwise, the agent will execute the action prescribed by its own policy (in this case, a greedy selection based on the trained Q-function).\n\nWe note a key feature of our method: it does not require the presence of an expert during train time. The only change at train time compared to traditional, expert-less Q-learning is the addition of the function estimator for M, which will be used at run-time to help provide a variance estimator. In turn, the variance estimator is used to decide when to request assistance from the expert, which is only needed at deployment.\n\n\n\n\n\n\n\n\u00a7 EVALUATION\n\n\n\n\n\n\n\n \u00a7.\u00a7 Environments\n\n\nWe evaluate HULA in a discrete navigation scenario, where the agent must reach a goal while avoiding obstacles and/or traps. Furthermore, we test our approach both on problems where the agent has exact knowledge of its current state (which is tractable via tabular Q-learning) and problems where the agent only has access to sensor data of limited range, creating ambiguity (which requires more powerful function estimators, and the use of DQN). \n\n\n\n  \u00a7.\u00a7.\u00a7 Fully-observable MDPs\n These are discrete navigation environments where the agent is provided its exact location as observation. In addition to the goal cell, the environment also contains obstacles and traps: colliding with a trap would terminate the episode, while colliding with an obstacle results in a failure of action, and the agent would stay in its original cell. The agent also receives a small penalty for each step it takes. Available actions for an agent are moving up, down, left, and right, and the observations are the coordinates (x, y) of the agent's location. We use the maps shown in Fig.\u00a0<ref>, and train and test individually on each map.\n\nFor this simple class of problems, an unsophisticated agent can easily learn to act optimally on its own. However, we introduce uncertainty in the form of a stochastic transition function: at every step, after selecting an action, the agent moves in the desired direction with probability \u03c8, and moves in a random direction with probability 1-\u03c8 (in practice, we use \u03c8=0.45). We think of this setting as a \"slippery world\". Thus, even with an optimal policy, the robot may not reach its goal. \n\nTo help, we introduce an expert: when called, the expert provides the optimal action in the direction of the goal, and the action provided by the expert is not subject to transition function stochasticity. In this setting, the expert can thus be thought of as an \u201caction corrector\u201d with a better understanding of the environment dynamics, and whose actions always produce the expected results. Intuitively, we would expect an agent to call the expert in high-risk situations where it needs to avoid a wrong move, such as very close to traps. In this experiment, we test on two maps: 1. trap world that requires an agent to navigate around traps with an expert; 2. shortcut grid where the agent can leverage the expert to take the shortcut for higher returns.\n\n\n\n  \u00a7.\u00a7.\u00a7 Partially-observable MDP's\n\nTo test how our algorithm performs in more complex environments, we extend to a case of partially observable MDP's where the agent does not have complete information about the system. \nInstead of observing state information as the (x, y) position in the grid, the observation now only includes a finite patch of grid cells around the agent's current location (we use 5x5 grids in our implementation). \n\nAs shown in Figure <ref>, due to partial observability, it is not always possible for the agent to uniquely identify its own location in the map from observations. In such ambiguous regions, the same action taken based on identical observations can lead to utterly different results. However, other parts of the map are uniquely identifiable based on sensor observations, and thus an autonomous agent can always select the optimal action.\n\nIn this experiment, the expert observes the full state and always provides optimal actions to the agent. Intuitively, we would expect the agent to make use of the expert when traversing ambiguous areas of the map.\n\n\n\n\n \u00a7.\u00a7 Evaluation Approach\n\n\nThe main evaluation metric for our approach focuses on its ability to make effective use of the expert: we would like to see how performance (measured as average episodic return) changes as a function of the number of expert calls made during deployment. We can measure this performance by varying the value of the variance threshold \u03f5 used in Alg.\u00a0<ref>: for large values of \u03f5, the agent will never make use of the expert; conversely, if \u03f5 is very small, the agent will call the expert in every state. By sweeping the value of \u03f5 between these extremes, we obtain more or less autonomous agents, and can plot performance as a function of the number of expert calls that result in each case.\n\nAs a baseline, we use a standard RL approach that simply integrates the expert into the training procedure. We refer to this baseline as the penalty-based agent. Specifically, the penalty-based agent treats calling expert as an extra action a_call that can be called alongside the other actions at both training and deployment time. \n\nHowever, since the expert is optimal, the penalty-based agent will learn to always call for help. To learn a nontrivial policy that does not call the expert excessively, we employ a penalty of calling an expert in the reward function at train time: r'(s, a) = r(s, a) + c, where c < 0 is a penalty assessed only when calling an expert. By training this method with varying values of c, we again obtain agents that are more or less autonomous: the penalty-based agents trained with high c will call the expert less often, while those trained will low c will call more often. Again, we sweep the value of c between these extremes, and plot performance as a function of the number of expert calls that result in each case. However, for a fair comparison against HULA, we do not assess the expert penalty at deployment time.\n\n\n\n\u00a7 RESULTS\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Uncertainty Estimation\n\n\n\nThe first question we would like to answer is whether successfully captures the uncertainty of the agent and allocate the expert calls efficiently. \nWe compare the learned variances by our method with the ground truth variance of returns by Monte Carlo sampling the environment with the same policy without the assistance of an expert.\n\nAs shown in Figure <ref>, in the fully-observable experiment, successfully estimates a variance map that is similar to the ground truth. Specifically, it is uncertain about its outcome in states close to traps and confident when it is far from traps. \n\nIn the partially observable case, identifies ambiguous states when the agent cannot localize itself in the map. In these states, the agent fails to produce an action faithfully in that the same action taken can lead to different results (e.g., hitting the boundary or reaching the goal). Our results align with the ground-truth variance map, which also has high uncertainty in a similar region.\n\nTo further investigate 's efficiency in expert allocation, we compare the states with the highest N variances with those in the ground truth variance map. \nThese states are usually where the agent calls the expert, especially when the agent has a budget of N expert calls.\n\nOur results show that can estimate the uncertainty of an RL agent accurately and allocate its expert call based on the return variance efficiently.\nAs shown in Table\u00a0<ref>, in trap world, 80% of the top-5 and top-10 variance states estimated by are also the top-5 and 10 accordingly in the ground-truth variance. In the shortcut world, although our method only correctly estimates 1 of 5 highest variance states, its performance is higher and reaches 60% in the top-10 case. This indicates that it allocates expert calls efficiently if given more budget. In the partially-observable experiment, our agent also recovers most of the high variance states in both the top-5 and top-10 evaluation.\n\n\n\n\n\n\n \u00a7.\u00a7 Task Performance\n\n\nIn all experiments, our results show that agents achieve higher performance when requesting help from experts compared to the ones that does not use an expert.\nIn the trap world, as shown in Fig.\u00a0<ref>, achieves similar performance as the penalty-based agent while using the same number of expert calls. \nInterestingly, in the shortcut world, we find that the penalty-based method has a higher average return than . \nThis is because our method does not evaluate the effect of calling an expert during training. As shown in Fig.\u00a0<ref>, the penalty-based agent learns to move towards states that ask for help from an expert since it knows it will call an expert in a future state, whereas navigates around the uncertain states to avoid them. However, in the highest-variance states (e.g., states between traps), both agents are able to call the expert and complete the task.\n\nIn this experiment, we find that is robust to different expert call budgets compared to the penalty-based agent. \nFor example, in the shortcut world, the penalty-based agent cannot learn a policy that uses 2 expert calls to complete the task even if we perform hyper-parameter sweeping with the expert penalty c. This can be caused by the high expert penalty that modifies the original reward structure and hinders the learning of the task.\nIn contrast, our method can flexibly incorporate different expert call budgets and maintain reasonable performance.\n\n\n\nIn the partially-observed grid world, the agent outperforms the penalty-based agent when the allowed expert call is limited in the range of (2, 8].  When both agents are given enough expert assistance, they achieve similar performance. This indicates that efficiently utilizes the expert to localize itself and improve the task performance.\n\n\nAn important feature of our method is that it does not introduce extra complexity in training an RL agent.\nPractically, we stop training when the Q function converges. Therefore, the training efficiency is similar to its underlying RL algorithm.\nIn contrast, the penalty-based agent requires access to an expert during training time. Our experiments show that training each penalty-based agent for the fully-observable environment results in about 70000 expert calls. The requirement of an expert during train time limits its application on learning policies that interact with humans.\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\n\n\nWe introduced , a method to learn human-in-the-loop policies by estimating an RL agent's uncertainty. \nWe proposed a return variance estimation method that captures an RL agent's uncertainty. Our experimental results demonstrate that can capture an RL agent's uncertainty and use it to request assistance from experts to achieve high task performance. An important feature of our method is that it does not require the presence of an expert during training. We envision that our approach can be applied to more complex problems. For example, we plan to extend to continuous RL algorithms (e.g. DDPG <cit.>) and learns HitL policies to solve continuous control problems. We also would like to explore the direction of learning uncertain agents in other domains, such as language-guided navigation.\n\n\n\n\n\n-12cm   \n                                  \n                                  \n                                  \n                                  \n                                  \n"}