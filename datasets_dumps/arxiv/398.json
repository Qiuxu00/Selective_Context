{"entry_id": "http://arxiv.org/abs/2303.06774v1", "published": "20230312232011", "title": "Better than square-root cancellation for random multiplicative functions", "authors": ["Max Wenqiang Xu"], "primary_category": "math.NT", "categories": ["math.NT", "math.CA", "math.PR"], "text": "\nBetter than square-root cancellation]Better than square-root cancellation for random multiplicative functions\n\n\nDepartment of Mathematics, Stanford University, Stanford, CA, USA\nmaxxu@stanford.edu\n\n\n\n\n    We investigate when the better than square-root cancellation phenomenon exists for \u2211_n\u2264 Na(n)f(n), where a(n)\u2208\u2102 and f(n) is a random multiplicative function. We focus on the case where a(n) is the indicator function of R rough numbers. We prove that loglog R \u224d (loglog x)^1/2 is the threshold for the better than square-root cancellation phenomenon to disappear.\n\n[\n    Max Wenqiang Xu\n    \n===================\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nThe study of random multiplicative functions has attracted intensive attention. Historically, they were introduced to model arithmetic functions. A Steinhaus random multiplicative function f(n) is a completely multiplicative function defined on positive integers such that f(p) are independently and uniformly distributed on the complex unit circle for all primes p. One may view it as a random model for arithmetic functions like Dirichlet characters \u03c7(n) or n^it. Another popular model is the Rademacher random multiplicative function f(n) which was first used by Wintner<cit.> as a random model for M\u00f6bius function \u03bc(n). In this note, we focus on the Steinhaus case. The obvious dependence between random variables f(m) and f(n) whenever (m, n)\u2260 1 makes the study of random multiplicative functions intriguing.\n\n\nArguably the most striking result so far in the study of random multiplicative functions is Harper's <cit.> remarkable resolution of Helson's conjecture<cit.>, that is, the partial sums of random multiplicative functions enjoy better than square-root cancellation\n\n    [|\u2211_n\u2264 x f(n) |] \u224d\u221a(x)/(loglog x)^1/4,\n\nwhere f(n) are random multiplicative functions. \nIn particular, with the natural normalization \u221a(x), the partial sums \u2211_n\u2264 x f(n) do not converge in distribution to the standard complex normal distribution (see also <cit.>). Before Harper's result <cit.>, there was progress on proving good lower bounds close to \u221a(x), e.g. <cit.>, and it was not clear that such better than square-root cancellation in (<ref>) would  appear until Harper's proof. See also recent companion work on analogous results in the character sums and zeta sums cases established by Harper <cit.>.\nIt is known that the better than square-root cancellation phenomenon in random multiplicative functions is connected to the \u201ccritical multiplicative chaos\" in the probability literature. We point out references <cit.> for related discussions.  \n\nA closely related important question in number theory is to understand the distribution of the Riemann zeta function over typical intervals of length 1 on the critical line \n\u211c\ud835\udd22(s)=1/2. One may crudely see the connection by viewing \u03b6(s) as a sum of n^-1/2-it for a certain range of n and n^it behaves like a Steinhaus random multiplicative function for randomly chosen t. A conjecture of Fyodorov, Hiary, and Keating (see e.g. <cit.>) suggests that there is a subtle difference between the true order of local maximal of log|\u03b6(1/2+it)| and one's first guess based on Selberg's central limit theorem for log|\u03b6(1/2+it)|. The existence of this subtle difference and the appearance of the better than square-root cancellation for random multiplicative functions both show that the corresponding nontrivial dependence can not be ignored. We refer readers to <cit.> for related discussions about partial sums of random multiplicative functions and zeta values distribution.\n\nIn this paper, we are interested in further exploring Harper's result (<ref>) and methods used there, by considering the problem in a more general context. \n Let a(n) be a sequence in . \nWhen does the better than square-root cancellation phenomenon hold for \u2211_n\u2264 N a(n)f(n), i.e.\n\n    [|\u2211_n\u2264 Na(n) f(n)|] = o(\u221a(\u2211_n\u2264 N|a(n)|^2))\u030a?\n\n\n\nWe first make some simple observations in the situations where a(n) is \u201ctypical\" or a(n) has a rich multiplicative structure. Then we focus on a particular case where the coefficient a(n) is an indicator function of a multiplicative set. \n\n\n \u00a7.\u00a7 Typical coefficients\n\nIf partial sums \u2211_n\u2264 Na(n)f(n) with the square-root size normalization behave like the complex standard Gaussian variable, then there is just square-root cancellation. One may attempt to prove such a central limit theorem by computing the high moments, however, the moments usually blow up and such a strategy does not work here (see e.g. <cit.> for moments computation results). It turns out that for \u201ctypical\" choices of a(n), such a central limit theorem does hold. It has been carried out in the concrete case where a(n)=e^2\u03c0 i n \u03b8 for some fixed real \u03b8 without too good Diophantine approximation property (such \u03b8 has relative density 1 in , e.g. one can take \u03b8=\u03c0) by Soundararajan and the author <cit.>, and also an average version of the result is proved by Benatar, Nishry and Rodgers <cit.>. The proof of the result in <cit.> is based on McLeish's martingale central limit theorem<cit.>, and the method was pioneered by Harper in <cit.>.  \nThe proof reveals the connection between the existence of such a central limit theorem and a quantity called multiplicative energy of a:= {a(n): 1\u2264 n\u2264 N} \n\n    E_\u00d7(\ud835\udc1a): = \u2211_m_1, n_1, m_2, n_2 \u2264 N\n     m_1m_2=n_1n_2a(m_1)a(m_2) a(n_1)a(n_2).\n\nA special case of a(n) is an indicator function of a set , and the quantity E_\u00d7() is a popular object studied in additive combinatorics. It is now known <cit.> that a crucial condition for such a central limit theorem holds for \u2211_n\u2264 Na(n)f(n) is that the set  has multiplicative energy \u2264(2+\u03f5)||^2. See <ref> for more discussions on a(n) being a \u201ctypical\" choice.  We refer readers who are interested in seeing more examples of when a central limit theorem holds for partial (restricted) sums of random multiplicative functions to <cit.>.  \n\n\n\n \u00a7.\u00a7 Large multiplicative energy and sparse sets\n Let us focus on the case that a_n is an indicator function of a set . As we mentioned that if the set  has small multiplicative energy (among other conditions), then partial sums exhibit square-root cancellation. Suppose we purposely choose a set  with very large multiplicative energy, will it lead to better than square-root cancellation? One extreme example is = {p^n: 1\u2264 n \u2264log_p N} being a geometric progression, where p is a fixed prime. A standard calculation gives that\n\n    [| \u2211_n\u2208 f(n) |] =  \u222b_0^1 |\u2211_n\u2264log_p Ne(\u03b8 n)| d\u03b8\u224dloglog N,\n\nwhile [| \u2211_n\u2208 f(n) |^2] \n = ||\u224dlog N. \nIt shows that there is a great amount of cancellation in this particular example. One may also take  to be some generalized (multidimensional) geometric progression and get strong cancellation of this type. We note that the sets mentioned here with very rich multiplicative structures all have small sizes. \n\nBased on the initial thoughts above, we may lean toward believing that better than square-root cancellation only appears when a(n) has some particular structure that is perhaps related to multiplicativity.  \nTo fully answer Question\u00a0<ref> seems hard. The majority of the paper is devoted to a special case, where a(n) is an indicator function of a set with multiplicative features. We focus on fairly large subsets.\n\n\n \u00a7.\u00a7 Main results: multiplicative support\n\nSuppose now that a(n) is a multiplicative function with |a(n)|\u2264 1. \nThe particular example we study in this paper is that a(n) is the indicator function of R-rough numbers, although the proof here may be adapted to other cases when a(n) is multiplicative.  \nWe write \n\n    _R(x): = {n\u2264 x: p|n  p\u2265 R}.\n\n\nBy a standard sieve argument, for all 2\u2264 R\u2264 x/2 (the restriction R\u2264 x/2 is only needed for the lower bound), we have asymptotically \n\n    |_R(x)| \u224dx/log R.\n\nWe expect the following threshold behavior to happen. If R is very small, the set _R(x) is close to [1,x] and better than square-root cancellation appears as in <cit.>. If R is sufficiently large, then weak dependence  may even lead to a central limit theorem. Indeed, an extreme case is that R> \u221a(x), in which _R(x) is a set of primes and {f(n): n\u2208} is a set of independent random variables. It is natural to ask to what extent the appearance of small primes is needed to guarantee better than square-root cancellation.\nOur Theorem\u00a0<ref> and Theorem\u00a0<ref> answer the question. We show that loglog R \u2248 (loglog x)^1/2 is the threshold.\n\n\n\n  Let f(n) be a Steinhaus random multiplicative function and x be large. Let _R(x) be the set of \n  R rough numbers up to x. For any loglog R\u226a (loglog x)^1/2, we have   \n  \n    [|\u2211_n\u2208_R(x) f(n) |] \u226a\u221a(|_R(x)|)\u00b7 ( loglog R +  logloglog x/\u221a(loglog x))^1/2 .\n\n In particular, if loglog R = o((loglog x)^1/2), then  \n   \n    [|\u2211_n\u2208_R(x) f(n) |] =o ( \u221a(|_R(x)|)).\n\n\nThe term logloglog x is likely removable. But for the convenience of the proof, we state the above version. See Remark\u00a0<ref> for more discussions.\n\n\n  Let f(n) be a Steinhaus random multiplicative function and x be large. Let _R(x) be the set of \n  R rough numbers up to x. For any loglog R\u226b  (loglog x)^1/2, we have   \n  \n    [|\u2211_n\u2208_R(x) f(n) |] \u226b\u221a(|_R(x)|) .\n\n\nOne probably can prove a lower bound of the shape \u221a(||)\u00b7(loglog R / \u221a(loglog x))^-1/2\n when loglog R =o(\u221a(loglog x)). We do not pursue this as we focus on finding the threshold value of R instead of caring about the quantification of the exact cancellation. \n\n\nWe note that one way to derive a lower bound on L^1 norm is by proving an upper bound on L^4 norm. A simple application of H\u00f6lder's inequality gives that\n\n    |_R(x)| = [|\u2211_n\u2208_R(x) f(n) |^2] \u2264([|\u2211_n\u2208_R(x) f(n) |^4])^1/3([|\u2211_n\u2208_R(x) f(n) |])^2/3.\n\nThe fourth moment  \u226a||^2 would imply that L^1 norm \u226b\u221a(||). However, to achieve such a bound on the fourth moment, one needs log R \u226b (log x)^c for some constant c, and thus this approach would not give the optimal range as in Theorem\u00a0<ref>.\n\nAnother reason for studying  the fourth moment (multiplicative energy) is to understand the distribution. As mentioned before, this is the key quantity that needs to be understood in order to determine if random sums have Gaussian limiting distribution, via the criteria in <cit.>. One may establish a central limit theorem in the range R\u226bexp((log x)^c) for some small positive constant c[One trick to get a smaller c than by directly computing the fourth moment over the full sum is to take the anatomy of integers into account. We refer interested readers to <cit.> to see how this idea is connected to the correct exponent in extremal sum product conjecture of Elekes and Ruzsa <cit.>.]. Interested readers are suggested to adapt the proof of <cit.>. We do not pursue results along this direction in this note. \n\nTheorem\u00a0<ref> and Theorem\u00a0<ref> are both proved by adapting Harper's robust method in <cit.>, with some modifications, simplifications and new observations, and we sketch the strategy with a focus on how we find the threshold. We also refer readers to a model problem in the function field case by Soundararajan and Zaman <cit.>.  The first step is to reduce the L^1 norm estimate to a certain average of the square of random Euler products. Basically, we prove that \n\n    [|\u2211_n\u2208 f(n)|] \u2248(x/log x)^1/2\u00b7[(\u222b_-1/2^1/2 |F^(R)(1/2 + it)|^2 dt  )^1/2 ],\n\nwhere F^(R)(1/2+it) := \u220f_R\u2264 p\u2264 x (1-f(p)/p^1/2+it)^-1 is the random Euler product over primes R\u2264 p \u2264 x. The challenging part is to give a sharp bound on the above expectation involving |F^(R)(1/2+it)|^2 for |t|\u2264 1/2. \n\nWe first discuss the upper bound proof. If we directly apply H\u00f6lder's inequality (i.e. moving the expectation inside the integral in (<ref>)), then \nwe would only get the trivial upper bound  \u226a\u221a(||) as [|F^(R)(1/2+it)|^2]\u2248log x/log R. Harper's method starts with putting some \u201cbarrier events\" on the growth rate of all random partial Euler products for all t. Roughly speaking, it requires that for all k,\n\n    \u220f_x^e^-(k+1)\u2264 p \u2264 x^e^-k |1-f(p)/p^1/2+it|^-1\u00a0\u201cgrows as expected\" for all |t|\u2264 1.\n\nDenote such good events by \ud835\udca2 and write s=1/2+it. By splitting the probability space based on the event \ud835\udca2 holding or not, and applying Cauchy\u2013Schwarz inequality, we have \n\n    [(\u222b_-1/2^1/2 |F^(R)(s)|^2 dt  )^1/2 ]\n       \u2248[(\u222b_-1/2^1/21_\ud835\udca2 |F^(R)(s)|^2 dt  )^1/2] +  [(\u222b_-1/2^1/21_\ud835\udca2\u00a0fail |F^(R)(s)|^2 dt  )^1/2]  \n       \u226a[(\u222b_-1/2^1/21_\ud835\udca2 |F^(R)(s)|^2 dt  )^1/2] + (1_\ud835\udca2\u00a0fail)^1/2 ([|F^(R)(s)|^2])^1/2 .\n\nAccording to the two terms above, there are two tasks that remain to be done. \n\n    \n  * Task 1: Show that the expectation is small, conditioning on 1_\ud835\udca2.\n\n  * Task 2: Show that (1_\ud835\udca2\u00a0fail) is sufficiently small. \n \nTo accomplish task 1, Harper's method connects such an estimate to the \u201cballot problem\" or say Gaussian random walks (see <ref>), which is used to estimate the probability of partial sums of independent Gaussian variables having a certain barrier in growth. Task 2 of estimating the probability of such good events \ud835\udca2 happening can be done by using some concentration inequality, e.g. Chebyshev's inequality. \nOur main innovation lies in setting up \u201cbarrier events\" in (<ref>) properly which is not the same as in <cit.>. On one hand, it should give a strong enough restriction on the growth rate of the products so that [(\u222b_-1/2^1/21_\ud835\udca2 |F^(R)(s)|^2 dt  )^1/2] has a saving, compared to it without conditioning on 1_\ud835\udca2. On the other hand, one needs to show that such an event \ud835\udca2 is indeed very likely to happen which requires that the designed \u201cbarrier\" can not be too restrictive. To make the two goals simultaneously achieved, we need loglog R = o( \u221a(loglog x)) and this is the limit that we can push to (see Remark\u00a0<ref>).\n\nThe lower bound proof in Theorem\u00a0<ref> uses the same strategy as in <cit.> but is technically simpler. After the deduction step of reducing the problem to studying a certain average of the square of random Euler products (see (<ref>)), we only need to give a lower bound of the shape \u226b (log x / log R)^1/2 for the expectation on the right-hand side of (<ref>). Since the integrand |F^(R)(s)|^2 is positive, it suffices to prove such a lower bound when t is restricted to a random subset \u2112. We choose \u2112 to be the set of t such that certain properly chosen \u201cbarrier events\" hold.  The main difficulty is to give a strong upper bound on the restricted product [1_t_1, t_2\u2208\u2112|F^(R)(1/2+it_1)|^2|F^(R)(1/2+it_2)|^2] in the sense that the bound is as effective as in the ideal situation where the factors |F^(R)(1/2+it_1)|^2 and |F^(R)(1/2+it_2)|^2 are independent (see Proposition\u00a0<ref>), and this is also the main reason that the condition loglog R \u226b\u221a(loglog x) is needed subject to our chosen \u201cbarrier events\". Our proof of Theorem\u00a0<ref> does not involve the \u201ctwo-dimensional Girsanov calculation\", which hopefully makes it easier for readers to follow.\n \n\n\n\n\n \u00a7.\u00a7 Organization\n We set up the proof outline of Theorem\u00a0<ref> in Section\u00a0<ref> and defer the proof of two propositions to Section\u00a0<ref> and Section\u00a0<ref> respectively. We put all probabilistic preparations in Section\u00a0<ref> which will be used in the proof for both theorems. The proof of Theorem\u00a0<ref> is done in Section\u00a0<ref> and again we defer proofs of two key propositions to Section\u00a0<ref> and Section\u00a0<ref> respectively. Finally, we give more details about the \u201ctypical\" choices of a(n) in Section\u00a0<ref>, as well as mentioning some natural follow-up open problems. \n\n\n\n \u00a7.\u00a7 Acknowledgement\n\nWe thank Adam Harper  for helpful discussions, corrections, and comments on earlier versions of the paper and for his encouragement. We also thank Kannan Soundararajan for the interesting discussions. The author is supported by the Cuthbert C. Hurd Graduate Fellowship in the Mathematical Sciences, Stanford. \n\n\n\n\u00a7 PROOF OF THEOREM\u00a0<REF>\n\nWe follow the proof strategy of Harper in <cit.>.\nWe establish Theorem\u00a0<ref> in a stronger form that for 1/2\u2264 q \u2264 9/10 and R in the given range loglog R \u226a (loglog x)^1/2,  \n\n    [|\u2211_n\u2208_R(x) f(n) |^2q] \u226a |_R(x)|^q ( loglog R +  logloglog x/\u221a(loglog x))^q.\n\nOne should be able to push the range of q to 1 but for simplicity in notation, we omit it. Our interest is really about the case q=1/2.\nNote that in the given range of R, by (<ref>), it is the same as proving \n\n    [|\u2211_n\u2208_R(x) f(n) |^2q] \u226a(x/log R)^q ( loglog R +  logloglog x/\u221a(loglog x))^q.\n\n\nThe first step (Proposition\u00a0<ref>) is to connect the L^1 norm of the random sums to a certain average of the square of random Euler products. We define for all s with \u211c\ud835\udd22(s)>0 and integers 0\u2264 k\u2264loglog x -loglog R, the random Euler products\n\n    F_ k^(R)(s) : = \u220f_R\u2264 p\u2264 x^e^-(k+1) (1- f(p)/p^s)^-1 = \u2211_n\u2265 1 \n     p|n R\u2264  p\u2264 x^e^-(k+1)f(n)/n^s.\n\nWe also write \n\n    F^(R)(s): = \u220f_R\u2264 p\u2264 x (1- f(p)/p^s)^-1 = \u2211_n\u2265 1 \n     p|n R\u2264  p\u2264 xf(n)/n^s.\n\nWe use the notation X_2q: = ([|X|^2q])^1/2q for random variable X. \n\nLet f(n) be a Steinhaus random multiplicative function and x be large. Let F_k^(R)(s) be defined as in (<ref>) and loglog R \u226a (loglog x)^1/2. Set \ud835\udca6: =\u230alogloglog x \u230b. Then uniformly for all 1/2 \u2264 q\u2264 9/10, we have \n\n    \u2211_n\u2208 f(n)_2q\u2264\u221a(x/log x)\u2211_0\u2264 k \u2264\ud835\udca6\u222b_-1/2^1/2 | F_ k^(R)(1/2 - k/log x+it)|^2dt_q^1/2 + \u221a(x/log x).\n\n\nWe remind the readers that the upper bound we aim for in Theorem\u00a0<ref> is very close to \u221a(x/log R). The second term in (<ref>) is harmless since log R is much smaller than log x. \n\n\n\nThe second step deals with the average of the square of random Euler products in (<ref>), which lies at the heart of the proof.  \n\n\n\nLet F_k^(R)(s) be defined as in (<ref>) and loglog R \u226a (loglog x)^1/2. Then for all 0\u2264 k \u2264\ud835\udca6=\u230alogloglog x \u230b, and uniformly for all 1/2\u2264 q\u2264 9/10, we have \n\n    [(\u222b_-1/2^1/2 |F_ k^(R)(1/2 - k/log x + it)|^2dt)\u030a^q]\u030a\u226a e^-k/2\u00b7(log x/log R)\u030a^q ( loglog R + logloglog x/\u221a(loglog x))^q .\n\n\n\n\n\n\nApply Proposition\u00a0<ref> and Proposition\u00a0<ref> with q=1/2. Notice that when  loglog R \u226a (loglog x)^1/2, the term \u221a(x/log x) in (<ref>) is negligible and we complete the proof. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 PROBABILISTIC PREPARATIONS\n\nIn this section, we state some probabilistic results that we need to use later. The proof can be found in <cit.> (with at most very mild straightforward modification). \n\n\n \u00a7.\u00a7 Mean square calculation\n\nWe first state results on mean square calculations.\n\n\nLet f be a Steinhaus random multiplicative function. Then for any 400<x\u2264 y and \u03c3>-1/log y, we have\n\n    [\u220f_x<p\u2264 y |1-f(p)/p^1/2+\u03c3|^-2] = exp( \u2211_x<p\u2264 y1/p^1+2\u03c3 + O(1/\u221a(x)log x) ).\n\n\n\nThe proof is basically using the Taylor expansion and the orthogonality deduced from the definition of a Steinhaus random multiplicative function. See <cit.>. \n\n\nWe also quote the following result on two-dimensional mean square calculations. This will be used in proving the lower bound in Theorem\u00a0<ref>. \n\n   Let f be a Steinhaus random multiplicative function. Then for any 400<x\u2264 y and \u03c3>-1/log y, we have\n \n    [\u220f_x<p\u2264 y |1-f(p)/p^1/2+\u03c3|^-2 |1-f(p)/p^1/2+\u03c3+it|^-2 ] = exp( \u2211_x<p\u2264 y2+2cos(tlog p)/p^1+2\u03c3 + O(1/\u221a(x)log x) )\u030a.\n  \nMoreover, if x>e^1/|t|, then we further have \n\n    = exp( \u2211_x<p\u2264 y2/p^1+2\u03c3 + O(1) ) .\n\n\n\nThe proof of (<ref>) is in <cit.>. To deduce (<ref>), we only need to show the contribution involves cos(tlog p) terms are \u226a 1, which follows from a strong form of prime number theorem. See how it is done in <cit.> and <cit.>. \n\n\n\n\n \u00a7.\u00a7 Gaussian random walks and the ballot problem\n\nA key probabilistic result used in Harper's method is the following (modification of) a classical result about Gaussian random walks, which is connected to the \u201cballot problem\". \n\n\n    Let a \u2265 1. For any integer n > 1, let G_1, \u2026 , G_n be independent real\nGaussian random variables, each having mean zero and variance between 1/\n20 and 20, say. Let\nh be a function such that |h(j)| \u2264 10 log j. Then\n\n    (\u2211_m=1^j G_m \u2264 a + h(j),   \u2200 1\u2264 j\u2264 n) \u224dmin{1, a/\u221a(n)}.\n\n\nWithout the term h(j), it is a classical result and actually that is all we need in this paper. However, we state this stronger form as the h(j) term can be crucial if one wants to remove the logloglog x factor in Theorem\u00a0<ref>. We expect the random sum is fluctuating on the order of \u221a(j) (up to step j) and so the above result is expected to be true. The quantity h(j) is much smaller compared to \u221a(j) so it is negligible in computing the probability. \n\nWe do not directly use the above lemma. We shall use an analogous version for random Euler products (Proposition\u00a0<ref>). We do the \nGirsanov-type calculation in our study. As in <cit.>, we introduce the probability measure (here x is large and |\u03c3|\u2264 1/100, say)\n\n    (A) : = [1_A \u220f_p\u2264 x^1/e |1-f(p)/p^1/2+\u03c3|^-2  ]/[\u220f_p\u2264 x^1/e |1-f(p)/p^1/2+\u03c3|^-2] .\n\nFor each \u2113\u2208\u2115\u222a{0}, we denote the \u2113-th increment of the Euler product \n\n    I_\u2113(s):= \u220f_x^e^-(\u2113+2)<p\u2264 x^e^-(\u2113+1) (1-f(p)/p^s)^-1.\n\nSince we are restricted to R-rough numbers n,  the parameter \u2113 lies in the range 0\u2264\u2113\u2264loglog x - loglog R. All the rest setup is exactly the same as in <cit.>. \n\n\nThere is a large natural number B such that the following is true.\nLet n\u2264loglog x - loglog R - (B+1), and define the decreasing sequence (\u2113_j)_j=1^n of non-negative integers by \u2113_j = \u230aloglog x -loglog R \u230b -(B+1) - j. Suppose that |\u03c3|\u22641/e^B+n+1, and\nthat (t_j)_j=1^n is a sequence of real numbers satisfying |t_j|\u22641/j^2/3 e^B+j+1 for all j.\n\nThen uniformly for any large a and any function h(n) satisfying |h(n)| \u2264 10 log n, and with I_\u2113(s) defined as in (<ref>), we have  \n\n    (-a -Bj \u2264\u2211_m=1^jlog |I_\u2113_m (1/2+\u03c3 + it_m)| \u2264 a + j + h(j),   \u2200 j\u2264 n ) \u224dmin{1, a/\u221a(n)} .\n\n\nOne may view the above sum approximately as a sum of j independent random variables and each with mean \u2248\u2211_x^e^-(\u2113+2)<p\u2264 x^e^-(\u2113+1)1/p\u2248 1 and with constant variance between 1/20 and 20. This shows the connection to Lemma\u00a0<ref>. The deduction of Proposition\u00a0<ref> from Lemma\u00a0<ref> can be found in the proof of <cit.>. The only modification is changing the upper bound restriction from n \u2264loglog x -(B+1) to n \u2264loglog x - loglog R -(B+1) and all conditions remaining satisfied. \n\n\n\n\n\u00a7 PROOF OF PROPOSITION\u00a0<REF>\n\nThe proof follows closely to the proof of <cit.>. \nFor any integer 0\u2264 k \u2264\ud835\udca6= \u230alogloglog x \u230b, let \n\n    I_k: =(x_k+1, x_k] :=  (x^e^-(k+1) , x^e^-k].\n\nLet P(n) be the largest prime factor of n. For simplicity, we use _n to denote the sum where the variable n is R-rough.\nBy using Minkowski's inequality (as 2q\u2265 1), \n\n    \u2211_n\u2208 f(n)_2q\u2264\u2211_0\u2264 k \u2264\ud835\udca6_n\u2264 x \n     P(n)\u2208 I_k f(n)_2q + _n\u2264 x\n    P(n)\u2264 x^e^-(\ud835\udca6+1) f(n)_2q\n    .\n\nWe first bound the last term by only using the smoothness condition and it is bounded by \n\u2264\u03a8 (x, x^1/loglog x  )^1/2\u226a\u221a(x) (log x)^-clogloglog  x, which is acceptable. \nThe main contribution to the upper bound in (<ref>) can be written as\n\n    = \u2211_0\u2264 k \u2264\ud835\udca6\u2211_m\u2264 x \n     p|m  p \u2208 I_kf(m) _n\u2264 x/m \n     n\u00a0is x_k+1-smooth f(n) _2q.\n\nWe now condition on f(p) for p small but at least R. Write ^(k) to denote the expectation conditional on (f(p))_p\u2264 x_k+1. Then the above is\n\n    = \u2211_0\u2264 k \u2264\ud835\udca6 (^(k) [|\u2211_m\u2264 x\n     p|m  p \u2208 I_k f(m) _n\u2264 x/m\n     n\u00a0is x_k+1-smooth f(n)|^2q])^1/2q\n       \u2264\u2211_0\u2264 k \u2264\ud835\udca6 ([(^(k) [|\u2211_m\u2264 x\n     p|m  p \u2208 I_kf(m) _n\u2264 x/m\n     n\u00a0is x_k+1-smooth f(n)|^2])^q])^1/2q\n        = \u2211_0\u2264 k \u2264\ud835\udca6 ( [( \u2211_m\u2264 x \n     p|m  p\u2208 I_k  |_n\u2264 x/m\n     n\u00a0is x_k+1-smooth f(n)|^2)^q] )^1/2q.\n\nThen we only need to show that for each expectation in the sum, it is bounded as in (<ref>). Replace the discrete mean value with a smooth version. Set X=e^\u221a(log x), and we have the expectation involving primes in I_k is \n\n    \u226a[(\u2211_m\u2264 x \n     p|m  p\u2208 I_kX/m\u222b_m^m(1+1/X) |_n\u2264 x/t\n     n\u00a0is x_k+1-smooth f(n)|^2 dt )\u030a^q]\u030a\n        + [( \u2211_m\u2264 x \n     p|m  p\u2208 I_kX/m\u222b_m^m(1+1/X) |_x/t \u2264 n\u2264 x/m\n    n\u00a0is x_k+1-smooth f(n)|^2 dt )\u030a^q]\u030a.\n\nBy using H\u00f6lder's inequality, we upper bound the second term in (<ref>) by the q-th power of\n\n    \u2211_m\u2264 x \n     p|m  p\u2208 I_kX/m\u222b_m^m(1+1/X) [|_x/t \u2264 n\u2264 x/m\n    n\u00a0is x_k+1-smooth f(n)|^2] dt .\n\nDo the mean square calculation (<ref>) and throw away the restriction on the R-rough numbers. Then (<ref>) is at most \u226a  2^-e^k x/log x and thus the second term in (<ref>) is \u226a(2^-e^k x/log x)^q. Summing over k\u2264\ud835\udca6, this is acceptable and thus we only need to focus on the first term in (<ref>). By swapping the summation, it is at most\n\n    [ ( \u222b_x_k+1^x |_n\u2264 x/t \n     n\u00a0is x_k+1-smoothf(n) |^2\u2211_t/(1+1/X)\u2264 m \u2264 t\n     p|m  p\u2208 I_k  X/m dt)\u030a^q]\u030a.\n\nWe upper bound the sum over m by dropping the prime divisibility condition and using a simple sieve argument to derive that the above is at most \n\n    [  ( \u222b_x_k^x |_n\u2264 x/t \n     n\u00a0is x_k+1-smoothf(n) |^2dt/log t)\u030a^q]\u030a = x^q[  ( \u222b_1^x/x_k+1 |_n\u2264 z \n     n\u00a0is x_k+1-smoothf(n)|^2dz/z^2log(x/z))\u030a^q]\u030a,\n\nwhere in the equality above we used the substitution z: =x/t. \nA simple calculation shows that we can replace log(x/z) by log x without much loss. Indeed, if z\u2264\u221a(x) then log(x/z)\u226blog x; if \u221a(x)\u2264 z \u2264 x/x_k+1 then log (x/z) \u2265 z^-2k/log xlog x.  Thus, we further have the bound\n\n    \u226a(x/log x)\u030a^q[ ( \u222b_1^x/x_k+1 |_n\u2264 z \n     n\u00a0is x_k+1-smoothf(n)|^2dz/z^2-2k/log x)\u030a^q]\u030a .\n\nTo this end, we apply the following version of Parseval's identity, and its proof can be found in <cit.>.\n\n\n    Let (a_n)_n=1^\u221e be any sequence of complex numbers, and let A(s): = \u2211_n=1^\u221ea_n/n^s denote the corresponding Dirichlet series, and \u03c3_c denote its abscissa of convergence. Then for any \u03c3> max{0, \u03c3_c}, we have \n    \n    \u222b_0^\u221e|\u2211_n\u2264 xa_n|^2/x^1+2\u03c3dx = 1/2\u03c0\u222b_-\u221e^+\u221e|A(\u03c3 + it)/\u03c3 + it|^2 dt.\n\n\nApply Lemma\u00a0<ref> and the expectation in (<ref>) is\n\n    =\n    [  (\u222b_-\u221e^+\u221e|(1/2-k/log x +it)|^2/|1/2-k/log x +it|^2 dt)\u030a^q]\u030a\u2264\u2211_n\u2208[( \u222b_n-1/2^n+1/2|(1/2-k/log x +it)|^2/|1/2-k/log x +it|^2 dt )\u030a^q]\u030a.\n\nSince f(m)m^it has the same law as f(m) for all m, for any fixed n we have\n\n    [(\u222b_n-1/2^n+1/2 |(1/2-k/log x +it)|^2 dt )\u030a^q]\u030a = [ (\u222b_-1/2^1/2 |(1/2-k/log x +it)|^2 dt )\u030a^q]\u030a.\n\nFor n-1/2\u2264 t\u2264 n+1/2, we have \n1/|1/2-k/log x +it|^2\u224d 1/n^2 which is summable over n. \nWe complete the proof by inserting the above estimates into (<ref>). \n\n\n\n\n\n\n\n\n\u00a7 PROOF OF PROPOSITION\u00a0<REF>\n\n\nThis is the key part of the proof that reveals how loglog R \u2248\u221a(loglog x) could become the transition range. \nWe begin with a discretization process which is the same as in <cit.>.  For each |t|\u22641/2, set t(-1)=t, and then iteratively for each 0\u2264 j \u2264log(log x /log R) -2 define \n\n    t(j): = max{u\u2264 t(j-1): u = n/((log x) /e^j+1)log ((log x) /e^j+1)\u00a0for some n\u2208}.\n\nBy the definition, we have <cit.>\n\n    |t-t(j)|\u22642/((log x /e^j+1)log ((log x)/e^j+1).\n\nGiven this notation, let B be the large fixed natural number from Proposition\u00a0<ref>. Let \ud835\udca2(k) denote the event that for all |t|\u22641/2 and for all k\u2264 j \u2264loglog x - loglog R -B -2, we have\n\n    (log x/e^j+1log R e^C(x)  )^-1\u2264\u220f_\u2113 = j ^\u230aloglog x -loglog R \u230b-B-2\n    |I_\u2113(1/2-k/log x +it(\u2113)) | \u2264log x/e^j+1log R e^C(x),\n\nwhere notably, our C(x) is chosen as the \n\n\n\n\n\n    C(x):=loglog R + 100 logloglog x.\n\nWe shall establish the following two key propositions. The first proposition says that when we are restricted to the good event \ud835\udca2(k), the q-th moment is small. \n\nLet x be large and loglog R \u226a (loglog x)^1/2. Let \nC(x) be defined as in (<ref>). Let F_k^(R) be defined as in (<ref>) and \ud835\udca2(k) be defined as in (<ref>). For all 0\u2264 k \u2264\ud835\udca6 = \u230alogloglog x\u230b and 1/2\u2264 q \u2264 9/10, we have \n\n    [(\u222b_-1/2^1/21_\ud835\udca2(k) |F_ k^(R)(1/2 - k/log x + it)|^2dt)\u030a^q]\u030a\u226a( log x /e^klog R)\u030a^q ( C(x)/\u221a(loglog x))^q.\n\n\nThe second proposition is to show that indeed 1_\ud835\udca2(k) happens with high probability. \n\n\nLet \ud835\udca2(k) be defined as in (<ref>). For all 0\u2264 k \u2264\ud835\udca6= \u230alogloglog x\u230b and uniformly for all 1/2\u2264 q \u2264 9/10 and\nC(x) defined in (<ref>), we have\n\n    (\ud835\udca2(k)\u00a0fails) \u226a e^-C(x).\n \n\nThe above two key propositions imply Proposition\u00a0<ref>. \n\n   According to the good event \ud835\udca2(k) happening or not, we have \n   \n    [(\u222b_-1/2^1/2  |F_ k^(R)(1/2 - k/log x + it)|^2dt)\u030a^q]\u030a\n       \u2264[(\u222b_-1/2^1/21_\ud835\udca2(k) |F_ k^(R)(1/2 - k/log x + it)|^2dt)\u030a^q]\u030a +   [(\u222b_-1/2^1/21_\ud835\udca2(k)fails |F_ k^(R)(1/2 - k/log x + it)|^2dt)\u030a^q]\u030a\n       \u2264( log x /e^klog R)\u030a^q ( C(x)/\u221a(loglog x))^q +   (\u222b_-1/2^1/2 [|F_ k^(R)(1/2 - k/log x + it)|^2]dt )\u030a^q(\ud835\udca2(k)\u00a0fails)^1-q,\n\n  where in the first term we used Proposition\u00a0<ref> and we applied H\u00f6lder's inequality with exponents 1/q, 1/1-q to get the second term. We next apply the mean square calculation (<ref>) to derive that the above is \n    \n    \u226a( log x /e^klog R)\u030a^q(  ( C(x)/\u221a(loglog x))^q +  (\ud835\udca2(k)\u00a0fails)^1-q)\u030a.\n\n  Plug in the definition of C(x) and use Proposition\u00a0<ref> with 1-q \u2265 1/10 (and then the exceptional probability to the power 1/10 is negligible) to deduce that \n  \n    \u226a   e^-k/2( log x /log R)\u030a^q\u00b7 ( C(x)/\u221a(loglog x))^q,\n\n  which completes the proof. \n\n\n\nWe remark that in (<ref>), the quantity C(x)= loglog R +100 logloglog x is different from just being a constant C in <cit.>. The reason for our choice of C(x) is the following. Firstly, to keep the q-th moment in Proposition\u00a0<ref> has a saving (i.e. to make ( C(x)/\u221a(loglog x))^q small), we require that C(x)= o(\u221a(loglog x)).  Secondly, it turns out that in order to make the exceptional probability in Proposition\u00a0<ref> small enough, one has the constraint loglog R \u226a C(x). The combination of the above two aspects together leads to loglog R =o(\u221a(loglog x)). \n\n\n\n\n   In the deduction of Proposition\u00a0<ref>, we did not use an iterative process as used in <cit.>. Instead, we added an extra term 100logloglog x for the purpose of getting strong enough bounds on (\ud835\udca2(k)\u00a0fails). We simplified the proof by getting a slightly weaker upper bound in Theorem\u00a0<ref> as compensation.\n\n\n\n\n\n \u00a7.\u00a7 Proof of Proposition\u00a0<ref>\n\n\n\nThe proof of Proposition\u00a0<ref> is a simple modification of the proof of Key Proposition 1 in <cit.>. We emphasize again the main difference is instead of using a large constant C as in <cit.> but replacing it with C(x) defined in (<ref>), and we do not need the extra help from the quantity h(j) which hopefully makes the proof conceptually easier.  \n\nBy using H\u00f6lder's inequality, it suffices to prove that \n\n    [\u222b_-1/2^1/2 |F_k^(R)(1/2-k/log x + it)|^2dt ]\u226a e^-k\u00b7log x/log R\u00b7C(x)/\u221a(loglog x) ,\n\nuniformly for 0\u2264 k \u2264\ud835\udca6 = \u230alogloglog x \u230b and 1/2 \u2264 q\u2264 9/10. \nWe can upper bound the left-hand side of (<ref>) by \n\n    \u2264\u222b_-1/2^1/2[ |F_k^(R)(1/2-k/log x + it)|^2] dt\n\nwhere  is the event that \n\n    (log x/e^j+1log R e^C(x)  )^-1\u2264\u220f_\u2113 = j ^\u230aloglog x -loglog R \u230b-B-2\n    |I_\u2113(1/2-k/log x +it(\u2113)) | \u2264log x/e^j+1log R e^C(x)\n\nfor all k\u2264 j \u2264loglog x -loglog R -B -2. This is an upper bound as  is the event of  holds for all |t|\u22641/2. By the fact that f(n) has the same law as f(n)n^it, we have\n\n    \u222b_-1/2^1/2[ |F_k^(R)(1/2-k/log x + it)|^2] dt = \u222b_-1/2^1/2[ |F_k^(R)(1/2-k/log x )|^2 ]dt,\n\nwhere  denotes the event that \n\n    (log x/e^j+1log R e^C(x)  )^-1\u2264\u220f_\u2113 = j ^\u230aloglog x -loglog R \u230b-B-2\n    |I_\u2113(1/2-k/log x +i(t(\u2113)-t)) | \u2264log x/e^j+1log R e^C(x),\n\nfor all k\u2264 j \u2264loglog x- loglog R - B -2. \n We next apply Proposition\u00a0<ref>. \n\n\n\n\n\n\nIt is clear that \u210b(k,t) is the event treated in Proposition\u00a0<ref> with n=\u230aloglog x- loglog R \u230b -(B+1)-k; \u03c3= k/log x and t_m = t(\u230aloglog x- loglog R \u230b -(B+1) -m)-t for all m; and \n\n    a = C(x) + B+1,    h(j)=0.\n\nThe parameters indeed satisfy |\u03c3|\u22641/e^B+n+1 and |t_m|\u22641/m^2/3e^B+m+1\nfor all m. Apply Proposition\u00a0<ref> to derive\n\n    [ |F_k^(R)(1/2-k/log x)|^2]/[|F_k^(R)(1/2-k/log x)|^2] = (\u210b(k,t)) \u226amin{ 1, a/\u221a(n)}.\n\nA simple mean square calculation (see (<ref>)) gives that \n\n    [|F_k^(R)(1/2-k/log x)|^2] = exp(\u2211_R\u2264 p \u2264 x^e^-(k+1)1/p^1-2k/log x +O(1))\u030a\u226alog x/e^klog R.\n\nCombining the above two inequalities and the relation in (<ref>), we get the desired upper bound for the quantity in (<ref>). Thus, we complete the proof of (<ref>) and Proposition\u00a0<ref>.  \n\n\n\n\n\n\n \u00a7.\u00a7 Proof of Proposition\u00a0<ref>\n\nIn the proof, we will see why it is necessary to make C(x) large enough compared to loglog R. The proof starts with the union bound. We have \n\n    (\ud835\udca2(k)\u00a0fails) \u2264_1 +_2,\n\nwhere\n\n    _1 = \u2211_k\u2264 j \u2264log (log x/log R) -B-2( \u220f_\u2113 = j ^\u230alog (log x/log R) \u230b-B-2\n    |I_\u2113(1/2-k/log x + i t(\u2113)) | >log x/e^j+1log R e^C(x)\u00a0for some t)\u030a\n\nand \n\n    _2 = \u2211_k\u2264 j \u2264log (log x/log R) -B-2( \u220f_\u2113 = j ^\u230alog (log x/log R) \u230b-B-2\n    |I_\u2113(1/2-k/log x +i t(\u2113)) |^-1 >log x/e^j+1log R e^C(x)\u00a0for some t)\u030a,\n\nwhere |t|\u2264 1/2. \nWe focus on bounding _1, and _2 can be estimated similarly. Replace the set of all |t|\u2264 1/2 by the discrete set \n\n    \ud835\udcaf(x, j): = {n/((log x)/e^j+1) log ((log x)/e^j+1) : |n|\u2264 ((log x)/e^j+1) log ((log x)/e^j+1)  }\u030a,\n\nand apply the union bound to get \n\n    _1 = \u2211_k\u2264 j \u2264log (log x/log R) -B-2\n     t(j) \u2208\ud835\udcaf(x,j)( \u220f_\u2113 = j ^\u230alog (log x/log R) \u230b-B-2\n    |I_\u2113(1/2-k/log x +it(\u2113)) | >log x/e^j+1log R e^C(x))\u030a.\n\nBy using Chebyshev's inequality this is at most \n\n    \u2264\u2211_k\u2264 j \u2264log (log x/log R) -B-2\n     t(j) \u2208\ud835\udcaf(x,j)1/(log x/e^j+1log R e^C(x))^2[ \u220f_\u2113 = j ^\u230alog (log x/log R) \u230b-B-2\n    |I_\u2113(1/2-k/log x +it(\u2113)) |^2  ].\n\nSince f(n) and f(n)n^it have the same law, the above is \n\n    \u226a\u2211_k\u2264 j \u2264log (log x/log R) -B-2| \ud835\udcaf(x,j)|/(log x/e^j+1log R e^C(x))^2[ \u220f_\u2113 = j ^\u230alog (log x/log R) \u230b-B-2\n    |I_\u2113(1/2-k/log x ) |^2  ].\n\nThe expectation here is, again through a mean square calculation (<ref>), \u226alog x/e^j+1log R. Note |\ud835\udcaf(x, j)| \u2264 ((log x)/e^j+1) log ((log x)/e^j+1). \nWe conclude that \n\n    _1 \u226a\u2211_k\u2264 j \u2264log (log x/log R) -B-2e^loglog R-2C(x) + loglog ( log x / e^j+1)   \u226a e^-C(x),\n\nwhere in the last step we used that C(x)=   loglog R + 100 logloglog x. Thus we complete the proof of Proposition\u00a0<ref>. \n\n\n\n\n\n\n\u00a7 PROOF OF THEOREM\u00a0<REF>\n\n We first notice that if R>x^1/A for any fixed large constant A, then _R(x) is a set of elements with only O_A(1) number of prime factors. This would immediately imply that [|\u2211_n\u2208 f(n)|^4] \u226a_A ||^2 and by (<ref>), the conclusion follows. From now on, we may assume that \n\n    R\u2264 x^1/A.\n\nThe proof strategy of Theorem\u00a0<ref> again follows from <cit.>. The main differences lie in the design of the barrier events and taking advantage of R being large.  In particular, we do not need a \u201ctwo-dimensional Girsanov-type\" calculation which makes our proof less technical. \nWe first do the reduction step to reduce the problem to understanding certain averages of random Euler products, as in the upper bound proof. \n\nThere exists a large constant C such that the following is true. Let x be large and loglog R \u226b\u221a(loglog x). Let F^(R)(s) be defined as in (<ref>). Then,\nuniformly for all 1/2 \u2264 q\u2264 9/10 and any large V, we have \u2211_n\u2208 f(n)  _2q\n\n    \u226b\u221a(x/log x)( \u222b_-1/2^1/2 | F^(R)(1/2 +4V/log x + it)|^2 dt_q^1/2  - C/e^V\u222b_-1/2^1/2 | F^(R)(1/2 +2V/log x + it)|^2 dt_q^1/2 -C ).\n\n\n\nThe remaining  tasks are to give a desired lower bound on F^(R)(1/2 +4V/log x + it)_q^1/2 and  an upper bound on F^(R)(1/2 +2V/log x + it)_q^1/2. \nThe upper bound part is simple. Indeed, simply apply H\u00f6lder's inequality and do a mean square calculation (<ref>) to get\n\n    [(\u222b_-1/2^1/2|F^(R)(1/2 +2V/log x + it)|^2  dt)^q] \u226a(\u222b_-1/2^1/2[|F^(R)(1/2 +2V/log x + it)|^2]  dt )^q\u226a(log x/Vlog R)^q .\n\n\nWe next focus on the main task, giving a good lower bound on F^(R)(1/2 +4V/log x + it)_q^1/2. For each t\u2208\u211d, \nwe use L(t) denote the event for all \u230alog V \u230b +3 \u2264 j \u2264loglog x - loglog R -B -2, the following holds\n\n    (log x/e^j+1log R e^D(x)  )^-B\u2264\u220f_\u2113 = j ^\u230aloglog x -loglog R \u230b-B-2\n    |I_\u2113(1/2+4V/log x +it) | \u2264log x/e^j+1log R e^D(x),\n\nwhere D(x):= c\u221a(loglog x -loglog R) with \n\n    c= 1/4min{loglog R/\u221a(loglog x-loglog R) , 1 }\u224d 1.\n\nWe are now ready to define a random set \n\n    \u2112: = {-1/2\u2264 t \u2264 1/2: L(t)\u00a0defined by (<ref>) holds}.\n\nIt is clear that\n\n    [(\u222b_-1/2^1/2|F^(R)(1/2 +4V/log x + it)|^2  dt)^q] \u2265[(\u222b_\u2112|F^(R)(1/2 +4V/log x + it)|^2  dt)^q].\n\nWe use the following estimate and defer its proof to Section\u00a0<ref>.\n\nLet x be large and loglog R \u226b\u221a(loglog x). Let F^(R)(s) be defined as in (<ref>) and V be a large constant. Let \u2112 be the random set defined in (<ref>). Then uniformly for any 1/2\u2264 q\u2264 9/10, we have\n\n    [(\u222b_\u2112|F^(R)(1/2 +4V/log x + it)|^2  dt)^q] \u226b(log x/Vlog R)^q .\n\n\nPlug (<ref>), (<ref>) and (<ref>) into Proposition\u00a0<ref> with q=1/2\n(and choosing V to be a sufficiently large fixed constant so that C/e^V kills the implicit constant) to get that \n \n    [|\u2211_n\u2208_R(x) f(n) |] \u226b\u221a(|_R(x)|) .\n\nThis completes the proof of Theorem\u00a0<ref>.\n\n\n\n\n\u00a7 PROOF OF PROPOSITION\u00a0<REF>\n\n\nThe proof proceeds the same as in <cit.> (see also <cit.>) and we provide a self-contained proof here and highlight some small modifications. \n\nLet P(n) denote the largest prime factor of n as before. We have assumed that (<ref>) holds, e.g. R\u2264\u221a(x) (This restriction is not crucial but makes the notation later easier). Let \u03f5 denote a Rademacher random variable independent of f(n), and recall that  indicates that the variable n under the summation is R rough. For 1/2\u2264 q\u2264 9/10, we have \n\n    [|_n\u2264 x\n     P(n)>\u221a(x)f(n) |^2q]     = 1/2^2q[|_n\u2264 x\n     P(n)\u2264\u221a(x)f(n) + _n\u2264 x\n     P(n)>\u221a(x)f(n)+_n\u2264 x\n     P(n)>\u221a(x)f(n)-_n\u2264 x\n     P(n)\u2264\u221a(x)f(n)|^2q]\n       \u2264[|_n\u2264 x\n     P(n)\u2264\u221a(x)f(n) + _n\u2264 x\n     P(n)>\u221a(x)f(n)|^2q] + [|_n\u2264 x\n     P(n)>\u221a(x)f(n)-_n\u2264 x\n     P(n)\u2264\u221a(x)f(n)|^2q]\n        = 2[|\u03f5_n\u2264 x\n     P(n)>\u221a(x)f(n) + _n\u2264 x\n     P(n)\u2264\u221a(x)f(n)|^2q] = 2[|_n\u2264 x f(n)|^2q],\n\nwhere the last step we used the law of \n    \u03f5_n\u2264 x\n     P(n)>\u221a(x)f(n)= \u03f5\u2211_\u221a(x)<p\u2264 x f(p) _m\u2264 x/p f(m)\n conditional on (f(p))_R\u2264 p \u2264\u221a(x) is the same as the law of _n\u2264  x\n P(n)>\u221a(x)f(n). By the above deduction, it suffices to give a lower bound on _n\u2264 x\n P(n)>\u221a(x)f(n)_2q. \nDo the decomposition\n\n    _n\u2264 x\n     P(n)>\u221a(x)f(n) =\u2211_\u221a(x)\u2264 p \u2264 x f(p) _m\u2264 x/pf(m).\n\nThe inner sum is determined by (f(p))_R\u2264 p\u2264\u221a(x) and apply the Khintchine's inequality <cit.> to get\n\n    [|_n\u2264 x\n     P(n)>\u221a(x)f(n)|^2q] \u226b[(\u2211_\u221a(x)< p \u2264 x |_m\u2264 x/pf(m)|^2 )^q] \u22651/(log x)^q[(\u2211_\u221a(x)< p \u2264 xlog p\u00b7|_m\u2264 x/pf(m)|^2)^q].\n\nNext, do the smoothing step as we did in the upper bound case. Again set X = e^\u221a(log x). \nWrite \n\n    \u2211_\u221a(x)< p\u2264 xlog p \u00b7 |_m\u2264 x/pf(m)|^2 = \u2211_\u221a(x)<p\u2264 xlog p \u00b7X/p\u222b_p^p(1+1/X) |_m\u2264 x/p f(m)|^2dt.\n\nOne has |a+b|^2\u2265 a^2/4 - min{|b|^2, |a/2|^2}\u2265 0 and thus the above is at least \n\n    1/4\u2211_\u221a(x)<p\u2264 xlog p \u00b7X/p\u222b_p^p(1+1/X) |_m\u2264 x/t f(m)|^2dt \n        -\u2211_\u221a(x)<p\u2264 xlog p \u00b7X/p\u222b_p^p(1+1/X)min{|_x/t\u2264 m\u2264 x/p f(m)|^2, 1/4 |_m\u2264 x/t f(m)|^2}.\n\nIt follows that the quantity we are interested in has the lower bound\n\n    [|_n\u2264 x\n     P(n)>\u221a(x)f(n)|^2q] \u2265   1/(log x)^q[(1/4\u2211_\u221a(x)<p\u2264 xlog p \u00b7X/p\u222b_p^p(1+1/X) |_m\u2264 x/t f(m)|^2dt)^q] \n        - 1/(log x)^q[(\u2211_\u221a(x)<p\u2264 xlog p \u00b7X/p\u222b_p^p(1+1/X) |_ x/t<m\u2264 x/p f(m)|^2dt)^q].\n\nUse H\u00f6lder's inequality and throw away the R-rough condition to upper bound the subtracted term in (<ref>) by\n\n    \u22641/(log x)^q(\u2211_\u221a(x)<p\u2264 xlog p \u00b7X/p\u222b_p^p(1+1/X)[ |\u2211_ x/t<m\u2264 x/p f(m)|^2]dt)\u030a^q\n       \u226a1/(log x)^q(\u2211_\u221a(x)<p\u2264 x log p \u00b7  (x/pX +1) )^q\u226a1/(log x)^q(xlog x/X +x)^q\u226a (x/log x)^q.\n\nThe first term in (<ref>) (without the factor 1/4(log x)^q) is \n\n    [(\u2211_\u221a(x)<p\u2264 xlog p \u00b7X/p\u222b_p^p(1+1/X) |_m\u2264 x/t f(m)|^2dt)^q] \n        = [(\u222b_\u221a(x)^x_t/1+1/X <p\u2264 t log p \u00b7X/p|_m\u2264 x/tf(m)|^2 dt )^q]\n       \u226b [(\u222b_\u221a(x)^x|_m\u2264 x/tf(m)|^2dt )^q] = x^q[(\u222b_1^\u221a(x)  |_m\u2264 z f(m) |^2dz/z^2)^q].\n\nTo this end, we impose the smooth condition to invert the sums to Euler products. We have for any large V, \n\n    [(\u222b_1^\u221a(x)  |_m\u2264 z f(m) |^2dz/z^2)^q] \u2265[(\u222b_1^\u221a(x)  |_m\u2264 z\n     x-smooth f(m) |^2dz/z^2+8V/log x)^q]\n       \u2265[(\u222b_1^+\u221e  |_m\u2264 z\n     x-smooth f(m) |^2dz/z^2+8V/log x)^q]- [(\u222b_\u221a(x)^+\u221e  |_m\u2264 z\n     x-smooth f(m) |^2dz/z^2+8V/log x)^q]\n       \u2265[(\u222b_1^+\u221e  |_m\u2264 z\n     x-smooth f(m) |^2dz/z^2+8V/log x)^q]- 1/e^2Vq[(\u222b_1^+\u221e  |_m\u2264 z\n     x-smooth f(m) |^2dz/z^2+4V/log x)^q].\n\nApply Lemma\u00a0<ref> to get that the first term is \n\n    \u226b[(\u222b_-1/2^1/2|F^(R)(1/2 + 4V/log x + it)|^2 dt )^q].\n\nFor the second term, an application of Lemma\u00a0<ref> gives\n\n    \u226a e^-2Vq[(\u222b_-\u221e^+\u221e|F^(R)(1/2 + 2V/log x + it)/|1/2 + 2V/log x + it|^2 dt )^q] \u226a e^-2Vq[(\u222b_-1/2^1/2 |F^(R)(1/2 + 2V/log x + it)|^2 )^q]\n\nwhere in the last step we used the fact that f(n)n^it has the same law as f(n) and \u2211_n \u2265 1 n^-2 converges. Bounds in (<ref>) and (<ref>) together give the desired bound for the first term in (<ref>) and we complete the proof. \n\n\n\n\n\n\u00a7 PROOF OF PROPOSITION\u00a0<REF>\n\nIn this section, \nwe prove Proposition\u00a0<ref>. The proof significantly relies on the following proposition, which is a mean value estimate of the product of |F^(R)(\u03c3 + it_1)|^2\nand |F^(R)(\u03c3 + it_2)|^2. Our upper bound matches the guess if you pretend the two products are independent. \n\nLet x be large and loglog R \u226b\u221a(loglog x). Let F^(R)(s) be defined as in (<ref>) and V be a large constant. Let \u2112 be the random set defined in (<ref>). Then we have\n    \n    [(\u222b_\u2112|F^(R)(1/2 +4V/log x + it)|^2  dt)^2]  \u226a (log x/Vlog R)^2.\n\n\n\n\n\n\nThe proof starts with an application of H\u00f6lder's inequality. We have \n\n    [(\u222b_\u2112|F^(R)(1/2 +4V/log x + it)|^2  dt)^q] \u2265([\u222b_\u2112|F^(R)(1/2 +4V/log x + it)|^2  dt])^2-q/([(\u222b_\u2112|F^(R)(1/2 +4V/log x + it)|^2  dt)^2])^1-q.\n\nProposition\u00a0<ref> gives a desired upper bound for the denominator. We next give a lower bound on the numerator. By using that f(n)n^it has the same law as f(n), the numerator is\n\n    (\u222b_-1/2^1/2 [1_L(t)|F^(R)(1/2 +4V/log x + it)|^2]  dt)^2-q= ([ 1_L(0)|F^(R)(1/2 +4V/log x )|^2] )^2-q .\n\nWe next use Proposition\u00a0<ref> by taking n=\u230aloglog x - loglog R \u230b - (B+1) - \u230alog V \u230b, a =D(x)=c\u221a(loglog x -loglog R ) and h(j)=0 to conclude that (1_L(0))\u226b 1. Combining with the mean square calculation\u00a0(<ref>), we have\n\n    [1_L(0)|F^(R)(1/2 +4V/log x + it)|^2] \u226b(1_L(0))\u00b7 [|F^(R)(1/2 +4V/log x + it)|^2] \u226blog x/V log R.\n\nWe complete the proof by plugging (<ref>) and (<ref>) into (<ref>). \n\n\nThe proof of Proposition\u00a0<ref> is a bit involved and its proof is inspired by <cit.> and <cit.>. We are not using the \u201ctwo-dimensional Girsanov-type\" computation as used in <cit.> which significantly simplified the proof. We do not expect any further savings when R is as large as stated in Proposition\u00a0<ref> while for a smaller R, one might expect there could be further cancellation as in <cit.> which may be verified by adapting the \u201ctwo-dimensional Girsanov-type\" calculation. \n\n\n\n    Expand the square and it equals\n    \n    [ \u222b_-1/2^1/21_L(t_1) |F^(R)(1/2 +4V/log x + it_1)|^2 dt_1 \u222b_-1/2\n        ^1/21_L(t_2)|F^(R)(1/2 +4V/log x + it_2)|^2 dt_2   ].\n\n   By using that f(n)n^it has the same law as f(n), we write the above as (t:= t_1-t_2)\n   \n    \u222b_-1^1 [1_L(0) |F^(R)(1/2 +4V/log x )|^21_L(t)|F^(R)(1/2 +4V/log x + it)|^2 ] dt.\n\nFor |t| large enough, the two factors behave independently, which is the easier case. Indeed,  if |t|>1/log R, drop the indicator functions and bound the corresponding integration by\n\n    \u226amax_ 1/log R < |t|\u2264 1 [  |F^(R)(1/2 +4V/log x)|^2\u00b7 |F^(R)(1/2 +4V/log x + it)|^2   ]  .\n \n Apply the two dimensional mean square calculation (<ref>) with (x, y)=(R, x)  to conclude that the above is \n \n    \u226a(log x/V log R)^2.\n\n \nWe next focus on the case |t|\u2264 1/log R. Since f(p) are independent of each other, we can decompose the Euler products into pieces and analyze their contributions to (<ref>) separately.\nDefine the following three sets of primes based on the sizes of primes\n\n    \ud835\udcab_1: = {p\u00a0prime: R\u2264 p < x^e^-(\u230aloglog x -loglog R \u230b-B-2)},\n\n\n    \ud835\udcab_2: = {p\u00a0prime:  x^e^-(\u230aloglog x -loglog R \u230b-B-2)\u2264 p \u2264 x^e^-(\u230alog V \u230b +3)},\n\nand \n\n    \ud835\udcab_3: = {p\u00a0prime: x^e^-(\u230alog V \u230b +3) < p \u2264 x }.\n\nWe proceed as follows. Note that the events L(0) and L(t) are irrelevant to f(p) for p\u2208\ud835\udcab_1 \u222a\ud835\udcab_3. For partial products over primes p\u2208\ud835\udcab_1 \u222a\ud835\udcab_3, we directly do mean square calculations.\nFor partial products over primes p\u2208\ud835\udcab_2, we will crucially use the indicator functions 1_L(0) and 1_L(t) defined in (<ref>) with j= \u230alog V \u230b +3. This separation gives that the integration in (<ref>) over |t|\u2264 1/log R is\n\n    \u222b_|t|\u22641/log R[\u220f_p\u2208\ud835\udcab_1 \u222a\ud835\udcab_3  |1-f(p)/p^1/2 +4V/log x|^-2 |1-f(p)/p^1/2 +4V/log x+it|^-2] \n    \u00d7[1_L(0)1_L(t)\u220f_p\u2208\ud835\udcab_2  |1-f(p)/p^1/2 +4V/log x|^-2 |1-f(p)/p^1/2 +4V/log x+it|^-2]  dt.\n\nWe first upper bound the expectation over primes in \ud835\udcab_1 \u222a\ud835\udcab_3 uniformly over all t. By using independence between f(p) and (<ref>), we can bound it as \n\n    \u226aexp( \u2211_p\u2208\ud835\udcab_14/p^1+8V/log x + \u2211_p\u2208\ud835\udcab_34/p^1+8V/log x).\n\nBy simply using the prime number theorem and the definition of \ud835\udcab_1 and \ud835\udcab_3, one has that both sums in (<ref>)  are \u226a 1 so that (<ref>) is \u226a 1, where we remind readers that B is a fixed constant. Now our task is reduced to establishing the following\n\n    \u222b_|t|\u22641/log R[1_L(0)1_L(t)\u220f_p\u2208\ud835\udcab_2  |1-f(p)/p^1/2 +4V/log x|^-2 |1-f(p)/p^1/2 +4V/log x+it|^-2]  dt \u226a(log x/V log R)^2.\n\n\nOur strategy would be, roughly speaking, using the barrier event 1_L(t) to bound certain partial products involved with t directly and then use the mean square calculation to deal with the rest of the products. The exact partial products that we will apply barrier events would depend on the size of t.  \n\nWe first do a simple case, which helps us get rid of the very small t, say |t|<V/log x. We use the the condition 1_L(t) and pull out the factors related to L(t) to get that the contribution from |t|<V/log x is at most \n\n    \u226a\u222b_|t|\u2264V/log x e^2c\u221a(loglog x- loglog R)\u00b7 (log x/V log R)^2\u00b7[1_L(0)\u220f_p\u2208\ud835\udcab_2  |1-f(p)/p^1/2 +4V/log x|^-2]  dt \n       \u226aV/log x\u00b7  e^2c\u221a(loglog x- loglog R)\u00b7(log x/V log R)^2\u00b7[\u220f_p\u2208\ud835\udcab_2  |1-f(p)/p^1/2 +4V/log x|^-2] \n       \u226a(log x/V log R)^2,\n\nwhere in the second last step we dropped the 1_L(0) condition, and in the last step we applied (<ref>) together with log R \u2265exp(4c  \u221a(loglog x)) where c is defined in (<ref>). Thus we only need to establish the following \n\n    \u222b_V/log x\u2264 |t|\u22641/log R[1_L(0)1_L(t)\u220f_p\u2208\ud835\udcab_2  |1-f(p)/p^1/2 +4V/log x|^-2 |1-f(p)/p^1/2 +4V/log x+it|^-2]  dt \u226a(log x/V log R)^2 .\n\n\nWe now enter the crucial part where we will apply the barrier events according to the size of |t|.\nWe decompose the set \ud835\udcab_2 into two parts according to |t|.\nFor each fixed V/log x\u2264 |t| \u2264 1/log R, we write \n\n    \ud835\udcab_2 = \ud835\udcae(t) \u222a\u2133(t),\n\nwhere \n\n    \ud835\udcae(t):={p\u00a0prime:  x^e^-(\u230aloglog x -loglog R \u230b-B-2)\u2264 p \u2264 e^V/|t|},\n\nand \n\n    \u2133(t):= {p\u00a0prime:  e^V/|t|\u2264 p \u2264 x^e^-(\u230alog V \u230b +3)}.\n\nThe set of primes \ud835\udcae(t) would be those we will apply barrier events and \u2133(t) would be estimated by a mean square calculation. Note that for p\u2208\u2133(t), there is a nice decorrelation as we needed in (<ref>) due to that p\u2265 e^V/|t|. \nLet us now see how such a decomposition of \ud835\udcab_2 would help us. We use a local notation\n\n    G(p, t): = |1-f(p)/p^1/2 + 4V/log x+it|^-2.\n\nThen the quantity in (<ref>) is the same as \n\n    \u222b_V/log x\u2264 |t|\u22641/log R[1_L(0)1_L(t)\u220f_p\u2208\ud835\udcab_2 G(p, 0) \u220f_p\u2208\ud835\udcae(t) G(p, t) \u220f_p\u2208\u2133(t) G(p, t)  ]  dt.\n\nWe apply the barrier events condition 1_L(t) to bound the product over p\u2208\ud835\udcae(t) so that the above is at most \n\n    \u226a(V/log R)^2\u00b7 e^2c\u221a(loglog x - loglog R)\u00b7\u222b_V/log x\u2264 |t|\u22641/log R1/t^2[1_L(0)\u220f_p\u2208\ud835\udcab_2 G(p, 0) \u220f_p\u2208\u2133(t) G(p, t)  ]  dt.\n\nWe next upper bound the expectation in (<ref>) uniformly for all V/log x\u2264 |t| \u2264 1/log R. We first drop the indicator function and rewrite the product based on the independence between f(p) to derive that\n\n    [1_L(0)\u220f_p\u2208\ud835\udcab_2 G(p, 0) \u220f_p\u2208\u2133(t) G(p, t)]\u2264[ \u220f_p\u2208\ud835\udcae(t) G(p, 0)] \u00b7 [\u220f_p\u2208\u2133(t) G(p, 0)G(p, t)].\n\n Use the mean square calculation results in (<ref>) and (<ref>) to further get an upper bound on the expectation\n \n    \u226aV/|t|/log R\u00b7(tlog x/V^2)^2\u226a|t|(log x)^2/V^3log R .\n\nNow we plug the above bound to (<ref>) to get that (<ref>) is crudely bounded by \n\n    (log x/log R)^2\u00b7e^2c\u221a(loglog x- loglog R)/Vlog R\u00b7\u222b_V/log x\u2264 |t|\u22641/log R1/|t| dt \u226a(log x/log R)^2.\n\nIn the last step we used that log R \u2265exp(4c  \u221a(loglog x)) where c is defined in (<ref>).  This completes the proof of (<ref>) and thus the proof of the proposition. \n\n\n\n\n\u00a7 CONCLUDING REMARKS\n\n\n\n\n \u00a7.\u00a7 Typical behavior and small perturbations\n\nWe give a sketch of the situation when a(n) itself is independently and randomly chosen. \nWe write \n\n    a(n) = r(n) X(n)\n\nwhere r(n)>0 is deterministic and X(n) are independently distributed with [|X(n)|^2]=1. We may naturally assume that there is some r such that\n\n    r(n) \u224d r(m) \u224d r\n\nfor all n, m, i.e. no particular random variable would dominate the whole sum in size. One may also just assume r=1 throughout the discussion here. \nWe claim that for typical X(n), the random sums satisfy the sufficient condition established in <cit.> on having a Gaussian limiting distribution.\n\nThe key condition one needs to verify is that almost surely (in terms of over X(n)), we have \n\n    R_N(a) : =\u2211_ m_i, n_j\u2264 N \n    \n     m_i\u2260 n_j \n     m_1m_2=n_1n_2   a(n_1)a(n_2) a(m_1) a(m_2) = o(r^4N^2).\n\nThe proof of (<ref>) is straightforward. By using the divisor bound, we know there are \u226a N^2+\u03f5 number of quadruples (m_1, m_2, n_1, n_2) under the summation. If we expect some square-root cancellation among a(n_1)a(n_2) a(m_1) a(m_2), then R_N(a) above should be around r^4N^1+ typically. \nIndeed, by using the fact that all a(n) are independent, we have the L^2 bound \n\n    [|R_N|^2] = [R_N R_N] \u226a r^8 N^2+.\n\nThis leads to, \nalmost surely (in terms of over X(n)), that we have \n\n    R_N(a) = o(r^4N^2).\n\nTo this end, by using <cit.>, almost surely, we have a central limit theorem for the random partial sums of a Steinhaus random multiplicative function.\nSee <cit.> for a closely related result where they used the method of moments.  \n\n\n\n\nIn Question\u00a0<ref>, we asked if it is possible to characterize the choices of a(n) that give better than square-root cancellation. On one hand, as discussed above, we know for typical a(n), there is just square-root cancellation. On the other hand, if a(n) is a deterministic multiplicative function taking values on the unit circle, then by the fact that a(n)f(n) has the same distribution as f(n) and the result established by Harper (<ref>), the partial sums \u2211_n\u2264 N a(n)f(n) have better than square-root cancellation. Our main theorems study one particular example of multiplicative nature.  Combining these observations, \nwe believe that any small perturbation coming from a(n) that destroys the multiplicative structure would make the better than square-root cancellation in (<ref>) disappear. We ask the following question in a vague way as a sub-question of Question\u00a0<ref>.\n\nIs it true that the only \u201cessential choice\" of a(n) leading to better than square-root cancellation is of multiplicative nature? \n\n\n\n\n \u00a7.\u00a7 Threshold in other settings and the limiting distribution\n\nThe main theorems of this paper prove that there is square-root cancellation for loglog R \u226b (loglog x)^1/2. What is the limiting distribution then? We have remarked earlier that one may establish a central limit theorem when R\u226bexp((log x)^c) for some  constant c<1 by understanding the corresponding multiplicative energy. It becomes less clear for smaller R. \n\n\n What is the limiting distribution of  \u2211_n\u2208_R(x) f(n) with \u201cproper\" normalization, for all ranges of R?   \n\n\nWe finally comment that there is another family of partial sums that naturally has the threshold behavior for better than square-root cancellation. Let = [x, y] with y\u2264 x. We would like to know for what range of y, typically, \n\n    \u2211_x\u2264 n \u2264 x+y f(n) = o(\u221a(y)).\n\nWe believe one can adapt the argument here to find that the threshold behavior is around log (x/y) \u2248\u221a(loglog x). It is certainly interesting to understand the limiting distribution for the short interval case thoroughly, beyond the previous result in <cit.>.  \n\n\n\n\n\n\nplain\n\n"}