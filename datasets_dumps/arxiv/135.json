{"entry_id": "http://arxiv.org/abs/2303.07169v2", "published": "20230313151230", "title": "Dynamic Event-based Optical Identification and Communication", "authors": ["Axel von Arnim", "Jules Lecomte", "Stanislaw Wozniak", "Naima Elosegui Borras", "Angeliki Pantazi"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.NE"], "text": "\n\n\nValidation of uncertainty quantification metrics: a primer \n    Pascal PERNOT 0000-0001-8586-6222\n    March 30, 2023\n===========================================================\n\n\n\n\nOptical identification is often done with spatial or temporal visual pattern recognition and localization. Temporal pattern recognition, depending on the technology, involves a trade-off between communication frequency, range and accurate tracking. We propose a solution with light-emitting beacons that improves this trade-off by exploiting fast event-based cameras and, for tracking, sparse neuromorphic optical flow computed with spiking neurons. In an asset monitoring use case, we demonstrate that the system, embedded in a simulated drone, is robust to relative movements and enables simultaneous communication with, and tracking of, multiple moving beacons. Finally, in a hardware lab prototype, we achieve state-of-the-art optical camera communication frequencies in the kHz magnitude.\n\n\n\nNeuromorphic Computing, Event-Based Sensing, Optical Camera Communication, Optical Flow\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nIdentifying and tracking objects in a visual scene has many applications in sports analysis, swarm robotics, urban traffic, smart cities and asset monitoring. Wireless solutions have been widely used for object identification, such as radio frequency identification <cit.> or more recently Ultra Wide Band <cit.>, but these do not provide direct localization and require meshes of anchors and additional processing. One efficient solution is to use a camera to detect specific visual patterns attached to the objects. However, spatial pattern detection has a limited range depending on the camera's spatial resolution. With the rise of fast cameras, temporal light patterns can be used instead, where the detection range depends only on the light intensity of emitting beacons.\nThis technique is known as Optical Camera Communication (OCC) and has been developed primarily for communication between static objects <cit.>.\n\nIn applications such as asset monitoring on a construction site, it is important to track dynamically moving objects. OCC techniques potentially enable simultaneous communication with, and tracking of, beacons. However, two challenges arise in the presence of relative movements: filtering out the noise and tracking the  beacons' positions. Increasing the temporal frequency of the transmitted signal, since noise has lower frequencies than the beacon's signal, addresses this problem. Nevertheless, current industrial cameras do not offer a satisfying spatio-temporal resolution trade-off. Biologically-inspired event cameras, operating with temporally and spatially sparse events, achieve pixel frequencies on the order of 10^4\u00a0Hz and can be combined with Spiking Neural Networks (SNNs) to build low-latency neuromorphic solutions.\n\nIn this paper, we propose to exploit the fine temporal and spatial resolution of event cameras to tackle the challenge of simultaneous OCC and tracking, where the latter is based on the optical flow computed from events by an SNN. We evaluate our approach with a simulated drone monitoring assets on a construction site.\nWe further introduce a hardware prototype comprising a beacon and an event camera, which we use for demonstrating an improvement over state-of-the-art OCC range.\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\nOptical identification is commonly implemented with frame-based cameras, either by recognizing a spatial pattern in each single image \u2013 for instance for license plate recognition <cit.> \u2013 or by reading a temporal pattern from an image sequence <cit.>. In the latter, blinking beacons encode a number in binary format, similarly to Morse code, to identify cars or road signs. As explained before, because of the resolution/frame rate trade-off, frame-based cameras impose a hard limit on the beacon's frequency.\n\nEvent cameras overcome this limitation by capturing individual pixel intensity changes extremely fast rather than full frames <cit.>. Early work combined the fine temporal and spatial resolution of an event camera with blinking LEDs at different frequencies to perform visual odometry <cit.>. Recent work also relies on smart beacons to transmit a message with the UART protocol <cit.>, delivering error-free messages at up to 4\u00a0kbps indoors and up to 500\u00a0bps at 100\u00a0m distance outdoors with brighter beacons.\n\nOn the tracking front \u2013 to track moving beacons in our case \u2013 a widely used technique is optical flow <cit.>. Model-free techniques relying on event cameras for object detection have been implemented in <cit.> and  <cit.>.\nTo handle the temporal and spatial sparsity of an event camera, a state-of-the-art deep learning frame-based approach <cit.> was adapted to produce dense optical flow estimates from events <cit.>. However, a much simpler and more efficient solution is to compute sparse optical flow with inherently sparse biologically-inspired SNNs <cit.>.\n\n\n\n\n\u00a7 SYSTEM DESCRIPTION\n\n\nThe system that we propose is composed of an emitter and a receiver. The former is a beacon emitting a temporal pattern with near infrared light (visible to cameras, but not to humans), attached to the object to be identified and tracked. The receiver component is an event-based camera connected to a computer which, in turn, executes decoding and tracking algorithms. The receiver part comprises algorithmic components for clustering and tracking in which an SNN calculates optical flow. The entire process, from low-level event-processing to high-level sequence-decoding, is schematically depicted in Fig. <ref>.\n\nb\n\n\n\n \u00a7.\u00a7 Event-Based Communication\n\n\n\nThe emitter is synchronously transmitting, with a blinking pattern, a binary sequence S that consists of a start code S_c, a data payload (identification number) S_p and a parity bit f(S_p), where f returns 1 if S_p has an even number of ones, or 0 otherwise. The start code and the parity bit delimit the sequence and confirm its validity, as illustrated in Fig. <ref>. On the receiver side, the event camera asynchronously generates events upon pixel brightness changes, which can be caused by either a change in the beacon's signal or visual noise in the scene. The current state of the beacon (on or off) cannot be detected by the sensor. Rather, the sensor detects when the beacon transitions between these states. The signal frequency being known, the delay between those transitions gives the number of identical bits emitted. In comparison to a similar architecture with a frame-based camera (200\u00a0Hz frame rate) as in <cit.>, our setup relies on an event camera and a beacon blinking in kHz frequency, allowing for a short beacon decoding time, better separation from noise and easier tracking since beacon's motions are relatively slower. \n\n\n\n\n\n \u00a7.\u00a7 Event-Based Optical Flow\n\n\nb\n\nEvent-based optical flow is used as a given input for tracking. It is computed from the same camera and events that are used for decoding, and delivers a sparse vector field for visible events with velocity and direction.\n\nWe implemented an SNN architecture with Spiking Neural Units (SNUs) <cit.> and extended the model with synaptic delays that we call \u0394SNU. Its state equations are:\n\n    s_t    = g(W d_\u0394(x_t) + l(\u03c4)  s_t-1 (1-y_t-1) ) \n    \n    y_t    = h(s_t - v_th),\n\nwhere W are the weights, v_th is a threshold, s_t is the state of the neuron and l(\u03c4) its decay rate, y_t is the output, g is the input activation function, h is the output activation function, and d is the synaptic delay function.\nThe delay function d is parameterized with a delay matrix \u0394 that for each neuron and synapse determines the delay at which spikes from each input x_t will be delivered for the neuronal state calculation.\n\nOptical flow is computed by a CNN with 5\u00d75 kernels, illustrated in Fig.<ref>a.\nEach \u0394SNU is attuned to the particular direction and speed of movement through its specific synaptic delays, similarly to <cit.>. When events matching the gradient of synaptic delays are observed, a strong synchronized stimulation of the neuron leads to neuronal firing. This results in sparse detection of optical flow.\nThe synaptic delay kernels are visualized in Fig. <ref>b. We use 8 directions and 4 magnitudes, with the maximum delay period corresponding to 10 executions of the tracking algorithm. Weights are set to one and v_th=5, which yielded the best tracking results.\n\n\n\n\n\n \u00a7.\u00a7 Object Tracking\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Clustering\n\n\nCamera events are being accumulated in a time window and clustered with the Density-Based Spatial Clustering of Applications with Noise <cit.>, chosen to get rid of noisy, isolated events and to retrieve meaningful objects from the visual scene. Such clusters are filtered according to:\n\n    N_e/\u03c0\u00d7 |b-d|^2 > r,   \n        N_e \u2208{N_min;N_max}\n\nwhere N_e is the number of events in the cluster, |b-d| the Euclidean distance between the cluster's barycenter b and d its most distant event, r a shape ratio, and N_min and N_max the minimal/maximal emitter size in pixel.\n\nWe reduce the remaining clusters to their barycenter, size and their polarity and call these \"targets\". The polarity of a target P is given by P=(\u2211_ip_i)/ N_e where p_i=1 for a positive polarity and -1 for a negative one for each event i.\n\n\n\n  \u00a7.\u00a7.\u00a7 Tracking\n\n\nTargets are kept in memory for tracking over time and are then called tracks. A Kalman filter is attributed to each track and updated for every processed time window, as depicted in Fig. <ref>. Similarly to <cit.>, predicted tracks' states are matched to detected targets to minimize the L1-norm between tracks and targets. Unmatched targets are registered as new tracks.\n\n\n\nb\n\n\n\n  \u00a7.\u00a7.\u00a7 Identification\n\n\nA matched track's sequence is updated using the target's mean event polarity P.\n\n  \n  * If P\u22650.5 then the beacon is assumed to have undergone an on transition. We add n=(t_c-t_t) / f_beacon zeros to the binary sequence where t_c is the current timestamp, t_t is the stored last transition timestamp and f_beacon is the beacon blinking frequency and set t_t=t_c.\n  \n  * If P\u2264-0.5 then the beacon is assumed to have undergone a transition to the off state. Likewise, we add n ones to the binary sequence.\n  \n  * Otherwise, the paired beacon most likely has not undergone a transition but just moved.\n\nSimilarly to <cit.>, a confidence value is computed to classify tracks as new, valid or invalid, as illustrated in Alg.<ref>. Indeed, noise can pass clustering filters but will soon be invalidated as its confidence will never rise. To correct for errors (for instance due to occlusions), the confidence increments are larger than decrements. When a track's sequence is long enough to be decoded, this sequence is valid if it complies with the protocol and maintains the same payload (if this track was previously correctly recognized).\n\n\n\n\u00a7 RESULTS\n\n\n\n\n\n \u00a7.\u00a7 Static Identification\n\nOur hardware beacon has four infrared LEDs (850\u00a0nm) and an ESP32 micro-controller to set the payload S_p=42 and the blinking frequency. To receive the signal, we used a DVXplorer Mini camera, with a resolution of 640\u00d7480 and a 3.6\u00a0mm focal length lens. \nIn a static indoor setup, the hardware event camera enables us to achieve high data transmission frequencies, plotted in Fig. <ref>. The metric is the Message Accuracy Rate (MAR): the percentage of correct 11-bit sequences decoded from the beacon's signal during a recording. The MAR stays over 94\u00a0% up to 2.5\u00a0kHz, then decreases quickly, due to the temporal resolution of the camera. Using a 16\u00a0mm focal length lens we could identify the beacon at a distance of 11.5\u00a0m indoors, with 87\u00a0% MAR and a frequency of 1\u00a0kHz and obtained 100\u00a0% MAR at 100\u00a0Hz at 16\u00a0m \u2013 see Fig <ref>.\n\n\n\n\nt0.42\n\n\n\n \u00a7.\u00a7 Dynamic Identification\n\nTo evaluate our identification approach in a dynamic setup, where tracking is required, a simulated use case was developed in the Neurorobotics Platform <cit.>. A Hector drone model, with an on-board event camera plugin <cit.>, flies over a construction site with assets (packages and workers) to be identified and tracked. These are equipped with blinking beacons. The drone follows a predefined trajectory and the scene is captured from a bird's eye view \u2013 see Fig. <ref>. A frame-based camera with the same view is used for visualization. Noise is simulated with beacons of different sizes blinking randomly. For varying drone trajectories, assets were correctly identified at up to 28\u00a0m, with drone speeds up to 10\u00a0m/s (linear) and 0.5\u00a0radian/s (self rotational). Movements were fast, relative to the limited 50\u00a0Hz beacon frequency imposed by the simulator. A higher MAR was obtained with a Kalman filter integrating optical flow (<ref>) than without it \u2013 see Tab. <ref>. MAR and Bit Accuracy Rate (BAR) are correlated in simulation because they drop together only upon occlusion. Finally, we conducted hardware experiments where a beacon was moved at 2\u00a0m/s reaching a 94\u00a0% BAR at 5m and a 87\u00a0% BAR at 16m. This shows that our system enables accurate identification and data transmission even with moving beacons, which, to our knowledge, is beyond the state-of-the-art.\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\n\n\nWe propose a novel approach for identification that combines the benefits of event-based fast optical communication and signal tracking with spiking optical flow.\nThe approach was validated in a simulation of drone-based asset monitoring on a construction site. A hardware prototype setup reached state-of-the-art optical communication speed and range. We propose the first \u2013 to the best of our knowledge \u2013 system to identify fast moving, variable beacons with an event camera.\n\n\n\n\u00a7 ACKNOWLEDGEMENTS\n\n\nThe research was carried out within IBM and fortiss Center for AI (C4AI).\nWe thank its team for discussions and assistance.\nThe research at fortiss was supported by the HBP Neurorobotics Platform funded through the European Union\u2019s Horizon 2020 Framework Program for Research and Innovation under the Specific Grant Agreements No. 945539 (Human Brain Project SGA3).\n\nIEEEbib\n\n\n\n"}