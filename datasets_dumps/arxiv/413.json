{"entry_id": "http://arxiv.org/abs/2303.06747v1", "published": "20230312204907", "title": "Raising The Limit Of Image Rescaling Using Auxiliary Encoding", "authors": ["Chenzhong Yin", "Zhihong Pan", "Xin Zhou", "Le Kang", "Paul Bogdan"], "primary_category": "cs.CV", "categories": ["cs.CV", "eess.IV"], "text": "\n\n\nDTT: An Example-Driven Tabular Transformer by Leveraging Large Language Models \n    Davood Rafiei\n    March 30, 2023\n===============================================================================\n\n\n\nNormalizing flow models using invertible neural networks (INN) have been widely investigated for successful generative image super-resolution (SR) by learning\nthe transformation between the normal distribution of latent variable z and the conditional distribution of high-resolution\n(HR) images gave a low-resolution (LR) input.  Recently, image rescaling models like IRN utilize the bidirectional nature of INN to push the performance limit of image upscaling\nby optimizing the downscaling and upscaling steps jointly.  While the random sampling of latent variable z is useful in generating diverse\nphoto-realistic images, it is not desirable for image rescaling when accurate restoration of the HR image is more important.\nHence, in places of random sampling of z, we propose auxiliary encoding modules to further push the limit of image rescaling performance.\nTwo options to store the encoded latent variables in downscaled LR images, both readily supported in existing image file format,\nare proposed. \nOne is saved as the alpha-channel, the other is saved as meta-data in the image header, and the corresponding modules are denoted as suffixes -A and -M respectively.\nOptimal network architectural changes are investigated for both options to demonstrate their effectiveness\nin raising the rescaling performance limit on different baseline models including IRN and DLV-IRN.\n\n\n\n\n\n\nSuper-Resolution, Image Rescaling\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nCurrently, ultra-high resolution (HR) images are often needed to be reduced from their original resolutions to lower ones due to various limitations like display or transmission.\nOnce resized, there could be subsequent needs of scaling them up so it is useful to restore more high-frequency details\u00a0<cit.>.\nWhile deep learning super-resolution (SR) models\u00a0<cit.> are powerful tools to reconstruct HR images from low-resolution (LR) inputs, they are often limited to pre-defined image downscaling methods. Additionally, due to memory and speed constraints, HR images or videos are also commonly resized to lower resolution for downstream computer vision tasks\nlike image classification and video understanding. Similarly, they rely on conventional resizing methods which are subject to information loss and have negative impact on downstream tasks\u00a0<cit.>.\nHence, learned image downscaling techniques with minimum loss in high-frequency information are quite indispensable for both scenarios.\nLastly, it is known that SR models optimized for upscaling only are subject to model stability issues when multiple downscaling-to-upscaling cycles are applied\u00a0<cit.> so it further\nvalidates the necessity of learning downscaling and upscaling jointly.\n\n \n \n\n \n\n\n\n\n\n\n\n\n\n\n\nTo overcome these challenges and utilize the relationship between upscaling and downscaling steps, recent works designed the encoder-decoder framework to unite these two independent tasks together.\nKim \u00a0<cit.> utilized autoencoder (AE) architecture, where the encoder is the downscaling network and the decoder is the upscaling network, to find the optimal LR result that maximizes the restoration performance of the HR image. Sun \u00a0<cit.> designed a learned content adaptive image downscaling model in which an SR model is trained simultaneously to best recover the HR images. \nLater on, Li \u00a0<cit.> proposed a learning approach for image compact-resolution using a convolutional neural network (CNN-CR) where the image SR problem is formulated to jointly minimize the reconstruction loss and the regularization loss. Although the above models can efficiently improve the quality of HR images recovered from corresponding LR images, these works only optimize downscaling and SR separately, while ignoring the potential mutual intension between downscaling and inverse upscaling. \n\nMore recently, a jointly optimized rescaling model was proposed by Xiao\u00a0\u00a0<cit.>\nto achieve significantly improved performance.\n\nAn Invertible Rescaling Net (IRN) was designed to model the reciprocal nature of the downscaling and upscaling processes.\nFor downscaling, IRN was trained to convert HR input to visually-pleasing LR output and a latent variable z.\nAs z is trained to follow an input-agnostic Gaussian distribution, the HR image can be accurately reconstructed\nduring the inverse up-scaling procedure although z is randomly sampled from a normal distribution. Nevertheless,\nthe model's performance can be further improved if the high-frequency information remaining in z is efficiently stored.  \n\nTo resolve the above difficulties and take full potential of the IRN, here we propose two approaches, namely the IRN-meta (IRN-M) and IRN-alpha (IRN-A), respectively, to efficiently compress the high frequency information stored in z, which can be used to recover z\nand help restore HR image consequently during the inverse up-scaling.\n\nFor IRN-A, we train the model to extract a fourth LR channel in addition to the LR RGB channels.\nIt represents essential high frequency information which was lost in the IRN baseline due to random sampling of z,\nand is saved as the alpha-channel of saved LR output.\nFor IRN-M approach, an AE module is trained to compress z as a compact latent variable, which can be saved\nas metadata of the LR output. In the inverse upscaling process, z is restored from the latent space by utilizing the well-trained decoder.\nBoth modules are also successfully applied to the state-of-the-art (SOTA) rescaling model DLV-IRN\u00a0<cit.>.\nIn summary, the main contribution of this paper is that we are the first to compress the high-frequency information in z,\nwhich is not fully utilized in current invertible image rescaling models, to improve the restored HR image quality in upscaling progress. \n\n\n\n\n\n\u00a7 PROPOSED METHOD\n\n\n\n\n\n\n\n \u00a7.\u00a7 IRN-A\n\n\n\nFig.\u00a0<ref> (a) shows the IRN-A network architectures, where the invertible neural network blocks (InvBlocks) are referenced from previous work IRN\u00a0<cit.>. In the new model, the input HR image is resized via Haar transformation before\nsplitting to a lower branch x_l and a higher branch x_h.\nMore specifically, Haar transformation converts the input HR image (C, H, W) into a matrix of shape (4C, H/2, W/2), where C, H, and W represent image color channels, height and width respectively. The first C channels represent low-frequency components of the input image in general\nand the remaining 3C channels represent the high-frequency information on vertical, horizontal and diagonal directions respectively.\nDifferent from the IRN baseline, which uses only the C low-frequency channels in the lower branch,\nwe add 1 additional channel, denoted as alpha-channel for convenience as it would be stored as the alpha-channel in RGBA format,\nin the lower branch x_l to store the compressed high-frequency information.\nAfter the first Haar transformation, the alpha-channel is initialized with the average value across all 3C high-frequency channels,\nand only 3C-1 channels are included in x_h as the first channel is removed to make the total number of channels remain constant. \n\n\n\nAfter channel splitting, x_l and x_h are fed into cascaded InvBlocks and transformed to an LR RGBA image y and an auxiliary\nlatent variable z.\nFirst three channels of y consist of the visual RGB channels and the fourth channel contains the compressed high-frequency\ncomponents transformed along the InvBlocks. The alpha-channel was normalized via a sigmoid function, S(\u03b1)=1/1+e^-\u03b1, to help quantization\nof the alpha-channel and maintain training stability.\n\n\n\n\n\n\n\nFor the inverse upscaling process,\nthe model needs to recover z, denoted as \u1e91 as it is not stored. In previous work, \u1e91 was randomly drawn from normal Gaussian distribution.\nWhile this helps creating diverse samples in generative models, it is not optimal for tasks like image rescaling which aims to restore one HR image instead\nof diverse variations.\n\nTherefore, we set \u1e91 as 0, the mean value of the normal distribution, for the inverse up-scaling process.\nThis technique was also validated in previous works like FGRN\u00a0<cit.> and DLV-IRN\u00a0<cit.>.\nOf note, at the end of inverse process, the deleted high frequency channel needs to be recovered as\n\n    x_m=3C\u00d7 x_\u03b1-\u2211_i=1^3C-1x^i_h\n\nwhere x_m represents the channel removed from x_h and x_\u03b1 represents the alpha-channel in x_l.\n\n\n\n\n\n \u00a7.\u00a7 IRN-M\n\n\n\nBesides storing the compressed high-frequency information in a separate alpha-channel, we also propose an alternative space-saving approach to store the extracted information as metadata of the image file. Image metadata is text information pertaining to an image file that is embedded into the image file or contained in a separate file in a digital asset management system.  Metadata is readily supported by existing image format so this proposed method could be easily integrated with current solutions.  \n\nThe network architecture of our metadata approach is shown in Fig.\u00a0<ref> (b). Here x_l and x_h, same as the IRN baseline,\nare split from Haar transformed 4C channels to C and 3C channels respectively.\nUnlike the RGBA approach, the metadata method uses an encoder at the end to compress the z and save the latent vector S as metadata,\nrather than saving as the alpha-channel of the output. S will be decompressed by the decoder for the inverse upscaling step.\nIn our AE architecture, the encoder compacts the number of z channels from 3C\u00d7 n^2 -C to 4 via 2D convolution layers and\ncompresses the z's height and width from (H/2^n, W/2^n) to (H/2^n+2, W/2^n+2) by using max-pooling layers.\nHere n is 1 or 2 depending on the scale factor of 2\u00d7 or 4\u00d7.\nOf note, the AE was pre-trained with MSE loss before being embedded into the model structure.\n\nAfter placing the well-trained AE in the IRN architecture, the entire structure was trained to minimize the following mixture loss function:\n\n    L = \u03bb_1 L_r+\u03bb_2 L_g+\u03bb_3 L_d+\u03bb_4 L_mse\n\nwhere L_r is the L1 loss for reconstructing HR image; L_g is the L2 loss for the generated LR image; L_d is the distribution matching loss; and L_mse is the MSE loss between the input of the encoder and the output of the decoder. \n\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\nFollowing the same training strategy and hyperparameters in IRN baseline, our models were trained on the DIV2K\u00a0<cit.> dataset, which includes 800 HR training images. IRN-M and IRN-A were trained with 500,000 and 250,000 iterations respectively.\nBoth models were evaluated across five benchmark datasets: Set5\u00a0<cit.>, Set14\u00a0<cit.>, BSD100\u00a0<cit.>, Urban100\u00a0<cit.> and the validation set of DIV2K. The upscaled images quality across different models were assessed via the peak noise-signal ratio (PSNR) and SSIM on the Y channel of the YCbCr color space.\nFollowing previous works\u00a0<cit.>, as it is not beneficial to add\nrandomness in restoring HR images, we set \u1e91 as 0 during  the inverse up-scaling process\nfor both training and validation steps in all experiments.\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Ablation study\n\n\n\nAs the transformed\nalpha-channel is the key innovation for improved performance for IRN-A,\nthe pre-splitting and initial settings of the alpha-channel\nbefore the forward transformation process are very important.\n\nFor better analysis of their effects, Table\u00a0<ref> shows an ablation study that compares the results for different settings of the alpha-channel, where\n\u201cpost-split\" and \u201cpre-split\" refer to splitting the alpha-channel after the downscaling module or before the InvBlock respectively, and\n\u03b1_avg represents presetting the average value of high-frequency information in the pre-split alpha-channel.\nFrom Table\u00a0<ref>, we notice that using the \u03b1_avg with pre-split architecture performs best across all options. \n\nThe IRN-M model constructs the HR image by decoding the latent space s saved in the metadata file. Table\u00a0<ref> shows another ablation study for determining the optimal AE structure, where AE_p represents that AE, before training as part of IRN-M, is pre-trained using MSE loss with standalone random z;\nAE_f represents fixing the AE during training the IRN-M; and \u201c2layers\" and \u201c4layers\" represent two and four convolutional layers used in AE respectively.\nAs shown in Table\u00a0<ref>, using the IRN-M with pre-trained 4 layers AE and not fixing the AE during training has the best performance.\nOf all three settings, pre-training of AE is the most critical factor in maximizing performance.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Image rescaling\n\n\n\nThe quantitative comparison results for HR image reconstruction are shown in Table\u00a0<ref>. \nRather than choosing SR models which only optimize upscaling steps, we consider SOTA bidirectional (jointly optimizing downscaling and upscaling steps) models for fair comparison\u00a0<cit.>. \nAs shown in Table\u00a0<ref>, DLV-IRN-A is efficient at storing high-frequency information in the alpha-channel and consequently outperforms its baseline DLV-IRN, as well as other models,\nincluding HCFlow and IRN models, which randomly samples \u1e91 for the upscaling step.\nFor DLV-IRN-M, while not as good as the -A variant, \n\nit still performs better  than all other models, only trailing behind FGRN for two small test sets at 4\u00d7.\n\nHence we conclude that both -M and -A modules can improve the modeling of the high-frequency information and\nhelp restore the HR image consequently.  Visual examples of the 4\u00d7 test in Fig\u00a0<ref> also validate the improved performance from our models.\n\n\n\n\n\n\u00a7 CONCLUSIONS\n\n\n\nTo fully mine the potential of image rescaling models based on INN, two novel modules are proposed to store otherwise lost high-frequency information z.  The IRN-M model utilizes an autoencoder to compress\nz and save as metadata in native image format so it can be decoded to an approximate of z, while IRN-A adds an additional channel to store crucial high-frequency information, which can be quantized and stored\nas the alpha-channel, in addition to the RGB channels, in existing RGBA format.  With carefully designed autoencoder and alpha-channel pre-split,\nit is shown that both modules can improve the upscaling performance significantly comparing to the IRN baseline.\nThe proposed modules are also applicable to newer baseline models like DLV-IRN and DLV-IRN-A is by far the best, which further pushes the limit of image rescaling performance with a significant margin.\n \n\n\n\n\n\nIEEEbib\n\n\n"}