{"entry_id": "http://arxiv.org/abs/2303.06838v2", "published": "20230313035949", "title": "Sample Complexity Analysis for Adaptive Optimization Algorithms with Stochastic Oracles", "authors": ["Billy Jin", "Katya Scheinberg", "Miaolan Xie"], "primary_category": "math.OC", "categories": ["math.OC"], "text": "\n\nDDFM: Denoising Diffusion Model for Multi-Modality Image Fusion\n    Zixiang Zhao^1,2  Haowen Bai^1  Yuanzhi Zhu^2  Jiangshe Zhang^1  Shuang Xu^3\n\n            Yulun Zhang^2  Kai Zhang^2  Deyu Meng^1  Radu Timofte^2,4  Luc Van Gool^2\n\n\t\t^1Xi\u2019an Jiaotong University   ^2Computer Vision Lab, ETH Z\u00fcrich\n\n            ^3Northwestern Polytechnical University   ^4University of Wurzburg\n\n\n    Received: date / Accepted: date\n============================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\tSeveral classical adaptive optimization algorithms, such as line search and trust region methods, have been recently extended to stochastic settings where function values, gradients, and Hessians in some cases, are estimated via stochastic oracles. Unlike the majority of stochastic methods, these methods do not use a pre-specified sequence of step size parameters, but adapt the step size parameter according to the estimated progress of the algorithm and use it to dictate the accuracy required from the stochastic approximations. The requirements on stochastic approximations are, thus, also adaptive and the oracle costs can vary from iteration to iteration. The step size parameters in these methods can increase and decrease based on the perceived progress, but unlike the deterministic case they are not bounded away from zero due to possible oracle failures, and bounds on the step size parameter have not been previously derived. This creates obstacles in the total complexity analysis of such methods, because the oracle costs are typically decreasing in the step size parameter, and could be arbitrarily large as the step size parameter goes to 0.  Thus, until now only the total iteration complexity of these methods has been analyzed. In this paper, we derive a lower bound on the step size parameter that holds with high probability for a large class of adaptive stochastic methods. We then use this lower bound to derive a framework for analyzing the expected and high probability total oracle complexity of any method in this class. Finally, we apply this framework to analyze the total sample complexity of two particular algorithms, STORM <cit.> and SASS <cit.>, in the expected risk minimization problem.\n\t\n\n\n\u00a7 INTRODUCTION\n\n\n\nThe widespread use of stochastic optimization algorithms for problems arising in machine learning and signal processing has made the stochastic gradient method and its variants become overwhelmingly popular despite their theoretical and practical shortcomings.  Adaptive stochastic optimization algorithms, on the other hand,  borrow from decades of advances in deterministic optimization research, and offer new paths forward for stochastic optimization to be more effective and even more applicable. Adaptive algorithms can avoid many of the practical deficiencies of contemporary methods (such as the tremendous costs of tuning the step sizes of an algorithm for each individual application) while possessing strong convergence and worst-case complexity guarantees in surprisingly diverse settings. \n\nAdaptive optimization algorithms have a long and successful history in deterministic optimization and include line search, trust region methods, cubic regularized Newton methods, etc. All these methods have a common iterative framework, where at each iteration a candidate step is computed by the algorithm based on a local model of the objective function and a step size parameter that controls the length of this candidate step.   The candidate step is then evaluated in terms of the decrease it achieves in the objective function, with respect to the decrease that it was expected to achieve based on the model. Whenever the decrease is sufficient, the step is accepted and the step size parameter may be increased to allow the next iteration to be more aggressive. If the decrease is not sufficient (or not achieved at all) then the step is rejected and a new step is computed using a smaller step size parameter. The model itself remains unchanged if it is known that a sufficiently small step will always succeed, as is true, for example with first- and second-order Taylor models of smooth functions. The analysis of such methods relies on the key property that the step size parameter is bounded away from zero and that once it is small enough the step is always accepted and the objective function gets reduced. \n\nWhen stochastic oracles are used to approximate the objective function and its derivatives, the models no longer have the same property as the Taylor models; steps that decrease the model might not decrease the function, no matter how small the step size is. Thus all stochastic variants of these methods recompute the model at each iteration. The requirement on the stochastic model is then that it achieves a Taylor-like approximation with sufficiently high probability. This also means that steps may get rejected even if the step size parameter is small, simply because the stochastic oracles fail to deliver desired accuracy. Despite this difficulty, one can develop and analyze stochastic variants of adaptive optimization methods.\n\nRecently developed stochastic variants of line search (which we will call step search, since unlike the deterministic version the search direction has to change on each iteration, thus the algorithm is not searching along a line) include <cit.>.  Stochastic trust region methods have been analyzed in <cit.>, and an adaptive cubic regularized method based on random models has been analyzed in <cit.>. For all these methods,  bounds on iteration complexity have been derived, either in expectation or in high probability, under the assumption that stochastic oracles involved in approximating the objective function deliver sufficiently high accuracy with sufficiently high probability. \nWhile this probability is usually fixed, the accuracy requirement of the oracles is adaptive and depends on the step size parameter. Specifically, the smaller that parameter is, the more accurate the oracles need to be, in order to maintain the Taylor-like behavior of the model used by the algorithm. In most applications, having more accurate stochastic oracles implies a higher per-iteration cost. Furthermore, unlike the deterministic case, the step sizes in the stochastic case are not bounded away from zero due to possible oracle failures, and bounds on the step size parameter have not been previously derived. This creates significant difficulty in the analysis of the total oracle complexity of such methods because the oracle costs could be arbitrarily large as the step size parameter goes to zero. \n   \nIn this paper, we derive a lower bound on the step size parameter for a general class of stochastic adaptive methods that encompasses all the algorithms in the preceding paragraph. This enables us to derive a bound on the total oracle complexity for any algorithm within this class and specific stochastic oracles arising, for example, from expected risk minimization. \n  Our key contributions are as follows:\n  \n  \n  * Provide a high probability lower bound on the step size parameter for a wide class of stochastic adaptive methods using a coupling argument between the stochastic process generated by the algorithm and a one-sided random walk.\n  \n  * Derive a framework for analyzing expected and high probability total oracle complexity bounds for this general class of stochastic adaptive methods. \n  \n  * Apply these bounds to STORM <cit.> and SASS <cit.>  to derive their total sample complexity for expected risk minimization, and show they essentially match the complexity lower bound of first-order algorithms for stochastic non-convex optimization <cit.>.\n   \n\nWe consider a continuous optimization problem of the form\n\n    min_x \u2208^m \u03d5(x),\n\nwhere \u03d5 is possibly non-convex, (twice-)continuously differentiable with Lipschitz continuous derivatives.\nNeither function values \u03d5(x), nor gradients \u2207\u03d5(x) are assumed to be directly computable. Instead, given any x \u2208^m, it is assumed that stochastic approximations of \u03d5(x), \u2207\u03d5(x), and possibly \u2207^2 \u03d5(x)  can be computed, and these approximations may possess different levels of accuracy and reliability depending on the particular setting of interest.  \n\n\nMany adaptive stochastic algorithms have been developed recently <cit.> that use these stochastic approximations to compute an \u03b5-optimal point x_\u03b5, which means \u03d5(x_\u03b5)-inf_x\u03d5(x) \u2264\u03b5 if \u03d5 is convex or  \u2207\u03d5(x_\u03b5)\u2264\u03b5 if \u03d5 is non-convex.\nIn the next section, we will introduce the general framework that encompasses these methods and discuss particular examples in more detail. In <Ref>, we discuss the stochastic process generated by the algorithmic framework, including the stochastic step size parameter. In <Ref>, we derive a lower bound on the step size parameter which holds in high probability. In <Ref>, we use this lower bound to derive abstract expected and high probability total oracle complexity for any algorithm in this framework. In <Ref>, we particularize these bounds to the specific examples of first-order STORM <cit.> and SASS <cit.> algorithms to bound their total sample complexity when applied to expected risk minimization. We conclude with some remarks in <Ref>.\n\n\n\n\n\n\u00a7 ALGORITHM FRAMEWORK AND ORACLES\n \n\nWe now introduce and discuss an algorithmic framework for adaptive stochastic optimization in Algorithm <ref>. The framework is assumed to have access to stochastic oracles, that for any given point x can generate random quantities f(x,\u03be_0)\u2248\u03d5(x) (via zeroth-order oracle), g(x,\u03be_1) \u2248\u2207\u03d5(x)  (first-order oracle) and, possibly,  H(x,\u03be_2)\u2248\u2207^2 \u03d5(x) (second-order oracle). At each iteration, given x_k, a  stochastic local model m_k(x_k+s) : ^m\u2192 of \u03d5(x_k+s) is constructed using  g(x,\u03be_1) and possibly H(x,\u03be_2).   Using this model, a step s_k(\u03b1_k)  is computed so that m(x_k+s_k(\u03b1_k)) is a sufficient improvement  over m_k(x_k), where \u03b1_k is the step size parameter, which directly or indirectly controls the step length.    The iterate x_k and the trial point x_k^+ =x_k+s_k(\u03b1_k) are evaluated using the zeroth-order oracle  f(x_k,\u03be_0,k), f(x_k^+,\u03be_0,k^+).  If these estimates suggest that sufficient improvement is attained, then the step is deemed successful, x_k^+ is accepted as the next iterate, and the parameter \u03b1_k is increased up to a multiplicative factor; otherwise, the step is deemed unsuccessful, the iterate does not change, and \u03b1_k is decreased by a multiplicative factor. Unlike in the deterministic case, new calls to all oracles are made at each iteration even when the iterate does not change.\n\n\n\n\nWe now discuss how various methods fit into the general framework. For each method we give the form of m_k(x_k+s), in terms of  \ng_k=g(x_k,\u03be_1,k) and  H_k=H(x_k,\u03be_2,k),  s_k(\u03b1_k) and the sufficient reduction criterion  in terms of \n f_k^0=f(x_k,\u03be_0,k) and f_k^+=f(x_k^+,\u03be^+_0,k). \n\n\n\n \u00a7.\u00a7 Step search method\n\nIn the case of the step search (SS) methods in <cit.> the particulars are as follows. Quantities f_k^0  f_k^+ and g_k are random outputs of the stochastic oracles and H_k is some positive definite matrix (e.g., the identity). \n\n\n  * m_k(x_k+s)=\u03d5(x_k)+ g_k^Ts+1/2\u03b1_ks^TH_ks,\n\n  * s_k(\u03b1_k)=-\u03b1_k H_k^-1g_k\n\n  * Sufficient reduction:  f_k^0-f_k^+ \u2265 - g_k^Ts_k(\u03b1_k)-r\n\n Here \u2208 (0,1), and r is a small positive number that compensates for the noise in the function estimates. We will discuss the choice of r after we introduce conditions on the oracle outputs f(x,\u03be_0) and g(x,\u03be_1). \n \n \n\n\t\n  * SS.0. Given a point x, the oracle computes a (random) function estimate f(x,\u03be_0) such that \n\t\n    \u2119_\u03be_0 ( \u03d5(x) - f(x, \u03be_0)< \u03f5_f + t ) \u2265 1-\u03b4_0(t),\n\n\tfor some \u03f5_f\u22650 and any t>0. \n\t\n  * SS.1. \n\tGiven a point x and the current step size parameter \u03b1, the oracle computes a (random) gradient estimate g(x, \u03be_1) such that\n\t\n    \u2119_\u03be_1 (g(x, \u03be_1)-\u2207\u03d5(x)\u2264max{\u03f5_g, min{\u03c4, \u03ba\u03b1}g(x, \u03be_1) })\u2265 1-\u03b4_1\n\n\tfor some nonnegative constants \u03f5_g, \u03ba, \u03c4 and \u03b4_1. \n\n\nIn <cit.>, \u03f5_f=0 and \u03b4_0(t)\u2261 0, which means that the zeroth-order oracle is exact, and r = 0. In  <cit.>,  \u03f5_f>0 and \u03b4_0(t)\u2261 0,\nwhich means that the zeroth-order oracle has a bounded  error with probability one, and r = 2\u03f5_f.\nIn <cit.>,  \u03f5_f>0 and \u03b4_0(t)= e^-\u03bb t, for some \n\u03bb >0, which means the error can take any probability if less than \u03f5_f, and the tail of the error decays exponentially beyond \u03f5_f. In <cit.>, r > 2 sup_x _\u03be_0 [  \u03d5(x) - f(x, \u03be_0)  ].\n\nIn <cit.>, \u03f5_g=0 and \u03b4_1<1/2.  In  <cit.>, \u03f5_g>0   and  \u03b4_1 is sufficiently small with a more complicated upper bound. \nIn  <cit.>,  is finite, thus \u03c4 is \u03ba. In <cit.>,  is infinity, and \u03c4 is simply assumed to be some constant intrinsic to the oracle.\n\n\n\n\n \u00a7.\u00a7 Trust region method \n\nStochastic trust region (TR) methods that fall into the framework of Algorithm\u00a0<ref> have been developed and analyzed in <cit.>.  \nIn the case of TR algorithms,  f_k^0,  f_k^+, g_k,  and (possibly) H_k are random outputs of the stochastic oracles, and  \n \n \n  * m_k(x_k+s)=\u03d5(x_k)+g_k^Ts+1/2s^T H_ks, \n\n  * s_k(\u03b1_k)=min_s: s\u2264\u03b1_km_k(x_k+s)\n\n  * Sufficient reduction:  f_k^0-f_k^++r/m_k(x_k)-m_k(x_k+s_k(\u03b1_k))\u2265\n\n  * Additional requirement for a successful iteration: g_k\u2265\u03b8_2 \u03b1_k, for some \u03b8_2>0. \n\n\nThe requirements for the oracles are as follows. In the case of first-order analysis in <cit.>, the following first-order oracle is assumed to be available.\n\n\t\t\t\n  * TR1.1. Given a point x and the current trust-region radius \u03b1, the oracle computes a gradient estimate g(x, \u03be_1) such that\n\t\t\n    \u2119_\u03be_1 (g(x, \u03be_1)-\u2207\u03d5(x)\u2264\u03f5_g+\u03ba_eg\u03b1)\u2265 1-\u03b4_1.\n\n\t\tHere, \u03ba_eg and \u03b4_1 are nonnegative constants. \n\nIn the second-order analysis, the following first- and second-order oracles are used:\n\n\t\t\t\n  * TR2.1. Given a point x and the current trust-region radius \u03b1, the oracle computes a gradient estimate g(x, \u03be_1) such that\n\t\t\n    \u2119_\u03be_1 (g(x, \u03be_1)-\u2207\u03d5(x)\u2264\u03f5_g+\u03ba_eg\u03b1^2)\u2265 1-\u03b4_1.\n\n\t\tHere, \u03ba_eg and \u03b4_1 are nonnegative constants. \n\t\t\t\t\t\n  * TR2.2. Given a point x and the current trust-region radius \u03b1, the oracle computes a Hessian estimate H(x, \u03be_2) such that\n\t\t\n    \u2119_\u03be_2 (H(x, \u03be_2)-\u2207\u03d5(x)\u2264\u03f5_h+ \u03ba_eh\u03b1)\u2265 1-\u03b4_2.\n\n\t\tHere, \u03ba_eh and \u03b4_2 are nonnegative constants. \n\n\u03f5_h and \u03f5_g are assumed to equal 0 in  <cit.> but are allowed to be positive in <cit.>. \n\nIn terms of the zeroth-order oracles, the three works make different assumptions. Specifically, in <cit.>, as in <cit.>, the zeroth-order oracle is assumed to be exact. \nIn <cit.> the zeroth-order oracle is the same as in <cit.>. For the first-order analysis in <cit.>, however, the zeroth-order oracle is as follows. \n\n\t\n  * TR1.0. Given a point x and the current trust-region radius \u03b1, the oracle computes a function estimate f(x, \u03be_0) such that\n\t\n    _\u03be_0(f(x, \u03be_0) - \u03d5(x)\u2264\u03ba_ef\u03b1^2) \u2265 1 - \u03b4_0,\n\n\twhere \u03ba_ef and \u03b4_0 are some nonnegative constants.\n\nFor the second-order analysis in <cit.>, the zeroth-order oracle requirements are tighter. \n\n\t\n  * TR2.0. Given a point x and the current trust-region radius \u03b1, the oracle computes a function estimate f(x, \u03be_0) such that\n\t\n    _\u03be_0(f(x, \u03be_0) - \u03d5(x)\u2264\u03ba_ef1\u03b1^3) \u2265 1 - \u03b4_0\n\n\tand \n\t\n    _\u03be_0[f(x, \u03be_0) - \u03d5(x) ] \u2264\u03ba_ef2\u03b1^3\n\n\twhere \u03ba_ef1, \u03ba_ef2 and \u03b4_0 are some nonnegative constants.\n\nIn <cit.>, r = 0. In  <cit.>, r > 2\u03f5_f + 2/\u03bblog 4.\n\n\n\n \u00a7.\u00a7 Cubicly regularized Newton method\n\nThe cubicly regularized (CR) Newton method in <cit.> also fits the framework of Algorithm\u00a0<ref> with \n\n\n  * m_k(x_k+s)=\u03d5(x_k)+g_k^Ts+1/2s^T H_ks+1/3\u03b1_ks^3,\n\n  * s_k(\u03b1_k)=min_s m_k(x_k+s),\n\n  * Sufficient reduction:  f_k^0-f_k^+/m_k(x_k)-m_k(x_k+s_k(\u03b1_k))\u2265.\n\n\nThe zeroth-order oracle is assumed to be exact, that is f_k^0=\u03d5(x_k) and f_k^+=\u03d5(x_k+s_k(\u03b1_k)). The following first- and second-order oracles are used. \n\n\n\n\t\n  * CR.1. Given a point x and the current parameter \u03b1, the oracle computes a gradient estimate g(x, \u03be_1) such that\n\t\n    \u2119_\u03be_1 (g(x, \u03be_1)-\u2207\u03d5(x)\u2264\u03ba_eg\u03b1^2) \u2265 1 - \u03b4_1,\n\n\twhere \u03ba_eg and \u03b4_1 are nonnegative  constants. \n\t\t\n  * CR.2. Given a point x, and the current parameter \u03b1, the oracle computes a Hessian estimate H(x, \u03be_2) such that \n\t\n    \u2119_\u03be_2 (H(x, \u03be_2)-\u2207^2 \u03d5(x)\u2264\u03ba_eh\u03b1) \u2265 1 - \u03b4_2,\n\n\twhere \u03ba_eh and \u03b4_2 are nononegative  constants. \n\n\n\nThe actual conditions on the oracles in <cit.> are different from what we present above and are as follows:\n\n    \u2119_\u03be_1, \u03be_2 (g(x, \u03be_1)-\u2207\u03d5(x)\u2264\u03ba_egs^2  and (H(x, \u03be_2)-\u2207^2 \u03d5(x))s\u2264\u03ba_ehs^2) \u2265 1 - \u03b4,\n\nfor some   \u03ba_eg , \u03ba_eh and \u03b4. Here, s is the trial step obtained from minimizing the cubicly regularized model. \n\n \nBy taking \u03b4_1 = \u03b4_2 = \u03b4/2 and \u03b1 =  O(1/\u03c3) (where \u03c3 is the penalty parameter used in the cubic regularization and the constant in the big-O can be chosen according to Lemma 5.1 of <cit.>), these oracles imply (<ref>) by Lemma 5.1 of <cit.>.\n\n\n\n\n\n\n\n\n\n\nIt is apparent that all of the algorithms that we discussed above rely on oracles whose accuracy requirements change adaptively with \u03b1. It is also clear that for many settings, the higher the accuracy requirement is,  the higher the oracle complexity is. For example, if a stochastic oracle is delivered via sample averaging, then more samples are needed to provide a higher accuracy.  Therefore, to bound the total oracle complexity of the algorithm, we need to bound the accuracy requirement over the iterations, or equivalently provide a lower bound for the parameter \u03b1. \n\t\n\n\t\n\n\n \u00a7.\u00a7 Notions of the stochastic process\n\n\nWhen applied to problem (<ref>), Algorithm <ref>  generates a stochastic process (with respect to the randomness underlying the stochastic oracles). \n\n\nSpecifically, let (X_k)_k\u2265 0 be the random iterates with realizations x_k, let (G_k)_k\u2265 0 be the gradient estimates with realizations g_k, and let (\ud835\udc9c_k)_k \u2265 0 be the step size parameter values with realizations \u03b1_k. The prior works that analyze different algorithms of the framework <ref>  define this stochastic process rigorously, with appropriate filtrations. Here for brevity, we will omit those details, as we do not use them in the analysis. \n\n\n\nWe now define a stopping time for the process.\n\n\tFor \u03b5 > 0, let T_\u03b5 be the first time such that a specified optimality condition is satisfied. For all the settings considered in this paper,\n\tT_\u03b5=min{k: \u2207\u03d5(x_k)\u2264\u03b5} if \u03d5 is non-convex, and T_\u03b5=min{k: \u03d5(x_k) - inf_x \u03d5(x)  \u2264\u03b5} if \u03d5 is strongly convex. We will refer to T_\u03b5 as the stopping time of the algorithm.\n\n\n\n\n The following property is crucial in the analysis of algorithms in the framework of <Ref>.\n \n \n\n\t[Properties of the stochastic process generated by the adaptive stochastic algorithm]\n\t\n\tThe random sequence of parameters \ud835\udc9c_k generated by the algorithm satisfies the following: \n\t\n\t\t\n  (i) For all k, \ud835\udc9c_k \u2208{\u03b3\ud835\udc9c_k-1, min{, \u03b3^-1\ud835\udc9c_k-1}}, and\n\t\t\n  (ii) There exist constants \u03b1\u0305> 0, and p>1/2, such that for all iterations  k < T_\u03b5, if  \ud835\udc9c_k  \u2264\u03b1\u0305 then\n\t\t\n    (\ud835\udc9c_k+1 = \u03b3^-1\ud835\udc9c_k|\u2131_k) \u2265 p.\n\n\t\tHere, \u2131_k denotes the filtration generated by the algorithm up to iteration k.\n\t       \n\n\n The algorithms in  <cit.> all satisfy Assumption <ref>, under appropriate lower bounds on the oracle probabilities  \u03b4_0, \u03b4_1 (and \u03b4_2). \nIn the next section, under Assumption  <ref>, we derive a high probability lower bound on \u03b1_k as a function of the number of iterations n, \u03b1\u0305, p, and \u03b3. \n\nThroughout the remainder of this paper, we will use q to denote 1-p.\n\n \n\t\n\t\n\n\n\u00a7 HIGH PROBABILITY LOWER BOUND FOR THE STEP SIZE PARAMETER\n\n\n\nThe following theorem provides a high probability lower bound for \u03b1_k.\n\n\n\n\tSuppose Assumption <ref> holds for Algorithm <ref>. For any positive integer n, any \u03c9  > 0, with probability at least  1 - n^-\u03c9 - cn^-(1+\u03c9 ), we have \n\t\n    either  T_ < n\u00a0or\u00a0min_1 \u2264 k \u2264 n\u03b1_k  \u2265\u03b1\u0305\u03b3\u03b3^(1+\u03c9)log_1/2q n\n    =\u03b1\u0305\u03b3   n^-(1+\u03c9)log_1/2q 1/\u03b3,\n \twhere c = 2\u221a(pq)/(1-2\u221a(pq))^2 and q = 1-p.\n\t\n\n\n\n\nThe proof of this theorem involves two steps. First, in <Ref>, we show that for n<T_\u03b5, the sequence of step size parameters \ud835\udc9c_k generated by the algorithm can be coupled with a random walk on the non-negative integers. This reduces the problem to that of bounding the maximum value of a one-sided random walk in the first n steps. We then derive  a high probability upper bound on this maximum value in <Ref>.\n\nBefore moving to its proof, we illustrate the theorem using some plots and comment on some implications of the theorem.\n\nIllustration of <Ref>. <Ref> illustrates the high probability bound provided by <Ref>. The solid curves depict the lower bounds given by the theorem for \u03b1\u0305 = 1, \u03c9 = 1, p = 0.8, and for varying values of \u03b3. In comparison, the dotted lines correspond to one-sided random walks\n\ud835\udcb5_k that start at \u03b1\u0305 = 1. \n\nAt each step, \ud835\udcb5_k+1 = \u03b3\ud835\udcb5_k with probability 1-p, and \ud835\udcb5_k+1 = min{1, \u03b3^-1\ud835\udcb5_k} with probability p. The proof of <Ref> shown later implies that there is a coupling between the sequence of parameters \ud835\udc9c_k generated by the algorithm and \ud835\udcb5_k, such that \ud835\udc9c_k \u2265\ud835\udcb5_k, in other words, the sequence of parameters \ud835\udc9c_k generated by the algorithm stochastically dominates \ud835\udcb5_k. \n\n\n\n\n\n\nRemarks on <Ref>. \n\n\t\n  * For fixed n, \u03b3, and \u03b1\u0305, the lower bound is a function of p. It increases as p increases. Specifically, the exponent of n changes with p, and the exponent goes to 0 as p goes to 1.\n\tHence as p goes to 1, this lower bound simplifies to \u03b1\u0305\u03b3, which matches the lower bound in the deterministic case.\n\t\n\t\n  * When p is close to 1 (i.e. when the stochastic oracles are highly reliable), this lower bound decreases slowly as a function of n, since the exponent of n is close to 0. Alternatively, when the stochastic oracles are not highly reliable,  increasing the value of \u03b3 allows the algorithm to maintain a slow decrease of the step size.\n\t\n\t\n  * Enlarging \u03b3 as p decreases makes intuitive sense for the algorithm.\n\tWhen p is large, an unsuccessful step is more likely to be caused by the step size being too large rather than the failure of the oracles to deliver the desired accuracy. On the other hand, when p is small,\n\tunsuccessful iterations are likely to occur even when the step size parameter is already small.\n\t\n\tThus in the latter case, larger \u03b3 values help avoid an erroneous rapid decrease of the step size parameter.\n\t\n\t\n  * If we  choose \n\n\t\u03b3=(1/2q)^-1/4\n\tthen the minimum step size is lower bounded by \u03b1\u0305\u03b3 n^-1/2 with high probability. This coincides with the typical choice of the step size decay schemes for the stochastic gradient method applied to non-convex functions.\n\n\nThe theorem implies that we can even bound \u03b1 by a constant times \u03b1\u0305 with high probability, provided we set \u03b3 as a function of n.  \n\n\n\nLet Assumption <ref> hold for Algorithm <ref>, then for any positive integer n, any \u03c9  > 0, and any \u03b2 < 1/2, if \n\t\n    \u03b3\u2265max{1/2, (1/2q)^log(2\u03b2)/(1+\u03c9)log n},\n\n\tthen with probability at least  1 - n^-\u03c9 - cn^-(1+\u03c9 ), where c = 2\u221a(pq)/(1-2\u221a(pq))^2,we have \n\t\n    either T_ < n or min_1 \u2264 k \u2264 n\u03b1_k  \u2265\u03b2\u03b1\u0305.\n\n\n\n\nThis follows from <Ref> by substituting in the specified value of \u03b3.\n\n\t \nIn the remainder of this section, we prove <Ref> in two steps.\n  \n\n\n\n\n\n\n\n \u00a7.\u00a7 Step 1: reduction to random walk\n\n\n\t\n\tWe will use a coupling argument to obtain the reduction to a random walk. \n\n\n\n\n\n\n\n\n\nLet {_k}_k=0^\u221e denote the random sequence of parameter values (whose realization is {\u03b1_k}_k=0^\u221e), for  Algorithm <ref>.  Let us assume, WLOG, that _0 = \u03b3^j \u03b1\u0305, for some integer j\u22640.  (Recall here that 0 <\u03b3 < 1.) Then we observe that \ud835\udc9c_k = \u03b3^Y\u0305_k\u03b1\u0305,   where  {Y\u0305_k}_k=0^\u221e is a random sequence of  integers, with Y\u0305_0 =j\u2264 0, which increases by one on every unsuccessful step, and decreases by one on every successful step. Moreover, by \nAssumption  (<ref>), whenever k < T_ and Y\u0305_k \u2265 0, the probability that it decreases by one is at least p. Define Y_k as follows:\n\n    Y_k = Y\u0305_k   if k \u2264 T_,\n        \n    Y_k = \n    \n    Y_k-1 - 1    w.p. p\n    \n    Y_k-1 + 1    w.p. 1-p  if k > T_.\n\nIn other words, Y_k follows the algorithm until T_, and then behaves like a random walk with downward drift p after T_.\nWe now couple  {Y_k}_k=0^\u221e  with a random walk {Z_k}_k=0^\u221e which stochastically dominates Y_k. \n\n\nConsider the following one-sided random walk {Z_k}_k=0^\u221e, defined  on the non-negative integers. \n\n    Z_0 = 0,      Z_k+1 = \n    \tZ_k +1,    w.p. 1-p,\n    \n    \tZ_k-1,    w.p. p, if Z_k \u2265 1,\n    \n    \t0,    w.p. p, if Z_k = 0.\n\n\n\n\n\n\t\n\tThere exists a coupling between Z_k  and Y_k, where Z_k stochastically dominates Y_k. \n\n\nInitially, Z_0=0 and Y_0 \u2264 0. For each k, we show how to update Z_k to Z_k+1 according to how Y_k changes to Y_k+1. We consider two cases depending on whether k < T_ or k \u2265 T_. \n\nCase 1: k < T_.\n If Y_k\u2264 -1, we update Z_k+1 from Z_k according to <Ref>, independently of how Y_k changes to Y_k+1. If Y_k \u2265 0, then we first check if Y_k increased or decreased. Let p' be the probability that Y_k+1 = Y_k -1 on this sample path. Since Y_k \u2265 0, we know by <Ref> that p' \u2265 p. Now, if Y_k+1 = Y_k + 1, then we set Z_k+1 = Z_k + 1. On the other hand, if Y_k+1 = Y_k - 1, then we set Z_k+1 = Z_k + 1 with probability 1 - p/p', and Z_k+1 = max{Z_k-1, 0} with probability p/p'. Note that these probabilities are well-defined because p' \u2265 p. \n \nCase 2: k \u2265 T_. If Y_k+1 = Y_k + 1, then set Z_k+1 = Z_k+1. Otherwise, if Y_k+1 = Y_k-1, then set Z_k+1 = max{Z_k-1, 0}. \n\nObserve that under this coupling, Z_k \u2265 Y_k on every sample path. Moreover, {Z_k} and {Y_k} have the correct marginal distributions. For Y_k, this is easy to see, since it evolves according to its true distribution and we are constructing Z_k from it. For Z_k, on any step with k \u2265 T_, Z_k+1 evolves from Z_k correctly according to <Ref> by construction. On a step with k < T_ there are two cases: 1) Y_k \u2264 -1,  and 2) Y_k \u2265 0. In the first case, the update from Z_k to Z_k+1 clearly follows <Ref>. This is also true in the second case, since there the probability that Z_k increases is (1-p') + p'(1-p/p') = 1-p.\n\nTo summarize, we have exhibited a coupling between {Z_k} and {Y_k}, under which Z_k \u2265 Y_k on any sample path. \n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Step 2: upper-bounding the maximum value of the random walk\n\n\n\nWe now derive a high probability upper bound on the maximum value reached by the random walk.\n\nLet N(l, n) be the random variable that denotes the number of times Z_k=l  in the first n steps of the random walk.  \n\n\nBy definition of N(l, n), we have N(l, n) > 0 if and only if state l is visited in the first n steps of the random walk. The next proposition  upper bounds the probability that N(l, n) > 0. \n\n\n\tLet q=1-p. We have\n\n\n\t\n    (N(l,n) >0 ) \u2264(n-l+1) 1-(q / p)/1-(q / p)^l+1(q/p)^l  +\n    \t2 \u221a(pq)/(1-2 \u221a(p q))^2(2q)^l.\n\n\n\n\n\n\n\tFirst, observe that (N(l, n) > 0) remains unchanged if we change the state space from {0, 1,2, \u2026} to {0, 1,2, \u2026, l} and modify the walk to hold in state l with probability q (instead of moving from l to l+1 with that probability). This defines a Markov chain on {0,1,2,\u2026, l}, and let P be its transition matrix. Noting that P_0,l^m is the probability that the Markov chain is in state l at time m, we see that\n\t\n    (N(l,n) >0 ) \u2264\u2211_m=l^n P_0, l^m.\n\n\t\n\tThe matrix P is explicitly diagonalized in [1, Section XVI.3]. By (3.16) in that section,\n\t\n    P_0,l^m=1-(q/p)/1-(q / p)^l+1(q/p)^l \n    \t\t-2 q/l+1(q/p)^l-1/2\u2211_r=1^l[sin\u03c0 r/l+1][sin\u03c0 rl/l+1][2 \u221a(pq)cos\u03c0 r/l+1]^m/1-2 \u221a(p q)cos\u03c0 r/l+1.\n\n\tThe absolute value of the sum appearing in (<ref>) can of course be bounded above by\n\t\n    \u2211_r=1^l(2 \u221a(p q))^m/1-2 \u221a(p q)=l (2 \u221a(p q))^m/1-2 \u221a(p q)\n\n\tand this readily yields\n\t\n    P_0, l^m\u22641-(q/p)/1-(q / p)^l+1(q/p)^l \n    \t\t+\n    \t\t2 p/1-2 \u221a(p q)(q/p)^l+1/2(2 \u221a(p q))^m.\n\n\tSumming (<ref>) over m=l, \u2026, n and using (<ref>), we obtain the bound on (N(l,n) >0 ) claimed in the proposition.\n\n\n\tThe bound for Proposition <ref> is essentially tight, as the decay of (N(l,n) >0 ) is not faster than geometric; q^l is a lower bound. \n\n \n\nWith the above proposition at hand, Theorem <ref> is proved by choosing an appropriate level l, for which (N(l,n) =0 ) is high. \n\n\n\tLet q=1-p. By Proposition <ref>, we have:\n\t\n    (N(l,n) >0 ) \u2264(n-l+1) 1-(q / p)/1-(q / p)^l+1(q/p)^l  +\n    \t2 \u221a(pq)/(1-2 \u221a(p q))^2(2q)^l.\n\n\tIn other words,\n\t\n    (N(l,n) =0 )    \u2265 1-(n-l+1) 1-(q / p)/1-(q / p)^l+1(q/p)^l  -\n    \t\t2 \u221a(pq)/(1-2 \u221a(p q))^2(2q)^l \n       \u2265 1- n (q/p)^l  -\n    \t\t2 \u221a(pq)/(1-2 \u221a(p q))^2(2q)^l.\n\n\tLet a>0 be a parameter to be set later, and take l=alog(n). Then, the above inequality implies:\n\t\n    (N(l,n) =0 )    \u2265 1- n (q/p)^alog(n)  -\n    \t2 \u221a(pq)/(1-2 \u221a(p q))^2(2q)^alog(n)\n       = 1- n^1-alog(p/q)  -\n    \t2 \u221a(pq)/(1-2 \u221a(p q))^2n^-alog(1/(2q)).\n\n\n\n\n\n\tLet a = 1+\u03c9/log (1/2q). Then, a log(p/q)  \u2265  a log(1/2q) = 1+\u03c9, so \n\t\n    (N(l,n) =0 )\u2265 \n    \t1- n^-\u03c9   -\n    \t2 \u221a(pq)/(1-2 \u221a(p q))^2n^-(1+\u03c9).\n\n\tIn other words, with probability at least 1 - n^-\u03c9 - cn^-(1+\u03c9) with c = 2\u221a(pq)/(1-2\u221a(pq))^2, the random walk will remain below l=(1+\u03c9)log_1/2q n in the first n steps. By construction of the coupling, with the above probability, we know the  \u03b1 parameter in the algorithm either remains above \u03b3^alog n\u03b1\u0305 = \u03b3^(1+\u03c9)log_1/2q n\u03b1\u0305  throughout the first n steps, or the algorithm has reached its stopping time in n steps. \n\n\n\n\n\n\u00a7 EXPECTED AND HIGH PROBABILITY TOTAL ORACLE COMPLEXITY\n\n\n\nWe now use the tools derived in the previous section to obtain abstract expected and high probability upper bounds on the total oracle complexity of <Ref>. In <Ref>, we will derive concrete bounds for the total oracle complexity of two specific algorithms (STORM and SASS), and the specific oracles arising in expected risk minimization.\n\n\n\nThe cost of an oracle call may depend on the step size parameter \u03b1 and the probability parameter 1-\u03b4, thus we denote the cost by (\u03b1, 1-\u03b4). \nWe will use (\u03b1) in the paper to simplify the notation because for all algorithms in the class, \u03b4 can be treated as a constant.\n Moreover, the cost of an oracle call is a non-increasing function of  \u03b1 for all algorithms developed so far that fit into the framework.\n\n\n\n\t\n\n  (\u03b1) is nonincreasing in \u03b1.\n\n\nFor a positive integer n, let (n) be the random variable which denotes the total oracle complexity of running the algorithm for n iterations. In other words,\n\n    (n) = \u2211_k=1^n (\ud835\udc9c_k).\n\n\n\n\n\n\n \u00a7.\u00a7 Abstract expected total oracle complexity\n\nWe now proceed to bound (min{T_\u03f5, n}) in expectation, where n is an arbitrary positive integer.\n\n\n\n\nLet <Ref> and <Ref> hold in Algorithm <ref>. For any positive integer n, we have\n\n\n\t\n    [(min{T_\u03f5, n})] \u2264 n  \u2211_l=1^nmin{ 1, n(q/p)^l + 2 \u221a(pq)/(1-2 \u221a(p q))^2(2q)^l}\u00b7(\u03b1\u0305\u03b3^l)\n    \t+ n  (\u03b1\u0305).\n\n\t\n\n\n\nFirst, observe that if the \u03b1_k parameters are all above some value \u03b1^* in the first n steps, then by <Ref>, (n) \u2264 n \u00b7(\u03b1^*). \nTherefore, for any integer l \u2265 0, we have\n\n    ((min{T_\u03f5, n}) > n \u00b7(\u03b1\u0305\u03b3^l)) \n    \u2264(N(l+1,n) > 0).\n\nBy Proposition \n<ref>, \n\n\n\n\n    (N(l+1,n) >0 ) \u2264 n (q/p)^l+1  +\n    \t2 \u221a(pq)/(1-2 \u221a(p q))^2(2q)^l+1.\n\n\nThis implies\n\n\n\n\n    ((min{T_\u03f5, n}) > n \u00b7(\u03b1\u0305\u03b3^l)) \u2264min{ 1, n(q/p)^l+1 + 2 \u221a(pq)/(1-2 \u221a(p q))^2(2q)^l+1}.\n\n\n\n\n\nBy the definition of expectation,\n\n    [(min{T_\u03f5, n})]\n       = \u2211_i=0^n \u00b7(\u03b1\u0305\u03b3^n)i \u00b7((min{T_\u03f5, n}) = i)\n       \u2264\u2211_l=0^n-1((min{T_\u03f5, n}) \u2208 (n \u00b7(\u03b1\u0305\u03b3^l) , n \u00b7(\u03b1\u0305\u03b3^l+1)]) \u00b7 n  (\u03b1\u0305\u03b3^l+1) \n           + ((min{T_\u03f5, n}) \u2208 [0 , n  (\u03b1\u0305)]) \u00b7 n  (\u03b1\u0305) \n       \u2264\u2211_l=0^n-1((min{T_\u03f5, n})> n  (\u03b1\u0305\u03b3^l) ) \u00b7  n  (\u03b1\u0305\u03b3^l+1) + \n    \tn  (\u03b1\u0305)  \n       \u2264\u2211_l=0^n-1min{ 1, n(q/p)^l+1 + 2 \u221a(pq)/(1-2 \u221a(p q))^2(2q)^l+1}\u00b7 n  (\u03b1\u0305\u03b3^l+1)\n    \t+ n  (\u03b1\u0305).\n\n\n\n\n\n \u00a7.\u00a7 Abstract high probability total oracle complexity\n\n\nWe now proceed to bound (T_) in high probability, using <Ref>.\n\n\n\n\tLet <Ref> and <Ref> hold in Algorithm <ref>.\n\t For any \u03c9  > 0 and positive integer n, with probability at least 1-(T_> n)- n^-\u03c9 - cn^-(1+\u03c9 ), \n    (T_)\u2264 n\u00b7(\u03b1^*(n)),\n \n\t where \u03b1^*(n)= \u03b1\u0305\u03b3   n^-(1+\u03c9)log_1/2q 1/\u03b3 , and c is as defined in <Ref>.\n\t \n\t If \u03b3 is chosen to be at least max{1/2, (1/2q)^log(2\u03b2)/(1+\u03c9)log n} for some \u03b2 < 1/2, then \u03b1^*(n) \u2265\u03b2\u03b1\u0305, thus\n\t \n    (T_)\u2264 n\u00b7(\u03b2\u03b1\u0305).\n \n\n\n\n\t\n\tLet _\ud835\uddcb\ud835\uddd0(n) be the total oracle complexity of the first n iterations with the corresponding sequence of parameters \u03b1_k induced by the one-sided random walk (that is, the sequence defined by \u03b1_k = \u03b1\u0305\u03b3^Z_k, where Z_k is defined in <Ref>). In other words, \n\t\n    _\ud835\uddcb\ud835\uddd0(n) = \u2211_k=1^n (\u03b1\u0305\u03b3^Z_k).\n\n\tWith probability 1 - (T_ > n), we have T_\u2264 n, which implies\n\t\n    (T_)\u2264_\ud835\uddcb\ud835\uddd0(T_) \u2264_\ud835\uddcb\ud835\uddd0(n).\n\n\tHere, the first inequality is by <Ref> and <Ref>, and the second inequality is by T_\u2264 n.\n\t\n\tThe same arguments used in the proof of <Ref> show that with probability at least 1 - n^-\u03c9 - c n^-(1+\u03c9),  we have min_1 \u2264 k \u2264 n\u03b1\u0305\u03b3^Z_k\u2265\u03b1^*(n). Thus, with at least this probability, _\ud835\uddcb\ud835\uddd0(n) \u2264 n\u00b7(\u03b1^*(n)). \n\t\n\tPutting these together with a union bound, the result follows.\n\t\n\tThe second part of the theorem follows from substituting in the specific choice of \u03b3.\n\n\n\n\n\n\n\u00a7 APPLYING TO STORM AND SASS\n\nIn this section, we demonstrate how the generic oracle complexity bounds in the previous section can be applied to concrete combinations of oracles and algorithms.  We will consider the specific setting of expected risk minimization and two algorithms, first-order STORM and SASS, which are described earlier in the paper and fully analyzed in <cit.> and  <cit.>,  respectively.  For each case, we will state the bounds on (\u03b1) as a function of \u03b1, and use those bounds in conjunction with the known bounds on T_ (that have been derived in previous papers), to obtain a bound on the total oracle complexity for each algorithm. \n\nThe results we obtain are the first ones that bound the total oracle complexity of STORM and SASS, and we show that both algorithms are essentially near optimal in terms of total gradient sample complexity. \nWhen deriving these results, for simplicity of presentation, we omit most of the constants involved in the specific bounds on T_ and specific conditions on various algorithmic constants. For all such details, we refer the reader to  <cit.> and  <cit.>. We will include short comments regarding these constants, but otherwise replace them with a O(\u00b7) notation. \n\n Problem Setting: Expected risk minimization (ERM) can be written as  \n\n    min_x\u2208^m\u03d5(x)=_d \u223c\ud835\udc9f[l(x,d)].\n \nHere, x represents the vector of model parameters, d is a data sample following distribution \ud835\udc9f, and l(x,d) is the loss when the model parameterized by x is evaluated on data point d. This problem is central in supervised machine learning and other settings such as simulation optimization <cit.>. For this problem, it is common to assume the function \u03d5 is L-smooth and is bounded from below, and gradients of functions \u2207_x l(x, d) can be computed for any d\u223c\ud835\udc9f, so we will consider this setting in this section.\n\nIn this setting, the zeroth- and first-order oracles are usually computed as follows: \n\n    f(x, \ud835\udcae) = 1/|\ud835\udcae|\u2211_d\u2208\ud835\udcael(x,d),   g(x, \ud835\udcae) = 1/|\ud835\udcae|\u2211_d\u2208\ud835\udcae\u2207_x l(x,d),\n\nwhere \ud835\udcae is the \u201cminibatch\" - that is a set of i.i.d samples from \ud835\udc9f. Generally, |\ud835\udcae| can be chosen to depend on x.\n\nIn what follows we will refer to the total number of times an algorithm computes l(x,d) for a specific x and d as its total function value sample complexity and the number of times the algorithm computes  \u2207 l(x,d) as its  total gradient sample complexity. The total (oracle or sample) complexity of the algorithm is defined as the sum of these two quantities. \n\n\n\n \u00a7.\u00a7 Total sample complexity of first-order STORM\n\n\n\n\nWe first consider the first-order stochastic trust region method (STORM) as introduced and analyzed \nin <cit.>. \n\n The algorithm uses zeroth- and first-order oracles defined in TR1.0 and TR1.1 in <Ref>. Trust region algorithms are usually applied to nonconvex functions and  the stopping time of STORM is defined as T_ = min{k: \u2207\u03d5(x_k)\u2264}. In Section 3.3 of <cit.>, it is shown  that Assumption <ref> is satisfied with \u03b1\u0305= /\u03b6, where \u03b6 is\n a moderate constant that depends on \u03ba_eg, L  and some constant chosen by the algorithm. \n \nIn  <cit.>, the oracle costs of STORM in the ERM setting are briefly discussed under the following assumptions on l(x, d).\n\n\n\n  * Function value: It is assumed that there is some \u03c3_f \u2265 0 such that for all x, _d\u223c\ud835\udc9f[l(x,d)] \u2264\u03c3_f^2. \n\n\n  * Gradient: It is assumed that _d \u223c\ud835\udc9f [\u2207_x l(x, d)] = \u2207\u03d5(x), and that there is some \u03c3_g \u2265 0 such that for all x, \n\n\n\n\n    _d\u223c\ud835\udc9f\u2207_x l(x,d)-\u2207\u03d5(x)^2\u2264\u03c3_g^2.\n \n\n\n The cost of each oracle call is the number of samples in the associated minibatch \ud835\udcae.\nBy applying   Chebyshev's inequality it is easy to bound the oracle costs of TR1.0 and TR1.1. \n\n\n\n\n\n  * Cost of TR1.0 with parameter \u03b1: _0(\u03b1) = \u03c3_f^2/\u03b4_0\u03ba_ef^2\u03b1^4,\n\t\n  * Cost of TR1.1 with parameter \u03b1: _1(\u03b1) = \u03c3_g^2/\u03b4_1\u03ba_e g^2\u03b1^2.\n\n\n\nBelow we substitute the specific oracle costs into <Ref> to obtain the expected total sample complexity for the first-order STORM algorithm. Specifically, we will bound the total sample complexity of STORM [(min{T_, n})]  by deriving bounds on the the expectation of the total\n function value sample complexity _0 and the total gradient sample complexity _1, where \n\n    _0(n) = \u2211_k=1^n _0(\ud835\udc9c_k), \u00a0\u00a0_1(n) = \u2211_k=1^n _1(\ud835\udc9c_k)  \u00a0and\u00a0(n)=_0(n)+_1(n).\n\n\n\n\n \n\nLet p = 1 - \u03b4_0 - \u03b4_1 and q = 1 - p. For the first-order STORM algorithm, for any iteration budget n \u2208^+, and \u03b3 > (2q)^1/4, we have\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    [(min{T_, n})] \u2264     O(nlog_p/q(n)  (\n    \u03c3_f^2/^4 \n    n^4log_q/p\u03b3  + \u03c3_g^2 /^2 n^2log_q/p\u03b3)).\n\n\n\nIf \u03b3\u2265(q/p)^log c/log n for some constant c > 1 (so that n^log_q/p\u03b3\u2264 c), the above simplifies to be \n\n    [(min{T_, n})] \u2264 nlog(n) \u00b7 O(\u03c3_f^2/^4 + \u03c3_g^2/^2).\n\n\n\n\n\n\nBy Theorem <ref>, the total expected cost of the zeroth-order oracle over n iterations is bounded above by:\n\n    [_0(min{T_, n})]    \u2264 n  \u2211_l=1^nmin{ 1, n(q/p)^l + 2 \u221a(pq)/(1-2 \u221a(p q))^2(2q)^l}\u00b7_0(\u03b1\u0305\u03b3^l)\n    \t+ n  _0(\u03b1\u0305) \n       \u2264 n\u2211_l=1^nmin{1, n(q/p)^l}\u00b7_0(\u03b1\u0305\u03b3^l)_=:A\n    \t + 2n\u221a(pq)/(1-2\u221a(pq))^2\u2211_l=1^n(2q)^l\u00b7_0(\u03b1\u0305\u03b3^l)_=: B\n    \t+ n _0(\u03b1\u0305).\n\n \nFor the zeroth-order oracle, _0(\u03b1) = \u03c3_f^2/\u03b4_0\u03ba_ef^2\u03b1^4 =  O(\u03c3_f^2/\u03b1^4). We use  this  to calculate upper bounds for A and B. First, we consider A. Note that min{1, n(q/p)^l} = 1, if and only if l \u2264log_p/q(n). Therefore, \n\n    A    \u2264\u2211_l=1^log_p/q(n)_0(\u03b1\u0305\u03b3^l) \n    + \n    n\u2211_l \u2265log_p/q(n)( q/p)^l _0(\u03b1\u0305\u03b3^l) \n       \u2264\u2211_l=1^log_p/q(n) O(\u03c3_f^2/\u03b1\u0305^4\u03b3^4l) \n    + \n    n\u2211_l \u2265log_p/q(n)( q/p)^l  O(\u03c3_f^2/\u03b1\u0305^4\u03b3^4l) \n       \u2264 O(\u03c3_f^2/\u03b1\u0305^4) \n    (\n    \u2211_l=1^log_p/q(n)1/\u03b3^4l\n    +\n    n\u2211_l \u2265log_p/q(n)( q/p)^l 1/\u03b3^4l) \n       \u2264 O(\u03c3_f^2/^4) \n    (\n    log_p/q(n)\n    \u00b7(1/\u03b3^4)^log_p/q(n) \n    +\n    n(q/p\u03b3^4)^log_p/q(n)1/1 - q/p\u03b3^4) \n       =    O(\u03c3_f^2/^4 log_p/q(n) \n      n^log_p/q(1/\u03b3^4)) \n       =    O(\u03c3_f^2/^4 log_p/q(n) \n      n^4log_q/p\u03b3).\n\nNext we bound B. We have \n\n    B = \u2211_l=1^n(2q)^l\u00b7_0(\u03b1\u0305\u03b3^l) \n    \u2264\u2211_l=0^\u221e(2q)^l\u00b7 O(\u03c3_f^2/\u03b1\u0305^4\u03b3^4l)\n    \u2264 O(\u03c3_f^2/^4) \n    ( 1/1 - 2q \u00b71/\u03b3^4) =  O(\u03c3_f^2/^4) .\n\nUsing these bounds on A and B in the  expression for [_0(min{T_\u03f5, n})], we obtain the bound on the  \ntotal function value sample complexity as O(\u03c3_f^2/^4  n log_p/q(n) \n  n^4log_q/p\u03b3).\n\n\nA similar calculation using the cost of the first-order oracle yields the bound O(\u03c3_g^2/^2  n log_p/q(n) \n  n^2log_q/p\u03b3) for [_1(min{T_\u03f5, n})]. Since (min{T_\u03f5, n})= _0(min{T_\u03f5, n})+_1(min{T_\u03f5, n}) by definition, the result follows.\n\n\n\n\nLet us discuss the implications of Theorem <ref>.  In  <cit.>, a lower bound on the total gradient sample complexity for stochastic optimization of non-convex, smooth functions is derived and shown to be, in the worst case,  C(1/^4), for some positive constant C.  This complexity lower bound holds even when exact function values \u03d5(x) are also available. We note that the definition of complexity in <cit.> is the smallest number of sample gradient evaluations required to return a point x with [\u2207\u03d5 (x)] \u2264, which is different from (T_\u03f5) which we are aiming to bound here. We believe that the lower bound in <cit.> applies to our definition as well, but this is a subject of a separate study. \n\n\nIn <cit.>,  it is shown that [T_] \u2264C_1/^2 for some C_1 sufficiently large that depends on \u03b4_1, \u03b4_0, \u03ba_eg, L and some algorithmic constants. \nThus, if n = C_1/^2 in inequality\n (<ref>) of Theorem <ref> , as long as \u03b3 is sufficiently large,  we obtain \n[(min{T_, n})] \u2264 O( (\u03c3_f^2/^6  + \u03c3_g^2/^4) log(1/) ).\nIn particular, the total gradient sample complexity is O(\u03c3_g^2/^4log(1/)), which essentially matches the complexity lower bound as described in <cit.> up to a logarithmic factor. The total function value sample complexity is worse than that of the gradient  if \u03c3_f^2 is large. However, if \u03c3_f^2\u2264 O(\u03c3_g^2^2), the total sample complexity bound  of\nSTORM matches the lower bound up to a logarithmic factor. \n\n\nWe note now that choosing   n = C_1/^2 in Theorem <ref> does not in fact guarantee that T_<n, since\nfor STORM, only a bound on  [T_] has been derived. However, \nthis statement can be made true in probability, thanks to  <Ref>, by simply applying Markov inequality for n = C_2 C_1/^2 (where C_2 > 1).  \n\n \n For the first-order STORM algorithm applied to expected risk minimization, let \nn be chosen such that n \u2265 C_2 C_1/^2 (for some C_1 sufficiently large so that C_1/^2\u2265[T_], and any C_2>1), and \u03b3 be chosen so that \u03b3\u2265max{1/2, (1/2q)^log(2\u03b2)/(1+\u03c9)log n} (for some \u03b2\u22641/2, and any \u03c9  > 0). Then, with probability at least\t1-1/C_2-  O(n^-\u03c9), \n\t\n\n\n\n\n\n\n\n\n\n\n\n\n\t\n    (T_)  \u2264 O(\u03c3_f^2/^6 +  \u03c3_g^2/^4).\n\n\n\nThe theorem is a simple application of <Ref> to the specific setting. \n\n\n\n\n\n\t\n  * Compared to the expected total sample complexity bound, this high probability bound is smaller by a log factor.\n\n\n\n\t\n  * In <cit.>,  a first-order  trust region algorithm similar to STORM with the same first-order oracle  (i.e. TR1.1 with \u03f5_g = 0), but with an exact zeroth-order oracle (i.e. TR1.0 with \u03ba_ef=\u03b4_0=0) is analyzed. In this case, it is shown that (T_ > n) \u2264exp(-C_1n) (for some constant C_1 depends on \u03b4_1), for any n \u2265C_2/^2 (with some sufficiently large C_2). Using a similar application of  <Ref>, we can show that as long as \u03b3 is sufficiently large, the total gradient sample complexity of that trust region algorithm is bounded above by O(\u03c3_g^2/^4) with probability at least 1 - exp(-C_1n) -  O(n^-\u03c9) (which is a significant improvement over the probability in Theorem <ref>).\n\n\t\n  * Another first-order trust region algorithm, with weaker oracle assumptions than those in <cit.>  is introduced and analyzed in <cit.>. This algorithm relies on the first-order oracle  as described in TR1.1 and the zeroth-order oracle  as described in SS.0. For this algorithm, it is shown that  (T_ > n) \u2264 2exp(-C_1n) + exp(-C_2) (C_2 being any positive constant), where n =  C_3C_2/^2 for some sufficiently large C_3 and some positive C_1 that depends on \u03b4_0 and \u03b4_1. Thus, again, using  <Ref>  we can show that as long as  \u03b3 is sufficiently large, the total sample complexity of the first-order trust region algorithm in <cit.> is bounded above by O(\u03c3_f^2/^6 + \u03c3_g^2/^4) with probability at least 1-2 exp(-C_1n) - exp(-C_2) -  O(n^-\u03c9).\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Total sample complexity of SASS\n\n\nWe now consider the SASS algorithm[This algorithm was also referred to as ALOE in <cit.>. Its name has been changed to SASS since <cit.>.], analyzed in <cit.> and described in <Ref>. \n\nBy Proposition 1, 2 and 4 of <cit.>,  <Ref> is satisfied, with \u03b1\u0305 as given in the propositions. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the empirical risk minimization  setting,  the following  assumptions on l(x, d) are made in <cit.>.\n\n\n\n  * Function value: It is assumed that l(x,d) - \u03d5(x) is a subexponential random variable and that there is some \u03c3_f \u2265 0 such that for all x, _d\u223c\ud835\udc9f[l(x,d)] \u2264\u03c3_f^2. \n\nFor example, if l(x,d) is uniformly bounded, then l(x,d) - \u03d5(x) is subexponential.\n\n\n  * Gradient: It is assumed that _d \u223c\ud835\udc9f [\u2207_x l(x, d)] = \u2207\u03d5(x), and that for some M_c, M_v\u2265 0 and for all x, \n\n\n\n\n    _d\u223c\ud835\udc9f\u2207_x l(x,d)-\u2207\u03d5(x)^2\u2264 M_c+M_v\u2207\u03d5(x)^2.\n \nThis assumption is fairly general and is studied in the literature <cit.>. \n\n\n\n\nFor non-convex functions, the stopping time is defined as T_ = min{k: \u2207\u03d5(x_k)\u2264}, same as in the case of STORM. \nFor strongly convex functions, the stopping time is defined as T_ = min{k: \u03d5(x_k) - inf_x \u03d5(x) \u2264}. To achieve the desired accuracy,  oracles SS.0 and SS.1 have to be sufficiently accurate in the sense that \u03f5_f and \u03f5_g have to be sufficiently small with respect to . In the case of expected risk minimization, \nthe oracles can be implemented for any \u03f5_f and \u03f5_g by choosing an appropriate mini-batch size. Thus, here we will first fix  and then \ndiscuss the oracles that deliver sufficient accuracy for such , for the theory in  <cit.> to apply. \n\nOracle Costs per Iteration. \nIn <cit.>, it is shown that given the desired convergence tolerance , sufficiently accurate oracles SS.0 and SS.1 can be implemented for any \n step size parameter  \u03b1 as follows: \n\n\t\n  * Zeroth-order oracle: \n\nProposition 5 of <cit.> shows that a sufficiently accurate zeroth-order oracle can be obtained by using a minibatch of size \n\n\n    _0(\u03b1) = \n     O(\u03c3_f^2/^4),    for the  non-convex case,\n     O(\u03c3_f^2/^2),    for the strongly convex case.\n\nNote that the cost of the zeroth-order oracle is independent of \u03b1.\n\n\n\n\n\n\n\n\n  * First-order oracle: Proposition 6 of <cit.> implies that a sufficiently accurate first-order oracle can be obtained by using a minibatch of size\n\n\n\n\n\n\n\n\n\n\n\n\n\n    _1(\u03b1) = \n    \n    \n     O(M_c/^2 + M_v/min{\u03c4, \u03ba\u03b1}^2),    for the non-convex case,\n     O(M_c/ + M_v/min{\u03c4, \u03ba\u03b1}^2),    for the strongly convex case.\n\nThe cost of the first-order oracle is indeed non-increasing in \u03b1, so <Ref> is satisfied. \nFor simplicity of the presentation and essentially without loss of generality, we will assume \u03c4\u2265\u03ba\u03b1\u0305.  \n\n\n\n\n\n\n\n\n\n\n\n\nSubstituting these bounds into <Ref>, we obtain the following expected total sample complexity. \n\n\n\n\n\n\n\nFor the SASS algorithm applied to expected risk minimization, for any iteration budget n \u2208^+, and any \u03b3 > (2q)^1/2, we have\n\n\n\n  * Non-convex case:\n\n    [(min{T_, n})] \u2264     O(\u03c3_f^2/^4\u00b7 n + M_c/^2\u00b7 n\n    + M_v \u00b7 n\n    (\n    n^log_p/q(1/\u03b3^2)log_p/q(n)\n    )).\n\n\n  * Strongly convex case:\n\n    [(min{T_, n})] \u2264     O(\u03c3_f^2/^2\u00b7 n + M_c/\u00b7 n\n    + M_v\u00b7 n\n    (\n    n^log_p/q(1/\u03b3^2)log_p/q(n)\n    )).\n\n\n\n\nMoreover, if \u03b3\u2265(q/p)^log c/2log n for some constant c > 1 (so that n^log_p/q(1/\u03b3^2)\u2264 c), the above simplifies to\n\n\t\n  * Non-convex case:\n\t\n    [(min{T_\u03f5, n})] \u2264 O(n (\u03c3_f^2/^4 + M_c/^2 +  M_v log(n))).\n\n\t\n  * Strongly convex case:\n\t\n    [(min{T_\u03f5, n})] \u2264 O(n (\u03c3_f^2/^2 + M_c/ +  M_v log(n))).\n\n\n\n\n\n\nSince the cost of each call to the zeroth-order oracle (<ref>) is independent of \u03b1, the total function value sample complexity over n iterations is simply obtained by multiplying (<ref>) by n.\n\nThe  cost of the first-order oracle (<ref>) consists of two parts, _1(\u03b1) = _1,1(\u03b1) + _1,2(\u03b1). The first part, _1,1(\u03b1),  is O(M_c/^2). Since this is independent of \u03b1, the total contribution of this part to the total gradient sample complexity over n iterations is n  _1,1(\u03b1), which is bounded by O(M_c/^2 n).\n\nThe second part of the cost of the first-order oracle is _1,2(\u03b1) :=  O(M_v/min{\u03c4, \u03ba\u03b1}^2). By Theorem <ref>, the total expected cost over n iterations from this part is bounded above by:\n\n    n  \u2211_l=1^nmin{ 1, n(q/p)^l + 2 \u221a(pq)/(1-2 \u221a(p q))^2(2q)^l}\u00b7_1,2(\u03b1\u0305\u03b3^l)\n    \t+ n  _1,2(\u03b1\u0305) \n       \u2264 n\u2211_l=1^nmin{1, n(q/p)^l}\u00b7_1,2(\u03b1\u0305\u03b3^l)_=:A\n    \t + 2n\u221a(pq)/(1-2\u221a(pq))^2\u2211_l=1^n(2q)^l\u00b7_1,2(\u03b1\u0305\u03b3^l)_=: B\n    \t+ n _1,2(\u03b1\u0305).\n\n \n\n\nNote that the expression above only involves _1,2(\u03b1) for \u03b1\u2264\u03b1\u0305. Therefore, by our earlier assumption that \u03c4\u2265\u03ba\u03b1\u0305, we have _1,2(\u03b1) =  O(M_v/\u03ba^2\u03b1^2) =  O(M_v/\u03b1^2) (since \u03ba is a constant). We now use this to calculate upper bounds for A and B. Using similar arguments as the proof of <Ref>, we have \n\n    A =  O(M_v/\u03b1\u0305^2 log_p/q(n)   n^log_p/q(1/\u03b3^2)) \n     \u00a0and\u00a0 B =  O(M_v/\u03b1\u0305^2) .\n\n\nTogether with the previous arguments, the result follows.\n\n\nIn <cit.>, the iteration bound on T_\u03f5 is shown to be C_1/^2+log_1/\u03b3\u03b1_0/\u03b1\u0305 in the non-convex case, and C_2log1/+log_1/\u03b3\u03b1_0/\u03b1\u0305 in the strongly convex case (for some C_1 and C_2 sufficiently large) in high probability. Thus, we can select n appropriately and derive the high probability bound on the total sample complexity of SASS using <Ref>.\n\n\n \n\nFor the SASS algorithm applied to expected risk minimization, let n \u2265C_1/^2+log_1/\u03b3\u03b1_0/\u03b1\u0305 in the non-convex case, and n \u2265 C_2log1/+log_1/\u03b3\u03b1_0/\u03b1\u0305 in the strongly convex case.\n\n\nFor any \u03c9  > 0,\n\n\twith probability at least 1-2exp(-C_3n) -  O(n^-\u03c9) (for some C_3 >0), we have\n\t\n\t\t\n  * Non-convex case:\n\t\t\n    (T_)  \u2264 O((1/^2+log_1/\u03b3\u03b1_0/\u03b1\u0305)\u00b7( \u03c3_f^2/^4  + M_c/^2 + M_v/^4(1+\u03c9)log_2q(\u03b3))).\n\n\t\t\n  * Strongly convex case:\n\t\t\n    (T_)  \u2264 O( ( log1/+log_1/\u03b3\u03b1_0/\u03b1\u0305 )\u00b7(\u03c3_f^2/^2  + M_c/ + M_v(log1/)^2(1+\u03c9)log_2q(\u03b3)) ).\n\n\t\n\n\n\n\n\n\n\n\n\n\n\n\t\n\tIf \u03b3\u2208[ max{1/2, (1/2q)^log(2\u03b2)/(1+\u03c9)log n}, (\u03b1\u0305/\u03b1_0)^1/cn ], where \u03b2 is any constant smaller than 1/2, and c is any constant in (0,1), the above simplifies to\n\t\n\t\t\n  * Non-convex case:\n\t\n    (T_)  \u2264 O(1/^2\u00b7( \u03c3_f^2/^4 +  M_c/^2 + M_v/\u03b2^2)).\n\n\t\t\n  * Strongly convex case:\n\t\t\n    (T_)  \u2264 O(log1/\u00b7( \u03c3_f^2/^2 +  M_c/ + M_v/\u03b2^2)).\n\n\t\n\n\t\n\t\n\n\nThe bounds (<ref>) and (<ref>) follow by using (<ref>) and (<ref>) in <Ref>, and Theorem 3.8 of <cit.>.\n\nThe bounds (<ref>) and (<ref>) follow from (<ref>) and (<ref>), respectively, by using the fact that \u03b3 lies in the appropriate range and log_1/\u03b3\u03b1_0/\u03b1\u0305\u2264 cn with c \u2208 (0,1).\n\nIn the non-convex case, we have 1/^2 + log_1/\u03b3\u03b1_0/\u03b1\u0305 =  O(1/^2) and M_v/^4(1+\u03c9)log_2q(\u03b3) = \ud835\udcaa(M_v/\u03b2^2) when \u03b3 lies in the specified range. It is worth noting that c \u2208 (0,1) implies there is some n= O(1/^2) that satisfies n \u2265C_1/^2+log_1/\u03b3\u03b1_0/\u03b1\u0305. The result for the strongly convex case follows similarly.\n\n\n\n\n\n\n\n\n\t\n\t\n  * \n\tWe consider some of the implications of <Ref> below. Similar implications hold for the expected total sample complexity.\t\n\t\n\t\t\n  * In the non-convex case, from (<ref>), the total function value sample complexity is O(\u03c3_f^2/^6)  and the total gradient sample complexity is  O(M_c/^4 +  M_v/^2). \n\t\t\t\tIn particular, the total gradient sample complexity matches that of SGD, and it essentially matches the complexity lower bound as described in <cit.> (for a different definition of complexity). Specifically, if \u03c3_f=0  (i.e., function values are exact),\n\tthe lower bound in <cit.> applies and the total sample complexity of SASS\tmatches it. \t\t\n\n\n\n \n  * If M_c=0 (sometimes referred to as the interpolation case), then the total gradient sample complexity reduces to O(M_v/^2). Hence, the total gradient sample complexity matches that of SGD under interpolation <cit.>.\n\n\n\t\t\n  * In the strongly convex case, the total function value sample complexity is O( \u03c3_f^2/^2log1/)  and the total gradient sample complexity is O( (M_c/ + M_v)log1/).\n\t\t\n\t\tIn particular, the total gradient sample complexity matches that of SGD up to a logarithmic term.\n\t\t\n\t\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\n\n\nWe analyzed the behavior of the step size parameter in <Ref>, an adaptive stochastic optimization framework that encompasses a wide class of algorithms analyzed in recent literature. We derived a high probability lower bound for this parameter, and as a result, developed a simple strategy for controlling this lower bound. \n\nFor many settings, having a fixed lower bound on the step size parameter implies an upper bound on the cost of the oracles that compute the gradient and function estimates.  We developed a framework to analyze the expected and high probability total oracle complexity for this general class of algorithms, and illustrated the use of it by deriving total sample complexity bounds for two specific algorithms - the first-order stochastic trust region (STORM) algorithm <cit.> and a stochastic step search (SASS) algorithm <cit.> in the expected risk minimization setting.   We showed that the sample complexity of both these algorithms essentially matches the complexity lower bound of first-order algorithms for stochastic non-convex optimization <cit.>, which was not known before. \n\n\n\n\u00a7 ACKNOWLEDGMENTS\n\n\t\n\tThe authors wish to thank James Allen Fill for pointing out useful references and the proof of Proposition <ref>.\n\tThis work was partially supported by NSF Grants  TRIPODS 17-40796, CCF 2008434, CCF 2140057 and \n\tONR Grant N00014-22-1-2154. \n\t\n\t Billy Jin was partially supported by NSERC fellowship PGSD3-532673-2019. \n\n\n\nalpha \n\n\n\n\n"}