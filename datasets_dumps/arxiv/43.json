{"entry_id": "http://arxiv.org/abs/2303.07327v1", "published": "20230313174539", "title": "Unsupervised HDR Image and Video Tone Mapping via Contrastive Learning", "authors": ["Cong Cao", "Huanjing Yue", "Xin Liu", "Jingyu Yang"], "primary_category": "cs.CV", "categories": ["cs.CV", "eess.IV"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnsupervised HDR Image and Video Tone Mapping via Contrastive Learning\n    Cong Cao, Huanjing Yue, Member, IEEE, Xin Liu, Jingyu Yang, Senior Member, IEEE\nThis work was supported in part by the National Natural Science\nFoundation of China under Grant 62072331 and Grant 62231018.\nC. Cao, H. Yue (corresponding author), X. Liu, and J. Yang are with the School of Electrical and Information Engineering, Tianjin University.\n    March 30, 2023 at \n==============================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\n\nCapturing high dynamic range (HDR) images (videos) is attractive because it can reveal the details in both dark and bright regions. Since the mainstream screens only support low dynamic range (LDR) content, tone mapping algorithm is required to compress the dynamic range of HDR images (videos). Although image tone mapping has been widely explored, video tone mapping is lagging behind, especially for the deep-learning-based methods, due to the lack of HDR-LDR video pairs. In this work, we propose a unified framework (IVTMNet) for unsupervised image and video tone mapping. To improve unsupervised training, we propose domain and instance based contrastive learning loss. Instead of using a universal feature extractor, such as VGG to extract the features for similarity measurement, we propose a novel latent code, which is an aggregation of the brightness and contrast of extracted features, to measure the similarity of different pairs. We totally construct two negative pairs and three positive pairs to constrain the latent codes of tone mapped results. For video tone mapping, we propose a temporal-feature-replaced (TFR) module to efficiently utilize the temporal correlation and improve the temporal consistency of video tone-mapped results. We construct a large-scale unpaired HDR-LDR video dataset to facilitate the unsupervised training process for video tone mapping. Experimental results demonstrate that our method outperforms state-of-the-art image and video tone mapping methods. Our code and dataset will be released after the acceptance of this work.\n\n\n\n\n\n\nImage and video tone mapping, contrastive learning, video tone mapping dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn recent years, high dynamic range (HDR) imaging has been attracting more and more attention due to its superior performance in simultaneously revealing the details in dark and bright regions. However, since most devices only support low dynamic range display, tone mapping algorithms are required to compress the dynamic range of HDR images/videos to enable watching on LDR screens <cit.>.\n\n\n\n\n\nTraditional HDR image tone mapping methods can be classified into global tone mapping <cit.> and local tone mapping <cit.>. Recently, the deep learning based methods have also been introduced to tone mapping. For supervised learning, the problem is how to obtain the ground truths for the input LDRs.  One solution is utilizing several available tone-mapping algorithms to generate the LDRs and the LDR result with the highest tone mapped quality index (TMQI) is selected as the ground truth <cit.>. However, the performance is limited by the upper bound of the available tone-mapping methods. Another solution is training the tone mapping network with enhancement datasets, where the paired data are originally constructed for image retouching or low-light image enhancement <cit.>. Since there exists a domain gap between the under-exposure or low-light images and HDR images, the network trained on the enhancement dataset cannot work well for HDR tone mapping. Recently, Vinker et al. <cit.> propose to use unpaired HDR-LDR images for unsupervised training and has achieved promising performance. They utilize structure loss to preserve the structure consistency between HDR and LDR output, and utilize generative adversarial network (GAN) loss to force the brightness and contrast of outputs close to those of high quality LDRs. However, there still remains a lot of under-enhanced areas in their results. Therefore, developing better unsupervised training strategy is demanded.\n\nFor HDR video tone mapping, only traditional methods are developed <cit.>. How to avoid temporal flickering and maintain rich details simultaneously is still a challenge in video tone mapping. The works in <cit.> utilize global operators which can generate results with good temporal coherency but low spatial contrast. The works <cit.> using local operators can generate results with high contrast but more temporal artifacts. Therefore, developing an effective video tone mapping method to achieve a good balance between temporal consistency and richful details is demanded.\n\n\n\n\n\n\n\n\n\n\nBased on the above observations, we propose a unified method for both image and video tone mapping. Our contributions are summarized as follows.\n\n\n\n\n  * We propose an effective HDR image tone mapping network. We propose a Spatial-Feature-Enhanced (SFE) module, which utilizes graph convolution, to enable information exchange and transformation of nonlocal regions. We propose a Temporal-Feature-Replaced (TFR) module to extend our method to HDR video tone mapping which is MAC-free and can efficiently utilize the temporal correlation and improve the temporal consistency of tone-mapped results.\n\n\n\n\n\n  * Unsupervised learning is difficult to be optimized and we propose a set of unsupervised loss functions to improve the result. First, we propose domain and instance based contrastive loss, in which we construct five negative and positive pairs to constrain the outputs to be close with well LDRs and construct a suitable latent space for measuring the similarity of the latent codes in negative and positive pairs. Second, we further propose naturalness loss to constrain the brightness and contrast of outputs.\n\n\n\n\n\n  * We construct a large-scale unpaired HDR-LDR video dataset with both real and synthetic HDR and LDR videos, which is beneficial for the development of video tone mapping. Experimental results demonstrate that our method outperforms existing state-of-the-art image and video tone-mapping methods.\n\n\n\n\n\n\u00a7 RELATED WORKS\n\n\nIn this section, we give a brief review of related work on HDR image and video tone mapping, image and video enhancement, and contrastive learning.\n\n\n\n \u00a7.\u00a7 HDR Image and Video Tone Mapping\n\n\n\nTraditional HDR image tone mapping algorithms include global tone mapping <cit.> and local tone mapping <cit.>. Global tone mapping <cit.> utilizes a global curve to compress the HDR image, which can keep the relative brightness of the input image, but usually leads to a severe reduction in local contrasts. Local tone mapping <cit.> is good at improving local contrasts and details, but usually leads to halo artifacts among high-contrast edges. For DNN-based HDR image tone mapping methods, there are mainly three categories. One category focuses on supervised learning <cit.>, and they apply several tone-mapping algorithms to HDR images and select the result which has the highest TMQI <cit.> as the ground truth. The second category regards tone mapping as image enhancement task and uses the enhancement dataset with paired data for training <cit.>. The last category gets rid of the LDR-HDR pairs, and utilizes unpaired HDR and LDR data <cit.> or only HDR data <cit.> for unsupervised training. These works either focus on the unsupervised loss functions <cit.> or utilize no-reference image quality assessment metric to optimize the tone mapping network on HDR images <cit.>.\n\n\nOn the basis of image TMO, traditional video tone mapping algorithms <cit.> further introduce temporal processing to keep temporal stability. Although there exist lots of DNN-based HDR image TMO methods, there is still no DNN-based video TMO methods. In this work, We propose a unified unsupervised learning method for both image and video tone mapping. We further construct a large-scale unpaired HDR-LDR video dataset to facilitate the development of video tone mapping.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Image and Video Enhancement\n\nImage (video) enhancement is similar to tone mapping since it also aims at improving the brightness and contrast of the inputs.\nTraditional image enhancement methods are usually histogram-based (HE)  or retinex-based methods. HE-based methods pay attention to changing the histogram of the input image to improve the brightness and contrast <cit.>. Retinex-based methods decompose an input image into reflectance and illumination layers, and then enhance the image by adjusting the illumination layer <cit.>. For DNN-based image enhancement methods, there are three kinds of training strategies which are full-supervised learning <cit.>, semi-supervised learning <cit.>, and unsupervised learning methods <cit.>. For unsupervised learning, the work in <cit.> maps low-quality smartphone photo to high-quality DSLR photo by utilizing CycleGAN-like architecture <cit.>. The works in <cit.> design unsupervised loss functions to train zero-shot models. The work in <cit.> proposes a self-regularized perceptual loss to constrain content consistency between low-light images and enhanced images, and utilize adversarial learning to enhance contrast and brightness.\n\nThere are several DNN-based video enhancement methods <cit.>, but all of them are based on supervised learning and paired data. Since there exists distribution gap between the low-light images for image enhancement and the HDR images, directly applying image (video) enhancement methods to tone mapping can not achieve satisfactory results (see the experiments).\n\n\n\n \u00a7.\u00a7 Contrastive Learning\n\n\n\n\n\nContrastive learning has achieved promising progress in self-supervised and unsupervised representation learning <cit.>. It aims to improve the representation of the anchor by pushing it away from negative samples and pulling the anchor close to positive samples in the latent space. The key is how to construct positive and negative pairs, and find the latent space for distance measuring. Recently, contrastive learning has been applied to low-level vision tasks, such as image translation <cit.>, super-resolution <cit.>, dehazing <cit.>, deraining <cit.>, and underwater image restoration <cit.>.\nFor the strategy to construct positive and negative pairs, the work in <cit.> views content consistent patches in two domains as positive pairs and views other patches as negative samples. The work in <cit.> applies contrastive learning to unsupervised degradation estimation, taking the same and different degradations as positive and negative pairs respectively.  The works in <cit.> take high-quality images as positive samples, and take low-quality input images as negative samples. For the latent feature (code) for distance measuring, <cit.> utilizes VGG <cit.> network to extract latent features, <cit.> utilizes an extra encode network to code the degradation types,  <cit.> utilize the features extracted by the training network itself. For unsupervised image (video) tone mapping, since there are no paired supervisions, the extracted features by the generator of different images (videos) have different contents and they cannot be directly utilized for distance measuring. Therefore, we propose to aggregate the mean brightness and contrast of different channels and utilize it as the latent code of the tone-mapped images (videos).\n\n\n\n\u00a7 THE PROPOSED METHOD\n\n\nIn this section, we first introduce the network structure and then present our compound loss functions.\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Network Structure\n\n\nGiven an HDR image (video) I^h, we aim to generate its high-quality LDR image (video) I^o through our network IVTMNet, as shown in Fig. <ref>. IVTMNet is constructed by a UNet-like generator and a discriminator. To capture the global statistics of brightness and contrast, we introduce the spatial feature enhanced (SFE) module at the bottom level of the UNet. For video tone mapping, we propose temporal feature replaced (TFR) module, which is beneficial for temporal consistency. It is convenient to switch between video and image TMO by removing the TFR module since this module is MAC (multiply\u2013accumulate operation) free.\n\n\n\nThe discriminator is utilized to distinguish the tone mapped result and it is constructed by cascaded convolutions with stride 2.\nInspired by <cit.>, we also perform tone-mapping on the luminance channel (Y) in YUV space. We denote the input and output Y channel as Y^h and Y^o, respectively. Y^o is converted to I^o by incorporating the original U and V channels of I^h. The key modules of our network are TFR and SFE, whose details are given in the following.\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Spatial-Feature-Enhanced (SFE) Module\n\nA large receptive field is beneficial for image (video) enhancement since it can capture global statistics of brightness and contrast <cit.>. In this work, we adopt graph convolution proposed in <cit.> to further enhance the spatial features and enlarge the receptive field of the network. We split the feature map into a number of patches and each patch is treated as a node. These nodes construct a graph by connecting neighboring nodes. This graph can transform and exchange information from all the nodes. In this way, similar patches with long distances can exchange and share information, which can improve the performance. Following  <cit.>, we also apply multi-layer perceptron (MLP) module after the graph convolution for node feature transformation. To save computation cost, we only utilize the SFE module at the bottom scale of the UNet.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Temporal-Feature-Replaced (TFR) Module\n\nCompared with image tone mapping, the major challenge for video tone mapping is making the results temporal consistent. Traditional video tone mapping methods usually solve this problem in two ways, i.e.,  utilizing flow-guided local filtering or\ntemporal smoothness after tone mapping. A simple temporal smoothness strategy is blending the results of previous frames with current frame\n\nI_t, denoted by\n\n    \u03a6\u0303(I_t) = \u03b1\u03a6(I_t)+ (1-\u03b1)\u03a6\u0303(I_t-1),\n\nwhere \u03a6() represents the tone mapping operation and \u03a6\u0303(I_t) represents the temporal filtering result of \u03a6(I_t). However, this may introduce ghosting artifacts. In this work, we propose a TFR module to imitate it. Recently, alignment-free module has been widely used in video denoising <cit.>. TFR module can be regarded as a kind of alignment-free module. Since the TFR module is applied during the feature extraction process other than the final fusion process, our module can alleviate the ghosting artifacts while improving the temporal consistency.\n\n\n\n\nSpecifically, for each frame Y_t, we split its corresponding feature F_t into F^1_t and F^2_t along the channel dimension by a certain ratio \u03b2 (it is set to 1/32 in our experiments). We replace part of the feature of current frame F^1_t with the feature of its previous frame F^1_t-1, then concatenate F^1_t and F^2_t-1, constructing the temporal enhanced feature F\u0302_t for the t^th frame. The whole process is formulated by\n\n\n    F^1_t, F^2_t = Split(F_t)\n       F^1_t-1, F^2_t-1 = Split(F_t-1)\n       F\u0302_t = Concat(F^1_t,F^2_t-1)\n\nAs Fig. <ref> shows, we insert the TFR module after each feature extraction block. Since this module is applied on the multi-scale feature level,\n\nthe network can selectively utilize the feature F^2_t-1, which help reduce the flicking but avoid the ghosting artifacts. For the completely static regions, this operation can help reduce noise since two frames with the same objects but different noise are fused.\nTherefore, the proposed TFR can help reduce the flicking artifacts and avoid ghosting artifacts. Note that, during testing, we utilize a buffer to save the features of the previous frame, which can be recurrently used for predicting the result of the next frame.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Loss Functions\n\nSince there are no perfect HDR-LDR image (video) pairs for supervised learning, we propose unsupervised losses to optimize the network. The following are the details for these loss functions.\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Structure Loss\n\nThe tone mapping process should only change the brightness of the objects and not change the content. Therefore, we adopt the structure loss proposed in <cit.> to preserve the content and structure between the input HDR data and output of the network. The structural similarity between two images is measured by Pearson correlation <cit.>, which can be formulated as\n\n    \u03c1(I_1,I_2) = 1/n_p\u2211_p_I_1,p_I_2cov(p_I_1, p_I_2)/\u03c3(p_I_1)\u03c3(p_I_2),\n\nwhere p_I_1 and p_I_2 are patches in images I_1 and I_2, cov(\u00b7,\u00b7) and \u03c3(\u00b7) are covariance and standard deviation computed on patches, n_p is the total number of patches. It is extended to structure loss as:\n\n    \u2112_Struct = \u2211_t=1^T\u2211_k\u2208{0,1,2}\u03c1(\u2193^kY^h_t,\u2193^kY^o_t),\n\nwhere \u2193^k represents downsampling operations for the  k^th spatial scale (k=0 represents the original resolution and k=1 represents the 1/2 downsampled resolution) and t represents the temporal index for consecutive frames in video. For image tone mapping, T is 1.\n\n\n\n  \u00a7.\u00a7.\u00a7 Adversarial Loss\n\nWe utilize adversarial learning to help the network generate pleasant results. We train our discriminator network D to distinguish between the generator's output Y^o and unpaired well LDR data Y^wl. The generator G is trained to generate good images (videos) to fool the discriminator. Different from <cit.>, which uses least-squares GAN <cit.>, we adopt the Dual Contrastive GAN <cit.> for training since it can further improve the feature representations.\nThe discriminator loss is formulated as\n\n    \u2112_DCL-D   = \ud835\udd3c_Y^wl[loge^D(Y^wl)/e^D(Y^wl)+\u2211_Y^o e^D(Y^o)] \n       +\ud835\udd3c_Y^o[loge^-D(Y^o)/e^-D(Y^o)+\u2211_Y^wl e^-D(Y^wl)],\n\nwhere \u2211_xf(x) represents the sum of N processed samples f(x).\nThe first item aims at teaching the discriminator to disassociate a single well LDR image (video) against a batch of generated images (videos). The second item aims at\ndisassociating a single generated image (video) against a batch of well-LDR images (videos).\nThe generator loss can be correspondingly formulated as\n\n    \u2112_DCL-G   = \ud835\udd3c_Y^wl[loge^-D(Y^wl)/e^-D(Y^wl)+\u2211_Y^o e^-D(Y^o)] \n       +\ud835\udd3c_Y^o[loge^D(Y^o)/e^D(Y^o)+\u2211_Y^wl e^D(Y^wl)],\n\n\nThe final adversarial loss can be formulated as\n\n    \u2112_DCL-Adv = \u2112_DCL-D + \u03bb_Adv\u2112_DCL-G\n\nwhere \u03bb_Adv is the weighting parameter (which is set to 0.1) to control the ratio between discriminator and generator loss.\n\n\n\n  \u00a7.\u00a7.\u00a7 Contrastive Learning Loss\n\n\n\nIn this work, we propose domain-based and instance-based contrastive learning (CL) losses for tone mapping.\n\nDomain-based Contrastive Learning Loss\nFor low level vision tasks, CL methods usually assume low-quality inputs as negative samples and high-quality targets as positive samples, and make the output (anchor) to be close to the targets and far away from the inputs <cit.>.\nHowever, the output could easily fall into the third domain: enough far away from the input domain, but not very close to the target domain. To avoid falling into this situation in tone mapping, besides the input HDR, we further select LDR data with poor quality (such as unpleasant brightness and contrast) from the public dataset as another negative domain. The poor LDRs can be regarded as failure cases of tone mapping. We push the output not only far away from the input HDR domain but also far away from the poor LDR domain. In other words, our negative pairs consist of two kinds, i.e., the outputs and the inputs, the outputs and the poorly-exposed LDRs. Our positive pairs are constructed by the outputs and the well-exposed LDRs.\n\nThe other question is finding the latent space for distance measuring. Considering that our discriminator has learned feature representations to distinguish the outputs of our generator and well LDRs, we utilize the discriminator (D) to extract features for CL, other than utilizing a universal VGG network <cit.><cit.>. Specifically, we utilize the features after the last convolution layer of D but before being flattened into a vector. Then we calculate the mean value (\u03bc_i) and the mean contrast (\u03c4_i, which is calculated according to <cit.>) of each channel (C_i) and concatenate them together to build the latent code, denoted as z=[\u03bc_1,\u03bc_2,...\u03bc_m,\u03c4_1,\u03c4_2,...,\u03c4_m], where m is the channel number of the convolution feature.\nWe denote this process as \u03d5. In this way, we construct the latent codes for Y^o, Y^wl (the well exposed LDR data), Y^pl (the poorly exposed LDR data), and Y^h, and they are denoted as z_D^o, z_D^wl, z_D^pl, and z_D^h respectively, where D represents these latent codes are from the discriminator features. Since Y^o and Y^h are different formats of the same content, z_D^h can be viewed as the intra-negative sample of the anchor z_D^o. Similarly, z_D^wl is the inter-positive sample and z_D^pl is the inter-negative sample since they have different contents from the anchor. Therefore, our domain-based contrastive learning loss can be formulated as\n\n    \u2112_CLD   = E[-logs(z^o_D, z^wl_D)/s(z^o_D, z^wl_D)+\u2211_i=1^Ns(z^o_D, z^h_i_D)] \n         + E[-logs(z^o_D, z^wl_D)/s(z^o_D, z^wl_D)+\u2211_i=1^Ns(z^o_D, z^pl_i_D)],\n\nwhere the first (second) term utilizes z_D^h (z_D^pl) as negative samples and N is the negative sample number.\nWe propose to combine the cosine distance and \u2113_1 distance to measure the similarity between the latent codes, i.e.,\n\n    s(u, v)=exp(u^T v/\u03b7+c|u-v|_1),\n\nwhere \u03b7 and c denote weighting parameters to balance the two distances.\n\n\n\n\n\n\n\n\n\n\n    \u2112_D-CL   = E[-logsim(F^O_D, F^WL_D)/sim(F^O_D, F^WL_D)+\u2211_i=1^Nsim(F^O_D, F^H_D)] \n         + E[-logsim(F^O_D, F^WL_D)/sim(F^O_D, F^WL_D)+\u2211_i=1^Nsim(F^O_D, F^BL_D)],\n\nwhere sim(u, v)=exp(u^T v/c+k|u-v|_1), we combine cosine distance and L1 distance to measure the similarity between feature vectors, c and k denote weighting parameters to balance two distances. And we take F^WL_D as positive sets, take F^H_D and F^BL_D as negative sets in the first and second item respectively.\n\n\nInstance-based Contrastive Learning Loss\nBesides contrastive learning with images (videos) from different domains, we further make the generator learn from itself dynamically. We observe that for one batch of images (videos), some samples are tone-mapped well while some are not. Therefore, we propose instance-based contrastive learning to push the output apart from the poorly tone-mapped samples and pull the output close to the well tone-mapped samples. We utilize TMQI <cit.> metric to evaluate the tone-mapped result. For each batch, the image with the highest TMQI score is selected as the positive sample, and the image with the lowest TMQI score is selected as the negative sample. Since the results of one batch have different TMQI scores, their features before the final output layer of the generator also contain many differences. Therefore, we utilize the generator's feature maps before the last convolution to construct the latent codes. The construction process is the same as that in domain-based contrastive learning loss. The latent codes for the inter-positive and inter-negative samples are denoted as z_G^wo and z_G^po, respectively. The latent codes for the whole batch are denoted as z_G^o. In this way, our instance-based contrastive learning loss can be formulated as\n\n\n\n\n\n\n\n    \u2112_CLI   = E[-logs(z^o_G, z^wo_G)/s(z^o_G, z^wo_G)+ s(z^o_G, z^po_G)],\n\nwhere s(u, v) is the same as that defined in Eq. <ref>.\n\n\n\n\n\n\n    \u2112_I-CL   = E[-logsim(F^O_D, F^WO_D)/sim(F^O_D, F^WO_D)+\u2211_i=1^Nsim(F^O_D, F^BO_D)],\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Naturalness Loss\n\nWe further propose the naturalness loss by measuring the naturalness of an image with its mean brightness and mean contrast (which is calculated according to <cit.>). On the one hand, we supervise the naturalness of the output with other well LDR data (namely inter supervision), which have pleasant brightness and contrast. This loss is formulated as\n\n    \u2112_N_1 = \u2211_t=1^T|\u03d5(Y^wl_t)-\u03d5(Y^o_t)|_1,\n\nwhere \u03d5 denotes the operation for calculating naturalness. On the other hand, we observe that for one image, there exist some areas well-tone-mapped. Therefore, we split the output into n\u00d7n (n is set to 2 in our experiment) patches and select the patch with the highest TMQI score as the intra label, denoted as Y^wp_t. This loss is formulated as\n\n    \u2112_N_2 = \u2211_t=1^T|\u03d5(Y^wp_t)-\u03d5(Y^o_t)|_1.\n\nThe two terms together form our naturalness loss. For image TMO, the T in the above two formulas is equal to 1.\n\n\nReal-label Naturalness Loss Well LDR videos has the pleasant brightness and contrast. We take LDR videos as real label to supervise the naturalness of generator's output. Our real-label naturalness loss can be formulated as\n\n    \u2112_R-N = \u2211_t=1^T|\ud835\udca9(Y^WL_t)-\ud835\udca9(Y^O_t)|_1,\n\nwhere \ud835\udca9 denotes the operation for caculating naturalness.\n\nPseudo-label Naturalness Loss For each iteration, in the output of generator, there exists some area enhanced well. We split the output into n\u00d7n patches and select the patch with highest TMQI score as pseudo label Y^P_t. Our pseudo-label naturalness loss can be formulated as\n\n    \u2112_P-N = \u2211_t=1^T|\ud835\udca9(Y^P_t)-\ud835\udca9(Y^O_t)|_1,\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Total Variation Loss\n\nTo remove magnified noise in dark regions, we further apply total variation (TV) loss to the generator's output Y^o_t, denoted as\n\n\n    \u2112_TV = \u2211_t=1^T (|\u2207_xY^o_t|+|\u2207_yY^o_t|)^2,\n\nwhere T is the number of video frames in video TMO, and is equal to 1 in image TMO. \u2207_x and \u2207_y represent the horizontal and vertical gradient operations, respectively.\n\n\nWith the aforementioned loss functions, our full loss function can be denoted as\n\n    \u2112   = \u2112_Struct + \u03bb_1 \u00b7\u2112_DCL-Adv + \u03bb_2 \u00b7\u2112_CLD + \u03bb_3 \u00b7\u2112_CLI\n        + \u03bb_4 \u00b7\u2112_N_1 + \u03bb_5 \u00b7\u2112_N_2 + \u03bb_6 \u00b7\u2112_TV\n\nwhere the weighting parameters (\u03bb_1 ... \u03bb_6) are used to control the ratio of each loss.\n\n\n\n\u00a7 DATASET\n\n\n\n\n \u00a7.\u00a7 Image Tone Mapping Dataset\n\nFollowing <cit.>, we also utilize 1000 HDR images from HDR+ dataset and the well-exposed LDR images from DIV2K dataset <cit.> for image tone mapping training. Since our domain-based CL loss needs poorly-exposed LDR data, we further utilize 1300 images with unpleasant brightness and contrast from <cit.> as poorly-exposed LDR images for training. Follow the settings of <cit.>, we crop and rescale every training image to a size of 256\u00d7256. We evaluate the performance on the HDR Survey <cit.>, HDRI Haven <cit.> and LVZ-HDR dataset <cit.>, which is the same as <cit.>.\n\n\n\n \u00a7.\u00a7 Video Tone Mapping Dataset\n\n\nThere is no available supervised or unsupervised dataset for video tone mapping and there is no perfect solution to generate ground truth LDR videos for HDR inputs. Therefore, we construct the unpaired HDR-LDR video dataset to enable the unsupervised training of our network.\n\nFirst, we construct the unpaired HDR-LDR video dataset with real captured videos. We collect HDR videos from traditional HDR tone mapping works <cit.> and HDR reconstruction works <cit.>. Considering that the HDR videos with large flicker artifacts will affect the evaluation of tone mapping algorithms, we remove them and totally collect 100 HDR videos with high quality. For LDR videos, we select 80 well-exposed LDR videos from DAVIS dataset <cit.> for adversarial learning and constructing positive pairs in contrastive learning. Most of our collected HDR videos are 1280\u00d7720, so that we resize the other videos (1920\u00d71080, 1476\u00d7753) to 1280\u00d7720 to unify the resolution, which will simplify the following patch cropping process. The 100 HDR videos are split into training and validation set (80 videos), and testing set (20 videos).\n\n\n\n\n\nConsidering that both adversarial learning and contrastive learning require a large amount of training data to improve the optimization process, we further construct synthesized HDR and LDR videos. Specifically, we synthesize videos from static images by dynamic random cropping. We utilize 1000 HDR images from HDR+ dataset <cit.>, 780 well LDR images from DIV2K dataset <cit.> and 1300 poorly LDR images with unpleasant brightness and contrast from <cit.>. For one image, We first randomly downsample it with a ratio of \u03b3 and then randomly crop T patches with a resolution of 256\u00d7256 to construct a sequence with T frames. The downsampling ratio \u03b3 ranges from 1 to 2.8. When \u03b3 is small, the probability that the cropped patches have overlapped regions is small. In this case, we simulate videos with large movements and vice versa. The synthesized poorly-LDR videos are used for negative pair construction in CL. The well LDR videos are used for adversarial learning and positive pair construction.\n\nIn summary, our video tone mapping dataset contains 1100 HDR videos, 860 well LDR videos, and 1300 poorly LDR videos. Among them, 20 real captured HDR videos are selected for testing.\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n \u00a7.\u00a7 Training Details\n\nDuring image TMO training, each image is cropped and rescaled to provide two 256\u00d7256 images, which is the same as <cit.>. During video TMO training, we resize the real HDR and LDR videos from 1920\u00d71080 to 455\u00d7256, and randomly crop 256\u00d7256 squences for training. The synthetic HDR and LDR videos are also downsampled and then cropped to 256\u00d7256 squences, which has been described in Section <ref>. Therefore, the training patches has a large dynamic range.\n\n\nThe batch size is set to 8. For video tone mapping, the frame number T is set to 3. We train our generator and discriminator with learning rate 1e-5 and 1.5e-5 respectively, and they decay by half for every 10 epochs. For each iteration in the training process, we first optimize the discriminator by maximizing Eq. <ref> and then optimize the generator by minimizing Eq. <ref>.\n\nThe sample number N in Eqs. <ref>, <ref> and <ref> is set to 16.\nThe parameters \u03b7 and c in Eq. <ref> are set to 1e-2 and 1. The weighting parameters \u03bb_1 ... \u03bb_6 for different loss functions are dynamically updated. At the early stage (smaller than 7 epochs), we optimize the network mainly by adversarial learning and contrastive learning. Therefore, \u03bb_1 ... \u03bb_6 are set to 1, 0.5, 0.1, 0.001, 0.001, 0.001, respectively. At the middle stage (7-10 epochs), we magnify the weight \u03bb_4 (by setting it to 0.5), which forces the network to generate better brightness and contrast. At the later stage (10-20 epochs), we further force the network to learn from itself and \u03bb_5, \u03bb_6 are set to 0.5 and 0.2, respectively.\n\n\n\n \u00a7.\u00a7 Ablation Study\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we perform ablation study to demonstrate the effectiveness of the proposed loss functions and TFR, SFE modules. To explore the influence of each component for the tone mapping and temporal consistence, we perform ablation study on our video TMO test data. We utilize two measurements to evaluate the tone-mapping quality. The first is the widely used TMQI <cit.> score. The second is warping error (WE) <cit.>, which is widely used to evaluate the temporal consistence of processed frames. However, it tends to reward the tone-mapping methods that generate dark results which are not the aim of the tone-mapping task. Therefore, we propose the relative warping error (RWE) measurement by dividing the warping error by the mean intensity, which is formulated as\n\n    Y\u0305^o_t   = \u03c8(Y^o_t,Y^o_t-1) \n    \n    RWE   = 2/HW\u2211_i=1^H \u2211_j=1^W |Y^o_t-1(i,j)-Y\u0305^o_t(i,j)|/Y^o_t-1(i,j)+Y\u0305^o_t(i,j),\n\nwhere \u03c8(x,y) denotes warping x towards y, and Y\u0305^o_t is the warping result. H and W denote the height and width of Y^o_t. In this work, we utilize deepflow <cit.> to serve as the warping operation.  For each video, we select the first six frames and calculate the mean TMQI and RWE scores for all the testing videos. The results are given in  Table <ref>. For the  baseline model (\u2112_Struct), we utilize structure consistency loss and GAN loss. It can be observed that the TMQI score is consistently improved by introducing the proposed loss functions. After introducing the proposed contrastive loss, the RWE score is increased since more details are revealed. Fortunatelly, after introducing the TV loss, the RWE score is reduced and TMQI score is also increased slightly. When the TFR module is removed , the TMQI score is degraded and the RWE value is increased from 4.38 to 4.92. It verifies that the propsoed TFR module is beneficial for temporal consistency. In addition, the proposed SFE module can improve the TMQI score since it encourages information exchange in non-local regions.\n\n\n\nWe further give the ablation for the detailed settings of our contrastive learning (CL) loss and naturalness loss.\n\n1) Negtative pair ablation. CL methods for low-level vision usually assume low-quality inputs as negative samples and high-quality targets as positive sample <cit.>. Different from them, we utilize extra poorly-exposed LDRs as negative samples to calculate our domain-based contrastive learning loss, which has been formulated as Eq. <ref>. The first and the second term utilizes input HDR z_D^h and poorly-exposed LDRs z_D^pl as negative samples, respectively. To analyze the influence of poorly-LDR negative samples, we remove the second term, and the results are given in the first row of Table <ref>. When the poorly-LDR negative samples are removed, the TMQI score is decreased and the RWE value is increased, which demonstrates that the introduced poorly-LDR samples are important to the success of contrastive learning.\n\n2) Latent code ablation. In our CL loss, our latent code is built by the the mean value (\u03bc_i) and the mean contrast (\u03c4_i) of each feature channel (C_i). Here, we evaluate it by replacing it by an intuitive setting. Specifically, we replace our latent code with global averaging of feature maps. It can be observed that the TMQI score is degraded by replacing our latent code with the intuitive one, as shown in Table <ref>.\n\n3) Distance measurement ablation. We propose a new distance measurement to calculate the similarity between latent codes, which has been formulated as Eq. <ref>. To demonstrate the effectiveness of the distance measurement, we replace it with cosine distance. As shown in Table <ref>, our proposed distance measurement is much better than cosine distance.\n\n\nand list the quantitative comparison results on our HDR video test set in Table <ref>.  The averaged TMQI scores of all frames are listed in the second column of the Table <ref>. Warping Error (WE) <cit.> is a common metric to evaluate the temporal stability of models, but not suitable for tone-mapping results with different brightness. For example, Warping Error will reduce when multiply the results with a darkness factor. We propose the Relative Warping Error (RWE) to evaluate the temporal stability of HDR video tone mapping results. We calculate RWE by dividing WE by the image pixel, which can be formulated as\n\n    Y\u0305^O_t = Warp(Y^O_t,Y^O_t-1) \n    \n       RWE = 1/H\u00d7 W\u2211_i=1^H \u2211_j=1^W |Y^O_t-1-Y\u0305^O_t|_1/Y^O_t-1+Y\u0305^O_t,\n\nwhere Y\u0305^O_t denotes warping current frame Y^O_t to previous frame Y^O_t-1, H and W denote the height and width of images.\n\n\nFig. <ref> presents the visual comparison results for the ablation study. It can be observed that introducing contrastive loss can improve the contrast for both dark and over-exposed areas. It is mainly due to that our domain-based contrastive learning  can push the output away from the poorly LDR videos which contain lots of under-exposed and over-exposed regions. Our instance-based contrastive learning can further help the network learn form itself to generate better results. By introducing the naturalness loss, the dark areas can be further lightened.\n\n\n\n\n \u00a7.\u00a7 Comparison for HDR Image Tone Mapping\n\n\n\n\n\n\n\n\nTo demonstrate the effectiveness of the proposed TMO for image tone mapping, we compare with state-of-the-art image tone mapping methods on HDR Survey dataset <cit.>, HDRI Haven dataset <cit.> and LVZ-HDR dataset <cit.>, respectively. Since TMOCAN<cit.> did not release the pretrained model, we retrain it on the HDR+ dataset for fair comparison. The results are shown in Tables <ref>, <ref>, and <ref>, respectively. Besides TMQI, we further utilize blind TMQI (BTMQI) <cit.> to evaluate the tone mapping quality. It can be observed that our method achieves the best TMQI and BTMQI scores in all three image TMO datasets. For HDR Survey dataset, our method outperforms the second best method UnpairedTMO by 0.013 and 0.06 gain for TMQI and BTMQI scores. For HDR Haven dataset, our method outperforms the second best method by 0.014 and 0.11 gain for TMQI and BTMQI scores. For LVZ-HDR dataset, our method outperforms the second best method by 0.016 and 0.131 gain for TMQI and BTMQI scores.\n\nFig. <ref> present the visual comparison results on the three image TMO datasets respectively. Among them, DeepTMO <cit.> sometimes generates results with false colors, such as the bluish tree in the first row of Fig. <ref>. The work in <cit.> generates over-enhanced results which have unnatural contrast and textures. The works in <cit.> and <cit.> sometimes generate under-exposed results, especially on several scenes in the HDRI Haven dataset. The work in <cit.> usually generates over-exposed results, as shown in Fig. <ref> (c). UnpairedTMO <cit.> can generate better results, but there usually exists under-exposed and over-exposed areas. Compared with these methods, our method has the best visual quality with suitable brightness and contrast.\n\n\n\n\n\n \u00a7.\u00a7 Comparison for HDR Video Tone Mapping\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we further demonstrate the effectiveness of the proposed TMO for video tone mapping. Since there is no DNN-based video TMOs, we compare with state-of-the-art traditional image <cit.> and video tone mapping methods <cit.>, DNN-based image tone mapping methods <cit.> and image enhancement methods <cit.> on our proposed HDR video test set. Among them, the works in <cit.> only utilize HDR data for training, namely zero-reference based solution. The works in <cit.> and our method utilize unpaired HDR-LDR data for unsupervised learning. For fair comparison, we retrain all the DNN-based methods on our dataset (due to this reason, many tone-mapping methods without codes cannot be compared). Table <ref> lists the quantitative results. It can be observed that our proposed method achieves the best TMQI and BTMQI scores.\nFor TMQI, our method outperforms the second best method UnpairedTMO <cit.> by 0.013 and meanwhile our RWE value is smaller than that of <cit.>. It demonstrates that our method is good at revealing details and keeping temporal consistency.  Compared with image enhancement methods, our method achieves nearly 0.15 and 2 gain for TMQI and BTMQI scores, respectively. This verifies that the enhancement methods are not suitable for tone mapping. Compared with traditional tone mapping methods, our method still achieves obvious gains in terms of TMQI and BTMQI scores. Note that the video tone mapping method <cit.> achieves the best RWE value but its TMQI and BTMQI scores are much worse than other methods. It implies that the lower RWE value of <cit.> may be caused by results with low contrast.\n\n\n\nFig. <ref> presents the visual comparison results on four scenes in our test set. Since the retrained image enhancement methods have uncompetitive results and the page space is limited, we only show the visual comparison results with the video tone mapping method (a) <cit.> and two DNN based tone-mapping methods (b) TMOCAN <cit.>, and (c) UnpairedTMO <cit.>. Full comparisons are provided in our supplementary file.  It can be observed that our method has the best visual quality with suitable brightness and contrast.  The video tone mapping method <cit.> generates darker results than other methods, but the lantern in the third row is over-exposed. <cit.> generates over-enhanced results which have unnatural contrast and textures. For example, for the fourth image, the buildings are under-enhanced and the clouds are over-enhanced. UnpairedTMO <cit.> can generate better results, but the wall and curtain in the first and third scenes are under-exposed.  In contrast, our results have pleasant brightness and contrast in all areas.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Computing Complexity\n\nIn this section, we further give the computing complexity comparison in terms of multiply-add operations (MACs), as shown in Table <ref>. For our method, we give three realizations with different channel numbers. Specifically, we reduce the channel number of IVTMNet_full to construct IVTMNet_0.75 and IVTMNet_0.5, whose channel numbers are 0.75 and half of our full solution, respectively. It can be observed that IVTMNet_full, IVTMNet_0.75 and IVTMNet_0.5 are the top three models in terms of TMQI and BTMQI scores. IVTMNet_0.5 has the lowest MACs and its RWE value is slightly worse than that of IVTMNet_full. IVTMNet_0.75 achieves a good balance between tone mapping performance and computing complexity.\n\n\n\n\u00a7 CONCLUSION\n\n\nIn this paper, we explore unsupervised image and video tone mapping by proposing an effective IVTMNet and a set of unsupervised loss functions. It is convenient to switch between image and video TMO by introducing TFR module, which can improve the temporal consistency of video tone-mapped results. To improve unsupervised training, we propose domain and instance based contrastive learning loss and propose a new latent space in which to measure the similarities in negative and positive pairs. Experimental results on image and video tone mapping dataset demonstrate the superiority of the proposed method in enhancing brightness and contrast and keeping temporal consistency of tone-mapped results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIEEEtran\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}