{"entry_id": "http://arxiv.org/abs/2303.06919v2", "published": "20230313083630", "title": "NeRFLiX: High-Quality Neural View Synthesis by Learning a Degradation-Driven Inter-viewpoint MiXer", "authors": ["Kun Zhou", "Wenbo Li", "Yi Wang", "Tao Hu", "Nianjuan Jiang", "Xiaoguang Han", "Jiangbo Lu"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\tNeRFLiX: High-Quality Neural View Synthesis \nby Learning a Degradation-Driven Inter-viewpoint MiXer\n\t\n \n    \n\tKun Zhou1,21   Wenbo Li3Equal contribution    Yi Wang4 \n\n\tTao Hu3   Nianjuan Jiang2   Xiaoguang Han1    Jiangbo Lu2Corresponding author  \n\n\t^1SSE, CUHK-Shenzhen,   ^2SmartMore Corporation \n\t  ^3CUHK   ^4Shanghai AI Laboratory\n\n\tkunzhou@link.cuhk.edu.cn,  {wenboli,taohu}@cse.cuhk.edu.hk\n\n\t hanxiaoguang@cuhk.edu.cn,{jiangbo.lu,wygamle}@gmail.com \n\n\t\n\n\n    March 30, 2023\n=====================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\t\n    < g r a p h i c s >\n\n\t\tfigureWe propose NeRFLiX, a general NeRF-agnostic restorer that is capable of improving neural view synthesis quality. The first example is from Tanks and Temples\u00a0<cit.>, the second/third examples are from LLFF\u00a0<cit.>, and the last one is a user scene captured by a mobile phone. RegNeRF-V3\u00a0<cit.> means the model trained with three input views.\n\t\t\n\t\n\t\n\n\t\n\t\n\t\tNeural radiance fields\u00a0(NeRF) show great success in novel view synthesis. However, in real-world scenes, recovering high-quality details from the source images is still challenging for the existing NeRF-based approaches, due to the potential imperfect calibration information and scene representation inaccuracy. Even with high-quality training frames, the synthetic novel views produced by NeRF models still suffer from notable rendering artifacts, such as noise, blur, etc. Towards to improve the synthesis quality of NeRF-based approaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by learning a degradation-driven inter-viewpoint mixer. Specially, we design a NeRF-style degradation modeling approach and construct large-scale training data, enabling the possibility of effectively removing NeRF-native rendering artifacts for existing deep neural networks. Moreover, beyond the degradation removal, we propose an inter-viewpoint aggregation framework that is able to fuse highly related high-quality training images, pushing the performance of cutting-edge NeRF models to entirely new levels and producing highly photo-realistic synthetic views. Our project page is available at <https://redrock303.github.io/nerflix/>.\n\t\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n\t\n\t\n\n\u00a7 INTRODUCTION\n\n\t\n\t\n\t\n\tNeural radiance fields\u00a0(NeRF) can generate photo-realistic images from new viewpoints, playing a heated role in novel view synthesis. \n\tIn light of NeRF's\u00a0<cit.> success, numerous approaches\u00a0<cit.> along these lines have been proposed, continually raising the performance to greater levels. In fact,\n\tone prerequisite of NeRF is the precise camera settings of the taken photos for training \u00a0<cit.>. However, accurately calibrating camera poses is exceedingly difficult in practice. Contrarily, the shape-radiance co-adaption issue\u00a0<cit.> reveals that while the learned radiance fields can perfectly explain the training views with inaccurate geometry, they poorly generalize to unseen views. On the other hand, the capacity to represent sophisticated geometry, lighting, object materials, and other factors is constrained by the simplified scene representation of NeRF\u00a0<cit.>.\n\tOn the basis of such restrictions, advanced NeRF models may nonetheless result in notable artifacts (such as blur, noise, detail missing, and more), which we refer to as NeRF-style degradations in this article and are shown in Fig.\u00a0<ref>. \n\t\n\tTo address the aforementioned limitations, numerous works have been proposed. For example, some studies, including\u00a0<cit.>, jointly optimize camera parameters and neural radiance fields to refine camera poses as precisely as possible in order to address the camera calibration issue. Another line of works\u00a0<cit.> presents physical-aware models that simultaneously take into account the object materials and environment lighting, as opposed to using MLPs or neural voxels to implicitly encode both the geometry and appearance. \n\tTo meet the demands for high-quality neural view synthesis, one has to carefully examine all of the elements when building complex inverse rendering systems. In addition to being challenging to optimize, they are also not scalable for rapid deployment with hard re-configurations in new environments. Regardless of the intricate physical-aware rendering models, is it possible to design a practical NeRF-agnostic restorer to directly enhance synthesized views from NeRFs?\n\t\n\tIn the low-level vision, it is critical to construct large-scale paired data to train a deep restorer for eliminating real-world artifacts\u00a0<cit.>.  When it comes to NeRF-style degradations, there are two challenges: (1) sizable paired training data; (2) NeRF degradation analysis. First, it is unpractical to gather large-scale training pairs (more specifically, raw outputs from well-trained NeRFs and corresponding ground truths). Second, the modeling of NeRF-style degradation has received little attention. Unlike real-world images that generally suffer from JPEG compression, sensor noise, and motion blur, the NeRF-style artifacts are complex and differ from the existing ones. As far as we know, no previous studies have ever investigated NeRF-style degradation removal which effectively leverages the ample research on image and video restoration.\n\t\n\t\n\t\n\t\n\tIn this work, we are motivated to have the first study on the feasibility of simulating large-scale NeRF-style paired data, opening the possibility of training a NeRF-agnostic restorer for improving the NeRF rendering frames.\n\tTo this end, we present a novel degradation simulator for typical NeRF-style artifacts (e.g., rendering noise and blur) considering the NeRF mechanism.  We review the overall NeRF rendering pipeline and discuss the typical NeRF-style degradation cases. Accordingly, we present three basic degradation types to simulate the real rendered artifacts of NeRF synthetic views and empirically evaluate the distribution similarity between real rendered photos and our simulated ones. The feasibility of developing NeRF-agnostic restoration models has been made possible by constructing a sizable dataset that covers a variety of NeRF-style degradations, over different scenes. \n\t\n\t\n\t\n\t\n\tNext, we show the necessity of our simulated dataset and demonstrate that existing state-of-the-art image restoration frameworks can be used to eliminate NeRF visual artifacts. Furthermore,\n\twe notice, in a typical NeRF setup, neighboring high-quality views come for free, and they serve as potential reference bases for video-based restoration with a multi-frame aggregation and fusion module.\n\tHowever, this is not straightforward because NeRF input views are taken from a variety of very different angles and locations, making the estimation of correspondence quite challenging. To tackle this problem, we propose a degradation-driven inter-viewpoint \u201cmixer\" that progressively aligns image contents at the pixel and patch levels. In order to maximize efficiency and improve performance, we also propose a fast view selection technique to only choose the most pertinent reference training views for aggregation, as opposed to using the entire NeRF input views.\n\t\n\t\n\t\n\t\n\tIn a nutshell, we present a NeRF-agnostic restorer (termed NeRFLiX) which learns a degradation-driven inter-viewpoint mixer. As illustrated in Fig.\u00a0<ref>, given NeRF synthetic frames with various rendering degradations, NeRFLiX successfully restores high-quality results. Our contributions are summarized as\n\t\n\t\n\t\t\n\t\t\n  * Universal enhancer for NeRF models. NeRFLiX is powerful and adaptable, removing NeRF artifacts and restoring clearly details,  pushing the performance of cutting-edge NeRF models to entirely new levels.\n\t\t\n\t\t\n  * NeRF rendering degradation simulator. We develop a NeRF-style degradation simulator\u00a0(NDS), constructing massive amounts of paired data and aiding the training of deep neural networks to improve the quality of NeRF-rendered images.\n\t\t\n  * Inter-viewpoint mixer. Based on our constructed NDS, we further propose an inter-viewpoint baseline that is able to mix high-quality neighboring views for more effective restorations.\n\t\t\n\t\t\n  * Training time acceleration. We show how NeRFLiX makes it possible for NeRF models to produce even better results with a 50% reduction in training time.\n\t\n\t\n\t\n\t\n\n\u00a7 RELATED WORKS\n\n\t\n\t\n\n\tNeRF-based novel view synthesis.\n\tNeRF-based novel view synthesis has received a lot of attention recently and has been thoroughly investigated. For the first time, Mildenhall\u00a0\u00a0<cit.> propose the neural radiance field to implicitly represent static 3D scenes and synthesize novel views from multiple posed images. Inspired by their successes, a lot of NeRF-based models\u00a0<cit.> have been proposed. For example, point-NeRF\u00a0<cit.> and DS-NeRF\u00a0<cit.> incorporate sparse 3D point cloud and depth information for eliminating the geometry ambiguity for NeRFs, achieving more accurate/efficient 3D point sampling and better rendering quality. Plenoxels\u00a0<cit.>, TensoRF\u00a0<cit.>, DirectVoxGo\u00a0<cit.>, FastNeRF\u00a0<cit.>, Plenoctrees\u00a0<cit.>, KiloNeRF\u00a0<cit.>,  and Mobilenerf\u00a0<cit.>,  aim to use various advanced technologies to speed up the training or inference phases. Though these methods have achieved great progress, due to the potential issue of inaccurate camera poses, simplified pinhole camera models as well as scene representation inaccuracy, they still suffer from rendering artifacts of the predicted novel views. \n\t\n\t\n\n\tDegradation simulation.\n\tSince no existing works have explored the NeRF-style degradation cases, we will overview the real-world image restoration works that are most related to ours. The previous image/video super-resolution approaches\u00a0<cit.> typically follow a fix image degradation type\u00a0(e.g., blur, bicubic/bilinear down-sampling). Due to the large domain shift between the real-world and simulated degradations, the earlier image restoration methods\u00a0<cit.> generally fail to remove complex artifacts of the real-world images. In contrast, BSRGAN\u00a0<cit.> design a practical degradation approach for real-world image super-resolution. In their degradation process, multiple degradations are considered and applied in random orders, largely covering the diversity of real-world degradations. Compared with the previous works, BSRGAN achieves much better results quantitatively and qualitatively. Real-ESRGAN\u00a0<cit.> develops a second-order degradation process for real-world image super-resolution. In this work, we propose a NeRF-style degradation simulator and construct a large-scale training dataset for modeling the NeRF rendering artifacts.\n\t\n\t\n\n\tCorrespondence estimation.\n\tIn the existing literature, the video restoration methods\u00a0<cit.> aim to restore a high-quality frame from multiple low-quality frames. To achieve this goal, cross-frame correspondence estimation is essential to effectively aggregate the informative temporal contents. Some works\u00a0<cit.> explore building pixel-level correspondences through optical-flow estimation and perform frame-warping for multi-frame compensation. Another line of works\u00a0<cit.> tries to use deformable convolution networks\u00a0(DCNs\u00a0<cit.>) for adaptive correspondence estimation and aggregation. More recently, transformer-based video restoration models\u00a0<cit.> implement spatial-temporal aggregation through an attention mechanism and achieve promising performance. However, it is still challenging to perform accurate correspondence estimation between frames captured with very distinctive viewpoints.\n\t\n\t\n\t\n\t\n\t\n\n\u00a7 PRELIMINARIES\n\n\t \n\tIn this section, we review the general pipeline of NeRF-based novel view synthesis and discuss potential rendering artifacts. As shown in Fig.\u00a0<ref>, three main steps are involved in the rendering: \n\t\n\t(1) Ray Shooting.\n\tTo render the color of a target pixel in a particular view, NeRF utilizes the camera's calibrated parameters\u00a0\u03c0 to generate a ray \ud835\udc2b(\ud835\udc28,\ud835\udc1d) through this pixel, where \ud835\udc28,\ud835\udc1d are the camera center and the ray direction.\n\t(2) Ray Marching. \n\tA set of 3D points are sampled along the chosen ray as it moves across the 3D scene represented by neural radiance fields. The NeRF models encode a 3D scene and predict the colors and densities of these points.\n\t(3) Radiance Accumulation.\n\tThe pixel color is extracted by integrating the predicted radiance features of the sampled 3D points.\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n\tDiscussion. We can see that establishing a relationship between 2D photos and the 3D scene requires camera calibration. Unfortunately, it is very challenging to precisely calibrate the camera poses, leading to noisy 3D sampling. Meanwhile, some previous works\u00a0<cit.> also raise other concerns, including the non-linear pinhole camera model\u00a0<cit.> and shape-radiance ambiguity\u00a0<cit.>. Because of these inherent limitations, as discussed in Section\u00a0<ref>, NeRF models still synthesize unsatisfied novel test views.\n\t\n\t\n\t\n\t\n\t \n\t\n\t\n\n\u00a7 METHODOLOGY\n\n\t\n\t\n\t\n\tOverview.\n\tIn this work, we present NeRFLiX, a general NeRF-agnostic restorer which employs a degradation-driven inter-viewpoint mixer to enhance novel view images rendered by NeRF models.\n\tIt is made up of two essential components: a NeRF-style degradation simulator\u00a0(NDS) and an inter-viewpoint mixer\u00a0(IVM). As seen in Fig.\u00a0<ref>(a), during the training phase, we employ the proposed NDS to create large-scale paired training data, which are subsequently used to train an IVM for improving a NeRF-rendered view using two corresponding reference pictures (reference views). In the inference stage, as illustrated in Fig.\u00a0<ref>(b), IVM is adopted to enhance a rendered view by fusing useful information from the selected most relevant reference views. \n\t\n\t\n\t\n\n\t\n\n \u00a7.\u00a7 NeRF-Style Degradation Simulator\u00a0(NDS)\n\n\t\n\t\n\t\n\t\n\tDue to the difficulties in gathering well-posed scenes under various environments and training NeRF models for each scene, it is infeasible\u00a0to directly collect large amounts of paired NeRF data for artifact removal.\n\tTo address this challenge, motivated by BSRGAN\u00a0<cit.>,\n\twe design a general NeRF degradation simulator to produce a sizable training dataset that is visually and statistically comparable to NeRF-rendered images\u00a0(views).\n\t\n\tTo begin with, we collect raw data from LLFF-T[the training parts of LLFF\u00a0<cit.>.] and Vimeo90K<cit.> where the adjacent frames are treated as raw sequences. Each raw sequence consists of three images {I^gt,I_1^r,I_2^r}: a target view I^gt and its two reference views {I_1^r,I_2^r}. To construct the paired data from a raw sequence, we use the proposed NDS to degrade I^gt and obtain a simulated degraded view I, as shown in Fig.\u00a0<ref>(a). \n\t\n\t\n\t\n\t\n\tThe degradation pipeline is illustrated in Fig\u00a0<ref>. We design three types of degradation for a target view I^gt: splatted Gaussian noise\u00a0(SGN), re-positioning\u00a0(Re-Pos.), and anisotropic blur\u00a0(A-Blur). It should be noted that there may be other models for such a simulation, and we only utilize this route to evaluate and justify the feasibility of our idea.\n\t\n\t\n\n\tSplatted Gaussian noise.\n\tAlthough additive Gaussian noise is frequently employed in image/video denoising, NeRF rendering noise clearly differs. Rays that hit a 3D point will be re-projected within a nearby 2D area because of noisy camera parameters. As a result, the NeRF-style noise is dispersed over a 2D space. This observation led us to present a splatted Gaussian noise, which is defined as\n\t\n    I^D1 = (I^gt  + n) \u229b g ,\n\n\twhere n is a 2D Gaussian noise map with the same resolution as I^gt and g is an isotropic Gaussian blur kernel.\n\t \n\t\n\t\n\n\tRe-positioning.\n\tWe design a re-positioning degradation to simulate ray jittering. We add a random 2D offset \u03b4_i,\u03b4_j \u2208 [-2,2] with probability 0.1 for a pixel at location (i,j)\n\t\n\n\t\n    I^D2(i,j) = \n    \t\t\n    \t\t\tI^D1(i,j)    if\u00a0\u00a0 p>0.1 \n    \n    \t\t\tI^D1(i+ \u03b4_i,j+\u03b4_j)    else\u00a0\u00a0 p\u22640.1\n\n\twhere p is uniformly distributed in [0, 1].\n\t\n\t\n\t\n\t\n\t\n\t\n\n\tAnisotropic blur.\n\tAdditionally, from our observation, NeRF synthetic frames also contain blurry\u00a0contents. To simulate blur patterns, we use anisotropic Gaussian kernels to blur the target frame.\n\t\n\t\n\t\n\t \n\t\n\t\n\t\n\n\tRegion adaptive strategy.\n\tNeural radiance fields are often supervised with unbalanced training views. As a result, given a novel view, the projected 2D areas have varying degradation levels. Thus, we carry out each of the employed degradations in a spatially variant manner. More specifically, we define a mask M as a two-dimensional oriented anisotropic Gaussian\u00a0<cit.>\n\t\n    M(i,j)= G(i-c_i, j-c_j; \u03c3_i, \u03c3_j, A),\n \n\twhere (c_i,c_j),(\u03c3_i, \u03c3_j) are the means and standard deviations and A is an orientation angle. After that, we use the mask M to linearly blend the input and output of each degradation, finally achieving region-adaptive degradations. As shown in Fig.\u00a0<ref>, our simulated rendered views visually match the real NeRF-rendered ones. All the detailed settings of NDS are elaborated in our supplementary materials.\n\t\n\tAt last, with our NDS, we can obtain a great number of training pairs, and each paired data consists of two high-quality reference views {I_1^r,I_2^r}, a simulated degraded view I, and the corresponding target view I^gt. Next, we show how the constructed paired data {I,I_1^r,I_2^r|I^gt} can be used to train our IVM.\n\t\n\t\n\t\n\t\n\t\n\t\n\t \n\t\n\t\n\n \u00a7.\u00a7 Inter-viewpoint Mixer\u00a0(IVM)\n\n\t\n\t\n\t\n\tProblem formulation. Given a degraded view I produced by our NDS or NeRF models, we aim to extract useful information from its two high-quality reference views {I_1^r,I_2^r} and restore an enhanced version \u00ce.\n\t\n\t\n\t\n\t\n\n\tIVM architecture.\n\tFor multi-frame processing, existing techniques either use optical flow\u00a0<cit.> or deformable convolutions\u00a0<cit.> to realize the correspondence estimation and aggregation for consistent displacements. In contrast, NeRF rendered and input views come from very different angles and locations, making it challenging to perform precise inter-viewpoint aggregation.\n\t\n\t\n\t\n\tTo address this problem, we propose IVM, a hybrid recurrent inter-viewpoint \u201cmixer\" that progressively fuses pixel-wise and patch-wise contents from two high-quality reference views, achieving more effective inter-viewpoint aggregation. There are three modules i.e., feature extraction, hybrid inter-viewpoint aggregation and reconstruction, as shown in Fig.\u00a0<ref>. Two convolutional encoders are used in the feature extraction stage to process the degraded view I and two high-quality reference views {I_1^r,I_2^r}, respectively. We then use inter-viewpoint window-based attention modules and deformable convolutions to achieve recurrent patch-wise and pixel-wise\u00a0aggregation. Finally, the enhanced view \u00ce is generated using the reconstruction module under the supervision\n\t\n\n\t\n    Loss = |\u00ce - I^gt|, where  \u00ce = f(I,I_1^r,I_2^r;\u03b8),\n\n\twhere \u03b8 is the learnable parameters of IVM. The framework architecture is given in our supplementary materials.\n\t\n\t\n\t\n\t\n\n \u00a7.\u00a7 View Selection\n\n\t \n\t\n\tIn the inference stage, for a NeRF-rendered view I, our IVM produces an enhanced version by aggregating contents from two neighboring high-quality views. But, multiple input views are available and only a part of them are largely overlapped with I. In general, only the most pertinent input views are useful for the inter-viewpoint aggregation.\n\t\n\tTo this end, we develop a view selection strategy to choose two reference views {I_1^r,I_2^r} from the input views that are most overlapped with the rendered view I. Specifically, we formulate the view selection problem based on the pinhole camera model. An arbitrary 3D scene can be roughly approximated as a bounding sphere in Fig.\u00a0<ref>, and cameras are placed around it to take pictures. When camera-emitted rays hit the sphere, there are a set of intersections. We refer to the 3D point sets as \u03a6_i={p_0^i,p_1^i,\u22ef,p_M_i^i} and \u03a6_j={p_0^j,p_1^j,\u22ef,p_M_j^j} for the i-th and j-th cameras. For m_i-th intersection p_m_i^i \u2208\u03a6_i of view i, we search its nearest point in view j with the L2 distance\n\t\n    p_m_i^i \u2192 j = min_p \u2208\u03a6_j ( || p - p_m_i^i||_2^2 ).\n\n\tThen the matching cost from the i-th view to the j-th view is calculated by\n\t\n\n\t\n    C_i \u2192 j = \u2211_m_i=0^M_i|| p_m_i^i - p_m_i^i \u2192 j||_2^2 .\n\n\tWe finally obtain the mutual matching cost between views i and j as\n\t\n    C_i \u2194 j = C_i \u2192 j + C_j \u2192 i.\n\n\tIn this regard, two reference views {I_1^r,I_2^r} are selected at the least mutual matching costs for enhancing the NeRF-rendered view I. Note that we also adopt this strategy to decide the two reference views for the LLFF-T\u00a0<cit.> data during the training phase.\n\t\n\t\n\t\n\n\u00a7 EXPERIMENTS\n\n\t\n\t\n\t\n\n \u00a7.\u00a7 Implementation Details\n\n\t\n\tWe train the IVM for 300K iterations. The batch size is 16 and the patch size is 128. We adopt random cropping, vertical or horizontal flipping, and rotation augmentations. Apart from the inherent viewpoint changes over {I,I_1^r,I_2^r}, random offsets\u00a0(\u00b15 pixels) are globally applied to the two reference views\u00a0(I_1^r,I_2^r) to model more complex motion. We adopt an Adam\u00a0<cit.> optimizer and a Cosine annealing strategy to decay the learning rate from 5 \u00d7 10^-4 to 0. We train a single IVM on the LLFF-T and Vimeo datasets and test it on all benchmarks\u00a0(including user-captured scenes).\n\t\n\t\n\n \u00a7.\u00a7  Datasets and Metrics\n \n\tWe conduct the experiments on three widely used datasets, including LLFF\u00a0<cit.>, Tanks and Temples\u00a0<cit.>, and Noisy LLFF Synthetic.\n\t\n\t\n\t\n\n\tLLFF\u00a0<cit.>. LLFF is a real-world dataset, where 8 different scenes have 20 to 62 images. Following the commonly used protocols\u00a0<cit.>, we adopt 1008\u00d7 756 resolution for LLFF-P1 and 504\u00d7 376 resolution for LLFF-P2.\n\t\n\t\n \n\tTanks and Temples\u00a0<cit.>.\n\t\n\t\n\t\n\tIt contains 5 scenes captured by inward-facing cameras. There are 152-384 images in the 1920\u00d7 1080 resolution. It should be noted that the viewpoints of different frames are significantly larger than LLFF.\n\t\n\t\n\n\tNoisy LLFF Synthetic\u00a0<cit.>.\n\t\n\tThere are 8 virtual scenes, each of which has 400 images with a size of 800 \u00d7 800. To simulate noisy in-the-wild calibration, we ad hoc apply camera jittering\u00a0(random rotation and translation are employed) to the precise camera poses.\n\t\n\t\n\t\n\t\n\t\n\t\n\n\tMetrics.\n\tFollowing previous NeRF methods, we adopt PSNR\u00a0(\u2191)/SSIM\u00a0<cit.>\u00a0(\u2191)/LPIPS\u00a0<cit.>(\u2193) for evaluation.\n\t\n\t\n\t\n\t\n\t \n\t\n\n \u00a7.\u00a7 Improvement over SOTA NeRF Models\n\n\t\n\tWe demonstrate the effectiveness of our approach by showing that it consistently improves the performance of cutting-edge NeRF approaches across various datasets.\n\t\n\t\n\t\n\t\n\t\n\n\tLLFF. In order to fully verify the generalization ability of our NeRFLiX, we investigate six representative models, including NeRF\u00a0<cit.>, TensoRF\u00a0<cit.>, Plenoxels\u00a0<cit.>, NeRF-mm\u00a0<cit.>, NLF\u00a0<cit.>, and RegNeRF\u00a0<cit.>. Using rendered images of NeRF models as inputs to our model, we aim to further improve the synthesis quality. The quantitative results are provided in Table\u00a0<ref>. We find that under both of the two protocols, our method raises NeRF model performance entirely to new levels. For example, NeRFLiX improves Plenoxels\u00a0<cit.> by 0.61dB/0.025/0.054 in terms of PSNR/SSIM/LPIPS.\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n\tTanks and Temples. Due to large variations of camera viewpoints, even advanced NeRF models, e.g., TensoRF\u00a0<cit.> and DIVeR\u00a0<cit.>, show obviously inferior rendering quality on this dataset. As illustrated in Table\u00a0<ref>, we show that our NeRFLiX can still boost the performance of these models by a large margin, especially TensoRF\u00a0<cit.> achieves 0.51dB/0.01/0.022 improvements on PSNR/SSIM/LPIPS. \n\t\n\t\n\t\n\t\n\t\n\t\n\n\tNoisy LLFF Synthetic. Apart from in-the-wild benchmarks above, we also demonstrate the enhancement capability of our model on noisy LLFF Synthetic. From the results shown in Table\u00a0<ref>, we see that our NeRFLiX yields substantial improvements for two SOTA NeRF models.\n\t\n\t\n\t\n\t\n\t\n\t\n\n\tQualitative results. In Fig.\u00a0<ref>, we provide some visual examples for qualitative assessment. It is obvious that our NeRFLiX restores clearer image details while removing the majority of NeRF-style artifacts in the rendered images, clearly manifesting the effectiveness of our method. More results are provided in our supplementary materials.\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n \u00a7.\u00a7 Training Acceleration for NeRF Models\n\n\t\n\tIn this section, we show how our approach makes it possible for NeRF models to produce better results even with a 50% reduction in training time. To be more precise, we use NeRFLiX to improve the rendered images of two SOTA NeRF models after training them with half the training period specified in the publications. The enhanced results outperform the counterparts with full-time training, as shown in Table\u00a0<ref>.  Notably, NeRFLiX has reduced the training period for Plenoxels\u00a0<cit.> from 24 minutes to 10 minutes while also improving the quality of the rendered images.\n\t\n\t\n\n \u00a7.\u00a7 Ablation Study\n\n\t\n\tIn this section, we conduct comprehensive experiments on LLFF\u00a0<cit.> under the LLFF-P1 protocol to analyze each of our designs. We use TensoRF\u00a0<cit.> as our baseline[The TensoRF results\u00a0(26.70dB/0.838/0.204) we examined differ slightly from the published results\u00a0(26.73dB/0.839/0.204).].\n\t\n\n\t\n\n  \u00a7.\u00a7.\u00a7 NeRF-Style Degradation Simulator\n\n\t\n\t \n\t\n\t\n\n\tSimulation quality. \n\t\n\tWe first examine the simulation quality of the proposed NeRF-style degradation simulator. To this end, we analyze the distribution of our degraded images, BSR\u00a0<cit.> degraded images and NeRF rendered images on LLFF\u00a0<cit.>. We use t-SNE\u00a0<cit.> to visualize deep image features\u00a0(by Inception-v3\u00a0<cit.>) and results are shown in Fig.\u00a0<ref>. Our simulated data is statistically much closer to the real rendered images than BSR. This conclusion is also supported by Table\u00a0<ref>, which demonstrates that our NDS significantly surpasses BSR and yields 0.6-1.0dB improvements when used for learning NeRF degradations.\n\t\n\t\n\t\n\t\n\t\n\t\n\n\tDegradation type.\n\t\n\tWe also evaluate the detailed contribution of each data degradation. We use simulated data to train our models by gradually including four types of degradation, as illustrated in Table\u00a0<ref>. From quantitative comparisons on LLFF\u00a0<cit.>, we observe that all employed degradations are beneficial to our system.\n\t\n\t\n\t\n\n\t \n\n\t\n\n\t\n\n  \u00a7.\u00a7.\u00a7 Inter-viewpoint Mixer\n\n\tView selection strategy.\n\t\n\tWe develop a view selection strategy to make full use\u00a0of high-quality reference views. As\u00a0shown in Fig.\u00a0<ref>, our system can identify the most relevant views for quality enhancement when compared to random selection. Also, the quantitative results in Table \u00a0<ref> suggest that our view selection achieves significantly improved results, illustrating the usefulness of our method.\n\t\n\t\n\t\n\t\n\t\n\n\tHybrid recurrent multi-view aggregation.\n\t\n\t\n\tTo handle large viewpoint differences in reference and rendered views, we develop a hybrid recurrent inter-viewpoint aggregation network. We train models using either pixel-wise or patch-wise aggregation and test different iterations to assess the proposed IVM. Models using a single aggregation approach, as illustrated in Table\u00a0<ref>, perform worse than our full configuration. Additionally, by gradually increasing the iteration numbers from 1 to 3, we achieve improvements of 0.12dB and 0.06dB, albeit at an additional cost of 66 ms and 46 ms for aggregation. Last, compared with the existing state-of-the-art models in Table\u00a0<ref>, thanks to the recurrent hybrid aggregation strategy, our IVM outperforms all of these models in terms of quantitative results, demonstrating the strength of our aggregation design.\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n\tLimitation. Though NeRFLiX achieves promising progress for its universal improvements over existing NeRF models. There are still some future directions that deserve further exploration.\n\t(1) Our NDS is one of many possible solutions for NeRF degradation simulation.\n\t(2) Exploring real-time inter-viewpoint mixers is interesting and useful.\n\t\n\t\n\n\u00a7 CONCLUSION\n\n\tWe presented NeRFLiX, a general NeRF-agnostic restoration paradigm for high-quality neural view synthesis. We systematically analyzed the NeRF rendering pipeline and introduced the concept of NeRF-style degradations. Towards to eliminate NeRF-style artifacts, we presented a novel NeRF-style degradation simulator and constructed a large-scale simulated dataset. Benefiting from our simulated dataset, we demonstrated how SOTA deep neural networks could be trained for NeRF artifact removal. To further restore missing details of NeRF rendered frames, we proposed an inter-viewpoint mixer that is capable of aggregating multi-view frames captured from free viewpoints. Additionally,  we developed a view selection scheme for choosing the most pertinent reference frames, largely alleviating the computing burden while achieving superior results. Extensive experiments have verified the effectiveness of our NeRFLiX. Code will be made publicly available.\n\t\n\t\n\tieee_fullname\n\t\t\n\t\n\n\t\n\t\t\n\n\u00a7 NERF DEGRADATION SIMULATOR\n\n\t\t\n\n\t\tRaw data collection.\n\t\tWe collect raw sequences from Vimeo90K\u00a0<cit.> and LLFF-T\u00a0<cit.>. In total, Vimeo90K contains 64612 7-frame training clips with a 448\u00d7256 resolution. Three frames\u00a0(two reference views and one target view) are selected from a raw sequence of Vimeo90K in a random order. As described in Sec.\u00a05.1, apart from the inherent displacements within the selected views, we add random global offsets to the two reference views, largely enriching the variety of inter-viewpoint changes.\n\t\tOn the other hand, we also use the training split of the LLFF dataset, which consists of 8 different forward-facing scenes with 20-62 high-quality input views. Following previous work, we drop the eighth view and use it for evaluation. To construct a training pair from LLFF-T, we randomly select a frame as the target view and then use the proposed view selection algorithm\u00a0(Sec.\u00a04.3) to choose two reference views that are most overlapped with the target view. \n\t\t\n\t\t\n\t\t \n\t\t\n\t\t\n\n\t\tHyper-parameter setup.\n\t\tIn Eq.\u00a0(1), the 2D Gaussian noise map n is generated with a zero mean and a standard deviation ranging from 0.01 to 0.05. The isotropic blur kernel g has a size of 5 \u00d7 5. We employ a Gaussian blur kernel to produce blurry contents by randomly selecting kernel sizes\u00a0(3-7), angles\u00a0(0-180), and standard deviations\u00a0(0.2-1.2). Last, in order to obtain a region-adaptive blending map M in Eq.\u00a0(3), we use random means\u00a0(c_i,c_j \u2208(-16,144)), standard deviations\u00a0(\u03c3_i \u2208 (13,25),\u03c3_j \u2208(0,24)), and orientation angles\u00a0(A \u2208(0,180)). Additionally, we visualize some generated masks using different hyper-parameter combinations\u00a0([c_i, c_j; \u03c3_i, \u03c3_j, A]) in Fig.\u00a0<ref>.\n\t\t\n\t\t\n\n\t\tTraining data size.\n\t\tWe investigate the influence of training data size. Under the same training and testing setups, we train several models using different training data sizes. As illustrated in Table\u00a0<ref>, we can observe that the final performance is positively correlated with the number of training pairs. Also, we notice the IVM trained with only LLFF-T data or additional few simulated pairs\u00a0(10% of the Vimeo90K) fails to enhance the TensoRF-rendered results, , there is no obvious improvement compared to TensoRF\u00a0<cit.>. This experiment demonstrates the importance of sizable training pairs for training a NeRF restorer.\n\t\t\n\t\t\n\t\t\n\t\t\n\n\t\t\n\n\u00a7 INTER-VIEWPOINT MIXER\n\n\t\t\n\n\t\tIn Sec.\u00a04.2, we briefly describe the framework architecture of our inter-viewpoint mixer (IVM). Here we provide more details. As illustrated in Fig.\u00a0<ref>(a), there are two convolutional modules\u00a0(\u201cEncoder 1/2\") to extract features of the degraded view I and its two reference views {I_1^r,I_2^r}, respectively. Then, we develop a hybrid recurrent aggregation module that iteratively performs pixel-wise and patch-wise fusion. At last, a reconstruction module is implemented by a sequence of residual blocks\u00a0(40 blocks) to output the enhanced view \u00ce. The default channel size is 128.\n\t\t\n\t\t\n\n\t\tFeature extraction.\n\t\tGiven a rendered view I and its two reference views I_1,2^r, we aim to utilize the two encoders to extract deep image features \ud835\udc1f and \ud835\udc1f_1,2^r, respectively. As detailed in Fig.\u00a0<ref>(a), the two encoders share an identical structure. A convolutional layer is first adopted to convert an RGB frame to a high-dimensional feature. Then we further extract the deep image feature using 5 stacked residual blocks followed by another convolution layer. \n\t\t\n\t\t\n\t\t\n\n\t\tHybrid recurrent aggregation. As depicted in Fig.\u00a0<ref>(a), we employ three hybrid recurrent aggregation blocks\u00a0(termed \u201cHybrid-R1(2,3)\") to progressively fuse the inter-viewpoint information from the image features\u00a0(\ud835\udc1f and \ud835\udc1f_{1,2}^r). Next, we take the first iteration as an example to illustrate our aggregation scheme.\n\t\t\n\t\t\n\n\t\tPixel-wise aggregation.\n\t\tAs shown in Fig.\u00a0<ref>(b), we first merge the target view feature \ud835\udc1f and one of the reference features\u00a0\ud835\udc1f_{1,2}^r by channel concatenation. Then we use a convolutional layer to reduce the channel dimension and five residual blocks followed by another convolutional layer to obtain a fused deep feature. Later on, the fused feature and the reference feature are further aggregated through a deformable convolution. And the other reference image follows the same processing pipeline. In this case, we finally obtain two features after the pixel-wise aggregation.\n\t\t\n\t\t\n\n\t\tPatch-wise aggregation.\n\t\tWe adopt a window-based attention mechanism\u00a0<cit.> to accomplish patch-wise aggregation. In detail, the pixel-wisely fused features are first divided into several 3D slices through a 3D patch partition layer. Then, we obtain 3D tokens via a linear embedding operation and aggregate patch-wise information using a video Swin transformer block. Finally, 3D patches are regrouped into a 3D feature map.\n\t\t\n\t\tIn the next iteration, we split the 3D feature map into two \u201creference\" features \u1e1f_{1,2}^r and repeat the pixel-wise and patch-wise aggregation. Note that, the weights of pixel-wise and patch-wise modules are shared across all iterations to reduce the model complexity.\n\t\t\n\t\t\tComparisons with SOTA image restorers.\n\t\t\t\n\nThanks for this good suggestion. We have tested five off-the-shelf SOTA image/video restoration models (\u201cM1-5\" refer to BSRGAN in ICCV 2021, MPRNet in CVPR 2021, Real-ESRGAN in ICCVW 2021, Restormer and RealBasicVSR in CVPR 2022, respectively) to remove NeRF-rendered artifacts. These methods, though good for real-world image restoration, are not designed nor trained specifically to tackle NeRF-style artifacts. In fact, as shown in the Table\u00a0<ref> below, the quality of the synthesized images is basically not improved or becomes even worse. Conversely, if using our simulated NDS dataset to re-train the BSRGAN model, we find it obviously outperforms the original BSRGAN by 0.62dB\u00a0(Table 3b of the main paper). This experiment indicates the existing image/video restoration model cannot enhance NeRF-rendered frames, confirming the necessity of NeRFLiX.\n\t\t\t\n\n\t\t\t\n\t\t\n\t\t\n\n\u00a7 ADDITIONAL RESULTS\n\n\t\t\n\t\t\n\t\t\n\n\t\tNumber of reference views.\n\t\tBy default, we perform inter-viewpoint aggregation using two reference views (termed IVM-2V). We train another three models (IVM-0V, IVM-1V, and IVM-3V) adopting different numbers of reference views. The results are shown in Table\u00a0<ref>. The model without using reference views\u00a0(IVM-0V) achieves the lowest PSNR and SSIM values compared with other models. Meanwhile, it is observed that the more reference views used, the higher IVM performance, indicating the importance of utilizing high-quality reference views.\n\t\t\n\t\t\n\t\t\n\t\t\n\n\t\tView selection.\n\t\tFig.\u00a0<ref> exhibits the selected views by our algorithm in different NeRF scenes. We see that the proposed view selection strategy is able to choose the most relevant ones from freely captured views.\n\t\t\n\t\t\n\n\t\tQualitative results.\n\t\t\n\t\tHere, we provide more visual examples to adequately validate the effectiveness of our approach. As shown in Fig.\u00a0<ref>, Fig.\u00a0<ref>, Fig.\u00a0<ref>, Fig.\u00a0<ref>, NeRFLiX consistently improves NeRF-rendered images with clearer details and fewer artifacts for all NeRF models.  For example, NeRFLiX successfully recovers recognizable characters, object textures, and more realistic reflectance effects, while effectively eliminating the rendering artifacts.\n\t\t\n\t\t\n\t\t\n\t\t\n\n\t\tVideo demo.\n\t\tWe also provide a video demo[available at our project website <https://redrock303.github.io/nerflix/>] for a clear visual comparison. First, we show some NeRF-rendered views and the restored counterparts of NeRFLiX. Then, we provide two video cases (one is from LLFF and the other is an in-the-wild scene) to compare the rendered views of TensoRF\u00a0<cit.> and enhanced results of our NeRFLiX. It is observed that NeRFLiX is capable of producing clearer image details and removing the majority of the NeRF rendering artifacts.  \n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\n"}