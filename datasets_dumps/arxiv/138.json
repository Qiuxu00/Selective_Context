{"entry_id": "http://arxiv.org/abs/2303.07166v1", "published": "20230313150952", "title": "Improved Tree Search for Automatic Program Synthesis", "authors": ["Aran Carmon", "Lior Wolf"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.PL", "cs.SE"], "text": "\n\n[\nImproved Tree Search for Automatic Program Synthesis\n\n\n\n\n\n\n\n\n\n\n\n\nAran Carmontau\nLior Wolftau\n\n\ntauThe School of Computer Science , Tel Aviv University\n\nLior Wolfliorwolf@gmail.com\nAran Carmonarancarmon@mail.tau.ac.il\n\nAutomatic Program Synthesis, Exploration, MCTS\n\n0.3in\n]\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the task of automatic program synthesis, one obtains pairs of matching inputs and outputs and generates a computer program, in a particular domain-specific language (DSL), which given each sample input returns the matching output. A key element is being able to perform an efficient search in the space of valid programs. Here, we suggest a variant of MCTS that leads to state of the art results on two vastly different DSLs. The exploration method we propose includes multiple contributions: a modified visit count, a preprocessing procedure for the training dataset, and encoding the part of the program that was already executed. \n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nSearch is a key part of many machine learning tasks, in which the output is a  sequence. We study the specific task of automatic program synthesis, given a specification in the form of input/output pairs\u00a0<cit.>. In this sequence generation task, similar to other generation tasks, such as machine translation and image captioning, there are multiple correct answers.\n\nDifferent from most sentence generation tasks in NLP, in this task, one is able to directly evaluate the correctness of the output, by running the generated program. This leads to a well-defined and natural reward, when viewed as a reinforcement learning problem\u00a0<cit.>: either the generated program produces the specified outputs given the matching inputs or not. Other variants of this reward may consider, for example, the length of the program, encouraging the generated program to be more efficient.\n\nHowever, as reported by previous work\u00a0<cit.>, employing a reinforcement learning approach, as opposed to training using a maximum likelihood loss to generate the single program that is available as the ground truth, either hurts performance or leads to a small increase in performance. This is despite training the MLE approach in a teacher-forcing way, in which, during training and unlike during test time, the partial programs considered are the prefix of the ground truth programs.\n\nIn this work we, therefore, focus on the MLE approach and consider different tree search strategies. These methods include the classical beam search, which is often used in the program synthesis domain, the CAB variant of it, which was used successfully in the past\u00a0<cit.>, and various MCTS approaches that we develop and explore.\n\nIn addition to studying the specific building blocks of the application of MCTS to this problem, we also suggest two other improvements. The first is a pre-processing step that is applied to the training set, and the second is the addition of an encoder of the current program's history. We demonstrate how these techniques can improve the obtained accuracy, both separately and when combined.\n\n\n\n\n\u00a7 RELATED WORK\n\n\nWe experiment with two DSLs. The first is the DeepCoder DSL\u00a0<cit.>, in which integer registers are being manipulated. Each register may contain either an integer or a list of integers, and the program may apply on the registers,  functions such as sort, tail, and multiply.\u00a0<cit.> have used the predictions of a neural network that was applied to the input/output pairs, in order to augment classical program search techniques, such as SMT-solvers and enumerative search. Specifically, the neural network produced a vector of probabilities for the existence of every command or function in the program.  Later on,\u00a0<cit.> used a neural network to predict the probability of the next statement given the execution state, after applying the partial program as part of a beam search. Specifically, the CAB search strategy\u00a0<cit.> was used. Since, in this DSL, every statement returns one integer or list into the memory, and since this memory is limited, a learned garbage collection mechanism was used to discard the results of previous statements from the memory. Our work on this DSL extends the method of\u00a0<cit.>, and also employs this garbage collector.\n\nThe Karel DSL\u00a0<cit.> acts on a 2D grid world that contains an actor, various markers, and obstacles. The Karel DSL contains conditions and loops, which are not part of the DeepCoder DSL. However, the latter has some additional high-level functions, such as sort or map.\u00a0<cit.> have demonstrated how to perform RL for improving the results on this DSL.\u00a0<cit.> have contributed the encoding of the current execution state (similar to\u00a0<cit.>), as well as the usage of ensembles. In our work, we do not study the effect of ensembles.\n\n\n\n\nBeam search is perhaps the most popular method for exploring search trees. It is similar in nature to a best-first search, but instead of keeping in memory just the most promising candidate, beam search keeps several candidate tracks simultaneously. The number of candidates is the beam width, and the number of the derived expansions extracted from each candidate track  (prior to pruning back to the beam width) \nis the extraction width.\n\nComplete Anytime Beam (CAB) search\u00a0<cit.> is an extension of beam search, which is better-suited for searching under a fixed time budget. In beam search, if the beam width and expansion size are configured too low, the beam search will be too weak to reach its destination. If they are too high, the beam search will reach the execution time limit, before achieving its goal. CAB extends beam search by running it repeatedly, starting with weak and cheap settings and increasing the search parameters after each search failure, as long as it still has time to run.\n\n\nMonte Carlo Tree Search (MCTS)\u00a0<cit.> is a popular method for prioritizing exploration in game search trees. The search is gradually expanded towards a mix of promising nodes and unexplored nodes. It includes the following steps: (1) Selection: traverse the search tree, until a new leaf node is reached. (2) Expansion: add the leaf node to the tree. (3) Simulation: evaluate the value of the leaf node (4) Backpropogation: update the values of the parent nodes on the track leading to the leaf node, according to its value. In our work, we do not employ a value function and the last two steps are not employed.\n\nA major milestone in the field of artificial intelligence was the achievement of super-human performance in Go by using deep learning, reinforcement learning, and MCTS\u00a0<cit.>.\nOur method shares with it the specific fraction that is based on the visit count. However, we advocate for performing the count at the environment level, while in the previous Go work the count is based on the location in the tree and not on the board configuration. \n\nOur method does not use a learned or a simulation (rollout) based value function, as  done by\u00a0<cit.>. We have tried learning such a value function and observed no improvement in the results. This may stem from the fact that the end goal, i.e., the output of our programs, varies, making such learning too challenging. In addition, as noted for the case of solving Rubik's cube with RL\u00a0<cit.>, the value function mainly reduces the depth of the MCTS and not its breadth. In our case, since the length of the programs is limited, the depth is less of a concern.\n\n\n\n\u00a7 METHOD\n\n\nOur method is based on applying MCTS in lieu of beam search or CAB, which served as the search technique for previous work. We employ the same networks that are used in\u00a0<cit.>. An encoder E embeds the input states of the execution, as well as the output for each sample and mean pooling is employed in order to obtain a single state vector. Based on this vector, a network P predicts the next statement and a network G predicts the variables to be dropped. P and G share most of their layers. In our work, we propose to add, as an additional input to the network P, the embedding of the partial program that was executed up to the current state, see Sec.\u00a0<ref>.\n\n\n\n \u00a7.\u00a7 Our MCTS variant\n\n\n\nGiven a trained network P that can prioritize the most likely next commands, we can explore the programs' space with a search tree. The nodes of the tree correspond to the execution states, after choosing the statements. The search favors nodes that have been less explored in the past (exploration), as well as nodes that result from commands that are predicted to have higher likelihood (exploitation). The score\n\nU(s,a) = P(s,a)/N(s,a)+1 is used,\n\nwhere P(s,a) is the output of the trained network (pseudo-probability of choosing action a, i.e., command, given state s), and N(s,a) is the number of times we took action a, as a response to being in state s.\n\nUnlike the literature we are aware of, in our variant of the search we do not backtrack. Instead, we start the search each time from the root node. At first,  we pick the most promising command according to the network's output, until we reach a solution, an invalid execution state, or a maximum program length limit. If a solution was not found, we start the search over, this time employing the updated score U, which discounts nodes that have been visited. \n\n\n\n  \nShared count \n\nWhile the MCTS methods we are aware of count the number of visits per tree node, we found it useful to count the number of times each state is visited.\n\nA state is the memory status obtained after running the partial program (from the root node to the current node) on each of the inputs in the input/output specifications. Two states are identical,  if the memory status is identical for all of the inputs in the specification.\n\nIn the DeepCoder DSL, the memory contains the various registers. We sort the registers such that two states are the same, even if the order of the registers is permuted, as long as the underlying values are the same. In the Karel DSL, the memory contains the status of the grid world. \n\n\n\n\n \u00a7.\u00a7 Pruning\n\n\nSince there are multiple execution pathways that result in the same states, learning with an MLE loss could be suboptimal. Previous work\u00a0<cit.> has suggested employing RL to overcome this;  However, as noted in subsequent work, this does not necessarily lead to significantly improved performance. \n\nWe suggest, instead, to preprocess the training set such that programs are replaced with shorter programs that are consistent with the input/output specifications. For this task, we employ a network that is pretrained on the original training data. If when applying this network, with our variant of MCTS, to the training dataset, a shorter (specification consistent) program is found, we replace the ground truth program. After this pruning, training is repeated from scratch.\n\nPruning has a drastic effect on the length of programs in the training datasets, as can be seen in Fig.\u00a0<ref>. For example, for the DeepCoder DSL, it shortens 82% of the length 12 programs, which is the maximal length available in the training dataset. \n\n\n\n\n \u00a7.\u00a7 Encoding of Partial Programs\n\n\n\nThe input in PCCoder\u00a0<cit.> to the network P, which predicts the next statement, is the embedding of the current registers of the  execution environments (one environment per each sample input/output), including the end-goal (output) register of each sample. For Karel, we employ a similar architecture to PCCoder. In this case, P receives an embedding of the input/output grids and an embedding of the last command executed, where the latter part was added following\u00a0<cit.>.\n\nIn this work, we propose to add an additional input, which encodes the previous commands executed by the partial program that has already been constructed during the search.\n\nFor the DeepCoder DSL, the partial program is given as a sequence of 4-tuples containing an operator index, argument indices (one or two, if the operator takes one argument, we duplicate it), and an output index. We apply embedding layers on these four indices. \n\nLSTM is then applied to the concatenation of the four embeddings. The number of hidden units is taken to be the sum of the number of options in each look-up-tables we employ for the index embedding (66). The hidden state at the end of the sequence is then concatenated to the activations that arise from the other inputs of P, after the 5 densenet layers of P are applied to these inputs. Two fully-connected layers (of the same size) are then applied, followed by a softmax layer of the same dimensionality, as in the original P.\n\nIn Karel, the input and output grids are encoded, using the same convolutional networks used by\u00a0<cit.>. The partial program is encoded by running two LSTMs: the first is run on the embedding of the commands (there are no arguments) and the second on the nesting level (is the command executed within an if condition, a while loop, etc). The LUTs contain 38 command options and six nesting options. The LSTMs have a corresponding number of hidden units. The hidden states of the two LSTMs at the end of the sequence are concatenated to the embedded input/output samples (after the CNN encoding) and passed through three linear layers, with 512 hidden units. This is done separately for each input/output sample, and is aggregated by max pooling, followed by a softmax layer.\n\n\n\n\u00a7 EXPERIMENTS\n\n\nWe test the various search methods and evaluate the effect on performance of each of our contributions. In all of our experiments, the PCCoder architecture is used. Applying this method for Karel involved the same input/output encoding used in\u00a0<cit.>. Since, unlike this previous work, we do not use an LSTM to generate the program (PCCoder predicts only the next command), the LSTM is replaced with a fully connected network. A garbage collection network G is not required in this DSL.\n\nNote that given enough time, one can find, using a na\u00efve search approach,  the programs, up to a certain length, that are consistent with the input/output pairs. The shortest such program can then be selected, in order to improve generalization. Therefore, it is important to enforce a maximal allowed run time. This has obvious drawbacks when comparing across contributions. However, given the same standard hardware, a fixed runtime budget provides a reliable performance metric. In our experiments, we employ a commercial cloud infrastructure to perform all experiments.\n\nThe DeepCoder DSL experiments were done with eight registers and the benchmark of\u00a0<cit.> that includes training programs of lengths up to 12, and ground truth test programs of length 14 (all test environments are constructed with programs of length 14, which may or may not have shorter equivalents).\n\n\n\n\n\nThe results are given in Tab.\u00a0<ref>. As can be seen, all of our contributions (pruning of the dataset, encoding the partial program, replacing CAB with our variant of MCTS, and the shared count MCTS) improve performance over the baseline. This is true for the three different values of timeout. Our best result, which includes all contributions, presents an accuracy of 85.2%, in comparison to 50.2% of the previous work, for the same timeout of 2000 seconds. Partial program encoding by itself adds 15% to the CAB search. Pruning has a less dramatic effect: it adds 3% to CAB. \n\nIn addition, pruning does not add to the shared visit count MCTS method by itself. However, when combined with encoding of the partial history, it adds significantly, e.g., 5.2% for the 500 second timeout.\n\nIn all four groups obtained by applying or not applying pruning or partial program encoding, the MCTS method with the shared counts outperforms the other search method by a significant margin, and our variant of MCTS without the shared counts outperforms CAB.\n\nThe Karel benchmark used is taken from\u00a0<cit.> and is also used by\u00a0<cit.>. However, \non this benchmark, we found it challenging to compare directly with previous work, since the previous work did not employ a timeout. Instead, a fixed number of beams was used, which is not compatible with our MCTS variant going back to the root at each failed search. \n\nPerformance is measured by exposing five input/output samples to the searcher and measuring if the searcher succeeded in finding a program which is not only consistent with the five input/output examples, but also generalizes to a sixth unseen input/output sample. In Tab.\u00a0<ref>, we present accuracy for both goals (consistency with the five or the six samples). The results are given for timeouts of 1000 or 2000 seconds. Beam search results are given for the best width and expansion parameters that we have found (128 and 5, respectively). The CAB parameters were also optimized to maximize the test accuracy, and starting with a similar beam search, it doubles the beam width and increases the expansion size by one, at each repeated attempt.\n\nFor this DSL, the combination of MCTS with past encoding and dataset pruning obtains the best results in both timeouts and for both success criteria. Sharing the visit counts between identical world grids did not improve results and the added bookkeeping increased runtime, leading to slightly lower results.\n\n\n\n\u00a7 ACKNOWLEDGEMENT\n\nThis work was supported by an ICRC grant.\n\n\nicml2019\n\n"}