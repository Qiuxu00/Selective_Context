{"entry_id": "http://arxiv.org/abs/2303.06873v1", "published": "20230313054656", "title": "Interventional Bag Multi-Instance Learning On Whole-Slide Pathological Images", "authors": ["Tiancheng Lin", "Zhimiao Yu", "Hongyu Hu", "Yi Xu", "Chang Wen Chen"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\nInterventional Bag Multi-Instance Learning On Whole-Slide Pathological Images\n    Tiancheng Lin1,2     Zhimiao Yu1,2      Hongyu Hu1,2     Yi Xu1,2Corresponding author.     Chang Wen Chen3\n\n1 Shanghai Key Lab of Digital Media Processing and Transmission, Shanghai Jiao Tong University\n\n2 MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\n\n3 The Hong Kong Polytechnic University, Hong Kong, China\n\n{ltc19940819, carboxy, mathewcrespo, xuyi}@sjtu.edu.cn, changwen.chen@polyu.edu.hk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Received: date / Accepted: date\n==================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\nMulti-instance learning (MIL) is an effective paradigm for whole-slide pathological images (WSIs) classification to handle the gigapixel resolution and slide-level label. \nPrevailing MIL methods primarily focus on improving the feature extractor and aggregator.\nHowever, one deficiency of these methods is that the bag contextual prior may trick the model into capturing spurious correlations between bags and labels. This deficiency is a confounder that limits the performance of existing MIL methods. In this paper, we propose a novel scheme, Interventional Bag Multi-Instance Learning (IBMIL), to achieve deconfounded bag-level prediction. Unlike traditional likelihood-based strategies, the proposed scheme is based on the backdoor adjustment to achieve the interventional training, thus is capable of suppressing the bias caused by the bag contextual prior. Note that the principle of IBMIL is orthogonal to existing  bag MIL methods. \nTherefore, IBMIL is able to bring consistent performance boosting to existing schemes, achieving  new state-of-the-art performance. Code is available at <https://github.com/HHHedo/IBMIL>.\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nThe quantitative analysis of whole-slide pathological images (WSIs) is essential for both diagnostic and research purposes\u00a0<cit.>.\nBeyond complex biological structures, WSIs are quite different from natural images in the gigapixel resolution and expensive annotation, which is thus formulated as a multi-instance learning (MIL)\u00a0<cit.> problem: treating each WSI as a labeled bag and the corresponding patches as unlabeled instances.\nSuch a de facto paradigm has been demonstrated in extensive tasks on WSIs, e.g., classification\u00a0<cit.>, regression\u00a0<cit.> and segmentation\u00a0<cit.>.\nThe prevailing scheme for WSI classification \u2014 bag-level MIL \u2014 is depicted in\u00a0<ref>. Given the patchified images as instances, each instance is embedded in vectors by a feature extractor in the first stage. Second, for each bag, their corresponding instance features are aggregated as a bag-level feature for classification.\n\n\n\n\nMore and more new frameworks are proposed to improve the two stages following this scheme\u00a0<cit.>.\nIt is convinced that learning better instance features and modeling more accurate instance relationships can bring better performance of MIL.\nWhile we have witnessed the great efforts, they still leave the \u201cbag contextual prior\" issue unsolved: the information shared by bags of the same class but irrelevant to the label, which may affect the final predictions. \nFor example, in\u00a0<ref>, due to the dataset bias, most of the instances in the positive bags are stained pink but purple in the negative bags. \nThe co-occurrence of specific color patterns and labels may mislead the model to classify bags by color statistics instead of the key instances \u2014 the more pink instances a bag contains, the more likely it is a positive bag. \u00a0<ref> illustrates another example: even if the prediction is correct, the underlying visual attention is not reasonable, where the high attention scores are put on the disease-irrelevant instances outside the blue curves in the bags.\nFrom the causal lens, the bag contextual prior is a confounder that opens up a backdoor path for bags and labels, causing spurious correlations between them.\nTo suppress such a bias, we need a more efficient mechanism for the actual causality between bags and labels, i.e., the bag prediction is based on the bag's content (e.g., key instances), which can not be fully achieved only by above mentioned new frameworks.\n\n\nIn fact, it is challenging to achieve unbiased bag predictions as such a bias happens in the  data generation \u2013 the tissue preparations, staining protocols, digital scanners, etc.\nIn this paper, we propose a novel MIL scheme, Interventional Bag Multi-Instance Learning (IBMIL), to tackle this challenge.\nIn particular, we propose a structure causal model (SCM)\u00a0<cit.> to analyze the causalities among bag contextual prior, bags and labels. \nThe key difference of IBMIL is that it contains another stage of interventional training (see \u00a0<ref> right). \nGiven the  aggregator trained in the second stage, instead of directly using it for inference via likelihood: P(Y|X), we apply it for the approximation of confounders. \nWith the confounders observed, we eliminate their effect via the backdoor adjustment formulation\u00a0<cit.>, where the intuitive understanding is: if a  WSI model can learn from   \u201cpurple\u201d and \u201cpink\u201d positive/negative bags, respectively, then the bag context of color will no longer confound the recognition. \nTherefore, our IBMIL is fundamentally different from the existing scheme as we use a causal intervention: P(Y|do(X))  for bag prediction. \n\n\n\nWe conduct experiments on two public WSI datasets, i.e., Camelyon16\u00a0<cit.> and TCGA-NSCLC. \nExperimental results show that IBMIL is agnostic to both feature extractors  and aggregation networks, i.e., it brings consistent performance boosting to all compared state-of-the-art MIL methods in the WSI classification tasks. \nFurther ablation  studies and analyses demonstrate the effectiveness of interventional training.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n \u00a7.\u00a7 Instance-level MIL on WSIs\n\nInstance-level MIL represents each instance by a score and aggregates instance scores into a bag score. One widely used baseline is SimpleMIL \u00a0<cit.>, which directly propagates the bag label to its instances.\nWhen applying SimpleMIL for WSIs, the unbalanced dataset could result in noisy instance-level supervision since a WSI (e.g., Camelyon16) might only contain a small portion of a disease-positive tissue in clinic\u00a0<cit.>.\n\nThe following works in this line improve this baseline via various modifications. \nCleaner annotations: SemiMIL\u00a0<cit.> directly introduces cleaner annotations for partial instances with the help of pathologists, where these annotated regions are assigned with larger weights as they carry higher confidence. \nInstance selection: PatchCNN\u00a0<cit.> selects instances via a delicate thresholding scheme at both WSI and class levels. Similarly, Top-k MIL\u00a0<cit.> only uses the top-k instances for each bag, but the fixed number of selected instances fails to make a trade-off between preserving clean instances and discarding noisy instances. RCEMIL\u00a0<cit.> proposes rectified cross-entropy (RCE) loss to select instances in a softer manner, while the loss requires statistics of possible abnormal tissues among all WSIs. \n\n\n\n\nMore recently, IMIL\u00a0<cit.> summarizes the previous works from a causal lens and propose IMIL to select instance via causal intervention and effect.\nHowever, the performance of instance-level MIL methods is usually inferior to bag-level counterparts\u00a0<cit.>.\n\n\n\n\n\n \u00a7.\u00a7 Bag-level MIL on WSIs\n \nThe instances are represented as embedding vectors and classified by bag-to-bag distance/similarity or a bag classifier <cit.>. \n\nConducting bag-level MIL on WSI is non-trivial, because the intermediate results of all patches still need to be stored in memory for backpropagation. \nTherefore, some recently proposed frameworks separate the training of instance-level feature extractors and aggregation networks, resulting in a two-stage modeling approach\u00a0<cit.>.\nThey contribute differently at both stages. \nFor the feature extractor, they introduce different architectures from convolutional neural networks (CNNs) to transformer-based models\u00a0<cit.>, and training paradigms from ImageNet pre-training\u00a0<cit.> to self-supervised learning\u00a0<cit.>. \n\nSimultaneously, many works pay attention to new  designs of aggregation networks, from non-parametric poolings, e.g., max/mean-poolings\u00a0<cit.>, to learnable ones,  e.g., graph convolution networks\u00a0<cit.> and attention mechanisms\u00a0<cit.>. \nOur work lies in this line but aims at empowering these existing methods. Thus the contributions are orthogonal.\n\n\n\n\n\n \u00a7.\u00a7 Causal Inference in Computer Vision\n\nCausal inference, a general framework, has been introduced to various computer vision tasks, including classification\u00a0<cit.>, semantic segmentation\u00a0<cit.>, unsupervised representation learning\u00a0<cit.>  and so on. \nIn MIL problems, StableMIL\u00a0<cit.> takes \u201cadding an instance to a bag\u201d as a treatment for bag-level prediction, while IMIL\u00a0<cit.> uses inverse probability weighting and causal effects for instance-level tasks. \nUnlike them, our IBMIL is based on backdoor adjustment formulation and works as a general framework to empower existing bag-level MIL for WSI classification tasks.\n\n\n\n\n\u00a7 METHOD\n\n\n\n\n \u00a7.\u00a7 Preliminaries\n\nMIL formulation. Due to the gigapixel resolution and lacking fine-grained labels, performing downstream tasks on WSIs is formulated as an MIL problem, such that each WSI is treated as a labeled bag with corresponding patches as unlabeled instances. \nTake binary classification as an example, let X = {(x_1,y_1),...,(x_n, y_n)} be a WSI bag, which contains n instances of x_i. The instance-level labels {y_i ,...,y_n}  are unavailable.\nUnder the standard MIL assumption, the bag label Y is further given by:\n\n    Y= 0,     iff \u2211 y_i=0 \n     1,     otherwise\n\nwhich can be modelled by max-pooling\u00a0<cit.>. \nA general three-stage approach goes like 1) Instance transformation: a feature extractor f(\u00b7) is trained for instance-wise features b, 2) Instance combination: the pooling operation \u03c3(\u00b7) is targeted for bag feature B, 3) Bag transformation:  a downstream classifier g(\u00b7) is used for prediction, which can be formulated as:\n\n\n    b_i = f(x_i), B = \u03c3(b_1,\u22ef,b_n), \u0176 = g(B),\n\nwhere the pooling \u03c3(\u00b7) should be a permutation-invariant function\u00a0<cit.> for the spatial-invariant MIL method. Some works further absorb the classifier g(\u00b7) into the pooling operation \u03c3(\u00b7), referred to as aggregator/aggregation networks.\nWhen applying the MIL methods for WSIs, it should be noted that 1) the diagnosis for WSI analysis can be based on different tissue regions with multiple concepts \u2014 the collective MIL assumption, 2)  the bag length n for a WSI can be extremely large, e.g., about 8,000 on average<cit.>.\nTherefore, the bag MIL methods for WSIs are with learnable aggregators and trained in a two-stage procedure, i.e., training the feature extractor and aggregator stage by stage. \n\n\nCurrent works mainly follow this formulation and improve the framework from both feature extractor and aggregator, while our proposed method aims to empower existing works from a causal perspective.\n\n\n\nAnalysis MIL through causal inference.\nAs shown in\u00a0<ref>, we formulate the MIL framework as a causal graph (a.k.a, Pearl's structural causal model or SCM\u00a0<cit.>), which contains three nodes: X: whole-slide pathological image (bag), Y: bag label, C: bag contextual information.\n\n\n\n\nX\u2192 Y: This path indicates that the MIL model can learn to predict the bag label on the bag content, e.g., key instances.\n\nC\u2192 X: This path indicates the generation of the whole-slide pathological image. Due to the differences in tissue preparations, staining protocols and digital scanners, the appearance of WSIs can be significantly affected, potentially introducing biases.\n\nC\u2192 Y: This path indicates that the bag prediction is affected by the contextual prior information in the training dataset. For example, in\u00a0<ref>, an MIL model predicts all bags with purple color as positive regardless of content information related to the real label.\n\nIn the causal graph, C confounds X and Y via the backdoor path X \u2192 C \u2192 Y and causes a spurious correlation between them, which prevents learning robust bag MIL models. For example, the model may wrongly predict the bags when the data are out-of-distribution, i.e., with different context prior.\n\nAn ideal MIL method should capture the true causality between X and Y, but the conventional correlation of P(Y|X) fails to do so, as such a spurious correlation is inevitable.\nTherefore, we instead seek to use the causal intervention P(Y|do(X)), where the do-operation do(\u00b7) means forcibly assigning a specific value to the variable X. As shown in\u00a0<ref>, it can be considered as a modification of the graph \u2014  cutting off the backdoor path, thus mitigating the bias caused by confounders. The ideal way of do(\u00b7) is the random controlled trials\u00a0<cit.> \u2014 enumerating each bag with all possible contexts, which is impossible in practice. Next, we propose a practical intervention method to remove the confounding effect caused by the bag contextual prior. \n\n\n\n \u00a7.\u00a7 Interventional Bag Multi-Instance Learning\n\n\nWe propose to use the backdoor adjustment formulation to achieve the causal intervention: P(Y|do(X))  for bag-level prediction.\nFormally, we have the backdoor adjustment for the graph in\u00a0<ref> as:\n\n\n\n\n\n\n    P(Y | d o(X))=\u2211 _i  P(Y | X, h(X,c_i)) P(c_i),\n\n\nwhere c_i loops over the confounder set and h(\u00b7) is a function defined later in <ref>. Different from Bayes rule,  in <ref>, c_i is no longer affected by X but subject to its prior P(c_i), since the causal intervention forces  X to incorporate each c_i fairly. \nNow, we are ready to introduce our interventional bag multi-instance learning stage by stage. <ref> illustrates the overview of IBMIL.\n\n\n\n\n\n\nStage 1: Training feature extractors.\nWe learn a feature extractor f(\u00b7) on the patchified images of WSIs {x_1 ,...,x_n}, aiming at encoding each instance as a discriminative feature vector. \n\n\n\n\n\nStage 2: Training aggregators.\nGiven the features of instances  {b_1 ,...,b_n}, the aggregator employs MIL pooling \u03c3(\u00b7)  to assemble them into a bag feature B sequentially or simultaneously, and a classifier g(\u00b7)  for discrimination. Formally,  the loss for training aggregator is defined as:\n\n    \u2112=-1/N\u2211_i=1^N Y_i log\u0176_i+(1-Y_i) log(1-\u0176_i),\n\nwhere N is number of bags in the training set. \nNote our IBMIL is no not limited to specific feature extractor or aggregators, including the architectures and training paradigms. Please refer to <ref>  for our choices.\n\n\n\n\nStage 3: Causal intervention via backdoor adjustment.\nThe traditional two-stage bag MIL stops at stage 2 and uses the trained models for inference directly.\nInstead, we introduce another stage of interventional training, which needs the practical implementation of <ref>. \nNote that backdoor adjustment assumes that we can observe and stratify the confounders of a bag context. Thanks to the powerful ability of deep MIL models, context information is naturally encoded in the higher-level layers\u00a0<cit.>. \n\nTo constitute the confounder set, we use a confounder dictionary\n\n\nC=[c_1, \u2026, c_K]\nfor approximation, as collecting all confounders is impossible. \nGiven the trained feature extractor and aggregator, we use K-means over all the bag features in the training set, partitioning the bags into clusters. \nWe average the bag features of each cluster to represent a confounder stratum c_i, resulting in a  confounder dictionary with the shape of d \u00d7 K, where d is the dimension of bag features.\nNote that our approximation is reasonable in that these global clusters are susceptible to the visual biases\u00a0<cit.>, which is exactly the confounders.\nThen, we define:\n\n\n\n\n\n\n\n\n    [                           h(X,c_i)= \u03b1_i c_i,; [\u03b1_1,\u22ef ,\u03b1_K]=softmax((W_1 B)^T(W_2 C)/\u221a(l)), ]\n\nwhere B=\u03c3(f(X)) is the bag feature, W_1, W_2 \u2208\u211d^l \u00d7 d are two learnable projection matrices to project bag feature B and confounder C into a joint space, and \u221a(l) is used for feature normalization\u00a0<cit.>. Since the prediction comes from both bag X and confounder C (see\u00a0<ref>), we further define\n\n    P(Y | X, h(X,c_i)) = P(Y | B \u2295 h(X,c_i)),\n\nwhere \u2295 denotes vector concatenation, and other implementations can be found in ablation studies.\nWe assume P(c_i) is a uniform prior of 1/K for a safe estimation, and a more reasonable assumption, e.g., incorporating expert knowledge, will be our future work. \nPlugging <ref>, <ref> and defined P(c_i) into <ref>, we are ready to calculate P(Y|do(X)) via passing the network multiple times. In practice, to avoid the expensive cost, we further apply Normalized Weighted Geometric Mean\u00a0<cit.> to move the outer sum into the Softmax:\n\n\n\n\n\n\n\n\n    P(Y | d o(X)) \u2248 P(Y | B \u2295\u2211_i=1^K \u03b1_i c_i P(c_i) )  .\n\nThus,  backdoor adjustment can be achieved by one feed-forward of the network. \n\n\n\n \u00a7.\u00a7 Justification\n\nIn our implementation of the causal intervention, there are some aspects we need to discuss further. \n\nCompatible with large-scale unlabelled datasets. \nWe constitute the confounder set in an unsupervised fashion.\nOne alternative implementation is to use the available bag labels for guidance, preserving the intra-class variation and capturing the class-relevant characteristics .\nThere are two main reasons  for our choice. 1) The unsupervised fashion makes our scheme compatible with large-scale unlabelled datasets, e.g., The Cancer Genome Atlas (TCGA), for better approximation of confounders. 2) The confounder could be irrelevant to the class identity, e.g., the stain color of positive and negative instances can be the same. \nWe explore the other implementations in\u00a0<ref>.\n\nOne possible more elegant scheme. As we need the trained aggregator to generate the bag features (the stage 2), one more stage is needed to retrain the aggregator (the stage 3).\nWe are thus motivated to further simplify our scheme to avoid extra computational cost.\nSpecifically, we can achieve the bag features by applying the traditional non-parametric aggregators, e.g., max/mean-pooling, to the instances in a bag. \nIt is inspired by the fact that these non-parametric aggregators serve as strong baselines, and we conjecture that statistic bag information they provide can be used for a reasonable approximation of confounders.\nTherefore, we can omit the stage 2. \nThe experiment results in <ref>  support that our scheme can be more elegant.\n\n\nConnection to other methods. Embedding-based MIL: As we approximate the confounder set based on bag features, these confounders can be seen as a denoised abstraction of bag features. From this perspective, we share the same spirit with the embedding-based MIL\u00a0<cit.>, i.e., exploring the relations between bags. That means our IBMIL also explains the effectiveness of embedding-based MIL.\nColor Normalization: Some works\u00a0<cit.> propose color normalization methods for H&E stained WSIs. However, color is just one of the confounders, and some confounders are even unobserved. Our method does not focus on color only, and thus is the more reasonable partially observed children of the unobserved confounder\u00a0<cit.>; Instance augmentation:  IMIL\u00a0<cit.> uses strong instance augmentation to train the feature extractor for instance prediction. However, the augmentation may affect the statistical information in the bag. Therefore, our method is more suitable for bag MIL. Remix\u00a0<cit.> proposes  data augmentations for MIL by exploring the relations of instances, but our method explores the bag-level relations based on the causal theory.\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n\nDataset and evaluation protocol.  \nWe conduct the experiments on two public WSI datasets, i.e., Camelyon16\u00a0<cit.> and TCGA-NSCLC. Camelyon16 is a dataset of H&E stained slides for metastasis detection in breast cancer, consisting of 399 WSIs. Following\u00a0<cit.>, we crop each WSI into 256\u00d7 256 non-overlapping patches, and remove the background region. \nThere are roughly 2.8 million patches at 20\u00d7 magnification in total, with about 7,200 patches per bag. \nTCGA-NSCLC includes two subtypes in lung cancer, i.e., Lung Squamous Cell Carcinoma (LUSC) and Lung Adenocarcinoma(LUAD). The dataset consists of 1,054 WSIs.  We directly used the patches released by \u00a0<cit.>, which are about 5.2 million patches at 20\u00d7 magnitude, with an average of 5000 patches for each bag.\nFollowing the evaluation protocol of \u00a0<cit.>, we use 270 training images and 129 test images for Camelyon16, and 836 training images and 210 test images for TCGA-NSCLC(some corrupted slides are discarded). \nWe  report the class-wise precision, recall, accuracy and area under the curve (AUC) scores.\n\n\n\nFeature extractor.\nWe adopt different network architectures with different training paradigms to thoroughly evaluate our IBMIL. ResNet-18\u00a0<cit.> is a widely used CNN-based model in our community, and we adopt the ImageNet pre-trained one released by PyTorch. ViT-small\u00a0<cit.> is a typical transformer-based model, which is good at modeling the long-range dependencies in the data. CTransPath\u00a0<cit.> is a hybrid CNN and transformer architecture, customized for WSIs. We adopt the ViT pre-trained with MoCo V3\u00a0<cit.> and CTransPath pre-trained with semantically-relevant contrastive learning (SRCL), where the used data is about 15 million images from 9 datasets\u00a0<cit.>.\nPlease refer to the Supplementary for more details.\n\n\nAggregators for MIL models. \nWe build our proposed method upon 4 SOTA methods. \nABMIL\u00a0<cit.> is a classic attention-based MIL, where the attention scores are predicted by a multi-layer perceptron (MLP).\n\nDSMIL\u00a0<cit.>, a dual-stream framework, jointly learns an instance and a bag classifier. The highest-score instance is further used to re-calibrate other instances into a bag feature. \nTransMIL\u00a0<cit.> is a correlated MIL framework built on transformer to explore both morphological and spatial information, where self-attention is used for bag aggregation. DFTD-MIL\u00a0<cit.> proposes to virtually enlarge the number of bags by introducing the concept of pseudo-bags, resulting in a double-tier MIL framework. To align with DSMIL, we use the maximum attention score selection (MaxS) for the feature distillation strategy. For more results of DTFD-MIL (MaxMinS), please refer to the Supplementary.\n\n\nWe use DSMIL\u2019s code base for implementation and evaluation, and build other models based on their officially released codes. Since the feature extractors we use are all pre-trained, we can directly transform instances into feature vectors. For stages 2 and 3, all MIL models are optimized for 50 epochs with learning rate of 0.0001, and other settings are followed their official code. \nWe  set the number of confounder K=8 and project dimension l=128 by default for all the main experiments. See Supplementary for more details.\n\n\n\n\n \u00a7.\u00a7 Experimental Results\n\n\n\nWe present the results on two benchmark WSI datasets, Camelyon16 and TCGA-NSCLC, covering binary class MIL with unbalanced bags and multiple class MIL with balanced bags, respectively. \nBy \u201cunbalanced\u201d, it means only a small portion of positive instances in positive bags, e.g., roughly 10% in Camelyon16\u00a0<cit.>.\nFrom\u00a0<ref>, we observe that 1) IBMIL consistently improves all feature extractors with all aggregators (12 possible combinations) on both datasets, which suggests that IBMIL is agnostic to feature extractors, aggregators and datasets.\n2) In particular, we find the improvement on the ImageNet pre-trained ResNet is larger than others. For example, the average gain of AUC is 5.4% in Camelyon16 and 1.5% in TCGA-NSCLC. This is mainly because  ResNet is more likely to learn context patterns as it is supervised trained on ImageNet\u00a0<cit.>, while the other two are self-supervised trained with strong data augmentations \u2014 the \u201cphysical intervention\u201d. 3) Our IBMIL improves more on Camelyon16 than TCGA-NSCLC in most cases. The main reason is that the former is a binary class MIL with unbalanced bags, which suffer more severe bag contextual prior \u2014 learning the key instances is much harder than context information.\nNote that the performance could be further improved by tuning the number of confounders for each setting.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Ablation on Model Design Variants\n\n\n\n\nIn <ref> and <ref>, experiments are conducted on TCGA-NSCLC dataset with feature extractor of ResNet-18 and aggregator of ABMIL, unless specified otherwise.\n\n \n\n\n\n\n\n\nSize of confounder dictionary.\nWe ablate size K of the confounder dictionary on three feature extractors, including ResNet-18, CTransPath and ViT. From <ref>, the performance of IBMIL is relatively robust to the size of confounder dictionary. Therefore, we need not elaborately tune this hyper-parameter and an arbitrary size within a wide range is able to boost the performance. \n\n\n\n\n\n\n\nDimension of joint feature space.\nAs mentioned above, the confounders and bag features are projected into a joint feature space with a dimension of l and attention scores are calculated subsequently. We ablate dimension l on three feature extractors. The results in <ref> reveal that performance does not improve monotonically with increased dimension and is saturated at l=256. We choose a dimension of 128 as the default configuration. \n\n\n\n\n\nLearnable vs. unlearnable confounders.\nWe explore the effect of learnable and unlearnable confounders. For the former, we update them in an end-to-end manner via backpropagation.\n\n\n  0.8!\n\n\n\n  Learnable       Precision       Recall       Accuracy       AUC  \n     83.81     83.82     83.81      90.82 \n\n    85.42     85.17     85.24     91.26  \n\n\n\n\n\n\nAs can be seen, both of them outperform the baseline accuracy of 81.43%. However, freezing confounders during interventional training beats learnable confounders by 1.43% on accuracy.\nThe reason may be that it is challenging to learn both confounders and interventional training with only bag-level labels, and introducing context-level supervision could be a solution\u00a0<cit.>.\nWe set confounders unlearnable as the default configuration.\n\nImplementation of backdoor adjustment.\nWe study the effect of different implementations of backdoor adjustment. Given a bag feature B \u2208\u211d^d and the combination of confounders \u2211_i=1^K \u03b1_i c_i P(c_i) \u2208\u211d^d, we explore three variants to combine them, i.e. B \u22c6\u2211_i=1^K \u03b1_i c_i P(c_i) and \u22c6\u2208{\u2295 ,+, - }, where +/- is element-wise addition/subtraction.\n\n\n0.8!\n\n\nMethod                        Precision       Recall       Accuracy       AUC  \n \u2295      85.42     85.17     85.24     91.26 \n\n +                            84.99     84.68     84.76     89.28 \n\n -                               84.70     84.18    84.29     90.14 \n\n\n\n\n\n\n\nWe observe all these implementations can lead to performance improvements, which demonstrates the stability  and effectiveness of the proposed intervention. \n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Analysis and Discussion.\n\n\n\n\n\n\n\n\n\n\n\nDo improvements come from more epochs?\n\nNote  that our proposed method requires an extra stage to train the aggregator. A natural question is whether we can improve baseline performance by training the baseline methods for as many epochs as the extra stage. <ref> displays that  more epochs do not bring about performance improvement, showing that our proposed method could empower baseline methods by backdoor adjustment rather than more training epochs. In most cases, training longer even brings performance degradation, which can be caused by the over-fitting problem in MIL\u00a0<cit.>.\n\n\n\n\n\n\nIs stage 2 necessary?\nRecent MIL methods aggregate the instance features into a bag feature via the weighted average operator. The weights, also referred to as attention scores, are generated by parametric networks, which need an extra stage of training aggregators.\nAlternatively, we turn to three non-parametric settings to skip this stage and efficiently achieve the bag features. We consider:\n\n    \n  * \u201cMean\" / \u201cMax\" denotes a bag feature is obtained through a mean-pooling / max-pooling layer among a bag of instance features, which is inspired by the strong baseline of non-parametric MIL method\u00a0<cit.>.\n    \n    \n  * \u201cInstance\" denotes that K-means is directly performed over all the instance features in training set, since each instance can be regarded as a bag with length of one.\n\nThen, interventional training is applied to baseline methods (including ABMIL and DSMIL) and we report the results in <ref>. \n\nNotably, even with such simple aggregation strategy, IBMIL still outperforms the baseline, and remains competitive or even better compared to  \u201cDefault\" setting, which indicates stage 2 in our scheme is unnecessary. By omitting stage 2, our scheme can be more elegant without performance degradation in most cases.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan IBMIL improve non-parametric baselines?\nBesides using non-parametric aggregators to generate bag features for confounder set, we further take them as baselines and verify whether IBMIL is also able to improve them (i.e., max/mean-pooling).\nSurprisingly, in\u00a0<ref>, IBMIL brings significant improvements under all settings, where the best performance is even comparable to these attention-based aggregators.\nIt indicates that IBMIL is indeed compatible with all compared MIL methods, including the non-parametric ones.\n\n\n\nConstituting  confounder set w/  or w/o bag labels?\n\n\n\nGiven bag labels, we explore the class-specific  K-means. In particular,\nwe  apply K-means to each class respectively, preserving the intra-class variation and class-relevant characteristics.\nFrom\u00a0<ref>, we observe no obvious performance gap between class-specific and class-agnostic K-means. \nWe conjecture that 1) the confounders could be independent of the class identity, and 2) bag features are already separable by classes. We will explore the way of incorporating bag labels in future work.\nOn the other hand, the unsupervised fashion makes our scheme compatible with large-scale unlabelled datasets. We explore more unlabelled bags via  combining the bags of TCGA and Camelyon16, and constituting the confounder set via the non-parametric aggregators.\nFrom\u00a0<ref>, we observe a clear improvement on AUC under both max- and mean-poolings.\nThat indicates, with more bags, our implementation can achieve better approximation of confounders.\n\n\nIs IBMIL just post-processing?\n\nSince our proposed IBMIL shares some commonalities with the embedding-based MIL\u00a0<cit.>, one may ask: Do the improvements only come from exploring the bag relations?\nTo answer this question, we make minor modifications on ABMIL. Instead of interventional training with confounders, we obtain the confounder dictionary via the class-specific  K-means and treat it as a KNN classifier for evaluation. \n\n\nAs can be seen, it  brings limited improvements or even degrades the performance, verifying the improvements comes from interventional training, which is not just post-processing.\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSIONS\n\nThe vast majority of recent efforts in this field seek to enhance the feature extractor and aggregator. This paper addresses MIL from a novel perspective via analyzing the confounders between bags and labels. This leads to the proposed novel Interventional Bag Multi-Instance Learning (IBMIL), a new deconfounded bag-level prediction approach to suppress the bias caused by the bag contextual prior. IBMIL introduces a structure causal model to reveal the causalities and eliminates their effect through the backdoor adjustment with practical implementations. Comprehensive experiments have been conducted on various MIL benchmarks and the results show that IBMIL can boost existing methods significantly. In future, we plan to approximate confounder set in a more efficient and elegant manner. As a general method to use causal intervention for bag-level prediction, IBMIL provides fresh insight into MIL problem.\n\nAcknowledgments.  \nDr. Yi Xu was supported in part by NSFC 62171282, Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), 111 project BP0719010, and SJTU Science and Technology Innovation Special Fund ZH2018ZDA17 and YG2022QN037. \n\n\n\n\nieee_fullname\n\n\n\n"}