{"entry_id": "http://arxiv.org/abs/2303.06857v1", "published": "20230313050234", "title": "An automated pipeline to create an atlas of in situ hybridization gene expression data in the adult marmoset brain", "authors": ["Charissa Poon", "Muhammad Febrian Rachmadi", "Michal Byra", "Matthias Schlachter", "Binbin Xu", "Tomomi Shimogori", "Henrik Skibbe"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\nContinuous-Time Zeroth-Order Dynamics with Projection Maps: Model-Free Feedback Optimization with Safety Guarantees\n    Xin Chen, Jorge I. Poveda,  Na Li \nXin Chen is with the MIT Energy Initiative at the Massachusetts Institute of Technology, MA, USA. E-mail: xinch512@mit.edu. Jorge I. Poveda is with ECE Department, University of California, San Diego, CA, USA. E-mail: poveda@ucsd.edu. Na Li is with the School of Engineering and Applied Sciences at Harvard University, MA, USA. E-mail: nali@seas.harvard.edu.\n  This work was supported in part by NSF CAREER: ECCS 2305756, NSF CAREER: ECCS-1553407, NSF EAGER: ECCS-1839632, and AFOSR grant FA9550-22-1-0211.\n\n\n    Received: date / Accepted: date\n===================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\nWe present the first automated pipeline to create an atlas of in situ hybridization gene expression in the adult marmoset brain in the same stereotaxic space. The pipeline consists of segmentation of gene expression from microscopy images and registration of images to a standard space. Automation of this pipeline is necessary to analyze the large volume of data in the genome-wide whole-brain dataset, and to process images that have varying intensity profiles and expression patterns with minimal human bias. To reduce the number of labelled images required for training, we develop a semi-supervised segmentation model. We further develop an iterative algorithm to register images to a standard space, enabling comparative analysis between genes and concurrent visualization with other datasets, thereby facilitating a more holistic understanding of primate brain structure and function.\n\n\n\ncontrastive learning, gene atlas, segmentation, semi-supervised learning, registration\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nCharacterization of gene expression in the brain is necessary to understand brain structure and function. Cellular diversity in the brain points at the need to characterize gene expression at single-cell resolution. Gene expression brain atlases in lower-order model organisms have led to better understanding of anatomical structures and cell types based on spatial expression patterns of genes. However, interspecies differences limits the extrapolation of findings to the human brain. The common marmoset (Callithrix jacchus) exhibits human-like social traits, a fast reproductive cycle, and has proven to be amenable to genetic manipulation, characteristics that make it a candidate model organism for primate research. \n\nThe gene-atlas.brainminds.jp/Marmoset Gene Atlas, created by the Brain/MINDS project in Japan, is an in situ hybridization (ISH) database of gene expression in the neonate and adult marmoset brain <cit.>. Characterization of neonate marmoset ISH gene expression images led to the discovery of regional- and species-specific patterns of gene expression in the developing marmoset brain <cit.>. However, like other existing atlases <cit.>, segmentation of ISH gene expression was conducted manually <cit.>. Manual methods are susceptible to human bias and error and not feasible for characterizing gene expression on a whole-brain, genome-wide, multi-age level. Furthermore, existing marmoset brain atlases lack transcriptomic data such as the ISH dataset (e.g. <cit.>).\n\nOur goal is to develop an automated pipeline to create a gene expression atlas from ISH images, consisting of binary segmentations of gene expression from ISH images, registered to a standard space. We describe the image preprocessing, segmentation, and registration steps to achieve this for the adult marmoset brain (Figure <ref>). Segmentation of gene expression is necessary to clearly define areas of expression; true positive pixels are often difficult to discern in ISH images due to great variability in image contrast between images and in expression patterns between genes. We develop a semi-supervised deep learning segmentation model due to their superior performance over fully-supervised models in biomedical segmentation tasks despite fewer training labels <cit.>. Registration of ISH images is difficult to achieve because each gene has a unique expression pattern. Thus, we additionally develop an automated iterative algorithm that utilizes the Advanced Normalization Tools (ANTS) toolbox <cit.> to register brain images to the Brain/MINDS Marmoset Connectivity Atlas (BMCA) template <cit.>, to which neuronal tracer data, fiber tractography data, and anatomical labels have already been registered. Integration of the ISH dataset to the BMCA standard space will add transcriptomic data, facilitating a more holistic understanding of the marmoset brain. To our knowledge, this is the first report of automating the integration of marmoset ISH data into a standard space. Our code is publicly available: <https://github.com/BrainImageAnalysis/MarmosetGeneAtlas_adult/MarmosetGeneAtlas_adult>.\n\n\n\n\n\n\n\u00a7 METHODOLOGY\n\n\n\nData acquisition was conducted by the Laboratory for Molecular Mechanisms of Brain Development at the RIKEN Center for Brain Science <cit.>. We describe the image analysis pipeline. \n\n\n\n \u00a7.\u00a7 Preprocessing\n\n\nData preprocessing consisted of downscaling, filtering, and morphological operations to remove artifacts. Metadata and data were reorganized to be in a machine-readable format.\n\n\n\n \u00a7.\u00a7 Segmentation\n\n\nTo train the model, 3D image stacks of ISH gene expression from 14 genes (2470 2D images), were used in a 7:3 split for training and validation. To evaluate the model, 3D image stacks of ISH gene expression from five genes (520 2D images), which were separate from the training and validation datasets, were used. Ground truth segmentations were manually generated by an expert (CP).\n\nThe model was based on a 2D U-Net <cit.>, consisting of three levels (Figure <ref>). Each level in the encoder consisted of 2D convolution, batch normalization, and LeakyReLU layers. The number of features were doubled every step. In the decoder, 2D convolutions were replaced with 2D transposed convolutions. A sigmoid was applied to the output of the decoder. Input image patches were 400x400 pixels. \n\n\n\nThe model was trained using the Adam optimization method and two losses, the supervised binary cross-entropy loss (Lsupervised) and the unsupervised contrastive loss (Lcontrastive).\n\nThe contrastive loss, previously described by Oord et al. <cit.> and Chen et al. <cit.>, shown in Equation <ref>, calculates the loss between positive pairs of samples by maximizing agreement between features (z) of two augmented views of the same image patch (positive pair: i,j). In Equation <ref>, \u03c4 is a temperature parameter and \ud835\udd40_k\u22621 is an indicator function. We used augmentations that were optimized by Chen et al. <cit.>: ColorJitter, RandomGrayscale, and GaussianBlur (Torchvision library). These augmentations vary the image contrast, brightness, hue, and saturation; parameters which already differ between images, and one reason why segmentation of this dataset difficult. The contrastive loss maximizes the agreement of image patches on the basis of image content, regardless of differences in colour profile and contrast. The contrastive loss was applied on features from the bottleneck layer of the model which were projected through a multilayer perceptron with one hidden layer (see <cit.> for details). \n\nTo train the model with both losses, skip connections were excluded to avoid leakage. We used a batch size of 16, which produced 30 negative samples for every positive pair. Code was written in PyTorch and PyTorch Lightning. Training was conducted using one NVIDIA A100 GPU.\n\n\n\n    l_i,j=-logexp(sim(z_i,z_j)/\u03c4)/\u2211_k=1^2N\ud835\udd40_k\u22621exp(sim(z_i,z_j)/\u03c4)\n\n\n\nWe additionally trained a second version of the model, which was pretrained using the unsupervised contrastive loss only (model w/ pretraining in Evaluation). The dataset used for pretraining contained 186 unlabelled images.\n\n\n\n \u00a7.\u00a7 Registration\n\n\n\n\n\n\n\nWe created an iterative algorithm using the ANTs Toolbox <cit.> that creates a 3D brain image by automatically aligning and stacking the brain images to recover the original shape of the subject's brain, followed by registration to the BMCA reference marmoset brain template (Figure <ref>). \n\nTo achieve the first stage of registration, blockface (BF) and backlit (BL) images were obtained during image acquisition (Figure <ref>). Blockface images are photos of the brain tissue before sectioning, and therefore show the shape of the brain with minimal spatial deformations. Backlit images are microscopy images of brain slices mounted onto slides (i.e. after sectioning), but prior to ISH or Nissl processing, and therefore are less spatially deformed than ISH and Nissl images. A reconstruction of the entire brain was achieved by concatenating the blockface images <cit.>. This blockface reconstruction was used as the first reference point to reconstruct a 3D image stack of backlit images (Figure <ref>). In the second step, we used the backlit image stack to align all ISH gene expression images. In the third step, we mapped all ISH gene expression image stacks to the BMCA template.\n\n\nFor the reconstruction, we used an iterative algorithm that optimized the following objective function.\n\n\n    T\u0302^\u0302\u00ee=Tamin\u00a0a\u2112(BL^i_j\u2218 T\u2218 T^k,BL^(i-1)_j)\n       +b\u2112(BL^(i-1)_j\u2218 T \u2218 T^k,BL^i_j)\n       +c\u2112(BL^i_j\u2218 T \u2218 T^k,BL^(i-1)_(j-1))\n       +c\u2112(BL^i_j\u2218 T \u2218 T^k,BL^(i-1)_(j+1)).\n\n\n\n\nLet BL^i be the 3D backlit image stack after the i-th iteration, and let BL^i_j the j-th 2D image section. BF and BF_j are the blockface image stack and its sections, respectively. Figure <ref> shows an overview of the backlit registration process. In the first step, ANTs's affine registration was used to map each 2D backlit section to its corresponding blockface section. The objective function is T\u0302^\u03020\u0302=Tamin\u00a0\u2112(BL^0_j\u2218 T,BF^0_j), where T is the objective, an affine transformation, and \u2112 is the normalized mutual information. After optimization, the process generated BL^1:=BL^1 * G(\u03c3), a Gaussian smoothed 3D image of the aligned image stack, with \u03c3=3 being the filter width. The next iterations used BL^1 as the target image instead of the blockface image. In addition, we aimed for a smooth transition between neighbouring image sections. Therefore, three additional terms were added to the objective function; see Equation (<ref>). The terms favor similarity with the previous iteration of the same section, but also with its predecessor and successor. We heuristically found that a=1, b=0.5, and c=0.5 worked best. T^k is the transformation from a  previous iteration. Until iteration three, T^k is the previous affine registration T^k=T^i-1. From iteration 3, we used deformable registration (SyN) instead of the affine registration, and set a=0, b=1, and c=0.25. T^k=T^2 was kept constant between iteration 3 and 6 to suppress high frequency artifacts from large non-linear deformations in the first SyN iterations. \n\nThe next step was done separately for each gene. Each gene's images were registered to the newly created 3D backlit image stack. This was achieved in three steps. Since for each ISH section, there exists a corresponding backlit image, affine image registration was used to pre-align each ISH section to its corresponding backlit counterpart. Two additional SyN iterations were used to reconstruct the ISH 3D image stacks. The loss function in the last two iterations was similar to (<ref>), where  BL^(i-1)_j, BL^(i-1)_(j-1) and BL^(i-1)_(j+1) were replaced with their ISH counterparts. \n\nIn the final step, a 3D affine and 3D SyN registration were applied to map the 3D backlit image, and therefore the ISH images, to the BMCA 3D marmoset brain reference space.\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 EVALUATION\n\n\n\nTo evaluate the segmentation model, model outputs with and without pretraining (model w/ pretraining and model) were compared to ground truth segmentations (gt), two other human-generated sources (thresholded, manual), and one other machine-generated source (unet), summarized below:\n\n\n  \n  * gt: ground-truth, manually generated by CP\n  \n  * thresholded: thresholded images, thresholds were manually set for each image by CP\n  \n  * manual: manually generated by five other annotators (MFR, MB, MS, BX, HS) to evaluate the consistency among human annotators\n  \n  * model: our model without pretraining\n  \n  * model w/ pretrain: our model with pretraining\n  \n  * unet: fully-supervised vanilla 2D three-level UNet\n\n\n\nQuantitatively, segmentations were evaluated using the Dice score; we report the mean and standard deviation in Table <ref>. Our model outperformed all other methods by a wide margin. High standard deviations observed in human-generated segmentations (thresholded* and manual*), and overall low Dice scores (<0.5) show the difficulty in segmenting gene expression from ISH images due to variations in expression patterns between genes and differences in image contrast even for images obtained from the same marmoset. High standard deviation observed in model w/ pretraining segmentations can likely be improved with longer pretraining and optimization of augmentations. \n\n\n\n\n\n\n\n\n\n\nA sample of segmentations are shown in Figure <ref>. Qualitatively, it can be seen manual* segmentations performed the worst (see row 4, where all methods segmented the correct structure except for manual*).\n\n\n\n\n\n \u00a7.\u00a7 Automated stack alignment\n\n\n\n\n\nTo assess the quality of 3D stack alignment, we defined seven landmarks in the reference template of the marmoset brain: (single points unless indicated otherwise): anterior commissure, anterior thalamus, midline, dorsal tip of the anterior cingulate cortex (CC), posterior commissure of the midbrain (MB, two points), subthalamic nucleus (STN, two points), and the intersection of the anterior limb of the internal capsule and the anterior commissure (intersection ALIC/AC, two points). For each ISH image stack, three experts manually placed the landmarks. For comparison, landmarks were automatically mapped based on the transformation fields generated by the image registration pipeline. The smaller the displacement between a pair of landmarks manually placed by two different annotators, the better the agreement. The same comparison was done between manual landmarks and automatically mapped landmarks. The median match between manual annotations and automation was compared to the best match between two human annotations, which gave an advantage to human annotations. Figure <ref> shows the scores sorted by displacement, shown in units of 100 \u03bcm. In this scenario, automation could maintain the performance of manual methods. Of note, if we took the median displacement between manually placed landmarks as well, automation outperformed manual methods for all landmarks. \n\n\n\n\u00a7 CONCLUSION\n\nWe describe the novel development of an automated pipeline to integrate adult marmoset gene expression data into a standard space. Quantitative and qualitative evaluations showed that the unsupervised contrastive loss improved segmentation of ISH gene expression. We expect that pretraining with a greater number of unlabelled images and optimizing augmentation parameters for the ISH dataset will improve performance. High standard deviation in human-generated segmentations show the unreliability of manual labelling. Comparison of registration annotations between automation and manual methods revealed that automation also performed on par with humans. We plan to explore deep learning registration methods to improve registration <cit.>, as well as other segmentation models. This automated pipeline can be used to process and integrate data from different imaging modalities for co-visualization and comparative analyses.\n\n\n\n\u00a7 COMPLIANCE WITH ETHICAL STANDARDS\n\n\nThis research study was conducted retrospectively using marmoset imaging data made available in open access at https://gene-atlas.brainminds.riken.jp/. The use of marmosets followed the guidelines of and were approved by the RIKEN Institutional Animal Care Committee, described in <cit.>.\n\n\n\n\u00a7 ACKNOWLEDGMENTS\n\n\nThis work was supported by the Japan AMED (JP15dm0207001) and the Japan Society for the Promotion of Science. All authors state no potential conflicts of interest.\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\n\nIEEEbib\n\n\n\n"}