{"entry_id": "http://arxiv.org/abs/2303.09280v2", "published": "20230313124432", "title": "Topology optimization with physics-informed neural networks: application to noninvasive detection of hidden geometries", "authors": ["Saviz Mowlavi", "Ken Kamrin"], "primary_category": "cs.CE", "categories": ["cs.CE", "cs.AI", "cs.LG"], "text": "\n\nAPS/123-QED\n\n\n\n\n mowlavi@merl.com\nDepartment of Mechanical Engineering, MIT, Cambridge, MA 02139, USA\nMitsubishi Electric Research Laboratories, Cambridge, MA 02139, USA\n\n kkamrin@mit.edu\nDepartment of Mechanical Engineering, MIT, Cambridge, MA 02139, USA\n\n\n\nDetecting hidden geometrical structures from surface measurements under electromagnetic, acoustic, or mechanical loading is the goal of noninvasive imaging techniques in medical and industrial applications. Solving the inverse problem can be challenging due to the unknown topology and geometry, the sparsity of the data, and the complexity of the physical laws.\n\nPhysics-informed neural networks (PINNs) have shown promise as a simple-yet-powerful tool for problem inversion, but they have yet to be applied to general problems with a priori unknown topology.  \n\nHere, we introduce a topology optimization framework based on PINNs that solves geometry detection problems without prior knowledge of the number or types of shapes. We allow for arbitrary solution topology by representing the geometry using a material density field that approaches binary values thanks to a novel eikonal regularization. \nWe validate our framework by detecting the number, locations, and shapes of hidden voids and inclusions in linear and nonlinear elastic bodies using measurements of outer surface displacement from a single mechanical loading experiment.\n\nOur methodology opens a pathway for PINNs to solve various engineering problems targeting geometry optimization.\n\n\nTopology optimization with physics-informed neural networks: \n application to noninvasive detection of hidden geometries\n    Ken Kamrin\n    March 30, 2023\n========================================================================================================================\n\n\n\n\n\n\nNoninvasive detection of hidden geometries is desirable in countless applications including medical imaging and diagnosis <cit.>, nondestructive evaluation of materials <cit.>, and mine detection <cit.>. The goal is to infer the locations, and shapes of structures hidden inside a matrix from surface measurements of the response to an applied load such as a magnetic field <cit.>, an electric current <cit.>, or a mechanical traction <cit.>. \nIdentifying these internal boundaries from the measured data constitutes a challenging inverse problem due to the unknown topology, the large number of parameters required to describe arbitrary geometries <cit.>, the potential sparsity of the data, and the complexity of the underlying physical laws which usually take the form of linear or nonlinear partial differential equations (PDEs) <cit.>.\n\nIn recent years, physics-informed neural networks (PINNs) have emerged as a robust tool for problem inversion across disciplines and over a range of model complexity <cit.>. PINNs' ability to seamlessly blend measurement data or design objectives with governing PDEs in nontrivial geometries has enabled practitioners to solve easily a range of inverse problems involving identification or design of unknown properties in fields ranging from mechanics to optics and medicine <cit.>. Encouraged by these early successes, we introduce in this paper a general topology optimization (TO) framework to solve noninvasive geometry detection problems using PINNs, leveraging both the measurements and the governing PDEs. Building on the strength of PINNs, our approach is straightforward to implement regardless of the complexity of the physical model, produces accurate results using measurement data from a single experiment, and does not require a training dataset. To the best of our knowledge, the present work marks the first time that PINNs have been applied to problems involving a priori unknown topology and geometry.\n\nClassical approaches to solve geometry identification inverse problems tend to be complex as they combine traditional numerical solvers such as the finite-element or boundary-element method, adjoint techniques to evaluate the sensitivity of the error residual with respect to the shape or the topology, and gradient descent-based optimization algorithms to update the geometry at every iteration <cit.>. \nFurthermore, these techniques call for carefully chosen regularization schemes and do not always yield satisfactory results, especially in the presence of sparse measurements acquired by only one or a few sets of experiments. For example, in the mechanical loading case where \n\nvoids and inclusions in an elastic body are to be identified from measurements of surface displacement in response to a prescribed traction, past studies limit themselves to simple shapes like squares and circles or fail to find the right number of shapes <cit.>.\nAttempts to apply PINNs to geometry detection are in their infancy, so far restricted to cases where the number and shape-type of hidden voids or inclusions in an elastic structure are provided in advance <cit.>.\n\n\n\n\n\n\n\nOur PINN-based TO framework does not require any prior knowledge on the number and types of shapes. We allow for arbitrary solution topology by representing the geometry using a material density field equal to 0 in one phase and 1 in the other. The material density is parameterized through a neural network, which needs to be regularized in order to push the material density towards 0 or 1 values. Thus, one key ingredient in our framework is a novel eikonal regularization, inspired from fast-marching level-set methods <cit.> and neural signed distance functions <cit.>, that promotes a constant thickness of the interface region where the material density transitions between 0 and 1, leading to well-defined boundaries throughout the domain. \nThis eikonal regularization enters as an additional term in the standard PINN loss, which is then used to train the neural networks underlying the material density and physical quantities to yield a solution to the geometry detection problem.\n\nAs an illustration, we apply our framework to cases involving an elastic body under mechanical loading, and discover the topology, locations, and shapes of hidden structures for a variety of geometries and materials.\n\n\n\n\n\n\u00a7 RESULTS\n \n\n\n\n \u00a7.\u00a7 Problem formulation\n\n\nWe consider noninvasive geometry detection problems of the following form. Suppose we have a continuous body \u212c containing an unknown number of hidden voids or inclusions, with unknown shapes and at unknown locations within the body. The material properties are assumed to be known and homogeneous within the body and the inclusions. We then apply a certain type of loading (e.g.\u00a0mechanical, thermal, acoustic, etc) on the body's external boundary \u2202\u212c^ext, which produces a response within the body that can be described by a set of n physical quantities (e.g.\u00a0displacements, stresses, temperature, etc). These physical quantities can be lumped into a vector field \u03c8 : \u212c\u2192\u211d^n and satisfy a known set of governing PDEs.\n\n\n\n \n The goal of the inverse problem is to identify the number, locations, and shapes of the voids or inclusions based on measurements along \u2202\u212c^ext of some of the physical quantities contained in \u03c8.\n\nAs a concrete example, we consider two prototypical plane-strain elasticity inverse problems.\n\n\n\nIn the first case, a square elastic matrix with hidden voids or inclusions is pulled by a uniform traction P_o on two sides (Fig.\u00a0<ref>a). The goal of the inverse problem is to identify the number, locations, and shapes of the voids or inclusions using discrete measurements of the displacement of the outer boundary of the matrix. In the second case, an elastic layer on top of a hidden rigid substrate is compressed from the top by a uniform pressure P_o, with periodic lateral boundary conditions (Fig.\u00a0<ref>b). The goal is to identify the shape of the substrate using discrete measurements of the displacement of the top surface. For both cases, the constitutive properties of all materials are assumed to be known. We will consider two different types of constitutive laws: compressible linear elasticity, which characterizes the small deformation of any compressible elastic material, and incompressible nonlinear hyperelasticity, which models the large deformation of rubber-like materials. In the linear elastic case, there exists a unique solution to the inverse problem (see proof in Supplementary Information), making it well-suited to evaluating the accuracy of our TO framework.\n\nFollowing density-based TO methods <cit.>, we avoid any restriction on the number and shapes of hidden structures by parameterizing the geometry of the elastic body \u212c through a discrete-valued material density function \u03c1 : \u03a9\u2192{0,1}, where \u03a9 is a global domain comprising both \u212c and the hidden voids or inclusions. The material density is defined to be equal to 1 in the elastic body \u212c and 0 in the voids or inclusions. \nThe physical quantities \u03c8 can then be extended to the global domain \u03a9 by introducing an explicit \u03c1-dependence in their governing PDEs, leading to equations of the form \n\n\n    \ud835\udc2b(\u03c8(\ud835\udc31),\u03c1(\ud835\udc31))    = 0,   \ud835\udc31\u2208\u03a9, \n    \ud835\udc1b(\u03c8(\ud835\udc31))    = 0,   \ud835\udc31\u2208\u2202\u03a9,\n\n\nwith known boundary conditions defined solely on the external boundary \u2202\u03a9 = \u2202\u212c^ext. Note that the residual functions \ud835\udc2b and \ud835\udc1b may contain partial derivatives of \u03c8 and \u03c1. The inverse problem is now to find the distribution of material density \u03c1 in \u03a9 so that the corresponding solution for \u03c8 matches surface measurements \u03c8_i^m at discrete locations \ud835\udc31_i \u2208\u2202\u03a9^m \u2282\u2202\u03a9, that is,\n\n    \u03c8(\ud835\udc31_i) = \u03c8_i^m,   \ud835\udc31_i \u2208\u2202\u03a9^m.\n\nIn practice, we might only measure select quantities in \u03c8 at some of the locations, but we do not write so explicitly to avoid overloading the notation.\n\nFor the linear elasticity problem that we consider as an example, \u03c8 = (\ud835\udc2e,\u03c3) where \ud835\udc2e(\ud835\udc31) and \u03c3(\ud835\udc31) are displacement and stress fields, respectively, and the governing equations comprise equilibrium relations \u2211_j\u2202\u03c3_ij/\u2202 x_j = 0 and a constitutive law F(\u03c3, \u2207\ud835\udc2e, \u03c1) = 0, both defined over \u03a9. The presence of \u03c1 in the constitutive law specifies different material behaviors for the elastic solid phase and the void or rigid inclusion phase. The applied boundary conditions take the form \ud835\udc2e = \ud835\udc2e\u0305 on \u2202\u03a9_u and \u03c3\ud835\udc27 = \ud835\udc2d\u0305 on \u2202\u03a9_t, where \u2202\u03a9_u and \u2202\u03a9_t are partitions of the external boundary with applied displacements and applied tractions, respectively, and \ud835\udc27 is the outward unit normal. In the case of the elastic layer, the outer boundary also comprises a portion \u2202\u03a9_p with periodic boundary conditions on the displacement and traction. Finally, the requirement that the predictions for \ud835\udc2e at the surface match the measurement data is expressed as \ud835\udc2e(\ud835\udc31_i) = \ud835\udc2e_i^m, \ud835\udc31_i \u2208\u2202\u03a9^m. See Methods for a detailed formulation of the governing equations and boundary conditions for all considered cases.\n\nSimilar to density-based TO methods <cit.>, we relax the binary constraint on the material density by allowing intermediate values of \u03c1 between 0 and 1. This renders the problem amenable to gradient-based optimization, which underpins the PINN-based TO framework that we introduce in the next section. However, the challenge is to find an appropriate regularization mechanism that drives the optimized material distribution towards 0 and 1 rather than intermediate values devoid of physical meaning. As we will show in the discussion, common strategies employed in TO <cit.> do not yield satisfactory results in our PINN-based framework for geometry detection problems. Thus, we have developed a novel eikonal regularization scheme inspired from level-set methods and signed distance functions, which we will describe after presenting the general framework.\n\n\n\n\n\n \u00a7.\u00a7 General framework\n\n\n\n\n\n\n\nWe propose a TO framework based on PINNs for solving noninvasive geometry detection problems (Fig.\u00a0<ref>). \n\nAt the core of the framework are several deep neural networks that approximate the physical quantities \u03c8(\ud835\udc31) describing the problem and the material density \u03c1(\ud835\udc31). For the physical quantities, each neural network maps the spatial location \ud835\udc31 = (x_1,x_2) to one of the variables in \u03c8 = (\u03c8_1, \u22ef, \u03c8_n); this can be expressed as \u03c8_i = \u03c8\u0305_i(\ud835\udc31; \u03b8_i) where \u03c8\u0305_i is the map defined by the ith neural network and its trainable parameters \u03b8_i (see Methods).\n\n\nFor the material distribution, we first define a neural network with trainable parameters \u03b8_\u03d5 that maps \ud835\udc31 to a scalar variable \u03d5 = \u03d5\u0305(\ud835\udc31; \u03b8_\u03d5). A sigmoid function is then applied to \u03d5 to yield \u03c1 = sigmoid(\u03d5/\u03b4) = sigmoid(\u03d5\u0305(\ud835\udc31; \u03b8_\u03d5)/\u03b4), which we simply write as \u03c1 = \u03c1\u0305(\ud835\udc31,\u03b8_\u03d5). This construction ensures that the material density \u03c1 remains between 0 and 1, and \u03b4 is a transition length scale that we will comment on later. We define the phase transition to occur at \u03c1 = 0.5 so that the zero level-set of \u03d5 delineates the boundary between the two material phases, hence \u03d5 is hereafter referred to as a level-set function <cit.>.\n\n\nWe now seek the parameters \u03b8_\u03c8 = {\u03b8_1, \u2026, \u03b8_n} and \u03b8_\u03d5 so that the neural network approximations for \u03c8(\ud835\udc31) and \u03c1(\ud835\udc31) satisfy the governing equations (<ref>) and applied boundary conditions (<ref>) while matching the surface measurements (<ref>). This is achieved by constructing a loss function of the form\n\n    \u2112(\u03b8_\u03c8, \u03b8_\u03d5)    = \u03bb_meas\u2112_meas(\u03b8_\u03c8) + \u03bb_gov\u2112_gov(\u03b8_\u03c8, \u03b8_\u03d5) \n          + \u03bb_reg\u2112_eik(\u03b8_\u03d5),\n\nwhere \u2112_meas and \u2112_gov measure the degree to which the neural network approximations do not satisfy the measurements and governing equations, respectively, \u2112_eik is a crucial regularization term that drives \u03c1 towards 0 or 1 values and that we will explain below, and the \u03bb's are scalar weights. The measurement loss takes the form\n\n    \u2112_meas(\u03b8_\u03c8) = 1/|\u2202\u03a9^m|\u2211_\ud835\udc31_i \u2208\u2202\u03a9^m |\u03c8\u0305(\ud835\udc31_i;\u03b8_\u03c8)-\u03c8_i^m|^2,\n\nwhere |\u2202\u03a9^m| denotes the size of the set \u2202\u03a9^m. A trivial modification of this expression is necessary in the case where only select quantities in \u03c8 are measured. The governing equations loss takes the form\n\n    \u2112_gov(\u03b8_\u03c8, \u03b8_\u03d5) = 1/|\u03a9^d|\u2211_\ud835\udc31_i \u2208\u03a9^d |\ud835\udc2b(\u03c8\u0305(\ud835\udc31_i;\u03b8_\u03c8),\u03c1\u0305(\ud835\udc31_i;\u03b8_\u03d5))|^2,\n\nwhere \u03a9^d is a set of collocation points in \u03a9, and we use automatic differentiation to calculate in a mesh-free fashion the spatial derivatives contained in \ud835\udc2b. We design the architecture of our neural networks in such a way that they inherently satisfy the boundary conditions (see Methods, section \u201cDetailed PINNs formulation\u201d).\n\n\n\n\n\n\n\n\nFinally, the optimal parameters \u03b8_\u03c8^* and \u03b8_\u03d5^* that solve the problem can be obtained by training the neural networks to minimize the loss (<ref>) using stochastic gradient descent-based optimization. The corresponding physical quantities \u03c8\u0305(\ud835\udc31;\u03b8_\u03c8^*) will match the discrete surface measurements while satisfying the governing equations of the problem, while the corresponding material density \u03c1\u0305(\ud835\udc31;\u03b8_\u03d5^*) will reveal the number, locations, and shapes of the hidden voids or inclusions.\n\n\n\n \u00a7.\u00a7 Material density regularization\n\n\n\nWe now describe the key ingredient that ensures the success of our framework. As mentioned above, the main challenge is to promote the material density \u03c1(\ud835\udc31) to converge towards 0 or 1 away from the material phase boundaries, given by the zero level-set \u03d5 = 0. Moreover, we desire the thickness of the transition region along these boundaries, where \u03c1 goes from 0 to 1, to be uniform everywhere in order to ensure consistency of physical laws across the interface (e.g.\u00a0stress jumps). \n\nTo visualize what happens in the absence of regularization, consider a random instance of the neural network \u03d5 = \u03d5\u0305(\ud835\udc31,\u03b8_\u03d5) (Fig.\u00a0<ref>a, left) and the corresponding material distribution \u03c1 = sigmoid(\u03d5/\u03b4) with \u03b4 = 0.01 (Fig.\u00a0<ref>a, center).\n\n\n\nThe sigmoid transformation ensures that \u03c1 never drops below 0 or exceeds 1, leading to large regions corresponding to one phase or the other. However, the thickness of the transition region where \u03c1 goes from 0 to 1 is not everywhere uniform, resulting in large zones where \u03c1 assumes nonphysical values between 0 and 1 (Fig.\u00a0<ref>a, center). This behavior stems from the non-uniformity of the gradient norm |\u2207\u03d5| along the material boundaries \u03d5 = 0, with small and large values of |\u2207\u03d5| leading to wide and narrow transition regions, respectively (Fig.\u00a0<ref>a, right).\n\nWe propose to regularize the material density by forcing the gradient norm |\u2207\u03d5| to be unity in a narrow band \u03a9_eik of width w along the material boundaries defined by the zero level-set \u03d5 = 0. In this way, \u03d5 becomes a signed distance function to the material boundary in the narrow band, thereby constraining the gradient of \u03c1 to be constant along the interface. To ensure that the narrow band covers the near-entirety of the transition region where \u03c1 goes from 0 to 1, we choose w = 10 \u03b4 so that \u03c1 = sigmoid(\u00b1 w/2\u03b4) = sigmoid(\u00b1 5) \u2243 0 or 1 along the edge of the narrow band. To illustrate the effect of such regularization, we consider the previous random instance of the neural network \u03d5 = \u03d5\u0305(\ud835\udc31,\u03b8_\u03d5) and enforce the constraint |\u2207\u03d5| = 1 in the narrow band \u03a9_eik along its zero level-set (Fig.\u00a0<ref>b, left and right). The zero level-set is kept fixed to facilitate comparison with the unregularized case (Fig.\u00a0<ref>a). With \u03d5 now behaving like a signed-distance function in the narrow band, a uniform transition thickness for \u03c1 along all material boundaries is achieved, without large regions of intermediate density values (Fig.\u00a0<ref>b, center).\n\nIn practice, we implement this regularization into our PINN-based TO framework by including an `eikonal' loss term \u2112_eik in (<ref>), which takes the form\n\n    \u2112_eik(\u03b8_\u03d5) = 1/|\u03a9_eik^d|\u2211_\ud835\udc31_i \u2208\u03a9_eik^d(| \u2207\u03d5 (\ud835\udc31_i)| - 1 )^2,\n\nwhere \u03a9_eik^d = {\ud835\udc31_i \u2208\u03a9^d : |\u03d5(\ud835\udc31_i)| < w/2}. The aim of this term is to penalize deviations away from the constraint |\u2207\u03d5| = 1 in the narrow band \u03a9_eik of width w along the interface defined by the zero level-set \u03d5 = 0. Because finding the subset of collocation points \ud835\udc31_i in \u03a9^d belonging to the true narrow band of width w at every step of the training process would be too expensive, we instead relax the domain over which the constraint |\u2207\u03d5| = 1 is active by utilizing the subset \u03a9_eik^d of collocation points that satisfy |\u03d5(\ud835\udc31_i)| < w/2. As the constraint |\u2207\u03d5| = 1 is progressively better satisfied during the training process, \u03a9_eik^d will eventually overlap the true narrow band of width w along the zero level-set of \u03d5 (Fig.\u00a0<ref>c). \n\nSince the constraint |\u2207\u03d5| = 1 in the narrow band takes the form of an eikonal equation, we call this approach eikonal regularization. We emphasize that in contrast to recent works training neural networks to solve the eikonal equation <cit.>, our eikonal regularization does not force \u03d5 to vanish on a predefined boundary. Rather, the zero level-set of \u03d5 evolves freely during the training process in such a way that the corresponding material distribution \u03c1 = sigmoid(\u03d5/\u03b4) and physical quantities \u03c8 minimize the total loss (<ref>), eventually revealing the material boundaries delineating the hidden voids or inclusions. \n\n\n\n \u00a7.\u00a7 Setup of numerical experiments\n\n\n\nWe evaluate our TO framework on a range of challenging test cases involving different numbers and shapes of hidden structures and various materials (Methods, Tabs.\u00a0<ref> and <ref>). As a substitute for real experiments, we use the finite-element method (FEM) software Abaqus to compute the deformed shape of the boundary of the elastic structure and generate the measurement data for each case (Methods, section \u201cFEM simulations\u201d). Using this measurement data, we run our TO framework to discover the number, locations, and shapes of the hidden voids or rigid inclusions (for implementation and training details, see Methods, section \u201cArchitecture and training details\u201d). We then compare the obtained results with the ground truth \u2014 the voids or inclusions originally fed into Abaqus \u2014 to assess the efficacy of our framework.\n\n\n\n \u00a7.\u00a7 Elastic matrix experiments\n\n\n\n\n\n\n\nWe first apply our framework to cases involving a linear elastic matrix  (Fig.\u00a0<ref>a) containing voids (cases 1, 3, 8, 10, 15, 17 in Methods, Tab.\u00a0<ref>). As the various loss components are minimized during training (Fig.\u00a0<ref>a), the material density \u03c1 evolves and splits in a way that progressively reveals the number, locations, and shapes of the hidden voids (Extended Data Fig.\u00a0<ref> and https://www.dropbox.com/s/tm26buzgn0w35s1/Movie1.mp4?dl=0Supplementary Movie 1), without advance knowledge of their topology. By the end of the training, the transition regions where the material density goes from 0 to 1 have uniform thickness along all internal boundaries (Fig.\u00a0<ref>d), thanks to the eikonal regularization that encourages the level-set gradient \u2207\u03d5 to have unit norm in a band along the material boundaries \u03d5 = 0 (Fig.\u00a0<ref>b,c). The agreement between the final inferred shapes and the ground truth is remarkable, with our framework able to recover intricate details such as the three lobes and the concave surfaces of the star-shaped void (Fig.\u00a0<ref>d, second from left), or the exact aspect ratio and location of a thin slit (Fig.\u00a0<ref>d, third from left). The stress and strain fields of the deformed matrix are also obtained as a byproduct of the solution process (Extended Data Fig.\u00a0<ref>).\n\nThe only case that is not completely identified is the U-shaped void (Fig.\u00a0<ref>d, first from right), a result of the miniscule influence of the inner lobe on the outer surface displacements due to its low level of strain and stress (Extended Data Fig.\u00a0<ref>). Finally, our framework maintains accurate results when reducing the number of surface measurement points or restricting measurements to a few surfaces (Extended Data Figs.\u00a0<ref>-<ref>).\n\nNext, we consider cases involving linear elastic and rigid inclusions in the linear elastic matrix (cases 4, 5, 6, 11, 12, 13 in Methods, Tab.\u00a0<ref>). Our framework successfully identifies the inclusions in almost all cases (Fig.\u00a0<ref>e). Inferred displacements and stresses of the deformed matrix (Extended Data Fig.\u00a0<ref>) confirm the intuition that voids or soft inclusions soften the matrix while stiff or rigid inclusions harden the matrix. The U-shaped soft and stiff elastic inclusions (Fig.\u00a0<ref>e, second and third from right) are better detected by the framework than their void or rigid counterparts (Fig.\u00a0<ref>d, first from right and Fig.\u00a0<ref>e, first from right), since an elastic inclusion induces some strain and stress on the inner lobe (Extended Data Fig.\u00a0<ref>).\n\nFinally, we consider cases involving a soft, incompressible Neo-Hookean hyperelastic matrix with the same void shapes considered previously (cases 2, 7, 9, 14, 16, 18 in Methods, Tab.\u00a0<ref>). \nThe geometries are identified equally well (Fig.\u00a0<ref>f) in this large deformation regime (Extended Data Fig.\u00a0<ref>) as with linear elastic materials, which illustrates the ability of the framework to cope with nonlinear governing equations without any added complexity in the formulation or the implementation.\n\n\n\n \u00a7.\u00a7 Elastic layer experiments\n\n\n\nWe finally apply our framework to the periodic elastic layer (Fig.\u00a0<ref>b), where a linear elastic material covers a hidden rigid substrate (cases 20, 21, 22 in Methods, Tab.\u00a0<ref>).\n\n\n\nContrary to the matrix problem, this setup only provides access to measurements on the top surface, and the hidden geometry to be discovered is not completely surrounded by the elastic material. Our TO framework is nevertheless able to detect the correct depths and shapes of the hidden substrates (Fig.\u00a0<ref>). This example demonstrates the versatility of the framework in adapting to various problem setups.\n\n\n\n\u00a7 DISCUSSION\n\n\n\n\n\n\nAs with any TO method relying on a material density field to parameterize the geometry, the success of our PINN-based framework hinges on the presence of an appropriate regularization mechanism to penalize intermediate density values. Although we have shown that our novel eikonal regularization leads to consistently accurate results, other regularization approaches have been employed in classical adjoint-based TO methods <cit.>. These include the total variation dimishing (TVD) regularization <cit.> that penalizes the L_1 norm of the density gradient \u2207\u03c1, the explicit penalization regularization <cit.> that penalizes the integral over the domain of \u03c1 (1-\u03c1), and the Solid Isotropic Material with Penalization (SIMP) approach <cit.> that relates material properties such as the shear modulus and the material density through a power-law with exponent p. The latter is the most popular regularization mechanism in structural optimization <cit.>. However, when implemented in our PINN-based framework for the detection of hidden geometries, these methods yield inferior results to the eikonal regularization (Fig.\u00a0<ref>). Indeed, we compare all four approaches on a challenging test case involving a linear elastic rectangular matrix pulled from the top and bottom and containing soft inclusions in the shape of the letters M, I, and T (case 19, Tab.\u00a0<ref>). The measurements consist of the displacement along the outer boundary, similar to the previous square matrix examples. We consider different values of the regularization weight \u03bb_reg (for the eikonal, TVD and explicit penalization regularizations) and the exponent p (for the SIMP regularization), and solve the inverse problem using four random initializations of the neural networks in each case. Not only was the eikonal regularization the only one to find the right shapes, it did so over three orders of magnitude of \u03bb_reg, demonstrating a desirable robustness with respect to \u03bb_reg (Fig.\u00a0<ref>).\n\n\n\n\n\n\nThanks to the flexibility of the PINN framework, adapting our approach to various scenarios involving partial information, three-dimensional geometries, or other types of noninvasive imaging experiments (e.g.\u00a0using thermal <cit.>, acoustic <cit.>, electric <cit.>, or magnetic <cit.> loading) should be straightforward. As an illustration, we identify a hidden inclusion in a nonlinearly conducting matrix using partially unknown thermal loading (Supplementary Information), mimicking an inaccessible surface. This experiment reveals our method's ability to generate good results without further modifications even when the forward problem is ill-posed, which would require including the unknown boundary condition as an additional optimization variable in a classical adjoint-based approach.\n\nIn conclusion, we have presented a PINN-based TO framework with a novel eikonal regularization, which we have applied to the noninvasive detection of hidden inclusions. By representing the geometry through a material density field combined with a novel eikonal regularization, our framework is able to discover the number, shapes and locations of hidden structures, without any prior knowledge required regarding the number or the types of shapes to expect.\n\nFinally, the idea of parameterizing geometries of arbitrary topologies with a material density field regularized with the eikonal constraint opens a pathway for PINNs to be applied to a wide range of design optimization problems constrained by physical governing equations. These include, for instance, the design of lenses that achieve targeted optical properties <cit.> or the design of structures and metamaterials that exhibit desirable mechanical, acoustic, or thermal properties <cit.>.\n\n\n\n\n\n\u00a7 METHODS\n\n\n\n\n \u00a7.\u00a7 Governing equations\n\n\n\nThe two plane-strain elasticity inverse problems considered in this study (Fig.\u00a0<ref>) are defined in a two-dimensional domain \u03a9\u2282\u211d^2 formed by the union of the elastic body \u212c and the voids or inclusions. Denoting with \ud835\udc31 = (x_1,x_2) \u2208\u03a9 the planar spatial coordinates, the hidden geometrical layout of voids or inclusions is characterized by a material density \u03c1(\ud835\udc31) equal to 1 in the body \u212c and 0 in the voids or inclusions.\n\n\n\n  \u00a7.\u00a7.\u00a7 Small-deformation linear elasticity\n \n\n\nWe first consider the case where the elastic body and inclusions consist of linear elastic materials, with Young's modulus E and Poisson's ratio \u03bd for the body, and Young's modulus E\u0305 and Poisson's ratio \u03bd\u0305 for the inclusions. Voids and rigid inclusions correspond to the limits E\u0305\u2192 0 and E\u0305\u2192\u221e, respectively. The deformation of the elastic body containing the inclusions is described by a vector field \u03c8(\ud835\udc31) = (\ud835\udc2e(\ud835\udc31), \u03c3(\ud835\udc31)), where \ud835\udc2e(\ud835\udc31) is a planar displacement field with components u_i(\ud835\udc31) and \u03c3(\ud835\udc31) is a Cauchy stress tensor with components \u03c3_ij(\ud835\udc31). Indices i and j will hereafter always range from 1 to 2. \n\nThe governing PDEs comprise the equilibrium equations\n\n    \u2211_j\u2202\u03c3_ij/\u2202 x_j = 0,   \ud835\udc31\u2208\u03a9,\n\nas well as a linear elastic constitutive law F(\u03c3, \u2207\ud835\udc2e, \u03c1) = 0 that we will express in two different but equivalent ways, depending on whether the inclusions are softer or stiffer than the matrix. For voids and soft inclusions, we consider the constitutive law in stress-strain form,\n\n    \u03c3   = \u03c1[ \u03bb tr (\u03f5)  \ud835\udc08 + 2 \u03bc \u03f5], \n          (1 - \u03c1) [ \u03bb\u0305 tr (\u03f5)  \ud835\udc08 + 2 \u03bc\u0305 \u03f5],   \ud835\udc31\u2208\u03a9,\n\nwhere \u03f5 = (\u2207\ud835\udc2e + \u2207\ud835\udc2e^T)/2 is the infinitesimal strain tensor, tr(\u03f5) denotes its trace, \u03bb = E \u03bd/(1+\u03bd)(1-2\u03bd) and \u03bc = E/2(1+\u03bd) are the Lam\u00e9 constants of the body, and \u03bb\u0305 = E\u0305\u03bd\u0305/(1+\u03bd\u0305)(1-2\u03bd\u0305) and \u03bc\u0305 = E\u0305/2(1+\u03bd\u0305) are the Lam\u00e9 constants of the inclusions. Notice that the case of voids, the stress vanishes in the \u03c1 = 0 regions. For stiff and rigid inclusions, \n\nwe consider the constitutive law in the inverted strain-stress form\n\n    \u03f5   = \u03c1[ 1+\u03bd/E\u03c3 - \u03bd(1+\u03bd)/E tr (\u03c3)  \ud835\udc08] \n          (1-\u03c1) [ 1+\u03bd\u0305/E\u0305\u03c3 - \u03bd\u0305(1+\u03bd\u0305)/E\u0305 tr (\u03c3)  \ud835\udc08],   \ud835\udc31\u2208\u03a9,\n\nwhere tr(\u03c3) is the trace of the stress tensor. This relation differs from the three-dimensional one due to the plane strain assumption. Notice that the case of rigid inclusions, the strain vanishes in the \u03c1 = 0 regions.\n\n\n\n\n\n\n\n\n\n\n\nThe boundary conditions on \u2202\u03a9 and surface displacement measurement locations \u2202\u03a9^m are different in the two problems. For the elastic matrix (Fig.\u00a0<ref>a), the domain is \u03a9 = [-0.5,0.5] \u00d7 [-0.5,0.5] and the boundary conditions are\n\n\n\u03c3(\ud835\udc31) \ud835\udc27(\ud835\udc31)    = -P_o \ud835\udc1e_1,         \ud835\udc31 \u2208{-0.5,0.5} \u00d7[-0.5,0.5], \n\n\u03c3(\ud835\udc31) \ud835\udc27(\ud835\udc31)    = 0,         \ud835\udc31 \u2208[-0.5,0.5] \u00d7{-0.5,0.5}.\n \n\nThe measurement locations \u2202\u03a9^m are distributed along the entire external boundary \u2202\u03a9. In the case of the M, I, T inclusions (case 19, Tab.\u00a0<ref>), the boundary conditions (<ref>) are changed to account for the fact that the matrix is pulled from the top and bottom boundaries and covers the domain \u03a9 = [-1,1] \u00d7 [-0.5,0.5]. For the elastic layer (Fig.\u00a0<ref>b), the domain is \u03a9 = [0,1] \u00d7 [-0.5,0] and the boundary conditions are\n\n\n\u03c3(\ud835\udc31) \ud835\udc27(\ud835\udc31)    = -P_o \ud835\udc1e_2,          \ud835\udc31 \u2208[0,1] \u00d7{0}, \n\n\ud835\udc2e    = 0,        \ud835\udc31 \u2208[0,1] \u00d7{-0.5},\n \n\nas well as periodic for the displacement and traction on \ud835\udc31\u2208{0,1}\u00d7 [-0.5,0]. The measurement locations \u2202\u03a9^m are distributed along the top surface \u2202\u03a9_t = [0,1] \u00d7{0}.\n\nThe geometry identification problem that we solve can then be stated as follows. Given surface displacement measurements \ud835\udc2e_i^m at locations \ud835\udc31_i \u2208\u2202\u03a9^m, find the distribution of material density \u03c1 in \u03a9 such that the difference between the predicted and measured surface displacements vanish, that is,\n\n    \ud835\udc2e(\ud835\udc31_i) = \ud835\udc2e_i^m,   \ud835\udc31_i \u2208\u2202\u03a9^m.\n\nThe predicted displacement field must satisfy the equilibrium equation (<ref>), the constitutive relation (<ref>) or (<ref>), and the boundary conditions (<ref>) or (<ref>).\n\n\n\n  \u00a7.\u00a7.\u00a7 Large-deformation nonlinear hyperelasticity\n \n\n\nNext, we consider the case where the elastic body consists of an incompressible Neo-Hookean hyperelastic material with shear modulus \u03bc.\nWe now have to distinguish between the reference (undeformed) and current (deformed) configurations. \nWe denote by \ud835\udc31 = (x_1,x_2) \u2208\u03a9 and \ud835\udc32 = (y_1,y_2) \u2208\u03a9^* the coordinates in the reference and deformed configurations, respectively, with \u03a9^* the deformed image of \u03a9. \nThe displacement field \ud835\udc2e(\ud835\udc31) with components u_i(\ud835\udc31) moves an initial position \ud835\udc31\u2208\u03a9 into its current location \ud835\udc32 = \ud835\udc31 + \ud835\udc2e(\ud835\udc31) \u2208\u03a9^*. \nIn order to formulate the governing equations and boundary conditions in the reference configuration \u03a9, we need to introduce the first Piola-Kirchhoff stress tensor \ud835\udc12(\ud835\udc31) with components S_ij(\ud835\udc31). Unlike the Cauchy stress tensor, the first Piola-Kirchhoff stress tensor is defined in \u03a9 and is not symmetric. The deformation of the elastic body is then described by the vector field \u03c8(\ud835\udc31) = (\ud835\udc2e(\ud835\udc31), \ud835\udc12(\ud835\udc31),p(\ud835\udc31)) defined over \u03a9, where p(\ud835\udc31) is a pressure field that serves to enforce the incompressibility constraint.\n\nThe equilibrium equations are\n\n    \u2211_j\u2202 S_ij/\u2202 x_j = 0,   \ud835\udc31\u2208\u03a9,\n\nwhere the derivatives in \u2207_\ud835\udc31 are taken with respect to the reference coordinates \ud835\udc31. We only consider the presence of voids so that the nonlinear constitutive law F(\ud835\udc12, \u2207_\ud835\udc31\ud835\udc2e, p, \u03c1) = 0 is simply expressed as\n\n    \ud835\udc12 = \u03c1[ -p \ud835\udc05^-T + \u03bc\ud835\udc05],   \ud835\udc31\u2208\u03a9,\n\nwhere \ud835\udc05(\ud835\udc31) = \ud835\udc08 + \u2207_\ud835\udc31\ud835\udc2e(\ud835\udc31) is the deformation gradient tensor. Notice that the stress vanishes in the \u03c1 = 0 regions. Finally, we have the incompressibility constraint\n\n    \u03c1[ (\ud835\udc05) - 1 ] = 0,   \ud835\udc31\u2208\u03a9,\n\nwhich turns itself off in the \u03c1 = 0 regions since voids do not deform in a way that preserves volume.\n\nWe only treat the matrix problem (Fig.\u00a0<ref>a) in this hyperelastic case. The domain is \u03a9 = [-0.5,0.5] \u00d7 [-0.5,0.5] and the boundary conditions are\n\n\n\ud835\udc12(\ud835\udc31) \ud835\udc27_0(\ud835\udc31)    = -P_o \ud835\udc1e_1,         \ud835\udc31 \u2208{-0.5,0.5} \u00d7[-0.5,0.5], \n\n\ud835\udc12(\ud835\udc31) \ud835\udc27_0(\ud835\udc31)    = 0,         \ud835\udc31 \u2208[-0.5,0.5] \u00d7{-0.5,0.5}.\n \n\nAs in the linear elastic case, the measurement locations \u2202\u03a9^m are distributed along the entire external boundary \u2202\u03a9.\n\nThe geometry identification problem can then be stated identically as in the linear elastic case. This time, the predicted displacement field must satisfy the equilibrium equation (<ref>), the constitutive relation (<ref>), the incompressibility condition (<ref>), and the boundary conditions (<ref>).\n\n\n\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Rescaling\n \n\n\nThe various physical quantities involved in the elasticity inverse problem span a wide range of scales; for instance, displacements may be orders of magnitude smaller than the length scale associated with the geometry. Thus, we rescale all physical quantities into nondimensional values of order one, as also done in <cit.>. Lengths are rescaled with the width L of the elastic matrix or elastic layer, tractions and stresses with the magnitude P_o of the applied traction at the boundaries, and displacements with the ratio L P_o / E, where E is the Young's modulus of the elastic material (in the hyperelastic case, we use the equivalent Young's modulus E = 3 \u03bc, where \u03bc is the shear modulus of the hyperelastic material). This rescaling is critical to enable the neural networks underlying our framework to handle elasticity problems across a wide range of material moduli and applied loads.\n\n\n\n \u00a7.\u00a7 Solution methodology\n\n\n\nHere, we describe in detail the application of our TO framework to the solution of the two plane-strain elasticity inverse problems formulated in the introduction. We will treat separately the small-deformation linear elasticity case and the large-deformation hyperelasticity case.\n\n\n\n  \u00a7.\u00a7.\u00a7 Small-deformation linear elasticity\n\n\nSince the problem is described by the physical quantities \u03c8 = (u_1,u_2,\u03c3_11,\u03c3_22,\u03c3_12), we introduce the neural network approximations\n\n\n    u_1(\ud835\udc31)    = u\u0305_1(\ud835\udc31; \u03b8_1), \n    \n    u_2(\ud835\udc31)    = u\u0305_2(\ud835\udc31; \u03b8_2), \n    \u03c3_11(\ud835\udc31)    = \u03c3\u0305_11(\ud835\udc31; \u03b8_3), \n    \u03c3_22(\ud835\udc31)    = \u03c3\u0305_22(\ud835\udc31; \u03b8_4), \n    \u03c3_12(\ud835\udc31)    = \u03c3\u0305_12(\ud835\udc31; \u03b8_5), \n    \u03d5(\ud835\udc31)    = \u03d5\u0305(\ud835\udc31; \u03b8_\u03d5).\n \n\nThe last equation represents the level-set neural network, which defines the material density as \u03c1(\ud835\udc31) = \u03c1\u0305(\ud835\udc31;\u03b8_\u03d5) = sigmoid(\u03d5\u0305(\ud835\udc31;\u03b8_\u03d5)/\u03b4). We then formulate the loss function (<ref>) by specializing the loss term expressions presented in results section to the linear elasticity problem. Omitting the \u03b8's for notational simplicity, we obtain\n\n\n    \u2112_meas(\u03b8_\u03c8)    = 1/|\u2202\u03a9^m|\u2211_\ud835\udc31_i \u2208\u2202\u03a9^m  |\ud835\udc2e\u0305(\ud835\udc31_i) - \ud835\udc2e_i^m|^2, \n    \u2112_gov(\u03b8_\u03c8, \u03b8_\u03d5)    = 1/|\u03a9^d|\u2211_\ud835\udc31_i \u2208\u03a9^d |\ud835\udc2b_eq(\u03c3\u0305(\ud835\udc31_i))|^2 \n          + 1/|\u03a9^d|\u2211_\ud835\udc31_i \u2208\u03a9^d |\ud835\udc2b_cr(\ud835\udc2e\u0305(\ud835\udc31_i),\u03c3\u0305(\ud835\udc31_i),\u03c1\u0305(\ud835\udc31_i))|^2, \n    \u2112_eik(\u03b8_\u03d5)    = 1/|\u03a9_eik^d|\u2211_\ud835\udc31_i \u2208\u03a9_eik^d(| \u2207\u03d5\u0305 (\ud835\udc31_i)| - 1 )^2,\n\n\nwhere \ud835\udc2e\u0305 = (u\u0305_1,u\u0305_2) and \u03c3\u0305 has components \u03c3\u0305_i,j, i,j=1,2. In (<ref>), the terms \ud835\udc2b_eq and \ud835\udc2b_cr refer to the residuals of the equilibrium equation (<ref>) and the constitutive relation (<ref>) or (<ref>). The eikonal loss term is problem-independent and therefore identical to (<ref>). \n\nWe note that instead of defining neural network approximations for the displacements and the stresses, we could define neural network approximations solely for the displacements, that is, \u03c8 = (u_1,u_2). In this case, the loss term (<ref>) would only include the residual of the equilibrium equation (<ref>), in which the stress components would be directly expressed in terms of the displacements and the material distribution using the constitutive relation (<ref>). However, several recent studies <cit.> have shown that the mixed formulation adopted in the present work results in superior accuracy and training performance, which could partly be explained by the fact that only first-order derivatives of the neural network outputs are involved since the displacements and stresses are only differentiated to first oder in (<ref>) and (<ref>). In our case, the mixed formulation holds the additional advantage that it enables us to treat stiff and rigid inclusions using the inverted constitutive relation (<ref>) instead of (<ref>). Finally, the mixed formulation allows us to directly integrate both displacement and traction boundary conditions into the output of the neural network approximations, as we describe in the next paragraph.\n\nWe design the architecture of the neural networks in such a way that they inherently satisfy the boundary conditions, treating the latter as hard constraints <cit.>. For the elastic matrix, we do this through the transformations\n\n\n    u\u0305_1(\ud835\udc31; \u03b8_1)    = u\u0305_1'(\ud835\udc31; \u03b8_1), \n    u\u0305_2(\ud835\udc31; \u03b8_2)    = u\u0305_2'(\ud835\udc31; \u03b8_2), \n    \u03c3\u0305_11(\ud835\udc31; \u03b8_3)    = (x-0.5)(x+0.5)  \u03c3\u0305_11'(\ud835\udc31; \u03b8_3) + P_o, \n    \u03c3\u0305_22(\ud835\udc31; \u03b8_4)    = (y-0.5)(y+0.5)  \u03c3\u0305_22'(\ud835\udc31; \u03b8_4), \n    \u03c3\u0305_12(\ud835\udc31; \u03b8_5)    = (x-0.5)(x+0.5) \u00b7\n          (y-0.5)(y+0.5)  \u03c3\u0305_12'(\ud835\udc31; \u03b8_5), \n    \u03d5\u0305(\ud835\udc31; \u03b8_\u03d5)    = (x-0.5)(x+0.5) \u00b7\n          (y-0.5)(y+0.5)  \u03d5\u0305'(\ud835\udc31; \u03b8_\u03d5) + w,\n\n\nwhere the quantities with a prime denote the raw output of the neural network. In this way, the neural network approximations defined in (<ref>) obey by construction the boundary conditions (<ref>). Further, since we know that the elastic material is present all along the outer surface \u2202\u03a9, we define \u03d5\u0305 so that \u03d5 = w on \u2202\u03a9, which ensures that \u03c1 = sigmoid(\u03d5/\u03b4) \u2243 1 on \u2202\u03a9 (recall that w is such that sigmoid(w/2\u03b4) \u2243 1). In the case of the M, I, T inclusions, these transformations are changed to reflect the fact that the matrix is wider and pulled from the top and bottom. For the periodic elastic layer, we introduce the transformations\n\n\n    u\u0305_1(\ud835\udc31; \u03b8_1)    = (y+0.5)  u\u0305_1'(cos x, sin x, y; \u03b8_1), \n    u\u0305_2(\ud835\udc31; \u03b8_2)    = (y+0.5)  u\u0305_2'(cos x, sin x, y; \u03b8_2), \n    \u03c3\u0305_11(\ud835\udc31; \u03b8_3)    = \u03c3\u0305_11'(cos x, sin x, y; \u03b8_3), \n    \u03c3\u0305_22(\ud835\udc31; \u03b8_4)    = y  \u03c3\u0305_22'(cos x, sin x, y; \u03b8_4) - P_o, \n    \u03c3\u0305_12(\ud835\udc31; \u03b8_5)    = y  \u03c3\u0305_12'(cos x, sin x, y; \u03b8_5), \n    \u03d5\u0305(\ud835\udc31; \u03b8_\u03d5)    = y(y+0.5)  \u03d5\u0305'(cos x, sin x, y; \u03b8_\u03d5) \n          + w(4y+1),\n\n\nso that the neural network approximations defined in (<ref>) obey by construction the boundary conditions (<ref>) and are periodic along the x direction. Further, since we know that the elastic material is present all along the top surface y = 0 and the rigid substrate is present all along the bottom surface y = -0.5, we define \u03d5\u0305 so that \u03d5 = w for y = 0 and \u03d5 = -w for y=-0.5, which ensures that \u03c1 = sigmoid(\u03d5/\u03b4) \u2243 1 for y = 0 and \u03c1\u2243 0 for y=-0.5.\n\n\n\n  \u00a7.\u00a7.\u00a7 Large-deformation hyperelasticity\n\n\nThe problem is now described by the physical quantities \u03c8 = (u_1,u_2,S_11,S_22,S_12,S_21,p). We therefore introduce the neural network approximations\n\n\n    u_1(\ud835\udc31)    = u\u0305_1(\ud835\udc31; \u03b8_1), \n    \n    u_2(\ud835\udc31)    = u\u0305_2(\ud835\udc31; \u03b8_2), \n    \n    S_11(\ud835\udc31)    = S\u0305_11(\ud835\udc31; \u03b8_3), \n    \n    S_22(\ud835\udc31)    = S\u0305_22(\ud835\udc31; \u03b8_4), \n    \n    S_12(\ud835\udc31)    = S\u0305_12(\ud835\udc31; \u03b8_5), \n    \n    S_21(\ud835\udc31)    = S\u0305_21(\ud835\udc31; \u03b8_6), \n    \n    p(\ud835\udc31)    = p\u0305(\ud835\udc31; \u03b8_7), \n    \u03d5(\ud835\udc31)    = \u03d5\u0305(\ud835\udc31; \u03b8_\u03d5),\n \n\nand the material distribution is given by \u03c1(\ud835\udc31) = \u03c1\u0305(\ud835\udc31;\u03b8_\u03d5) = sigmoid(\u03d5\u0305(\ud835\udc31;\u03b8_\u03d5)/\u03b4). We then formulate the loss function (<ref>) by specializing the loss term expressions presented in Section <ref> to the linear elasticity problem, using the governing equations given in Appendix <ref>. Omitting the \u03b8's for notational simplicity, we obtain\n\n\n    \u2112_meas(\u03b8_\u03c8)    = 1/|\u2202\u03a9^m|\u2211_\ud835\udc31_i \u2208\u2202\u03a9^m  |\ud835\udc2e\u0305(\ud835\udc31_i) - \ud835\udc2e_i^m|^2, \n    \u2112_gov(\u03b8_\u03c8, \u03b8_\u03d5)    = 1/|\u03a9^d|\u2211_\ud835\udc31_i \u2208\u03a9^d |\ud835\udc2b_eq(\ud835\udc12\u0305(\ud835\udc31_i))|^2 \n         + 1/|\u03a9^d|\u2211_\ud835\udc31_i \u2208\u03a9^d |\ud835\udc2b_cr(\ud835\udc2e\u0305(\ud835\udc31_i),\ud835\udc12\u0305(\ud835\udc31_i),p\u0305(\ud835\udc31_i),\u03c1\u0305(\ud835\udc31_i))|^2 \n         + 1/|\u03a9^d|\u2211_\ud835\udc31_i \u2208\u03a9^d |\ud835\udc2b_inc(\ud835\udc2e\u0305(\ud835\udc31_i),\u03c1\u0305(\ud835\udc31_i))|^2, \n    \u2112_eik(\u03b8_\u03d5)    = 1/|\u03a9_eik^d|\u2211_\ud835\udc31_i \u2208\u03a9_eik^d(| \u2207\u03d5\u0305 (\ud835\udc31_i)| - 1 )^2,\n\n\nwhere \ud835\udc2e\u0305 = (u\u0305_1,u\u0305_2) and \ud835\udc12\u0305 has components S\u0305_i,j, i,j=1,2. In (<ref>), the terms \ud835\udc2b_eq, \ud835\udc2b_cr, and \ud835\udc2b_inc refer to the residuals of the equilibrium equation (<ref>), the constitutive relation (<ref>), and the incompressibility constraint (<ref>). The eikonal loss term is problem-independent and therefore identical to (<ref>). \n\nAs in the linear elasticity case, we design the architecture of the neural networks in such a way that they inherently satisfy the boundary conditions. For the elastic matrix problem,\n\n\n    u\u0305_1(\ud835\udc31; \u03b8_1)    = u\u0305_1'(\ud835\udc31; \u03b8_1), \n    u\u0305_2(\ud835\udc31; \u03b8_2)    = u\u0305_2'(\ud835\udc31; \u03b8_2), \n    S\u0305_11(\ud835\udc31; \u03b8_3)    = (x-0.5)(x+0.5)  S\u0305_11'(\ud835\udc31; \u03b8_3) + P_o, \n    S\u0305_22(\ud835\udc31; \u03b8_4)    = (y-0.5)(y+0.5)  S\u0305_22'(\ud835\udc31; \u03b8_4), \n    S\u0305_12(\ud835\udc31; \u03b8_5)    = (y-0.5)(y+0.5)  S\u0305_12'(\ud835\udc31; \u03b8_5), \n    S\u0305_21(\ud835\udc31; \u03b8_6)    = (x-0.5)(x+0.5)  S\u0305_21'(\ud835\udc31; \u03b8_6), \n    p\u0305(\ud835\udc31; \u03b8_7)    = p\u0305'(\ud835\udc31; \u03b8_7), \n    \u03d5\u0305(\ud835\udc31; \u03b8_\u03d5)    = (x-0.5)(x+0.5) \u00b7\n          (y-0.5)(y+0.5)  \u03d5\u0305'(\ud835\udc31; \u03b8_\u03d5) + w,\n\n\nwhere the quantities with a prime denote the raw output of the neural network. In this way, the neural network approximations defined in (<ref>) obey by construction the boundary conditions of the problem. As before, since we know that the elastic material is present all along the outer surface \u2202\u03a9, we define \u03d5\u0305 so that \u03d5 = w on \u2202\u03a9, which ensures that \u03c1 = sigmoid(\u03d5/\u03b4) \u2243 1 on \u2202\u03a9.\n\n\n\n \u00a7.\u00a7 Architecture and training details\n\n\n\nHere, we provide implementation details regarding the architecture of the deep neural networks, the training procedure and corresponding parameter values.\n\n\n\n  \u00a7.\u00a7.\u00a7 Neural network architecture\n\n\n\nState variable fields of the form \u03c8(\ud835\udc31) are approximated using deep fully-connected neural networks that map the location \ud835\udc31 to the corresponding value of \u03c8 at that location. This map can be expressed as \u03c8(\ud835\udc31) = \u03c8\u0305(\ud835\udc31;\u03b8), and is defined by the sequence of operations\n\n\n    \ud835\udc33^0    = \ud835\udc31, \n    \ud835\udc33^k    = \u03c3(\ud835\udc16^k \ud835\udc33^k-1 + \ud835\udc1b^k),    1 \u2264 k \u2264\u2113-1, \n    \u03c8 = \ud835\udc33^\u2113   = \ud835\udc16^\u2113\ud835\udc33^\u2113-1 + \ud835\udc1b^\u2113.\n\n\nThe input \ud835\udc31 is propagated through \u2113 layers, all of which (except the last) take the form of a linear operation composed with a nonlinear transformation. Each layer outputs a vector \ud835\udc33^k \u2208\u211d^q_k, where q_k is the number of `neurons', and is defined by a weight matrix \ud835\udc16^k \u2208\u211d^q_k \u00d7 q_k-1, a bias vector \ud835\udc1b^k \u2208\u211d^q_k, and a nonlinear activation function \u03c3(\u00b7). Finally, the output of the last layer is assigned to \u03c8. The weight matrices and bias vectors, which parametrize the map from \ud835\udc31 to \u03c8, form a set of trainable parameters \u03b8 = {\ud835\udc16^k,\ud835\udc1b^k}_k=1^\u2113.\n\nThe choice of the nonlinear activation function \u03c3(\u00b7) and the initialization procedure for the trainable parameters \u03b8 are both important factors in determining the performance of neural networks. While the tanh function has been a popular candidate in the context of PINNs <cit.>, recent works by Refs.\u00a0<cit.> have shown that using sinusoidal activation functions can lead to improved training performance by promoting the emergence of small-scale features. In this work, we select the sinusoidal representation network (SIREN) architecture from Ref.\u00a0<cit.>, which combines the use of the sine as an activation function with a specific way to initialize the trainable parameters \u03b8 that ensures that the distribution of the input to each sine activation function remains unchanged over successive layers. Specifically, each component of  \ud835\udc16^k is uniformly distributed between - \u221a(6/q_k) and \u221a(6/q_k) where q_k is the number of neurons in layer k, and \ud835\udc1b^k = 0, for k=1, \u2026, \u2113. Further, the first layer of the SIREN architecture is \ud835\udc33^1 = \u03c3(\u03c9_0 \ud835\udc16^1 \ud835\udc33^0 + \ud835\udc1b^1) instead of (<ref>), with the extra scalar \u03c9_0 promoting higher-frequency content in the output.\n\n\n\n  \u00a7.\u00a7.\u00a7 Training procedure\n\n\nWe construct the total loss function (<ref>) and train the neural networks in TensorFlow 2. The training is performed using ADAM, a first-order gradient-descent-based algorithm with adaptive step size <cit.>. In each case, we repeat the training over four random initializations of the neural networks parameters and report the best results. Three tricks resulted in noticeably improved training performance and consistency:\n\n\n  * First, we found that pretraining the level-set neural network \u03d5(\ud835\udc31) = \u03d5\u0305(\ud835\udc31; \u03b8_\u03d5) in a standard supervised setting leads to much more consistent results over different initializations of the neural networks. During this pretraining step, carried out before the main optimization step in which all neural networks are trained to minimize the loss (<ref>), we minimize the mean-square error \n\n    \u2112_sup(\u03b8_\u03d5) = 1/|\u03a9^d|\u2211_\ud835\udc31_i \u2208\u03a9^d |\u03d5\u0305(\ud835\udc31_i; \u03b8_\u03d5) - \u03d5_i |,\n\nwhere \u03a9^d is the same set of collocation points as in (<ref>), the supervised labels \u03d5_i = |\ud835\udc31_i| - 0.25 for the elastic matrix, and \u03d5_i = y_i + 0.25 for the elastic layer. The material density \u03c1\u0305(\ud835\udc31;\u03b8_\u03d5) = sigmoid(\u03d5\u0305(\ud835\udc31;\u03b8_\u03d5)/\u03b4) obtained at the end of this pretraining step is one outside a circle of radius 0.25 centered at the origin for the elastic matrix, and it is one above the horizontal line y = -0.25 for the elastic layer. This choice for the supervised labels is justified by the fact that \u03c1 is known to be one along the outer boundary of the domain \u03a9 for the elastic matrix, and it is known to be one (zero) along the top (bottom) boundary of \u03a9 for the elastic layer. \n\n  * Second, during the main optimization in which all neural networks are trained to minimize the loss (<ref>), we evaluate the loss component \u2112_gov in (<ref>) using a different subset, or mini-batch, of residual points from \u03a9^d at every iteration. Such a mini-batching approach has been reported to improve the convergence of the PINN training process <cit.>, corroborating our own observations. In our case, we choose to divide the set \u03a9^d into 10 different mini-batches of size |\u03a9^d|/10, which are then employed sequentially to evaluate \u2112_gov during each subsequent gradient update\n\n\n    \u03b8_\u03c8^k+1   = \u03b8_\u03c8^k - \u03b1_\u03c8(k) \u2207_\u03b8_\u03c8\u2112(\u03b8_\u03c8^k, \u03b8_\u03d5^k), \n    \u03b8_\u03d5^k+1   = \u03b8_\u03d5^k - \u03b1_\u03d5(k) \u2207_\u03b8_\u03d5\u2112(\u03b8_\u03c8^k, \u03b8_\u03d5^k).\n \n\nAn epoch of training, which is defined as one complete pass through the whole set \u03a9^d, therefore consists of 10 gradient updates.\n\n  * Third, the initial nominal step size \u03b1_\u03c8 governing the learning rate of the physical quantities neural networks is set to be 10 times larger than its counterpart \u03b1_\u03d5 governing the learning rate of the level-set neural network. This results in a separation of time scales between the rate of change of the physical quantities neural networks and that of the level-set neural network, which is motivated by the idea that physical quantities should be given time to adapt to a given geometry before the geometry itself changes.\n \n\n\n\n  \u00a7.\u00a7.\u00a7 Parameter values\n\n\nThe parameter values described below apply to all results presented in this paper.\n\n\n\n  * Neural network architecture. For all cases except the M, I, T inclusions (case 19, Tab.\u00a0<ref>), we opted for neural networks with 4 hidden layers of 50 neurons each, which we found to be a good compromise between expressivity and training time. For the M, I, T inclusions, we used 6 hidden layers with 100 neurons each. Further, we choose \u03c9_0 = 10 as the scalar appearing in the first layer of the SIREN architecture.\n\n  * Collocation and measurements points. In the square and rectangle elastic matrix problems, we consider that the boundary displacement is measured along each of the four external boundaries at 100 equally-spaced points, which amounts to |\u2202\u03a9^m| = 400. In the elastic layer problem, we consider that the boundary displacement is measured along the top boundary at 100 equally-spaced points, which amounts to |\u2202\u03a9^m| = 100. For both geometries except the M, I, T inclusions, the set of collocation points \u03a9^d consists of 10000 points distributed in \u03a9 with a Latin Hypercube Sampling (LHS) strategy, yielding 10 mini-batches containing 1000 points each. For the M, I, T inclusions, \u03a9^d consists of 50000 points, yielding 50 mini-batches containing 1000 points each.\n\n  * Training parameters. The pretraining of the level-set neural network is carried out using the ADAM optimizer with nominal step size 10^-3 over 800 training epochs, employing the whole set \u03a9^d to compute the gradient of \u2112_sup at each update step. The main optimization, during which all neural networks are trained to minimize the total loss (<ref>), is carried out using the ADAM optimizer. For the matrix cases except the M, I, T inclusions, we use a total of 150k training epochs starting from a nominal step size 10^-4 for the level-set neural network and 10^-3 for the other neural networks. This step size is reduced to 10^-4 for all neural networks at 60k epochs, and again to 10^-5 at 120k epochs. The schedule is the same for the elastic layer cases, with the difference that we use a total of 200k training epochs. For the matrix case with the M, I, T inclusions, we use a total of 50k epochs (note that each epoch contains 5 times as many mini-batches as in the other cases) starting from a nominal step size 10^-4 for the level-set neural network and 10^-3 for the other neural networks. This step size is reduced to 10^-4 for all neural networks at 16k epochs, and again to 10^-5 at 40k epochs. Finally, the scalar weights in the loss (<ref>) are assigned the values \u03bb_meas = 10, \u03bb_gov = 1, and \u03bb_reg = 1 for all cases. We also multiply the second term of \u2112_gov in (<ref>) and (<ref>) with a scalar weight \u03bb_cr = 10.\n\n\n\n\n \u00a7.\u00a7 FEM simulations\n\n\n\nThe FEM simulations that provide the boundary displacement data and the ground truth are performed in the software Abaqus, using its Standard (implicit) solver. The list of all cases considered in provided in Tab.\u00a0<ref> for the elastic matrix setup (Fig.\u00a0<ref>a) and in Tab.\u00a0<ref> for the periodic elastic layer setup (Fig.\u00a0<ref>b). Every case is meshed using a linear density of 200 elements per unit length along each boundary, corresponding to between 25k to 80k total elements depending on domain size as well as number and shapes of voids or inclusions. We employ bilinear quadrilateral CPE4 plain-strain elements for the cases involving a linear elastic material, and their hybrid constant-pressure counterpart CPE4H for the cases involving a hyperelastic material. We apply a load P_o/E = 0.01 for the cases involving a linear elastic material, and a load P_o/E = 0.173 for the cases involving a hyperelastic material.\n\n\n\n\n\nabbrvnat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}