{"entry_id": "http://arxiv.org/abs/2303.07284v1", "published": "20230313170142", "title": "Align and Attend: Multimodal Summarization with Dual Contrastive Losses", "authors": ["Bo He", "Jun Wang", "Jielin Qiu", "Trung Bui", "Abhinav Shrivastava", "Zhaowen Wang"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\nAlign and Attend: Multimodal Summarization with Dual Contrastive Losses\n    Bo He^1Part of this work was done when Bo was an intern at Adobe Research., Jun Wang^1, Jielin Qiu^2, Trung Bui^3, Abhinav Shrivastava^1, Zhaowen Wang^3\n\n^1University of Maryland, College Park     ^2Carnegie Mellon University     ^3Adobe Research\n\n{bohe,abhinav}@cs.umd.edu, junwong@umd.edu, jielinq@andrew.cmu.edu, {bui,zhawang}@adobe.com\n\n    March 30, 2023\n========================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\nThe goal of multimodal summarization is to extract the most important information from different modalities to form summaries.\nUnlike unimodal summarization, the multimodal summarization task explicitly leverages cross-modal information to help generate more reliable and high-quality summaries. \nHowever, existing methods fail to leverage the temporal correspondence between different modalities and ignore the intrinsic correlation between different samples.\nTo address this issue, we introduce Align and Attend Multimodal Summarization (), a unified multimodal transformer-based model which can effectively align and attend the multimodal input. \nIn addition, we propose two novel contrastive losses to model both inter-sample and intra-sample correlations.\nExtensive experiments on two standard video summarization datasets (TVSum and SumMe) and two multimodal summarization datasets (Daily Mail and CNN) demonstrate the superiority of , achieving state-of-the-art performances on all datasets.\nMoreover, we collected a large-scale multimodal summarization dataset BLiSS, which contains livestream videos and transcribed texts with annotated summaries.\nOur code and dataset are publicly available at\n\u00a0<https://boheumd.github.io/A2Summ/>.\n\n\n\n\u00a7 INTRODUCTION\n\n\nWith the development in multimodal learning, multimodal summarization has drawn increasing attention\u00a0<cit.>.\nDifferent from traditional unimodal summarization tasks, such as video summarization\u00a0<cit.> and text summarization\u00a0<cit.>, multimodal summarization aims at generating summaries by utilizing the information from different modalities.  \nWith the explosive growing amount of online content (, news, livestreams, vlogs, ), multimodal summarization can be applied in many real-world applications.\nIt provides summarized information to the users, which is especially useful for redundant long videos such as livestream and product review videos. \n\n\n\n\n\nPrevious multimodal summarization methods\u00a0<cit.> leverage the additional modality information but can only generate the main modality summary, , either a video summary or a text summary, severely limiting the use of \ncomplementary benefits in the additional modality. \nRecently, multimodal summarization with multimodal output (MSMO) has been explored in several studies\u00a0<cit.>, which aim at generating both video and text summaries using a joint model.\nCompared to previous methods, which only produce a unimodal summary, MSMO provides a better user experience with an easier and faster way to get useful information. However, we find that the existing MSMO methods still have the following limitations.\nFirst, even if both modalities are learned together, the correspondence between different modalities is not exploited. For example, given a video and its transcripts, which are automatically matched along the time axis, no existing method utilizes the mutual temporal alignment information and treats the two modalities separately.\nSecond, previous works adopt simple strategies to model the cross-modal correlation by sequence modeling and attention operation\u00a0<cit.>, which requires a large number of annotated multimodal data which is hard to obtain. \n\n\n\n\nMotivated by the above observations, we propose a novel architecture for multimodal summarization based on a unified transformer model, as shown in Figure\u00a0<ref>. \nFirst, to leverage the alignment information between different modalities, we propose alignment-guided self-attention module to align the temporal correspondence between video and text modalities and fuse cross-modal information in a unified manner.\nSecond, inspired by the success of self-supervised training\u00a0<cit.>, which utilizes the intrinsic cross-modality correlation within the same video and between different videos,\nwe propose dual contrastive losses with the combination of an inter-sample and an intra-sample contrastive loss, to model the cross-modal correlation at different granularities. \nSpecifically, the inter-sample contrastive loss is applied across different sample pairs within a batch, which leverages the intrinsic correlation between each video-text pair and contrasts them against remaining unmatched samples to provide more training supervision. \nMeanwhile, the intra-sample contrastive loss operates within each sample pair, which exploits the mutual similarities between ground-truth video and text summaries and contrasts the positive features against hard-negative features.\n\n\nTo facilitate the research of long video summarization with multimodal information, we also collected a large-scale livestream video dataset from the web.\nLivestream broadcasting is growing rapidly, and the summarization of livestream videos is still an unexplored area with great potential.\nPrevious video summarization datasets consist of short videos with great variations in scene transitions. On the contrary, livestream videos are significantly longer (in hours as opposed to minutes) and the video content changes much more slowly over time, which makes it even harder for the summarization task.\nBesides, there has been a lack of annotated datasets with focus on transcript summarization, which can be a great complement to the livestream video summarization.\nTherefore, we collect a large-scale multimodal summarization dataset with livestream videos and transcripts, which are both annotated with ground-truth summaries by selecting important frames and sentences.\n\nTo summarize, our contributions include:\n\n    \n\n    \n  * We propose , a unified transformer-based architecture for multimodal summarization. It can handle multimodal input with time correspondences which previous work neglects.\n    \n\n    \n  * We present dual contrastive losses that account for modeling cross-modal information at different levels. Extensive experiments on multiple datasets demonstrate the effectiveness and superiority of our design. \n    \n\n    \n  * A large-scale Behance LiveStream Summarization (BLiSS) dataset is collected containing livestream videos and transcripts with multimodal summaries.\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n  \nVideo Summarization. \nCurrent techniques for video summarization can be divided into two categories, unsupervised and supervised. \nUnsupervised learning approaches, including\u00a0<cit.> utilize different hand-crafted features to score and select the video frames without the human-annotated summaries. \nDR-DSN\u00a0<cit.> explores an unsupervised reward function to tackle video summarization. \nGLRPE\u00a0<cit.> attempts to apply self-attention with relative position representation for unsupervised video summarization. \nWith the help of the annotated video summarization datasets\u00a0<cit.>, numerous supervised learning methods\u00a0<cit.> have been proposed in recent years to summarize videos. Among them, \nDSNet\u00a0<cit.> formulates supervised video summarization as a temporal interest detection process. \nRSGN\u00a0<cit.> utilizes LSTM and GCN to model frame-level and shot-level dependencies.\niPTNet\u00a0<cit.> jointly trains the video summarization task and correlated moment localization task to utilize additional moment localization data samples to boost the video summarization performance. \n\n\n\n\n\n  \nText Summarization.\nIn general, text summarization can be categorized into two groups: (i) Extractive summarization\u00a0<cit.> generates output summary by identifying the salient parts of the input document. NN-SE\u00a0<cit.> develops a neural attention model to select sentences or words of the input document as the output summary. SummaRuNNer\u00a0<cit.> employs RNN for extractive summarization. Miller\u00a0<cit.> adopts clustering algorithm in the feature space to select important sentences. \n(ii) Abstractive summarization\u00a0<cit.> performs the summarization by paraphrasing the important parts of the input document. Lead3\u00a0<cit.> applies the attentional encoder-decoder RNN for the task of abstractive text summarization. However, those approaches are designed for pure unimodal summarization that doesn't consider cross-modal alignment and fusion.\nRecently, StreamHover\u00a0<cit.> presents an unsupervised model for transcript summarization and collects a livestream transcript summarization dataset.\nInspired by it, we collect a new livestream dataset with a much larger scale and richer multimodal annotations for the multimodal summarization task.\n\n\n\n\n\n\n\n\n  \nMultimodal Summarization.\nExisting work\u00a0<cit.> commonly utilize additional complementary modalities to enhance the feature representation for the primary modality, however, they typically generate summaries from a single modality.\nFor example, CLIP-It\u00a0<cit.> builds a language-guided framework to obtain a summary video conditioned on the text. MMS\u00a0<cit.> learns joint representations of text and images and outputs text summaries.\nRecently, multimodal summarization with multimodal output (MSMO) has been explored in several studies.\nZhu et al.\u00a0<cit.> propose the first MSMO model and collect a multimodal summarization dataset with text and image modalities. \nLi et al.\u00a0<cit.> extend it with video-based news articles and adopt conditional self-attention for text and video fusion.\nRecently, Fu et al.\u00a0<cit.> collect a multimodal dataset with more modalities included such as audio and transcript.\n\n\n\n\n\n\n\n\u00a7 METHOD\n\n\n\n\n \u00a7.\u00a7 Overview\n\nGiven the untrimmed multimodality input (video, text, and sound), the multimodal summarization task aims at selecting the most important parts from each modality. \nFigure\u00a0<ref>(a) illustrates an overview of our framework. \nThe input to our model is the multi-modality (, video and transcribed text in our case) with N video frames and M sentences.\nSince each transcribed sentence has its start time and end time, the video and text modalities can be automatically aligned by the corresponding timestep.\nThe overall architecture can be divided into three parts: the input embedding (Sec.\u00a0<ref>), the multimodal alignment and fusion (Sec.\u00a0<ref>), and the loss function (Sec.\u00a0<ref>).\n\n\n\n\n \u00a7.\u00a7 Input Embedding\n\n\nSimilar to previous work\u00a0<cit.>, we use pre-trained feature extraction models (, GoogleNet\u00a0<cit.> and RoBERTa\u00a0<cit.>) to extract deep neural features for each frame and sentence. \nAfter feature extraction, features from different modalities are projected into a common C-dimensional embedding space by a linear fully connected (FC) layer.\nSpecifically, we denote the generated video and text features as F\u2208\u211d^\ud835\udc0d\u00d7 \ud835\udc02 and S\u2208\u211d^\ud835\udc0c\u00d7 \ud835\udc02, respectively.\n\nFor each modality, there is a special token \u201c[CLS]\u201d prepended at the start of the feature sequences, which enables a holistic representation.\nFollowing BERT\u00a0<cit.>, we add a learnable position embedding to each feature sequence so that the order information can be incorporated.\nTo utilize the time correspondence information between the video frames and text sentences, we add an additional learnable segment-based positional embedding at the input stage.\nMore precisely, each sentence has its own timestep information denoted as [t_s, t_e], where t_s and t_e denote the start and the end time index of each sentence. \nWe note that a single text sentence usually corresponds to several video frames, making M \u2264 N. \nFor all frames inside each time index window {F_i}_i\u2208[t_s, t_e], the segment embedding is shared across these frames and the corresponding sentence.\nAfter adding these positional embeddings, the input sequences to the multimodal transformer from both modalities are concatenated along the time axis, denoted as X \u2208\u211d^(M+N)\u00d7 C.\n\n\n\n\n\n \u00a7.\u00a7 Multimodal Alignment and Fusion\n\n\n\n\n  \nAlignment-Guided Self-Attention.\nA core component of is the alignment-guided self-attention module which allows us to exploit the time correspondence between video and text modalities. \nInspired by the superior advantages of Transformers\u00a0<cit.> in modeling different modalities (, visual, language, and audio) on various multimodal tasks (, visual question answering\u00a0<cit.>, vision-language pre-training\u00a0<cit.>), we adopt the transformer architecture to align and fuse our multimodal input.\nHowever, for the multimodal summarization task, the inputs are often untrimmed videos and text sentences, which are dominated by irrelevant backgrounds.\nDirectly applying global self-attention across inputs from all modalities may introduce extra noise to the multimodal fusion process.\n\nMotivated by this observation, we propose the alignment-guided self-attention module to fuse the input across different modalities.\nWe formulate this process by using a masked self-attention operation in Figure\u00a0<ref>(b). \nSpecifically, an attention mask A\u2208\u211d^(N+M)\u00d7 (N+M) initialized with all 0 is defined to indicate the timestep alignment information, where N and M denote the length of the video and text feature sequences, respectively.\nFor the intra-modality modeling, we follow the standard procedure with global attention operation, where features from the same modality can attend to each other such that all entries corresponding to intra-modality attention are filled with value 1 in the attention mask.\nFor the cross-modality attention between video and text input, we only fill in the entries from the same segment with value 1.\nFor example, suppose the k^th sentence S_k corresponding to the time index window [t_s, t_e]. We consider the frames which also lie into the same time window [t_s, t_e] to be the same segment, denoted as {F_i}_i\u2208[t_s, t_e]. Then, we assign the elements of attention mask as follows A[N+k, t_s: t_e] = 1.\nThe attention mask is then applied to the attention matrix computed by the standard self-attention approach\u00a0<cit.>:\n\n    Q   =XW_Q,  K=XW_K,  V = XW_V, \n    \n        D_i,j   = A_i,jexp(Q_iK_j^T/\u221a(D))\u2211_k A_i,kexp(Q_iK_k^T/\u221a(D)), \n    \n        Z   =X+DV W_O,\n\nwhere i,j\u2208[1, M+N] are the entry indices of the matrix, X is the concatenated input from video and text modalities, and W_Q,W_K,W_V,W_O\u2208\u211d^C\u00d7 C are the linear projection matrices for generating the query, key, value, and the output. Multi-head attention\u00a0<cit.> is also adopted to improve the capacity of the attention module.\nIn this way, we explicitly utilize the alignment correspondence between different modalities, avoiding the negative impacts caused by noisy background frames or irrelevant sentences.\n\n\n\n\n\n  \nMixture-of-Modality-Experts. Based on the mixture-of-modality-experts transformer\u00a0<cit.> in the multimodal tasks, after the self-attention layer, we introduce two different experts to jointly model features from different modalities including the video expert (Video-FFN), and text expert (Text-FFN) rather than the standard shared FFN <cit.>. \n\n\n\n\n\n  \nScore Prediction.\nFinally, on top of the cascaded transformer blocks, two separate score prediction branches assign relevance scores to each frame and each sentence.\nBased on predicted scores, two different procedures are followed to generate the final summary.\nFor the standard video summarization datasets (, SumMe\u00a0<cit.> and TVSum\u00a0<cit.>), based on the pre-processed KTS\u00a0<cit.> segmentation results, segment-level scores are computed from frame-level scores, and the final video summary is generated by selecting top 15% of video durations by Knapsack algorithm.\nFor the multimodal summarization datasets (, Daily Mail\u00a0<cit.>), the frames and sentences with the top highest scores are selected to generate the final summary prediction for the video and text modalities separately.\n\n\n\n\n\n \u00a7.\u00a7 Loss Function\n\n\nWe employ three different loss functions to train our model, including the classification loss and the novel dual contrastive losses, which consist of the inter-sample contrastive loss and the intra-sample contrastive loss.\n\n\n\n\n\n  \nClassification Loss.\nWe apply the focal loss\u00a0<cit.> for the importance score classification, which handles the class imbalance issue by down-weighting losses for well-classified samples.\nThe details are shown below:\n\n    \u2112_cls_m=-1/N\u2211_i=1^N {[    -\u03b1(1-p_i)^\u03b3log(p_i),               if  y_i=1; -(1-\u03b1) p_i^\u03b3log(1-p_i),               if  y_i=0 ].\n\n\n    \u2112_cls = \u2112_cls_video + \u2112_cls_text\n\nwhere m could be either video or text, and p_i is the predicted score for each frame/sentence while y_i is the ground-truth label. If y_i=1, it indicates the i^th frame/sentence is the key-frame/key-sentence. \nThe final classification loss is the sum of the two single modality losses.\n\n\n\n\n\n\n\n\n  \nInter-Sample Contrastive Loss.\nDriven by the success of contrastive learning in the image-language pre-training tasks\u00a0<cit.>, we want to utilize the intrinsic relationships between each input video and text pair.\nAs shown in Figure\u00a0<ref>(c), given a batch of B sample pairs, we design an auxiliary inter-sample contrastive loss to predict which of the B^2 possible video-text pairs across a batch correctly matches and belongs to the same sample.\nSpecifically, we use the pre-pended [CLS] token as a holistic representation for each video and text sample.\nSimilar to CLIP\u00a0<cit.>, we maximize the cosine similarity of the video embedding [CLSV] and the text embedding [CLST] from B real pairs in the batch while minimizing the cosine similarity of embeddings from the B^2 - B incorrect pairs. \nSpecifically, the inter-sample contrastive loss is calculated as \n\n    \u2112_inter =     \ud835\udd3c_z\u223c[CLSV]_j, z^+\u223c[CLST]_j, z^-\u223c\u2110_k\u2260 j[CLST]_k\u2113(z,z^+,z^-) \n    \n        +     \ud835\udd3c_z\u223c[CLST]_j, z^+\u223c[CLSV]_j, z^-\u223c\u2110_k\u2260 j[CLSV]_k\u2113(z,z^+,z^-)\n\nwhere \u2113(z,z^+,z^-) is the standard contrastive loss\u00a0<cit.> with the following equation:\n\n    \u2113(z,z^+,z^-) \n    \n        =  - log( exp(z^T \u00b7 z^+/\u03c4)/exp(z^T \u00b7 z^+/\u03c4) + \u2211_kexp(z^T \u00b7 z^-_k/\u03c4))\n\nand \u03c4 is a learnable temperature parameter.\n\n\n\n\n\n\n\n\n\n  \nIntra-Sample Contrastive Loss.\nWhile the above inter-sample contrastive loss only considers the relationship across different samples, however, for the summarization task, to correctly detect the key-frames and key-sentences from each untrimmed video and text input, \nmore fine-grained information modeling, in particular, is crucial.\nIt would require the model to accurately distinguish the key-frames and key-sentences from the background frames and less-related sentences.\nIntuitively, the human-annotated key-frames and key-sentences share mutual information with each other. Meanwhile, they both should reveal the most salient parts from the original untrimmed video and text sequences.\nFor instance, for a cooking recipe video with transcribed text, the annotated key-frames and key-sentences should clearly reveal the instructions for each step. \nMore importantly, these key-frames and key-sentences should be deeply correlated with each other and share similar high-level semantic meanings.\nMotivated by this observation, we propose the intra-sample contrastive loss which is calculated within each video and text pair sample rather than across different sample pairs.\n\nSpecifically, we assign features associated with the pre-defined ground-truth key timesteps as positive pairs for both modalities.\nTo form the contrastive pairs, as pointed out by\u00a0<cit.>, the quality of the negative samples is of vital importance for the effectiveness of contrastive learning. \nTherefore, we need to select the hard negative samples for video and text separately.\nSpecifically, since the pre-annotated non-key timesteps are negative samples, based on the prediction scores for each frame (p_i_i=1^N) and sentence (q_i_i=1^M), we argue that the wrongly classified timesteps with highest prediction scores are hard-negative samples. \nIntuitively, for a long untrimmed video, due to the time dependencies, the frames adjacent to the key-frames have very similar visual contents and should also be treated as the key-frames. \nHowever, if these frames are selected as the hard-negative samples, it tends to confuse the model and may hurt the final performance.\nTherefore, we exclude those timesteps before selecting the hard-negative samples.\n\n\nAs shown in Figure\u00a0<ref>(a), given the ground truth (GT) key-frame label, we first expand the key-frame segments on both sides to include more adjacent frames as dummy key-frames.\nThen, based on the predicted scores for each timestep, we select timesteps with top-k highest scores but not in the expanded GT key-frame labels as hard-negative samples. \nHere, k_video=\u230aN/r\u230b, k_text=\u230aM/r\u230b,  r is a hyper-parameter controlling the total number of selected hard-negative samples.\nIn this way, we form contrastive pairs for both video and text modalities.\nFormally, we denote the positive frames, hard-negative frames, positive sentences, and hard-negative sentences as \u2110_PF, \u2110_HNF, \u2110_PS, and \u2110_HNS, respectively.\nAs shown in Figure\u00a0<ref>(b), the proposed intra-sample contrastive loss is applied as follows:\n\n\n    \u2112_intra =     \ud835\udd3c_z\u223c\u2110_PF, z^+\u223c\u2110_PS, z^-\u223c\u2110_HNF\u2113(z,z^+,z^-) \n    \n        +     \ud835\udd3c_z\u223c\u2110_PS, z^+\u223c\u2110_PF, z^-\u223c\u2110_HNS\u2113(z,z^+,z^-)\n\nwhere \u2113 follows the same contrastive equation as Eq.\u00a0<ref>.\n\n\n\n\n\n  \nOverall Loss.\nThe final loss is the combination of the above three losses,\n\n    \u2112=\u2112_cls + \u03b2\u00b7\u2112_inter + \u03bb\u00b7\u2112_intra\n\nwhere \u03b2 and \u03bb are hyper-parameters controlling the trade-off between three loss components.\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n\n\n \u00a7.\u00a7 Datasets and Implementation Details\n\n\n\n  \nDatasets.\nWe evaluate on two standard video summarization datasets (SumMe\u00a0<cit.> and TVSum\u00a0<cit.>), two multimodal summarization datasets (Daily Mail\u00a0<cit.> and CNN\u00a0<cit.>), and a new Behance LiveStream Summarization (BLiSS) dataset.\nTVSum dataset consists of 50 videos pertaining to 10 categories. SumMe dataset consists of 25 videos capturing multiple events. \nDaily Mail dataset contains 1,970 samples and CNN dataset contains 203 samples, which are crawled from the news website including video, images, text articles, and captions. We follow the same data split as\u00a0<cit.>.\nThe BLiSS dataset consists of 13,303 pairs of livestream videos and transcribed text, with annotated summaries for both modalities. \n\n\n\n\n\n  \nEvaluation Metrics.\nFor SumMe and TVSum datasets, following previous work\u00a0<cit.>,  we evaluate the video summarization dataset by the F1 score metric. \nHowever, as pointed out by\u00a0<cit.>, the performance of F1 evaluation is mostly determined by the pre-processing segmentation step, and a random method is able to reach similar performance scores. As a result, they propose to utilize rank order statics (Kendall's \u03c4\u00a0<cit.> and Spearman's \u03c1\u00a0<cit.>) as alternative evaluation metrics which are more reliable compared to F1 score.\nFor multimodal summarization datasets, we evaluate the generated text summary by ROUGE\u00a0<cit.> following previous works\u00a0<cit.>. Specifically, R-1, R-2, and R-L represent ROUGE-1, ROUGE-2, and ROUGE-L F1 scores, respectively, which are widely used to calculate the n-grams overlapping between the output text summary and ground truth text summary.\nSame as\u00a0<cit.>, the cosine image similarity is measured between the features of the predicted video summary and ground-truth video summary.\n\n\n\n\n\n  \nImplementation Details.\nFor standard video summarization datasets (SumMe and TVSum), we follow previous work\u00a0<cit.> and use the pre-extracted GoogLeNet\u00a0<cit.> feature as the video input.\nTo collect the corresponding text modality, we adopt the pre-trained image caption model GPT-2\u00a0<cit.>[https://huggingface.co/nlpconnect/vit-gpt2-image-captioning] to generate the caption for each frame.\nNext, for all the text modality input, we apply the pre-trained RoBERTa\u00a0<cit.>[https://huggingface.co/distilroberta-base] to extract textual features for each sentence.\nFor multimodal summarization datasets (Daily Mail and CNN), we use the same feature extractor as\u00a0<cit.>.\nFor the BLiSS dataset, pre-trained CLIP\u00a0<cit.> and RoBERTa\u00a0<cit.> are adopted to extract features for each frame and each sentence.\nThe focal loss <cit.> with \u03b1 = 0.25 and \u03b3 = 2.0 is adopted for the classification loss. \nMore dataset-specific training/testing details and hyper-parameter choosing are described in the supplementary material.\n\n\n\n\n\n\n \u00a7.\u00a7 BLiSS Dataset\n\nBehance[http://behance.net/] is a public website with a large amount of livestream videos created by artists showing their work process. The videos are generally hours long and accompanied with transcripts of streamers' speeches. We follow previous work StreamHover\u00a0<cit.> to expand their dataset to a much larger scale with more modalities.\n\n\n\n\n\n  \nData Collection.\nWe collected 674 livestream videos with transcripts and other metadata. Each video was further divided into 5-minute long clips for human annotation. Annotators were instructed to select the key sentences from transcripts, and write the text summary and key phrases in their own words for the entire clip.\nFor each video, we also obtained its thumbnail animation from the website, and selected the most similar frame to the thumbnail from each clip as ground truth key-frames.\nMore details about the collection process are elaborated in the supplementary material.\n\n\n\n\n\n  \nComparison with Existing Multimodal Datasets.\nThe BLiSS dataset is much larger than the standard video summarization datasets (SumMe and TVSum) and multimodal summarization datasets (Daily Mail and CNN).\nBLiSS has 13,303 data samples and 1,109 total video hours, which is much longer than TVSum (3.5 hours) and Daily Mail (44.2 hours).\nFor the text modality, the total number of text tokens is 5.4M (BLiSS), greater than 1.3M (Daily Mail) and 0.2M (CNN).\nThere are other multimodal summarization datasets for the abstractive text summarization task with additional image or video modalities. \nFor example, MSMO\u00a0<cit.>, MMSS\u00a0<cit.>, VMSMO\u00a0<cit.> and How2\u00a0<cit.>. However, none of them have aligned video and text modalities.\nWe also annotated the abstractive text summary for each livestream video, but in this paper, we only focus on the extractive text summary task.\nFurthermore, we keep some metadata, including the title, streamer information, and audio modality for further potential research.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Results\n\n\n\n\n  \nSumMe and TVSum Datasets.\nWe compare the proposed method with the previous state-of-the-art (SOTA) methods on SumMe <cit.> and TVSum <cit.> datasets in Table\u00a0<ref>.\nWe first observe that achieves the best performance on both datasets. \nExcept for the F1 score metric, our is slightly worse than CLIP-It\u00a0<cit.> but still higher than it for the other two metrics on the TVSum dataset.\nCLIP-It also adopts transformer architecture to fuse different modalities by cross-attention, which takes in the generated video caption as text modality. However, it ignores the time correspondence between video and text modalities. \nInstead, our aligns cross-modality information and exploits the intrinsic correlation between the video and text at different granularities by our inter-sample and intra-sample contrastive losses.\nIn addition, the state-of-the-art method iPTNet\u00a0<cit.> utilizes an additional moment localization dataset Charades-STA\u00a0<cit.> to help address the data scarcity problem but results in a much longer training time, however, without utilizing extra datasets, our can still outperform it on all the metrics, which strongly justifies the superiority of our design.\n\n\n\n\n\n\n\n  \nDaily Mail and CNN Datasets.\nAs shown in Table\u00a0<ref>, we also compare our with previous methods on the CNN <cit.> and Daily Mail <cit.> datasets. \nSince the text modality of CNN and Daily Mail datasets do not have time information, we only apply the inter-sample and intra-sample contrastive losses without the alignment-guided self-attention module.\nWe first observe that can indeed greatly benefit from leveraging the multimodal information, which boosts the text summary metric by 1-2% ROUGE F1 score and increases the video summary cosine similarity by 0.9%. \nCompared to the state-of-the-art multimodal method M^2SM\u00a0<cit.>, which utilizes additional transcript extracted from videos as the bridge between video and text modality,\nis better by 3% and 2.4% in ROUGE-1 F1 score on two datasets respectively.\nFor the video summarization, our transformer-based can outperform multimodal summarization method M^2SM\u00a0<cit.> and state-of-the-art video summarization model CLIP-It\u00a0<cit.> by 1%. \n\n\n\n\n\n\n\n\n  \nBLiSS Dataset.\nWe validate on the livestream videos from the BLiSS dataset by comparing it with existing video and text summarization methods.\nAs shown in Table\u00a0<ref>, when comparing with video summarization methods DSNet-AF\u00a0<cit.> and CLIP-It\u00a0<cit.>, our achieves the best results on the video cosine similarity metric. \nAlthough CLIP-It also utilizes the additional text modalities by the cross-attention operation, can still outperform it by 1%.\nCompared to the extractive text summarization method SummaRuNNer\u00a0<cit.> and the abstractive text summarization method BART\u00a0<cit.>, outperforms both of them by at least 3% on all the ROUGE scores.\nIt further demonstrates the superior effectiveness of on livestream videos.\n\n\n\n\n \u00a7.\u00a7 Ablation Studies\n\n\n\nTo further investigate the contribution of each component in , we conduct ablation studies in Table\u00a0<ref>.\nWe first observe that adding the text modality input can enhance the final results of video summarization.\nHowever, as we mentioned before, without alignment of video and text modalities, directly applying global attention between untrimmed video and text input tends to introduce too much noise and result in inferior performance.\nAfter we align and attend the video and text modalities with the proposed alignment-guided self-attention module, we can improve the F1 score by 1%.\nFurthermore, for the dual contrastive losses, it is obvious that a consistent gain can be achieved by adding either one of these two losses. \nIn particular, introducing the intra-sample contrastive loss significantly increases the performance by 2.5%.\nIt proves that exploring the intrinsic mutual correlation between video and text and mining hard negative samples can greatly enhance the ability to localize the important frames and sentences.\nIn addition, two contrastive losses are complementary to each other.\nWhen incorporating all three proposed components together, our approach boosts the final performance from 50.5% to 55.0%.\n\n\n\n\n\n \u00a7.\u00a7 Visualization\n\n\n\nFigure\u00a0<ref> shows the visual comparison between baseline and .\nWe observe that the typical errors of the baseline model can be addressed by the proposed alignment module and dual contrastive losses, such as missing detection of important segments and inaccurate summary boundary prediction.\nIt further verifies the effectiveness of .\nMore visualizations on the BLiSS dataset are provided in the supplementary material.\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\n\nIn this paper, we present , a novel unified transformer-based framework for multimodal summarization. is designed to align and attend different modalities by leveraging time correspondences that previous methods neglect.\nAlso, we introduce dual contrastive losses to exploit the inter-sample and intra-sample cross-modality information.\nExtensive experiments on multiple datasets validate the effectiveness of our .\nIn addition, we collect a large-scale multimodal summarization dataset focusing on livestream videos and transcripts. We hope it can be beneficial for further research in this area.\n\n\n\n\n\n"}