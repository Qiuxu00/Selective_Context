{"entry_id": "http://arxiv.org/abs/2303.06825v1", "published": "20230313025059", "title": "Best-of-three-worlds Analysis for Linear Bandits with Follow-the-regularized-leader Algorithm", "authors": ["Fang Kong", "Canzhe Zhao", "Shuai Li"], "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1]Fang Kong\n1]Canzhe Zhao\n1]Shuai Li Corresponding author.\n\n\n\n\n\n[1]Shanghai Jiao Tong University\n\n\n\n[ ]{fangkong,canzhezhao, shuaili8}@sjtu.edu.cn\n\n\n Best-of-three-worlds Analysis for Linear Bandits with Follow-the-regularized-leader Algorithm\n    [\n    \n=============================================================================================\n\n\n\n\n\n\n    The linear bandit problem has been studied for many years in both stochastic and adversarial settings. Designing an algorithm that can optimize the environment without knowing the loss type attracts lots of interest. <cit.> propose an algorithm that actively detects the loss type and then switches between different algorithms specially designed for different settings. However, such an approach requires meticulous designs to perform well in all settings. Follow-the-regularized-leader (FTRL) is another popular algorithm type that can adapt to different environments. This algorithm is of simple design and the regret bounds are shown to be optimal in traditional multi-armed bandit problems compared with the detect-switch type algorithms. Designing an FTRL-type algorithm for linear bandits is an important question that has been open for a long time. In this paper, we prove that the FTRL-type algorithm with a negative entropy regularizer can achieve the best-of-three-world results for the linear bandit problem with the tacit cooperation between the choice of the learning rate and the specially designed self-bounding inequality.  \n    \n\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\n\n\n\n\n\nThe linear bandit problem is a fundamental sequential decision-making problem, in which the learner needs to choose an arm to pull and will receive a loss of the pulled arm as a response in each round t <cit.>. Particularly, the expected value of the loss of each arm is the inner product between the pulled arm and an unknown loss vector. \nThe learning objective is to minimize the regret, defined as the difference between the learner's cumulative loss and the loss of some best arm  in hindsight. There are two common environments characterizing the schemes of the loss received by the learner.\nIn the stochastic environment, the loss vector is fixed across all rounds and the learner only observes a stochastic loss \n\nsampled from a distribution with the expectation being the inner product between the feature vector of the pulled arm and the loss vector. \nIn this line of research, it has been shown that the instance-optimal regret bound is of order O(log T) (omitting all the dependence on other parameters) <cit.>.\nMeanwhile, in the adversarial environment, the loss vector can arbitrarily change in each round. In this case, the minimax-optimal regret has been proved to be O(\u221a(T)) <cit.>.\n\nWhile several algorithms are able to achieve the above (near) optimal regret guarantees in either stochastic environment <cit.> or adversarial environment <cit.>, these algorithms require prior knowledge about the regime of the environments. If such knowledge is not known a priori, the algorithms tailored to the adversarial environment will have too conservative regret guarantees in a stochastic environment. Also, without knowing such knowledge, the algorithms tailored to the stochastic environment may have linear regret guarantees in the adversarial environment and thus completely fail to learn. To cope with these issues, recent advances have emerged in simultaneously obtaining the (near) optimal regret guarantees in both the stochastic and adversarial environments, which is called the best-of-both-worlds (BoBW) result. \n\nFor the special case of multi-armed bandit (MAB) problems,\nalgorithms with (near) optimal regret in both adversarial and stochastic environments\nhave been proposed <cit.>.\nIn general, these algorithms can be divided into two approaches.\n\nThe first approach achieves the BoBW regret guarantees by conducting statistical tests to distinguish between the stochastic and adversarial environments. Then it will perform an irreversible switch\ninto adversarial operation mode if an adversarial environment is detected <cit.>. The other approach modifies the algorithms originally designed for adversarial bandits to achieve improved regret guarantee in the stochastic environment, without compromising the adversarial regret guarantee <cit.>. Compared with the first approach, the\nsecond approach has better adaptivity since it does not need to detect the nature of the environment.\n\nMore importantly, the second approach may have a tighter regret guarantee, and the follow-the-regularized-leader (FTRL) type\nalgorithm of <cit.> obtains regret guarantees that are optimal up to universal constants in both environments. For the linear bandit problem, while <cit.> achieve the BoBW result, their algorithm is based on the first approach, and hence still requires distinguishing between two kinds of environments. Therefore, a natural question is still open:\n\n\n\nDoes there exist an FTRL-type BoTW algorithm for linear bandits, with the same adaptivity to both the adversarial and (corrupted) stochastic environments?\n\nIn this paper, we give an affirmative answer to the above question, and hence solve the open problem of \n<cit.>. Specifically, we propose an algorithm based on the FTRL framework, which extends the similar idea of <cit.> for the MAB setting that runs a variant of FTRL with a special regularizer to handle both the stochastic and adversarial environments in the setting of the linear bandit. However, the extension is non-trivial as discussed by <cit.>, and our results require new ingredients regarding the theoretical analysis. In particular, the main challenge relies on regret analysis in the stochastic setting. Different from MAB problems, it is not easy to decompose the regret in stochastic linear bandits by the selecting times of each arm, since the number of arms can be huge and the exploration rate should depend on the linear structure. So we design the self-bounding inequality such that the regret is lower bounded by the product between the selecting times of all sub-optimal arms and a minimum reward gap \u0394_min. Based on this special design to link the regret with the selecting times and the choice of the learning rate, we are able to bound the regret of our algorithm by O(log T) in the stochastic setting and O(\u221a(T)) in the adversarial setting. \n\nAs a preliminary step, our analysis also contributes some new ideas for the BoTW problem in other online learning models with linear structures such as linear Markov decision processes (MDPs).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORKS\n\n\n\n  \nLinear Bandits\n\nLinear bandit is a fundamental model in online sequential decision-making problems. Its stochastic setting is originally proposed by <cit.>, and first solved by <cit.> using the optimism principle. Subsequently, the regret guarantees of this problem have been further improved by several algorithms <cit.>. The adversarial setting dates back to <cit.>. The first O(\u221a(T)) expected regret bound is obtained by the Geometric Hedge algorithm proposed by <cit.>. Latter, <cit.> establish the first computationally efficient algorithm achieving O(\u221a(T)) regret guarantee based on the FTRL framework. \n<cit.> further obtain the regret guarantee that is minimax optimal up to a logarithmic factor. \nThe recent work of <cit.> achieves the (near) optimal instance-dependence regret and minimax optimal regret in stochastic and adversarial environments respectively, via establishing a detect-switch type algorithm.\nFor linear bandits in a corrupted stochastic setting, <cit.> first obtain an instance-dependent regret guarantee, with an additional corruption term depending on the corruption level C  linearly and on T logarithmically. Subsequently, <cit.> attain the instance-independent bound in the same setting. For the more general setting of  linear contextual\nbandits where the arm set may vary in each round and can even be infinite, <cit.> establish the sublinear instance-independent and/or instance-dependent regret guarantees. Remarkably, <cit.> obtain the near-optimal instance-independent regret guarantees for both the corrupted and uncorrupted cases with the leverage of a weighted ridge regression scheme.\n\n\n\n\n\n  \nBest-of-Both-Worlds Algorithms \nThe best-of-both-worlds (BoBW) algorithms aim to obtain the (near) optimal theoretical guarantees in both the stochastic and adversarial environments simultaneously,\nand have been extensively investigated for a variety of problems in the online learning literature, including MAB problem <cit.>, linear bandit problem <cit.>, combinatorial semi-bandit problem <cit.>, the problem of bandits with graph feedback <cit.>, online learning to rank <cit.>,\nthe problem of prediction with expert advice <cit.>,\nfinite-horizon tabular Markov decision processes <cit.>,\nonline submodular minimization <cit.>, and partial monitoring problem <cit.>. \n\n\nIn addition to investigating the BoBW results for stochastic and adversarial environments, devising algorithms that can simultaneously learn in the stochastic environment with corruptions also gains lots of research attention recently <cit.>. Particularly, algorithms with (near) optimal regret guarantees in adversarial, stochastic, and corrupted stochastic environments are typically called the best-of-three-worlds (BoTW) algorithms. In this work, we are interested in achieving the BoTW results in the linear bandit problem, but without using a detect-switch type algorithm similar to that of <cit.>. Instead, we aim to achieve the BoTW results for linear bandits by devising an FTRL-type algorithm, whose key idea is to leverage the self-bounding property of regret. \n\nSimilar ideas have been used to obtain BoBW results <cit.> as well as the BoTW results <cit.> in various settings.\n\n\n\n\n\u00a7 PRELIMINARIES\n\n\n\n\n\nIn this section, we present the basic preliminaries regarding the linear bandit problem, \nin both the adversarial environments and stochastic environments (with possibly adversarial corruptions).\n\n\n\n\n\n\n\n\n\nWe denote by D\u2208\u211d^d the finite arm set with d being the ambient dimension of the arm feature vectors <cit.>. We further assume that D spans \u211d^d and x_2\u2264 1 for all x\u2208 D <cit.>. In each round t, the learner needs to select an arm x_t\u2208 D. Meanwhile, there will be an unknown loss vector \u03b8_t\u2208\u211d^d determined by the environment. Subsequently, the loss \u2113_t(x_t) of the chosen arm x_t will be revealed to the learner. Particularly, \u2113_t(x_t)=\u27e8 x_t,\u03b8_t\u27e9+\u03f5_t(x_t) in linear bandits, with \u03f5_t(x_t) being some independent zero-mean noise. The performance of one algorithm is measured by the regret (a.k.a., expected pseudo-regret), defined as\n\n    R(T)=\ud835\udd3c[\u2211_t=1^T\u2113_t(x_t)-\u2211_t=1^T\u2113_t(x^\u2217)] ,\n\nwhere x^\u2217\u2208_x\u2208 D\ud835\udd3c[\u2211_t=1^T\u2113_t(x)] is the best arm in expectation in hindsight, and the expectation is taken over the randomness of both the loss sequences \u2113_t and the internal randomization of the algorithm.\n\n\nFor linear bandits in adversarial environment, \n\u03b8_t is chosen by an adversary arbitrarily in each round t. In this work, we consider the non-oblivious adversary, \n\nwhich means that the choice of \u03b8_t may potentially depend on the history of the learner's actions {x_\u03c4}_\u03c4=1^t-1 up to round t-1, in addition to the adversary's own internal randomization.\n\nAlso, for ease of exposition, we assume \u03f5_t(x_t)=0 for all t in the adversarial environment. In the following, we present the definition of a general environment, which is first formalized by <cit.> and subsumes the stochastic and corrupted stochastic environments as special cases.\n\n\n\n\nLet C>0 and \u0394\u2208[0,2]^|D|.\nAn environment is defined as the adversarial regime with a (\u0394, C, T) self-bounding constraint, if for any algorithm, the regret satisfies \n\n    R(T)\u2265\u2211_t=1^T\u2211_x\u2208 D\u0394(x)\u2119(x_t=x)-C .\n\n\nThe stochastic environment satisfies \u03b8_t=\u03b8 for some fixed but unknown \u03b8.\nHence it has regret R(T)=\ud835\udd3c[\u2211_t=1^T \u0394(x_t)] with \u0394(x)=\u27e8 x-x^\u2217,\u03b8\u27e9 for all x\u2208 D. It follows that a stochastic environment is also an adversarial regime with a (\u0394,0,T) self-bounding constraint.\nFor this case, we assume the optimal arm x^* to be unique as previous works studying stochastic linear bandits <cit.> and BoTW problems <cit.>.\n\n\n\nFor the corrupted stochastic environment, it is also satisfied that \u03b8_t=\u03b8 for some unknown \u03b8, but the learner now receives loss \u2113_t(x_t)=\u27e8 x_t,\u03b8_t\u27e9+\u03f5_t(x_t)+c_t, where c_t is the corrupted value chosen by the adversary in each round t and may depend on the historical information {(\u2113_\u03c4,x_\u03c4)}_\u03c4=1^t-1. \nWe note that the corrupted stochastic environment considered in this work is slightly more general than that considered by <cit.>, since we do not impose any structural assumptions over c_t, while they further assume c_t is linear in x_t. As aforementioned, the corrupted stochastic environment is also an instance of the adversarial regime with a self-bounding constraint. To see this, let C=\u2211_t=1^T|c_t| be the corruption level, indicating the total amount of the corruptions. Then  a corrupted stochastic environment has regret satisfying Eq. (<ref>), and hence it is an instance of the adversarial regime with a (\u0394, C, T) self-bounding constraint.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 ALGORITHM\n\nIn this section, we present the proposed algorithm for linear bandits in both adversarial and (corrupted) stochastic environments, and detail its pseudo-code in Algorithm <ref>.\n\nIn general, our algorithm follows from the basic idea of FTRL that chooses the arm which minimizes the cumulative loss of past rounds together with the regularizer.\nIn specific, at each round t, our algorithm computes the regularized leader q_t by solving the following optimization problem (Line <ref>):\n\n    q_t \u2208_p\u2208\u0394(D)\u2211_s=1^t-1\u2113\u0302_s, p + \u03c8_t(p)  ,\n\nwhere \u0394(D) is the probability simplex over D, \u2113\u0302_s is the loss estimate in round s to be defined later and \u03c8_t(p) is the regularizer. In this work, different from the\ncomplex (hybrid) regularizers used in previous works studying FTRL-type BoTW algorithms <cit.>, we simply choose the negative Shannon entropy as the regularizer:\n\n    \u03c8_t(p) = \u03b2_t \u2211_x\u2208 D p(x)ln p(x) ,\n\n\nwhere \u03b2_t is the time-varying learning rate, whose concrete value is postponed to Theorem <ref>.\n\nThen, to permit enough exploration and control the variance of the loss estimates, our algorithm mixes the regularized leader q_t with a distribution \u03c0 of the G-optimal \ndesign over the arm set D (Line <ref>):\n\n    p_t = \u03b3_t \u00b7\u03c0 + (1-\u03b3_t)\u00b7 q_t ,\n\nwhere \u03b3_t is a time-varying hyper-parameter to be chosen later.\nIn particular, the  G-optimal design distribution \u03c0 is chosen such that\n\u03c0\u2208_\u03c1\u2208\u0394(D) g(\u03c1), where g(\u03c1)=max _x \u2208 Dx_V(\u03c1)^-1^2 and V(\u03c1)=\u2211_x \u2208 D\u03c1(x) x x^\u22a4 for any \u03c1\u2208\u0394(D).\n A similar exploration strategy can also be found in the work of <cit.>. Subsequently, our algorithm samples action from the mixed distribution p_t, and observes the corresponding loss \u2113_t(x_t) (Line <ref>).\n \nAt the end of each round t, our algorithm constructs a least squares loss estimate \u2113\u0302_t(x) for each x\u2208 D (Line <ref>):\n\n    \u2113\u0302_t(x) = x^\u22a4\u03a3_t^-1x_t \u2113_t(x_t) ,\n\nwhere \u03a3_t = \u2211_x\u2208 D p_t(x)xx^\u22a4 is the covariance matrix in terms of p_t. It is clear that \u2113\u0302_t is an unbiased estimate of the true loss \u2113_t, conditioned on p_t.\n\n\n\n\n\n\n\n\n\n\n\n  \nComparisons with Previous Methods \nAt a high level, our algorithm and previous works achieving the BoTW results share the similar idea that using the FTRL framework together with a carefully chosen regularizer to achieve (near) optimal regret guarantees in both the adversarial and (corrupted) stochastic environments <cit.>. \nHowever, perhaps surprisingly, our algorithm uses a rather common negative entropy entropy as the regularizer. This brings the computational efficiency compared with their complex or even hybrid regularizers, but also requires new ingredients that are not leveraged in previous works to obtain the desired BoTW regret guarantees for linear bandits.\n\nBesides, compared  with the algorithm of <cit.> that also obtains the BoTW regret guarantees for linear bandits, our algorithm departs from theirs significantly since we do not need to determine the nature of the environment, which is required by their detect-switch type algorithm.\nConsequently, our algorithm is conceptually simpler than that of <cit.>  and has the same adaptivity to all kinds of environments. More importantly, our algorithm has a regret guarantee with the same order as that of <cit.> in adversarial environment, and a tighter regret guarantee than theirs in corrupted stochastic environment, as we shall see in the analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the following theorem, we present the regret guarantees in all environments of Algorithm <ref>.\n\n\nthmthmregret\nChoose \u03c8_t(p) = \u03b2_t \u2211_x\u2208 D p(x)ln p(x):= -\u03b2_t H(p), \u03b3_t = ming(\u03c0)/\u03b2_t,1/2 where \u03b2_t is of the following value\n\n    d\u221a(ln T/ln |D|)+\u2211_\u03c4=1^t-1d\u221a(ln T/ln |D|)/\u221a(1+ln |D|^-1\u2211_s=1^\u03c4 H(q_s) ) .\n \nOur algorithm <ref> satisfies that:\n\n    \n  * In the adversarial environment, the regret can be upper bounded by\n    \n    R(T) \u2264    = Od\u221a(Tln T ln(|D|T)) .\n\n    \n  * In the stochastic environment with corruption level C, the regret can be bounded by\n    \n    R(T) \u2264 O  d^2ln T ln (|D|T) /\u0394_min + \u221a(Cd^2ln T ln (|D|T)/\u0394_min) .\n\n\n    \n    \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \nDiscussions. \nDue to the simple design and the potential to achieve better theoretical guarantees like previous works for multi-armed bandits (MAB), we believe deriving BOTW analysis for an FTRL-type algorithm is worthwhile even though a detect-switch one exists.  \nOur work provides a solution for the open problem in <cit.>. \n\n\nOne of the main challenges is to discover the optimal exploration rate in the stochastic setting as pointed out by <cit.>.    \nSimilar to previous FTRL algorithms for the multi-armed bandit problem <cit.>, the instance-dependent regret mainly relies on the self-bounding inequality that lower bounds the regret by the selecting probability of arms. However, in linear bandits, the regret upper bound does not explicitly depend on the selecting probability of each arm due to the linear structure and simply decomposing the regret by the regret caused by each arm may bring the dependence on the number of arms, which is not realistic in linear bandit problem as the number of arms can be huge. \nSo we mainly care about the minimum reward gap \u0394_min that characterizes the problem hardness and designs the self-bounding inequality by treating all sub-optimal arms together as shown in Lemma <ref>.\n\n\n\n(Self-bounding inequality)\n\nAlgorithm <ref> with Definition <ref> satisfies that\n\n    R(T) \u22651/2\u2211_t=1^T (1-q_t(x^*))\u0394_min -  C  .\n\n\n\n\n\n\n\n    R(T)    \u2265\u2211_t=1^T \u2113_t(x_t) - \u2113_t(x^*) - C\n       = \u2211_t=1^T \u2113_t, p_t - p^* - C\n       \u2265\u2211_t=1^T (1-\u03b3_t) \u2113_t, q_t - p^*  - C\n       = \u2211_t=1^T (1-\u03b3_t) \u2211_x q_t(x)  \u0394(x)    - C \n       \u22651/2\u2211_t=1^T 1-q_t(x^*)\u0394_min - C ,\n\nwhere the last inequality holds by choosing \u03b3\u2264 1/2. \n\n\n\n\n\n\n\n\n\nThe other key to achieving the BoTW results is the choice of the learning rate. \nBased on the standard analysis of the FTRL algorithm, the regret can be decomposed as \n\n    R(T) \u2264 O\u2211_t=1^T 1/\u03b2_t  + (\u03b2_t+1 -\u03b2_t) H(q_t+1) .\n\nIn the adversarial setting, we hope to get an O(\u221a(T)) regret, the natural choice is to select \u03b2_t = O(\u221a(t)) as both \u2211_t 1/\u221a(t) = O(\u221a(t)) and \u2211_t \u221a(t+1)-\u221a(t) = O(\u221a(t)). While in the stochastic setting, we aim to get an O(log T) regret, so it would be natural to choose \u03b2_t = O(t) as H(q_t) tends to be 0 when the policy always selects the unique optimal arm. Inspired by the motivation that H(q_t) can be regarded as a constant term in the adversarial setting  and a decreasing sequence that eventually tends to 0 in the stochastic setting, we set the learning rate \u03b2_t as O\u2211_\u03c4=1^t 1/\u221a(\u2211_s=1^\u03c4H(q_s) ).\n\n\n\n\n\n\n\nThe cooperation of the specially designed self-bounding inequality  and the choice of \u03b2_t_t contributes to the final results. The detailed proof is deferred to the next section. \n\n\n\n\n\n\n\u00a7 THEORETICAL ANALYSIS\n\nThis section provides the proof of Theorem <ref>. \n\nWe first present some useful lemmas and the main proof would come later. \n\n\nThe regret can be decomposed in the form of the cumulative entropy of exploitation probability q_t as the following lemma. \n(Regret Decomposition)\n    By choosing \u03b2_t = c+\u2211_\u03c4=1^t-1c/\u221a(1+ln |D|^-1\u2211_s=1^\u03c4 H(q_s) ) and \u03b3_t = ming(\u03c0)/\u03b2_t,1/2, the regret can be bounded as\n    \n    R(T)=    Odln T/c\u221a(ln |D|)  + c\u221a(ln |D|)\u221a(\u2211_t=1^T H(q_t)) .\n\n\n\n\n    R(T) =   \u2211_t=1^T \u2113_t(x_t) - \u2113_t(x^*)\n    \n        =   \u2211_t=1^T \u2113_t, p_t - p^*\n    \n        =   \u2211_t=1^T (1-\u03b3_t) \u2113\u0302_t, q_t - p^* + \u2211_t=1^T  \u03b3_t\u2113_t, \u03c0 - p^*\n    \u2264   \u2211_t=1^T (1-\u03b3_t) \u2113\u0302_t, q_t - p^* + \u2211_t=1^T  \u03b3_t \n        \n    \u2264   \u2211_t=1^T (1-\u03b3_t) \u2113\u0302_t, q_t - q_t+1 - D_t(q_t+1,q_t) + \u03c8_t(q_t+1) -   \u03c8_t+1(q_t+1)\n       + \u03c8_T+1(p^*) - \u03c8_1(q_1) + \u2211_t=1^T  \u03b3_t \n    \n         =   \u2211_t=1^T (1-\u03b3_t) \u2113\u0302_t, q_t - q_t+1 - D_t(q_t+1,q_t) + (\u03b2_t+1-\u03b2_t) H(q_t+1)\n       + \u03c8_T+1(p^*) - \u03c8_1(q_1) + \u2211_t=1^T  \u03b3_t \n    \u2264   \u2211_t=1^T (1-\u03b3_t) \u2113\u0302_t, q_t - q_t+1 - D_t(q_t+1,q_t)_part  1 + (\u03b2_t+1-\u03b2_t) H(q_t+1) +\u03b2_1 ln |D|+ \u2211_t=1^T  \u03b3_t  \n    \u2264   \u2211_t=1^T  2d/\u03b2_t + (\u03b2_t+1-\u03b2_t) H(q_t+1)_part 2 +\u03b2_1 ln |D| \n    \u2264   \u2211_t=1^T  2d/\u03b2_t + 2c \u221a(ln |D| \u2211_t=1^T H(q_t))  +cln |D|    \n    \u2264   2d \u2211_t=1^T \u221a(1+ln |D|^-1\u2211_s=1^t H(q_t))/c\u00b7 t + 2c \u221a(ln |D| \u2211_t=1^T H(q_t))  +cln |D| \n    \u2264    Odln T/c\u221a(ln |D|)\u221a(\u2211_t=1^T H(q_t))  + c\u221a(ln |D|)\u221a(\u2211_t=1^T H(q_t)) .\n\nwhere Eq. (<ref>) holds due to x_t \u223c p_t, Eq. (<ref>) holds since \u2113\u0302_t is an unbiased estimator, Eq. (<ref>) is because \u2113_t, \u03c0 - p^*\u2264 1, Eq. (<ref>) is based on Lemma <ref>, Eq. (<ref>) holds according to Lemma <ref> and the choice of \u03b3_t =min g(\u03c0)/\u03b2_t, 1/2\u2264 d/\u03b2_t, Eq. (<ref>) and (<ref>) are due to Lemma <ref> and the choice of \u03b2_t, the last inequality is based on Jensen's inequality.  \n\n\nWe next provide Lemma <ref>,  <ref> and <ref> that is used in the above analysis. \n\n\n\n\n\n\n(Decomposition of FTRL, Exercise 28.12 in <cit.>)\n    \n    \u2211_t=1^T  \u2113\u0302_t, q_t - p^*\u2264   \u2211_t=1^T \u2113\u0302_t, q_t - q_t+1 - D_t(q_t+1,q_t) + \u03c8_t(q_t+1) -   \u03c8_t+1(q_t+1)\n       + \u03c8_T+1(p^*) - \u03c8_1(q_1) .\n\n\n\n\n\n\n\n\n(Bound Part 1)\n    \n    \u2211_t=1^T (1-\u03b3_t) \u2113\u0302_t, q_t - q_t+1 - D_t(q_t+1,q_t)\u2264\u2211_t=1^T 2d/\u03b2_t .\n\n\n\n    \n    \u2211_t=1^T (1-\u03b3_t) \u2113\u0302_t, q_t - q_t+1 - D_t(q_t+1,q_t)\n    \u2264   \u2211_t=1^T (1-\u03b3_t)\u03b2_t \u2211_x\u2208 Dq_t(x)exp(-\u2113\u0302_t(x)/\u03b2_t ) +\u2113\u0302_t(x)/\u03b2_t -1  \n    \u2264   \u2211_t=1^T (1-\u03b3_t)\u03b2_t \u2211_x\u2208 Dq_t(x)\u2113\u0302^2_t(x)/\u03b2^2_t  \n    \n            =   \u2211_t=1^T (1-\u03b3_t)/\u03b2_t\u2211_x\u2208 Dq_t(x)\u2113\u0302^2_t(x)   \n    \u2264   \u2211_t=1^T 1/\u03b2_t\u2211_x\u2208 Dp_t(x)\u2113\u0302^2_t(x)    ,\n\nwhere Eq. (<ref>) is based on <cit.>, Eq. (<ref>) holds due to exp(-a)+a-1\u2264 a^2 when a\u2265 -1 and Lemma <ref>, Eq. (<ref>) is because q_t  = (p_t - \u03b3_t \u03c0)(1-\u03b3_t) \u2264 p_t/(1-\u03b3_t). \n\n    Recall that \u2113\u0302_t(x) = x^\u22a4\u03a3_t^-1 x_t \u2113_t(x_t). We have\n    \n    \u2113_t(x)^2 = \u2113_t(x_t)^2 x_t^\u22a4\u03a3_t^-1 x x^\u22a4\u03a3_t^-1 x_t \u2264 x_t^\u22a4\u03a3_t^-1 x x^\u22a4\u03a3_t^-1 x_t\n\n    and further, \n    \n    \u2211_x p_t(x) \u2113_t(x)^2 \u2264\u2211_x p_t(x)x_t^\u22a4\u03a3_t^-1 x x^\u22a4\u03a3_t^-1 x_t  = x_t^\u22a4\u03a3_t^-1 x_t = (x_tx_t^\u22a4\u03a3_t)  .\n\n    Above all, \n    \n    \u2211_x p_t(x) \u2113_t(x)^2\u2264\u2211 x x^\u22a4\u03a3_t = d  .\n\n    Lemma <ref> is then proved. \n\n\n\n\n\n(Bound Part 2)\n    By choosing \u03b2_t = c+\u2211_\u03c4=1^t-1c/\u221a(1+ln |D|^-1\u2211_s=1^\u03c4 H(q_s) ),\n    \n    \u2211_t=1^T (\u03b2_t+1-\u03b2_t)H(q_t+1) \u2264 2c \u221a(ln |D| \u2211_t=1^T H(q_t)) .\n\n\n\n    Based on the definition of \u03b2_t, it holds that\n    \n    \u2211_t=1^T (\u03b2_t+1-\u03b2_t)H(q_t+1) =    \u2211_t=1^T c/\u221a(1+ln |D|^-1\u2211_s=1^t H(q_s) ) H(q_t+1) \n    \u2264   \u221a(ln |D|)\u2211_t=1^T c/\u221a(ln |D|+ \u2211_s=1^t H(q_s) ) H(q_t+1) \n    \n            =    2\u221a(ln |D|)\u2211_t=1^T c/\u221a(ln |D|+ \u2211_s=1^t H(q_s) ) + \u221a(ln |D|+ \u2211_s=1^t H(q_s) ) H(q_t+1)\n    \u2264    2\u221a(ln |D|)\u2211_t=1^T c/\u221a(\u2211_s=1^t+1 H(q_s) ) + \u221a(\u2211_s=1^t H(q_s) ) H(q_t+1) \n    \n            =    2c\u221a(ln |D|)\u2211_t=1^T \u221a(\u2211_s=1^t+1 H(q_s) ) - \u221a(\u2211_s=1^t H(q_s) )\n    \n            =    2c\u221a(ln |D|)\u221a(\u2211_s=1^T+1 H(q_s) ) - \u221a(  H(q_1) )\n    \u2264     2c\u221a(ln |D|)\u221a(\u2211_t=1^T H(q_t) ) ,\n\n    where the last two inequalities holds since H(q_t) \u2264 H(q_1) = ln |D| for any t. \n\n\n    Denote \u03a3(\u03c0) = \u2211_x\u2208 D\u03c0(x) xx^\u22a4 and g(\u03c0) = max_x\u2208 D x^\u22a4\u03a3(\u03c0)^-1 x. By choosing \u03b3_t =\n    ming(\u03c0)/\u03b2_t,1/2, we have \u2113\u0302_t(x)/\u03b2_t \u2265 -1 for all x\u2208 D. \n\n\n\n\n\nSince \u03a3_t = \u2211_x p_t(x) xx^\u22a4 = \u2211_x  (1-\u03b3_t)q_t(x) + \u03b3_t \u03c0(x) xx^\u22a4\u227d\u2211_x \u03b3_t \u03c0(x) xx^\u22a4 = \u03b3_t\u03a3(\u03c0), we have \n\n    \u03a3_t^-1\u227c1/\u03b3_t\u03a3(\u03c0)^-1 .\n\nFurther, \n\n    x^\u22a4\u03a3_t^-1x_t \u2264x_\u03a3_t^-1x_t_\u03a3_t^-1\u2264max_v\u2208 D v^\u22a4\u03a3_t^-1v \u22641/\u03b3_tmax_v\u2208 D v^\u22a4\u03a3(\u03c0)^-1v = g(\u03c0)/\u03b3_t .\n\nWith the choice of \u03b3_t, it holds that\n\n    \u2113\u0302_t(x)/\u03b2_t = 1/\u03b2_t x^\u22a4\u03a3_t^-1 x_t \u2113_t(x_t)  \u22641/\u03b2_t x^\u22a4\u03a3_t^-1 x_t  \u2264g(\u03c0)/\u03b2_t \u03b3_t\u2264 1 ,\n\nThe last inequality is due to the choice of \u03b2_t and the fact that g(\u03c0)\u2264 d. \n\n\n\n\n\n\n\nRecall that we have already decomposed the regret by the cumulative entropy of the exploitation probability q_t_t. The next lemma further links the cumulative entropy with the expected selecting times on sub-optimal arms. \n\n(Relationship between H(q_t) and regret)\n    \n    \u2211_t=1^T H(q_t) \u2264\u2211_t=1^T(1-q_t(x^*))lne|D|T/\u2211_t=1^T(1-q_t(x^*))\n\n\n\n\n\n    According to the definition of H(p) for a distribution p over D and the fact that the optimal arm x^* \u2208 D is unique, we can bound H(p) as\n    \n    H(p) = \u2211_x p(x)ln1/p(x) =     p(x^*)lnp(x^*)+1-p(x^*)/p(x^*) + \u2211_x\u2260 x^*p(x)ln1/p(x)\n    \u2264    p(x^*)1-p(x^*)/p(x^*) + \u2211_x\u2260 x^*p(x)ln1/p(x)\n    \u2264    1-p(x^*) + (|D|-1) \u2211_x\u2260 x^*p(x)/|D|-1ln|D|-1/\u2211_x\u2260 x^*p(x)\n    \n            =    (1-p(x^*)) 1+ln|D|-1/(1-p(x^*)) ,\n\n    where the last inequality is due to Jensen's inequality. \n\n    Above all, \n    \n    \u2211_t=1^T H(q_t) \u2264   \u2211_t=1^T (1-q_t(x^*)) 1+ln|D|-1/(1-q_t(x^*))\n    \u2264   \u2211_t=1^T (1-q_t(x^*)) 1+lnT(|D|-1)/\u2211_t=1^T(1-q_t(x^*))\n    \u2264   \u2211_t=1^T(1-q_t(x^*))lne|D|T/\u2211_t=1^T(1-q_t(x^*)) .\n\n\n\nBased on Lemma <ref> and <ref>, we are now able to upper bound the regret as below. \n\n    \n    R(T) \u2264    Odln T/c\u221a(ln |D|) +c\u221a(ln |D|)\u221a(\u2211_t=1^T H(q_t))\n    \u2264    Odln T/c\u221a(ln |D|) +c\u221a(ln |D|)\u221a(\u2211_t=1^T(1-q_t(x^*))lne|D|T/\u2211_t=1^T(1-q_t(x^*))_term A)\n    \u2264     Odln T/c\u221a(ln |D|) +c\u221a(ln |D|)\u221a(lne|D|Tmaxe, \u2211_t=1^T(1-q_t(x^*)) )\n    \n           =    Odln T/c\u221a(ln |D|) +c\u221a(ln |D|)\u221a(lne|D|Tmaxe, \u2211_t=1^T(1-q_t(x^*)))\n    \n           =    O d\u221a(ln T)\u00b7\u221a(lne|D|Tmaxe, \u2211_t=1^T(1-q_t(x^*))) ,\n\n    where Eq. (<ref>) holds since term A \u2264\u2211_t=1^T(1-q_t(x^*)) ln |D|T if \u2211_t=1^T(1-q_t(x^*)) > e and term A \u2264 e ln e|D|^2T/(|D|-1)=O(elne|D|T) otherwise as \u2211_t=1^T(1-q_t(x^*)) \u2265 1-q_1(x^*) = 1-1/|D|; Eq. (<ref>) is due to c=d\u221a(ln T/ln |D|).  \n\n\n    \n\n\n  \nRegret analysis in the adversarial setting.\n    \n    According to Eq. (<ref>), since \u2211_t=1^T (1-q_t(x^*)) \u2264 T, it holds that\n    \n    R(T) \u2264    Od\u221a(ln T)\u221a(lne|D|T\u00b7 T   ) = Od\u221a(Tln T ln(|D|T)) .\n\n\n\n\n  \nRegret analysis in the stochastic setting with corruptions.\nAccording to Lemma <ref>, \n\n    \n    R(T) =    (1+\u03bb)R(T) - \u03bb R(T) \n    \u2264    O( (1+\u03bb)\u00b7 d\u221a(ln T)\u00b7\u221a(lne|D|Tmaxe, \u2211_t=1^T(1-q_t(x^*))). \n       . -1/2\u03bb\u2211_t=1^T (1-q_t(x^*))\u0394_min +\u03bb C  ) \n    \u2264       O(1+\u03bb)^2 d^2ln T ln (|D|T) /2\u03bb\u0394_min + \u03bb C \n    \n            =    Od^2ln T ln (|D|T)/2\u03bb\u0394_min +   d^2ln T ln (|D|T) /\u0394_min + \u03bb  d^2ln T ln (|D|T) /2\u0394_min + \u03bb C \n    \u2264    O  d^2ln T ln (|D|T) /\u0394_min + \u221a(Cd^2ln T ln (|D|T)/\u0394_min) ,\n\nwhere Eq. (<ref>) holds since a\u221a(x) - bx/2 = a^2/2b - 1/2 a/\u221a(b) - \u221a(bx)\u2264 a^2/2b\nfor any a,b,x\u2265 0; Eq. (<ref>) holds by choosing \u03bb = \u221a( d^2ln T ln (|D|T)/(2\u0394_min)/ d^2ln T ln (|D|T)/(2\u0394_min) +C   ). \n\nFurther, in the purely stochastic setting with C=0, we can conclude R(T) = O  d^2ln T ln (|D|T) /\u0394_min.\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\nIn this work, we establish the first FTRL-type BoTW algorithm for linear bandits. Our algorithm does not distinguish between the (corrupted) stochastic and adversarial environments and hence has better adaptivity than the detect-switch type algorithm of <cit.> for the same setting.\nWhen using the negative  Shannon entropy as a regularizer, we prove that our algorithm achieves O(d^2log^2 T/\u0394_min+\u221a(Cd^2log^2 T/\u0394_min)) and  O(d\u221a(Tlog^2 T )) regret guarantees in \ncorrupted stochastic and adversarial environments respectively. These results achieve similar or even better orderings than the previous detect-switch method <cit.> with a much simpler algorithmic design. \nAnd as a special case of linear MDP, we believe our analysis for linear bandits also provides new insights into the BoTW problem in linear MDPs.\n\n   \nStochastic linear bandit problem has a special property that the optimal result is the solution of an optimization problem which has no explicit form <cit.>. And our current approach provides a d/\u0394_min dependence, a natural upper bound for the solution of this optimization problem. \nOne interesting future direction would be to investigate such optimal exploration rates under FTRL-type algorithms and derive regret bounds in the stochastic setting that enjoys the form of the solution to the optimization problem like <cit.>.\nAnalyzing the FTRL algorithm for the BoTW problem in linear MDPs is also an important future work. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnamed\n\n\n\n"}