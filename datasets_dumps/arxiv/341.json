{"entry_id": "http://arxiv.org/abs/2303.06853v1", "published": "20230313044906", "title": "Representation Learning for Stack Overflow Posts: How Far are We?", "authors": ["Junda He", "Zhou Xin", "Bowen Xu", "Ting Zhang", "Kisub Kim", "Zhou Yang", "Ferdian Thung", "Ivana Irsan", "David Lo"], "primary_category": "cs.SE", "categories": ["cs.SE"], "text": "\n\n\n\n\n\n\n\n\n\n\n\njundahe@smu.edu.sg\n\n  Singapore Management University\n  Singapore\n  Singapore\n\n\n\nxinzhou.2020@phdcs.smu.edu.sg\n\n  Singapore Management University\n  Singapore\n  Singapore\n\n\n\nbowenxu@smu.edu.sg\n\n  Singapore Management University\n  Singapore\n  Singapore\n\n\n\ntingzhang.2019@phdcs.smu.edu.sg\n\n  Singapore Management University\n  Singapore\n  Singapore\n\n\n\nkisubkim@smu.edu.sg\n\n  Singapore Management University\n  Singapore\n  Singapore\n\n\n\nzyang@smu.edu.sg\n\n  Singapore Management University\n  Singapore\n  Singapore\n\n\n\nferdianthung@smu.edu.sg\n\n  Singapore Management University\n  Singapore\n  Singapore\n\n\n\nivanairsan@smu.edu.sg\n\n  Singapore Management University\n  Singapore\n  Singapore\n\n\n\ndavidlo@smu.edu.sg\n\n  Singapore Management University\n  Singapore\n  Singapore\n\n\n\n  <ccs2012>\n     <concept>\n         <concept_id>10010147.10010178.10010187</concept_id>\n         <concept_desc>Computing methodologies\u00a0Knowledge representation and reasoning</concept_desc>\n         <concept_significance>300</concept_significance>\n         </concept>\n     <concept>\n         <concept_id>10011007.10011074.10011099.10011693</concept_id>\n         <concept_desc>Software and its engineering\u00a0Empirical software validation</concept_desc>\n         <concept_significance>500</concept_significance>\n         </concept>\n   </ccs2012>\n\n  \n[300]Computing methodologies\u00a0Knowledge representation and reasoning\n[500]Software and its engineering\u00a0Empirical software validation\n\n\nThe tremendous success of Stack Overflow has accumulated an extensive corpus of software engineering knowledge, thus motivating researchers to propose various solutions for analyzing its content.\nThe performance of such solutions hinges significantly on the selection of representation model for Stack Overflow posts.\nAs the volume of literature on Stack Overflow continues to burgeon, it highlights the need for a powerful Stack Overflow post representation model and drives researchers' interest in developing specialized representation models that can adeptly capture the intricacies of Stack Overflow posts.\n\nThe state-of-the-art (SOTA) Stack Overflow post representation models are Post2Vec and BERTOverflow, which are built upon trendy neural networks such as convolutional neural network (CNN) and Transformer architecture (e.g., BERT). \nDespite their promising results, these representation methods have not been evaluated in the same experimental setting. To fill the research gap, we first empirically compare the performance of the representation models designed specifically for Stack Overflow posts (Post2Vec and BERTOverflow) in a wide range of related tasks, i.e., tag recommendation, relatedness prediction, and API recommendation.\nThe results show that the Post2Vec cannot further improve each state-of-the-art technique of downstream tasks, and BERTOverflow shows surprisingly poor effectiveness.\nTo find more suitable representation models for the posts, we further explore a diverse set of BERT-based models, including (1) general domain language models (RoBERTa and Longformer) and (2) language models built with software engineering-related textual artifacts (CodeBERT,  GraphCodeBERT, and seBERT).\nThis exploration shows that CodeBERT and RoBERTa are generally the most suitable for representing Stack Overflow posts.\nHowever, it also illustrates the \u201cNo Silver Bullet\u201d concept, as none of the models consistently wins against all the others.\nInspired by the findings, we propose SOBERT, which employs a simple-yet-effective strategy to improve the best-performing model by continuing the pre-training phase with the textual artifact from Stack Overflow. \nThe overall experimental results demonstrate that SOBERT can consistently outperform the considered models and increase the state-of-the-art performance by a significant margin for all the downstream tasks. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepresentation Learning for Stack Overflow Posts: How Far are We?\n    David Lo\n    Received: date / Accepted: date\n=================================================================\n\n\n\n\u00a7 INTRODUCTION\n\n\nServing as the most popular software question and answer (SQA) forum, (SO) has dramatically influenced modern software development practice. As of March 2023, the forum has accumulated more than 24 million questions and 35 million answers[<https://stackexchange.com/sites?view=list#traffic>]. is broadly recognized as an invaluable knowledge base and supplemental resource for the software engineering (SE) domain\u00a0<cit.>, which triggered the increased interest of researchers and software developers in a wide range of post-related tasks, for example, recommendation of post tags (aka. tag recommendation)\u00a0<cit.>, recommendation of APIs according to a natural language query (aka. API recommendation)\u00a0<cit.>, and the identification of related posts (aka. relatedness prediction)\u00a0<cit.>. \n\nAn essential step in yielding promising results for these -related tasks is to obtain suitable representations of the posts. A beneficial representation model can capture the semantic concept of the posts and reveal more explanatory features from the hidden dimensions. As the volume of SE literature on solving -related tasks\u00a0<cit.> cotinues to burgeon, it has underscored the demand for a quality representation.\n\n\n\n\n\nOver the years, numerous representation models have been specifically proposed for modeling posts. \nXu et al. proposed Post2Vec\u00a0<cit.>, a CNN-based\u00a0<cit.> representation model that leverages the tags of a post to guide the learning process and models the post as the combination of three complementary components (i.e., title, description, and code snippet). Their experimental results demonstrate that it can substantially boost the performance for a wide range of posts-related tasks\u00a0<cit.>. Tabassum et al.\u00a0<cit.> leveraged the more advanced BERT-based architecture and pre-trained BERTOverflow based on 152 million sentences from . Results showed that the embedding generated by BERTOverflow led to a significant improvement over other off-the-shelf models (e.g., ELMo\u00a0<cit.> and BERT\u00a0<cit.>) in the software named entity recognition (NER) task.\n\nAlthough these existing -specific methods have been proven to be beneficial, the effectiveness of Post2Vec is only evaluated\non limited solutions (i.e., Support Vector Machine\u00a0<cit.> and Random Forest\u00a0<cit.>) and BERTOverflow only experimented for the NER task. These motivate us to further study the performance of existing -specific representation models on a diverse set of tasks. Unexpectedly, we found that both Post2Vec and BERTOverflow perform poorly. Such findings motivate us to explore the effectiveness of a larger array of representation techniques in modeling posts.\n\n\nIn addition to the aforementioned -specific representation models, we further consider five BERT-based language models which could be potentially suitable for post representation learning. They are CodeBERT\u00a0<cit.>, GraphCodeBERT\u00a0<cit.>, seBERT\u00a0<cit.>, RoBERTa\u00a0<cit.>, and Longformer\u00a0<cit.>. CodeBERT is a SE knowledge-enriched BERT-based language model that utilizes CodeSearchNet\u00a0<cit.> dataset at the pre-training stage. GraphCodeBERT further incorporates data flow graphs and structure-aware pre-training (i.e., edge prediction and node alignment) and it outperforms CodeBERT in various code-related tasks. Different from the previous models which are based on a BERT_Base architecture, seBERT is a BERT_Large model and is pre-trained with data from GitHub commit messages and issues, Jira issues, and posts. \nCodeBERT, GraphCodeBERT, and seBERT are considered to be better at capturing the semantics of technical jargon of the SE domain. Finally, we also include models from the general domain as they are usually trained with a more diverse amount of data than domain-specific models. RoBERTa is one of the most popular BERT-based language models. It is trained with larger batch size and learning rates compared with the original BERT. Longformer is also considered as it overcomes the input length limit of conventional BERT-based language models. While BERT-based language models could maximumly accept an input length of 512 tokens, more than 50% of the posts have the surpass 512 limit\u00a0<cit.>. In contrast, Longformer could accept a maximum of 4,096 tokens as its input. \n\n\n\n\n\n\n\n\n\n\nWe evaluate the performance of the aforementioned representation models on multiple  -related downstream tasks (i.e., tag recommendation, API recommendation, and relatedness prediction). Furthermore, we build SOBERT, a stronger BERT-based language model for modeling posts. \nOur experimental results reveal several interesting findings:\n\n\n\n  * Existing post representation techniques fail to improve the SOTA performance of considered tasks. Xu et al. demonstrated that the addition of the feature vectors generated by Post2Vec is beneficial for improving the post representation for traditional machine learning techniques. However, we discover that appending the feature vectors from Post2Vec\u00a0<cit.> does not derive a beneficial effect on considered deep neural networks. \nFurthermore, we reveal that the embedding generated by BERTOverflow could only achieve reasonable performance in the API recommendation task and give surprisingly poor performance in the tag recommendation task.\n\n\n\n\n\n\n  * Among all the considered models, none of them can always perform the best. According to our experiment results, although the newly considered models can outperform the SOTA approaches, none of them can always perform the best. It motivates us to build an extensive model. \nOverall, CodeBERT produces the most promising representation among the considered models, and Longformer fails to beat conventional BERT-based language models, although it is expected to be capable of accepting a longer input. \n\n  * Continued pre-training based on textual artifact develops a consistently better model. We propose SOBERT by further pre-training with data. The overall results show that SOBERT consistently boosts the performance in all three considered tasks, implying a better representation.\n  \nOverall, we summarize the  contributions of our empirical study as follows: \n\n\n  * We comprehensively evaluate the effectiveness of seven representation models for posts in three downstream tasks.\n\n  * We propose SOBERT by pre-training based on 20 million posts from and show that SOBERT consistently outperforms other representation models in multiple downstream tasks.\n\n  * We derive several insightful lessons from the experimental results to the software engineering community.\n\n\nThe rest of the paper is organized as follows. Section <ref> categorizes representation learning models into three groups and briefly describes them.\nWe formulate the downstream tasks (i.e., tag recommendation, API recommendation, relatedness prediction) and their corresponding state-of-the-art method in Section <ref>.\nSection <ref> introduces our research questions and the experiment settings. \nIn Section <ref>, we answer the research question and report the experiment results. \nSection <ref> further analyzes the result and elaborates the insights with evidence. \nSection <ref> describes related works, and Section <ref> summarizes this study.\n\n\n\n\n\n\n\n \n\n\n\n\n\u00a7 REPRESENTATION LEARNING MODELS\n\n\nIn this section, we summarize the considered representation models for this paper. We explore a wide range of techniques across the spectrum of representing posts, including two Transformer-based Pre-trained Models (PTM) from the general domain (RoBERTa\u00a0<cit.> and Longformer\u00a0<cit.>), three SE-domain specific PTMs (CodeBERT\u00a0<cit.>, GraphCodeBERT\u00a0<cit.>, and seBERT\u00a0<cit.>) and two -specific post representation models (BERTOverflow\u00a0<cit.> and Post2Vec\u00a0<cit.>).\n\n\n \u00a7.\u00a7 BERT-based Language Models\n\n\n\nBERT (Bidirectional Encoder Representations from Transformers)\u00a0<cit.> based language models have revolutionized the representation learning of natural language\u00a0<cit.> by achieving phenomenal performance in a wide range of natural language processing (NLP) tasks, such as sentiment analysis\u00a0<cit.>, POS tagging\u00a0<cit.>, question answering\u00a0<cit.>. BERT-based language models inherit the Transformer\u00a0<cit.> architecture, whose self-attention mechanism can learn a bidirectional contextual representation of text.\nThese models usually perform the Masked Language Modeling (MLM) task in the pre-training phase. It initially corrupts the input data by randomly masking 15% of the tokens, and then it teaches the model to reconstruct the original data by predicting the masked words. BERT-based models are extensively pre-trained on large-scale datasets, which learn a meaningful representation that is reusable for various tasks, thus eliminating the process of training language models from scratch and saving a drastic amount of time and resources. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Existing Models for Posts\n\n\nBERTOverflow\u00a0<cit.>\nkeeps the original BERT_base architecture, and it leverages 152 million sentences and 2.3 billion tokens from to pre-train -specific word embeddings. The authors have leveraged the embedding generated by BERTOverflow to implement a software-related named entity recognizer (SoftNER). The performance of SoftNER is experimented with the name entity recognition (NER) task for the software engineering domain, focusing on identifying code tokens or programming-related named entities that appear within SQA sites like . The results show that BERTOverflow outperforms all other models in the proposed task.\n\n\u00a0<cit.>\nis the latest approach proposed specifically for post representation learning\u00a0<cit.>. Unlike the existing models,  is designed with a triplet architecture.  leverages CNNs as feature extractors for each post to encode three components (i.e., title, text, and code snippets) separately from the post. \nThe corresponding three output feature vectors are then fed to a feature fusion layer to produce the representation of the post. \nIn the end,  uses tag information of the post, which is considered as the post's general semantic meaning to supervise the representation learning process. \nThe representation learned by  is then leveraged by enhancing the feature vectors in -related downstream tasks (e.g., relatedness prediction and API recommendation). \nFor each downstream task, in\u00a0<cit.>, the vector representation learned by  is combined with the feature vector produced by the corresponding state-of-the-art approach to form a new feature vector. \nFinally, the new feature vector is used to boost the performance of the corresponding model for the task.\nFollowing the experiment settings of Xu et al., we use Post2Vec as a complementary feature vector to the state-of-the-art approach in this paper. We concatenate the post representation generated by Post2Vec to the original feature vector of the state-of-the-art approach. We then leverage the concatenated feature vector in further training.  \n\n\n\n\n\n \u00a7.\u00a7 Models from General Domain\n\n\nRoBERTa\u00a0<cit.>\nis a replication study on the pre-training objectives, along with the impact of several key hyper-parameters of BERT\u00a0<cit.>. They then proposed their improved model on BERT, namely RoBERTa. In comparison with BERT, RoBERTa has made several modifications to the pre-training stage, including: (1) training with larger batch size, more data, and longer training time; (2) abandoning the next sentence prediction (NSP) task of BERT and showed that removal of NSP slightly improves the model efficiency; (3) training with longer sequences; (4) masking the training data dynamically rather than statically. \n\nPre-trained models like BERT\u00a0<cit.> and RoBERTa\u00a0<cit.> only accept a maximum input of 512 tokens. However, according to the statistics conducted by He et al.\u00a0<cit.>, more than half of the questions have more tokens than the given 512 limit. A simple workaround is to truncate the given input sequence to the acceptable length restriction. However, it increases the risk of losing vital information. The self-attention mechanism suffers from the O(n^2) quadratic computational complexity problem, which restricts the ability of Transformer-based models to model long sequences.\n\nLongformer\u00a0<cit.>\naims to alleviate the limitation in processing long sequences. It leverages a combination of sliding window attention and global attention mechanism such that the computational memory consumption scales linearly as the sequence becomes longer. In contrast to models like RoBERTa and CodeBERT, which could only accept a maximum of 512 tokens as input, Longformer supports sequences of length up to 4,096. Similar to CNN\u00a0<cit.>, Longformer lets each input token only attends to surrounding neighbors that are within a fixed window size. Denoting the window size as w, each token could only attend to 1/2w tokens on both sides, thus decreasing the computation complexity to O(n \u00d7 w). \n\nHowever, the sliding window may compromise the performance as it cannot capture the whole context. To compensate for the side-effect, global tokens are selected. Such tokens are implemented with global attention, which attends to all other tokens, and other tokens also attend to the global tokens. As previous work showed that more than 50% of the posts exceed the size limit of conventional BERT-based models (512), it motivates us to explore whether Longformer is better at representing posts.  \n\n\n\n\n\n\n \u00a7.\u00a7 Models from SE domain\n\n\nCodeBERT\u00a0<cit.>\nThe strong versatility and capability of Transformer-based representational models drive researchers' interest in adopting them to the SE domain. CodeBERT\u00a0<cit.> is a SE knowledge-enriched bi-modal pre-trained model, which is capable of modeling both natural languages (NL) and programming languages (PL). CodeBERT inherits the architecture of BERT\u00a0<cit.>, and it continues pre-training based on the checkpoint of RoBERTa\u00a0<cit.> with the NL-PL data pairs obtained from the CodeSearchNet dataset\u00a0<cit.>.\nIt has two pre-training objectives: Masked Language Modeling (MLM) and Replaced Token Detection (RTD). The eventual loss function for CodeBERT at the pre-training stage is the combination of both MLM and RTD objectives, where \u03b8 denotes the model parameters:\n\n    \u03b8mini( \u2112_RTD(\u03b8)+\u2112_MLM(\u03b8))\n\n\nThe CodeBERT model has shown great effectiveness in a diverse range of SE domain-specific activities, for example, code search\u00a0<cit.>, traceability prediction\u00a0<cit.>, and code translation\u00a0<cit.>. \n\n\nGraphCodeBERT\u00a0<cit.>\nincorporates a hybrid representation in source code modeling. Apart from addressing the pre-training process over NL and PL, GraphCodeBERT utilizes the data flow graph of source code as additional inputs and considers two structure-aware pre-training tasks (i.e., Edge Prediction and Node Alignment) aside from the MLM prediction task. GraphCodeBERT is evaluated in code search\u00a0<cit.>, clone detection\u00a0<cit.>, code translation\u00a0<cit.>, and code refinement\u00a0<cit.>,respectively. It outperforms CodeBERT and all the other baselines, including RoBERTa (code version)\u00a0<cit.>, Transformer\u00a0<cit.>, LSTM\u00a0<cit.>, under their experimental setting.\n\n\nseBERT\u00a0<cit.> aims to advance the previous PTMs in the SE context with a larger model architecture and more diverse pre-training data. The authors pre-trained seBERT with the BERT_large architecture, i.e., with 24 layers, a hidden layer size of 1024, and 16 self-attention heads, with a total of 340 million parameters. seBERT is pre-trained with more than 119GB of data from four data sources, i.e., posts, Github issues, Jira issues, and Github commit messages. The model's effectiveness is verified in three classification tasks, i.e., issue type prediction, commit intent prediction, and sentiment mining. Results showed that seBERT is significantly better than BERToverflow in these tasks. \n\n\n\u00a7 DOWNSTREAM TASKS\n\n\n\n\n In this section, we formulate the target problems that are used to measure the effectiveness of the representation models and then describe the corresponding state-of-the-art solution. We select multiple -related downstream tasks, which have been popular research topics for posts. \nTo be more specific, we consider: Tag Recommendation\u00a0<cit.>, API Recommendation\u00a0<cit.> and Relatedness Prediction\u00a0<cit.>, covering a multi-label classification problem, a multi-class classification problem, and a ranking problem. All selected tasks operate on the abstraction of a post, which could be benefited from a high-quality post representation.\n\n\n\n\t\n\n\n\n \u00a7.\u00a7 Tag Recommendation\n\nThe user-annotated tags of a post serve as helpful metadata and have a critical role in organizing the contents of posts across different topics. Suitable tags precisely summarize the message of a post, while redundant tags and synonym tags make it more difficult in maintaining the content of the site.\nA tag recommendation system could effectively simplify the tagging process and minimize the effect of manual errors, therefore, avoiding problems like tag synonyms and tag redundancy.\n\n\n  \u00a7.\u00a7.\u00a7 Task Formulation\n\nWe formulate the tag recommendation task as a multi-label classification problem. Given \ud835\udcb3 as a corpus of posts, and \ud835\udcb4 denotes the total collection of tags, we represent each post as x_i, where 0 \u2264 i \u2264 |X|, i \u2208\u2115 and the tag of each post as y_i \u2282\ud835\udcb4. The goal is to recommend the most relevant set of tags y_i to x_i. \n\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 State-of-the-art technique\n PTM4Tag\u00a0<cit.> leverages three pre-trained models to solve the tag recommendation problem. PTM4Tag leverages three pre-trained models, which are responsible for modeling the title, description, and code snippet, independently.\n\n\n\n\n\n\n \u00a7.\u00a7 API Recommendation\n\n\n\n\n\n\n\n\nQuestions related to Application Programming Interfaces (APIs) are one of the most viewed topics on \u00a0<cit.>. consists of an enormous amount of discussion about API usage. Developers are more intended to search for relevant posts and pick out the APIs that seem useful in the discussions\u00a0<cit.> rather than checking API documentation, which makes the primary source for building a dataset of the API recommendation task.\nThe modern software development process heavily relies on third-party APIs, which leads to the research of an automated API recommendation approach that is intended to simplify API search\u00a0<cit.>. \n\n\n  \u00a7.\u00a7.\u00a7 Task Formulation\n\nWe follow the exact task definition as the previous literature\u00a0<cit.>, with the goal of recommending relevant APIs that answer the question or implement the function for a given NL query.\n\n\n\n  \u00a7.\u00a7.\u00a7 State-of-the-art technique\n Wei et al.\u00a0<cit.> proposed CLEAR, an automated approach that recommends API by embedding queries and posts with a BERT-based PTM (distilled version of the RoBERTa[<https://huggingface.co/distilroberta-base>]). To be more specific, given a natural language query, CLEAR initially picks a sub-set of candidate posts based on the embedding similarity to reduce the search space. Then, CLEAR ranks the candidate posts and recommends the APIs from the top-ranked posts. \n\n\n\n \u00a7.\u00a7 Relatedness Prediction\n\nThe notion of a Knowledge Unit (KU) is defined as a set containing a question along with all its answers\u00a0<cit.>. To find a comprehensive technical solution for a given problem, developers usually need to summarize the information from multiple related KUs. However, searching for related KUs can be time-consuming as the same question can be rephrased in many different ways. Thus, researchers have proposed several techniques to automate the process of identifying the related KUs\u00a0<cit.>, which could significantly improve the efficiency of the software development cycle.\n\n\n  \u00a7.\u00a7.\u00a7 Task Formulation\n \n The task is commonly formulated as a multi-class classification problem\u00a0<cit.>. The relatedness between questions is classified into four classes, from the most relevant to irrelevant, which are:\n\n    \n  * Duplicate: The Two KUs correspond to a pair of semantically equivalent questions. The answer of one KU can also be used to answer another KU. \n    \n  * Direct: One KU is beneficial in answering the question in another KU, for example, by explaining certain concepts and giving examples.\n    \n  * Indirect: One KU provides relevant information but does not directly answer the questions of another KU.\n    \n  * Isolated: The two KUs are semantically uncorrelated.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 State-of-the-art technique\n Recently, Pei et al. introduced ASIM\u00a0<cit.>, which yielded state-of-the-art performance in the relatedness prediction task. Pei et al. pre-trained word embeddings specialized to model posts with a corpus collected from the data dump. Then ASIM uses BiLSTM\u00a0<cit.> to extract features from posts and implements the attention mechanism to capture the semantic interaction among the KUs. \n\n\n\n\n\n\n\n\n\n\u00a7 RESEARCH QUESTIONS AND EXPERIMENTAL SETTINGS\n\n\nIn this section, we first introduce our research questions and then describe the corresponding experiment settings.\n\n\n\n \u00a7.\u00a7 Research Questions\n\n\n\n\n  \u00a7.\u00a7.\u00a7 RQ1. How effective are the existing post representation models?\n\n\nVarious methods have been proposed in modeling posts. However, there is still a lack of analysis of the existing -specific representation methods. For instance, Xu et al.\u00a0<cit.> have demonstrated that Post2Vec is effective in boosting the performance of traditional machine learning algorithms, i.e., support vector machine (SVM) and Random Forest. However, the efficacy of Post2Vec in facilitating deep learning-based models has not yet been investigated. Moreover, Tabassum et al.\u00a0<cit.> only leveraged the embeddings from BERTOverflow in the software-related NER task, but not for other popular -related tasks. In light of this research gap, we aim to evaluate the current -specific representation methods for popular -related tasks under the same setting for this research question. \n\n\n\n  \u00a7.\u00a7.\u00a7 RQ2. How effective are the popular BERT-based language models for the targeted downstream tasks?\n\n\nIn addition to the existing representation models, we explore the effectiveness of a wider spectrum of representation models. \nBERT-based language models have shown great performance and generalizability in representation learning. \nRepresentations generated by such models have demonstrated promising performance in a broad range of tasks with datasets of varying sizes and origins.\nBorrowing the best-performing representation models from various domains and investigating their performance can derive interesting results, as recent literature\u00a0<cit.> have revealed that they are potentially great candidates for representing posts as well. \nThis motivates us to employ RoBERTa\u00a0<cit.> and Longformer\u00a0<cit.> from the general domain and CodeBERT\u00a0<cit.>, GraphCodeBERT\u00a0<cit.>, and seBERT\u00a0<cit.> from the SE domain.\nWe set up the exact same experimental settings for each model. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 RQ3. Is further pre-training on data helpful in building a better model?\n\n\nFurther pre-trained models with domain-specific corpus have been common practice in the NLP domain, however, their effectiveness is not verified for representing posts. \nIn this RQ, we introduce SOBERT, which is obtained by continuing the pre-training process on CodeBERT with data, and we aim to investigate whether further pre-training with data improves the performance. \n\n\n\n \u00a7.\u00a7 Experimental Settings\n\n\n\n  \u00a7.\u00a7.\u00a7 Tag Recommendation\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Dataset\n\n\n\nThe dataset used by He et al.\u00a0<cit.> in the training of PTM4Tag only includes the posts dated before September 5, 2018. \nTo address this limitation, we use the data dump released in August of 2022 to construct a new dataset for our experiment. Idealy, a tag recommendation approach should only learn from high-quality questions. Therefore, we remove the low-quality questions when constructing the dataset.\nAccording to the classificaition of question quality defined by Ponzanelli et al.\u00a0<cit.>, we first filter out the questions which do not have an accepted answer and further removed the questions with a score of less than 10.\n\nMoreover, we remove the rare tags and rare posts. Previous literature in tag recommendation\u00a0<cit.> has defined a tag as rare if it occurs less than 50 times within the dataset, and a post is considered rare if all of its tags are rare tags. The usage of rare tags is discouraged since it implies the unawareness of the tag among developers. We follow the same definition as the previous literature and set the frequency threshold for rare tags as 50. \n\nIn the end, we obtain a dataset of 527,717 posts and 3,207 tags. We split the dataset into a training set, a validation set, and a test set according to the 8:1:1 ratio, which corresponds to 422,173, 52,772, and 52,772 posts, respectively.\n\n\n\n  \u00a7.\u00a7.\u00a7 Evaluation Metrics\n\n\n\nWe report the performance for this task using Precision@k, Recall@k, and F1-score@k, where k indicates the top-k recommendations. Such metrics are extensively used in previous works\u00a0<cit.>, and we calculate the average score for each of them. Mathematically speaking, the evaluation metrics are computed as follows:\n\n    Precision@k =\n    |Tag_True\u2229Tag_Predict|/k\n\n\n\n    Recall@k_i =   | Tag_True\u2229Tag_Predict/k|    if  | Tag_True| > \n          \n          k\n    | Tag_True\u2229Tag_Predict | /|Tag_True|   if  |Tag_True| \u2264 k\n\n\n    F1-score@k =  2 \u00d7 Precision@k  \u00d7 Recall@k / Precision@k  + Recall@k\n\n\nIn the above formulas, Tag_True refers to the ground truth tags and Tag_Predict refers to the predicted tags. \nNotice that the above formula of Recall@k is determined by conditions since Recall@k naturally disfavors small k. The revisited Recall@k has been widely adopted in previous experiments of tag recommendation\u00a0<cit.>. Since posts cannot have more than 5 tags, we report the results by setting the k as 1,3,and 5.\n\n\n  \u00a7.\u00a7.\u00a7 Implementation Details\n\n\n\nFor Longformer, we set the maximum accepted input sequence as 1,024, and for other BERT-based language models (i.e., RoBERTa, CodeBERT, BERTOverflow, and SOBERT) the maximum input sequence is set as 512. \n\nWe set the learning rate as 5e-5, batch size as 512, epoch number as 30, and use the Adam optimizer to update the parameters. We save the model at the end of each epoch and select the model with the smallest validation loss to run the evaluation.  \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 API Recommendation\n\n\n\n  \u00a7.\u00a7.\u00a7 Dataset\n\n\nWe reuse the BIKER dataset leveraged by Wei et al.\u00a0<cit.>. The training dataset contains 33K questions with corresponding relevant APIs in the accepted answers. \nThe test dataset contains 413 manually labeled questions from , which are looking for API to solve programming problems, and labeled the ground-truth API for these questions based on their accepted answers. The dataset is constructed by selecting posts satisfying three criteria: (1) the question has a positive score, (2) at least one answer to the question contains API entities (3) the answer has a positive score.\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Evaluation Metrics\n\n\nWe use the same evaluation metrics as previous literature\u00a0<cit.> for the API recommendation task. The metrics are: Mean Reciprocal Rank (MRR), Mean Average Precision (MAP), Precision@k and Recall@k. \n\n\n\n\n\n\n\n\n\nDifferent from tag recommendation, the API recommendation task is not a multi-label classification task, and the Recall@k metrics used in this task follow the conventional definition, which is:\n\n    Recall@k = |API_True\u2229API_Predict|/|API_True|\n\nTo be consistent with Wei et al.\u00a0<cit.>, we use k \u22081,3,5.\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Implementation Details\n\n\nCLEAR shows state-of-the-art performance in the API recommendation task by leveraging BERT sentence embedding and contrastive learning. The original architecture of CLEAR is implemented based on DistilRoBERTa [<https://huggingface.co/distilroberta-base>] during the training process.\nIn this study, we also explore the effectiveness of other representation methods by replacing the embedding of DistilRoBERTa in CLEAR. \nFor Post2Vec, we concatenate the post representation from Post2Vec to the original implementation of CLEAR.\n\nFor this task, we set the batch size as 256, and the epoch number as 30. Same to the description in Sec <ref>, we select the model with the smallest validation loss to run the test set. \n\n\n\n  \u00a7.\u00a7.\u00a7 Relatedness Prediction\n\n\n\n  \u00a7.\u00a7.\u00a7 Dataset\n\n\nThe experiments are conducted based on the KUs dataset provided by Shirani et al.\u00a0<cit.>. This dataset contains 347,372 pairs of KUs. To ensure a fair comparison with the prior work\u00a0<cit.>, we use the same data for training, validation, and testing, containing\n208,423, 347,37, and 104,211 pairs of KU, respectively.\n\n\n\n  \u00a7.\u00a7.\u00a7 Evaluation Metrics\n\n\nFollowing prior work\u00a0<cit.>,\nwe adopt the micro-averaging method to calculate Micro-precision, Micro-recall, and Micro-F1 as evaluation metrics.\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Implementation Details\n\n\nWe concatenate a pair of posts as the input to train a multi-class classifier. We fine-tuned Longformer on a sequence length of 1,024 and fine-tuned other pre-trained models on a sequence length of 512. \nFor all experiments, we set the batch size as 32 and the epoch number as 5. We select the model with the smallest validation loss to run the evaluation.  \n\n\n\u00a7 EXPERIMENTAL RESULTS\n\n\n\nThis section describes the experiment results and answers our research questions. The experimental results are summarized in Table <ref>, <ref>, and <ref> respectively. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 RQ1: How effective are the existing post representation models?\n\nThe experimental results for the tag recommendation task are summarized in Table <ref>. PTM4Tag originally achieves a performance of 0.417, 0.805, and 0.526 in terms of Precision@5, Recall@5, and F1-score@5. However, the extra inclusion of Post2Vec lowers the performance to 0.416, 0.804, and 0.525, respectively. BERTOverflow struggles in the task with scores of 0.083, 0.163, and 0.105.\n\nFor API recommendation (Table <ref>), combining Post2Vec with the state-of-the-art approach CLEAR also fails to boost the performance. CLEAR itself could obtain a score of 0.739 and 0.753 in MRR and MAP, while the performance drop to 0.735 and 0.745 when Post2Vec is added. BERTOverflow obtained a performance of 0.753 and 0.778. \n\nIn the relatedness prediction task (Table <ref>), the integration of Post2Vec with ASIM slightly lowers the performance from 0.785 to 0.768 in F1-score. BERTOverflow fails to beat ASIM with an F1-score of 0.697. \n\nOverall, Post2Vec can not improve the performance of the state-of-the-art solutions in our downstream tasks. BERTOverflow performs poorly in classification tasks and only achieves comparable performance with the state-of-the-art solution in API recommendation.\n\n\n    \n\n\n\n\n\n    Answer to RQ1: The existing representation methods fail to improve state-of-the-art performance from the three targeted downstream tasks.\n    \n    \n\n\n\n\n\n \u00a7.\u00a7 RQ2: How effective are the popular BERT-based language models for the\ntargeted downstream tasks?\n\n\n\n\nFor tag recommendation (Table <ref>), the F1-score@5 for state-of-the-art approach PTM4Tag is 0.526. CodeBERT and RoBERTa can both achieve a higher F1-score@5 of 0.527. For Longformer and GraphCodeBERT, their F1-score@5 are 0.502 and 0.517, respectively. Like BERTOverflow, seBERT struggles in this task with an F1-score@5 of 0.105. \n\nTable <ref> shows that CLEAR is no longer the best-performing method in API recommendation. Replacing the embedding of Distilled RoBERTa in the original design of CLEAR with other BERT-based language models increases the performance. In terms of MRR and MAP, seBERT scores 0.754 and 0.777. In contrast, CodeBERT, RoBERTa, GraphCodeBERT, and Longformer are all able to achieve higher scores than 0.767 and 0.782. In particular, GraphCodeBERT boosts the performance of CLEAR by 3.8% and 5.0% in terms of MRR and MAP. For Precision@1,3,5 and Recall@1,3,5, GraphCodeBERT outperforms CLEAR by 6.7%\u00a022.0%. \n\nFrom Table <ref>, we observe that ASIM, the state-of-the-art technique in relatedness prediction, is outperformed by other BERT-based language models. While ASIM achieves a score of 0.785 in F1-score, CodeBERT drives forward the state-of-the-art performance by 2.3% with an F1-score of 0.803. Moreover, RoBERTa, GraphCodeBERT, Longformer, and seBERT have an F1-score of 0.787, 0.801, 0.786, and 0.799. \n\n\n\nOverall, models like CodeBERT can consistently give promising representations in all three tasks, proving its generalizability and effectiveness in a wide range of SE-related tasks.\n\n\n\n    Answer to RQ2: Representations generated by CodeBERT and RoBERTa consistently outperform each state-of-the-art technique from the targeted downstream tasks.\n    However, none of the models can always be the best performer. Overall, CodeBERT is the most promising representation model. \n    \n    \n    \n    \n\n\n\n\n \u00a7.\u00a7 RQ3: Is further pre-training on data helpful in building a better model?\n\nOur experimental results show that there is no \"one-size-fits-all\" model in representing posts, which could consistently outperform others in the considered tasks. Such a phenomenon delivers an intuition that there is an improvement opportunity in the representation technique for .\n\nBased on such intuition and common practice that a second phase of in-domain pre-training leads to performance gains\u00a0<cit.>, we conduct additional pre-training for a BERT-based model (i.e., CodeBERT) with the dataset. We name it SOBERT.\n\n\n\n\n\n\n\nPre-training Details We have leveraged the dump dated August 2022 (which includes posts from July 2008 to August 2022) and selected 22 million question posts as the training corpus. \nThe raw dataset has a size of approximately 67G. Many previous works have removed the code snippets of a post during pre-processing stage\u00a0<cit.>.   \n\n\n\nAccording to the statistics conducted by Xu et al. \u00a0<cit.>, more than 70% of the contains at least one code snippet. As a result, the removal of code snippets would result in losing a significant of information, and they should be considered to learn an effective post representation. As the code snippets within the body of a post are enclosed in HTML tags pre code and \\code\\pre, we cleaned the redundant HTML tags with regular expression\npre code([\\ s\\ S]*?)\\\\code\\\\pre.\n\nWe have initialized SOBERT based on the checkpoint of the CodeBERT model and pre-trained SOBERT using the MLM objective with a standard masking rate of 15%. The batch size is set as 256, and the learning rate is 1e-4. The training process takes 100 hours for eight Nvidia V100 GPUs with 16 GB of memory to complete. The detailed code used is included in the replication package provided. \n\nThe experimental results show that SOBERT achieves the best performance for every downstream task. \nFor tag recommendation, SOBERT achieves an F1-score@5 of 0.544 and beats the CodeBERT and RoBERTa by 3.2%; for API recommendation, SOBERT performs with 0.809 in terms of MRR and outperforms GraphCodeBERT by 3.2%.; and for relatedness prediction, it accomplishes an F1-score of 0.824 and outperforms CodeBERT by 2.6%. \n\n\nWe conduct the Wilcoxon Signed Rank at a 95% significance level (i.e., p-value < 0.05) on the paired data corresponding to SOBERT and the best-performing representation model in each task (i.e., CodeBERT and RoBERTa in tag recommendation, GraphCodeBERT in API recommendation, and CodeBERT in relatedness prediction). \nThe significance test has been conducted on the values of evaluation metrics. \nWe observe that SOBERT significantly outperforms the comparing model.\n\n\n    Answer to RQ3: Further pre-training of CodeBERT with the data improves the original performance and consistently outperforms state-of-the-art performance in all the targeted downstream tasks.\n\n\n\n\n\u00a7 DISCUSSION\n\n\n\n\n\n \u00a7.\u00a7 Lessons Learned\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Lesson #1\n\nIncorporating post embeddings from an external approach does not boost the performance of neural network models. \n\nXu et al.\u00a0<cit.> demonstrated that appending the distributed post representation learned by Post2Vec to the manually crafted feature vector can increase the performance of traditional machine learning algorithms, for example, Support Vector Machine\u00a0<cit.> and Random Forest\u00a0<cit.>, in a set of -related tasks. \nHowever, these benefits are not observed for the state-of-the-art techniques that are based on deep neural networks. \nThis is potentially caused by the design of neural networks that automatically extract feature vectors and continuously optimize the representations. \nIt indicates that deep neural networks may lose the effectiveness of external embeddings while optimizing the parameters of the feature extractor.\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Lesson #2\n\n\n\n\nModels with broader background knowledge derive better results than those with specific knowledge.\n\n\nTextual artifacts from different domains follow dissimilar word distributions. BERTOverflow is expected to produce the desired post representation as it is specifically designed for data.\n\nA major difference between the models for post representation and others is the vocabulary. \nAs BERTOverflow is pre-trained from scratch with the data, its vocabulary should be more suitable than general domain models. \nNotice that since CodeBERT, GraphCodeBERT, and Longformer are initialized on the checkpoint of RoBERTa, these models inherit the same vocabulary as RoBERTa. Table <ref> presents five examples of the tokenization result of BERTOverflow and RoBERTa. \u201cMongoDB\u201d is separated into three sub-words (\u201cM\u201d, \u201congo\u201d, and \u201cDB\u201d) by RoBERTa, but BERTOverflow is capable of representing as a whole word. It confirms our hypothesis that BERTOverflow has a more suitable vocabulary for representing the SE domain technical terms. \n\n\nSurprisingly, our experiment results show that other BERT-based language models outperform BERTOverflow by a substantial margin across all three tasks. It gives an extremely poor performance in the tag recommendation task. By inspecting the prediction results of BERTOverflow in the tag prediction task, we notice that the top-5 predictions made by BERTOverflow are always the most frequent tags (`python', `java', `c#', `java-script', and `android') from the dataset. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe observe seBERT has similar performance as BERTOverflow in the tag recommendation task. \nWe perceive that it is potentially because these models lack a sufficient amount of pre-training to perform well. Beause seBERT and BERTOverflow are trained from scratch and requires much more pre-training effort than continued pre-training with extant models.\nTo prove this concept, we performed additional pre-training on BERTOverflow with the same pre-training corpus as SOBERT. \nThe further training was with the same hyper-parameters as SOBERT, and it took 23 hours for us to finish with 4 GPUs containing 16GB Nvidia V100 each.\nWe demote this new model as BERTOverflow_NEW, and we notice its notable performance improvements compared to BERTOverflow.\nThe results are reported in Table\u00a0<ref>.[Please note that we could not apply further pre-training to seBERT due to the constraints of limited resources to handle BERT_Large architecture.]\n\n\n\n\n\n\n\n\nOverall, our experiments have shown that, in all three tasks, vocabulary has a subdued effect, and the more important factor tends to be the scale for pre-training.\nAlso, pre-training from scratch is commonly considered as an expensive process. Initializing new representation models based on the checkpoint of an existing decent model lowers the risk and the tag recommendation task is a good indicator to demonstrate the generalizability and the sufficiency of pre-training for pre-trained models. \n\n\n\n  \u00a7.\u00a7.\u00a7 Lesson #3\n\nDespite considering a longer input length, Longformer does not produce better representations for posts. \n\n\nConventional BERT-based models like CodeBERT and RoBERTa are unable to handle long sequences due to the quadratic complexity of the self-attention mechanism\u00a0<cit.> and accept a maximum of 512 sub-token as the input. \nHowever, more than 50% of posts are longer than this given limit\u00a0<cit.>. Truncation is normally employed to deal with this limitation; however, applying truncation increases the risk of losing information. It motivates us to investigate Longformer as it is designed to handle long-length input with a maximum size of 4,096 tokens.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs all of our evaluations demonstrated, Longformer fails to perform better than the other model that belongs to the general domain (i.e., RoBERTa) as well as models from the SE domain, even though it takes more time and resources for training.\nWe further compare the performance of Longformer by varying the input size considering the first 512 and 1,024 tokens.\nThe additional experimental results are shown in Table\u00a0<ref>. \nThese additional settings do not differ in performance. \nIt indicates that diversifying the input size does not affect Longformer's performance on post representation.\nA potential interpretation would be the important features for representing posts lie in the first part of each post (e.g., Title serves as a succinct summary of the post).\nIt is not worth trying Longformer unless one strictly needs the entire content of posts.\n\n\n\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Lesson #4\n\n\nWe advocate future studies related to consider the SOBERT as the underlying baseline. \n\nOur experiment results demonstrate that further pre-training based on in-domain data leads to better post representation. By initializing SOBERT with the CodeBERT checkpoint and performing further pre-training on data, we have noticed that SOBERT consistently outperforms the original CodeBERT and produces new state-of-the-art performance for all three tasks.\n\nIn Table <ref>, we present three examples of the prediction results of CodeBERT and SOBERT for the tag recommendation task. We observe that CodeBERT is making wrong predictions like \u201c.net\u201d and \u201cc#\u201d when the question is about \u201chaskell\u201d while SOBERT is capable of making the correct predictions. CodeBERT may lack knowledge of programming languages like Haskell and Lua since it is pre-trained on artifacts from Python, Java, JavaScript, PHP, Ruby and Go. Taking the post with ID 13202867 as another example, the question is about Flexslider, a jQuery slider plugin. In the given example, SOBERT could successfully make connections to tags like `jQuery' and `css' while CodeBERT struggles to give meaningful predictions. \n\nOverall, by continuing the pre-training process on data, SOBERT outperforms CodeBERT in three popular -related tasks. \n\nFurthermore, Figure <ref> shows the learning curve of different representation models in the tag recommendation task (i.e., evaluated on the test data) by varying the number of epochs. SOBERT not only achieves state-of-the-art effectiveness, but it also converges faster than the other models. The same pattern is observed in the API recommendation and the relatedness prediction tasks. In practice, a model with a faster convergence speed is preferred as the fine-tuning stage would require less amount of resources, and it implies that the model provides a good initialization for the learning tasks. We advocate future studies to consider SOBERT as their underlying baseline. To facilitate the usage of the enhanced CodeBERT model proposed in this work, we plan to release it to HuggingFace[<https://huggingface.co/>] so that it can be used by simply calling the interface.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Threats to Validity\n\n\n\n\nThreats to internal validity. To ensure the correct implementation of the baseline methods (i.e., Post2Vec, PTM4Tag, CLEAR, and ASIM), we reused the replication package released by the original authors.[<https://github.com/maxxbw54/Post2Vec>][<https://github.com/soarsmu/PTM4Tag>][<https://github.com/Moshiii/CLEAR-replication>][<https://github.com/Anonymousmsr/ASIM>] \nWhen investigating the effectiveness of various pre-trained models, we used the implementation of each from the popular open-source community HuggingFace.\n\nThreats to external validity.\nOne threat to external validity relates our results may not generalize to those newly emerging topics or other -related downstream tasks. We have minimized this threat by considering multiple downstream tasks. \n\nThreats to construct validity. We reuse the same evaluation metrics in our baseline methods\u00a0<cit.>. To further reduce the risk, we conduct the Wilcoxon signed-rank statistical hypothesis test to check whether the output between the two competing approaches is significant.\n\n\n\n\u00a7 RELATED WORK\n\n\nIn this section, we review two lines of research that most relate to our work: pre-trained models for SE and mining Stack Overflow posts.\n\n\n\n \u00a7.\u00a7 Pre-trained Models for Software Engineering\n\nInspired by the success of pre-trained models achieved in the field of artificial intelligence, there is emerging research interest in exploring pre-training tasks and applying pre-trained models in SE\u00a0<cit.>.\n\nOne set of research focuses on learning semantic and contextual representations of source code; after pre-training, these models can be fine-tuned to solve SE downstream tasks.\nNote that the three Stack Overflow tasks considered in our work are understanding tasks. Thus, we focus on the encoder-based models (i.e., the Transformer Encoder component).\nIn the literature, there are other types of PTMs that can be used for generation tasks.\nThey are based on the Transformer decoder component (e.g., CodeGPT\u00a0<cit.>) or encoder-decoder architecture (e.g., CodeT5\u00a0<cit.>).\nIn this part, we review one model from each category.\nContraCode\u00a0<cit.> is another encoder-based model, which adopts a contrastive pre-training task to learn code functionality.\nThey organize programs into positive pairs (i.e., functionally similar) and negative pairs (i.e., functionally dissimilar).\nDuring contrastive pre-training, query programs are used to retrieve positive programs.\nPositive programs are pushed together, while negative ones have been pushed away.\nCodeGPT\u00a0<cit.> pre-trains a Transformer-decoder-based language model GPT\u00a0<cit.> on program languages.\nIt consists of 12 layers of Transformer decoders.\nCodeGPT has been pre-trained in Python and Java corpora from the CodeSearchNet dataset, which contains 1.1M Python functions and 1.6M Java methods.\nIt can be used for code completion and code generation tasks\u00a0<cit.>.\nWang et al.\u00a0<cit.> present CodeT5, a pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers.\nSimilar to T5, CodeT5 pre-trains on the masking span prediction task, which randomly masks spans with arbitrary length in the source sequence and then predicts the masked spans.\nIn addition, CodeT5 also pre-trains with two tasks to fuse code-specific structural information into the model, i.e., identifier tagging and masked identifier prediction.\n\nThe other set of research focuses on fine-tuning the pre-trained models to tackle SE challenges\u00a0<cit.>.\nZhang et al.\u00a0<cit.> conduct a comparative study on PTM with prior SE-specific tools in sentiment analysis for SE.\nThe experimental results show that PTM is more ready for real use than the prior tools.\nLin et al.\u00a0<cit.> find that BERT can boost the performance of traceability tasks in open-source projects.\nThey investigate three BERT architectures, i.e., Single-BERT, Siamese-BERT, and Twin-BERT.\nThe results indicate that the single-BERT can generate the most accurate links, while a Siamese-BERT architecture produced comparable effectiveness with significantly better efficiency.\nIn this paper, we conducted a comprehensive study on multiple SOTA PTMs for mining Stack Overflow tasks.\nDifferent from these works, ours is more comprehensive and covers several common tasks other than focusing on one specific task.\nExcept for fine-tuning PTMs, we also further pre-trained CodeBERT on Stack Overflow data.\n\n\n\n \u00a7.\u00a7 Mining Stack Overflow Posts\n\nWe address tag recommendation\u00a0<cit.>, API recommendation\u00a0<cit.>, and relatedness prediction\u00a0<cit.> in this work.\nOthers also explored other tasks for mining Stack Overflow posts to support software developers, such as post recommendation\u00a0<cit.>, multi-answer summarization\u00a0<cit.>, and controversial discussions\u00a0<cit.>.\n\nRubei et al.\u00a0<cit.> propose an approach named PostFinder, which aims to retrieve Stack Overflow posts that are relevant to API function calls that have been invoked.\nThey make use of Apache Lucene to index the textual content and code in Stack Overflow to improve efficiency.\nIn both the data collection and query phase, they make use of the data available at hand to optimize the search process.\nSpecifically, they retrieve and augment posts with additional data to make them more exposed to queries.\nBesides, they boost the context code to construct a query that contains the essential information to match the stored indexes.\n\nXu et al.\u00a0<cit.> investigate the multi-answer posts summarization task for a given input question, which aims to help developers get the key points of several answer posts before they dive into the details of the results.\nThey propose an approach AnswerBot, which contains three main steps, i.e., relevant question retrieval, useful answer paragraph selection, and diverse answer summary generation.\n\nRen et al.\u00a0<cit.> investigate the controversial discussions in Stack Overflow.\nThey find that there is a large scale of controversies in Stack Overflow, which indicates that many answers are wrong, less optimal, and out-of-date.\n\n\nOur work and their work are complementary to each other, and all aim to boost automation in understanding and utilizing Stack Overflow contents.\n\n\n\u00a7 CONCLUSION AND FUTURE WORK\n\n\n\nIn this paper, we empirically study the effectiveness of varying techniques for modeling posts, including approaches that are specially designed for posts (i.e., Post2Vec and BERTOverflow),\nSE domain representation models (i.e., CodeBERT, GraphCodeBERT, and seBERT) and general domain representation models (i.e., RoBERTa, and LongFormer). We evaluate the performance of these representation models on three popular and representative -related tasks, which are tag recommendation, API recommendation, and relatedness prediction.\n\nOur experimental results show that\nPost2Vec is unable to enhance the representations that are automatically extracted by deep learning-based methods and BERTOverflow performs surprisingly worse than other BERT-based language models. Furthermore, there does not exist one representation technique that could consistently outperform other representation models. Our findings indicate the current research gap in representing posts. Thus, we propose SOBERT with a simple-yet-effective strategy. We initialize SOBERT with the checkpoint of CodeBERT and continue the pre-training process with 22 million posts from . As a result, SOBERT improves the performance of the original CodeBERT and consistently outperforms other models on all three tasks, confirming that further pre-training on data is helpful for building representation. \n\nIn the future, we would also extend our research to other SQA sites, such as AskUbuntu[<https://askubuntu.com/>]. Moreover, we show that Longformer and BERTOverflow do not generate better representations for posts, therefore exploring better representation models which could handle noise in a longer input and  \nwith a vocabulary is still a possible direction to explore.  \n\n\n\n\u00a7 DATA AVAILABILITY\n\nThe replication package of the data and code used in this paper is available at <https://figshare.com/s/7f80db836305607b89f3>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nACM-Reference-Format\n\n\n\n"}