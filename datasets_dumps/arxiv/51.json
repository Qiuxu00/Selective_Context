{"entry_id": "http://arxiv.org/abs/2303.07317v1", "published": "20230313173858", "title": "Nearest-Neighbor Inter-Intra Contrastive Learning from Unlabeled Videos", "authors": ["David Fan", "Deyu Yang", "Xinyu Li", "Vimal Bhat", "Rohith MV"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\n\n\nNearest-Neighbor Inter-Intra Contrastive Learning from Unlabeled Videos\n    David Fan Deyu Yang Xinyu Li Vimal Bhat Rohith MV Amazon Prime Video \n\n{fandavi, deyu, xxnl, vimalb, kurohith}@amazon.com\n\n\n\n\n\n\n    March 30, 2023\n===================================================================================================================================\n\nempty\n\n\nContrastive learning has recently narrowed the gap between self-supervised and supervised methods in image and video domain. State-of-the-art video contrastive learning methods such as CVRL and \u03c1-MoCo spatiotemporally augment two clips from the same video as positives. By only sampling positive clips locally from a single video, these methods neglect other semantically related videos that can also be useful. To address this limitation, we leverage nearest-neighbor videos from the global space as additional positive pairs, thus improving positive key diversity and introducing a more relaxed notion of similarity that extends beyond video and even class boundaries. Our method, Inter-Intra Video Contrastive Learning (IIVCL), improves performance on a range of video tasks.\n\n\n\n\u00a7 INTRODUCTION\n\n\nThe success of supervised deep learning for computer vision can largely be attributed to the availability of large-scale labeled datasets\u00a0<cit.>, which are difficult and expensive to create. However, progress in compute and model representation capabilities has outpaced progress in dataset creation\u00a0<cit.>. Self-supervised learning is the paradigm of learning from unlabeled data to decouple reliance upon large-scale labeled datasets. It has already shown great potential in NLP\u00a0<cit.> for producing representations that generalize well to multiple tasks. \n\n\n-0.3cm\n\nRecently, a flavor of self-supervised learning known as contrastive learning / instance discrimination\u00a0<cit.> has become dominant in vision through works such as MoCo\u00a0<cit.> and SimCLR\u00a0<cit.> which are competitive with supervised learning in image domain. More recently, contrastive learning works for video such as CVRL\u00a0<cit.> and \u03c1-MoCo\u00a0<cit.> are competitive with supervised learning. These works learn a representation from unlabeled data by pulling positive pairs closer and pushing negative samples apart in the embedding space. In the case of video, these positive pairs are generated through random augmentations of sub-clips from the same video\u00a0<cit.>, while clips from other similar videos are never used as positives.\n\nBy only considering clips that belong to the same video to be positive, works such as CVRL\u00a0<cit.> and \u03c1-MoCo\u00a0<cit.> neglect other semantically related videos that may also be useful and relevant as positives for contrastive learning. Fig.\u00a0<ref> depicts such a scenario. Clips A and B from the same skiing video are the positive pair while video 2 (push-up) and video 3 (snowboarding) are negatives. Clips A and B are certainly more similar to each other than to the push-up and snowboarding videos, but the snowboarding video is far more similar to the positive anchor when compared to an indoor activity such as doing push-ups. Yet the relative similarity between skiing - snowboarding will never be exploited by methods such as CVRL\u00a0<cit.> and \u03c1-MoCo\u00a0<cit.>.\n\n\n\n\nThis raises the question of what constitutes a desirable video representation; by focusing too much on local intra-video semantics, we may miss the larger picture and hierarchy of visual concepts. This might lead to overfitting to tasks that are similar to the pretraining dataset and thus hurt generalization. On the other hand, if we focus too much on global inter-video semantics, we may lose sight of granular details that are also important for video understanding.\n\nTo balance the two, we propose learning notions of similarity both within the same video and between different videos, by leveraging inter-video nearest-neighbor (NNs) from the global space in addition to existing intra-video clips as positive pairs for contrastive learning. For any pair of intra-video clips, our method \u201cInter-Intra Video Contrastive Learning\u201d () defines a second positive key as the most similar video found from a dynamically evolving queue of randomly sampled videos in the learned representation space, as shown in Fig.\u00a0<ref>.\n\nThe benefit of adding globally sampled NNs as positives is two-fold. First, NNs present additional diversity in viewpoint and scene orientation, which cannot be expressed through sampling subclips from the same video, as shown by comparing Fig.\u00a0<ref> a) and b). This is true whether the subclips are augmented, shuffled or otherwise modified per the pretext task, so long as they belong to the same video. Image works such as NNCLR\u00a0<cit.> show that improving positive key diversity leads to better generalization on image tasks, and we explore whether the same is true for video.\n\nSecond, because the set of positive keys is expanded beyond the boundaries of a single video, it becomes possible to also learn from weaker yet relevant and useful positive pairs, such as the snowboarding-skiing example from Fig.\u00a0<ref>. By combining these nearest-neighbor pairs with intra-video pairs as positives for contrastive learning, the model can learn notions of similarity both within the same video and between different videos, thus striking a balance between learning granular details and learning higher-level visual concepts. We then evaluate whether this leads to improved representations on downstream video tasks.\n\n\n\nIn summary, our contributions are the following:\n\n(i) Unlike other video-based works, we go beyond single-video positives using only RGB by leveraging globally sampled nearest-neighboring videos as a simple and effective way to increase the semantic diversity of positive keys and introduce higher-level notions of similarity.\n\n(ii) We introduce  as a novel yet simple self-supervised video representation learning method that learns a joint representation of both intra-video pairs and nearest-neighbors \u2014 without clustering nor multiple modalities.\n\n(iii) We demonstrate that striking a balance between local and global similarity leads to improved performance over existing intra-video contrastive learning methods\u00a0<cit.> on video action recognition, action detection, and video retrieval \u2014 even in a few-shot learning setting.\n\n\n\n\u00a7 RELATED WORK\n\n\n\nSelf-supervised image representation learning.   Earlier works focused on designing pretext tasks whose solution would yield a useful representation for downstream classification. Some pretext tasks include predicting the relative positioning of patches\u00a0<cit.>, image rotation\u00a0<cit.>, image colorization\u00a0<cit.>, solving jigsaw puzzles\u00a0<cit.>, and counting visual primitives\u00a0<cit.>. While promising, these approaches were not competitive with fully-supervised representations\u00a0<cit.>, partly because it is easy to learn shortcuts that trivially solve the pretext task. \n\nThe re-emergence of contrastive learning elevated self-supervised image representation learning as a viable alternative paradigm to fully-supervised learning\u00a0<cit.>. These methods encourage models to be invariant to multiple augmented views of the same image.\u00a0<cit.> generates multiple views from the same image by splitting images into luminance and chrominance space.\n\nSome recent works such as\u00a0<cit.> go beyond single-instance contrastive learning by using clusters to find semantically relevant positive pairs. Our work is similar to NNCLR\u00a0<cit.> which uses nearest-neighbors for image contrastive learning, but differs in that we combine inter-video nearest-neighbors with intra-video positives for video contrastive learning, which brings unique challenges.\n\nSelf-supervised video representation learning.   Temporal information in videos enables the design of interesting pretext tasks that are not possible with images. Video in general is more challenging due to its temporal dimension. Pretext tasks for video self-supervised learning include predicting future frames\u00a0<cit.>, the correct order of shuffled frames\u00a0<cit.> or shuffled clips\u00a0<cit.>, video rotation\u00a0<cit.>, playback speed\u00a0<cit.>, directionality of video\u00a0<cit.>, motion and appearance statistics\u00a0<cit.>, and solving space time cubic puzzles\u00a0<cit.>. Other works have leveraged video correspondence by tracking patches\u00a0<cit.>, pixels\u00a0<cit.>, and objects\u00a0<cit.> across adjacent frames. \n\nSeveral recent works have considered contrastive learning for video. These methods differ in their definition of positive and negative samples. Some works use different subclips of equal-length from the same video as positive samples\u00a0<cit.>, or different frames from the same video\u00a0<cit.>.\u00a0<cit.> samples two subclips of different length from the same video to encourage generalization to broader context. Other works utilize optical flow in a cross-modal context;\u00a0<cit.> uses optical flow to mine RGB images with similar motion cues as positives, while other works do not mine positives but instead learn from the natural correspondence between optical flow and RGB within the same video\u00a0<cit.>, or within the same frame\u00a0<cit.>. Another set of works utilize pretext tasks such as pace prediction\u00a0<cit.>, clip shuffling\u00a0<cit.>, or a combination of both\u00a0<cit.>.\n\nIn contrast, our work goes beyond local definitions of positives from a single-video and expands to globally sampled nearest-neighbor videos, but without using optical flow nor separate training phases like\u00a0<cit.>. Unlike works such as\u00a0<cit.>, our work uses the online representation space to pick NNs on the fly instead of pre-computing video clusters. Unlike methods such as\u00a0<cit.> that go beyond single-video positives, our work only uses RGB frames.\n\n\n\n\u00a7 INTER-INTRA VIDEO CONTRASTIVE LEARNING\n\n\n\n\n\n \u00a7.\u00a7 Contrastive loss\n\nContrastive learning maximizes the similarity of a given embedded sample q with its embedded positive key k^+, while minimizing similarity to negative embeddings n_i. In the rest of this work, we refer to (q, k^+) as \u201cpositive pairs\u201d. We utilize the InfoNCE loss\u00a0<cit.> for self-supervised video representation learning, which is given below:\n\n    \u2112^NCE(q, k^+, \ud835\udca9^-) = -logexp(sim(q, k^+) / \u03c4)/\u2211_k \u2208{k^+}\u222a\ud835\udca9^-exp(sim(q, k) / \u03c4)\n\nwhere \u03c4 > 0 is a temperature hyper-parameter and sim(\u00b7) denotes the similarity function \u2014 which in this work is the dot product (cosine) similarity between two \u2113_2 normalized vectors: sim(q, k) = q \u00b7 k = q^Tk / (||q||  ||k||). \n\n\n\n \u00a7.\u00a7 Multi-Task Objective\n\nThe manner in which positive pairs are sampled during contrastive learning introduces an inductive bias that has a significant impact on downstream task performance\u00a0<cit.>. For example, works such as MoCo\u00a0<cit.> and SimCLR\u00a0<cit.> that use image augmentation encourage model invariance to different colors, crops, and orientations, potentially at the expense of other inductive bias. \n\nIntra-Video Contrastive Learning. Contrastive learning methods for video such as CVRL\u00a0<cit.> and \u03c1-MoCo\u00a0<cit.> use the embeddings of two subclips z_1 and z_2 from the same video as positives. Intra-video positive sampling teaches the model to identify whether two different clips correspond to the same video. Given a queue Q of randomly sampled embeddings, the intra-video contrastive loss is then:\n\n    \u2112_Intra(z_1, z_2, Q) = \u2112^NCE(z_1, z_2, Q)\n\n\nNearest-Neighbor Contrastive Learning.\nMotivated by the observation in Fig.\u00a0<ref> that intra-video sampling excludes other semantically similar videos from ever being used as positives; we seek to expand the positive keyset beyond individual video boundaries to improve diversity. Offline positive sets are not suitable because they cannot be efficiently updated at every iteration or even at every epoch. Clustering to find similar videos requires additional hyperparameters. Thus, we maintain a queue Q that is updated with embeddings from each forward pass (design details in Sec.\u00a0<ref>), which allows us to directly compute cosine similarities between the input video and queue. Given an embedded input video x and queue Q of randomly sampled embeddings across the dataset, we use the nearest-neighbor of x as its positive key:\n\n    NN(x, Q) = argmax_z \u2208 Q (x \u00b7 z)\n\n\nLet z_1 and z_2 be the embeddings of two subclips from the same video. We use Q for both selecting the NN as a positive key and providing negatives (excluding the NN). Using the nearest-neighbor operation in Eq.\u00a0<ref> to select the positive key for z_1 as NN(z_2, Q), and removing it from Q to form Q^- = Q \u2216NN(z_2, Q), we have the NN contrastive loss:\n\n    \u2112_NN(z_1, z_2, Q) = \u2112^NCE(z_1, NN(z_2, Q), Q^-)\n\n\nCombined Intra and Inter Training Objective We use the same backbone but separate MLP projection heads to process the intra-video and NN positive pairs. As each of these pretext tasks learns a different notion of similarity, we combine them via a multi-task loss. We also maintain two separate queues of embeddings: Q_Intra and Q_NN. Q_NN is used both to find the NN and provide negative keys (excluding the NN), while Q_Intra only provides negative keys. We expand on these details in section\u00a0<ref>.\n\nSpecifically, let f^q(\u00b7) and f^k(\u00b7) be the encoder and its offline momentum-updated version, g^Intra(\u00b7) and g^NN(\u00b7) be two separate MLP heads, and x_1 and x_2 be two subclips sampled from the same video. We first obtain the embeddings (z_1^Intra, z_2^Intra) and (z_1^NN, z_2^NN).\n\n    z_1^Intra = g^Intra(f^q(x_1));    z_2^Intra = g^Intra(f^k(x_2)) \n        z_1^NN = g^NN(f^q(x_1));    z_2^NN = g^NN(f^k(x_2))\n\nAfter obtaining the embeddings, we combine Eqs.\u00a0<ref> and\u00a0<ref> to get the final training objective. Note that we use a symmetric loss but show only one side for simplicity. \u03bb_Intra and \u03bb_NN are tunable parameters that control the contribution of each loss, which in our work is 1.0 for both. An ablation for this is in Tab. <ref>.\n\n    \u2112(z_1^Intra, z_2^Intra, z_1^NN, z_2^NN)    = \u03bb_Intra\u00b7\u2112_Intra(z_1^Intra, z_2^Intra, Q_Intra) \n       + \u03bb_NN\u00b7\u2112_NN(z_1^NN, z_2^NN, Q_NN)\n\nNote that class labels are not used and the model is free to learn its own notion of similarity. Over the course of pretraining, the model becomes better at picking semantically similar nearest-neighbor video clips while introducing additional diversity of positive samples that is not possible through sampling intra-video clips, as shown in Fig.\u00a0<ref>.\n\n\n\n \u00a7.\u00a7 Pretraining Methodology\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Momentum Encoder and Queue\n\nWe make several design choices that enable end-to-end learning from unlabeled videos using our method. As contrastive learning requires large batch sizes\u00a0<cit.> and computing video embeddings is expensive, we use a FIFO queue that is updated with embeddings from a momentum encoder, similar to\u00a0<cit.>. The momentum encoder's weights \u03b8_k are updated as a moving average of the encoder's weights \u03b8_q, with momentum coeff. m \u2208 [0, 1), as given by Eq.\u00a0<ref>. Thus the momentum encoder receives no gradients.\n\n    \u03b8_k \u2190 m \u03b8_k + (1 - m) \u03b8_q\n\n\nWe share the same encoder but utilize separate MLP heads when processing intra-video and nearest-neighbor positives. We also maintain two separate queues for each task. Note that the queue contains approximate representations of a large subset of pretraining data in memory and is dynamically updated \u201cfor free\u201d since we only use embeddings that are already computed during the forward pass. Our method scales to large data and is more efficient than methods that utilize clustering such as\u00a0<cit.>. Our method also allows for more up-to-date representations than methods that use an offline positive set\u00a0<cit.>.\n\n\n \n\n  \u00a7.\u00a7.\u00a7 Implementation Details\n\n\nLoss.   The temperature \u03c4 = 0.1. \u03bb_Intra = 1.0, \u03bb_NN = 1.0.\n\n\nEncoder.   For all experiments, we use a ResNet3D-50 8x8 slow pathway from\u00a0<cit.> with He initialization\u00a0<cit.>, unless otherwise indicated. Outputs are taken after the global average pooling layer to form a 2048-d embedding. Following\u00a0<cit.>, we use a 3-layer projection MLP during pretraining only. The MLP has hidden dimension 2048 and final embedding dimension of 128 with no batch norm (BN). The MLP is then removed for downstream experiments. As mentioned above, we use two separate MLP heads for producing intra-video and NN embeddings.\n\nPretraining Hyperparameters.   We train for 200 epochs using SGD optimizer (momentum 0.9, weight decay 10^-4) with a total batch size of 512. BN statistics are computed per GPU. We linearly warmup the learning rate to 0.4 over the first 35 epochs, then use half-period cosine decay\u00a0<cit.>.\n\nWe use a queue storing 65536 negatives and shuffling-BN to avoid information leakage and overfitting\u00a0<cit.>. The momentum encoder weights are updated per Eq.\u00a0<ref> with an annealed momentum coeff. as in\u00a0<cit.>, initialized to 0.994.\n\nData and Augmentations\nWe sample two 8-frame clips with a temporal stride of 8 from each video for self-supervised pretraining. We apply random shortest-side resizing to [256, 320] pixels, color jittering (ratio 0.4, p=0.8), grayscale conversion (p=0.2), Gaussian blur (p=0.5), horizontal flip (p=0.2), and random cropping to 224 \u00d7 224.\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n \u00a7.\u00a7 Baselines\n\n\nCVRL <cit.> and \u03c1-MoCo <cit.> are two leading contrastive learning works that sample intra-video clips. We primarily compare against \u03c1-MoCo for \u03c1=2 (two clips per video), pretrained for 200 epochs on unlabeled K400, and call this baseline  . The original paper does not test  on all downstream datasets, so we rerun all downstream experiments (even those reported in the paper) for fair comparison. We compare our  against  to distill the effect of improved positive key diversity and balanced global-local context on downstream task performance.\n\n\n\n\n\n \u00a7.\u00a7 Action Recognition on UCF101, HMDB51, and Kinetics-400\n\n\nUnless otherwise noted, we train  on unlabeled K400\u00a0<cit.> (240K videos) for 200 epochs. After self-supervised pretraining, we transfer the learned weights to target datasets for downstream evaluation. We use two evaluation protocols that are popular in the literature for evaluating the quality of self-supervised representations: (i) Linear evaluation freezes the backbone and trains a linear classifier on the target dataset, and (ii) Finetuning trains the entire network end-to-end on the target dataset.\n\nWe first report top-1 accuracy on three of the most popular action datasets. For UCF101\u00a0<cit.> and HMDB51\u00a0<cit.>, we report finetuning top-1 accuracy on split 1. UCF101 contains 9.5K/3.7K train/test videos with 101 action classes, and HMDB51 contains 3.5K/1.5K videos (mostly from movies) with 51 action classes. For K400\u00a0<cit.>, we report linear evaluation top-1 accuracy. Kinetics contains 240K/19K train/test videos with 400 action classes. We sample 8 \u00d7 8 clips for all datasets. At test-time, we use standard 10 (temporal) \u00d7 3 (spatial) crop evaluation\u00a0<cit.>. We report the avg. of three runs.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Comparison to state-of-the-art.\n\nIn Table <ref>, we compare  against state-of-the-art self-supervised methods in the literature that use only RGB frames (no optical flow nor audio), as well as our intra-video baseline  using the protocols introduced at the beginning of Sec. <ref>.\n\nFirst, we compare  against the  baseline to distill the effect of improved positive key diversity and added global context on downstream performance.  outperforms  by 1.5% on UCF, 0.5% on HMDB, and by 0.3% on K400. The smaller performance delta on K400 may be due to the fact that linear evaluation is not as reflective of real performance as finetuning the entire network\u00a0<cit.>. Consistent improvements over  highlights the effectiveness of combining intra-video and nearest-neighbor sampling, compared to sampling only intra-video positives.\n\nNext, we compare  against other methods.  also outperforms works that trained for more epochs (e.g. VTHCL <cit.>) or used more frames during pretraining (e.g. SpeedNet <cit.>, VideoMoCo <cit.>).  pretrained for 400 epochs surpasses CVRL\u00a0<cit.> which pretrained for 2.5x more epochs using 2x more frames.\n\nTo fairly compare against other video contrastive learning works, we also present results for a weaker version of our model trained for 200 epochs with a smaller backbone (R18) and smaller input resolution (128x128). Under this setting, our model still convincingly outperforms methods that use larger backbones such as VTHCL\u00a0<cit.> and MemDPC\u00a0<cit.>, and larger input resolution such as VideoMoco\u00a0<cit.> and LSFD\u00a0<cit.>.\n\n\n\n \u00a7.\u00a7 Action Recognition on Something-Something v2\n\nTo further demonstrate the effectiveness of nearest-neighbor sampling for video contrastive learning, we evaluate  on Something-Something v2\u00a0<cit.> (SSv2), which is a challenging benchmark focused on understanding granular motions. Unlike UCF101 and HMDB51 which are very similar to K400, SSv2 distinguishes between directionality for the same higher-level action. For example \"putting something into something\" vs. \"putting something next to something\" are different action classes, as well as \"moving something up\" vs. \"pushing something from right to left\". We finetune on SSv2 using the same settings as\u00a0<cit.>.\n\n marginally outperforms the  baseline. As SSv2 is very different in nature from the K400 dataset which we pretrained on, these results further demonstrate that improving positive key diversity and introducing global similarity leads to more generalized video representations that outperform pure intra-video sampling-based methods.\n\n\n\n\n\n \u00a7.\u00a7 Action Detection on AVA\n\nThus far, we have demonstrated that  can generalize to new domains within the same task of action recognition. To test whether our method can also generalize to novel downstream tasks, we evaluate  on the new task of action detection, which not only requires accurately classifying the action but also localizing the bounding box of the person performing the action.\n\nWe evaluate on the AVA dataset\u00a0<cit.> which contains 221K/57K training and validation videos, and report mean Average Precision (mAP) at IOU threshold 0.5. We follow\u00a0<cit.> and use our self-supervised trained R3D-50 as the backbone for a Faster R-CNN detector. We then extend the 2D RoI features into 3D along the temporal axis, and apply RoIAlign and temporal global average pooling. The RoI features are then max-pooled and fed to a per-class sigmoid classifier. We also use a similar training schedule as\u00a0<cit.>, except we train for only 20 epochs with batch size 64, and use an initial learning rate of 0.1 with 10x step-wise learning rate decay at epochs 5, 10, and 15.\n\n outperforms the  baseline on the new task of action detection, on a dataset sourced from cinematic movies rather than Internet videos.  also outperforms CVRL\u00a0<cit.> despite CVRL being trained for 5x more epochs and using 2x more pretraining frames. As AVA is highly different from the unlabeled K400 videos used for pretraining (while UCF101 and HMDB51 are highly similar to K400), this further supports our hypothesis that improved positive key diversity and added global context through nearest-neighbor sampling improves generalization on video tasks.\n\n\n\n\n\n \u00a7.\u00a7 Ablations\n\nWe perform a series of ablations to validate our design decisions. Unless otherwise indicated, we pretrain on full unlabeled K400 (240K videos), and the same settings as the main experiments, except for the ablated parameters.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 MLP Heads\n\nWe first ablate the use of separate MLP heads during pretraining. We trained a version of  that shares the MLP head for both intra-video and NN pairs. In this case, the pretraining loss fails to converge and downstream task results are poor. We hypothesize this is due to the different feature spaces learned for intra-video clips vs. inter-video NNs. Thus, we share the backbone but use separate MLP heads during pretraining. No MLP is used for downstream eval.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Task Generalization of Intra and NN Weights\n\nIn Table <ref>, we summarize the results from above sections in which  outperformed  on every dataset per task and per evaluation protocol. We also ablate our choice of \u03bb_NN by providing results for (\u03bb_Intra=0.0, \u03bb_NN=1.0), and (\u03bb_Intra=1.0, \u03bb_NN=1.0). Note that (\u03bb_Intra=0.0, \u03bb_NN=1.0) corresponds to a pure NN sampling strategy that uses no intra-video pairs, aka a video-analog of NNCLR\u00a0<cit.>. We summarize the average rank per task for each configuration. The best configuration achieves lowest average rank of 1.2.\n\nPure NN sampling (video analog of NNCLR\u00a0<cit.>) is surprisingly competitive with pure intra-video sampling on every task, despite learning zero local semantics during SSL pretraining. However, combining the intra and NN loss (\u03bb_Intra=1.0, \u03bb_NN=1.0) does not result in as large of a boost as expected. This may suggest that local and global information are complimentary, but may still provide orthogonal directions that are challenging to learn from.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Effect of More Epochs and More NNs\n\nTable\u00a0<ref> shows that downstream accuracy increases with the number of temporal samples per video and duration of pretraining. Due to limited compute we do not test more combinations, but expect further room for improvement.\n\n\n\n\n \u00a7.\u00a7 Few-Shot Learning\n\nAnother way to measure the quality of a self-supervised representation is through data efficiency, or how performance varies with respect to the amount of data available for downstream classification <cit.>. Many works report that self-supervised representations are more data efficient than supervised representations <cit.>.\n\nWe first compare against  on UCF101 using the finetune protocol when training data is limited to 1%, 5%, and 20%, and the evaluation set remains the same. We observe that  is more data efficient across all three subsets. We then compare against  on K400 using the linear evaluation protocol when training data is limited to 1% and 10%, and the evaluation set remains the same. We observe similar improvements across both subsets for . For both UCF and K400, the delta between  and  is largest for the smallest training set of 1% data, indicating that inter-video nearest-neighbors are particularly helpful for generalizing to few-shot settings. We also significantly outperform the supervised baseline in which R3D-50 is initialized from scratch. See Table <ref>.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Zero-Shot Video Retrieval\n\nWe also evaluate on video retrieval where the extracted features are directly used to find the nearest-neighbors, so there is no further training. Following common practice <cit.>, we use test-set videos to search the k nearest-neighboring video samples from the train set. We evaluate using Recall at k (R@K), which means that the retrieval is correct if any of the top k nearest-neighbors are from the same class as the query. See Table <ref>.\n\n outperforms  for all but one recall threshold on UCF and all recall thresholds on HMDB. This indicates that even without any downstream training,  is better able to push similar videos of a different downstream dataset closer in the embedding space.\n\n\n\n\n\u00a7 DISCUSSION\n\n\n\n\n \u00a7.\u00a7 Co-Occurrence of Semantic Classes During Pretraining\n\nAlthough labels are not used during self-supervised pretraining, we analyze the probability that a video in the negative queue belongs to the same class as the query video, to understand why the nearest-neighbor objective is beneficial. Assume the queue samples are uniformly sampled and that each class is balanced. Let the pretraining dataset have K balanced classes, and the queue have Q uniformly sampled samples where Q is smaller than the size of the dataset. Then the probability of the above event is 1 - [(K-1)/K] ^ Q, which is well over 0.9 for K=400 (number of classes in Kinetics-400), Q=1024. Note that this calculation also applies to approaches that sample negatives from the mini-batch; let Q be the mini-batch size. CVRL <cit.> uses a mini-batch size of 1024 during pretraining. Thus, it is extremely likely that videos belonging to same class as the query are pushed away in the embedding space as negatives in works like <cit.>. Our work does not address this issue by removing poor choices of negatives from the negative set, but rather leverages those similar videos as additional positive keys for a second loss term via the NN sampling strategy, thus providing additional sources of similarity to learn from that would otherwise be ignored.\n\nAdditionally, by dynamically computing the positive key using the learned representation space and sampling videos globally, we allow the model to continually evolve its notion of semantic similarity; the quality of the chosen NNs improves as the model learns as demonstrated by\u00a0Fig. <ref>. With intra-video positive pair sampling, the learned representation is not used to choose the positive pairs \u2014 two clips are simply randomly sampled from within a single video.\n\n\n\n \u00a7.\u00a7 Limitations and Future Work\n\nWhile the focus of our work was on improving the diversity of positive keys and balancing global with local notions of similarity, our method can be improved by reducing false negatives similar to works such as\u00a0<cit.>. Our method could also be improved by leveraging audio-video correspondence\u00a0<cit.> and exploring more nuanced ways to combine the intra and inter-video positives.\n\n\n\n\u00a7 CONCLUSION\n\n\nWe presented , which addresses limitations of existing self-supervised contrastive learning works for video <cit.> that sample only intra-video clips as positives. By leveraging nearest-neighboring samples from a global neighborhood as positives, we expand the positive keyset beyond individual video boundaries, thus improving the diversity of positive keys and blending global with local context. We then demonstrated that our method improves performance over a wide range of video-related tasks, compared to our intra-video contrastive learning baseline. Our method is simple, effective, and directly plugs into existing contrastive frameworks.  scales to large datasets, unlike methods that require clustering or offline positive sets. We believe that our method has the potential to excel on large unstructured datasets, where self-supervised learning can reach its full potential for video understanding.\n\n\n\nieee_fullname\n\n\n\n\n\n\n\n"}