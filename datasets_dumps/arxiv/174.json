{"entry_id": "http://arxiv.org/abs/2303.07110v1", "published": "20230313134404", "title": "Upcycling Models under Domain and Category Shift", "authors": ["Sanqing Qu", "Tianpei Zou", "Florian Roehrbein", "Cewu Lu", "Guang Chen", "Dacheng Tao", "Changjun Jiang"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.LG"], "text": "\n\n\n\n\n\n\nUpcycling Models under Domain and Category Shift\n    Sanqing Qu^1Equal Contribution , Tianpei Zou^1, Florian R\u00f6hrbein^2,\nCewu Lu^3, Guang Chen^1Corresponding author: guangchen@tongji.edu.cn ,\n Dacheng Tao^4,5, Changjun Jiang^1\n\n^1Tongji University, ^2Chemnitz University of Technology,\n\n^3Shanghai Jiao Tong University, ^4JD Explore Academy, ^5The University of Sydney\n\n\n    March 30, 2023\n=================================================================================================================================================================================================================================================================================================================================\n\n\n\nDeep neural networks (DNNs) often perform poorly in the presence of domain shift and category shift. How to upcycle DNNs and adapt them to the target task remains an important open problem. Unsupervised Domain Adaptation (UDA), especially recently proposed Source-free Domain Adaptation (SFDA), has become a promising technology to address this issue. Nevertheless, existing SFDA methods require that the source domain and target domain share the same label space, consequently being only applicable to the vanilla closed-set setting. In this paper, we take one step further and explore the Source-free Universal Domain Adaptation (SF-UniDA). The goal is to identify \u201cknown\" data samples under both domain and category shift, and reject those \u201cunknown\" data samples (not present in source classes), with only the knowledge from standard pre-trained source model. To this end, we introduce an innovative global and local clustering learning technique (GLC). Specifically, we design a novel, adaptive one-vs-all global clustering algorithm to achieve the distinction across different target classes and introduce a local k-NN clustering strategy to alleviate negative transfer. We examine the superiority of our GLC on multiple benchmarks with different category shift scenarios, including partial-set, open-set, and open-partial-set DA. Remarkably, in the most challenging open-partial-set DA scenario, GLC outperforms UMAD by 14.8% on the VisDA benchmark. The code is available at <https://github.com/ispc-lab/GLC>.\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nAt the expensive cost of given large-scale labeled data and huge computation resources, deep neural networks (DNNs) have made remarkable progress in various tasks. However, DNNs often generalize poorly to the unseen new domain under domain shift and category shift. How to upcycle DNNs and adapt them to target tasks is still a long-standing open problem. In the last decade, many efforts have been devoted to unsupervised domain adaptation (UDA)\u00a0<cit.>, which capitalizes on labeled source data and unlabeled target data in a transduction manner, and has achieved significant success. Despite this, the access to source raw data is inefficient and may violate the increasingly stringent data privacy policies\u00a0<cit.>. Recently, Source-free Domain Adaptation (SFDA)\u00a0<cit.> has become a promising technology to alleviate this issue, where only a pre-trained source model is provided as supervision rather than raw data. However, to avoid model collapse, most existing methods\u00a0<cit.> assume that the label space is identical across the source and target domain, thus being only applicable to vanilla closed-set scenarios.\n\n\n\nIn reality, target data may come from a variety of scenarios. Therefore, it is too difficult to hold such a strict assumption. For a better illustration, we suppose \ud835\udcb4_s and \ud835\udcb4_t as the label space of source domain and target domain, respectively. In addition to the well-studied vanilla closed-set (\ud835\udcb4_s = \ud835\udcb4_t.), we often encounter several other situations, e.g., the partial-set (\ud835\udcb4_s \u2283\ud835\udcb4_t), the open-set (\ud835\udcb4_s \u2282\ud835\udcb4_t), and the open-partial-set (\ud835\udcb4_s \u2229\ud835\udcb4_t \u2205, \ud835\udcb4_s \u2288\ud835\udcb4_t, \ud835\udcb4_s \u2289\ud835\udcb4_t). Currently, there have been several source data-dependent works\u00a0<cit.> developed to target category shift. However, methods devised for one situation are commonly infeasible for others. In practice, the target domain is unlabeled and we cannot know which of these category shifts will occur in advance. Not to mention that the requirement to source raw data makes it inefficient and potentially violates data protection policies. To tackle these limitations, and handle those category shifts in a unified manner, in this paper, we take one step further and delve into the Source-free Universal Domain Adaptation (SF-UniDA). The goal is to upcycle the standard pre-trained source models identifying \u201cknown\" data samples and rejecting those \u201cunknown\" data samples (not present in source classes) under domain and category shift. We conceptually present the SF-UniDA in Fig.\u00a0<ref>. Note that, very few works\u00a0<cit.> have studied the source-free model adaptation in open-partial-set scenarios. Nevertheless, their approaches demand dedicated model architectures, greatly limiting their practical applications. SF-UniDA is appealing in view that model adaptation can be resolved only on the basis of a standard pre-trained closed-set model, i.e., without specified model architectures.\n\nTo approach such a challenging DA setting, we propose a simple yet generic technique, Global and Local Clustering (GLC). \nDifferent from existing pseudo-labeling strategies that focus on closed-set scenarios, we develop a novel one-vs-all global clustering based pseudo-labeling algorithm to achieve \u201cknown\" data identification and \u201cunknown\" data rejection. As we have no prior about the category shift, we utilize the Silhouettes\u00a0<cit.> metric to help us realize adaptive global clustering. To avoid source private categories misleading, we design a global confidence statistics based suppression strategy. Although the global clustering algorithm encourages the separation of \u201cknown\" and \u201cunknown\" data samples, we find that some semantically incorrect pseudo-label assignments may still occur, leading to negative knowledge transfer. To mitigate this, we further introduce a local k-NN clustering strategy by exploiting the intrinsic consensus structure of the target domain.\n\nWe validate the superiority of our GLC via extensive experiments on four benchmarks (Office-31\u00a0<cit.>, Office-Home\u00a0<cit.>, VisDA\u00a0<cit.>, and Domain-Net\u00a0<cit.>) under various category shift situations, including partial-set, open-set and open-partial-set. Empirical results show that GLC yields state-of-the-art performance across multiple benchmarks, even with stricter constraints.\n\nOur contributions can be summarized as follows:\n\n    \n\n    \n  * To the best of our knowledge, we are the first to exploit and achieve the Source-free Universal Domain Adaptation (SF-UniDA) with only a standard pre-trained closed-set model. \n    \n\n    \n  * We propose a generic global and local clustering technique (GLC) to address the SF-UniDA. GLC equips with an innovative global one-vs-all clustering algorithm to realize \u201cknown\" and \u201cunknown\" data samples separation under various category-shift.\n    \n  * Extensive experiments on four benchmarks under various category-shift situations demonstrate the superiority of our GLC technique. Remarkably, in the open-partial-set DA situation, GLC attains an H-score of 73.1% on the VisDA benchmark, which is 14.8% and 16.7% higher than UMAD and GATE, respectively.\n\n\n\n\n\n\u00a7 RELATED WORK\n\nUnsupervised Domain Adaptation: To alleviate performance degeneration caused by domain shift, unsupervised domain adaptation (UDA) has received considerable interest in recent years. Existing methods can be broadly classified into three categories: discrepancy based, reconstruction based, and adversarial based. Discrepancy based methods\u00a0<cit.> usually introduce a divergence criterion to measure the distance between the source and target data distributions, and then achieve model adaptation by minimizing the corresponding criterion. Reconstruction based methods\u00a0<cit.> typically introduce an auxiliary image reconstruction task that guides the network to extract domain-invariant features for model adaptation. Inspired by GAN\u00a0<cit.>, adversarial based approaches\u00a0<cit.> leverage domain discriminators to learn domain-invariant features. Despite of effectiveness, these methods typically focus only on the vanilla closed-set domain adaptation.\n\nUniversal Domain Adaptation: To handle category-shift, there have been some methods proposed for partial-set\u00a0<cit.>, open-set\u00a0<cit.>, and open-partial-set domain adaptation\u00a0<cit.>. However, most of these methods are designed for a specified situation, and are typically not applicable to other category-shift situations.\nAs an example, an open-partial-set method\u00a0<cit.> even underperforms the source model in the partial-set scenario. Recently, <cit.> propose a truly universal UDA method, which is applicable to all three category-shift situations. Nevertheless, most existing methods need access to source data during adaptation, which is inefficient and may violate the increasing data protection policies\u00a0<cit.>.\n\nSource-free Domain Adaptation: Recently, several works\u00a0<cit.> have attempted to achieve domain adaptation with knowledge from only the pre-trained source model rather than raw data. However, to avoid model collapse, these methods commonly focus on the vanilla closed-set domain adaptation, significantly limiting their usability. Very recently, few works\u00a0<cit.> have studied the source-free domain adaptation in open-partial-set scenarios. Nevertheless, the requirement of dedicated source model architectures, e.g., specialized two-branch open-set recognition frameworks, greatly limits their deployment in reality. In this paper, we target for achieving truly universal model adaptation, including partial-set, open-set, and open-partial-set scenarios, with the knowledge from vanilla source closed-set model.\n\n\n\n\n\n\n\u00a7 METHODOLOGY\n\n\n\n \u00a7.\u00a7 Preliminary\n\nIn this paper, we aim to achieve model upcycling under both domain shift and category shift, i.e., the source-free universal domain adaptation (SF-UniDA). In particular, we consider the K-way classification. In this setting, there is a well-designed source domain \ud835\udc9f_s = {(x^i_s, y^i_s)}^N_s_i=1 where x^i_s \u2208\ud835\udcb3_s, y_s^i \u2208\ud835\udcb4_s, and an unlabeled target domain \ud835\udc9f_t = {(x_t^i, ?)}^N_t_i=1 where x_t^i \u2208\ud835\udcb3_t. For a better illustration, we denote \ud835\udcb4 = \ud835\udcb4_s \u2229\ud835\udcb4_t as the common label space, \ud835\udcb4\u0305_s = \ud835\udcb4\u2216\ud835\udcb4_t as the source private label space, and \ud835\udcb4\u0305_t = \ud835\udcb4\u2216\ud835\udcb4_s as the target private label space, respectively. As aforementioned, there are three possible category shifts, i.e., the partial-set DA, PDA, (\ud835\udcb4_s \u2283\ud835\udcb4_t); the open-set DA, OSDA, (\ud835\udcb4_s \u2282\ud835\udcb4_t); and the open-partial-set DA, OPDA, (\ud835\udcb4\u2205, \ud835\udcb4\u0305_s \u2205, \ud835\udcb4\u0305_t \u2205). The final goal is to identify \u201cknown\" samples (belonging to \ud835\udcb4) and reject \u201cunknown\" samples (belonging to \ud835\udcb4\u0305_t) of \ud835\udc9f_t, with the knowledge only from source pre-trained model f_s. \ud835\udc9f_s is not available, and we do not have prior knowledge of what kind of category shift we are facing.\n\nThere have been few works\u00a0<cit.> explored source-free domain adaptation under category shift. However, these methods are limited to specific category shift, and require dedicated source model architectures. \nTo address these limitations, we propose to achieve SF-UniDA on the basis of only the vanilla closed-set model. Following existing closed-set source-free domain adaptation methods\u00a0<cit.>, given a source model f_s = h_s \u2218 g_s, consisting of a feature module g_s and a classifier module h_s, we capitalize on the source hypothesis to achieve source and target domain alignment. That is, we only learn a target-specific feature module g_t and keep the classifier h_t = h_s. To realize \u201cknown\" data identification and \u201cunknown\" data rejection under both domain shift and category shift, we devise a novel, adaptive, global one-vs-all clustering algorithm. Besides, we further employ a local k-NN clustering strategy to alleviate negative transfer. The pipeline is presented in Fig.\u00a0<ref>. More details will be described in the following.\n\n\n\n \u00a7.\u00a7 One-vs-all Global Clustering\n\nPseudo-labeling is a promising technique in unsupervised learning. Traditional pseudo-labeling strategies\u00a0<cit.> assign pseudo labels directly based on sample-level predictions, which are often noisy, especially in the presence of domain shifts. To mitigate this, there are some pseudo-labeling strategies\u00a0<cit.> exploit the data structure of the target domain, i.e., the target-specific prototypes. However, these strategies assume that the source and target domain share identical label space, making it infeasible under category shift. Therefore, a question naturally arises: How to achieve pseudo-labeling with inconsistent label space? Especially, for universal domain adaptation, we have no prior about the category shift between \ud835\udcb4_s and \ud835\udcb4_t. \n\nTo tackle this, we first view this problem from a simplified perspective: If \ud835\udcb4_s \u2282\ud835\udcb4_t (i.e., the OSDA setting), and we were to know the number of categories in the target domain is C_t, what kind of pseudo-labeling strategy should we apply? Intuitively, target domain, in this case, should be grouped into C_t clusters, each corresponding to a specific category. We can then assign pseudo labels via the nearest cluster centroid classifier. However, even though we apply existing clustering algorithms, such as K-means\u00a0<cit.>, to divide the target domain into C_t clusters. It is still challenging to associate the corresponding semantic category for each cluster, in particular for the SF-UniDA, as we have no access to the source raw data.\n\nIn view of this, to ease the challenging semantic association, we devise a novel one-vs-all global clustering pseudo-labeling algorithm. The main idea is that For a particular \u201cknown\" category c \u2208 C_s, in order to decide whether a data sample belongs to the c-th category, we need to figure out what is and what is not the c-th category. The detailed procedure is presented as follows:\n\n    \n  * For a particular c-th category, we first aggregate the top-K \u03b4_c(f_t(x_t)) scores represented instances along all target domain \ud835\udc9f_t as positive \ud835\udcab_c , and the rest as negative \ud835\udca9_c. Here, \u03b4_c(f_t(x_t)) denotes the soft-max probability of target instance x_t belonging to the c-th class.\n    We empirically set K = N_t / C_t.\n    \n  * Then, we obtain the positive prototype representation p_c (i.e., what is the c-th category), and negative prototypes {n_c^i}^M_i=1 (i.e., what are not the c-th category) via K-means. Noting that we have employed multiple prototypes to represent the negatives since the negatives contain distinct classes. We set M to C_t instead of C_t - 1, considering that the \u201cknown\" category of the target domain typically involves some hard samples that are difficult to be selected by top-K sampling. \n    \n\n    \n    p_c    = 1/K\u2211_x_t\u2208\ud835\udcab_c g_t(x_t),\n            \n    {n_c^i}_i=1^M   = Kmeans_x_t \u2208\ud835\udca9_c (g_t(x_t)).\n\n    \n\n    \n  * Thereafter, we decide whether data sample x_t belongs to the c-th category via the nearest centroid classifier:\n    \n    q\u0302_c = {\n         1,if S(g_t(x_t), p_c) \u2265max{S(g_t(x_t), n_c^i)}_i=1^M\n    \n         0,if S(g_t(x_t), p_c) < max{S(g_t(x_t), n_c^i)}_i=1^M.\n\n    where S(a, b) measures the similarity between a and b. We apply the cosine similarity function by default.\n    \n  * Finally, we iterate the above process to obtain the pseudo labels \u0177_t for all \u201cknown\" category c \u2208 C_s. Since each data sample either belongs to the unknown or to one of the categories in the source domain, it is not possible to belong to multiple categories at the same time. Thereby, we introduce a filtering strategy to avoid semantic ambiguity. Here, we just set the category with maximum similarity as the target. It is worth noting that our algorithm does not require the above pseudo-label \u0177_t to be one-hot encoded. Those pseudo labels with all-zero encoding mean that these data samples belong to the \u201cunknown\" target-private categories \ud835\udcb4\u0305_t. To realize \u201cknown\" and \u201cunknown\" separation, we then manually set those all-zero encoding pseudo labels to a uniform encoding, i.e., q\u0302_c = 1/C_s.\n\n\n\n\n \u00a7.\u00a7 Confidence based Source-private Suppression\n\n\nIn the above section, we developed the one-vs-all global clustering algorithm to assign pseudo labels for OSDA, i.e., \ud835\udcb4_s \u2282\ud835\udcb4_t, when the number of categories in the target domain C_t is available. However, in addition to OSDA, we may also encounter PDA and OPDA, where the source domain contains categories absent in the target domain. To make the above algorithm applicable to both OSDA, PDA and OPDA, it is necessary to tailor the proposed algorithm to prevent those source-private categories from misleading pseudo-label assignments.\n\nWe empirically found that on positive data group \ud835\udcab sampled with top-K on the target domain, those source-private categories generally yield lower mean prediction confidence than those source-target shared categories. In light of this observation, we design a source-private category suppression strategy based on the mean prediction confidence of the positive data group \ud835\udcab. Specifically, for a particular category c\u2208 C_s, we tailored the Eq.\u00a0<ref> to:\n\n    \u03f5_c    = \u03c1 + 1-\u03c1/K\u2211_x_t\u2208\ud835\udcab_c\u03b4_c(f_t(x_t)), \n    q\u0302_c    = {\n     1,if \u03f5_c\u00b7 S(g_t(x_t), p_c) \u2265max{S(g_t(x_t), n_c^i)}_i=1^M\n    \n     0,if \u03f5_c\u00b7 S(g_t(x_t), p_c) < max{S(g_t(x_t), n_c^i)}_i=1^M.\n\nwhere \u03f5_c is the designed source-private suppression weight for the c-th category, and \u03c1 is a hyper-parameter to control this weight. We empirically set \u03c1 to 0.75 for all datasets. Its sensitivity analysis can be found in the experiment.\n\n\n\n \u00a7.\u00a7 Silhouette Based Target Domain C_t Estimation\n\n\nBased on the previous sections, we now have achieved the pseudo-labeling algorithm for SF-UniDA. However, it is still not applicable yet, due to the requirement of prior information, i.e., the number of categories C_t in the target domain, which is commonly unavailable in reality. Therefore, the last obstacle for us is: How to determine the number of categories C_t in the target domain?\nTo address this, a feasible solution is to first enumerate the possible values of the number of categories C_t in the target domain and divide the target domain into the corresponding clusters by applying a clustering algorithm like K-means\u00a0<cit.>. Then the clustering evaluation criteria\u00a0<cit.> can be employed to determine the appropriate number of target domain categories C\u0303_t.\nIn this paper, we employ the Silhouette criterion\u00a0<cit.> to facilitate estimating C\u0303_t. Technically, for a data sample x_t \u2208\ud835\udc9e_I, the Silhouette value s(x_t) is defined as:\n\n    a(x_t)    = 1/|\ud835\udc9e_I| - 1\u2211_x \u2208\ud835\udc9e_I,  x  x_t d(x_t, x),\n    \n         b(x_t)    = min_J\u2260 I1/|\ud835\udc9e_J|\u2211_x \u2208\ud835\udc9e_J d(x_t, x),\n    \n         s(x_t)    = b(x_t) - a(x_t)/max{a(x_t), b(x_t)}.\n\nwhere a(x_t) and b(x_t) measure the similarity of x_t to its own cluster \ud835\udc9e_I (cohesion) and other clusters \ud835\udc9e_J, J\u2260 I (separation), respectively. d(x_i, x_j) measures the distance between data points x_i and x_j, and |\ud835\udc9e_I| denotes the size of cluster \ud835\udc9e_I. The Silhouette value s(x_t) ranges from -1 to +1, where a high value indicates that the data sample x_t has a high match with its own cluster and a low match with neighboring clusters. Therefore, if most of the data samples have high Silhouette values, then the clustering configuration is appropriate; otherwise, the clustering configuration may have too many or too few clusters. \n\nSince it is challenging to obtain the exact number of target domain categories C_t, in our implementation, we empirically enumerate the possible values of C\u0303_t as [1/3C_s, 1/2C_s, C_s, 2C_s, 3C_s], taking into account the scenarios may encounter. Note that we only estimate the value of C\u0303_t at the beginning, and subsequently, we do not change the value of C\u0303_t considering the overall efficiency.\n\n\n\n \u00a7.\u00a7 Local Consensus Clustering\n\nAlthough the global one-vs-all clustering pseudo-labeling algorithm  encourages the separation between \u201cknown\" and \u201cunknown\" data samples, semantically incorrect pseudo-label assignments still occur due to domain shift and category shift, resulting in negative transfer.\n\nTo mitigate this, we further introduce a local k-NN consensus clustering strategy that exploits the intrinsic consensus structure of the target domain \ud835\udc9f_t. Specifically, during model adaptation, we maintain a memory bank \ud835\udca2_t = {g_t(x_t), \u03b4(f_t(x_t))}_x_t \u2208\ud835\udc9f_t, which contains the target features and corresponding prediction scores. The local k-NN consensus clustering is then realized by:\n\n    l_c^i    = 1/|L^i|\u2211_x_t\u2208L^i\u03b4_c(f_t(x_t)),\n    \u2112_tar^loc   = -1/N\u2211_i=1^N\u2211_c=1^C_s l_c^ilog\u03b4_c(f_t(x_t^i)).\n\nwhere \u03b4_c(f_t(x_t)) denotes the soft-max probability of data instance x_t belonging to the c-th class, L^i refers to the set of nearest neighbors of data x_t^i in the embedding feature space. Here, we apply the cosine similarity function to find the nearest neighbors L^i of x_t^i in the memory bank \ud835\udca2_t. We then encourage minimizing the cross entropy loss between x_t^i and the nearest neighbors L^i to achieve the local semantic consensus clustering.\n\n\n\n\n \u00a7.\u00a7 Optimization Objective\n\nThe overall training loss of GLC can be written as:\n\n    \u2112_tar^glb   = -1/N\u2211_i=1^N\u2211_c=1^C_sq\u0302_c^ilog\u03b4_c(h_t(g_t(x_t^i)))),\n           \n    \u2112_tar   = \u03b7\u2112_tar^glb + \u2112_tar^loc.\n\nwhere q\u0302_c^i denotes the global clustering pseudo label for data sample x_t^i, and \u2112_tar^glb is the corresponding global cross-entropy loss. \u03b7 > 0 is a trade-off hyper-parameter.\n\n\n\n \u00a7.\u00a7 Inference Details\n\nAs there is only one standard classification model, we apply the normalized Shannon Entropy\u00a0<cit.> as the uncertainty metric to separate known and unknown data samples:\n\n    I(x_t) = - 1/log C_s\u2211_c=1^C_s\u03b4_c(f_t(x_t)) log\u03b4_c(f_t(x_t))\n\nwhere C_s is the class number of source domain \ud835\udc9f_s, and \u03b4_c(f_t(x_t)) denotes the soft-max probability of data sample x_t belonging to the c-th class. The higher the uncertainty, the more the model f_t tends to assign an unknown label to the data sample. During inference stage, given an input sample x_t, we first compute I(x_t) and then predict the class of y(x_t) with a pre-defined threshold \u03c9 as:\n\n    y(x_t)    = {   unknown,    if I(x_t) \u2265\u03c9\n       argmax(f_t(x_t)),    if I(x_t) < \u03c9.\n\nwhich either rejects the input sample x_t as unknown or classifies it into a known class. In our implementation, we set \u03c9 = 0.55 for all standard benchmark datasets. Its sensitivity analysis can be found in the experiments.\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n \u00a7.\u00a7 Setup\n\nDataset: We utilize the following standard datasets in DA to evaluate the effectiveness and versatility of our method. Office-31\u00a0<cit.> is a widely-used small-sized domain adaptation benchmark, consisting of 31 object classes (4,652 images) under office environment from three domains (DSLR (D), Amazon (A), and Webcam (W)). Office-Home\u00a0<cit.> is another popular medium-sized benchmark, consisting of 65 categories (15,500 images) from four domains (Artistic images (Ar), Clip-Art images (Cl), Product images (Pr), and Real-World images (Rw)). VisDA-C\u00a0<cit.> is a more challenging benchmark with 12 object classes, where the source domain contains 152,397 synthetic images generated by rendering 3D models and the target domain consists of 55,388 images from Microsoft COCO. Domain-Net\u00a0<cit.>, is the largest domain adaptation benchmark with about 0.6 million images, which contains 345 classes. Similar to previous works\u00a0<cit.>, we conduct experiments on three subsets from it (Painting (P), Real (R), and Sketch (S)). We evaluate our GLC on partial-set DA (PDA), open-set DA (OSDA), and open-partial-set DA (OPDA) scenarios. Detailed classes split are summarized in Table\u00a0<ref>.\n\n\n\n\nEvaluation protocols: For a fair comparison, we utilize the same evaluation metric as previous works\u00a0<cit.>. Specifically, in PDA scenario, we report the classification accuracy over all target samples. In OSDA and OPDA scenarios, considering the trade-off between \u201cknown\" and \u201cunknown\" categories, we report the H-score, i.e., the harmonic mean of the accuracy of \u201cknown\" and \u201cunknown\" samples.\n\nImplementation details: We adopt the same network architecture with existing baseline methods. Specifically, we adopt the ResNet-50\u00a0<cit.> pre-trained on ImageNet\u00a0<cit.> as the backbone for all datasets. For preparing the source model, here, we utilize the same network structure and training recipe as SHOT\u00a0<cit.>. We present more details about source model training in the supplementary. During target model adaptation, we apply the SGD optimizer with momentum 0.9. The batch size is set to 64 for all benchmark datasets. We set the learning rate to 1e-3 for Office-31 and Office-Home, and 1e-4 for VisDA and DomainNet. For hyper-parameter, as we described in previous sections, we set \u03c1 to 0.75 for all datasets. For local k-NN consensus clustering, |L| is set to 4 for all benchmarks. As for \u03b7, we set it to 0.3 for Office-31, VisDA, and 1.5 for Office-Home and DomainNet. All experiments are conducted on an RTX-3090 GPU with PyTorch-1.10.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Experiment Results\n\nTo verify the effectiveness of our GLC, we conduct extensive experiments on three possible category-shift scenarios, i.e., open-partial-set DA (OPDA), open-set DA (OSDA), and partial-set DA (PDA). We compare GLC with data-dependent and more recent data-free methods to empirically demonstrate the merit of GLC. In adaptation, data-dependent methods typically require access to source raw-data, while data-free methods require source pre-trained models. In particular, GLC requires only a standard pre-trained source model, i.e., without any dedicated model architectures as\u00a0<cit.>. For a fair comparison, all methods are performed without the prior knowledge of category-shift, except those designed only for specific scenarios.\n\nResults on OPDA: We first conduct experiments on the most challenging setting, i.e., OPDA, in which both source and target domains involve private categories. Results on Office-Home are summarized in Table\u00a0<ref>, and results on Office-31, VisDA and DomainNet are summarized in Table\u00a0<ref>. As shown in Table\u00a0<ref> and Table\u00a0<ref>, our GLC achieves new state-of-the-arts, even compared to previous data-dependent methods. Especially, on VisDA, GLC achieves the H-score of 73.1%, which surpasses GATE\u00a0<cit.> and UMAD\u00a0<cit.> by a wide margin (16.7% and 14.8%). On the largest benchmark, i.e., DomainNet, GLC still achieves consistent performance improvements compared to UMAD and GATE, with gains of approximately 8.0% and 3.0%. \n\nResults on OSDA: We then conduct experiments on OSDA, where only the target domain involves categories not presented in the source domain. Results on Office-Home, Office-31, and VisDA are summarized in Table\u00a0<ref>. As shown in Table\u00a0<ref>, GLC still achieves state-of-the-art performance. Specifically, GLC obtains 69.8% H-score on Office-Home and 72.5% H-score on VisDA, with an improvement of 3.4% and 5.7% compared to UMAD.\n\nResults on PDA: We last verify the effectiveness of GLC on PDA, where the label space of the target domain is a subset of the source domain. Results summarized in Table\u00a0<ref> show that GLC still achieves comparable performance compared to methods tailored for PDA. In a fairer comparison, GLC clearly outperforms UMAD, specifically achieving performance gains of 6.2%, 4.6%, and 7.7% on Office-31, Office-Home, and VisDA, respectively.\n\n\n\n \u00a7.\u00a7 Experiment Analysis\n\n\n\n\n\n\nAblation Study: To verify the effectiveness of different components within GLC, we conduct extensive ablation studies on Office-31, Office-Home, and VisDA in OPDA scenarios. The results are summarized in Table\u00a0<ref>. Here GLC w/o \u2112_tar^glb refers to that we only employ the local k-NN consensus clustering loss to regulate model adaptation, while GLC w/o \u2112_tar^loc denotes that we only employ the global one-vs-all clustering based pseudo-labeling algorithm to achieve model adaptation. From these results, we can conclude that our local and global clustering strategies are complementary to each other. And global clustering is of vital importance to help us distinguish \u201cknown\" and \u201cunknown\" categories. For example, on VisDA, with only \u2112_tar^glb, we can advance the source model from the H-score of 25.7% to 66.0%, and outperform GATE by 9.6%.\n\nHyper-parameter Sensitivity: We first study the parameter sensitivity of \u03b7 and \u03c1 on Office-31 under OPDA setting in Fig.\u00a0<ref> (a-b), where \u03b7 is in the range of [0.1, 0.2, 0.3, 0.4, 0.5], and \u03c1 is in the range of [1/2, 2/3, 3/4, 4/5, 1.0]. Note that \u03c1 = 1.0 denotes that we do not introduce the confidence based source-private suppression mechanism. It is easy to find that results around the selected parameters \u03b7 = 0.3 and \u03c1 = 0.75 are stable, and much better than the source model. By oracle validation, we may find better hyper-parameter settings, e.g., \u03b7 = 0.1 and \u03c1 = 0.50. \nIn Fig.\u00a0<ref> (c), we present the H-score with respect to \u03c9 on Office-Home in OPDA. For all benchmarks, we pre-define \u03c9 = 0.55 to separate \u201cknown\" and \u201cunknown\" samples. The results show that H-score is relatively stable around our selection, and we could achieve better performance when setting \u03c9 to 0.65 via oracle validation. Besides, in Fig.\u00a0<ref> (d), we illustrate the H-score convergence curves on VisDA.\n\nVarying Unknown Classes: As increasing \u201cunknown\" classes, it becomes more difficult to correctly identify the \u201cunknown\" and \u201cknown\" objects. To examine the robustness of GLC, we compare GLC with other methods when varying unknown classes on Office-Home under OPDA setting. Fig.\u00a0<ref> shows that GLC achieves more stable and much better performance against existing methods.\n\n\n\n\n\n \u00a7.\u00a7 Discussion\n\nSo far, most existing domain adaptation methods designed for category shift are not applicable to the vanilla closed-set DA (CLDA). To verify the effectiveness of GLC in CLDA, we have conducted experiments on Office-31 and Office-Home in the Appendix. Moreover, existing methods usually perform experiments only on standard computer science benchmarks. Here, we have validated the effectiveness of GLC in more realistic applications, including remote sensing in PDA, wildlife classification in OSDA, and single-cell RNA sequence identification in OPDA. These results are also presented in the Appendix.\n\n\n\n\n\u00a7 CONCLUSION\n\nIn this paper, we have presented Global and Local Clustering (CLC) for upcycling models under domain shift and category shift. Technically, we have devised an innovative one-vs-all global clustering strategy to realize \u201cunknown\" and \u201cknown\" data separation, and introduced a local k-NN clustering strategy to alleviate negative transfer. Compared to existing approaches that require source data or are only applicable to specific category shifts, GLC is appealing by enabling universal model adaptation on the basis of only standard pre-trained source models. Extensive experiments in partial-set, open-set, and open-partial-set DA scenarios across several benchmarks have verified the effectiveness and superiority of GLC. Remarkably, GLC significantly outperforms existing methods by almost 15% on the VisDA benchmark in open-partial-set DA scenario.\n\n\nAcknowledgment: This work was supported by Shanghai Municipal Science and Technology Major Project (No.2018SHZDZX01), ZJ Lab, and Shanghai Center for Brain Science and Brain-Inspired Technology, the Shanghai Rising Star Program (No.21QC1400900), and the Tongji-Westwell Autonomous Vehicle Joint Lab Project.\n\n\n\n\n\n\n\n\n\n\u00a7 SOURCE MODEL PREPARING\n\nAs aforementioned, in this paper, we focus on the K-way classification. For a given source domain \ud835\udc9f_s = { (x_i^s, y_i^s) }_i=1^N_s where x^i_s \u2208\ud835\udcb3_s and y^i_s \u2208\ud835\udcb4_s \u2282\u211d^K, we adopt the same recipe as SHOT\u00a0<cit.> and BMD\u00a0<cit.> to prepare the source model. Specifically, the source model f_s parameterized by a deep neural network consists of two modules: the feature encoding module g_s: \ud835\udcb3_s \u2192\u211d^d and the classifier module h_s: \u211d^d \u2192\u211d^K, i.e., f_s = h_s \u2218 g_s. We optimize f_s with the following loss:\n\n    \u2112_src = -1/N\u2211_i=1^N\u2211_k=1^K q_k log\u03b4_k(f_s(x^i_s))\n\n\nwhere \u03b4_k (f_s(x_s)) denotes the softmax probability of source sample x_s belonging to the k-th category, q_k is the smoothed one-hot encoding of y_s, i.e., q_k = (1 -\u03b1)*1_[k=y_s] + \u03b1 / K, and \u03b1 is the smoothing parameter which is set to 0.1 for all benchmarks.\n\n\n\n\u00a7 EXPERIMENTS ON CLOSED-SET ADAPTATION\n\nExisting methods designed for category shift, typically do not perform well for the vanilla closed-set domain adaptation scenario (CLDA). To examine the effectiveness and robustness of GLC, we further conduct experiments on Office-31, and Office-Home. All implementation details are the same as before, e.g., we adopt the ResNet-50\u00a0<cit.> as the backbone, the learning rate is set to 1e-3, and \u03c1 is set to 0.75. The results are listed in Table\u00a0<ref> of this supplementary material. As shown in this Table, despite the fact that the GLC is not tailored for CLDA, we still attain comparable or even better performance compared to existing methods designated for CLDA, e.g., MDD\u00a0<cit.>. Specifically, GLC obtains the overall accuracy of 88.1% and 70.4% on Office-31 and Office-Home, respectively. While MDD attains 88.9% and 68.1% on Office-31 and Office-Home, respectively. In a fairer comparison, GLC significantly outperforms UMAD\u00a0<cit.>, which is also model adaptation method designed for category-shift. Specifically, GLC outperforms UMAD by 6.4% and 7.3% on Office-31 and Office-Home.\n\n\n\n\u00a7 EXPERIMENTS ON REALISTIC APPLICATIONS\n\nSo far, most existing methods usually perform experiments only on standard computer science benchmarks. Here, we have further validated the effectiveness and superiority of GLC in realistic applications, including remote-sensing recognition, wild-animal classification, and single-cell RNA sequence identification. We present more details in the following.\n\n\n\n \u00a7.\u00a7 Partial-set Model Adaptation on Remote Sensing Recognition\n\nRemote sensing has great potential to manage global climate change, population movements, ecosystem transformations, and economic development. However, due to data protection regulations\u00a0<cit.>, it is difficult for researchers to obtain multi-scene, high-resolution satellite imagery. For example, there are strict data regulation policies in China for high-resolution remote sensing images in meteorological, oceanic, and environmental scenarios\u00a0<cit.>. To validate the effectiveness of GLC on remote sensing, we conduct experiments on two existing large-scale datasets, the PatternNet\u00a0<cit.> and the NWPU45\u00a0<cit.> dataset. PatternNet is one of the largest satellite image datasets collected from Google Earth imagery in the US. It contains 38 scene classes and 30,400 high-resolution (0.2\u223c6m per pixel) satellite images, such as airport, beach, dense residential, forest, etc. NWPU45 dataset consists of 45 scene classes and 31,500 satellite images covering more than 100 countries and regions around the world. Its spatial resolution varies from about 30 to 0.2 m per pixel. The heterogeneity of spatial resolution and geographic location poses a significant challenge to model adaptation. In this paper, we set the PatternNet as source dataset and the NWPU45 as target dataset to investigate the model adaptation from high-resolution satellite images to low-resolution satellite images. There are 21 overlapping scenes classes between PatternNet and NWPU45. Thus, we transfer the scene classes from the PatternNet to the 21 overlapping scene classes in the NWPU45 and compare the results with the original labels from the NWPU45 for performance evaluation.\n\n\n\n\n\nAn illustration of boxplot in Fig.\u00a0<ref>b basically demonstrates that GLC effectively realizes model adaptation and achieves more accurate performance with less variance than existing methods. Quantitatively, GLC achieves 64.6 \u00b1 0.22 % overall accuracy with 5 different random seeds. In contrast to GLC, the baseline methods DCC, ETN, DANCE, and BA3US obtain 55.2 \u00b1 0.80 %, 54.9 \u00b1 0.77 %, 62.0 \u00b1 1.02 % and 59.6 \u00b1 1.25 % overall accuracy, respectively. To verify the robustness, we further conduct an ablation experiment on decreasing the number of overlapping scenes between source and target domains. The number gets smaller, there is more probability of overlapping samples being categorized into other scenes. Despite this, the results in Fig.\u00a0<ref>c show that GLC is still capable of addressing this challenge and even achieving better performance. Quantitatively, GLC obtains 64.5% average accuracy in four different target situations. In contrast, the baseline methods DCC, ETN, DANCE, and BA3US attain 55.4%, 55.0 %, 52.4%, and 49.6% overall accuracy, respectively. We attribute this to our global one-vs-all clustering algorithm, which is able to discover non-existent scene categories and suppress model adaptation over these categories.\n\n\n\n \u00a7.\u00a7 Open-set Model Adaptation on Wild Animal Classification\n\n\nWe next study a more challenging setting, the open-set model adaptation on wild animal classification. Having the ability to accurately classify wild animals is important for studying and protecting ecosystems\u00a0<cit.>, especially the ability to identify novel species\u00a0<cit.>. However, it is almost impossible for a database to cover all animal species, and it is also typically difficult to collect and annotate a large number of wild animal images in practice. Thereby, it would be ideal if we develop an animal classification system based on the existing large number of virtual animal images on the Internet. In this article, we execute experiments on the I2AWA\u00a0<cit.> benchmark to investigate open-set model adaptation from virtual to real-world. I2AWA consists of a virtual source domain dataset and a real-world target domain dataset with a total of 50 animal categories. We divide the first 30 into known categories in alphabetical order and the remaining 20 into unknown categories. The source domain dataset consists of 2,970 virtual animal images collected through the Google-Image search engine, while the target domain dataset comes from the AWA2\u00a0<cit.> dataset with a total of 37,322 images from the real world. Due to differences in image styles between virtual and real-world datasets, directly deploying a DNN model trained on the virtual images can lead to severe performance degradation. For a qualitative demonstration, we enumerate some animal images on target domain in Fig.\u00a0<ref>a and apply the Grad-CAM heatmap\u00a0<cit.> technique to compare the source model with the adapted target model by our GLC technique. From this, we can conclude that the source model typically fails to locate and extract key information for animal identification, while the upcycled model overcomes these failures well. For quantitative performance evaluation, we compare GLC with the methods dedicated to open-set domain adaptation (DANN\u00a0<cit.>, OSBP\u00a0<cit.>), and the methods designed for universal domain adaptation (DANCE\u00a0<cit.>, OVANet\u00a0<cit.>).\n\nAn inspection of the tSNE plots (Fig.\u00a0<ref>b) indicates that our GLC algorithm effectively realizes known animal classification and unknown animal separation. This observation is further demonstrated by the quantitative metric in Fig.\u00a0<ref>c. Specifically, GLC achieves 79.1 \u00b1 0.28 % overall H-Score. In contrast, the baseline methods DANN, OVANet, OSBP and DANCE obtains 70.1 \u00b1 0.85%, 70.8 \u00b1 0.58%, 72.2 \u00b1 1.61%, 74.5 \u00b1 0.32% overall H-Score. As presented in Fig.\u00a0<ref>d, compared to existing methods, GLC further provides significant savings in target-domain side computational resource overhead (about 48.4% training time reduction in this case). This is due to the fact that our GLC merely fine-tunes the source model to realize adaptation, while existing source data-dependent methods need to train the target models from scratch.\n\nTo visually assess the separation between the known and unknown classes, we present the uncertainty density distribution in Fig.\u00a0<ref>e. The higher the uncertainty, the more the model treated the input animal image as an unknown species. The results show that while OVANet and DANCE are able to achieve promising classification of known classes of animals, they have trouble in unknown animal separation. In contrast, our GLC draws a better trade-off between known animal classification and unknown animal identification. \n\nWe further examine the robustness of GLC in different open-set situations, e.g., varying target domain unknown categories and source domain known categories. As illustrated in Fig.\u00a0<ref>f, we can find that GLC maintains a promising H-score compared to existing methods. Specifically, GLC achieves 78.3% overall H-Score in four different target domain unknown categories situations, while DANN, OSBP, OVANet, and DANCE obtain 69.3%, 72.9%, 70.8%, 75.4% average H-Score, respectively. Similarly, when source domain known categories varies, GLC arrives 83.4% overall H-Score in four different situations, still significantly outperforming existing methods.\n\n\n\n\n\n \u00a7.\u00a7 Open-partial-set Model Adaptation on Single-cell Identification\n\nWe finally consider the most challenging scenario, open-partial-set adaptation, where both source and target domains contain private categories. Here, we implement experiments on single-cell identification. It has great potential in the studies of cell heterogeneity, developmental dynamics, and cell communications\u00a0<cit.>. Currently, there are two main types of single-cell sequencing technologies, namely scRNA-seq and scATAC-seq. However, it has been noted that the extreme scarcity of scATAC-seq data tends to limit its ability for cell type identification.  In contrast, large amounts of well-annotated scRNA-seq datasets have been curated as cell atlases.  It motivates us to upcycle models trained on the scRNA-seq datasets and adapt them to the scATAC-seq datasets. Nevertheless, the cell types contained in different atlas data are generally inconsistent, which poses substantial challenges for model adaptation across atlases. In this article, we apply our GLC to two mouse cell atlases, the Tabula Muris atlas\u00a0<cit.> for scRNA-seq data and the Cusanovich atlas\u00a0<cit.> for scATAC-seq data. The Tabula Muris atlas consists of 73 cell types totaling 96,404 cells from 20 organs with two protocols profiling transcriptomics, while the Cusanovich atlas consists of 29 cell types totaling 81,173 cells from 13 tissues. There are 19 cell types common between the Tabula Muris atlas and the Cusanovich atlas. For performance evaluation, as in the wild animal experiments above, we utilize the harmonic mean accuracy H-Score of the known cell types and the unknown cell types as the quantitative metric. We compare our GLC with recently developed and applied methods for scRNA-seq and scATAC-seq integration, including the scJoint\u00a0<cit.> and the Seurat v.3\u00a0<cit.>.\n\nWe illustrate the tSNE plots in Fig.\u00a0<ref>a to compare with the ground truth labels annotated in the Cusanovich atlas\u00a0<cit.>. The tSNE plots are generated by applying the singular value of the term frequency-inverse document frequency (TD-IDF) transformation of scATAC-seq peak matrix as in the Cusanovich atlas\u00a0<cit.>. It observes that GLC achieves a better trade-off between known cell types identification and unknown cell types separation than the other methods. This observation is further quantitatively demonstrated by the H-Score metric. Specifically, GLC obtains 62% overall H-Score compared with 58% for Seurat and 56% for scJoint. As presented in Fig.\u00a0<ref>c, not only is there a significant performance improvement, but our GLC also brings significant savings in target domain computational resource overhead (about 75.9% training time reduction).\n\nIn addition to tSNE plots, we also present the uncertainty density distribution in Fig.\u00a0<ref>b, where the higher the uncertainty, the more the model tends to group the cell into the unknown cell types group. To find the best trade-off point, a global decision boundary search was performed for all methods. The decision boundary for GLC is 0.5 compared with 0.25 for Seurat and 0.05 for scJoint. It further indicates that our GLC attains an optimal trade-off in cell types identification to the other methods.\n\n\n\n\u00a7 DISCUSSION\n\nDuring the past decades, deep neural networks (DNNs) have achieved remarkable success in various applications and fields. However, DNNs are typically restricted to the training data domain. If the test data is collected in another modality or from other types of instruments, we will typically suffer from a significant performance degradation\u00a0<cit.>. This phenomenon is likely to worsen when training and testing data do not share the same ground-truth class space. Although DNNs can be adapted to different application scenarios with additional supervised learning, this paradigm asks for annotation of large-scale target domain data. It would require significant resources and experts in real-world applications, such as clinical staff for medical imaging diagnosis and genetic scientists for single-cell sequence analysis, making it extremely expensive and impossible for most scenarios. \nIn this paper, we find that it is possible to productively upcycle existing pre-trained models and adapt them to new scenarios. Numerous empirical evidences on standard computer science benchmarks and simulated realistic applications basically demonstrate that GLC is a promising, simple, and general solution for a variety of real-world application tasks, including single-cell sequence analysis, remote sensing recognition, and other such domain-dependent problems.\n\nieee_fullname\n\n\n"}