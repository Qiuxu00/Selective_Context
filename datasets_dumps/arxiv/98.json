{"entry_id": "http://arxiv.org/abs/2303.07230v1", "published": "20230313160414", "title": "Systematic Evaluation of Deep Learning Models for Failure Prediction", "authors": ["Fatemeh Hadadi", "Joshua H. Dawes", "Donghwan Shin", "Domenico Bianculli", "Lionel Briand"], "primary_category": "cs.SE", "categories": ["cs.SE"], "text": "\n\n\n\n\n\n\n\n\nSystematic Evaluation of Deep Learning Models for Failure Prediction\n\nThis work was supported by the Canada Research Chair and Discovery Grant programs of the Natural Sciences and Engineering Research Council of Canada (NSERC), by a University of Luxembourg\u2019s joint research program grant, and by European Union's Horizon 2020 Research and Innovation Programme under grant agreement No. 957254 (COSMOS).\nThe experiments conducted in this work were enabled in part by Digital Alliance of Canada  (alliancecan.ca).\n\n\n    \n\tFatemeh Hadadi\n        Joshua H. Dawes\n        Donghwan Shin\n        Domenico Bianculli\n        Lionel Briand\n\n    \n==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\n\n\nWith the increasing complexity and scope of software systems, their dependability is crucial. The analysis of log data recorded during system execution can enable engineers to automatically predict failures at run time. Several Machine Learning (ML) techniques, including traditional ML and Deep Learning (DL), have been proposed to automate such tasks. However, current empirical studies are limited in terms of covering all main DL types\u2014Recurrent Neural Network (RNN), Convolutional Neural network (CNN), and transformer\u2014as well as examining them on a wide range of diverse datasets.\n\nIn this paper, we aim to address these issues by systematically investigating the combination of log data embedding strategies and DL types for failure prediction. To that end, we propose a modular architecture to accommodate various configurations of embedding strategies and DL-based encoders. To further investigate how dataset characteristics such as dataset size and failure percentage affect model accuracy, we synthesised 360 datasets, with varying characteristics, for three distinct system behavioral models, based on a systematic and automated generation approach. Using the F1 score metric, our results show that the best overall performing configuration is a CNN-based encoder with Logkey2vec. Additionally, we provide specific dataset conditions, namely a dataset size >350 or a failure percentage >7.5%, under which this configuration demonstrates high accuracy for failure prediction.\n\nKeywords: Logs, Failure Prediction, Deep Learning, Embedding Strategy, Synthesised Data Generation, Systematic Evaluation\n \n\n\n\u00a7 INTRODUCTION\n\n\nAs software systems continue to increase in complexity and scope, reliability and availability play a critical role in quality assurance and software maintenance\u00a0<cit.>. During runtime, software systems often record log data about their execution, designed to help engineers monitor the system\u2019s behavior\u00a0<cit.>. One important quality assurance activity is to predict failures at run time based on log analysis, as early as possible before they occur, to enable corrective actions and minimise the risk of system disruptions\u00a0<cit.>. \n\n\n\nHowever, software systems typically generate a vast quantity of log data which makes manual analysis error-prone and extremely time-consuming. Therefore, a number of automatic log analysis methods, particularly for failure prediction\u00a0<cit.> and anomaly detection\u00a0<cit.>, have been proposed over the past few years. \nMachine Learning (ML) has played a key role in automatic log analysis, from Traditional ML methods (e.g., Random Forest (RF)\u00a0<cit.>, Support Vector Machine (SVM)\u00a0<cit.>, Gradient Boosting (GB)\u00a0<cit.>) to Deep Learning (DL) methods (e.g., DeepLog\u00a0<cit.>, LogRobust\u00a0<cit.>, LogBERT\u00a0<cit.>) relying on various DL network architectures, including Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN), and transformers\u00a0<cit.>. \n\nAlthough several studies have explored the use of DL models with various log sequence embedding strategies\u00a0<cit.>, they have been limited in terms of evaluating the three main types of DL networks\u2014RNN, CNN, and transformer\u2014combined with different embedding strategies; for instance, two studies by <cit.> and <cit.> included CNN-based models but did not cover transformer-based models.\nMoreover, previously studied models were often applied to a limited number of available datasets, which severely limited the generalizability of results\u00a0<cit.>. Indeed, because these few datasets exhibit a limited variety of characteristics, studying the robustness and generalizability of DL models, along with their embedding strategies, is unlikely to yield practical guidelines.\n\nIn this paper, we aim to systematically investigate the combination of the main DL architectures and embedding strategies, based on datasets whose main characteristics (e.g., dataset size and failure percentage) are controlled. \nTo achieve this, we first introduce a modular architecture for failure prediction, where alternative log embedding strategies and DL models can be easily applied. \nThe architecture consists of two major steps: an embedding step that converts input logs into log embedding vectors followed by a classification step that predicts failures by processing the embedding vectors using encoders that are configured by different DL models, called DL encoders. \nIn the embedding step, two alternative strategies, i.e., the pretrained BERT\u00a0<cit.> and trainable Logkey2vec\u00a0<cit.>, are considered.\nIn the classification step, four types of DL models, including LSTM\u00a0<cit.>, BiLSTM<cit.>, CNN<cit.>, and transformer\u00a0<cit.>, with an attention mechanism\u00a0<cit.> for BiLSTM and transformer, are used.\n\n\nAlso, to address the issue of the limited availability of adequate datasets, we designed a rigorous approach for generating synthesised data relying on behavioral models built by applying model inference algorithms\u00a0<cit.> to available system logs. When synthesizing data, we control key dataset characteristics such as the size of the dataset and the percentage of failures. Additionally, we define patterns that are associated with system failures and are used to classify logs for the failure prediction task. The goal is to associate failures with complex patterns that are challenging for failure prediction models. \nFurther, based on our study, we investigated how the dataset characteristics determine the accuracy of model predictions and then derive practical guidelines. \n\n\nOur empirical results conclude that the best model includes the improved CNN-based encoder with Logkey2vec as an embedding strategy.  Using a wide variety of datasets showed that this combination is also very accurate when  certain conditions are met in terms of dataset size and failure percentage. \n\nTo summarise, the main contributions of this paper are:\n\n    \n  * A large-scale, systematic investigation of the application of various DL encoders\u2013LSTM-, BiLSTM-, CNN-, and transformer-based\u2013 and embedding strategies\u2013BERT\u00a0<cit.> and Logkey2vec\u00a0<cit.>\u2013 for failure prediction modeling\n\n  * A systematic and automated approach to synthesise log data, with a focus on experimentation in the area of failure prediction, to enable the control of key data set characteristics while avoiding any other form of bias.   \n    \n  * Practical guidelines for using DL-based failure prediction models depending on dataset characteristics such as  dataset size and failure rates. \n    \n  * A publicly available replication package, containing the implementation, generated datasets with behavioural models, and results.\n\n\nThe rest of the paper is organised as follows. Section\u00a0<ref> presents the basic definitions and concepts that will be used throughout the paper. Section\u00a0<ref> illustrates related work. Section\u00a0<ref> describes the architecture of our failure predictor with its different configuration options. Section\u00a0<ref> describes our research questions, empirical methodology, and synthetic log data generation. Section\u00a0<ref> reports empirical results. Section\u00a0<ref> discusses the implications of the results. Section\u00a0<ref> concludes the paper and suggests future directions for research and improvements.\n \n\n\u00a7 BACKGROUND\n\n\nIn this section, we provide background information on the main concepts and techniques that will be used throughout the paper. \nFirst, we briefly introduce the concepts related to finite state automata (FSA) and regular expressions in \u00a0<ref> and execution logs in \u00a0<ref>. \nWe then describe two important log analysis tasks (anomaly detection and failure prediction) in \u00a0<ref> and further review machine-learning (ML)-based approaches for performing such tasks in \u00a0<ref>. \nWe conclude by providing an overview of embedding strategies for log-based analyses in \u00a0<ref>.\n\n\n\n \u00a7.\u00a7 Finite State Automata and Regular Expressions\n\n\nA deterministic FSA is a tuple \u2133 = \u27e8 Q, A, q_0, \u03a3, \u03b4\u27e9, where Q is a finite set of states, A \u2286 Q is the set of accepting states,  q_0 \u2208 Q is the starting state, \u03a3 is the alphabet of the automaton, and \u03b4 Q \u00d7\u03a3\u2192 Q is the transition function. The extended transition function \u03b4^* : Q \u00d7\u03a3^* \u2192 Q, where \u03a3^* is the set of strings over \u03a3, is defined as follows:\n\n    \n  * For every q \u2208 Q, \u03b4^*(q,\u03f5) = q, where \u03f5 represents the empty string;\n    \n  * For every q \u2208 Q, every y \u2208\u03a3^*, and every \u03c3\u2208\u03a3, \u03b4^*(q, y\u03c3) = \u03b4(\u03b4^*(q, y), \u03c3).\n\nLet x \u2208\u03a3^*; the string x is accepted by \u2133 if \u03b4^*(q_0, x) \u2208 A and is rejected by \u2133, otherwise.\n\nThe language accepted by an FSA \u2133 is denoted by \u2112(\u2133) and is defined as the set of strings that are accepted by \u2133; more formally, \u2112(\u2133) = {w |\u03b4^*(q_0, w) \u2208 A }.\nA language accepted by an FSA is called a regular language.\n\nRegular languages can also be defined using regular expressions; given a regular expression r we denote by \u2112(r) the language it represents. \nA regular expression r over an alphabet \u03a3 is a string  containing symbols from \u03a3 and special meta-symbols like \u201c|\u201d (union or alternation), \u201c.\u201d (concatenation), and \u201c*\u201d (Kleene closure or star), defined recursively using the following rules:\n\n\n    \n  * \u2205 is a regular expression denoting the empty language \u2112(\u2205)=\u2205;\n    \n  * For every a \u2208\u03a3, a is a regular expression corresponding to the language \u2112(a)={a};\n    \n  * If s and t are regular expressions, then r= s | t and r=s.t (or r=st) are regular expressions denoting, respectively, the union and the concatenation of \u2112(s) and \u2112(t);\n    \n  * If s is a regular expression, then r=s^* is a regular expression denoting the Kleene closure of \u2112(s).\n\n\n\n\n\n \u00a7.\u00a7 Logs\n\n\nIn general, a log is a sequence of log messages generated by logging statements (e.g., , ) in the source code\u00a0<cit.>. \nA log message is textual data composed of a header and content\u00a0<cit.>. In practice, the logging framework determines the header (e.g., ) while the content is designed by developers and is composed of static and dynamic parts. \nThe static parts are the fixed text written by the developers in the logging statement (e.g., to describe a system event), while the dynamic parts are determined by expressions (involving program variables) evaluated at runtime. \nFor instance, let us consider the execution of the log printing statement ; during the execution, assuming variable  is equal to , the log message  is printed. In this case,  is the static part while  is the dynamic part, which changes depending on the value of  at run time.\n\nA log template (also called event template or log key) is an abstraction of the log message content, in which dynamic parts are masked with a special symbol (e.g., ); for example, the log template corresponding to the above log message is . Often, each unique log template is identified by an ID number for faster analysis and efficient data storage. \n\nA log sequence is a fragment of a log, i.e., a sequence of log messages contained in a log;  in some cases, it is convenient to abstract log sequences by replacing the log messages with their log templates. \nLog sequences are obtained by partitioning logs based on either log message identifiers (e.g., session IDs) or log timestamps (e.g., by extracting consecutive log messages using a fixed/sliding window). \nFor a log sequence l, |l| indicates the length of the log sequence, i.e., the number of elements (either log templates or log messages), not necessarily unique, inside the sequence.  \n\nFigure\u00a0<ref> shows an example summarizing the aforementioned concepts. On the left side, the first three log messages are partitioned (using a fixed window of size three) to create a log sequence. The first message in the log sequence (LogMessage1) is . It is decomposed into the header  and the content . The log template for the content is ; the dynamic parts are  and .\n\n\n\n\n\n\n \u00a7.\u00a7 Log Analysis Tasks\n\n\nIn the area of log analysis, several major tasks for reliability engineering such as anomaly detection and failure prediction have been automated\u00a0<cit.>. \n\n\n\n\n  \u00a7.\u00a7.\u00a7 Anomaly Detection\n\nAnomaly detection is the task of identifying anomalous patterns in log data that do not conform to expected system behaviors\u00a0<cit.>, indicating possible errors, faults, or failures in software systems. \nTo automate the task of anomaly detection, log data is often partitioned into smaller log sequences. \nThis partitioning is typically based on log identifiers (e.g., session_ID or block_ID), which correlate log messages within a series of operations; alternatively, when log identifiers are not available, timestamp-based fixed/sliding windows are also used. \nLabeling of partitions is then required, each partition usually being labeled as an anomaly either when an error, unknown, or failure message appears in it or when the corresponding log identifier is marked as anomalous. Otherwise, it is labeled as normal. \n\n\n\n  \u00a7.\u00a7.\u00a7 Failure Prediction\n\nFailure prediction attempts to proactively generate early alerts to prevent failures, which often lead to unrecoverable outages\u00a0<cit.>. Similar to anomaly detection, log data is often partitioned into sequences using log identifiers. Partitioned log sequences are labeled as failures according to mechanisms that are specific to the application being monitored. Like for anomalies, failures can be associated with complex patterns in log sequences.\n\n\n\n\n \u00a7.\u00a7 DL Techniques in Log Analysis\n\n\nIn recent years, a variety of deep learning (DL) techniques have been applied to log analysis, and more specifically to failure prediction and anomaly detection. Compared to traditional ML techniques such as Random Forests (RF) and K-nearest Neighbours (KNN), DL techniques incrementally learn high-level features from data, removing complex feature extraction activities based on domain expertise. \n\nAccording to <cit.>, there are three main categories of DL approaches in log analysis: (1) Recurrent Neural network (RNN), (2) Convolutional Neural Network (CNN), and (3) transformer. In each category, different variations can be adopted; for instance, Long Short-Term Memory networks (LSTM) and Bidirectional Long Short-Term Memory networks (BiLSTM), which fall into the RNN category, have been repeatedly used for anomaly detection and failure prediction\u00a0<cit.>. We now explain the major features of each category as well as their variations. \n\n\n\n\n  \u00a7.\u00a7.\u00a7 RNN\n\n \n\nLSTM\u00a0<cit.> is an RNN-based model commonly used in both anomaly detection and failure prediction\u00a0<cit.>. An LSTM network consists of multiple units, each of which is composed of a cell, an input gate, an output gate\u00a0<cit.>, and a forget gate\u00a0<cit.>. An LSTM-based model reads an input sequence (x_1, \u2026, x_n) and produces a corresponding sequence (y_1, \u2026, y_n) with the same length.\nAt each time step t>1, an LSTM unit reads the input x_t as well as the previous hidden state h_t-1 and the previous memory c_t-1 to compute the hidden state h_t. The hidden state is employed to produce an output at each step.\nThe memory cell c_t is updated at each time step t by partially forgetting old, irrelevant information and accepting new input information. The forget gate f_t is employed to control the amount of information to be removed from the previous context (i.e., c_t-1) in the memory cell c_t. \nAs a recurrent network, an LSTM shares the same parameters across all steps, which reduces the total number of parameters to learn. Learning is achieved by minimizing the error between the actual output and the predicted output. Moreover, to improve the regularization of an LSTM-based model, a dropout layer is applied between LSTM layers. It randomly drops some connections between memory cells by masking their value.\nLSTM-based models have shown significant performance in several studies in log-based failure prediction and anomaly detection \u00a0<cit.>.\n\nBiLSTM is an extension of the traditional LSTM\u00a0<cit.>. However, BiLSTM reads the sequence in both directions, enabling it to comprehend the relationships between the previous and the upcoming inputs. To make this possible, a BiLSTM network is composed of two layers of LSTM nodes, whereby each of these layers learns from the input sequence in the opposite direction. At time step t, the output h_t is calculated by concatenating h_t^f (the hidden states in a forward pass) and h_t^b (the hidden states in a backward pass). By allowing this bi-directional computation, BiLSTM is able to capture complex dependencies and produce more accurate predictions. The BiLSTM-based model has achieved accurate results for anomaly detection\u00a0<cit.>.\n\n\n\n  \u00a7.\u00a7.\u00a7 CNN\n\n \n\nCNN is a neural network primarily employed for image recognition\u00a0<cit.>. It has a unique architecture designed to handle 2D and 3D input data such as images and matrices. A CNN leverages convolutional layers to perform feature extraction and pooling layers to downsample the input. \n\nThe 1D convolutional layer uses a set of filters to perform convolution operation with the 2D input data to produce a set of feature maps (CNN layer output). According to <cit.>, let w \u2208\ud835\udc45^k \u00d7 d be a filter which is applied to a window of k elements in a d-dimension input log sequence, and let x_i represent the i-th elements in the sequence. A feature c_i \u2208\ud835\udc45 is calculated as\nc_i = \u03c3(w . x_i:i+k-1+b),\nwhere \u03c3 is the activation function (i.e., ReLu), x_i:i+k-1 represents the concatenation of elements {x_i, x_i+1, ..., x_i+k-1}, and b \u2208\ud835\udc45 denotes a bias term. After this filter is applied to each window in the sequence ({x_1:k, x_2:k, ..., x_n-k+1:n}), a feature map c = [c_1, c_3, ..., c_n-k+1] is produced, \nwhere c \u2208\ud835\udc45^n-k+1. \nParameter k represents the kernel size; it is as an important parameter of the operation. Note that there is no padding added to the input sequence, leading to feature maps smaller than the input sequence. Padding is a technique employed to add zeros to the beginning and/or end of the sequence; it allows for more space for the filter to cover, controlling the size of output feature maps. Padding is commonly used so that \nthe output feature map has the same length as the input sequence\u00a0<cit.>.\n\nThe pooling layer reduces the spatial dimensions of the feature maps extracted by the convolutional layer and simplifies the computational complexity of the network. \n\nRecently, CNNs have shown high-accuracy performance in anomaly detection\u00a0<cit.>.\n\n\n\n  \u00a7.\u00a7.\u00a7 Transformer\n\n \n\nThe transformer is a type of neural network architecture designed for natural language processing tasks, introduced by <cit.>. The main innovation of transformers is the self-attention mechanism. More important parts of the input receive higher attention, which facilitates learning the contextual relationships from input data. This is implemented by calculating a weight for each input element, which represents the importance of that element with respect to the adjacent elements. Hence, a model with self-attention (not necessarily a transformer) can capture long-range dependencies in the input. \nSince the transformers do not process inputs sequentially like LSTM, positional encoding is needed. Positional encoding vectors are fixed-size, added to the input to provide information about the position of each element in the input sequence. Further, a transformer involves a stack of multiple transformer blocks. Each block contains a self-attention layer and a feed-forward neural network layer. In the self-attention layer, the model computes attention scores (weights) for each element, allowing it to capture the relationship between all input elements. \nThe feed-forward layer is used to transform the representation learned by the self-attention layer into a new representation entering the next transformer block. In the area of log analysis, transformers have been recently applied in a few studies on anomaly detection\u00a0<cit.>,  showing outstanding performance. \n\n\n\n\n\n \u00a7.\u00a7 Log Sequence Embedding Strategies\n\nWhen analyzing log sequences, the textual data of log sequences' elements must be converted into a vector representation that is understandable by a machine; such a conversion is called the log sequence embedding. Generally, there are two main approaches for doing this: 1) based on count vectors\u00a0<cit.> or ID numbers of the sequence elements, or 2) based on the contextual information of sequence elements. Here, we cover one widely used example for each case in the following sections.\n\n\n\n  \u00a7.\u00a7.\u00a7 Logkey2vec\n\nThere are many studies that have achieved high accuracy results by using log embedding strategies that rely on the ID numbers or count vectors of log sequence elements\u00a0<cit.>. Advantages include the speed of processing and model simplicity since text preprocessing (e.g., tokenization) is not required.\n\nSimilar to methods like word2vec\u00a0<cit.>, which assigns a unique vector to each different word based on a context mapping table, Logkey2vec maps each unique log template ID to a vector representation. While Word2vec is a pre-trained tool, Logkey2vec is a trainable layer implemented inside a neural network. It relies on a matrix called \u201ccodebook\u201d, where the number of rows is the vocabulary size and the number of columns is the embedding vector size of each log template ID. The embedding vectors are first initialised by random numbers and are improved through backpropagation during training. For a log sequence, Logkey2vec computes the embedding vector of each log template based on its log template ID; each row of the matrix represents the whole log sequence.\n\n\n\n  \u00a7.\u00a7.\u00a7 BERT\n\nIn the past few years, Bidirectional Encoder Representations from Transformers (BERT) has provided significant improvements in the semantic embedding of textual information by taking the contextual information of text into account. It has been used in a few studies in log-based anomaly detection\u00a0<cit.>. This model fares better than the other pretrained transformer-based models: GPT2\u00a0<cit.> and RoBERTa\u00a0<cit.> in log sequence embedding\u00a0<cit.>.\n\nThe pre-trained BERT base model\u00a0<cit.> provides the embedding matrix of log sequences where each row is the representation vector of its corresponding log template inside the sequence. The BERT model is applied to each log template separately and then the representation is aggregated inside a matrix. To embed the information of a log template into a 768-sized vector, the BERT model first tokenizes the log template text. BERT tokenizer uses WordPiece\u00a0<cit.>, which is able to handle out-of-vocabulary (OOV) words to reduce the vocabulary size. Further, the tokens are fed to the 12 layers of BERT's transformer encoder. After obtaining the output vectors of a log template's tokens, the log template embedding is calculated by getting the average of output vectors. This process is repeated for all the log templates inside the log sequence to create an n \u00d7 768 matrix representation where n is the size of the log sequence.\n\n \n\n\u00a7 RELATED WORK\n\n\nThere are several papers reporting empirical studies of different DL-based methods for log-based anomaly detection and failure prediction.  In our review, we include studies that covered more than one DL model, possibly based on the same DL type; given our focus, non-DL models such as RF, SVM, and clustering are not included.\nThe main studies are summarised in Table\u00a0<ref>. \nColumn \u201cDL Type(s)\u201d indicates the type of DL network covered in each paper. We indicate the Log Sequence Embedding (LSE) strategies, introduced in \u00a0<ref>, in the next column; notice there are a few models not using LSE, such as DeepLog\u00a0<cit.>. \nColumn \u201cDataset(s)\u201d indicates which datasets (whether existing datasets or synthesised ones) were used in the studies.\n Column \u201cDataSet Char Control\u201d indicates whether the dataset characteristics were controlled during the experiment and lists such characteristics. \n In the last column, the labeling scheme indicates the applied method(s) for log partitioning, as mentioned in \u00a0<ref>, based on either a log identifier or timestamp (represented by L and T, respectively).\n\n\n\nWe now briefly explain the papers with the aim to motivate our study and highlight the differences. We note that, unless we mention it, LSE strategies are implemented specifically for one DL model (combinations are not explored). Indeed, many of the reported techniques tend to investigate one such combination or simply do not rely on any embeddign strategy.  The studies are listed in chronological order. \n\n<cit.> (2018) introduced CNN for anomaly detection as well as the Logkey2vec embedding strategy (see \u00a0<ref>). They compared it to LSTM and MLP networks, also relying on the Logkey2vec embedding strategy.\n\n<cit.> (2019) developed LogAnomaly, an LSTM-based model, using their proposed embedding strategy, Template2Vec (a log-specific variant of Word2Vec). \n\n<cit.> (2020) introduced a state-of-the-art LSTM-based model, Aarohi, for predicting failures using log data and compared it to two LSTM-based models Dash\u00a0<cit.> and DeepLog\u00a0<cit.> with the aim of online prediction efficiency. None of these technqiues rely on a log embeddign strategy. \n\nThe first study considering transformers in their DL comparison is by <cit.> (2020), featuring three DL models: HitAnomaly (transformer-based), LogRobust\u00a0<cit.> (BiLSTM-based), and DeepLog (LSTM-based). HitAnomaly utilises transformer blocks (see \u00a0<ref>) as part of its LSE strategy, called Log Encoder. LogRobust employed TF-IDF technique\u00a0<cit.> while DeepLog did not utilise any LSE strategy. The authors also controlled dataset characteristics by manipulating the unstable log ratios. \n\n<cit.> (2021) proposed the GRU-based\u00a0<cit.> PLELog and compared it to LogRobust and DeepLog. PLELog used the TF-IDF technique, similar to LogRobust.\n\n<cit.> (2021) proposed a transformer-based model, LogBERT, and compared its performance with two LSTM-based models, LogAnomaly and DeepLog. LogBERT uses an Embedding Matrix for its embedding strategy, which is similar to Logkey2vec.\n\n<cit.> (2022) evaluated their proposed transformer-based model, Neurallog, against LogRobust (BiLSTM-base) and DeepLog (LSTM-based). The LSE strategies for the models were a pre-trained BERT (see \u00a0<ref>) for Neurallog and Log2Vec\u00a0<cit.> for DeepLog.\n\nFinally, <cit.> (2022) conducted a comprehensive evaluation of several DL models including LSTM-based models such as DeepLog and LogAnomaly, GRU-based model PLELog, BiLSTM-based model LogRobust, and CNN. The study focused on various aspects including data selection, data partitioning, class distribution, data noise, and early detection ability.\n\n\n\n  \nDatasets. Due to security concerns, in many of the works in the literature, the data sources are unavailable such as Clay-HPC (Clay high-performance computing (HPC) systems). Studies that used available datasets are limited to the following: Hadoop Distributed File System (HDFS) collected in 2009, and three HPC datasets, BGL, Spirit, and Thunderbird, collected between 2004 and 2006. Besides, there is the OpenStack dataset (2017) created by injecting a limited number of anomalies at different execution points which not only does limit the diversity of anomalies, it may not accurately reflect real-world scenarios. \n\n\n\n  \nMotivating this work.\nAlthough the above studies used various DL models (13 models), none of them covers all the main DL types, RNN, CNN, and transformer. Moreover, the LSE strategies are rarely evaluated across all the DL networks, keeping them inside which original model they were proposed. \nFurthermore, the investigation of various combinations of LSE strategies and DL types has been limited in existing studies; many popular models tend to adopt a fixed combination of these two elements.\n\nFrom table\u00a0<ref> we can also see that recent studies are incomplete evaluations of DL models with a limited number of datasets that are not recent. \n\nThese remarks highlight the need for a systematic and comprehensive study that 1) explores various DL types as well as their combinations with different LSE strategies; and 2) examines how various dataset characteristics affect performance. \n\n\n \n\n\u00a7 FAILURE PREDICTION ARCHITECTURE\n\n\n\n\nTo systematically evaluate various deep learning models for failure prediction, we rely on a generic architecture that can use different modules with respect to the embedding strategy and the DL encoder.\nThis modular architecture will allow us to easily change individual modules corresponding to the various DL techniques and log sequence embedding strategies.\n\n\n\nFigure\u00a0<ref> depicts the modular architecture. \nThe architecture consists of two main steps, embedding and classification, designed to adopt different embedding as well as DL techniques, respectively.\nWe note that preprocessing is not required in this architecture since log sequences are based on log templates which are already preprocessed from log messages. \n\nIn the embedding step, log sequences are given as input, and each log sequence is in the form  (x_1,x_2,...,x_i,..,x_n), where x_i is a log template ID and n is the length of the log sequence. \nAn embedding technique (e.g., BERT) converts each x_i to a \u03b8-dimensional vector representing the semantics of x_i, where \u03b8 is the size of log sequence embedding.\nThen each log sequence forms a matrix X \u2208\ud835\udc45^n\u00d7\u03b8. Different log sequence embedding strategies can be applied; more information is provided in \u00a0<ref>. \n\nIn the classification step, the embedding matrix is processed to predict whether the given log sequence is failure or not. \nA DL model, as an encoder \u03a6 encodes the matrix X into a feature vector z = \u03a6(X) \u2208\ud835\udc45^m, where m is the number of features, which is a variable depending on the architecture of \u03a6. \nDifferent DL encoders can be applied; more information is provided in \u00a0<ref>. \nSimilar to related studies\u00a0<cit.>, the output feature vector z is then fed to a feed-forward network (FFN) and softmax classifier to create a vector of size d (d=2), indicating the prediction of the input unit label. \n\nMore specifically, the FFN activation function is rectified linear unit (ReLu), and the output vector of the FNN r is defined as r = max(0, z W_1+b_1) where W_1 \u2208\ud835\udc45^m\u00d7 d_f and b_1 \u2208\ud835\udc45^d_f are a trainable parameter, and d_f is the dimensionality of FNN. \nFurther, the calculation of the softmax classifier is as follows.\n\n    o = rW_2+b_2\n    \ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65(o_p) = exp(o_p)/\u2211_jexp(o_j)\n\nwhere W_2\u2208\ud835\udc45^d_f \u00d7 d and b_2 \u2208\ud835\udc45^d are trainable parameters to convert r to t \u2208\ud835\udc45^d before applying softmax; o_p represents the p-th component in the o vector, and exp is the exponential function. After obtaining the softmax values, the position with the highest value determines the label of the input log sequence.\n\nOverall, the combination of an embedding strategy and a DL encoder forms a language model that takes textual data as input and transforms it into a probability distribution\u00a0<cit.>. This language model handles the log templates as well as learning the language of failure patterns to predict the label of sequences. \n\nTo train the above architecture, a number of hyper-parameters should be set such as the choice of the optimizer, loss function, learning rate, input size (for some deep learning models), batch size, and the number of epochs. Tuning these hyper-parameters is highly suggested as it increases the chances of achieving the best failure prediction accuracy. Section\u00a0<ref> will detail the training and hyper-parameter tuning in our experiments.\n\nAfter the model is trained, it is evaluated with a test log split from the dataset with stratified sampling. We used stratified sampling to keep the same distribution of failure log sequences as the original dataset. Similar to training data, the embedding step transforms the test log sequences into embedding matrices. The matrices are then fed to the trained DL encoder to predict whether log sequences are normal or not.\n\n\n\n\n \u00a7.\u00a7 Embedding Strategies.\n\nWhile the modular architecture can accommodate various log sequence embedding options, we only selected two in order to limit the number of possible models resulting from the architecture, given our experimental constraints[More details are provided in \u00a0<ref>]. Our selection criteria aimed to include both trainable and pretrained options, as well as an advanced strategy based on transformers. Hence, given the description provided in \u00a0<ref>, the \u201cEmbedding step\u201d of our base architecture is instantiated using two strategies: BERT or Logkey2vec. The BERT sequence embedding technique is pretrained and was recently adopted for log sequences\u00a0<cit.> while Logkey2vec is trainable and was proposed earlier\u00a0<cit.>. Note that these two techniques were not compared in the same study before, according to Table\u00a0<ref>. \n\n\n\n  \nBERT.\nThe maximum number of input tokens for BERT (see \u00a0<ref>) is 512 tokens. This limit does not constitute a problem in this work since the log templates in our datasets are relatively short and the total number of tokens in each log template is always less than 512. Even if log templates were longer than 512, there are related studies suggesting approaches to use BERT accordingly\u00a0<cit.>.\nEach layer of the transformer encoder contains multi-head attention sub-layers and FFNs to compute a context-aware embedding vector (\u03b8=768) for each token.\nThis process is repeated for all the log templates inside the log sequence to create a matrix representation of size n \u00d7 768, where n is the length of the input log sequence.\n\n\n\n  \nLogkey2vec.\nFor Logkey2vec (see \u00a0<ref>), we set the embedding size to 768, similar to BERT for better comparison. The vocabulary size is a parameter of Logkey2vec that is going to be set during the experiments. \n\n\n\n\n \u00a7.\u00a7 Deep Learning Encoder\n\nIn this section, we illustrate the main features of the four DL encoders that can be used in the \u201cClassification step\u201d when instantiating our base architecture. We selected four encoders (LSTM-, BiLSTM-, CNN-, and transformer-based) because they cover the main DL types in addition to the recently emerged mechanism of attention, as mentioned in \u00a0<ref>. \n\n\n\n  \nLSTM-based.\nThis DL model is inspired by the LSTM architecture suggested by related works, including DeepLog\u00a0<cit.>, Aarohi\u00a0<cit.>, and Dash\u00a0<cit.>. The model has one hidden layer of LSTM with 128 nodes and ReLu activation. A Dropout with a rate of 0.1 is applied to make the model generalise better. The output of the model is a feature vector of size 128.\n\n\n\n  \nBiLSTM-based.\nThe model has an architecture similar to LogRobust, which was proposed for anomaly detection. Due to its RNN-based architecture, its output is a feature vector with the same size as the input log sequence length\u00a0<cit.>. \n\n\n\n  \nCNN-based.\nThe CNN architecture is a variation of the convolutional design for the CNN-based anomaly detection mode\u00a0<cit.>. Based on our preliminary experimental results, 20 filters, instead of one, for each of the three 1D convolutions (see \u00a0<ref>) are used in parallel to capture relationships between log templates at different distances. To ensure that feature maps of each convolution have the same dimension as the input, the padding technique is used. Hence, the length of the output feature vector is the product of the number of filters (20), the number of convolutions (3), and the input size of the log sequence.\n\n\n\n\n\n  \nTransformer-based.\nOur architecture of the transformer model is inspired by recent work in anomaly detection\u00a0<cit.>. The model is composed of two main parts: positional embedding and transformer blocks. One transformer block is adopted after positional embedding, set similarly to a recent study\u00a0<cit.>. After global average pooling, the output matrix is mapped into one feature vector the same size as the log template embedding \u03b8 = 768, previously explained in \u00a0<ref>.\n\n \n\n\u00a7 EMPIRICAL STUDY DESIGN\n\n\n\n\n \u00a7.\u00a7 Research Questions\n\nThe goal of this study is to systematically evaluate the performance of failure predictors, by instantiating our base architecture with different combinations of DL encoders and log sequence embedding strategies, for various datasets with different characteristics. The ultimate goal is to rely on such analyses to introduce practical guidelines to select the right failure prediction model based on the characteristics of a given dataset. \nTo achieve this, we investigate the following research questions:\n\n\n  * What is the impact of different DL encoders on failure prediction accuracy?\n\n  * What is the impact of different log sequence embedding strategies on failure prediction accuracy?\n\n  * What is the impact of different dataset characteristics on failure prediction accuracy? \n\n\n\nRQ1 and RQ2 investigate how failure prediction accuracy varies across DL encoders and embedding strategies reported in the literature. Most of them have been evaluated in isolation or with respect to a few alternatives, often using ad-hoc benchmarks (see \u00a0<ref> for a detailed comparison). \nTo address this, we comprehensively consider all variations of our base architecture, obtained by combining  four DN encoders (LSTM, CNN, transformer, and BiLSTM) with two log sequence embedding strategies (Logkey2vec and BERT) that have been widely used in failure prediction and anomaly detection. \nFurthermore, we systematically vary the characteristics of the input datasets in terms of the number of log sequences, the length of log sequences, and the proportion of normal log sequences.  \nThe answer to these questions is expected to lead to practical guidelines for choosing the best failure prediction model given a dataset with certain characteristics. \n\nRQ3 additionally investigates the impact of the input dataset characteristics on failure prediction accuracy with a focus on the best DL encoder and log sequence embedding strategy found in RQ1 and RQ2.\nThe answer to this question will help us better understand under which conditions the combination of the best DL encoder and log sequence embedding strategy works sufficiently well for practical use, possibly leading to practical guidelines to best prepare input datasets for increasing failure prediction accuracy.\n\n\n\n \u00a7.\u00a7 Methodology\n\n\nAs discussed in \u00a0<ref>, we can instantiate the base architecture for failure prediction with different DL encoders and log sequence embedding strategies. \n\nTo answer RQ1 and RQ2, we train different configurations of the base architecture while systematically varying training datasets' characteristics (e.g., size and failure types). Then, we evaluate the relative performance of the configurations in terms of failure prediction accuracy, using test datasets having the same characteristics but not used during training.\n\nTo answer RQ3, we analyze the results of the best configuration (i.e., the best DL encoder and log sequence embedding strategy) identified in RQ1 and RQ2. Specifically, we build regression trees\u00a0<cit.> to automatically infer conditions describing how the failure prediction accuracy of the best configuration varies according to the dataset characteristics. \n\n\n\n\n  \u00a7.\u00a7.\u00a7 Log Sequence Embedding Strategies and DL Encoders\n\nAs for different log sequence embedding strategies, we consider Logkey2vec and BERT, which have shown to be accurate in the literature as discussed in \u00a0<ref>.\nFor BERT, we use the original pretrained model with 512 input tokens, where each token is represented as a 768-dimensional embedding vector. \nFor Logkey2vec, we set the size of an embedding vector to be the same as BERT for a fair comparison. \nAlso, Logkey2vec has an additional parameter: vocabulary size. We set it to 200, which is large enough for all datasets used in our evaluation.\n\n\nAs for different DL encoders in RQ1 and RQ2, we consider four encoders that have been previously used in failure prediction and anomaly detection. \nWe configured the encoders based on the recommendations reported in the literature (see \u00a0<ref> for further details). \n\n\n\n\n  \u00a7.\u00a7.\u00a7 Datasets with Different Characteristics\n\nAs for the characteristics of datasets, we consider four factors that are expected to affect failure prediction performance: \n(1) dataset size (i.e., the number of logs in the dataset), \n(2) log sequence length (LSL) (i.e., the length of a log sequence in the dataset), \n(3) failure percentage (i.e., the percentage of log sequences with failure patterns in the dataset), and\n(4) failure pattern type (i.e., types of failures).\n\nThe dataset size is important to investigate to assess the training efficiency of different DL models. To consider a wide range of dataset sizes while keeping the number of all combinations of the four factors tractable, we consider six levels that cover the range of real-world dataset sizes reported in a recent study\u00a0<cit.>: 200, 500, 1000, 5000, 10000, and 50000. \n\nThe LSL could affect failure prediction since a failure pattern that spans a longer log might be more difficult to predict correctly. Similar to observed lengths in real-world log sequences across publicly available datasets\u00a0<cit.>, we vary the maximum[We set the maximum LSL for to simplify control.] LSL across five levels: 20, 50, 100, 500, and 1000.\n\nThe failure percentage determines the balance of classes in a dataset, which may affect the performance of DL models\u00a0<cit.>. The training dataset is perfectly balanced at 50%. However, the failure percentage can be much less than 50% in practice, as observed in real-world datasets\u00a0<cit.>. Therefore, we vary the failure percentage across six levels: 5%, 10%, 20%, 30%, 40%, and 50%. \n\nRegarding failure patterns, we aim to consider patterns with potential differences in terms of learning effectiveness. However, failure patterns defined in previous studies are too simple; for example, <cit.> consider a specific, consecutive sequence of problematic log templates, called a \u201cfailure chain\u201d. But in practice, not all problematic log templates appear consecutively in a log. To address this, we use regular expressions to define failure patterns, allowing non-consecutive occurrences of problematic log templates. For example, a failure pattern \u201cx(y|z)\u201d indicates a pattern composed of two consecutive templates that starts with template x and ends with either template y or template z. \nIn addition, we consider two types of failure patterns (in the form of regular expressions), Type-F and Type-I, depending on the cardinality of languages accepted by the regular expressions (finite and infinite, respectively). This is because, if the cardinality of the language is finite, DL models might memorise (almost) all the finite instances (i.e., sequences of log templates) instead of learning the failure pattern. For example, the language defined by the regular expression \u201cx(y|z)\u201d is finite since there are only two template sequences (i.e., xy and xz) matching the expression \u201cx(y|z). In this case, the two template sequences might appear in the training set, making it straightforward for DL models to simply memorise them. On the contrary, the language defined by the regular expression \u201cx^*(y|z)\u201d is infinite due to infinite template sequences that can match the sub-expression `x^*'; therefore simply memorising some of the infinitely many sequences matching  \u201cx^*(y|z)\u201d would not be enough to achieve high failure prediction accuracy.\n\nTo sum up, we consider 360 combinations (six dataset sizes, five maximum LSLs, six failure percentages, and two failure pattern types) in our evaluation. \nHowever, we could not use publicly available datasets for our experiments due to the following reasons.\nFirst, although <cit.> reported several datasets in their survey paper, they are mostly labeled based on the occurrence of error messages (e.g., log messages with the level of ERROR) instead of considering failure patterns (e.g., sequences of certain messages). \nFurthermore, there are no publicly available datasets covering all the combinations of the four factors defined above, making it impossible to thoroughly investigate their impact on failure prediction.\nTo address this issue, we present a novel approach for synthetic log data generation in \u00a0<ref>.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Failure Predictor Training and Testing\n\nWe split each artificially-generated dataset into two disjoint sets, a training set and a test set, with a ratio of 80:20. Further, 20% of the training set is  separated as a validation set, which is used for early stopping\u00a0<cit.> during training to avoid over-fitting. \n\nFor training failure predictors, to control the effect of highly imbalanced datasets,  oversampling\u00a0<cit.> is performed on the minority class (i.e., failure logs) to achieve a 50:50 ratio of normal to failure logs in the training dataset.\nFor all the training datasets, we use the Adam optimizer\u00a0<cit.> with a learning rate of 0.001 and the sparse categorical cross-entropy loss function\u00a0<cit.> considering the Boolean output (i.e., failure or not) of the models.\nHowever, we use different batch sizes and numbers of epochs for datasets with different characteristics since they affect the convergence speed of the training error (particularly the dataset size, the maximum LSL, and the failure percentage). It would however be impractical to fine-tune the batch size and the number of epochs for 360 individual combinations. Therefore, based on our preliminary evaluation results, we use larger batch sizes with fewer epochs for larger datasets to keep the training time reasonable without significantly affecting training effectiveness. \nSpecifically, we set the two hyperparameters as follows:\n\n    \n  * Batch size: By default, we set it to 10, 15, 20, 30, 150, and 300 for dataset sizes of 200, 500, 1000, 5000, 10000, and 50000, respectively. If the failure percentage is less than or equal to 30 (meaning more oversampling will happen to balance between normal and failure logs, increasing the training data size), then we increase the batch size to 10, 15, 30, 60, 300, and 600, respectively, to reduce training time. Furthermore, regardless of the failure percentage, we set the batch size to 5 if the maximum LSL is greater than or equal to 500 to prevent memory issues during training.\n    \n  * Number of epochs: By default, we set it to 20. If the maximum LSL is greater than or equal to 500, we reduce the number of epochs to 10, 10, 5, and 5 for dataset sizes of 1000, 5000, 10000, and 50000, respectively, to reduce training time.\n\n\n\n\nTable\u00a0<ref> summarises the above conditions, where FP is the failure percentage and MLSL refers to the maximum LSL. \n \n\nOnce failure predictors are trained, we measure their accuracy on the corresponding test set in terms of precision, recall, and F1 score.\n\nWe conducted all experiments with cloud computing environments provided by the Digital Research Alliance of Canada\u00a0<cit.>, on the Cedar cluster with a total of 94528 CPU cores for computation and 1352 GPU devices.\n\n\n\n\n \u00a7.\u00a7 Synthetic Data Generation\n\n\nIn defining a set of factors, the methodology described in \u00a0<ref> makes it clear that there is a need for a mechanism that can generate datasets in a controlled, unbiased manner. For example, let us consider the factor of failure percentage (\u00a0<ref>). Such a factor requires that one be able to control whether the log sequence being generated does indeed correspond to a failure; this would ultimately allow one to control the percentage of failure log sequences in a generated dataset.\n\nWhile, for smaller datasets, one could imagine manually choosing log sequences that represent both failures and normal behaviour, for larger datasets this is not feasible. When considering the other factors defined in \u00a0<ref>, such as LSL, the case for a mechanism for automated, controlled generation of datasets becomes yet stronger.\n\n\n\n  \u00a7.\u00a7.\u00a7 Key Requirements\n\n\nWe now describe a set of requirements that must be met by whatever approach we opt to take for generating datasets. In particular, our approach should:\n\nreq\n\n\nreq:controlledAllow datasets' characteristics to be controlled.  This requirement has already been described, but we summarise it here for completeness. We must be able to generate datasets for each combination of levels (of the factors defined in \u00a0<ref>). Hence, our approach must allow us to choose a combination of levels, and generate a dataset accordingly.\n\nreq:realisticBe able to generate realistic datasets. A goal of this work is to present results that are applicable to real-world systems. Hence, we must require that the datasets with which we perform any evaluations reflect real-world system behaviours.\n\nreq:diverseBe able to generate datasets corresponding to a diverse set of systems.  While we require that the datasets that we use be realistic, we must also ensure that the data generator can be applied to any system for which we have this automaton, rather than being limited to a single system.\n\nreq:unbiasedAvoid bias in the log sequences that make up the generated datasets.  The previous requirement ensures that we do not introduce an approach that only works with one system. If our approach were to work solely with one system, we might say that it was biased at the use case level. The second kind of bias is at the log sequence-level. For example, for a given system, we wish to generate datasets containing log sequences that explore as much of the system's behaviour as possible (rather than being biased to a particular part of the system).\n\n\n\n  \u00a7.\u00a7.\u00a7 Automata for System Behaviour\n\n\nOur approach is based on finite-state automata. In particular, we use automata as approximate models of the behaviour of real-world systems. We refer to such automata as behaviour models, since they represent the computation performed by (i.e., behaviour of) some real-world system. We chose automata, or behaviour models, because some of our requirements are met immediately:\n\n\n\n  \nR<ref>.  Existing tools\u00a0<cit.> allow one to infer behaviour models of real-world systems from collections of these systems' logs (in a process called model inference). Such models attach log messages to transitions, which is precisely what we need. Importantly, collections of logs used are unlabelled, meaning that the models that we get from these tools have no existing notion of normal behaviour or failures.\n\n\n\n  \nR<ref>.  A result of meeting R<ref> is that one can easily infer behaviour models for multiple systems, provided the logs of those systems are accessible.\n\n\n\nThe remaining sections will give the complete details of our automata-based data generation approach. In presenting these details, we will show how R<ref> and R<ref> are met.\n\n\n\n  \u00a7.\u00a7.\u00a7 Behaviour Models\n\nWe take a behaviour model \u2133 to be a deterministic finite-state automaton \u27e8 Q, A, q_0, \u03a3, \u03b4\u27e9, with symbols as defined in \u00a0<ref>.\n\nA behaviour model has the particular characteristic that its alphabet \u03a3 consists of log template IDs (see \u00a0<ref>). A direct consequence of this is that one can extract log sequences from behaviour models. In particular, if one considers a sequence of states (i.e., a path) q_0, q_i, q_i+1, \u2026, q_n through the model, one can extract a sequence of log template IDs using the transition function \u03b4. For example, if the first two states of the sequence are q_0 and q_i, then one need only find s \u2208\u03a3 such that \u03b4(q_0, s) = q_i, i.e., it is possible to transition to q_i from q_0 by observing s.\nFinally, by replacing each log template ID in the resulting sequence with its corresponding log template, one obtains a log sequence (see \u00a0<ref>). These sequences can be divided into two categories: failure log sequences and normal log sequences. \n\nWe describe failures using regular expressions. This is natural since behaviour models are finite state automata, and sets of paths through such automata can be described by regular expressions. Hence, we refer to such a regular expression as a failure pattern, and denote it by \ud835\udc53\ud835\udc5d. By extension, for a given behaviour model \u2133, we then denote by \ud835\uddbf\ud835\uddba\ud835\uddc2\ud835\uddc5\ud835\uddce\ud835\uddcb\ud835\uddbe\ud835\uddaf\ud835\uddba\ud835\uddcd\ud835\uddcd\ud835\uddbe\ud835\uddcb\ud835\uddc7\ud835\uddcc(\u2133) the set {\ud835\udc53\ud835\udc5d_1, \ud835\udc53\ud835\udc5d_2, \u2026, \ud835\udc53\ud835\udc5d_\ud835\udc5b} of failure patterns paired with the model \u2133. Based on this, we characterise failure log sequences as such:\n\n\n\n  \nFailure log sequence. For a system whose behaviour is represented by a behaviour model \u2133, we say that a log sequence represents a failure of the system whenever its sequence of log template IDs matches some failure pattern \ud835\udc53\ud835\udc5d\u2208\ud835\uddbf\ud835\uddba\ud835\uddc2\ud835\uddc5\ud835\uddce\ud835\uddcb\ud835\uddbe\ud835\uddaf\ud835\uddba\ud835\uddcd\ud835\uddcd\ud835\uddbe\ud835\uddcb\ud835\uddc7\ud835\uddcc(\u2133).\n\n\n\nSince this definition of failure log sequences essentially captures a subset of the possible paths through \u2133, we define normal log sequences as those log sequences that are not failures:\n\n\n\n  \nNormal log sequence. For a system whose behaviour is represented by a behaviour model \u2133, we say that a log sequence l is normal, i.e., it represents normal behaviour, whenever l \u2208\u2112(\u2133) and l \u2209\u22c3_\ud835\udc53\ud835\udc5d\u2208\ud835\uddbf\ud835\uddba\ud835\uddc2\ud835\uddc5\ud835\uddce\ud835\uddcb\ud835\uddbe\ud835\uddaf\ud835\uddba\ud835\uddcd\ud835\uddcd\ud835\uddbe\ud835\uddcb\ud835\uddc7\ud835\uddcc(\u2133)\u2112(\ud835\udc53\ud835\udc5d) (we take \u2112(\u2133) and \u2112(\ud835\udc53\ud835\udc5d) to be as defined in \u00a0<ref>). Hence, defining a normal log sequence requires that we refer to both the language of the model \u2133, and the languages of all failure patterns associated with the model \u2133.\n\n\n\n  \u00a7.\u00a7.\u00a7 Generating Log Sequences for Failures\n\n\nLet us suppose that we have inferred a model \u2133 from the execution logs of some real-world system, and that we have defined the set \ud835\uddbf\ud835\uddba\ud835\uddc2\ud835\uddc5\ud835\uddce\ud835\uddcb\ud835\uddbe\ud835\uddaf\ud835\uddba\ud835\uddcd\ud835\uddcd\ud835\uddbe\ud835\uddcb\ud835\uddc7\ud835\uddcc(\u2133). Then we generate a failure log sequence that matches some \ud835\udc53\ud835\udc5d\u2208\ud835\uddbf\ud835\uddba\ud835\uddc2\ud835\uddc5\ud835\uddce\ud835\uddcb\ud835\uddbe\ud835\uddaf\ud835\uddba\ud835\uddcd\ud835\uddcd\ud835\uddbe\ud835\uddcb\ud835\uddc7\ud835\uddcc(\u2133) by:\n\n\n  * Computing a subset of \u2112(\ud835\udc53\ud835\udc5d). We do this by repeatedly generating single members of \u2112(\ud835\udc53\ud835\udc5d). Ultimately, this leads to the construction of a subset of \u2112(\ud835\udc53\ud835\udc5d). In practice, the Python package exrex<cit.> can be used to generate random words from the language \u2112(\ud835\udc53\ud835\udc5d), so we invoke this library repeatedly.\n    \n    If the language of the regular expression is infinite, we can run exrex multiple times, each time generating a random string from the language. The number of runs is set based on our preliminary results with respect to the range of dataset size (2500 times for each failure pattern). Doing this, we generate a subset of \u2112(\ud835\udc53\ud835\udc5d).\n\n    \n  * Choosing at random a log sequence l from the random subset \u2112(\ud835\udc53\ud835\udc5d) computed in the previous step, with |l| \u2264\ud835\udc5a\ud835\udc59\ud835\udc60\ud835\udc59 where mlsl refers to the value of maximum log sequence manipulated by LSL factor. (see \u00a0<ref>) (maximum LSL, see \u00a0<ref>). The Python package random<cit.> was employed for this.\n\n    We highlight that failure patterns are designed so that there is always at least one failure pattern that can generate log sequences whose length falls within this bound.\n\n    More details on this are provided in \u00a0<ref>.\n\n\n\nFor requirement R<ref>, since our approach relies on random selection of log sequences from languages generated by the exrex tool, we highlight that the bias in our approach is subject to the implementation of both exrex, and the Python package, random. Exrex is a popular package for RegEx that has more than 100k monthly downloads. Its method for generating a random matching sequence is implemented by a random selection of choices on the RegEx's parse tree nodes.\nRandom package uses the Mersenne twister algorithm\u00a0<cit.> to generate a uniform pseudo-random number used for random selection tasks. \n\n\n\n  \u00a7.\u00a7.\u00a7 Generating Log Sequences for Normal Behaviour\n\n\nWhile our approach defines how failures should look using a set of failure patterns \ud835\uddbf\ud835\uddba\ud835\uddc2\ud835\uddc5\ud835\uddce\ud835\uddcb\ud835\uddbe\ud835\uddaf\ud835\uddba\ud835\uddcd\ud835\uddcd\ud835\uddbe\ud835\uddcb\ud835\uddc7\ud835\uddcc(\u2133) defined over a model \u2133, we have no such definition of how normal behaviour should look. Instead, this is left implicit in our behaviour model. However, based on the definition of normal log sequences given in \u00a0<ref>, such log sequences can be randomly generated by performing random walks on behaviour models.\n\nThis fact forms the basis of our approach to generating log sequences for normal behaviour. However, we must also address key issues:  1) the log sequences generated by our random walk must be of bounded length, and 2) the log sequences must also lack bias.\n\nThere are two reasons for enforcing a bound on the length of log sequences:\n\n    \n  * Deep learning models (such as CNN) often accept inputs of limited size, so we have to ensure that the data we generate is compatible with the models we use.\n    \n  * One of the factors introduced in \u00a0<ref> is LSL, so we need to be able to control the length of log sequences that we generate.\n\n\nFor bias, we have two sources: 1) bias to specific regions of the behavioral model, 2) bias to limited variation of LSL. We must minimise bias in both cases.\n\nAlgorithm\u00a0<ref> gives our procedure for randomly generating a log sequence representing normal behaviour of a system. Algorithm\u00a0<ref> itself makes use of Algorithm\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n\n\nIn particular, Algorithm\u00a0<ref> generates a normal log sequence by:\n\n\n  * Generating a random log sequence by random walk (invoking Algorithm\u00a0<ref>);\n\n  * Looking for a failure pattern \ud835\udc53\ud835\udc5d\u2208\ud835\uddbf\ud835\uddba\ud835\uddc2\ud835\uddc5\ud835\uddce\ud835\uddcb\ud835\uddbe\ud835\uddaf\ud835\uddba\ud835\uddcd\ud835\uddcd\ud835\uddbe\ud835\uddcb\ud835\uddc7\ud835\uddcc(\u2133) that matches the generated log sequence;\n\n  * Repeating until a log sequence is generated that matches no failure pattern.\n\nUltimately, Algorithm\u00a0<ref> is relatively lightweight; the weight of the work is performed by Algorithm\u00a0<ref>, which we now describe in detail.\n\nThe input arguments of Algorithm\u00a0<ref>, which defines the procedure filteredRandomWalk, are a behaviour model \u2133 and the maximum LSL, \ud835\udc5a\ud835\udc59\ud835\udc60\ud835\udc59.\n\nThe algorithm proceeds as follows. First, on line 1, we invoke the calculateSValues function to compute a map that sends each state s \u2208 Q of \u2133 to the length of the shortest path from that state to an accepting state in A. Next, on line 2, the sequence variable is initialised to an empty sequence. As the algorithm progresses, this variable stores the generated sequence of log template IDs. To help with this, the variable currentState is initialised to keep track of the state that the algorithm is currently in during the walk of the behavioral model. Hence, this variable is initialised on line 4 as the initial state. The final step in the setup stage of our algorithm is to initialise the maximumWalk variable, which serves as a counter to ensure the limit on the length of the generated log sequence (defined by \ud835\udc5a\ud835\udc59\ud835\udc60\ud835\udc59) is respected. \n\nIn the while loop (line 5), as long as the current state, \ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc46\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc52, is not yet an accepting state, the random walk transits from the current state to a new state. The set of possible transitions to take is computed by line 7, and stored in the variable \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60. Each transition is represented by a triple containing the starting state, the symbol to be observed, and the state resulting from observation of that symbol. Once this set has been computed, the algorithm performs a filtering step. In particular, in order to ensure that we respect the limit imposed on the length of the generated path by \ud835\udc5a\ud835\udc59\ud835\udc60\ud835\udc59, we only consider transitions that lead to a state q' such that \ud835\udc60\ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52(q') < \ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc56\ud835\udc5a\ud835\udc62\ud835\udc5a\ud835\udc4a\ud835\udc4e\ud835\udc59\ud835\udc58. The resulting list of valid options is then held in the variable \ud835\udc5c\ud835\udc5d\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60.\n\nOnce the set \ud835\udc5c\ud835\udc5d\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60 has been computed, one transition \u27e8 q, s, q' \u27e9 will be chosen randomly from the set (line 13). This random choice eliminates bias because, each time we choose the next state to transition to, we do not favour any particular state (there is no weighting involved). This, extended over an entire path, means that we do not favour any particular region of a behaviour model. Now, from the randomly chosen transition \u27e8 q, s, q' \u27e9, the log template ID, s, is added to sequence (via sequence concatenation); currentState is updated to the next state, q'; and maximumWalk is decreased by one. Based on the condition of the while loop (line 5), when \ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc46\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc52\u2208 A (i.e., the algorithm has reached an accepting state), the generated sequence \ud835\udc60\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc52 is returned.\n\nWhile Algorithm\u00a0<ref> generates an unlabelled log sequence, Algorithm\u00a0<ref> generates a normal log sequence. To do this, it starts by generating a log sequence, by invoking the filteredRandomWalk procedure (Algorithm\u00a0<ref>). Since the sequence generated by Algorithm\u00a0<ref> is unlabelled, we must ensure that we do not generate a failure log sequence. We do this by checking whether the generated log sequence, \ud835\udc60\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc52, belongs to the language of any failure pattern in \ud835\uddbf\ud835\uddba\ud835\uddc2\ud835\uddc5\ud835\uddce\ud835\uddcb\ud835\uddbe\ud835\uddaf\ud835\uddba\ud835\uddcd\ud835\uddcd\ud835\uddbe\ud835\uddcb\ud835\uddc7\ud835\uddcc(\u2133). If this is indeed the case, another sequence must be generated. This process is repeated (line 2) until the log sequence generated by the call of \ud835\udc53\ud835\udc56\ud835\udc59\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc51\ud835\udc45\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc5c\ud835\udc5a\ud835\udc4a\ud835\udc4e\ud835\udc59\ud835\udc58 does not match any failure pattern in \ud835\uddbf\ud835\uddba\ud835\uddc2\ud835\uddc5\ud835\uddce\ud835\uddcb\ud835\uddbe\ud835\uddaf\ud835\uddba\ud835\uddcd\ud835\uddcd\ud835\uddbe\ud835\uddcb\ud835\uddc7\ud835\uddcc(\u2133). Once a failure log sequence has been generated, it is returned.\n\nWe acknowledge that this process could be inefficient (since we are repeatedly generating log sequences until we get one with the characteristics that we need). However, we highlight that failure patterns describe only a small part of a behaviour model (this is essentially the assumption that failure is a relatively uncommon event in a real system). Hence, normal log sequences generated by random walks can be generated without too many repetitions.\n\n\n\n  \nCorrectness and lack of bias. We now provide a sketch proof of the correctness of Algorithm\u00a0<ref>, along with an argument that the algorithm eliminates bias.\n\nTo prove correctness, we show that, for a behaviour model \u2133, the algorithm always generates a sequence of log template IDs that correspond to the transitions along a path through \u2133.\n\nThe algorithm begins at q_0, by setting \ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc46\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc52 to q_0 (line 4). From q_0, and each successive state in the path, the possible next states must be adjacent to \ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc46\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc52 (line 7). Hence, the final value of \ud835\udc60\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc52 after the while-loop at line 5 must be a sequence of log template IDs that correspond to the transitions along a path through \u2133.\n\nFurther, we must show that the sequence of log template IDs generated does not only correspond to a path through the behaviour model, but is of length at most \ud835\udc5a\ud835\udc59\ud835\udc60\ud835\udc59 (one of the inputs of Algorithms\u00a0<ref> and\u00a0<ref>). This is ensured by three factors:\n\n\n  * The initialisation of the variable \ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc56\ud835\udc5a\ud835\udc62\ud835\udc5a\ud835\udc4a\ud835\udc4e\ud835\udc59\ud835\udc58 on line 3.\n\n  * The subsequent decrease by one of that variable each time a new log template ID is added to \ud835\udc60\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc52.\n\n  * Filtering of the possible next states in the random walk on line 9. In particular, on line 9 we ensure that, no matter which state we transition to, there will be a path that 1) leads to an accepting state; and 2) has length less than \ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc56\ud835\udc5a\ud835\udc62\ud835\udc5a\ud835\udc4a\ud835\udc4e\ud835\udc59\ud835\udc58.\n\nFinally, bias is minimised by two factors:\n\n    \n  * On line 13, we choose a random next state. Of course, here we rely on the implementation of random choice that we use.\n    \n  * On line 9, while we respect the maximum length of the sequence of log template IDs, we do not enforce that we reach this maximum. Hence, we can generate paths of various lengths.\n\n\n\n\n  \nExample. To demonstrate Algorithm\u00a0<ref>, we now perform a random walk over the behaviour model shown in Figure\u00a0<ref>. We start with the behaviour model's starting state, q_0, with \ud835\udc5a\ud835\udc59\ud835\udc60\ud835\udc59 set to 5. Since q_0 \u2209A, we can execute the body of the while-loop at line 5. Hence, we can determine the set \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60 of transitions leading out of q_0:\n\n    {\u27e8 q_0,a,q_2\u27e9, \u27e8 q_0,b,q_2\u27e9, \u27e8 q_0,c, q_1\u27e9, \u27e8 q_0,d,q_1\u27e9}.\n\nOur next step is to filter these transitions to ensure that the state that we move to allows us to reach an accepting state within \ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc56\ud835\udc5a\ud835\udc62\ud835\udc5a\ud835\udc4a\ud835\udc4e\ud835\udc59\ud835\udc58 states. To do this, we filter the set \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60 with respect to the values in Table\u00a0<ref>. After this filtering step, the resulting set, \ud835\udc5c\ud835\udc5d\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60, is\n\n    {\u27e8 q_0,a,q_2\u27e9, \u27e8 q_0,b,q_2\u27e9, \u27e8 q_0,c, q_1\u27e9, \u27e8 q_0,d,q_1\u27e9}.\n\nAll states in \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60 are safe to transition to. To take one transition as an example, \u27e8 q_0, a, q_2\u27e9 has \ud835\udc60\ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52(q_2) = 1 < 5, so is kept.\n\nOnce we have computed \ud835\udc5c\ud835\udc5d\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60, we choose a transition at random. In this case, we arrive at \u27e8 q_0,c,q_1\u27e9, meaning that we set \ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc46\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc52 to q_1. Before we progress to the next iteration of the main loop of the algorithm, we also decrease \ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc56\ud835\udc5a\ud835\udc62\ud835\udc5a\ud835\udc4a\ud835\udc4e\ud835\udc59\ud835\udc58. This means that, during the next iteration of the while loop, we will be able to choose transitions leading to states from which an accepting state must be reachable within less than 4 states.\n\nIndeed, from q_1, there are four transitions, for which we compute the set\n\n    {\u27e8 q_1,a,q_0\u27e9, \u27e8 q_1,b,q_1\u27e9, \u27e8 q_1,c, q_3\u27e9, \u27e8 q_1,d,q_3\u27e9}.\n\nFrom this set, each possible next state has \ud835\udc60\ud835\udc49\ud835\udc4e\ud835\udc59\ud835\udc62\ud835\udc52 greater than \ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc56\ud835\udc5a\ud835\udc62\ud835\udc5a\ud835\udc4a\ud835\udc4e\ud835\udc59\ud835\udc58 (equal to 4), so all of them would be possible options for the next step. Suppose that we choose \u27e8 q_1,a,q_0\u27e9 at random. Hence, q_0 is the next state and a is added to the \ud835\udc60\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc52. For the remaining steps, a possible run of the procedure could yield the sequence of transitions \u27e8 q_0,b,q_2\u27e9, \u27e8 q_2,d,q_1\u27e9, \u27e8 q_1,d,q_3\u27e9, in which case the final sequence of log template IDs would be c, a, b, d, d.\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 A summary of requirements met\n\n\nWe now describe how the approach that we have described meets the remaining requirements set out in \u00a0<ref>.\n\nR<ref> is met because we have two procedures for generating failure log sequences (\u00a0<ref>) and normal log sequences (\u00a0<ref>). By having these procedures, we can precisely control the number of each type of log sequence in our dataset.\n\nR<ref> is met because of the randomisation used in our data generation algorithm, described in Sections\u00a0<ref> and\u00a0<ref>.\n\n\n\n \u00a7.\u00a7 Experimental Setting for Syntactic Data Generation\n\n\nTo generate diverse log datasets with the characteristics described in \u00a0<ref>, using the syntactic data generation approach described in \u00a0<ref>, we need two main artifacts: behavior models and failure patterns.\n\n\n\n  \u00a7.\u00a7.\u00a7 Behavior Models\n\nRegarding behavior models, as discussed in \u00a0<ref>, we can infer accurate models of real-world systems from their execution logs using state-of-the-art model inference tools, i.e., MINT\u00a0<cit.> and PRINS\u00a0<cit.>. \nAmong the potential models we could generate using the replication package of these tools, we choose models that satisfy the following criteria based on the model size and inference time reported by <cit.>:\n\n    \n  * The model should be able to generate (accept) a log with a maximum length of 20 (i.e., the shortest maximum LSL defined in \u00a0<ref>;\n    \n  * Since there is no straightforward way of automatically generating failure patterns for individual behavior models considering the two failure pattern types, we had to manually generate failure patterns (detailed in \u00a0<ref>). Therefore, the size of the model should be amenable to manually generating failure patterns by taking into account the model structure (i.e., the number of all states and transitions is less than 1000);\n    \n  * The model inference time should be less than 1 hour; and\n    \n  * If we can use both PRINS and MINT to infer a model that satisfies the above criteria for the same logs, then we use PRINS, which is much faster than MINT in general, to infer the model.\n\nAs a result, we use the following three models as our behavior models: \n\u2133_1 (generated from NGLClient logs using PRINS), \n\u2133_2 (generated from HDFS logs using MINT), and \n\u2133_3 (generated from Linux logs using MINT). \nTable\u00a0<ref> reports about the size of the three behavior models in terms of the number of templates (#Templates), states (#States), and transitions (#Transitions). \nIt additionally shows the number of states in the largest strongly connected component (#States-NSCC)\u00a0<cit.>, which indicates the complexity of a behavior model (the higher, the more complex). \n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Failure Patterns\n\nRegarding failure patterns, recall a failure pattern \ud835\udc53\ud835\udc5d of a behavior model \u2133 is a regular expression where \u2112(\ud835\udc53\ud835\udc5d) \u2282\u2112(\u2133), as described in \u00a0<ref>. \nAlso, note that we need two types of failure patterns (Type-F and Type-I), and the failure log sequences generated from the failure patterns must satisfy the dataset characteristics (especially the maximum LSL) defined in \u00a0<ref>. \nTo manually create such failure patterns (regular expressions) in an unbiased way, we used the following steps for each behavior model and failure pattern type:\n\n    \n  * We randomly choose the alphabet size of a regular expression and the number of operators (i.e., alternations and Kleene stars; the latter is not used for Type-F).\n    \n  * Using the chosen random values, for a given behavior model \u2133, we manually create a failure pattern (regular expression) \ud835\udc53\ud835\udc5d to satisfy \u2112(\ud835\udc53\ud835\udc5d) \u2282\u2112(\u2133) and the maximum LSL within the time limit of 1 hour; if we fail (e.g., if the shortest log in \u2112(\ud835\udc53\ud835\udc5d) is longer than the maximum LSL of 20), we go back and restart Step 1.\n    \n  * We repeat Steps 1 and 2 ten times to generate ten failure patterns and then randomly select three out of them.\n\nAs a result, we use 18 failure patterns (i.e., 3 failure patterns \u00d7 3 behavior models \u00d7 2 failure pattern types) for synthetic data generation. \n\n\u00a7 RESULTS\n\nThis section presents the results of RQ1 (DL encoders), RQ2 (log sequence embedding strategies), and RQ3 (dataset characteristics), respectively. \n\n\n\n \u00a7.\u00a7 RQ1: DL Encoders\n \n\n\n\n\u00a0<ref> shows boxplots of the failure prediction accuracy (F1 score) for different DL encoders (i.e., transformer-based, LSTM-based, CNN-based, and BiLSTM-based models) on the datasets generated by different behavior models (i.e., M1, M2, and M3). \nEach box is generated based on 360\u00d7 2 data points since we have 360 combinations of dataset characteristics and two log sequence embedding strategies. \nIn each box, a triangle indicates the mean value. \n\nOverall, the CNN-based model achieves the best performance in terms of F1 score for all behavior models. It has the highest mean values with the smallest interquartile ranges (IQRs), meaning that the CNN-based model consistently works very well regardless of dataset characteristics and log sequence embedding strategies.  \nThe BiLSTM-based model also shows promising results. However, the CNN-based model's results are significantly higher for all the behavioral models (paired t-test p-values << 0.001). In contrast, the LSTM-based and transformer-based models show poor results (low F1 score on average with very large IQRs).\nThese patterns are independent from both the embedding strategy and the model. Further, the large variance for LSTM-based and transformer-based models suggests that these models are very sensitive to the dataset characteristics. \n\n\nThe poor performance of the transformer-based encoder can be explained by the fact that the transformer blocks in the encoder are data-demanding (i.e., requiring much training data). When the dataset size is small (below 1000), the data-demanding transformer blocks are not well-trained, leading to poor performance. This limitation is thoroughly discussed in the literature\u00a0<cit.>.\n\nThe LSTM-based encoder, on the other hand, has two simple layers of LSTM units. Recall that an LSTM model sequentially processes a given log sequence (i.e., a sequence of templates), template by template. Although LSTM attempts to address the long-term dependency problem of RNN by having a forget gate (see \u00a0<ref>), it is still a recurrent network that has difficulties to remember a long input sequence\u00a0<cit.>. For this reason, since our log datasets contain long log sequences (up to a length of 1000), the LSTM-based encoder did not work well. \n\nThe BiLSTM-based encoder involves LSTM units and therefore has the weakness mentioned above. However, for BiLSTM, the input sequence flows in both directions in the network, utilizing information from both sides. Furthermore, it is enhanced by the attention mechanism that assigns more weight to parts of the input which are associated with the failure pattern\u00a0<cit.>.  Thus, the BiLSTM-based encoder can more easily learn the impact of different log templates on the classification results. However, the attention layer is more data-demanding than the convolution layers (see \u00a0<ref>) in the CNN-based encoder, and this explains why the BiLSTM-based encoder does not outperform the CNN-based encoder.\n\nThe high performance of the BiLSTM-based and CNN-based encoders can be explained by the number of trainable parameters; for these two encoders, unlike the transformer-based and LSTM-based ones, the number of trainable parameters increases as the input sequences get longer. The larger number of parameters makes the encoders more robust to longer input log sequences.\nFurthermore, CNN additionally processes spatial information (i.e., conjunctive relationships among templates) using multiple filters with different kernel sizes\u00a0<cit.>, which makes failure prediction more accurate even when the input size (sequence length) is large. \nThese characteristics make the CNN-based encoder the best choice in our application context.\n\n\nThe answer to RQ1 is that the CNN-based encoder significantly outperforms the other encoders regardless of dataset characteristics and log sequence embedding strategies.\n\n\n\n\n\n \u00a7.\u00a7 RQ2: Log Sequence Embedding Strategies\n \n\n\n\n\u00a0<ref> shows the boxplots of the failure prediction accuracy (F1 score) for the different log sequence embedding strategies considered in this study (i.e., BERT and Logkey2vec) on the datasets generated by the three behaviour models (M1, M2, and M3). Each box is generated based on 360\u00d7 4 data points since we have 360 combinations of dataset characteristics and four DL encoders. Similar to \u00a0<ref>, the triangle in each box indicates the mean value. We now inspect the plots shown inside with the aim of answering our research questions. The plots based on precision and recall are excluded since they draw similar conclusions.\n\n\u00a0<ref> shows that the BERT embedding strategy performs better than Logkey2vec for all behaviour models in terms of mean values and smaller IQRs. \nThis means that, on average, for all DL encoders, the semantic-aware log sequence embedding using BERT fares better than the embedding that solely relies on log template IDs using Logkey2vec, which does not account for the semantic information of templates.\n\n\n\n\n\nTo better understand the impact of log sequence embedding strategies on the performance of different DL encoders, we additionally performed paired t-tests to compare the F1 score distributions of BERT and Logkey2vec for each of the four DL encoders. Table\u00a0<ref> reports the statistical test results. For example, the low p-value in column M2 and row CNN indicates that Logkey2vec is significantly better than BERT when the CNN-based encoder is used on datasets generated by the M2 behaviour model. \n\nInterestingly, BERT is statistically better than or equal to Logkey2vec for all DL encoders except the CNN-based encoder (i.e., the best-performing DL encoder as investigated in \u00a0<ref>), a trend that is clearly observable in  \u00a0<ref>, depicting the F1 score distributions of BERT and Logkey2vec for the CNN encoder.\nIn other words, the combination of the CNN-based encoder and the Logkey2vec embedding strategy is the best combination of DL encoders and log sequence embedding strategies. \nAlthough, in contrast to BERT, Logkey2vec does not consider the semantic information of log templates, it accounts for the order of template IDs in each log sequence. Furthermore, Logkey2vec is trained together with the DL encoder, while BERT is pre-trained independently from the DL encoder. We suspect that such characteristics of Logkey2vec play a positive role in combination with the CNN-based encoder.\n\nWe note that BERT is still an attractive strategy for log sequence embedding when any other encoder than CNN is used. Although BERT is considerably larger than Logkey2vec in terms of parameters, using BERT does not require significantly more time and resources than Logkey2vec since BERT minimises repeated calculations by mapping each log template ID to its corresponding BERT embedding vector.\n\n\nThe answer to RQ2 is that the performance of the log sequence embedding strategies varies depending on the DL encoders used. Although BERT outperforms Logkey2vec overall across all encoders, Logkey2vec outperforms BERT when the CNN-based encoder is used. \n\n\n\n\n\n \u00a7.\u00a7 RQ3: Dataset Characteristics\n \n\n\n\n\u00a0<ref> shows the distributions of F1 scores according to different dataset characteristic values. We only use the data generated by the best combination of the DL encoder and the log sequence embedding strategy, i.e., the CNN-based encoder and Logkey2vec, based on the results from RQ1 and RQ2. Below, we discuss how the failure prediction accuracy of the best-performing combination varies with each of the dataset characteristics.\n\nIn \u00a0<ref>(a), we can see the impact of dataset size on the failure prediction accuracy; it is clear that accuracy decreases with smaller datasets, regardless of the behaviour models used to generate the log datasets. For example, when dataset size is 200, accuracy goes down below 0.7 in the worst case, whereas it always stays very close to 1.0 when dataset size is above or equal to 5000. Since larger datasets imply more training data, this result is intuitive. \n\n\u00a0<ref>(b) depicts the impact of maximum LSL values (mlsl) on the failure prediction accuracy. Compared to the impact of data set size, we can see that the impact of log sequence length is relatively small. This implies that the CNN-based encoder with the Logkey2vec embedding strategy works fairly well for long log sequence lengths of up to 1000. We suspect that the impact of log sequence length could be significant for much longer log sequences. However, log sequences longer than 1000 are not common in the publicly available, real-world log datasets\u00a0<cit.> as explained in Section\u00a0<ref>. Nevertheless, the investigation of much longer log sequences remains for future work.\n\nThe relationship between the failure percentage and the failure prediction accuracy (F1 score) is depicted in \u00a0<ref>(c). It is clear that, overall, the F1 score increases as the failure percentage increases. This is intuitive since a larger failure percentage means more instances of failure patterns in the training data, making it easier to learn such patterns. \nAn interesting observation is that the average failure prediction accuracy is above 0.9 even when the failure percentage is 10%. This implies that the failure predictor (i.e., DL encoder and log sequence embedding strategy) can cope well with unbalanced data.\n\n\u00a0<ref>(d) shows the failure prediction accuracy for different failure pattern types. There is no consistent trend across models M1, M2, and M3; Type-F (the corresponding language is finite) is better detected than Type-I (the corresponding language is infinite) in M2 and M3, whereas the opposite happens in M1. Considering the complexity of failure patterns, it is unclear why, in M1, detecting less complex failure patterns (Type-F) is more difficult than detecting more complex patterns (Type-I). We may not have defined failure pattern types in a way that is conducive to explaining variations in accuracy and different hypotheses will have to be tested in future work with respect to which pattern characteristics matter.\n\n\n\n\nWe focused above on the impact of individual characteristics on the accuracy of failure prediction, not their combinations. To identify conditions describing how the failure prediction accuracy of the best-performing failure predictor (i.e., the CNN-based encoder with the Logkey2vec embedding strategy) varies according to combinations of dataset characteristics, we built a regression tree\u00a0<cit.> where we explain variations in F1 scores according to  dataset characteristics. Out of 1080 data points (360 \u00d7 3 since we have 360 data points for each of the three behavior models), we used 720 (66.7%) and 360 (33.3%) of them, respectively, for training and testing the tree. \n\u00a0<ref> shows the resulting regression tree; each non-leaf node presents a partial condition and each leaf node presents the predicted accuracy of the condition corresponding to the path from the root to the leaf. \nFor example, the left-most leaf node means that the average failure prediction accuracy is predicted to be 0.516 if the dataset size is less than 350 and the failure percentage is less than 7.5. Otherwise, the failure average prediction accuracy will be at least 0.9. Further, we calculated the IQR of the F1 scores when the conditions for high accuracy are met, IQR reaches below 0.01.\nFrom the tree, it is clear that dataset size and failure percentage are the two main factors that explain failure prediction accuracy. Furthermore, we can see that the CNN-based encoder with the Logkey2vec embedding works very well, except when both the dataset size and failure percentage are small. The practical implication of this finding will be further discussed in Section\u00a0<ref>.\n\n\nThe answer to RQ3 is that, for the CNN-based encoder with the Logkey2vec embedding strategy, increasing the dataset size and failure percentage significantly increases the failure prediction accuracy, whereas the other factors (i.e., LSL and failure pattern type) do not have a clear relationship with failure prediction accuracy. Especially, the failure predictor is very accurate (F1 score above 0.95) and robust (IQR is below 0.01) when the dataset size is above 350 or the failure percentage is above 7.5%.\n\n\n\n\n\n \u00a7.\u00a7 Replication package\n\nWe plan to make the replication package, including implementation, generated datasets with behavioral models, and results, publicly available upon acceptance. \n\n\u00a7 DISCUSSION\n\n\n\n \u00a7.\u00a7 Findings and Implications\n\n\nOur study leverages the main DL types (LSTM, CNN, and transformer), along with different LSE strategies (BERT and Logkey2vec). In contrast to other studies mentioned in \u00a0<ref>, the full combination of DL types and LSE strategies are evaluated. Moreover, instead of using a limited number of datasets, using generated data enables us to control dataset characteristics to identify necessary conditions for achieving high-accuracy models. \n\n\nSeveral major findings are reported in \u00a0<ref>. \nFirst, the CNN-based DL encoder fares the best among different DL encoders, including the ones based on LSTMs, transformers, and BiLSTMs.\nSecond, the CNN-based DL encoder works best with the Logkey2vec embedding strategy, although BERT fares better than Logkey2vec overall for all DL encoders.\nThird, for the best combination, i.e., the CNN-based encoder with Logkey2vec, both the size and the failure percentage of input log datasets significantly drive the failure prediction accuracy, whereas the log sequence length and the failure pattern type do not. \nLast but not least, we found that the best combination works well if either the dataset size is above 350 or the failure percentage is above 7.5%. \n\nThese findings carry practical implications for both researchers and engineers.\nAlthough the CNN-based DL encoder and the Logkey2vec embedding strategy are not the most recent techniques in their respective fields, interestingly, their combination  works best for failure prediction. Based on this finding, we can recommend using the CNN-based encoder with Logkey2vec for accurate failure prediction among various combinations of DL encoders and log sequence embedding strategies. However, we have explored the ranges of dataset characteristics that have been observed in the literature. Different results may be observed beyond these ranges and for different failure patterns. \n\nFurthermore, the conditions driving failure prediction accuracy suggest practical guidelines. For example, for a log dataset size below 350 and a failure percentage below 7.5%, failure prediction will be inaccurate. In that case, one can increase either the log dataset size or the failure percentage to improve the situation. Although the failure percentage is inherent to the system under analysis and might not be easy to control in practice, collecting more log sequences during the operation of the system to increase the dataset size is usually feasible. \n\n\n\n\n \u00a7.\u00a7 Threats to Validity\n\nThere are a number of potential threats to the validity of our experimental results.\n\n\n\n  \nHyper-parameter tuning of models.\nThe hyper-parameters of failure predictors, such as optimizers, loss functions, and learning rates, can affect the results. To mitigate this, we followed recommendations from the literature. For the batch size and the number of epochs, as mentioned in \u00a0<ref>, we chose values for different combinations of dataset characteristics based on preliminary evaluation results. Better results could be obtained with different choices. \n\n\n\n  \nSynthetic data generation process. \nDue to the lack of a method to generate the datasets satisfying different dataset characteristics mentioned in \u00a0<ref>, we proposed a new approach, with precise algorithms, that can generate datasets in a controlled, unbiased manner as discussed in \u00a0<ref>. To mitigate any risks related to synthetic generation, we provided proof of the correctness of the algorithms and explained why it is unbiased during the generation process. To further support the validity of the generation process, we compared the results on actual datasets reported in the literature with those of the synthesised datasets for corresponding key parameters (e.g., dataset sizes and failure percentage). Results show to be remarkably consistent, thus backing up the validity of our experiments. For example, a BiLSTM-based model in the study of <cit.>, achieves results comparable to the synthesised data with similar characteristics [Using real-world datasets of BGL and Spirit, the failure predictor achieved 1.0 and 0.95 as F1 score, respectively, while the F1 score results of similar synthesised datasets using the same DL type as encoder are 0.99 and 0.99].\n\n\n\n  \nBehavioral models and failure patterns. \nThe behavioral models and failure patterns used for the generation of synthetic datasets may have a significant impact on the experimental results. \nWe want to remark that this is the first attempt to characterise failure patterns for investigating failure prediction performance. \nTo mitigate this issue, we carefully chose them based on pre-defined criteria described in \u00a0<ref>.\nNevertheless, more case studies, especially considering finer-grained failure patterns, are required to increase the generalizability of our findings and implications and, for that purpose, we provide in our replication package all the artifacts required.\n\n\n\n  \nPossible bugs in the implementation.\nThe implementation of the DL encoders, the log sequence embedding strategies, the dataset generation algorithms, and the scripts used in our experiments might include unexpected bugs. To mitigate this risk, we used the replication packages of existing studies\u00a0<cit.> as much as possible. Also, we carefully performed code reviews. \n\n\n \n\n\u00a7 CONCLUSION\n\n\nIn this paper, we presented a comprehensive and systematic evaluation of alternative failure prediction strategies relying on DL encoders and log sequence embedding strategies. We presented a generic, modular architecture for failure prediction which can be configured with specific DL encoders and embedding strategies, resulting in different failure predictors. We considered BERT and Logkey2vec as embedding strategies. We also covered the main DL categories, plus the attention mechanism, resulting in four DL encoders (LSTM-, BiLSTM-, CNN-, and transformer-based). Our selection was inspired by the previously used DL models in the literature. \n\nWe evaluated the failure prediction models on diverse synthetic datasets using three behavioural models inferred from available system logs. Four dataset characteristics were controlled when generating datasets: dataset size, failure percentage, Log Sequence Length (LSL), and failure pattern type. Using these characteristics, 360 datasets were generated for each of three behavioral models.\n\nEvaluation results show that the accuracy of the CNN-based encoder is significantly higher than the other encoders regardless of dataset characteristics and embedding strategies. Between the two embedding strategies, pretrained BERT outperformed the trainable Logkey2vec overall, although Logkey2vec fared better for the CNN-based encoder. The analysis of dataset characteristics confirms that increasing the dataset size and failure percentage increases the failure prediction accuracy, while the other factors (i.e., LSL and failure pattern type) did not show a clear relationship with failure prediction accuracy. Furthermore, the accuracy of the best configuration (i.e., CNN-based with Logkey2vec) consistently yielded high accuracy when the dataset size was above 350 or the failure percentage was above 7.5%, which makes it widely usable in practice.\n\nAs part of future work, we plan to further evaluate the best-performing configuration of the failure prediction architecture on real-world log data to further investigate the effect of other factors, such as log parsing techniques, on model accuracy. Additionally, by using real-world data, we also plan to include time-aware evaluation metrics, such as lead time\u00a0<cit.>, to assess the accuracy of these models at predicting failures early on.\n\n \nspbasic\n60\n\n\n\nurlstyle\n\n[Bauer and Adams(2012)]bauer2012reliability\nBauer E, Adams R (2012) Reliability and availability of cloud computing. John\n  Wiley & Sons\n\n[Black(2020)]SCC\nBlack PE (2020) Strongly connected component. Dictionary of Algorithms and Data\n  Structures\n  <https://www.nist.gov/dads/HTML/stronglyConnectedCompo.html>\n\n[Breiman(2001)]breiman2001random\nBreiman L (2001) Random forests. Machine learning 45(1):5\u201332,\n  10.1023/A:1010933404324\n\n[Breiman et\u00a0al.(1984)Breiman, Friedman, Olshen, and\n  Stone]breiman1984classification\nBreiman L, Friedman JH, Olshen RA, Stone CJ (1984) Classification and\n  Regression Trees. Wadsworth\n\n[Carvalho et\u00a0al.(2019)Carvalho, Soares, Vita, da\u00a0P.\u00a0Francisco, Basto,\n  and Alcal\u00e1]CARVALHO2019106024\nCarvalho TP, Soares FAAMN, Vita R, da\u00a0P\u00a0Francisco R, Basto JP, Alcal\u00e1 SGS\n  (2019) A systematic literature review of machine learning methods applied to\n  predictive maintenance. Computers & Industrial Engineering 137:106024,\n  https://doi.org/10.1016/j.cie.2019.106024,\n  <https://www.sciencedirect.com/science/article/pii/S0360835219304838>\n\n[Chen et\u00a0al.(2019)Chen, Yang, Lin, Zhang, Dong, Xu, Li, Kang, Zhang,\n  Gao, Xu, and Dang]Chen2019\nChen Y, Yang X, Lin Q, Zhang D, Dong H, Xu Y, Li H, Kang Y, Zhang H, Gao F, Xu\n  Z, Dang Y (2019) Outage prediction and diagnosis for cloud service systems.\n  The Web Conference 2019 - Proceedings of the World Wide Web Conference, WWW\n  2019 pp 2659\u20132665, 10.1145/3308558.3313501\n\n[Chen et\u00a0al.(2022)Chen, Li, Li, Guo, Du, and Xu]chen2022ai\nChen Y, Li L, Li W, Guo Q, Du Z, Xu Z (2022) AI Computing Systems: An\n  Application Driven Perspective. Elsevier Science,\n  <https://books.google.ca/books?id=RSWJEAAAQBAJ>\n\n[Chen et\u00a0al.(2021)Chen, Liu, Gu, Su, and Lyu]experimentreport\nChen Z, Liu J, Gu W, Su Y, Lyu MR (2021) Experience report: Deep learning-based\n  system log analysis for anomaly detection. 10.48550/ARXIV.2107.05908,\n  <https://arxiv.org/abs/2107.05908>\n\n[Cho et\u00a0al.(2014)Cho, Van\u00a0Merri\u00ebnboer, Bahdanau, and\n  Bengio]cho2014learning\nCho K, Van\u00a0Merri\u00ebnboer B, Bahdanau D, Bengio Y (2014) Learning phrase\n  representations using rnn encoder-decoder for statistical machine\n  translation. arXiv preprint arXiv:14061078\n\n[Chollet(2017)]padding\nChollet F (2017) Xception: Deep learning with depthwise separable convolutions.\n  In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n\n[Cortes and Vapnik(1995)]cortes1995support\nCortes C, Vapnik V (1995) Support-vector networks. Machine learning\n  20(3):273\u2013297\n\n[Das et\u00a0al.(2018)Das, Mueller, Siegel, and Vishnu]Das2018\nDas A, Mueller F, Siegel C, Vishnu A (2018) Desh: Deep learning for system\n  health prediction of lead times to failure in HPC. HPDC 2018 - Proceedings\n  of the 2018 International Symposium on High-Performance Parallel and\n  Distributed Computing pp 40\u201351, 10.1145/3208040.3208051\n\n[Das et\u00a0al.(2020)Das, Mueller, and Rountree]aarohi\nDas A, Mueller F, Rountree B (2020) Aarohi: Making Real-Time Node Failure\n  Prediction Feasible. Proceedings - 2020 IEEE 34th International Parallel and\n  Distributed Processing Symposium, IPDPS 2020 pp 1092\u20131101,\n  10.1109/IPDPS47924.2020.00115\n\n[Deerwester et\u00a0al.(1990)Deerwester, Dumais, Furnas, Landauer, and\n  Harshman]countvector\nDeerwester S, Dumais ST, Furnas GW, Landauer TK, Harshman R (1990) Indexing by\n  latent semantic analysis. Journal of the American society for information\n  science 41(6):391\u2013407\n\n[Devlin et\u00a0al.(2018)Devlin, Chang, Lee, and Toutanova]BERTbasemodel\nDevlin J, Chang MW, Lee K, Toutanova K (2018) Bert: Pre-training of deep\n  bidirectional transformers for language understanding. arXiv preprint\n  arXiv:181004805\n\n[Digital Research Alliance of Canada(2016)]computecanada\nDigital Research Alliance of Canada (2016) <https://alliancecan.ca/>,\n  accessed: March 2, 2023\n\n[Ding et\u00a0al.(2020)Ding, Zhou, Yang, and Tang]Ding2020CogLTXAB\nDing M, Zhou C, Yang H, Tang J (2020) Cogltx: Applying bert to long texts. In:\n  Neural Information Processing Systems\n\n[Du et\u00a0al.(2017)Du, Li, Zheng, and Srikumar]DeepLog\nDu M, Li F, Zheng G, Srikumar V (2017) Deeplog: Anomaly detection and diagnosis\n  from system logs through deep learning. In: Proceedings of the 2017 ACM\n  SIGSAC Conference on Computer and Communications Security, Association for\n  Computing Machinery, New York, NY, USA, CCS '17, p 1285\u20131298,\n  10.1145/3133956.3134015,\n  <https://doi.org/10.1145/3133956.3134015>\n\n[Gers et\u00a0al.(2000)Gers, Schmidhuber, and Cummins]GersLSTM\nGers FA, Schmidhuber JA, Cummins FA (2000) Learning to forget: Continual\n  prediction with lstm. Neural Comput 12(10):2451\u20132471,\n  10.1162/089976600300015015,\n  <https://doi.org/10.1162/089976600300015015>\n\n[Gu et\u00a0al.(2018)Gu, Wang, Kuen, Ma, Shahroudy, Shuai, Liu, Wang, Wang,\n  Cai, and Chen]CNNadvances\nGu J, Wang Z, Kuen J, Ma L, Shahroudy A, Shuai B, Liu T, Wang X, Wang G, Cai J,\n  Chen T (2018) Recent advances in convolutional neural networks. Pattern\n  Recognition 77:354\u2013377, https://doi.org/10.1016/j.patcog.2017.10.013,\n  <https://www.sciencedirect.com/science/article/pii/S0031320317304120>\n\n[Guo et\u00a0al.(2021)Guo, Yuan, and Wu]logBERT\nGuo H, Yuan S, Wu X (2021) Logbert: Log anomaly detection via bert. In: 2021\n  International Joint Conference on Neural Networks (IJCNN), pp 1\u20138,\n  10.1109/IJCNN52387.2021.9534113\n\n[He et\u00a0al.(2021)He, He, Chen, Yang, Su, and Lyu]He2021\nHe S, He P, Chen Z, Yang T, Su Y, Lyu MR (2021) A Survey on Automated Log\n  Analysis for Reliability Engineering. ACM Computing Surveys 54(6),\n  10.1145/3460345, 2009.07237\n\n[Hochreiter and Schmidhuber(1997a)]LSTM\nHochreiter S, Schmidhuber J (1997a) Long short-term memory. Neural\n  Comput 9(8):1735\u20131780, 10.1162/neco.1997.9.8.1735,\n  <https://doi.org/10.1162/neco.1997.9.8.1735>\n\n[Hochreiter and Schmidhuber(1997b)]HochSchm97\nHochreiter S, Schmidhuber J (1997b) Long short-term memory. Neural\n  Computation 9(8):1735\u20131780\n\n[Huang et\u00a0al.(2020)Huang, Liu, Fung, He, Zhao, Yang, and\n  Luan]HitAnomaly\nHuang S, Liu Y, Fung C, He R, Zhao Y, Yang H, Luan Z (2020) HitAnomaly:\n  Hierarchical Transformers for Anomaly Detection in System Log. IEEE\n  Transactions on Network and Service Management 17(4):2064\u20132076,\n  10.1109/TNSM.2020.3034647\n\n[Huang et\u00a0al.(2015)Huang, Xu, and Yu]BiLSTMfirstpaper\nHuang Z, Xu W, Yu K (2015) Bidirectional lstm-crf models for sequence tagging.\n  10.48550/ARXIV.1508.01991,\n  <https://arxiv.org/abs/1508.01991>\n\n[Johnson and Khoshgoftaar(2019)]ImbalanceLearning\nJohnson JM, Khoshgoftaar TM (2019) Survey on deep learning with class\n  imbalance. Journal of Big Data 6(1), 10.1186/s40537-019-0192-5,\n  <https://doi.org/10.1186/s40537-019-0192-5>\n\n[Jones(1979)]jones1979study\nJones KS (1979) A study of the tf-idf weight scheme for text classification.\n  In: Proceedings of the 3rd International Conference on Research in\n  Information Retrieval, ACM, pp 100\u2013107\n\n[Kim(2014)]CNNdefinition\nKim Y (2014) Convolutional neural networks for sentence classification. arXiv\n  preprint arXiv:14085882\n\n[Kingma and Ba(2015)]adam\nKingma DP, Ba J (2015) Adam: A method for stochastic optimization. In: Bengio\n  Y, LeCun Y (eds) 3rd International Conference on Learning Representations,\n  ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings,\n  <http://arxiv.org/abs/1412.6980>\n\n[Le and Zhang(2021)]Neurallog\nLe VH, Zhang H (2021) Log-based anomaly detection without log parsing. In: 2021\n  36th IEEE/ACM International Conference on Automated Software Engineering\n  (ASE), pp 492\u2013504, 10.1109/ASE51524.2021.9678773\n\n[Le and Zhang(2022)]10.1145/3510003.3510155\nLe VH, Zhang H (2022) Log-based anomaly detection with deep learning: How far\n  are we? In: Proceedings of the 44th International Conference on Software\n  Engineering, Association for Computing Machinery, New York, NY, USA, ICSE\n  '22, p 1356\u20131367, 10.1145/3510003.3510155,\n  <https://doi.org/10.1145/3510003.3510155>\n\n[Lin et\u00a0al.(2018)Lin, Hsieh, Dang, Zhang, Sui, Xu, Lou, Li, Wu, Yao,\n  Chintalapati, and Zhang]unbalance_failure\nLin Q, Hsieh K, Dang Y, Zhang H, Sui K, Xu Y, Lou JG, Li C, Wu Y, Yao R,\n  Chintalapati M, Zhang D (2018) Predicting node failure in cloud service\n  systems. In: Proceedings of the 2018 26th ACM Joint Meeting on European\n  Software Engineering Conference and Symposium on the Foundations of Software\n  Engineering, Association for Computing Machinery, New York, NY, USA, ESEC/FSE\n  2018, p 480\u2013490, 10.1145/3236024.3236060,\n  <https://doi.org/10.1145/3236024.3236060>\n\n[Lipton(2015)]Lipton2015ACR\nLipton ZC (2015) A critical review of recurrent neural networks for sequence\n  learning. ArXiv abs/1506.00019\n\n[Liu et\u00a0al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,\n  Zettlemoyer, and Stoyanov]Roberta\nLiu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L,\n  Stoyanov V (2019) Roberta: A robustly optimized bert pretraining approach.\n  In: Proceedings of the 2019 Conference on Empirical Methods in Natural\n  Language Processing and the 9th International Joint Conference on Natural\n  Language Processing (EMNLP-IJCNLP)\n\n[Lu et\u00a0al.(2018)Lu, Wei, Li, and Wang]CNN\nLu S, Wei X, Li Y, Wang L (2018) Detecting anomaly in big data system logs\n  using convolutional neural network. IEEE Access 6:21929\u201321940,\n  10.1109/ACCESS.2018.2811530\n\n[Matsumoto and Nishimura(1998)]Matsumoto1998\nMatsumoto M, Nishimura T (1998) Mersenne Twister: A 623-Dimensionally\n  Equidistributed Uniform Pseudo-Random Number Generator. ACM Transactions on\n  Modeling and Computer Simulation 8(1):3\u201330, 10.1145/272991.272995\n\n[Meng et\u00a0al.(2019)Meng, Liu, Zhu, Zhang, Pei, Liu, Chen, Zhang, Tao,\n  Sun, and Zhou]LogAnomaly\nMeng W, Liu Y, Zhu Y, Zhang S, Pei D, Liu Y, Chen Y, Zhang R, Tao S, Sun P,\n  Zhou R (2019) Loganomaly: Unsupervised detection of sequential and\n  quantitative anomalies in unstructured logs. In: International Joint\n  Conference on Artificial Intelligence\n\n[Meng et\u00a0al.(2020)Meng, Liu, Huang, Zhang, Zaiter, Chen, and\n  Pei]Log2Vec\nMeng W, Liu Y, Huang Y, Zhang S, Zaiter F, Chen B, Pei D (2020) A\n  semantic-aware representation framework for online log analysis. In: 2020\n  29th International Conference on Computer Communications and Networks\n  (ICCCN), pp 1\u20137, 10.1109/ICCCN49398.2020.9209707\n\n[Mikolov et\u00a0al.(2013)Mikolov, Chen, Corrado, and Dean]word2vec\nMikolov T, Chen K, Corrado GS, Dean J (2013) Efficient estimation of word\n  representations in vector space. In: International Conference on Learning\n  Representations\n\n[Nedelkoski et\u00a0al.(2020)Nedelkoski, Bogatinovski, Acker, Cardoso, and\n  Kao]LogSy\nNedelkoski S, Bogatinovski J, Acker A, Cardoso J, Kao O (2020) Self-attentive\n  classification-based anomaly detection in unstructured logs. Proceedings -\n  IEEE International Conference on Data Mining, ICDM\n  2020-Novem(Icdm):1196\u20131201, 10.1109/ICDM50108.2020.00148,\n  2008.09340\n\n[O'Shea and Nash(2015)]CNNfirstpaper\nO'Shea K, Nash R (2015) An introduction to convolutional neural networks.\n  10.48550/ARXIV.1511.08458,\n  <https://arxiv.org/abs/1511.08458>\n\n[Package(2019)]random\nPackage RP (2019)\n  <https://docs.python.org/3/library/random.html>, accessed\n  2022-11-14\n\n[Prechelt(1998)]earlystopping\nPrechelt L (1998) Early stopping-but when? In: Neural Networks: Tricks of the\n  Trade, Springer, pp 55\u201369\n\n[Radford et\u00a0al.(2019)Radford, Wu, Child, Luan, Amodei, and\n  Sutskever]GPT2\nRadford A, Wu J, Child R, Luan D, Amodei D, Sutskever I (2019) Language models\n  are unsupervised multitask learners. In: Neural Information Processing\n  Systems\n\n[Sahoo et\u00a0al.(2003)Sahoo, Oliner, Rish, Gupta, Moreira, Ma, Vilalta,\n  and Sivasubramaniam]Sahoo2003\nSahoo RK, Oliner AJ, Rish I, Gupta M, Moreira JE, Ma S, Vilalta R,\n  Sivasubramaniam A (2003) Critical event prediction for proactive management\n  in large-scale computer clusters. Proceedings of the ACM SIGKDD\n  International Conference on Knowledge Discovery and Data Mining pp 426\u2013435,\n  10.1145/956750.956799\n\n[Salfner et\u00a0al.(2010)Salfner, Lenk, and Malek]Salfner2010\nSalfner F, Lenk M, Malek M (2010) A survey of online failure prediction\n  methods. ACM Computing Surveys 42(3), 10.1145/1670679.1670680\n\n[Schuster and Paliwal(1997)]BiLSTM\nSchuster M, Paliwal K (1997) Bidirectional recurrent neural networks. IEEE\n  Transactions on Signal Processing 45(11):2673\u20132681, 10.1109/78.650093\n\n[Shin et\u00a0al.(2022)Shin, Bianculli, and Briand]PRINS\nShin D, Bianculli D, Briand L (2022) Prins: Scalable model inference for\n  component-based system logs. Empirical Softw Engg 27(4),\n  10.1007/s10664-021-10111-4,\n  <https://doi.org/10.1007/s10664-021-10111-4>\n\n[Sun et\u00a0al.(2019)Sun, Qiu, Xu, and\n  Huang]10.1007/978-3-030-32381-3_16\nSun C, Qiu X, Xu Y, Huang X (2019) How to fine-tune bert for text\n  classification? In: Sun M, Huang X, Ji H, Liu Z, Liu Y (eds) Chinese\n  Computational Linguistics, Springer International Publishing, Cham, pp\n  194\u2013206\n\n[Tauber(2018)]exrex\nTauber A (2018) exrex: Irregular methods for regular expressions.\n  <https://github.com/asciimoo/exrex>, accessed 2022-11-14\n\n[Upton and Cook(2008)]upton2008dictionary\nUpton G, Cook I (2008) A Dictionary of Statistics. Oxford Paperback Reference,\n  OUP Oxford, <https://books.google.ca/books?id=u97pzxRjaCQC>\n\n[Vaswani et\u00a0al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,\n  Gomez, Kaiser, and Polosukhin]AttentionIA\nVaswani A, Shazeer NM, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L,\n  Polosukhin I (2017) Attention is all you need. ArXiv abs/1706.03762\n\n[Walkinshaw et\u00a0al.(2013)Walkinshaw, Taylor, and Derrick]MINT\nWalkinshaw N, Taylor R, Derrick J (2013) Inferring extended finite state\n  machine models from software executions. In: 2013 20th Working Conference on\n  Reverse Engineering (WCRE), pp 301\u2013310, 10.1109/WCRE.2013.6671305\n\n[Weijie et\u00a0al.(2021)Weijie, Yunyi, Jing, and Xuchen]9587007\nWeijie D, Yunyi L, Jing Z, Xuchen S (2021) Long text classification based on\n  bert. In: 2021 IEEE 5th Information Technology,Networking,Electronic and\n  Automation Control Conference (ITNEC), vol\u00a05, pp 1147\u20131151,\n  10.1109/ITNEC52019.2021.9587007\n\n[Wu et\u00a0al.(2016)Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun,\n  Cao, Gao, Macherey, Klingner, Shah, Johnson, Liu, Kaiser, Gouws, Kato, Kudo,\n  Kazawa, Stevens, Kurian, Patil, Wang, Young, Smith, Riesa, Rudnick, Vinyals,\n  Corrado, Hughes, and Dean]wordpiece\nWu Y, Schuster M, Chen Z, Le QV, Norouzi M, Macherey W, Krikun M, Cao Y, Gao Q,\n  Macherey K, Klingner J, Shah A, Johnson M, Liu X, Kaiser L, Gouws S, Kato Y,\n  Kudo T, Kazawa H, Stevens K, Kurian G, Patil N, Wang W, Young C, Smith J,\n  Riesa J, Rudnick A, Vinyals O, Corrado G, Hughes M, Dean J (2016) Google's\n  neural machine translation system: Bridging the gap between human and machine\n  translation. CoRR abs/1609.08144,\n  <http://arxiv.org/abs/1609.08144>, 1609.08144\n\n[Xu et\u00a0al.(2020)Xu, Kumar, Yang, Zi, Tang, Huang, Cheung, Prince, and\n  Cao]trans-largedata\nXu P, Kumar D, Yang W, Zi W, Tang K, Huang C, Cheung JCK, Prince S, Cao Y\n  (2020) Optimizing deeper transformers on small datasets. In: Annual Meeting\n  of the Association for Computational Linguistics\n\n[Xu et\u00a0al.(2019)Xu, Li, Li, Li, and Li]logembedding\nXu Y, Li Z, Li Z, Li X, Li H (2019) Log embedding strategies for anomaly\n  detection in log data. IEEE Access 7:58264\u201358277\n\n[Yang et\u00a0al.(2021)Yang, Chen, Wang, Wang, Jiang, Dong, and\n  Zhang]PLELog\nYang L, Chen J, Wang Z, Wang W, Jiang J, Dong X, Zhang W (2021) Semi-supervised\n  log-based anomaly detection via probabilistic label estimation. In: 2021\n  IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pp\n  1448\u20131460, 10.1109/ICSE43902.2021.00130\n\n[Zhang et\u00a0al.(2019)Zhang, Xu, Lin, Qiao, Zhang, Dang, Xie, Yang,\n  Cheng, Li, Chen, He, Yao, Lou, Chintalapati, Shen, and Zhang]LogRobust\nZhang X, Xu Y, Lin Q, Qiao B, Zhang H, Dang Y, Xie C, Yang X, Cheng Q, Li Z,\n  Chen J, He X, Yao R, Lou JG, Chintalapati M, Shen F, Zhang D (2019) Robust\n  log-based anomaly detection on unstable log data. ESEC/FSE 2019 -\n  Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering\n  Conference and Symposium on the Foundations of Software Engineering pp\n  807\u2013817, 10.1145/3338906.3338931\n\n\n \n"}