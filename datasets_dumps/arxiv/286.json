{"entry_id": "http://arxiv.org/abs/2303.06946v1", "published": "20230313092752", "title": "Context-Aware Selective Label Smoothing for Calibrating Sequence Recognition Model", "authors": ["Shuangping Huang", "Yu Luo", "Zhenzhou Zhuang", "Jin-Gang Yu", "Mengchao He", "Yongpan Wang"], "primary_category": "cs.AI", "categories": ["cs.AI", "cs.LG"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#1\n@ACM@anonymous@addto@macroauthornotes\n    footnote#1\n\n\nCorresponding author.\n\n\n 1South China University of Technology, Guangzhou, China\n 2Pazhou Laboratory, Guangzhou, China\n 3DAMO Academy, Alibaba Group, Hangzhou, China\n 4Alibaba Group, Hangzhou, China\n \neehsp,jingangyu@scut.edu.cn, luoyurl@126.com, zhenzhouzhuang@foxmail.com, mengchao.hmc@alibaba-inc.com, yongpan@taobao.com\n\n\nShuangping Huang, Yu Luo, Zhenzhou Zhuang, Jin-Gang Yu, Mengchao He, Yongpan Wang\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDespite the success of deep neural network (DNN) on sequential data (i.e., scene text and speech) recognition, it suffers from the over-confidence problem mainly due to overfitting in training with the cross-entropy loss, which may make the decision-making less reliable. Confidence calibration has been recently proposed as one effective solution to this problem. Nevertheless, the majority of existing confidence calibration methods aims at non-sequential data, which is limited if directly applied to sequential data since the intrinsic contextual dependency in sequences or the class-specific statistical prior is seldom exploited. To the end, we propose a Context-Aware Selective Label Smoothing (CASLS) method for calibrating sequential data. The proposed CASLS fully leverages the contextual dependency in sequences to construct confusion matrices of contextual prediction statistics over different classes. Class-specific error rates are then used to adjust the weights of smoothing strength in order to achieve adaptive calibration. Experimental results on sequence recognition tasks, including scene text recognition and speech recognition, demonstrate that our method can achieve the state-of-the-art performance.\n\n\n\n\n\n\n\n<ccs2012>\n<concept>\n<concept_id>10010147.10010257.10010321.10010337</concept_id>\n<concept_desc>Computing methodologies\u00a0Regularization</concept_desc>\n<concept_significance>500</concept_significance>\n</concept>\n<concept>\n<concept_id>10003033.10003083.10003095</concept_id>\n<concept_desc>Networks\u00a0Network reliability</concept_desc>\n<concept_significance>500</concept_significance>\n</concept>\n</ccs2012>\n\n\n[500]Computing methodologies\u00a0Regularization\n[500]Networks\u00a0Network reliability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContext-Aware Selective Label Smoothing for Calibrating Sequence Recognition Model\n    Shuangping Huang^1,2,  Yu Luo^1,  Zhenzhou Zhuang^1,  Jin-Gang Yu^1,2*,  Mengchao He^3,  Yongpan Wang^4\n    \n===========================================================================================================\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nBenefited from the remarkable fitting capacity of deep learning models, the performance of sequential data recognition has been largely boosted recently, which consequently enables their deployment in plenty of user-facing applications, such as clinical text recognition <cit.>, autonomous driving <cit.> and speech recognition <cit.>. However, these deep models, most typically trained with the cross-entropy loss function, tend to assign a high probability even for a wrong prediction, which is the so-called over-confidence problem of deep sequential models.\n\nConfidence calibration has been proposed as one popular solution to the over-confidence problem. Typical methods in machine learning, including Platt scaling<cit.>, isotonic regression<cit.> and histogram binning<cit.>, inspire the confidence calibration of deep models <cit.>. Unfortunately, the majority of the existing methods mainly focus on non-sequential data, which cannot be trivially adapted to the calibration of sequential data. One key reason is that the task of sequence recognition involves a temporal process of predicting each token in order, where the contextual dependency among tokens is of critical importance (see in Fig. <ref>). Therefore, simply conducting uniform calibration on each token independently, unaware of inter-token dependency, is limited for sequential data.\n\nThere have already been a couple of previous attempts on confidence calibration of sequence recognition. Some authors considered the sequential characteristics and attempted to incorporate them into the sequential framework<cit.>. The varying sequential length is utilized as the setup basis of temperature values based on the temperature scaling technique <cit.>. Attention scores calculated during the sequence modeling are also utilized as the weights of temperature values <cit.>. In natural language processing (NLP), some measurements of sequence such as N-gram and BLEU<cit.> are exploited to guide the confidence allocation of correct and wrong predictions, to avoid the over-confidence typically caused by the cross-entropy loss <cit.>. Although the mentioned sequential properties are taken into account in these works, they still have two main limitations: 1) The influence of contextual dependency is seldom explored, and specialized manners of contextual feature extraction in NLP also make it hard to be generalized to sequential recognition tasks. 2) These methods ignore the uneven class distribution of predictions globally, while only considering the pairwise relationship between the prediction and the ground truth locally.\n\n\n\n\nIn this paper, we propose a Context-Aware Selective Label Smoothing method to implement a sequence-level calibration for sequential recognition models. To fully exploit the contextual dependency underlying the sequence, we construct confusion matrices for each class to represent the prediction distribution according to the contextual relation. Statistics are derived from a support set, which resembles the data distribution of the training set and thus is served as a reference for prediction distribution. Specifically, the probability distribution of labels belonging to different classes is reallocated based on the corresponding error rate in the confusion matrix, wherein error-prone labels are assigned with stronger smoothing strength. In this way, label smoothing on sequence can be implemented adaptively by considering contextual dependency and unbalanced prediction classes simultaneously. \nTo evaluate the effectiveness of the proposed method, we conduct experiments on two sequence recognition tasks, including scene text recognition and speech recognition. Experimental results demonstrate that our method is superior to the state-of-the-art calibration methods, and also can be well generalized on sequence recognition tasks.\n\nThe main contributions of this paper are summarized as:\n\n\n\n\n  * We propose a novel sequence-level confidence calibration method to adaptively calibrate the over-confident model.   \n\n  * The contextual dependency of sequence is fully explored to construct confusion matrices of contextual token prediction for each class, which is utilized as the weight basis for adaptive label smoothing. \n\n  * We demonstrate that our proposed method can generalize well on sequence recognition tasks to achieve calibrated results with a certain improvement on recognition accuracy.\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n \u00a7.\u00a7 Calibration on Non-sequential Data\n \n\nThe well-established calibration methods for non-sequential data related tasks, such as image classification, are borrowed from machine learning techniques<cit.>. With the prosperity of deep learning, the calibration of deep neural networks has recently attracted a lot of research interests in the literature. Post-processing based calibration methods learn a regression function based on a small held-out dataset to adjust the output confidence value without retraining the neural network from scratch and influencing the accuracy. Temperature scaling is the most commonly used method proposed by Guo et al. <cit.>. It is a variant version of Platt scaling through introducing a temperature parameter <cit.> to adaptively calibrate the models. Ji et al. <cit.> further implement temperature scaling in a bin-wise setting to improve the performance via denser prediction interval. However, as the confidences of all the predictions are calibrated with fixed parameters, it may lead to the underestimation of the predictions. Additional methods calibrate the confidence through optimizing the loss function in the training process. Label smoothing <cit.> is proposed as a regularization technique to soften the one-hot encoding distribution to calibrate the model <cit.>. Kumar et al. <cit.> and Mukhoti et al. <cit.> replace the general cross entropy loss with the distance between accuracy and confidence score and focal loss <cit.>, respectively. The decent results are achieved on non-sequential data, however, these methods are hard to be directly applied to the sequence. Simply conducting a uniform calibration across all the tokens of a sequence is too aggressive to produce good calibration performance.\n\n\n\n \u00a7.\u00a7 Calibration on Sequential Data\n\n\nInstead of trivially applying non-sequential calibration methods on each token, some methods attempt to exploit the intrinsic characteristics to facilitate sequence-level calibration.  Kuleshov and Liang <cit.> firstly consider confidence calibration in the context of structure prediction problem, which demonstrates that utilizing structural features will produce better calibration performance on structured prediction such as sequence. Recent methods for sequential data can be categorized into two directions. Based on temperature scaling <cit.>, Leathart et al. propose a sequential length-related calibration method <cit.>. As calibration error varies with sequential length, different temperature values are set for calibration. Ding et al. obtain the adaptive temperature values for different pixels or voxels for semantic segmentation <cit.>. Other methods are inspired by label smoothing <cit.>. In natural language processing, Elbayad et al. complete token-level smoothing with sequence-level smoothing methods. The nearby target sequence is sampled according to the BLEU score <cit.> to conduct sequence-level loss smoothing <cit.>. Lukasik et al. utilize n-gram overlapping with the target sequence in the label smoothing method to guarantee the semantic similarity of translated sequence <cit.>. Song et al. smooth the loss function according to the occurrence of the n-gram context in candidate word set as the weight <cit.>. Although better calibration performance could be achieved on sequential data, few methods consider the uneven distribution of dataset to flexibly calibrate the prediction of different labels, and additionally, the potential contextual information is not fully explored to calibrate on the recognition of sequential data tasks.\n\n\n\n\n\n\n\u00a7 PRELIMINARIES\n\n\n\n\n \u00a7.\u00a7 Sequential Confidence\n\n\nIn sequential data recognition tasks, we aim to assign an L-length sequential label vector Y={y_1, y_2, \u22ef, y_L} to the input x. For example, in scene text recognition, x is an image containing a text instance and Y is the sequence of character tokens. And in speech recognition, x is a sound waveform and Y is the sequence of phoneme tokens. For sequence prediction, the attention-based model consisting of RNN cells is commonly adopted. The token is predicted based on the information of previous states and the proceeding token. Assuming the set of class labels being {1,...,K}, the predicted token \u0177_t is obtained by maximizing the conditional probability as below:\n\n\n    \u0177_t= max_1\u2264 k \u2264 K p_t(y_t = k|x,y_<t)\n\nwhere the y_<t represents the previous tokens before the time step t. The token in the sequence is predicted in a temporal manner. After decoding at each time step, the sequence consisting of a series of tokens \u0176=(\u0177_1, \u0177_2,\u22ef, \u0177_L^') of length L^' can then be predicted.\n\nFor the calibration of sequential data, our objective is to optimize the holistic confidence score of sequence \u2119(\u0176|x), instead of individual token confidence p_t(y_t=\u0177_t|x). Probabilistically, the sequential confidence \u2119(\u0176|x) is a conditional joint probability that can be decomposed as follows:\n\n\n    \u2119(\u0176|x)=   \u2119(\u0177_1, \u0177_2,\u22ef, \u0177_L^'|x) c\n    \n         =    \u2119(\u0177_1|x) \u00d7\u2119(\u0177_2|x,\u0177_1) \u00d7\u22ef\u00d7\u2119(\u0177_L^'|x,\u0177_1,\u22ef,\u0177_L^'-1) \n    \n        =   \u220f_t=1^L^'p_t(y_t=\u0177_t|x,\u0177_<t)\n\nwhere \u0177_<t represents the previous tokens predicted before time step t. As shown in Eq.\u00a0<ref>, the cumulative multiplication of token confidence p_t(y_t=\u0177_t|x,\u0177_<t) predicted by a the sequence models, can probabilistically represent the confidence of sequence \u2119(\u0176|x).\n\n\n\n \u00a7.\u00a7  Evaluation of Confidence Calibration\n\n\nCalibration requires the confidence score to reflect the true accuracy of prediction. To evaluate whether a sequential model is well calibrated, several common metrics are introduced as follows.\n\nBrier Score <cit.> The brier score(BS) is known as the mean square error (MSE). By counting each correctly predicted sequence as one, while each wrong one as zero, the brier score is then defined by:\n\n\n    Brier Score =1/N\u2211_i=1^N(\ud835\udd40(\u0176_i = Y_i) - \u2119(\u0176_i|x))^2,\n\nwhere N denotes the total number of samples of testing data,\ud835\udd40(\u00b7) is the indicator function, and \u2119(\u0176_i|x) denotes the confidence of the i-th sequence.\n    \nExpected Calibration Error (ECE) <cit.> The ECE evaluates the calibration performance through measuring the absolute distance between the confidence and accuracy in a bin-wise manner. Firstly, the probability space [0,1] is split into M bins, and the test samples are also divided into different bins according to the corresponding confidence scores. The accuracy and the average confidence of the m-th bin are calculated as follows:\n\n\n    Acc(B_m) = 1/|B_m|\u2211_\u0176_i \u2208 B_m\ud835\udd40(\u0176_i = Y_i),\n\n\n\n    Conf(B_m) = 1/| B_m |\u2211_\u0176_i\u2208 B_m \u2119(\u0176_i|x),\n\nwhere B_m and | B_m | are the test samples and the number of samples in\nthe m-th bin, respectively. And the ECE is formally defined as:\n\n\n    ECE = \u2211_m=1^M| B_m |/N| Acc(B_m) - Conf(B_m) |,\n\n\nFor a well-calibrated model, the ECE value should be close to 0 with the accuracy is equal to the confidence score.\n\nReliability Diagram <cit.> The reliability diagram is the plot of Acc(B_m ) versus Conf(B_m ). As is shown in Fig. <ref> of Sec. <ref>, the solid concave curve in the reliability diagram represents the over-confidence state, while the convex one represents the confidence is less than the expected accuracy. If the model is well calibrated, the solid line should be aligned with the dotted diagonal line. \n\n\n\n\n\n\n\n \u00a7.\u00a7 Standard Label Smoothing\n\n \nAs the model training is associated with cross entropy loss using one-hot encoding, the large distance of 0-1 distribution is prone to make the model become over-confident for the prediction, which is undesirable for decision-making. Thus, the standard label smoothing (LS) is proposed as a common regularization technique for deep model <cit.>. It smooths the one-hot distribution with a \nhyper-parameter \u03b1 to obtain a soft distribution q\u0302_t(y_t=k) of every token y_t:\n\n\n    q\u0302_t(y_t=k) = {[     1-\u03b1,    if   y_t=k; \u03b1/K-1,    if   y_t \u2260 k;                        ].\n\n\nAlthough LS is proved to be helpful to alleviate the over-confident problem<cit.> by smoothing confidence 1-\u03b1 in soft distribution, imposing the same smoothing strength on the label of all the tokens indiscriminately is inapplicable for calibrating sequence. According to the Eq. <ref>, even if all the tokens are correctly predicted, the sequential confidence is calculated as \u2119(\u0176|x)\u2248 (1-\u03b1)^L^', which conversely leads to an exponential reduction on confidence.\n\n\n\n\n\u00a7 METHOD\n\n\n\n\n\nConsidering the uneven prediction distribution of different classes, we first design a selective label smoothing (SLS) method to conduct an adaptive calibration based on the confusion matrix of misprediction statistics (subsection <ref>). Furthermore, we exploit the contextual dependency in the sequence and propose a context-aware selective label smoothing method based on SLS (subsection <ref>).\n\n\n\n \u00a7.\u00a7 Selective Label Smoothing\n\n\nThe standard label smoothing method smooths the one-hot distribution of all classes evenly, while ignoring the differences between different character categories. It is a common phenomenon that the predicted error distribution varies from class to class. As shown in Fig. <ref>, the predicted error rate of character \u201cj\u201d is large than character \u201ca\u201d. It is reasonable to reduce the prediction confidence of error-prone character \"j\" by standard label smoothing. However, reducing the prediction confidence of non-error-prone character \"a\" is unreasonable. In this case, we focus on the frequent error-prone classes to implement an adaptive label smoothing. For the classes with few or even zero errors, it is needless to conduct label smoothing. \n\nTo obtain the statistical error-prone classes, we employ a support datasets that resembles the data distribution of the training set as the reference prediction distribution. And we construct a confusion matrix C_m=(c_i,j)\u2208 R^K \u00d7 K to quantitatively represent the prediction of K classes, where c_i,j denotes the number of the elements of the i-th class but are recognized as the j-th class. \nFig. <ref> is an example confusion matrix of the support dataset on scene text recognition task. Outsides the correct prediction distributed on the diagonal line, the misprediction distribution of some classes is higher than that of other classes. \n\nHere, we set a threshold to select those classes with a high error rate. And the error-prone set E can be defined:\n\n\n    E={ k | 1 - c_k,k/\u2211_jc_k,j > T , k = 1,2,\u22ef, K }\n\nwhere T denotes the probability threshold of the error-prone tokens.\n\nAnd the selective smoothed label distribution q\u0302(y_t=k) can be represented as follows:\n\n\n    q\u0302(y_t=k) = {[                    1 ,    if   y_t=k   and   y_t \u2209 E;                   1-\u03b1,    if   y_t=k   and   y_t \u2208 E;                  0 ,    if   k \u2260 y_t   and   y_t \u2209 E; \u03b1c_y_t,k/\u2211_jc_y_t,j,    if   y_t \u2260 k   and   y_t \u2208 E;                                                      ].\n\n\nIf the token y_t falls into the error-prone set E, a corresponding smoothing strategy will be conducted on the token. The error rate c_y_t,k/\u2211_jc_y_t,j is used as the weight of smoothing strength \u03b1, when the prediction k\u2260 y_t. \n\n\n\n \u00a7.\u00a7 Context-aware Selective Label Smoothing\n\n\n\nConsidering the contextual dependency existing in the sequential recognition model, the prediction errors distribution of every token depends on the different previous token. The error-prone set is related to the class of the previous token. Thus, we further propose a context-aware selective label smoothing method. We analyze the prediction error distribution under the condition of previous token of different classes, and similarly represent it in the form of a confusion matrix. Then, We obtain K confusion matrices {C_k | C_k=(c_k,i,j)\u2208 R^K \u00d7 K, k = 1, 2, \u22ef, K}. The C_k is the confusion matrix of predicted token, when the preceding token belongs to the k-th class. We name it as context confusion matrix. And c_k, i, j represents the number of current token belonging to the i-th class that is recognized as the j-th class for the k-th class of the preceding token. \n\nFig. <ref> is the example context confusion matrices of different classes on scene text recognition.  Errors rarely occur on the character tokens, when the preceding token is a digit \u201c3\u201d. There are few prediction errors for most character tokens when the preceding token is an \u201cA\u201d. But errors often occur on the character \u201cC\u201d when the preceding token is an \u201cV\u201d. It can be observed that it is rational to conduct a context related calibration since the misprediction distribution varies from class to class.\n\nSimilar to the process of SLS, totally K error-prone sets E_k are obtained:\n\n\n    E_k ={ i | 1 - c_k,i,i/\u2211_jc_k,i,j > T , i = 1,2,\u22ef, K },\n\nwhere E_k denote the error-prone token set of the k-th class.\n\nTherefore, the conditional probability distribution of context-aware selective label smoothing is correlated with the class of the preceding token y_t-1. The soft-target distribution q\u0302(y_t=k | y_t-1) is computed as:\n\n\n    q\u0302(y_t=k | y_t-1) = {[                                1 ,    if   y_t=k   and   y_t \u2209 E_y_t-1;                               1-\u03b1,    if   y_t=k   and   y_t \u2208 E_y_t-1;                              0 ,    if   y_t \u2260 k   and   y_t \u2209 E_y_t-1; \u03b1c_y_t-1,y_t,k/\u2211_jc_y_t-1,y_t,j,    if   y_t \u2260 k   and   y_t \u2208 E_y_t-1;                                                                        ].\n\n\nWhen the previous token is y_t-1, the distribution q\u0302(y_t=k | y_t-1) will adaptively select the error-prone set E_y_t-1. If the current token y_t is in the error-prone set E_y_t-1, a corresponding smoothing strategy will be conducted on the token y_t.\n\n\n    Loss = -1/L\u2211_t=1^L\u2211_k=1^Kq\u0302(y_t=k | y_t-1)log(P(\u0177_t = k)).\n\nThe corresponding loss function will be redefined in the training process.\n\n\n\n \u00a7.\u00a7 Sequence Alignment Strategy\n\n\nIn sequence recognition task, the existence of the omissive or redundant token in the prediction sequence sometimes leads to the misalignment between it and the reference sequence. Such misalignment results in two problems. Firstly, since one token is missing in the sequence prediction, the sequence confidence would be inaccurate without the attendance of the probability of the omissive token. The similar explanation is applied for redundant tokens. And the other problem is that the missing or redundant token makes it confusing to determine the alignment relationship between the ground truth token and the corresponding predicted token, which also influences the construction of confusion matrix.\n\nTo handle the misalignment problem, we introduce a \u201cblank\u201d class to fill the place of the missing token, and track the alignment between prediction and ground truth sequence along an operator sequence of edit distance. For example, as is shown in Fig. <ref>, the misalignment between predicted and reference sequence is caused by the redundant character \u201cl\u201d and the missing character \u201ct\u201d, which correspond to the deletion and insertion operation in edit distance, respectively. By tracking the flag of edit distance (deletion and insertion), we complement the \u201cblank\u201d space in the corresponding position to realize the alignment.\n\n\n\n\n\n\n \u00a7.\u00a7 Adaptive Smoothing Strength\n\n\nAs is analyzed in subsection <ref>, the sequential confidence will become quite small due to the cumulative effect of tokens confidence smoothed via the same hyper-parameter \u03b1. In order to solve the above-mentioned less confident problem of sequence, a trick is introduced to label smoothing. We adaptively adjust the value of hyper-parameter \u03b1 according to the length L of sequence. For each sequence, \u03b1=1-\u221a(1-\u03b1^'), which compensates for the negative impact of less confidence caused by the cumulative calculation.\n\n \n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENT AND ANALYSIS\n\n\n\nIn this section, the experimental results are described and discussed. As our method focuses on sequential data, we conduct it on two sequential recognition tasks: scene text recognition (STR) and speech recognition(SR). For each task, the experimental setup including datasets, base models and implementation details are firstly described. Furthermore, we discuss the calibration results under different conditions: uncalibrated model, the model calibrated via the LS, temperature scaling (TS) and the proposed methods.\n\n\n\n \u00a7.\u00a7 Scene text recognition\n\n\n\n  \u00a7.\u00a7.\u00a7 Experiment setup\n\n\nWe evaluate the proposed methods on the available benchmark datasets, including four regular datasets: IIIT5K-Words (IIIT5k) <cit.>, Street View Text (SVT) <cit.>, ICDAR 2003 (IC03) <cit.>, ICDAR 2013 (IC13)<cit.>, and three irregular datasets: ICDAR 2015 (IC15) <cit.>, SVT-Perspective (SVTP) <cit.>, CUTE80 (CUTE) <cit.>. And the support dataset is the ensemble of the training datasets of IIIT5k, SVT, IC03, IC13 and IC15. The support dataset contains a total of 8539 text instances and resembles the data distribution of training dataset <cit.>, from which we can get a general prediction distribution.\n\nThe experiment is conducted on the state-of-the-art models: ASTER model proposed by Shi et al.<cit.>, and modular STR framework proposed by Baek et al. <cit.>. ASTER is an attention-based model that uses a Spatial Transformer Network <cit.> for rectifying oriented or curved text, and a BiLSTM for sequence modeling. For the modular STR framework, we uniformly use the Spatial Transformer Network (T) and attention-based decoder (A). Specifically, the VGG-16 (V) <cit.> and ResNet-34 (R) <cit.> are utilized as backbone networks. And the model with or without a BiLSTM is considered (B/N). We adopt three architectures of different module combinations: TVBA, TRNA and TRBA.\n\nDuring the calibration, the dimension K of the confusion matrix corresponds to the number of prediction classes (26 characters, 10 digits and 1 blank) of STR task. We obtain confusion matrix and context confusion matrix from the predictions of the uncalibrated model on the support set, and then use SLS and CASLS to fine-tune the uncalibrated model. Here, we set up the threshold of the error-prone T=0.5, the smoothing parameter \u03b1^'=0.05, and the interval width of each bin B_m=15.\n\n\n\n  \u00a7.\u00a7.\u00a7 Results and Discussion\n\n\nAs is shown in Fig. <ref>, the blue curves below the diagonal line represent that all the uncalibrated STR models tend to be over-confident. As we analyzed before, the models trained with the LS(orange curves in Fig. <ref>) will become less confident instead, the curves is obviously convex in the high-confidence intervals. For all the calibration methods, SLS (green curves), TS (purple curves) and CASLS (red curves) approach are superior to the LS approach, among which the curves of TS and CASLS almost coincide with the diagonal line and can achieve the best calibration performance.\n\nTable <ref> shows the quantitative calibration results of LS, TS and the proposed SLS and CASLS methods. The accuracy, ECE and brier score(BS) are adopted as the measurements. In terms of ECE and BS, comparing with LS that widens the distance between accuracy and confidence, SLS and CASLS can achieve a good calibration performance on sequence through considering the uneven prediction distribution of different classes. And it is noted that, due to the utilization of contextual dependency, CASLS further outperforms SLS. And TS outperforms both of the proposed methods on TVBA, TRNA and TRBA. However, TS will not influence the accuracy, while the proposed methods can improve the accuracy based on the base model, and about 1.3% improvement is achieved on TVBA. \n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Speech Recognition\n\n\n\n  \u00a7.\u00a7.\u00a7 Experiment setup\n\n\nWe conduct the experiment on AISHELL-1 corpus <cit.>. It is a large-scale Mandarin benchmark that contains about 170 hours speech data recorded from 400 speakers. And there are a total of 141,600 sentences in the recording. In the corpus, the training dataset contains 120,098 sentences from 340 speakers, the test dataset contains 7,176 sentences from 20 speakers. The remaining data valid dataset is utilized as the support dataset to obtain the general prediction distribution.\n\nAnd we adopted the recent state-of-the-art toolkits as baseline: the WeNet proposed by Gulati et al. <cit.> and the ESPNet proposed by Watanabe et al.<cit.>. The WeNet utilizes a shared encoder that consists of multiple Transformer<cit.> or Conformer <cit.> encoder layers to extract the phonetic feature. And a joint CTC and attention decoder is used for sequence prediction. And ESPNet utilizes recurrent neural network as encoder, and adopts the same decoder as WeNet does. It is noted that both models are trained with LS (\u03b1^'=0.1) already. For a fair comparison, we retrain the models with cross-entropy loss to serve as uncalibrated models. And the CASLS is conducted based on the uncalibrated ones.\n\nDuring the calibration, we reduce the dimensions of confusion matrix as the number of phenom tokens is too large (4,000 classes) to calculate in the limited computer memory. Specifically, only the classes that belong to the error-prone set are reserved in the confusion matrix. Moreover, not only the prediction of many classes is sparse, but also the correct rate of the corresponding classes is extremely low. Therefore, the threshold of the error-prone T should be low. Here, we set up the threshold of the error-prone T=0, the smoothing parameter \u03b1=0.2, and the interval width of each bin B_m=15.\n\n\n\n  \u00a7.\u00a7.\u00a7 Results and Discussion\n\n\nFig. <ref> shows the reliability diagram of WeNet and ESPNet. Similarly, all the uncalibrated models (blue curves) and the models calibrated by the LS (orange curves) suffer from the over-confident and less-confident problems, respectively. The highest sequential confidence is even lower than 0.6 after conducting LS on each token on both models. And the TS (red curves) and CASLS (green curves) approaches can achieve a relatively better calibration performance.\n\nIn table <ref>, the calibration performance of LS and CASLS are listed. And an additional metric, word error rate (WER), is added based on the metrics used in STR task. Both models trained with CASLS achieve the improvement in accuracy and the reduction in WER. In terms of ECE and BS, the CASLS reduce the ECE by 53% on WeNet. On ESPNet, the effect of CASLS is slighter, but still achieves a 55% reduction in ECE. Although TS performs best in terms of ECE and BS, our proposed method can effectively improve the recognition metrics with the second best performance on calibration metrics.\n\n\n\n\n\u00a7 CONCLUSION\n\n\nIn this paper, we have investigated the problem of confidence calibration for sequence recognition. A so-called Context-Aware Selective Label Smoothing (CASLS) method has been proposed, which boosts label smoothing by leveraging the intrinsic contextual dependency underlying sequences and the statistical priors of class-specific prediction errors. Intensive experiments for two sequence recognition tasks, i.e., scene text recognition and speech recognition, both on publicly available benchmarking datasets, have been carried out to demonstrate our CASLS can achieve state-of-the-art performance. Ablation studies have also been conducted to further verify the effectiveness of the proposed method. In our future work, we will further verify our method on more sequential recognition tasks. We will also explore more effective strategies for making use of the contextual information.\n\n\n\n\n\u00a7 ACKNOWLEDGEMENTS\n\n\nThe research is supported in part by Guangdong Basic and Applied Basic Research Foundation (No. 2021A1515012282), National Nature Science\nFoundation of China (Granted No. 61936003, 62076099) and the Alibaba Innovative Research (AIR) Program.\n\n\n\n\nACM-Reference-Format\n\n\n"}