{"entry_id": "http://arxiv.org/abs/2303.06632v1", "published": "20230312110418", "title": "Focus on Change: Mood Prediction by Learning Emotion Changes via Spatio-Temporal Attention", "authors": ["Soujanya Narayana", "Ramanathan Subramanian", "Ibrahim Radwan", "Roland Goecke"], "primary_category": "cs.HC", "categories": ["cs.HC"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nop-tical net-works semi-conduc-tor IEEE-Xplore\nB-.05emi-.025em b-.08em\n    T-.1667em.7exE-.125emX\n\nL>l<C>c<R>r<\ud835\udc31\u0141 Le.g.i.e.E.g.et al.etc\nPre-print version\nShell et al.: Bare Demo of IEEEtran.cls for Signal Processing Society Society JournalsFocus on Change: Mood Prediction by Learning Emotion Changes via Spatio-Temporal Attention\n    Soujanya\u00a0Narayana,\u00a0Student\u00a0Member,\u00a0IEEE,\u00a0Ramanathan\u00a0Subramanian,\u00a0Senior\u00a0Member,\u00a0IEEE,\u00a0Ibrahim\u00a0Radwan,\u00a0Member,\u00a0IEEE,\u00a0and\u00a0Roland\u00a0Goecke,\u00a0Senior\u00a0Member,\u00a0IEEESoujanya Narayana, Ramanathan Subramanian, Ibrahim Radwan, and Roland Goecke are with the Human-Centred Technology Research Centre, Faculty of Science and Technology, Uni. Canberra, Bruce, ACT, Australia.\n    \n==========================================================================================================================================================================================================================================================================================================================================================================\n\n\nWhile emotion and mood interchangeably used, they differ in terms of duration, intensity and attributes. Even as multiple psychology studies examine the mood-emotion relationship, mood prediction has barely been studied. Recent machine learning advances such as the attention mechanism to focus on salient parts of the input data, have only been applied to infer emotions rather than mood. We perform mood prediction by incorporating both mood and emotion change information. We additionally explore spatial and temporal attention, and  parallel/sequential arrangements of the spatial and temporal attention modules to improve mood prediction performance. To examine generalizability of the proposed method, we evaluate models trained on the AFEW dataset with EMMA. Experiments reveal that (a) emotion change information is inherently beneficial to mood prediction, and (b) prediction performance improves with the integration of sequential and parallel spatial-temporal attention modules.\n\nMood, Emotion, Spatial Attention, Temporal Attention, Unimodal, Multimodal\n\n\n\u00a7 INTRODUCTION\n\nPsychology and cognitive science studies reveal that emotions play an essential role in rational decision making, perception, and other cognitive processes\u00a0<cit.>. There is also a growing body of evidence that a healthy balance of emotions is integral to human intelligence and problem solving\u00a0<cit.>. Although emotions have been studied by researchers for long, no single definition has been agreed upon. Emotions are considered as neurophysiological changes caused by external stimuli, and associated with feelings and behavioural changes\u00a0<cit.>. Mood is another affective state, which is often considered synonymous with emotion. Although there are no absolute boundaries between these two affective states, they differ in terms of duration, intensity and attributes. While emotion is a short-term affective state lasting a few seconds or minutes, mood denotes a long-term affective state lasting for minutes, hours or even days\u00a0<cit.>. Mood akin to emotion is known to influence thought process, creativity and judgement <cit.>. \n\nWhile much research has been devoted towards emotion inference[We use the term inferring instead of recognising emotions/mood.]<cit.>, very few computational studies attempt mood prediction\u00a0<cit.>. The psychology literature (a) recognizes mood and emotion as distinct mechanisms that repeatedly trigger the arousal of each other\u00a0<cit.>, and (b) ascribes mood as a higher-level disposition that activates lower-level states such as emotions and beliefs\u00a0<cit.>. Nevertheless, joint modelling of mood and emotion for affect prediction has been neglected. \n\nFrom an affective computing perspective, mood inference entails learning from data rich in affective state annotations, and assigning them to mood categories. The limited number of mood prediction studies can be attributed to the few databases with mood annotations. Popular affective databases such as HUMAINE\u00a0<cit.>, SEMAINE\u00a0<cit.> and DREAMER\u00a0<cit.> are rich in emotion annotations, whereas barely any dataset exists with mood annotations\u2013 these annotations denote the mood perceived by the human annotator rather than the actual mood expressed by the actor\u00a0<cit.>. Another reason for the limited availability of mood-labeled data is that affective labeling is onerous. While a large body of work leverages machine learning for emotion inference via  visual, acoustic, and textual data\u00a0<cit.>, very few explore mood inference.\n\nRecent computer vision studies are considerably inspired by the human ability to naturally focus on salient regions in complex scenes, known as attention. Whilst developed for machine translation tasks\u00a0<cit.>, attention is popularly incorporated in vision systems to improve their recognition performance\u00a0<cit.>. The attention mechanism guides models to learn what information is meaningful and where to look for the same for a certain task. Attention has also been applied for emotion inference, and to guide models to focus on emotionally relevant parts of speech\u00a0<cit.>, video frames\u00a0<cit.>, . While attention has been applied for detecting mood disorders\u00a0<cit.>, it has not been employed for mood prediction. Mood inference is critical for applications such as diagnosis of mood-related disorders, mood regulation, and developing affective interfaces.\n\n\n\nThis study explores the use of emotion information for mood prediction on the AFEW-VA dataset\u00a0<cit.> (subset of AFEW\u00a0<cit.>) comprising valence annotations. Valence denotes the extent of pleasure or displeasure elicited by an event, and we derive positive, negative and neutral mood labels from valence annotations. We additionally develop emotion change (\u0394) labels, which denote the change in emotion over a specific window size. For mood prediction, we preliminarily investigate (1) a single-branch 3-dimensional Convolution Neural Network (3D CNN) which uses only mood labels, (2) a two-branch (mood-\u0394) convolutional neural network with feature fusion followed by a Multi-Layer Perceptron, (3) a two-branch (mood-\u0394) convolutional neural network with end-to-end training, and (4) a teacher-student network which uses both mood and \u0394 labels. Empirical results described in our prior work\u00a0<cit.> reveal that emotion change information is beneficial, and enhances mood prediction performance. This work additionally explores the role of attention in mood prediction. We integrate spatial attention, temporal attention, and sequential/parallel spatial-temporal (SST/PST) attention modules with the above architectures to perform mood prediction. Furthermore, to examine generalisabilty, our trained models are evaluated on the EMMA dataset\u00a0<cit.>, which includes mood and emotion annotations. Fig.\u00a0<ref> describes our framework, including the prediction architectures and attention modules. Our research contributions are summarised below: \n\n\n  *  Incorporating emotional change information is beneficial and improves mood prediction performance as compared to models trained only with mood labels.  \n    \n  *  Mood prediction improves when spatial, temporal, sequential and parallel spatial-temporal attention modules are integrated within the inference architectures.\n   \n    \n    \n  *  Our mood inference framework is generalizable as high prediction accuracies are achieved on EMMA\u00a0<cit.> by models trained on AFEW-VA\u00a0<cit.>. Performance is evaluated at both the video and chunk levels, where a chunk denotes a snippet of five contiguous frames extracted from the original video.\n    \n    \n\n\nThe paper is organised as follows. Sec.\u00a0<ref> examines the literature on mood and emotion. Sec.\u00a0<ref> details the datasets used and the process of computing the mood and \u0394 labels. Mood inference models and attention modules employed for mood prediction are discussed in Sec.\u00a0<ref>. Empirical results are discussed in Sec.\u00a0<ref>, and Conclusions presented in Sec.\u00a0<ref>. \n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\nWe examine the literature in terms of (a) psychology studies relating mood and emotion, and (b) computational studies examining emotion and mood inference.  \n\n\n\n \u00a7.\u00a7 Mood, Emotion and their Interplay\n\nPsychologists consider mood and emotion as closely associated, but also distinguish them in terms of duration, intensity, stability, and attributes. Mood is described as a prolonged affect, lasting from minutes to hours or even days, with no specific trigger\u00a0<cit.>. Conversely, emotions are considered to be quite brief, lasting from a few seconds to minutes, and involve the dynamic phases of onset-apex-offset\u00a0<cit.>. \nAlso, while at least some emotions (known as Ekman's basic or universal emotions) are known to manifest via unique facial expressions\u00a0<cit.>, moods are not known to have unique manifestations. Cause and duration primarily distinguish the two affective states\u00a0<cit.>. \n\nDespite differences between the two affective states, the psychology literature also agrees on the confluence of the two. Ekman claims that, partially, mood is inferred from emotional signals associated with mood\u00a0<cit.>. E.g., we may deduce that someone is in a cheerful mood due to their joyful behaviour over a period of time. Mood is also known to be instantiated by emotions with varying intensity\u00a0<cit.>. Furthermore, mood acts as a bias while emotions are elicited, reinforcing similar emotions, while dampening dissimilar ones\u00a0<cit.>. This is similar to the mood-congruity effect, which states that positive mood facilitates the inference of mood-congruent (positive) emotions, but hampers the inference of mood-incongruent (negative) emotions, and vice-versa\u00a0<cit.>. The mood-emotion loop theory proposed in\u00a0<cit.> states that mood is a higher level variable activating lower level latent states, such as emotions and beliefs. This study also regards mood and emotions as mechanisms, which form a loop and repeatedly trigger the arousal of each other. To summarise, though mood and emotions are regarded as distinct affective states, the psychology literature identifies a relationship between the two\u00a0<cit.>. \n\n\n\n \u00a7.\u00a7 Mood & Emotion Classification\n\nMost studies focus on devising a computational framework for inferring emotions, while only a few examine mood from a computational perspective\u00a0<cit.>. Eye-tracking is utilized to observe that mood influences information processing styles\u00a0<cit.>. Positive mood results in a global information processing style, while negative mood influences attention to detail\u00a0<cit.>. Upper body behavior such as eye contact, arm openness, shoulder orientation, and head movement patterns are observed to perform mood inference in\u00a0<cit.>. Participants maintained a vertical head position for longer in a positive mood, as compared to negative mood. Mood prediction from recognised emotions shows that clustered emotions in the valence-arousal space[Arousal denotes the level of physiological excitation ranging from bored to excited following an event.] predict single mood better than multiple moods in a video\u00a0<cit.>.\n\nMachine learning algorithms are popularly used for affect inference due to their ability to learn high-level representations from input features. A preliminary step in mood inference is to compile a dataset with mood annotations. Most affective databases comprise continuous and dimensional emotion annotations, , HUMAINE\u00a0<cit.>, SEMAINE\u00a0<cit.>, AFEW\u00a0<cit.>, . In these corpora, emotions are expressed via facial expressions, bosy postures, and physiological signals. EMMA\u00a0<cit.> is an affective database with mood labels annotated via crowdsourcing. However, EMMA is intended for monitoring the affect of care center residents, with a focus on detecting negative mood.   \n\nGiven the complexity of emotions and how they manifest, a single modality becomes inadequate for learning and representing emotions. Hence, fusing multiple modalities for emotional inference has proved to be more effective than employing unimodal methods\u00a0<cit.>. Early fusion aims to integrate features extracted from multiple modalities <cit.>. Whereas, late fusion allows for adopting a modality-specific model, and fuses individual predictions by weighting or voting\u00a0<cit.>. Hybrid fusion corresponds to a trade-off between early fusion and late fusion, and aims at exploiting their advantages in a unified framework\u00a0<cit.>. Knowledge Distillation (KD) is the process of distilling knowledge from an ensemble of models into a single model\u00a0<cit.>. This technique achieves both model compression and learning with privileged information <cit.>. A large teacher network is trained with privileged information and transfers this knowledge to the smaller student network to achieve better inference efficiency. It is shown that knowledge can be effectively distilled using the soft target distribution produced by the teacher\u00a0<cit.>. KD is employed for facial expression recognition, with the teacher having access to fully visible faces, and the student accessing occluded faces in\u00a0<cit.>. For micro-expression recognition, a multi-task, multi-label network is designed with a residual network as the teacher and a two-layer CNN as the student\u00a0<cit.>.\n\n\n\n \u00a7.\u00a7 Attention for affect classification\n\nInspired by the way in which human perception focuses on certain parts of a given scene to better process information, the attention mechanism seeks to learn salient parts in the input data. Attention is a dynamic selection process that is realized by adaptively weighting features based on their relative importance in the input\u00a0<cit.>. Introduced in neural machine translation <cit.>, attention has been widely implemented in several visual tasks like face recognition\u00a0<cit.>, action recognition\u00a0<cit.>, pose estimation\u00a0<cit.>, emotion inference\u00a0<cit.>, . Attention methods include spatial attention (where to attend), channel attention (what to attend), temporal attention (when to attend), and hybrid approaches.  \n\nFor image classification and object detection tasks, Convolutional Block Attention Module (CBAM) is a simple-yet-efficient module implemented on feed-forward CNNs, with channel and spatial attention\u00a0<cit.>. Temporal attention modules are introduced in Long-Short Term Memory (LSTM) networks to emphasize the expressive speech parts in speech emotion inference\u00a0<cit.>. Temporal attention is used to weigh audio-visual time-windows that span short video clips, as temporal attention is a powerful approach for sequence modelling, and can be used to fuse audio-visual cues over time\u00a0<cit.>. To extract discriminative features from EEG signals and improve emotion inference, authors in\u00a0<cit.> employ an attention-based convolutional recurrent neural network (CRNN), which adopts a channel-wise attention mechanism to adaptively assign weights to different channels. \n\nWhile attention has been employed for emotion inference tasks, less has been done towards implementing attention for mood inference. Temporal attention is integrated with a CNN model to highlight speech responses in unipolar and bipolar disorder detection\u00a0<cit.>. Attention-based models are found to improve mood disorder detection accuracy by roughly 11%.\n\n\n\n \u00a7.\u00a7 Literature summary and research gaps\n\nA close examination of related work reveals that (a) whilst extensive research has focused on emotion inference, very few studies have examined mood prediction or modeled the mood-emotion interplay in their framework; (b) Labeled datasets for mood prediction are sparse, as most affective databases only include continuous/categorical emotion labels; (c) Attention mechanism has been employed largely for emotion inference, but not for mood prediction. \n\nDifferently, this work explores (i) the utility of incorporating emotion change information for mood classification into the neutral, positive and negative classes, (ii) integrating various arrangements of spatial and temporal attention modules with mood prediction architectures, (iii) model generalizability by evaluating AFEW-VA <cit.>-trained models on the EMMA <cit.> dataset, and (iv) evaluating mood prediction performance at the chunk and video levels. Our results demonstrate that accounting for the mood-emotion interplay,  by training a teacher-student network with both mood and \u0394 labels, enables better mood inference than mood-based models. Incorporating attention in this framework further improves prediction performance and model generalizability. \n\n\n\u00a7 MATERIALS\n\nThis section presents datasets used in our study, followed by the procedure adopted for synthesizing mood labels from valence annotations.\n\n\n\n \u00a7.\u00a7 Datasets\n\nWe employ the AFEW-VA dataset\u00a0<cit.> for compiling mood labels. We further evaluate the trained models on the EMMA dataset\u00a0<cit.>. The two datasets are described below:\n\n\n  \u00a7.\u00a7.\u00a7 AFEW-VA\n AFEW\u00a0<cit.> is a corpus containing facial expression videos. AFEW-VA\u00a0<cit.> is a subset of AFEW containing time-continuous valence-arousal annotations. AFEW-VA clips are 10\u2013145 frames long, and are captured under challenging indoor and outdoor conditions. Each video frame is annotated for valence and arousal intensities in the [-10,10] range. The annotations are produced by two expert annotators (one male, one female). \n\n  \u00a7.\u00a7.\u00a7 EMMA\n EMMA\u00a0<cit.> is an affective database containing 180 acted non-interactive videos typically depicting daily scenarios, and subtle facial and body expressions. The videos are lab-recorded, and depict 15 (6 male, 9 female) actors. The situational context and scenarios are defined to the actors, followed by mood induction procedures to elicit the target mood. Each video is associated with a categorical mood label acquired via crowdsourcing. \n\n \u00a7.\u00a7 Labels\n\n\n  \u00a7.\u00a7.\u00a7 Mood labels\n\nFor each AFEW-VA video, we assigned a mood label based on the per-frame valence level. We assigned a model label of +1 (positive mood) to frames with valence values in the range (3,10], 0 (neutral mood) to the valence range [-3,3], and -1 (negative mood) to the (-3,-10] range. We then synthesized video clips/input samples from the original video considering overlapping sequences of 5 contiguous frames. Each clip was assigned the mode of the per-frame mood labels in a sequence. Via this scheme, we extracted a total of 27,651 clips from the AFEW-VA dataset. \n\nAs EMMA already contains mood labels, we again consider overlapping five-frame sequences from each video, resulting in a total of 536,063 clips. The mood label for each EMMA sample is the same as the source video. \n\n\n\n\n  \u00a7.\u00a7.\u00a7 Emotion change (\u0394) labels\n\n\nIn addition to mood labels, each input sample is assigned an emotion change or \u0394 label, which refers to the change in emotional valence over k frames. For a video with n frames and a window size of k, it is computed as the difference in the valence between the t^th frame and (t - k + 1)^th frame, for t = k, k+1, ..., n. For example, considering k = 5, if v_5 = -4 and v_1 = -2, where v_5 and v_1  respectively denote the valence of the fifth and the first frames, \u0394 = v_5 - v_1 = -2 (See Fig.\u00a0<ref>). We assign the sample \u0394 label as (\u0394) defined below: \n\n\n    \u0394 = \n    -1    if \u0394 < 0, \n    \n    0    if \u0394 = 0, \n    \n    1    if \u0394 > 0.\n\n\nEach AFEW-VA sample has mood and \u0394 labels \u2208{0, -1, +1}, which are employed for the model training. Since EMMA is used only for testing, mood labels alone suffice for the EMMA samples.\n\n\n\n\u00a7 MOOD CLASSIFICATION ARCHITECTURES\n\n\nAs we seek to explore if the use of emotion information is beneficial for mood classification, we train different deep neural networks with mood and \u0394 labels, and compare their performance against models trained only with mood labels. The trained model architectures are described below followed by a discussion of the attention modules. \n\n\n\n \u00a7.\u00a7 Classification Architectures\n\n\n  \u00a7.\u00a7.\u00a7 1-CNN\n We employ a single branch three-layered convolutional neural network, or 1-CNN for mood classification. The model has three convolutional layers which comprise of 16, 32, and 32 kernels respectively with size 3 \u00d7 3 \u00d7 3 and each of them convolve the input sample with a stride of 3. Followed by each of the convolutional layer is an average pooling layer with a stride of 2-pixel regions. The output of the last convolutional layer is flattened, followed by batch normalization. Following the dense layer comprising 512 neurons is the Softmax layer with three neurons corresponding to the mood classes. \n\nThe input dimensionality for the 1-CNN is 5 \u00d7 32 \u00d7 32 \u00d7 3, with each sample comprising 5 frames of size 32 \u00d7 32 and 3 channels. The model is optimized with the categorical cross-entropy loss function, with the Adam optimiser used for stochastic gradient descent. Fine-tuned hyper-parameters include learning rate \u2208{10^-3, 10^-5}, batch size \u2208{64, 128, 256}, and dropout rate \u2208{0.4, 0.5}.\n\n\n\n  \u00a7.\u00a7.\u00a7 2-CNN + MLP\n\nMultimodal systems have gained increasing attention from researchers\u00a0<cit.> by overcoming limitations of unimodal counterparts\u00a0<cit.>. Multimodal systems fuse complementary information from different modalities at multiple levels. To examine the influence of emotion change (\u0394) on mood prediction, we fuse the two employing 2-CNN + Multi-layer Perceptron (MLP) as in Fig.\u00a0<ref>. The 2-CNN + MLP network is a two-branch model, with two identical 1-CNNs along each branch; mood labels are fed to one branch, and \u0394 labels fed to the other. From the penultimate layer in each branch, a 512-dimensional vector is obtained, and the two vectors are concatenated to obtain a 1024-feature vector input to the MLP for mood classification. \u0394 labels are assumed to be available during the train and test phases in this model. Dimensionality of the input samples and hyper-parameters are identical to 1-CNN. \n\n\n\n  \u00a7.\u00a7.\u00a7 2-CNN\n\nAs a substitute to model fusion, we trained an end-to-end model for mood classification\u00a0<cit.> via a 2-CNN model shown in Fig.\u00a0<ref>. Similar to the 2-CNN + MLP, the top and bottom branches are trained with mood and \u0394 labels respectively, but both branches are trained simultaneously but not individually. The categorical cross-entropy losses L_m and L_\u0394 from the two branches are summed up, and the cumulative loss is minimised. Unlike the 2-CNN + MLP, where \u0394 labels are used both in train and test phases, \u0394 labels act as auxiliary information for the 2-CNN model, and are required only during the training phase (mood labels suffice for testing).\n\n\n\n  \u00a7.\u00a7.\u00a7 TS-Net\n\nBesides model fusion and end-to-end training, we employ Knowledge Distillation, which is the process of transferring knowledge from a large model to smaller ones <cit.>. We employ a Teacher-Student Network (Fig.\u00a0<ref>(right)) where the teacher (large model) distills knowledge to the student (smaller model), and thereby the student tries to mimic the teacher. The pre-trained teacher model (2-CNN + MLP) uses both mood labels and \u0394 labels in the training phase to distill knowledge to the student (1-CNN). The student model, whic only utilizes mood labels, is activated for inference.  \n\nThe Softmax layer of the student model involves the temperature (T) hyper-parameter, which controls the smoothness of the output probabilities. As T grows, the output Softmax probabilities become softer, providing more information regarding the inter-class relations than one-hot labels <cit.>. The distillation loss is computed via the Kullback\u2013Leibler (KL) divergence measure, while the student loss is computed via sparse categorical cross-entropy. The overall loss of the TS-Net, L_TS is computed as the weighted sum of the distillation loss L_dist and the student loss L_stu, and is given by:\n\n\n    L_TS = \u03b1 L_stu  + (1 - \u03b1) L_dis\n\nwhere \u03b1 is a training hyper-parameter. The other fine-tuned hyper-parameters include batch size \u2208{16, 64, 128}, T\u2208{3, 5, 7} and \u03b1\u2208{0.05, 0.1, 0.15, 0.2, 0.25, 0.3}.\n\n\n\n \u00a7.\u00a7 Attention modules\n\n\n  \u00a7.\u00a7.\u00a7 Spatial attention\n\nIn humans, spatial attention allows to selectively process visual information through prioritization of locations within a field. Spatial attention modules have been widely implemented in tasks like action recognition, affect classification, . to improve model performance\u00a0<cit.>. Inspired by\u00a0<cit.>, we generate spatial attention maps to emphasize meaningful features. As shown in Fig.\u00a0<ref> (left), spatial attention is computed by applying max and average-pooling operations along the channel axis, followed by concatenation to generate a feature descriptor. A 3D convolutional layer is applied on the concatenated feature, and the output is multiplied with the input to generate a spatial attention map, which emphasizes where to look for in the video clip. \n\n\n\n  \u00a7.\u00a7.\u00a7 Temporal attention\n\nTemporal attention is a dynamic mechanism which determines when to pay attention, and directs visual focus to specific time instants. A video represents a sequence of visual segments with large content variance and complexity, where the amount of valuable information provided by frame segments is generally unequal\u00a0<cit.>, and discriminative information is provided by only some key frames. To model temporal dynamics in video, we apply LSTM networks, thereby capturing long-term temporal information. As shown in Fig <ref> (right), we use two LSTM layers with 128 hidden units per layer, followed by a dense layer with a single neuron. The input feature is multiplied with the output of the dense layer, facilitating the enhanced model to distinguish the most informative frame(s) for mood classification. \n\n\n\n  \u00a7.\u00a7.\u00a7 Integrating attention modules\n\nGiven an input clip, the spatial and temporal attention modules generate complementary attention information by respectively identifying salient regions and key frames. We integrate spatial and temporal attention modules into the classification networks, and also arrang these modules sequentially and parallely to synthesize the sequential spatial-temporal (SST) and parallel spatial-temporal (PST) attention architectures. \n\n  * SST Attention: In SST Attention (Fig.\u00a0<ref> (left)), the input sample I is sequentially fed into the spatial followed by the temporal attention modules. I is input to the spatial module first, and the attention map F_s is then convolved with I to produce I_s which is fed to the temporal module. The temporal map F_t is then multiplied with I_s to obtain I_st, which is fed to the classifier networks.\n    \n  * PST Attention: In PST Attention (Fig.\u00a0<ref> (right), the input sample I is fed parallelly to the spatial and temporal attention modules. The spatial (F_s) and temporal (F_t) attention maps are then convolved with I to respectively obtain I_s and I_t, which are multiplied to obtain I_st input to the classification models. \n\n\n \u00a7.\u00a7 Integration with classification models\n\n\nWe integrate the spatial and temporal attention modules individually and in combination with each of the mood classifer networks, as shown in Fig.\u00a0<ref>. Corresponding results are presented in Sec\u00a0<ref>.\n\n\n\n \u00a7.\u00a7 Evaluating on EMMA dataset\n\nAs mentioned in Sec <ref>, EMMA is an affective database comprising acted videos with mood and emotion annotations. There are \u2248 200 frames in each EMMA video, while the maximum video frames in AFEW-VA is 145. However, EMMA is compiled under lab conditions and with actors, unlike AFEW-VA which represents an in-the-wild video dataset. To examine if (a) the mood and \u0394 labels assigned over smaller (5-frame) AFEW-VA snippets were applicable over larger EMMA clips, and (b) the mood prediction models trained on AFEW-VA were generalizable to EMMA, we evaluated all mood classifiers on EMMA utilizing only the mood labels. \n\n\n\nOn performing a 90-10 stratified and random train-test split of the EMMA dataset, the test set comprised 18 videos, with 11 negative, 3 positive, and 4 neutral mood labels. Identical to AFEW-VA, each EMMA clip is of dimensionality 5 \u00d7 32 \u00d7 32 \u00d7 3, resulting in a total of 62,410 samples. We evaluate all trained models, with and without integrating attention modules, on EMMA. \nWhile evaluating models on EMMA, each 5-frame sample or chunk from the original video is tested, and we then evaluate models at both the chunk and video levels. This enables comparison of mood prediction performance over longer (video-level) and shorter (chunk-level) episodes depicting visual behavior. \n\n \u00a7.\u00a7 Performance metric\n \nSubject-independent 5-fold cross validation is used to evaluate all models to eliminate leakage effects. Mean accuracy over the five folds is reported as our performance metric. Results evaluating performance on the EMMA dataset also report the average performance over the five training models synthesized over the cross-validation process.\n\n\n\n\u00a7 RESULTS AND DISCUSSION\n\n\n \u00a7.\u00a7 Classification Experiments\n\n\n  \u00a7.\u00a7.\u00a7 Mood inference without attention modules\n\nTable\u00a0<ref> presents AFEW-VA mood classification results without and with various attention modules. The first column, which tabulates accuracies excluding attention modules, conveys that the 1-CNN (Mood) trained with mood labels performs worst, revealing that mood prediction is a non-trivial problem. As a pre-cursor to the 2-CNN + MLP and 2-CNN models which include a CNN-branch trained with \u0394 labels, we also evaluated a 1-CNN (\u0394) model trained and tested with \u0394 labels; this model performs better than 1-CNN (Mood) implying that emotion changes are easier detected than mood.  \n\nComparing the performance of the 2-CNN + MLP and 2-CNN networks, both of which are trained with both mood and \u0394 labels, we note that the 2-CNN + MLP network performs considerably than the 2-CNN whilst both achieve superior mood inference vis-\u00e1-vis the 1-CNN (Mood) model. These findings cumulatively reveal that including emotion-change information for mood prediction, either implicitly as auxiliary information (2-CNN) or explicitly as additional features (2-CNN + MLP), benefits mood inference. The highest mood inference accuracy of 0.89 is obtained with the TS-Net, where the 2-CNN + MLP denotes the pre-trained teacher model trained with both mood and auxiliary \u0394 labels, while 1-CNN denotes the student model, trained only with mood labels. \n\nOverall, these results affirm our claim that modeling the mood-emotion interplay and incorporating emotion change information for mood inference enables superior mood prediction as compared to utilizing mood information alone. \u0394 labels capture emotion changes over a short duration (local phenomenon), in comparison to mood labels which are applicable over a longer duration (global phenomenon). Our results reveal that short-term emotion changes play a crucial role in inferring long-term mood.\n\n\n  \u00a7.\u00a7.\u00a7 Mood inference with attention\n\nMood prediction results on integrating attention modules to the classification models are shown in Table\u00a0<ref> (`With attention' columns). We examine if integrating attention  into the classification architecture in addition to learning emotion changes improves mood prediction. A horizontal comparison in Table <ref> conveys that irrespective of the attention module, the 1-CNN performance significantly improves as compared to the baseline 1-CNN trained with mood labels.\n\nConversely, the 2-CNN + MLP and TS-Net (with the 2-CNN + MLP as the Teacher model) do not benefit when attention modules are integrated, and their prediction accuracies somewhat reduce. The 1-CNN (\u0394) does not benefit with either spatial or temporal attention, but its performance improves with sequence and parallel configurations of the spatial and temporal modules.  \n\nWith the 2-CNN network, where \u0394 labels are employed as auxiliary information, a significant performance improvement is noted with each of the attention modules. This observation reveals that the attention mechanism enables the 2-CNN to focus on informative pixels and important temporal segments for mood prediction. Fig.\u00a0<ref> qualitatively supports this finding, where the input sample with a neutral mood label is incorrectly predicted as positive by the 2-CNN without any attention modules. Integration of the spatial attention module enables the classification model to predominantly focus on facial regions and achieve the correct prediction. Overall, empirical results reveal that classification models inherently learning from emotion change information (1-CNN (\u0394), 2-CNN +MLP and TS-Net) do not additionally benefit from attention, while mood-based networks immensely benefit as attention facilitates focus on the facial region.   \n\nTo investigate the influence of emotion change (\u0394) information on mood prediction, we compare Table\u00a0<ref> results vertically. With spatial attention, while the 2-CNN and TS-Net perform better than the 1-CNN, the results are not significantly different. Likewise, integrating temporal attention also results in a higher accuracy with the 2-CNN and TS-Net as compared to 1-CNN (Mood). With respect to serial and parallel combinations of the spatial and temporal modules, with SST attention, the 2-CNN achieves significantly higher performance than 1-CNN (Mood) as per a two-sample t-test (t(8)=-4.7, p < 0.05). A significant difference is also observed between TS-Net and 1-CNN networks (t(8)=-2.6, p < 0.05). PST attention also yields significantly higher results with the 2-CNN (t(8)=-5.2, p<0.05), and the TS-Net (t(8)=-3.4, p<0.05) as compared with the 1-CNN. \n\nAs mentioned above, the attention modules and learning from emotion change information are complementary means to improve mood recognition performance. A qualitative depiction of how learning information pertaining to emotion change is shown in Fig.\u00a0<ref>. Attention maps for the input sample shown in the first row obtained with the 1-CNN (Mood) network including SST attention (2nd row) show that the model focuses regions in the vicinity of the face for mood inference. However, adding auxiliary information in the form of emotion change labels (3rd row) strongly localizes focus on the facial region to achieve superior mood inference. Cumulatively, Fig.\u00a0<ref> and Fig.\u00a0<ref> reinforce the observation that incorporating attention or emotion-change information for mood inference, are complementary means to the same end.\n\n\n\n  \u00a7.\u00a7.\u00a7 Evaluation on EMMA\n\n\nAs unlike the AFEW-VA dataset where mood labels are derived from valence annotations, the EMMA dataset comprises mood annotations. Also, mood labels in EMMA correspond to longer video segments as compared to EMMA. To examine if the models trained on AFEW-VA, and specifically to investigate if incorporating emotion change information can positively impact EMMA mood prediction, we evaluated all trained models on the EMMA dataset and results are presented in Table\u00a0<ref>. With no attention, the TS-Net performs best with 69% accuracy followed by the 1-CNN, while the 2-CNN performs worst on EMMA. Including attention modules improves 2-CNN mood prediction performance, while not benefitting inference with the 1-CNN or TS-Net. Overall, the TS-Net performs best on EMMA; we did not fine-tune the AFEW-VA models on EMMA as our main objective was to test model generalizability. \n\n\n\nWe also evaluated mood prediction performance at the chunk (5-frame episodes) and video levels on EMMA. Table\u00a0<ref> presents the corresponding results using TS-Net, which achieves optimal performance on AFEW-VA. While TS-Net without attention yields significantly higher results at the chunk level than at the video level, integrating attention modules results in significantly higher accuracies at the video-level. A highest accuracy of 0.89 is achieved with the models involving temporal attention. These results convey that while the AFEW-VA models generalize better to EMMA at the video-level, affirming the validity of our annotation and model training processes.\n\n\n\n \u00a7.\u00a7 Discussion Summary\n\nThe classification experiments convey that (a) incorporating emotion change information in addition to the mood labels positively impacts mood prediction; (b) integration of various attention modules to the architectures enables models to focus on key areas/input frames and improves mood prediction; (c) Qualitative results furthermore demonstrate that attention and emotion-change information play a complementary role towards enhancing mood inference. Additionally, AFEW-VA trained models generalize better to EMMA at the video-level than at the chunk-level, confirming that our methodology for annotating mood labels is valid, and can generalize to videos of longer duration.\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nWith majority of the contemporary studies focusing on emotion inference using multiple modalities and attention mechanism, negligible work has been done towards mood inference and examining the interplay between mood and emotions. The applicability of attention methods for examining mood is also neglected. As a step towards fulfilling these disparities, this work proposes using emotion change information along with attention mechanism for performing mood prediction. Using the AFEW-VA dataset, a single-branch CNN, two-branch CNN with MLP, two-branch CNN with end-to-end training and teacher-student network are employed as base models for mood prediction. Emotion change (\u0394) labels, referring to the difference in emotion over a fixed window size, is used in addition to the mood labels for examining mood. Further, various attention mechanisms are integrated to the base models to enhance the model's performance. These models trained and tested on AFEW-VA are evaluated on EMMA dataset to explore the generalisability of approach. \n\nOverall, we observe that without integrating attention modules in two-branch CNN and teacher-student network, incorporating emotion change information along with the mood labels improves mood prediction performance. The integration of parallel spatial-temporal attention modules in these models also yields similar results, further validating our claim. This approach is also generalizable as identical results with teacher-student network are observed when the model is evaluated on EMMA dataset. Better results in the video-level analysis as compared to chunk-level analysis shows that the model is also robust towards videos of longer duration. Future work will investigate the use of semi-supervised and weakly supervised techniques, and substitute emotion-change labels with difference or class-activation maps.\n\n\n\n\u00a7 ACKNOWLEDGEMENTS\n\nThis research is partially funded by the Australian Government through the Australian Research Council\u2019s Discovery Projects funding scheme (project DP190101294).\n\n\nIEEEtran -2 plus -1fil\n[\n    < g r a p h i c s >\n]Soujanya Narayana received her Bachelor's degree from Regional Institute of Education, Mysore, India in 2015 and Masters degree from Christ University, Bengaluru, India in 2017. She is currently a PhD student in the Human Centered Technology Research Centre at the University of Canberra, Australia. Her research interests include affective computing in human-computer interaction and computer vision. She is a student member of the IEEE.  -3 plus -1fil\n[\n    < g r a p h i c s >\n]Ramanathan Subramanian received his Ph.D. in Electrical and Computer Engg. from NUS in 2008. He is Associate Professor in the School of IT & Systems, University of Canberra. His past affiliations include IIT Ropar, IHPC (Singapore), U Glasgow (Singapore), IIIT Hyderabad and UIUC-ADSC (Singapore). His research focuses on Human-centered computing, especially on modeling non-verbal behavioral cues for interactive analytics. He is an IEEE Senior Member, and an ACM and AAAC member.\n -3 plus -1fil\n[\n    < g r a p h i c s >\n]Ibrahim Radwan received the Ph.D. degree in computer science from the University of Canberra in 2015. From 2014 to 2016, he was a researcher in a leading automotive industry warehouse, Research Fellow with The Australian National University and currently an assistant professor at the University of Canberra. His research includes computer vision, machine learning, robotics, and artificial intelligence.\n -3 plus -1fil\n[\n    < g r a p h i c s >\n]Roland Goecke received his Ph.D. degree in computer science from The Australian National University, Canberra, Australia, in 2004. He is Professor of Affective Computing with the University of Canberra, where he is serves as Director of the Human-Centred Technology Research Centre. His research interests include affective computing, pattern recognition, computer vision, human\u2013computer interaction and multimodal signal processing. He is a senior member of the IEEE, and an ACM and AAAC member."}