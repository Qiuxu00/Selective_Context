{"entry_id": "http://arxiv.org/abs/2303.07014v1", "published": "20230313112237", "title": "Reference-Guided Large-Scale Face Inpainting with Identity and Texture Control", "authors": ["Wuyang Luo", "Su Yang", "Weishan Zhang"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\n\n\n\nIEEE Transactions on Circuits and Systems for Video Technology\nHow to Use the IEEEtran  Templates\n\nReference-Guided Large-Scale Face Inpainting with Identity and Texture Control\n    Wuyang\u00a0Luo, Su\u00a0Yang, Weishan\u00a0Zhang\nThis work was supported by State Grid Corporation of China (Grant No. 5500-202011091A-0-0-00). Corresponding author: Su\u00a0Yang.\nWuyang Luo and Su Yang are with the Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai 200433, China\n(e-mail: wyluo18@fudan.edu.cn; suyang@fudan.edu.cn). \nWeishan Zhang is with School of Computer Science and Technology, China University of Petroleum, Qingdao 266580, China (e-mail: zhangws@upc.edu.cn).\nCopyright \u00a9 2023 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending an email to pubs-permissions@ieee.org.\n    March 30, 2023\n==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFace inpainting aims at plausibly predicting missing pixels of face images within a corrupted region. Most existing methods rely on generative models learning a face image distribution from a big dataset, which produces uncontrollable results, especially with large-scale missing regions. \nTo introduce strong control for face inpainting, we propose a novel reference-guided face inpainting method that fills the large-scale missing region with identity and texture control guided by a reference face image.\nHowever, generating high-quality results under imposing two control signals is challenging. \nTo tackle such difficulty, we propose a dual control one-stage framework that decouples the reference image into two levels for flexible control: High-level identity information and low-level texture information, where the identity information figures out the shape of the face and the texture information depicts the component-aware texture. \nTo synthesize high-quality results, we design two novel modules referred to as Half-AdaIN and Component-Wise Style Injector (CWSI) to inject the two kinds of control information into the inpainting processing. Our method produces realistic results with identity and texture control faithful to reference images. To the best of our knowledge, it is the first work to concurrently apply identity and component-level controls in face inpainting to promise more precise and controllable results. Code is available at <https://github.com/WuyangLuo/RefFaceInpainting>\n\n\n\nReference-Guided Face Inpainting, Large-Scale Missing Region, Identity and Texture Control.\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nImage inpainting refers to filling up masked areas while keeping coherence with context. However, it is an ill-posed problem because a corrupted image can correspond to countless plausible completed results. Recent successful image inpainting methods rely on generative models learning a distribution from a big dataset and then predicting pixels of masked regions by known regions, which makes the generated content hold in a generic sense. This uncontrollability is unacceptable for face inpainting with large-scale missing regions. Because human eyes are extremely sensitive to face images, especially when the person in the photo is someone you know. In this case, inpainted results may be far from what the user desires. \nRecent works introduce a reference image to control the face inpainting process, called reference-guided face inpainting. It is a critical technique due to its wide applications, such as face frontalization <cit.>, restoration <cit.>, and generation <cit.>. In this work, we concurrently introduce separate control of identity and texture into face inpainting, as shown in Fig. <ref>.\n\n\n\nThe existing works can be summarized into three frameworks, as shown in Fig. <ref>. \nRecognition-based framework <cit.> forces the identity of the inpainted image and the ground truth to be consistent using identity loss when training. This approach can only provide limited identity preservation. More importantly, it cannot freely control the identity of the output.\nInput concatenation framework <cit.> concatenates the corrupted image and the reference image as input. This strategy always produces incongruous results due to discrepancies between corrupted and reference images in terms of color and pose.\nTwo-stage framework <cit.> combines face inpainting and face swapping. It cascades two generators: The first generator is responsible for completing the face, and the second modifies the identity using identity features extracted from the reference image.\nInput concatenation and two-stage frameworks can effectively control the high-level identity of the results, but they ignore low-level features of reference images, such as texture. Consequently, previous methods suffer from limited control and low-quality results. \n\n\n\nIn this work, we propose a novel one-stage framework with dual control to concurrently exploit the high-level identity information and low-level texture information of reference images for controlling face inpainting and obtaining more realistic results. \nThere are two key differences between our proposed approach and the existing methods. First, we decouple the reference image into high-level identity information and low-level texture style via two separate feature extractors to control the inpainting process. Identity information dominates the global structure and layout of large missing regions while texture information describes the diverse styles of face components, so this decoupled design enables our model to leverage control information from different perspectives and provides more flexibility. However, previous methods only consider identity information. \nSecond, the two-stage framework splits the task into two separate steps: Face inpainting and face swapping, which leads to a larger number of parameters due to the fact that it requires two networks to achieve the goal. In contrast, our one-stage framework end-to-end optimizes our task within a neat model, achieving better performance with fewer parameters.\n\nIt is technically challenging to impose identity and texture control on an inpainting model independently and concurrently. For identity control, an identity vector can be injected into the generator via AdaIN <cit.>, which is employed for face swapping <cit.>. \nHowever, AdaIN is initially designed for the style transfer task, which is proven to wash away the original style of feature maps <cit.>, such as colors and textures. For our image inpainting task, the generated regions must be style-consistent with the known regions. Applying AdaIN directly to the entire feature map may result in inconsistent styles, as shown in Fig. <ref>. Therefore, we propose a light-overhead but effectively improved module referred to as Half-AdaIN, where we let half of the feature maps go through the AdaIN operation, and the remaining channels bypass AdaIN to avoid completely washing away the style of the input image.\n\nFor texture control, we design a Component-Wise Style Injector (CWSI) to locate, extract, and inject texture information of pre-defined face components. Precisely, CWSI first extracts the style codes of the reference image using a region-wise style encoder. \nThen, CWSI parses the feature maps to obtain the intermediate-level segmentation map for locating face component regions. \nFinally, CWSI injects style codes into corresponding component regions. To improve the texture control performance, we use a three-mode training scheme to approach better overall results.\nOur contributions are summarized as follows:\n\n\n  \u2219 We introduce precise separate control of identity and texture into face inpainting, for which we propose a novel dual control one-stage framework for reference-guided face inpainting.\n\n  \u2219 We design two new modules: Half-AdaIN and Component-Wise Style Injector (CWSI). They effectively inject identity and texture information to impose fine-grained controls on global profiles and local details simultaneously.\n\n  \u2219 Extensive experiments are performed to show that our method outperforms the state-of-the-art methods and can provide flexible control subject to reference images in terms of identity and texture.\n\n\n\n\n\n\n\n\u00a7 RELATED WORKS\n\n\n\n \u00a7.\u00a7 Image Inpainting\n\nThe existing inpainting methods can be divided into two categories: Traditional methods and deep learning based methods. Traditional methods use the remaining pixels to fill missing pixels, including diffusion-based methods <cit.> and patch-based methods <cit.>. These methods perform well on stationary textures. However, they often fail to synthesize satisfactory results for complex scenes because they do not understand high-level semantics. \nThe methods based on deep learning can learn to understand the high-level semantics of images by training neural networks on large-scale datasets. Recent works have been devoted to improving image inpainting performance by introducing adversarial training <cit.>, contextual attention mechanism <cit.>, new convolutions scheme <cit.>, additional information <cit.>, diversity  <cit.>, memory <cit.>, and vision transformer <cit.>.\n\nAs a prevalent branch of image inpainting, face inpainting has benefited a lot from image inpainting. \nFace inpainting methods often use rich prior knowledge or external conditions of human faces to impose constraints improving performance, such as semantic structure <cit.>, attributes <cit.>, example images <cit.>, identity <cit.>, symmetry <cit.>, and correlation among facial parts <cit.>.\nHowever, the existing methods only apply distribution learned from big datasets to direct inpainting, so the global structure as well as fine details are in general out of control, and not able to fit well into any specific desire.\n\n\n\n \u00a7.\u00a7 User-Guided Image Inpainting\n\nSome works introduced user interaction based clues to guide inpainting for control, such as segmentation maps <cit.>, line <cit.>, transformation <cit.>, image library  <cit.>, structures <cit.>, color <cit.>, and text <cit.>. Moreover, introducing another image as a reference is an intuitive way to exert comprehensive control. However, few works <cit.> have been developed to make use of all the potential clues from the reference image  <cit.>. <cit.> only considers how to inpaint a specific component within a small local area and does not care about the global structure. <cit.> only inserts reference images at the beginning of the generator and cannot control finely the final outcome. <cit.> requires the reference image and the inpainted image to be consistent in content, lighting, and view, which is impractical in many scenarios. <cit.> only considers high-level identity consistency but neglects local texture information. This paper proposes a new framework for reference-guided face inpainting tasks to make full use of the control information provided by the reference image in terms of identity and texture. \n\n\n\n \u00a7.\u00a7 Conditional Face Generation\n\nConditional face generation attempts to synthesize face images based on input conditions. It contains various subtasks, such as face image translation <cit.>, face swapping <cit.>, talking head synthesis <cit.>, face frontalization <cit.>, and face stylization <cit.>. Our work is related to two previous methods <cit.>. SimSwap <cit.> utilizes the AdaIN operation to build its generator, which transfers the identity information of the source face into the target face at feature level for arbitrary face swapping. To inject identity features while preserving the other important original features, we propose an improved version of AdaIN, referred to as Half-AdaIN. SEAN <cit.> synthesizes face images from segmentation maps, which can control the style of each semantic region individually. They utilize a style encoder network to extract the style of each region. \nWe reuse their design to extract style codes from reference images and then further employ the proposed CWSI for component-aware style injection serving face inpainting.\n\nThe proposed method introduces controllability to face inpainting so it can achieve an effect that looks similar to face swapping. However, our face inpainting task fundamentally differs from face swapping: \n(1) Different input: The input of our task is a face image with an arbitrary hole. Therefore, the structure of the input face is incomplete, e.g., missing nose or mouth; the input of face swapping is a complete face image. \n(2) Different goals: Face inpainting aims to reconstruct the missing region to match the known region. For example, if the central region of the input face is invisible, the face inpainting method needs to generate components of the face, such as the nose, at appropriate locations. Face swapping aims to change the overall identity of the input to another identity. Face swapping can change all identity-related elements, such as the shape of the nose and the size of the eyes. But face swapping does not need to reconstruct the structure.\n(3) Different outputs: Face inpainting fills the hole and generates a complete face. However, the content inside the hole is to some extent uncontrollable, e.g., if the upper part of the face falls into the missing region, the position and shape of the generated eyes may be different if using different face inpainting models. Face swapping does not need to reconstruct the structure of the face, so its output and input are aligned in terms of position and pose.\n\n\n\n\u00a7 METHOD\n\nWe describe our approach in a top-down manner. We first introduce the architecture of our generator and then give details of two novel modules for identity and texture control. \n\nOur generator has two inputs: (1) The corrupted image with the corresponding mask whose value is 0 in the missing region and 1 in the known region;  (2) The reference image provides control information that users desire.\nThe generator adopts an encoder-decoder architecture with skip connections <cit.>, as illustrated in Fig. <ref>(a). Specifically, the encoder is composed of several successive gated convolutional layers <cit.> with stride 2. The decoder is composed of several Half-AdaIN with upsampling operations. In addition, two CWSI are inserted into the decoder at the resolution of 64 \u00d7 64 and 128 \u00d7 128. For the inpainting task, we generally believe that the encoder can transfer the available information from the known pixels to missing pixels by gradually increasing the receptive field, and the decoder is responsible for the reconstruction of details  <cit.>, so we only place the control modules in the decoder.\n\nIn order to force the training generator to produce more realistic outputs, we employ a global discriminator and three local discriminators against the generator. We use SN-PatchGAN <cit.> as our global discriminator. The local discriminators are focused on specific sub-regions, including two eyes and mouths, which helps the generator synthesize high-frequency textures in these regions. The discriminators are omitted in Fig. <ref>.\n\n\n\n\n\n \u00a7.\u00a7 Identity Control\n\n\n\n\n1) Identity Feature Extraction: \nWe employ a well-trained face recognizer model <cit.> as the identity feature extractor. We take the output of the last layer prior to the classifier and normalize this 512-dimensional embedding as the identity vector Z_id.\nCompared to using reference images as guidance directly <cit.>, the high-level identity embedding extracted from the recognition model trained on large-scale face datasets can figure out more representative identity features. The identity embedding largely ignores pose, background, and lighting. Thus, it is unnecessary to require rigorous spatial alignment between the corrupted image and the reference image.\n\n\n\n\n\n\n2) Half-AdaIN For Identity Injection: \nThe previous face swapping work <cit.> borrows the AdaIN <cit.> operation to inject the identity feature. However, if we naively reuse AdaIN for our task, it produces many low-quality results as shown in Fig. <ref>. AdaIN washes away the original style of feature maps revealed by <cit.>, so it is inappropriate for our task that requires injecting an external identity vector while keeping coherence between generated content and known pixels. To tackle this problem, we propose a simple but effectively improved module referred to as Half-AdaIN, as illustrated in Fig. <ref>(b). The key novelty is that we split the input feature maps for different cues, half for identity control and the other half for preserving the original contextual information.\n\nLet F \u2208\u211d^C \u00d7 H \u00d7 W and Z_id\u2208\u211d^C_id denote input feature maps and the identity vector, respectively. Here, C is the number of channels, and H and W represent spatial dimensions.\nFirst, F passes through a standard convolutional layer to obtain F\u2208\u211d^C \u00d7 H \u00d7 W. Then, we split F into two equal slices along the channel dimension. Thus, we get two tensors F_1\u2208\u211d^C/2 \u00d7 H \u00d7 W([0: C/2) channels) and F_2\u2208\u211d^C/2 \u00d7 H \u00d7 W([C/2: C) channels).  We inject identity information by applying a standard AdaIN to F_1. Specifically, first, F_1 is convolved to F_1, and then we perform instance normalization on F_1:\n\n    F\u0305_\u03051\u0305=F_1-\u03bc/\u03c3\n\nwhere \u03bc\u2208\u211d^C/2 and \u03c3\u2208\u211d^C/2 are the channel-wise means and standard deviations of F_1. \nThen, we modulate feature maps with scaling parameters \u03b3 and shifting parameters \u03b2 learned from identity embedding Z_id. The modulation process is formulated as:\n\n    F_1= \u03b3\u2299F\u0305_\u03051\u0305\u2295\u03b2\n\nwhere \u03b3\u2208\u211d^C and \u03b2\u2208\u211d^C are generated from Z_id through two fully connected layers and expanded to match the spatial resolution of F\u0305_\u03051\u0305. \u2299 and \u2295 are element-wise multiplication and addition,  respectively. \nFor F_2, to adaptively distinguish valid and invalid pixels during the inpainting process, we apply a spatial attention mechanism following gated convolution <cit.>. Specifically, we generate a soft weight map W using F_2 through a convolutional layer followed by a sigmoid operation to make the values of W between 0 and 1. Finally, the output of the Half-AdaIN is formulated as: \n\n    F_out= ReLU(F_1 (F_2\u2299 W))\n\nwhere (\u00b7\u00b7) means concatenating two feature maps along the channel dimension and ReLU(\u00b7) is ReLU activation function. \n\n\n\n \u00a7.\u00a7 Texture Control\n\nThe texture information of the face component region is difficult to capture due to its small area and diversity. To extract image-specific local texture, we define five face regions with rich textures: Left eye, right eye, left eyebrow, right eyebrow, and lip. For texture control, we explicitly extract the texture information of these regions from the reference image and inject them into the corresponding regions of the corrupted image.\n\n\n\n\n1) Region-Wise Style Code Extraction: \nFirst, we obtain the segmentation map S \u2208\ud835\udd43^N \u00d7 H \u00d7 W of the reference image via a well-trained face parsing network <cit.>. Here, N is the number of semantic classes defined by the parsing network. H and W represent the height and width. Then, we can find five pre-defined regions according to S, and distill their style codes from the reference image using the same style encoder of SEAN <cit.>. Specifically, a region-wise pooling layer is applied at the last layer of the style encoder, and we extract a 512-dimensional style code matrix SM \u2208\u211d^5 \u00d7 512. Note that if a component has any pixels in the non-masked region, we ignore it and fill its style code value with 0. \n\n\n\n\n2) Component-Wise Style Injector (CWSI):\nThe structure of CWSI is shown in Fug. <ref>(c). Let F \u2208\u211d^C \u00d7 H \u00d7 W denote the input feature maps. The generator must online predict a segmentation map associated with F to locate the five pre-defined regions. To this end, we send F into a segmentation branch to obtain its segmentation map S\u0305\u2208\ud835\udd43^N \u00d7 H \u00d7 W and select the five pre-defined regions S\u2208\ud835\udd43^5 \u00d7 H \u00d7 W. The segmentation branch consists of two resnet blocks <cit.> and a softmax layer. We broadcast the style code into the corresponding region according to S through a matrix multiplication:\n\n    z_style=SM^\u22a4\u00d7S\n\nwhere SM^\u22a4 denotes the transposed matrix of SM. Thus, z_style\u2208\u211d^512 \u00d7 H \u00d7 W has the same layout as S but is filled by the style codes from SM. \nFinally, we use a denormalization operation similar to SPADE <cit.> to inject texture features. Specifically, we learn two parameters \u03b3 and \u03b2 from z_style to modulate F: \n\n    F= ReLU(\u03b3\u2299 IN(F) \u2295\u03b2)\n\nwhere IN(\u00b7) denotes instance normalization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n3) Training Scheme For Texture Control: \nThe success of texture control highly relies on the performance of CWSI's segmentation branch. If we cannot provide aligned supervision signals, texture control will fail, as described in Section 4.8. \nIdeally, we need an aligned segmentation map S_f to match the current inpainted result online when training, as illustrated in Fig. <ref>(a). \nIn practice, we can only pre-generate S_gt from ground truth as supervision signals for segmentation branches. However, it cannot be guaranteed that S_gt could be aligned with the inpainted result precisely in the working pipeline when the masked region is large, especially for small eye and mouth regions.\nThe general training scheme suffers from always appearing misalignment, if we only utilizes regular masks (inpainting mode) to generate corrupted images, where the training scheme as such is referred to as Single-Mode Training (SMT) depicted in Fig. <ref>(c). \nTo overcome the misalignment problem, we introduce a Three-Mode Training scheme (TMT) with two additional modes referred to as segmentation mode and style extracting mode, as shown in Fig. <ref>(b).\nSegmentation mode utilizes a small mask that does not cover the face\u2019s central region and takes ground truth as the reference image. \nUnder such settings, we force the generator to focus on improving the performance of the segmentation branch.\nStyle extracting mode employs a special mask corresponding with the five pre-defined facial components to enhance training on the facial components for texture control injection. \nIn two additional modes, S_gt is aligned with the inpainted result because missing regions are set to be small.\nTMT applies three training modes iteratively for each training step. Specifically, we alternately perform segmentation mode and style extracting mode between two adjacent inpainting modes, as described in Fig. <ref>(d).\n\n\n\n \u00a7.\u00a7 Loss Function\n\n\n\n\n\n1) Pixel Reconstruction Loss \u2112_R  \ncalculates the L1 distance between the inpainted image and the ground truth in RGB space.\n\n\n\n\n2) Perceptual Loss \u2112_P  \n<cit.> is used to force the output of the generator to be closer to the ground truth in a pre-trained VGG-19 <cit.> feature space at multiple layers. \n\n\n\n\n3) Identity-preserving Loss \u2112_id  \nis used to force the generated image's identity to be consistent with the reference image's identity. It is formulated as:\n\n    \u2112_id=1- cos(z_id(I_G), z_i d(I_r))\n\nwhere cos (\u00b7, \u00b7) represents the cosine similarity of two vectors.\n\n\n\n\n4) Semantic Segmentation Loss \u2112_S  \nis employed for training the segmentation branches of CWSI. We supervise the output of each segmentation branch using cross-entropy loss of semantic segmentation task <cit.>.\n\n\n\n\n5) Global Adversarial Loss \u2112_adv,G \nfollows SNPatchGAN <cit.> but with the hinge version <cit.>. It is applied to the entire image and evaluates whether the entire image is coherent as a whole.\n\n\n\n\n6) Local Adversarial Loss \u2112_adv,L \nis calculated on three sub-regions (left eye, right eye, and mouth) using the same loss function as the global adversarial loss following the previous work <cit.>. Then, the losses of the three regions are summed as the final local adversarial loss. \n\nThe overall Loss can be written as:\n\n    \u2112= \u03bb_R\u2112_R + \u03bb_P\u2112_P + \u03bb_id\u2112_id + \u03bb_S\u2112_S + \u2112_adv, G + I_adv, L\u2112_adv, L\n\nHere, \u03bb_R=20, \u03bb_P=10, \u03bb_id=3, \u03bb_S=2.5. I_adv, L is 1 in inpainting mode and 0 in other modes.\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n \u00a7.\u00a7 Dataset\n\nOur task needs a high-quality dataset for training. Based on Celeb-ID <cit.>, we build a dataset suitable for our task.  Celeb-ID contains around 1700 individual identities and a total of 100K images, but it contains a large number of extremely blurred and occluded photos. We manually eliminated the low-quality images. In the end, our dataset contains a total of 16,259 identities and 94,770 images and at least two photos are provided for each identity. 1,500 identities are used for testing, and the rest are used for training. \n\n\n\n \u00a7.\u00a7 Evaluation Metrics\n\n\n\n\n1) Fr\u00e9chet Inception Distance (FID)\n<cit.> utilizes a pre-trained Inception-v3 <cit.> network to extract features and measure the distance between generated samples and real samples. It has been widely demonstrated that it is consistent with human visual perception. Lower FID value indicates higher fidelity.\n\n\n\n\n2) Learned perceptual image patch similarity (LPIPS)\n<cit.> evaluates the similarity between the generated image and the corresponding ground truth in a pairwise manner. A lower LPIPS indicates that the generated image is closer to the target.\n\n\n\n\n3) ID retrieval accuracy (IDR): \nWe randomly select two images as the corrupted  images and the reference image for each identity in the test set, respectively. The unselected images are put into a retrieval pool. The identity embeddings are extracted from the inpainted image and the retrieval pool images, using a different face recognition model <cit.>. Then, the pair-wise distance is calculated using cosine similarity. We look for the nearest sample in the retrieval pool for each completed result and check whether they belong to the same identity. The accuracy of such retrieval is reported as identity retrieval accuracy (IDR).\n\n\n\n \u00a7.\u00a7 Implementation Details\n\nAll images are resized to 256 \u00d7 256, and the cropped sizes of eyes and mouth are 55 \u00d7 60, and 44 \u00d7 90, respectively, in calculating the local adversarial loss. In order to simulate a more general application scenario, we use free-form masks with different sizes. We employ Adam <cit.> optimizers for both the generator and the discriminators with momentum \u03b2_1=0.5 and \u03b2_2=0.999. The learning rate is set to 0.0002. We only update the parameters of the style encoder in style extracting mode. The proposed model is trained for 300 epochs on a single RTX 3090 GPU.\n\n\n\n \u00a7.\u00a7 Baselines\n\nWe introduce additional three types of baselines for an extensive comparison.\n\n\n\n\n1) Reference-Guided Face Inpainting Method: \nZhao et al. <cit.> are the most relevant works to our method and also uses a reference image to control inpainted results.\n\n\n\n\n2) General Image Inpainting Method: \nWe investigate the performance of two currently leading inpainting methods, ICT <cit.> and CoMod <cit.>. They show impressive results dealing with large-region masks. \n\n\n\n\n3) Combination Of Image Inpainting And Face Swapping:\nBecause the general inpainting method does not employ the reference image, we combine the image inpainting and face swapping as new baselines for a fairer comparison. Specifically, the results of ICT or CoMod with reference images are fed into a face-swapping method SimSwap <cit.>. SimSwap is capable of transferring the identity of an arbitrary source face into an arbitrary target face. Some famous open-source tools <cit.> are not considered because they require specialized training for different inputs. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Quantitative and Qualitative Comparison\n\nFig. <ref> provides a visual comparison. Zhao et al. <cit.> suffers from serious style inconsistency and misalignment. Since <cit.> directly uses the reference image as the condition, it is difficult for their generator to decompose information into identity and texture from the reference image.\nGeneral face inpainting <cit.> methods are able to produce visually pleasing results. However, they lack controllability and cannot produce desired results guided by specific reference images. Combination methods can control the inpainted result, but they suffer from several significant drawbacks:\n(1) Simply combining two methods leads to error accumulation. For example, face swapping will produce unpleasant results if the inpainting method produces some distorted structures. (2) Although the face swapping approach can inject identity information, it ignores the texture information provided by the reference image.\nGe et al. <cit.> only employ identity loss to constrain the inpainted results, which cannot effectively preserve identity information. SwapInpaint <cit.> can generate identity-consistent results but ignores local texture information provided by the reference image.\nOur method can generate high-quality results consistent with the reference image in terms of identity and texture. Significant performance gains benefit from our separate injection of identity and texture information, as well as the end-to-end joint optimization in the proposed comprehensive framework. As shown in Table <ref>, the quantitative results also demonstrate that our method can yield realistic results and maintain a high degree of consistency with reference images.\n\n\n\n \u00a7.\u00a7 Cross-Identity Face Inpainting\n\nOur model can also achieve cross-identity face inpainting, which is similar to face swapping.  However, it should be noted that our task is fundamentally different from face swapping. Face swapping can see the layout of the source image, but our task needs to predict the content in the missing region. Correspondingly, our results are demonstrated in Fig. <ref>. \n\n\n\n \u00a7.\u00a7 Separately Control Identity and Texture\n\nDue to our decoupled design, our model can flexibly control identity and texture. Furthermore, we use two different reference images to generate identity vectors and style codes to demonstrate how the decoupled controls work in cooperation with each other. Some visual results are shown in Fig. <ref>. \n\n\n\n \u00a7.\u00a7 Ablation Study\n\n\n\n\n1) Identity Control: \nIdentity information is introduced to control the identity of the result. We replace all Half-AdaIN with standard convolutional layers but still use identity preserving loss (w AdaIN). As shown in Fig. <ref>, the model without identity control cannot recall the identity information of the reference image. The quantitative results in Table <ref> show the effectiveness of identity control, especially from the perspective of retrieval accuracy. \nWe also report Face Recognition Rate (FRR). Specifically, we utilize a well-trained face recognizer <cit.> to calculate the distance between generated and reference images. The threshold value is set to 0.7\n\n\n\n\n2) Effect of Half-AdaIN: \nWe design Half-AdaIN to inject identity information for generating high-quality results. The distinct point of Half-AdaIN is that it preserves the contextual style from corrupted images.\nWe replace all Half-AdaIN with AdaIN (w/o Half-AdaIN). The visual results drop significantly and produce style inconsistency as illustrated in <ref>. Quantitative results are given in Table <ref>, which indicates that Half-AdaIN contributes to the performance gain. \n\n\n\n\n3) Effect of CWSI: \nWe develop CWSI to effectively control the texture of face component regions. We remove all CWSI in our model as a baseline (w/o CWSI). Fig. <ref> shows that the results generated by baseline are inconsistent with the reference image in texture style. This shows that CWSI can effectively extract the texture information of pre-defined regions from the reference image and inject them into the corresponding regions of the inpainted image. \n\n\n\n\n4) Effect of Training Scheme:\nThree-mode Training (TMT) scheme is designed for more efficient generation of intermediate segmentation maps and accurate injection of texture features into corresponding regions. We set a baseline (\"w SMT\") using Single-Mode Training scheme.\nWe apply the masks similar to those of segmentation mode\n, so the outputs of the segmentation branch should be the same as ground truth segmentation maps. Then, we evaluate the performance of the segmentation branch with the resolution of 128 \u00d7 128 for the two different training schemes. \nSome visual results are shown in Fig. <ref>(a). It is clear that SMT produces more false predictions on small face component regions, especially in mouth and eye regions. Quantitative results in Table <ref> show that TMT helps the generator significantly improve the segmentation branch's performance in terms of Mean Intersection over Union (mIoU) and Pixel Accuracy (Acc). \nWe demonstrate some inpainted examples, as shown in Fig. <ref>(b). We observe an apparent texture inconsistency between SMT's results and the reference images in component regions. This inconsistency is caused by SMT not generating accurate segmentation maps to locate face components.\n\n\n\n \u00a7.\u00a7 Cross-Dataset Evaluation\n\nTo demonstrate the generalization of the proposed method, we conduct a cross-dataset evaluation on CelebA dataset <cit.>. The generator is trained with the Celeb-ID dataset, where we utilize a pre-trained arcface model <cit.> to extract identity vectors.\nWe randomly selected 2000 identities from CelebA dataset and generated masks with random rates. The Qualitative and quantitative comparisons, as shown in Fig.  <ref> and Table <ref>, reveal that the proposed method can generate more realistic results and achieve better consistency with reference images.\nThe results in Fig. <ref> demonstrate more results of our model on CelebA dataset.\n\n\n\n \u00a7.\u00a7 Failure cases\n\nWe show some failure cases in Figure <ref>. Our model may produce unsatisfactory results if the input images have extreme poses. There are two reasons for this phenomenon: (1) Most training set images are frontal. Images with extreme poses are rare. Therefore, the generator does not have enough training samples. (2) Face images with extreme poses are difficult to inpaint due to their diversity and complexity.\n\n\n\n\n \u00a7.\u00a7 Efficiency\n\nThe average running time per image and FLOPs during testing are listed in Table <ref>. This experiment is performed on a NVIDIA RTX 3090 GPU.\n\n\n\n\n\u00a7 CONCLUSION\n\nIn this paper, we propose a comprehensive framework for reference-guided face inpainting. Our approach can efficiently control the generated results guided by a reference image. To accurately inject two types of control information and produce high-quality results, we gracefully designed modules: Half-AdaIN and CWSI. Half-AdaIN is a variant of AdaIN, which injects identity information while preserving the contextual style of the input face. CWSI can inject component-specific texture information precisely into the corresponding face regions by parsing feature maps. The proposed model can provide separate control for identity and texture over missing regions. Extensive experiments verify the superiority and practicability of our method. However, our method remains limited in processing face images with extreme poses, which needs to be addressed in future work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIEEEtran\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}