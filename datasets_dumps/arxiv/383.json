{"entry_id": "http://arxiv.org/abs/2303.06799v1", "published": "20230313010937", "title": "Gaussian Process on the Product of Directional Manifolds", "authors": ["Ziyu Cao", "Kailai Li"], "primary_category": "cs.LG", "categories": ["cs.LG"], "text": "\n\tObject-Centric Multi-Task Learning for Human Instances\n    Hyeongseok Son, Sangil Jung, Solae Lee, Seongeun Kim, Seung-In Park, ByungIn Yoo\n\nSamsung Advanced Institute of Technology\n\n{hs1.son, sang-il.jung, solae913.lee, se91.kim, si14.park, byungin.yoo}@samsung.com\n\n\n\n\n\n\n\n\n\n\n\n    March 30, 2023\n==============================================================================================================================================================================================================================\n\nempty\n\tempty\n\t\n\t\n\t\tWe present a principled study on establishing Gaussian processes over variables on the product of directional manifolds. As a basic functional component, a manifold-adaptive kernel is presented based on the von Mises distribution for Gaussian process regression on unit circles. Afterward, a novel hypertoroidal von Mises kernel is introduced to enable topology-aware Gaussian processes on hypertori with consideration of correlational circular components. Based thereon, we enable multi-output regression for learning vector-valued functions on hypertori using intrinsic coregionalization model and provide analytical derivatives in hyperparameter optimization. The proposed multi-output hypertoroidal Gaussian process is further embedded to a data-driven recursive estimation scheme for learning unknown range sensing models of angle-of-arrival inputs. Evaluations on range-based localization show that the proposed scheme enables superior tracking accuracy over parametric modeling and common Gaussian processes.\n\t\n\t\n\t\n\n\u00a7 INTRODUCTION\n\n\tInference on random variables belonging to directional manifolds appears in various control-related areas such as signal processing, robotics, and computer vision\u00a0<cit.>. Typical examples include the unit circle ^1\u2282^2, hypertorus ^3^1\u00d7^1\u00d7^1\u2282^6 (\u00d7 denotes the Cartesian product), and hyperspheres ^d-1\u2282^d (d\u22653), which are by nature topologically nonlinear and periodic. Consequently, it is nontrivial to model functions over directional quantities with theoretically-sound stochastic interpretation. Some adaptation measures have been introduced, e.g., based on local linearization, in order to apply conventional probabilistic schemes. However, this may induce poor inferential performance, in particular, on modeling stochastic systems under large uncertainties or fast transitions\u00a0<cit.>.\n\t\n\tRecent advances in directional statistics have provided new tools for manifold-adaptive uncertainty quantification on directional domains\u00a0<cit.>. Popular choices include the wrapped normal and the von Mises distributions on the unit circle, and the von Mises\u2013Fisher distribution on hyperspheres\u00a0<cit.>. For modeling directional quantities of antipodal symmetry, e.g., unit quaternions on ^3\u2282^4, the Bingham distribution is commonly used and has been widely exploited in reasoning and inference on uncertain spatial orientations\u00a0<cit.>. As for establishing probabilistic schemes on composite directional domains, one important aspect is to interpret correlation across manifold components\u00a0<cit.>. Former works were shown on the torus for temporal modeling of correlated wind directions\u00a0<cit.> and on the manifold of unit dual quaternions for spatial pose inference\u00a0<cit.>. \n\t\n\tThe aforementioned tools, though topology-aware, cannot be trivially applied to modeling stochastic processes over directional manifolds. In this regard, considerable effort has been dedicated to adapting Gaussian processes (GPs) underlying topologies. One general methodology is to exploit Riemannian geometry to determine the kernel function implicitly. By solving a stochastic partial differential equation incorporating the Laplace\u2013Beltrami operator, kernel functions can be obtained on nonlinear manifolds\u00a0<cit.>. Methods were proposed in\u00a0<cit.> to derive Mat\u00e9rn kernels on Riemannian manifolds using manifold Fourier feature expansions based on the Laplace\u2013Beltrami eigenpair and its spectral measure. The approach was further adopted in\u00a0<cit.> for modeling tangential vector fields with tailored kernels on Riemannian manifolds using projective geometry. Such a methodology is generally applicable to developing kernels on Riemannian manifolds, however, theoretically complex and computationally expensive (e.g., performing eigendecomposition of the Laplace\u2013Beltrami operators), inducing considerable gap towards its usage in engineering practices.\n\t\n\tAlternatively, manifold-adaptive kernels can be explicitly designed the underlying manifold structure. This can lead to a conciser procedure of establishing GPs. However, simply replacing the distance metric with the geodesic to generalize Gaussian kernels on nonlinear manifolds does not lead to positive definiteness\u00a0<cit.>. Instead, a principled prerequisite was given in\u00a0<cit.> that Laplacian kernels are eligible if and only if the distance metric is conditionally negative definite. In\u00a0<cit.>, kernels have been introduced on symmetric and periodic domains, and basic kernel combinations were provided based on summation or multiplication to handle complex topologies. For GP regression on cylindrical surface (^1\u00d7^1), a kernel function was proposed in\u00a0<cit.> by multiplying a sinusoidal kernel and a Laplacian kernel of \ud835\udcc1^1-norm. The approach was further applied to spatiotemporal modeling of meteorological fields with periodicity. In\u00a0<cit.>, various kernel functions were investigated based on hyperspherical geometry for regression on unit quaternions and unit dual quaternions with applications to spatial pose inference.\n\t\n\tThe aforementioned works showcased adapting GPs to nonlinear domains with topology-compatible kernels. However, no systematic study has been done for establishing GP schemes on the product of directional manifolds, in particular, on hypertori. Modeling stochastic processes over hypertoroidal variables refers to numerous real-world applications. Almost all mechanical systems such as ground/aerial vehicles are maneuvered by circular inputs from a certain number of rotational motors. Spatial motions of a mobile agent can thus be modeled as stochastic processes of hypertoroidal inputs.\n\t\n\t\n\n \u00a7.\u00a7 Contribution\n\n\tWe provide a principled study on establishing GPs over random variables on directional manifolds composing unit circles. Under the framework of (multi-output) Gaussian process introduced in sec:preliminaries, we first introduce the von Mises kernel for quantifying similarity of circular data in a topology-adaptive manner. Afterward, a novel hypertoroidal von Mises (HvM) kernel function is proposed for multi-output GPs on the hypertorus with corresponding closed-form derivatives in hyperparameter optimization (sec:gpcircle). We apply GPs using the proposed HvM kernel to learning unknown measurement models of angle-of-arrival inputs for data-driven recursive position estimation in sensor networks. Comparisons with parametric modeling and GPs using kernel multiplication show that the proposed scheme enables superior tracking performance in terms of accuracy and robustness (sec:evaluation).\n\t\n\t\n\n\u00a7 GAUSSIAN PROCESS REGRESSION AND INFERENCE\n\n\t\n\n \u00a7.\u00a7 Gaussian Process Modeling\n\n\tA Gaussian process is a collection of random variables on a certain domain. Any finite set of these random variables follows a multivariate Gaussian distribution. For instance, a GP of scalar output is denoted as ()\u223c(m(), (, ')), with m() and (, ) being the mean and covariance functions of (), respectively\u00a0<cit.>. Suppose there is an arbitrary function that is observed under uncertainty following =r()+ with noise \u223c(0,\u03c3_r^2). Given a set of n input locations and corresponding observations corrupted by noise, {(_\u2219,i,z_i)}_i=1^n, a GP yields a posterior of the function value at test locations {_\u2218,i}_i=1^m in the form of a Gaussian distribution _\u2218|{(_\u2219,i,z_i)}_i=1^n,{_\u2218,i}_i=1^m\u223c(_\u2218,_\u2218) ,\n\twith the mean and covariance given by\n\t\n    _\u2218 = _\u2218\u2219^-1 _\u2218=_\u2218\u2218-_\u2218\u2219^-1_\u2219\u2218 ,\n\n\trespectively\u00a0<cit.>, where =_\u2219\u2219+\u03c3_r^2_n. Element in matrix _\u2219\u2218\u2208^n\u00d7m at entry (i,j) is the covariance given by the kernel function evaluated at training and test locations (_\u2219,i,_\u2218,j). Matrices _\u2219\u2219\u2208^n\u00d7n, _\u2218\u2219\u2208^m\u00d7n can be obtained analogously. Vector =[z_1, \u22ef,z_n]^\u22a4 collects all observations, and _n\u2208^n\u00d7n is an identity matrix. Further, the set of predicted observations at test locations follows the distribution (_\u2218,_\u2218+\u03c3_r^2_m).\n\t\n\t\n\n \u00a7.\u00a7 Multi-Output Gaussian Processes\n\n\tFor modeling vector-valued functions expressed as =()+, where \u223c(_d,) is the noise term and =(\u03c3_r,1^2,\u22ef, \u03c3_r,d^2), the multi-output Gaussian process can be exploited. It takes the form ()\u223c((),(, ')), with ()\u2208^d being a multi-output function collecting the mean of each dimension and (, ') the matrix-valued covariance function. To obtain a valid (, '), one popular method is the intrinsic coregionalization model. This leads to\n\t\n    (, ') = (, ')  ,\u2208^d\u00d7d\n\n\tbeing a coregionalization matrix that is positive semidefinite\u00a0<cit.>. Given uncertain observations {_i}_i=1^n collected at training locations {_\u2219,i}_i=1^n, the posterior at a test location _\u2218 follows _\u2218|{(_\u2219,i,_i)}_i=1^n,_\u2218\u223c(_\u2218,_\u2218). Here, the mean and covariance are calculated according to the general form in (<ref>), with components newly defined as _\u2218\u2219=\u2297_\u2218\u2219, _\u2218\u2218= (_\u2218,_\u2218) and =\u2297_\u2219\u2219+\u2297_n. Note that _\u2219\u2219 still denotes the kernel matrix at training locations as given in (<ref>). Furthermore,\tvector  is obtained through the vectorization =([ _1,\u22ef,_n ]^\u22a4)\u2208^nd, and vector _\u2218\u2219=[(_\u2218,_\u2219,1),\u22ef,(_\u2218,_\u2219,n)]. Similarly to the scalar case, the predictive distribution of the observation at _\u2218 is obtained as _\u2218\u223c(_\u2218, _\u2218 + ).\n\t\n\t\n\n\u00a7 GAUSSIAN PROCESSES ON THE HYPERTORUS\n\t\n\t\n\n \u00a7.\u00a7 A Circular Kernel Based on the von Mises Distribution\n\n\tWe now consider a scalar-valued function r() defined on the unit circle, namely, r:^1\u2192 for GP modeling. In order to quantify the similarity between two circular inputs ,\u2208^1, we design a kernel function _vM(,)=\u03c9^2exp(\u03bb ^\u22a4) based on the von Mises distribution\u00a0<cit.>, with \u03c9 controlling signal variance and \u03bb> 0 the concentration. The kernel inherently handles the periodic nature of circular manifolds. To showcase its functionality within GP scheme compared with conventional kernel structures, we demonstrate the following case study.\n\t\n\t\tWe synthesize a function on the unit circle in the form of a mixture of typical circular distributions. It has four components of various configurations and is observed via z=\u2211_i=1^4w_if_circ^i()+\u03f5, with \u03f5\u223c (0,0.0025) being an additive noise. We embed the squared exponential (SE) kernel (distance metric \ud835\udcb9=\u03b8-\u03b8^', with \u03b8 and \u03b8^' denoting the angular positions of  and , respectively) and the proposed von Mises kernel (vM) into the same GP scheme as introduced in subsec:gp for regression over the whole circular domain. Shown in fig:s1gp-(A), the SE kernel produces discontinuous curves due to the aperiodic distance metric of Euclidean geometry. In contrast, the proposed vM kernel quantifies periodic similarity adaptively to circular geometry, inducing identical posteriors with period of 2\u03c0 as plotted in fig:s1gp-(B).\n\t\n\t\n\t\n\t\n\n \u00a7.\u00a7 The Hypertoroidal von Mises Kernel\n\n\tWe now aim to establish GPs on the hypertorus ^3\u2282^6. In order to develop a kernel function on domains built upon the Cartesian product, the common technique is to multiply corresponding kernels of each circular component\u00a0<cit.>. However, this disregards similarities lying in correlations across the components. Therefore, it is imperative to design a topology-adaptive kernel function that quantifies hypertoroidal similarities in a unified manner. For that, we propose the so-called hypertoroidal von Mises (HvM) kernel defined as\n\t\n    _HvM(,)=\u03c9^2exp(^\u22a4(,)+(,)^\u22a4(,)) ,\n\n\tfor an arbitrary pair of inputs =[ (^1)^\u22a4,(^2)^\u22a4,(^3)^\u22a4 ]^\u22a4, =[ (^1)^\u22a4,(^2)^\u22a4,(^3)^\u22a4 ]^\u22a4\u2208^1\u00d7^1\u00d7^1 expressed component-wise. Metric (,)=[ (^1)^\u22a4^1,(^2)^\u22a4^2,(^3)^\u22a4^3 ]^\u22a4 measures component-wise distances according to circular geometry. Similar to the von Mises kernel, \u03c9 controls signal variance.\tThe first term in the exponent of (<ref>) interprets concentration on each circular component via a positive-valued vector = [ \u03bb_1,\u03bb_2,\u03bb_3 ]^\u22a4. The second term allows for modeling correlations between pairs of circular components through the quadratic formulation with a nonnegative and symmetric weighting  matrix =[0   a_1   a_3\na_1   0   a_2\na_3   a_2   0]\u2208^3\u00d73. Note that both terms guarantee the periodic nature of hypertoroidal manifolds and symmetry of the kernel. The following case study further provides a concrete example of the HvM kernel based on a degenerate case.\n\t\n\t\n\t\tWe configure the kernel function in (<ref>) the torus ^1\u00d7^1\u2282^4 with four sets of parameters {(\u03c9_i,_i,_i)}_i=1^4, where\n\t\t\n\t\t\t\n  * \u03c9_1=\u03c9_2=\u03c9_3=\u03c9_4=1 ,\n\t\t\t\n  * _1=_2=[ 0.3,0.3 ]^\u22a4, _3= _4=[ 1,1 ]^\u22a4 , and \n\t\t\t\n  * _1=_3 = _2\u00d72, _2=[0   0.3\n0.3   0] , _4=[0   1\n1   0] .\n\t\t \n\t\tGiven one input =[ 1,0,1,0 ]^\u22a4 at angular locations (0,0) on each circle, the kernel function _HvM (,) is evaluated at =[ cos(\u03b1),sin(\u03b1),cos(\u03b2),sin(\u03b2) ]^\u22a4 angles (\u03b1,\u03b2). \n\t\t\n\t\tShown in fig:hvm-(A) to (D), the resulted kernel functions are plotted corresponding to the parameter sets 1 to 4 above. In all cases, the kernel function indicates the largest similarity at (\u03b1,\u03b2)=(0,0), namely, = . Correlations between circular components are inherently interpreted by the nonzero off-diagonal elements in _2 and _4 as plotted in fig:hvm-(B) and (D), respectively. When =_2\u00d72, as illustrated by fig:hvm-(A) and (C), the proposed hypertoroidal kernel function degenerates to a multiplication of von Mises kernels, which disregards the correlation between the two angular pairs (\u03b1,\u03b2) and (0,0).\n\t\n\t\n\t\n\t\n\t\n\t\n\n \u00a7.\u00a7 Hyperparameter Optimization\n\n\tVarying the free parameters, e.g., those in the kernel functions, has a considerable impact on the performance of GP regression and inference. In general, these hyperparameters  can be obtained via maximum likelihood estimation following ^*=_(). The objective is derived in the form of log marginal likelihood \n\t\n    ()   =2logp(|{_\u2219,i}_i=1^n,)\n       =-^\u22a4^-1-log||-nlog(2\u03c0) ,\n\n\twith  and  specified according to sec:preliminaries. In practice, the nonlinear optimization problem above is solved numerically with the kernel matrix evaluated in each iteration. The gradient of the objective the i-th element in  takes the following general form \n\t\n    ()\u03b8_i=^\u22a4^-1\u03b8_i^-1 -tr(^-1\u2202/\u2202\u03b8_i) ,\n\t\n\twith tr denoting the trace of a matrix.\n\t\n\tAs for multi-output GPs using the proposed hypertoroidal von Mises kernel, the hyperparameters can be collected into =[ \u03c9,^\u22a4,^\u22a4, ^\u22a4,_r^\u22a4 ]^\u22a4. The first three components, \u03c9, , and =[ a_1,a_2,a_3 ]^\u22a4 are free parameters in (<ref>). = vec() denotes the vectorized coregionalization matrix in (<ref>). And _r=[ \u03c3_r,1,\u22ef,\u03c3_r,d ]^\u22a4 indicates observation noise variance on each output dimension according to subsec:mgp.\n\t\n\tComputing (<ref>) boils down to deriving the derivative of () hyperparameters in , which we express element-wise now for clear explanation. The derivative of the kernel matrix the signal deviation \u03c9 follows \u03c9=\u2297_\u2219\u2219\u03c9=2\u2297_\u2219\u2219/\u03c9, where _\u2219\u2219 denotes the kernel matrix of training sets as introduced in subsec:mgp. Further, the derivative of  each element in the concentration vector  follows \u03bb_s=\u2297_\u2219\u2219\u03bb_s=\u2297(_\u2219\u2219\u2299^s_\u2219\u2219), with s\u2208{1,2,3} being the index of the s-th circular component of the hypertorus. Correspondingly, elements in ^s_\u2219\u2219\u2208^n\u00d7n follows (^s_\u2219\u2219)_ij(_\u2219,i^s)^\u22a4^s_\u2219,j, which interprets distance between training inputs on each circle, and \u2299 denotes the Hadamard product. As for differentiation on the nonzero elements in matrix , we have a_s=\u2297_\u2219\u2219a_s=2\u2297(_\u2219\u2219\u2299^s_\u2219\u2219\u2299^s  3+1_\u2219\u2219), with s\u2208{1,2,3}. The derivative of  the s-th element in the vectorized coregionalization matrix[Thus, b_s=()_ij, with s=d (j-1)+i\u2208{1,\u22ef,d^2}.] takes the form b_s=(\u2297_\u2219\u2219)b_s=(b_s)\u2297_\u2219\u2219=_ij\u2297_\u2219\u2219, with _ij\u2208^d\u00d7d denoting a matrix unit. The derivative of  observation deviation \u03c3_r,s of each output domain s\u2208{1,\u22ef,d} can be derived in a similar fashion. It is given by \u03c3_r,s=(\u2297_n)\u03c3_r,s=2\u03c3_r,s_ss\u2297_n , where _ss\u2208^d\u00d7d is a  matrix unit. In practice, we exploit the trust-regions method from Manopt\u00a0<cit.> for maximizing the objective in (<ref>).\n\t\n\t\n\t\n\t\n\n\u00a7 EVALUATION\n\n\tWe now evaluate the proposed HvM kernel within the particle filtering scheme, where unknown range sensor network is learned by hypertoroidal GPs taking angle-of-arrival inputs.\n\t\n\n \u00a7.\u00a7 Scenario Setup\n\n\tWe set up a two-dimensional area of 30\u00d730 m^2, where a mobile agent is tracked while moving along various trajectories. We have the following system dynamics according to a random walk \n\t\n    _t+1=_t+_t ,\n\n\twith _t, _t+1\u2208^2 denoting positions and _t\u2208^2 the process noise following _t\u223c(_2,_t). A range sensor, e.g., of ultrasonic or ultra-wideband modalities, is equipped onboard the platform observing distances to three reference points of coordinates {_s^r}_s=1^3\u2282^2 under uncertainty. This can be formulated as\n\t\n    _t=(_t)+_t+_t ,\n\n\twhere _t\u2208^3 and _t\u2208^3 denote the range measurement and corresponding noise, respectively, and _t\u223c(_3,_t). Function (_t)[\u2016_1^r-_t\u2016,\u2016_2^r-_t\u2016,\u2016_3^r-_t\u2016]^\u22a4 gives range readings of position state _t the three reference points. Moreover, a time-varying offset _t\u2208^3 is added to the range signal to include possible interference in sensor networks such as clock drift or signal reflection\u00a0<cit.>. For the sake of demonstration, we assume that it is proportional to the range signal (_t) by a ratio of c=0.05.\n\t\n\tWe now assume that the uncertainty in range observations is unknown. However, we are able to obtain accurate angle-of-arrival (AoA) readings from each reference point 24 \u00d7 10 grid points {_i}_i=1^n=240 that are uniformly spaced on the xy-plane. Meanwhile, range measurements given by the onboard sensor are collected, inducing a training set of {(_i,_i)}_i=1^n, where _i=[ (_i^1)^\u22a4,(_i^2)^\u22a4,(_i^3)^\u22a4 ]^\u22a4\u2208^3\u2282^6 denotes the AoA reading of each reference point grid point _i. Such a scenario occurs commonly when there exists no positioning system (e.g., camera networks) covering the whole tracking space, whereas certain portable devices such as total stations can be easily deployed to provide accurate relative angles between two locations\u00a0<cit.>. For nonparametric measurement modeling, we reformulate (<ref>) into _t=(_t)+_t, with _t\u2208^3 being the hypertoroidal input of AoA and (_t)(1+c)(_t) the observation modeled by a multi-output GP.\n\t\n\t\n\n \u00a7.\u00a7 GP-Based Particle Filtering\n\n\tWe apply particle filtering to tracking the mobile agents set up in subsec:sce\u00a0<cit.>. Given a particle _t|t-1 drawn at timestamp t from the prior estimate[Without specific need, we omit particle index for brevity.], we first compute the corresponding hypertoroidal state _t|t-1\u2208^3, where each circular component is given by _t|t-1^s=[ (_s^r-_t|t-1)_x,(_s^r-_t|t-1)_y]^\u22a4/\u2016_s^r-_t|t-1\u2016 each reference point indexed by s\u2208{1,2,3}. We further inquire the pre-trained hypertoroidal GP (using training data {(_i,_i)}_i=1^n) at _t|t-1 to predict corresponding range measurement distribution, i.e.,\n\t\n    _t|{(_i,_i)}_i=1^n,_t|t-1\u223c(_t|t-1,_t|t-1 + ) .\n\n\tBased thereon, the likelihood function is directly obtained for reweighting the prior particle _t|t-1 given current measurement _t. Afterward, particles are updated via resampling.\n\t\n\t\n\n \u00a7.\u00a7 Evaluation and Results\n\n\tWe equip the GP-based particle filter in sec:filter with the proposed HvM kernel. For comparison with existing kernel design on the hypertorus, we instrument the product squared exponential (PSE) and the product von Mises (PvM) kernels on hypertorus through kernel multiplication of the SE and vM kernels on the unit circle, respectively\u00a0<cit.>. All GPs of different kernels are trained using the same data set collected as introduced in subsec:sce. Furthermore, we approximate range measurement noise with a three-dimensional Gaussian distribution using the same training data to provide likelihood functions of parametric form. We set the process noise in (<ref>) with covariance _t=(0.16,0.16) and the measurement noise in (<ref>) with covariance _t=\u03be^2_3, where \u03be\u2208{0.01,0.05,0.1} controls the level of uncertainty. We deploy 100 particles to all particle filters for 1000 time steps and perform evaluation based on 100 Monte Carlo runs for each trajectory and each noise level.\n\t\n\tShown in fig:error, we collect evaluation results given by the particle filters configured above on all sequences throughout different trajectories (as plotted in T1 to T3 of fig:exa) and measurement noise levels (S1-S3 \u03be). The proposed HvM kernel delivers superior tracking accuracy of particle filtering via GP reweighting over the PvM and PSE kernels, and the parametric approximation method. Exemplary runs of HvM-GP-based particle filtering on the three trajectories are shown in fig:exa with noise level set at \u03be=0.01. On sequence S1-T2 (same noise level), we further plot in fig:2Dtraj representative estimates of particle filtering with likelihoods given by the parametric model and hypertoroidal GPs using kernel multiplication. The PvM kernel delivers better accuracy compared with the parametric method, whereas the PSE kernel fails tracking due to disregard of periodic nature of hypertori. Further comparison with T2 in fig:exa showcases the superior tracking performance from the HvM-GP-based particle filter.\n\t\n\tTo bring more insights to the functionality of the proposed HvM kernel on particle reweighting, we demonstrate in fig:comp range measurement distributions predicted by the parametric model and GPs using HvM and PSE kernels over time. The proposed HvM-GP produces data-adaptive predictive range distributions. The parametric method fails at modeling the time-varying noise pattern, and the PSE kernel induces discontinuity when periodicity in inputs takes effect.\n\t\n\t\n\n\u00a7 CONCLUSIONS\n\n\tWe provide a novel study on establishing multi-output GP schemes on the hypertorus. The von Mises kernel is first introduced to quantify similarity of inputs on ^1 in a geometry-adaptive manner. Based thereon, we propose the so-called hypertoroidal von Mises (HvM) kernel for topology-aware GP modeling on ^3\u2282^6. It computes covariance over pairs of hypertoroidal inputs by considering similarities lying on each circular component as well as the correlated area. Derivatives of marginal likelihood the hyperparameters are provided for efficient GP training. The proposed HvM-GP is embedded in a particle filter for data-driven likelihood reweighting given angle-of-arrival inputs. Simulations show that the proposed HvM-GP produces data-adaptive measurement predictions, leading to superior tracking performance over the parametric model and GPs using kernel multiplication.\n\t\n\tFor future work, the efficiency of the proposed method can be improved by reducing the computational complexity, e.g., via sparse Gaussian processes\u00a0<cit.>. In addition, the proposed HvM-GP is to be exploited in real-world scenarios for data-driven modeling of complex stochastic systems with rotational inputs\u00a0<cit.>. Also, the HvM kernel can be generalized to fit higher-dimensional spaces for more extensive applications. \n\t\n\tIEEEtran\n\t\n\t\n"}