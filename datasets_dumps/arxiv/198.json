{"entry_id": "http://arxiv.org/abs/2303.07080v1", "published": "20230313130533", "title": "Bag of Tricks with Quantized Convolutional Neural Networks for image classification", "authors": ["Jie Hu", "Mengze Zeng", "Enhua Wu"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\nEvolution of many-body systems under ancilla quantum measurements\n    Dmitry G.\u00a0Polyakov\n    March 30, 2023\n=================================================================\n\n\n\n\n\n\t\n\tDeep neural networks have been proven effective in a wide range of tasks. However, their high computational and memory costs make them impractical to deploy on resource-constrained devices. To address this issue, quantization schemes have been proposed to reduce the memory footprint and improve inference speed. While numerous quantization methods have been proposed, they lack systematic analysis for their effectiveness. \n    To bridge this gap, we collect and improve existing quantization methods and propose a gold guideline for post-training quantization. We evaluate the effectiveness of our proposed method with two popular models, ResNet50 and MobileNetV2, on the ImageNet dataset. By following our guidelines, no accuracy degradation occurs even after directly quantizing the model to 8-bits without additional training. \n    A quantization-aware training based on the guidelines can further improve the accuracy in lower-bits quantization. Moreover, we have integrated a multi-stage fine-tuning strategy that works harmoniously with existing pruning techniques to reduce cost even further. Remarkably, our results reveal that a quantized MobileNetV2 with 30% sparsity actually surpasses the performance of the equivalent full-precision model, underscoring the effectiveness and resilience of our proposed scheme.\n\n\n\n\nModel Quantization, Acceleration, Convolutional Neural Networks, Image Classification \n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nSince the introduction of AlexNet\u00a0<cit.>, there has been an exponential increase in the number of exceptional convolutional neural networks proposed, resulting in promising outcomes for a variety of visual tasks\u00a0<cit.>. Despite the remarkable results, deploying CNN models on embedded or mobile devices proves challenging as it poses an immense burden on computation and memory storage. To address this issue, a significant amount of research has been dedicated to reducing associated costs, thereby making CNN models more practical for real-world applications. Broadly speaking, this line of research can be categorized into three distinct areas: efficient structure design, network pruning, and network quantization.\n\nEfficient structural design is a challenge in research, with introduction of the separated convolution\u00a0<cit.> proposed as an effective technique. This method factorizes the standard convolution into a depthwise and pointwise convolution, reducing computation. Successful examples of its use in efficient networks include MobileNets\u00a0<cit.> and ShuffleNets\u00a0<cit.>. These networks are widely used on resource-constrained devices and have shown promise in practical applications.\nBesides that, various pruning strategies\u00a0<cit.> have also been proposed to reduce both the computational and storage burdens. However, these methods often incur accuracy degradation, making them less attractive for practical applications. As such, the focus has shifted towards developing efficient structural design methods that can provide high accuracy without compromising on computational efficiency.\n\nQuantization is an incredibly efficient method for deploying models. Its effectiveness has been demonstrated in recent work\u00a0<cit.> and has made it increasingly popular in the industry due to its hardware-friendly properties, which can significantly reduce computational and memory costs. However, despite its advantages, there exists a significant accuracy gap between a full-precision model and a quantized counterpart. This gap is especially pronounced in low-bitwidth quantization situations (e.g. 4-bits). Nevertheless, researchers are actively working on closing this gap and making quantization even more effective.\n\nThis paper presents a systematic exploration of the effects of each quantization factor on convolutional neural networks, forming a gold guideline for post-training quantization. Additionally, our proposed multi-stage fine-tuning strategy enables quantization to work in conjunction with existing pruning strategies. Exhaustive experiments with ResNet-50 and MobileNetV2, quantized using different bitwidths on ImageNet, showcase the effectiveness of our proposed method.\n\n\n\n\u00a7 QUANTIZATION SCHEME\n\n\nIn this section, we have collected, reviewed and improved upon existing methods for obtaining accurate quantized neural networks. We then conducted experiments to examine the effectiveness of these methods using two representative models, ResNet50\u00a0<cit.> and MobileNetV2\u00a0<cit.>, on the ImageNet dataset\u00a0<cit.>. The bitwidth of weights and activations is set to 8, unless stated otherwise in the experiments.\n\nThe ImageNet classification task is widely recognized as a challenging benchmark for testing the effectiveness of quantization algorithms. This dataset consists of 1.28M training images and 50K validation images. During the full-precision training, we utilized the random-size cropping strategy\u00a0<cit.>, which is then followed by random flipping and mean channel subtraction. Our mini-batch size is set to 256, and we use a momentum SGD optimizer. For ResNet50, the initial learning rate is set to 0.1, and we decay it by a factor of 10 at the 30th, 60th, and 90th epoch, respectively, for a total of 100 epochs. For MobileNetV2, the initial learning rate is also 0.1, and we decay it by a factor of 10 at the 90th, 180th and 270th epoch, for a total of 300 epochs.\n\n\n\n\n\n\n \u00a7.\u00a7 Post-training Quantization\n \n\n\nDetermining the optimal quantization scale. \nOne of the main challenges in conducting efficient quantization is determining the optimal quantization scale. Choosing a large scale for a given bitwidth allows for wider representation which can avoid data overflow, but also suffers from more distribution errors. Conversely, selecting a smaller scale can reduce the quantization error in most cases, but it may lead to data overflow due to a narrow data range, resulting in numerical calculation errors. To \ntackle this, NVIDIA's TensorRT\u00a0<cit.> employs a post-training quantization approach that sweep quantization scales by minimizing the Kullback-Leibler (KL) divergence between featuremaps distribution before and after quantization. This approach has shown excellent results in quantizing popular CNN models such as ResNet and MobileNet without the need for additional fine-tuning.\n\nIn the context of mathematical statistics, the KL-divergence is a measure of the difference between two probability distributions. Therefore, it can be used to determine the appropriate quantization scale by minimizing the KL-divergence between real values and their quantized counterparts. However, we have found through experimentation that directly using the scale corresponding to the minimal KL-divergence often results in inferior performance. Instead, we have discovered that the optimal scale that yields superior results is typically larger than the scale with the minimal KL-divergence. \nHerein, we propose the introduction of a tolerance coefficient T (T\u22651) to improve the performance of quantization in CNNs. Rather than selecting the scale with minimal KL-divergence, we sweep a range of scale candidates and choose the maximum value whose corresponding KL-divergence is less than T times the minimal KL-divergence. \n\nTable\u00a0<ref> presents the results of our method. For simplicity, we set a global tolerance parameter, T, for all layers in the network. A value of 1.0 for T corresponds to using the conventional KL-divergence algorithm for featuremap quantization, which selects the quantization scale with minimal KL-divergence and serves as a baseline for comparison. As T increases, we observe significant performance improvement for both ResNet50 and MobileNetV2. However, if T is set to a very high value, it degenerates into the naive symmetric MinMax quantization, which simply selects the maximum value of a tensor as the quantization threshold. We found that setting T to 1.3 yields a significant improvement over most models.\n\n\n\n\n\n\n\nGranularity of quantization. \nIn general, using a single quantization scale for the entire tensor often yields satisfactory results. However, a recent study by Jacob et al.\u00a0<cit.> has shown that leveraging separate scales for each kernel within the convolutional weights can yield excellent performance, without adding to the computational complexity when deploying the model on hardware. In the literature, weight quantization in convolutional layers is classified into two types: layer-wise and channel-wise quantization. While only layer-wise quantization is allowed for activations, channel-wise quantization can be applied to convolutional weights.\n\nTable\u00a0<ref> presents a comparison of layer-wise and channel-wise quantization for 8-bits and 7-bits, respectively. The results show that channel-wise quantization consistently outperforms layer-wise quantization in all cases, which is to be expected as it offers greater flexibility with less distribution error. Additionally, the performance gap becomes more pronounced as the bitwidth decreases. For instance, in 8-bits quantization, MobileNetV2 achieves a top-1 accuracy of 71.45% with channel-wise quantization, which outperforms the layer-wise variant by 0.54%. However, the gap widens to 2.48% in 7-bits quantization. These results demonstrate the superiority of channel-wise quantization for weights in neural networks.\n\n\n\n\n\n\nUnsigned quantization: ReLU activation is commonly employed in modern architectures to combat the issue of gradient vanishing. In ReLU, negative activations are clipped to zero. When quantizing the featuremap after ReLU, unsigned quantization can be applied to improve the representation of positive values by discarding the negative ones. Essentially, this approach is equivalent to adding an additional bit to the quantization of positive values, which effectively doubles the range of representation.\n\nTo assess the effectiveness of unsigned quantization, we compared its results with signed quantization. As shown in Table\u00a0<ref>, we observed that the type of quantization had a significant impact on efficient networks. With the use of unsigned quantization, MobileNetV2 achieved a top-1 accuracy of 71.94%, which is equal to the accuracy of full-precision training. On the other hand, the variant with signed quantization only achieved a top-1 accuracy of 71.28%. Therefore, the inclusion of unsigned quantization for positive activations is crucial to achieve better accuracy at no extra cost.\n\n\n\n\n\n\nQuantization Placement \nTypically, quantization follows convolutional layers, with or without activation layers. In residual-like networks that have shortcut connections via element-wise summation, it is common to quantize both the output activations of the residual branch and the features after summation. However, we have observed that performing quantization at the end of the residual branch can significantly degrade performance. This is because the quantization scale of the two input features needs to be consistent in the addition case. We consider this a hard constraint that can cause significant accuracy degradation. Therefore, we prefer to keep floating-point arithmetic (e.g., half-precision) in element-wise summation and use it in all of our experiments.\n\n\n\n\nAccumulation in INT16. \nTo avoid data overflow in the accumulation process of convolution, it is common practice to use the INT32 data type for storing intermediate accumulation results, even though weights and activations are quantized to 8-bits or fewer bitwidth. However, to further reduce the latency and memory footprint, we propose using the INT16 data type for accumulation when the summation of the bitwidth of weights and activations is 14 or less. In our settings, we quantize the weights of convolutions to 6 bits and the activations to 8 bits, which meets this requirement. \n\nTable\u00a0<ref> presents the results. We find that the accuracy is almost retained when replaced with INT16 accumulation in quantized MobileNetV2, which validates the efficacy of using INT16 for accumulation.\n\n\n\n\n\n\nGuideline for Post-training quantization. In summary, to improve accuracy for post-training quantization, we propose the following guidelines:\n\n\n\n\t\n  * Use an improved KL-divergence algorithm with a tolerance coefficient to determine the scales of activations, and apply MinMax quantization to determine the scales of weights. \n\n\t\n  * Use channel-wise quantization for weights, which is superior to layer-wise quantization, especially for efficient convolutions. \n\n\t\n  * Employ unsigned quantization on positive activations, such as the output of ReLU. \n\n\t\n  * Eliminate quantization for the inputs of addition in a network whenever possible, as this can result in significant gains with only a slight increase in computation.\n\n\n\n\nThe results of post-training quantization are shown in Table\u00a0<ref>. For comparison, we used the commercial, closed-source inference library TensorRT and report the average results of 8 experiment runs that used different calibration data. By following the guidelines outlined above, we were able to achieve full-precision accuracy levels in both ResNet50 and MobileNetV2 when quantized to 8 bits. In fact, our approach outperformed TensorRT by a small margin.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Quantization-Aware Training\n\n\n\n\n\nBatch Normalization folding. Batch Normalization (BN)\u00a0<cit.> is a crucial module that enhances the stability of training and facilitates optimization of deeper neural networks with minimal weight initialization requirements. During training, a BN operation is supposed to be a linear operation that can be absorbed into the previous convolutional layer. \n\nTo enable quantization-aware training, we need to implement BN folding to determine the optimal scale during forward pass. The gradients are then back-propagated in an unfolding manner, allowing us to separately update the previous convolutional weights as well as the scales and shifts of BN. The parameter folding strategy is formulated as below:\n\n    \u0174 = \u03b3 * W\u221a(\u03b4^2+\u03f5), B\u0302 = \u03b2 - \u03b3 * \u03bc\u221a(\u03b4^2+\u03f5),  S_\u0174 = \u03b3 * S_W\u221a(\u03b4^2+\u03f5)\n\nWhere W as the weights of the previous convolution layer,  \u0174 and B\u0302 as the combined weights and biases. \u03bc, \u03c3^2, \u03b3 and \u03b2 are the mean, variance, scale and shift value of BN, respectively. S_W and S_\u0174 are the quantization scale vector of convolutional weights before and after combination. It's worth noting that S_\u0174 only needs to be executed once after training.\n\n\n\n\nQuantization-Aware Training. \nWhile quantization of the weights and activations in a lower precision, such as 4-bits, the post-training quantization can not maintain the accuracy with a tolerance loss. In this situation, we fine-tune the quantized model to further improve the accuracy by enhancing the fitness and robustness of weights for quantization.\n\nWe first quantize a well-trained full-precision model according to the post-training guideline. Then, we fine-tune it with a modified training procedure that adapts the model to quantization. We disable aggressive random-size cropping and use weaker random cropping augmentation. The models are fine-tuned for 20 epochs with a fixed quantization scale for weights, improving training convergence. The initial learning rate is 5e-4, decayed by 5 at the 10th epoch, achieving comparable performance.\n\nTable\u00a0<ref> shows the comparison results on MobileNetV2. It is challenging to achieve quantization with lower precision, but our method achieves the best performance in 4-bits, 5-bits, and 6-bits quantization, surpassing other methods.\n\n\n\n\n\n\n\n\nIntegration with pruning. We now explore the integration of quantization with network pruning strategies that aim to accelerate inference and reduce memory footprint as well. We utilize a simple non-structured pruning approach that zeroes out the \u201cunimportant\" weights with relatively small absolute values, as the design of the sparsity strategy is not our main focus.\nTo incorporate pruning with quantization, we propose a pipeline that consists of the following steps: Sparsity \u2192 Full-precision Fine-tuning \u2192 Post-Training Quantization \u2192 Quantization-Aware Training. Specifically, we first apply the pruning strategy to a well-trained full-precision model M_1, which maintains its sparsity throughout the \nentire pipeline.\nFollowing the pruning step, we fine-tune the model with a small learning rate, which allows for minor adjustments to the remaining parameters and generates another sparse full-precision model M_2. The next step involves post-training quantization, followed by quantization-aware training, as outlined above. This results in two successive quantized models, namely M_3 and M_4.\n\nWe conducted experiments on the efficient MobilNetV2, and the results are shown in Table\u00a0<ref>. We first trained a full-precision model with a top-1 accuracy of 72.91%, then pruned the model with 0%, 10%, 20%, and 30% sparsity ratios. Interestingly, after the first fine-tuning, we observed that all pruned models outperformed the full-precision baseline, including the model with no pruning (Sparsity=0). This may be attributed to the weakening of data augmentation during fine-tuning, as in quantization-aware training. After quantization-aware training (i.e., the second fine-tuning), we obtained a quantized model with a top-1 accuracy of 73.35% without sparsity, the best performance for an 8-bits MobileNetV2 to date. Additionally, all quantized models with different prune ratios performed better than the full-precision counterpart. These results demonstrate that the quantization scheme can work well with network pruning strategies.\n\n\n\u00a7 CONCLUSION\n\n\nIn this work, we first systematically refine and improve the effective methods for post-training quantization and establish a gold guideline in practice. Following this guideline, the models in 8-bits  quantization can reach the accuracy of full-precision counterparts without the need for additional training. Additionally, we propose a simple and efficient quantization-aware training strategy that improves accuracy even further in lower precision settings. Lastly, we demonstrate that quantization can work in conjunction with network pruning strategies and propose a multi-stage fine-tuning pipeline to chain them together. These contributions pave the way for more efficient and accurate deep neural network models in resource-constrained environments.\n\n\n\n\nIEEEbib\n\n\n\n\n\n\n\n\u00a7 DETAILS FOR DETERMINING ACTIVATION QUANTIZATION SCALE\n\n\nThrough our experimentation, we have found that the optimal scale for superior results is often greater than the scale corresponding to the minimum KL-divergence as depicted in Fig.\u00a0<ref>. Based on this observation, we have enhanced the standard KL divergence algorithm proposed in \u00a0<cit.>, which was introduced earlier in Sec.\u00a0<ref>. A detailed description of our improved algorithm is provided in Algo.\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 ILLUSTRATION FOR QUANTIZATION PLACEMENT\n\n\nAs mentioned before, we prefer to utilize floating-point arithmetic (such as half-precision) for element-wise summation. An example of a quantized bottleneck block in ResNet is illustrated in Fig.\u00a0<ref>.\n\n\n\n\n\u00a7 ELABORATION ON ACCUMULATION IN INT16\n\n\n\n\nIn later Sec.\u00a0<ref>, we have briefly mentioned the accumulation process. Here, we aim to provide more details about this process. Figure\u00a0<ref> illustrates the accumulation process of a quantized convolution, where A, W and O denote the floating-point input activations, weights and output activations of a convolution layer. A quantized convolution operation is performed wherein A and W are initially quantized using quantization scales S_A and S_W to INT8 and INT6 representations, respectively, denoted by A_int and W_int. Conventionally, INT32 is used to store the accumulation to avoid the possibility of data overflow. However, our experiments have revealed that the more memory-efficient INT16 data type suffices to perform this task. To recover the floating-point result, one can simply multiply the accumulation in INT16 with the product of the quantization scales of the activation and weights utilized for their respective quantization.\n\n"}