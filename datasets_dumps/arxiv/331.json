{"entry_id": "http://arxiv.org/abs/2303.06872v1", "published": "20230313054621", "title": "FusionLoc: Camera-2D LiDAR Fusion Using Multi-Head Self-Attention for End-to-End Serving Robot Relocalization", "authors": ["Jieun Lee", "Hakjun Lee", "Jiyong Oh"], "primary_category": "cs.RO", "categories": ["cs.RO", "cs.CV"], "text": "\n\n\n\n\n\n\n\nFusionLoc: Camera-2D LiDAR Fusion Using Multi-Head Self-Attention for End-to-End Serving Robot Relocalization\n    Jieun Lee, Hakjun Lee, and Jiyong Oh*\nJ. Lee and J. Oh are with Robot IT Convergence Research Section, Daegu-Gyeongbuk Research Center, Electronics and Telecommunications Research Institute (ETRI), Daegu, Korea (e-mail: {jieun.lee, jiyongoh}@etri.re.kr). H. Lee is with Polaris3D, Pohang, Korea (e-mail: hakjunlee@polaris3d.co). J. Oh is the corresponding author.\n\n\n    Received: date / Accepted: date\n=================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\nWith the recent development of autonomous driving technology, as the pursuit of efficiency for repetitive tasks and the value of non-face-to-face services increase, mobile service robots such as delivery robots and serving robots attract attention, and their demands are increasing day by day.\nHowever, when something goes wrong, most commercial serving robots need to return to their starting position and orientation to operate normally again.\nIn this paper, we focus on end-to-end relocalization of serving robots to address the problem. \nIt is to predict robot pose directly from only the onboard sensor data using neural networks.\nIn particular, we propose a deep neural network architecture for the relocalization based on camera-2D LiDAR sensor fusion.\nWe call the proposed method FusionLoc.\nIn the proposed method, the multi-head self-attention complements different types of information captured by the two sensors.\nOur experiments on a dataset collected by a commercial serving robot demonstrate that FusionLoc can provide better performances than previous relocalization methods taking only a single image or a 2D LiDAR point cloud as well as a straightforward fusion method concatenating their features.\n\n\n\nFusionLoc, serving robot, camera-2D LiDAR fusion, multi-head self-attention, end-to-end relocalization. \n\n\n\n\n\u00a7 INTRODUCTION\n\n\nRecently, with the development of autonomous driving technology, as the pursuit of efficiency in repetitive tasks and the increase in the value of non-face-to-face services, the interest and demand for mobile robots in particular services are increasing day by day. Accordingly, mobile robots are helping people in various places, such as guiding people at airports or museums, disinfecting schools or hospitals, following people with heavy objects, and serving food in restaurants.\nIt is known that workers can walk around 8 to 15 kilometers a day when serving at a restaurant. Also, walking while carrying dishes will be more laborious than walking with empty hands. As a result, it can lower the quality of essential services for customers. For these reasons, serving robots are employed in more and more sites to improve more efficient working environments while reducing physical burden primarily.\n\nFor the autonomous driving of a serving robot, localization is indispensable.\nSome mobile robots use a global positioning solution based on markers attached to the ceiling for localization.\nHowever, it has limitations due to the cost of infrastructure construction and maintenance and the damage to aesthetic elements in cafes and restaurants.\nAnother localization methodology first produces a map of the place where the robot operates using simultaneous localization and mapping (SLAM) methods <cit.>, <cit.>. Then, the serving robot estimates its location on the map based on information captured from its various sensors, like wheel encoders, cameras, LiDARs, or IMUs. However, it often fails to estimate its location for some reasons, e.g., many people around the robot. The failure in the localization requires the serving robot user to reboot it at a predetermined starting position and orientation. That is one of the reasons why relocalization is needed.\n\n\n\n\nDeep learning studies for end-to-end relocalization have been actively conducted in the past few years. PoseNet <cit.> applied a convolutional neural network (CNN) to a single RGB image to regress 6 degrees of freedom (DoF) camera poses. It was shown that PoseNet is more robust to illumination variation and occlusion than point feature-based relocalization methods.\nVarious studies followed PoseNet to apply a Bayesian CNN <cit.>, long-short term memory (LSTM) <cit.>, or geometric reprojection loss function <cit.>.\nAlso, temporal information was utilized with an LSTM module <cit.>, and an encoder-decoder architecture was employed for camera relocalization in <cit.>. Moreover, geometric constraints of two consecutive poses were considered to improve relocalization accuracy in <cit.>, and a self-attention block was adopted in regressing the camera pose <cit.>. In recent studies, deep neural networks (DNN) took LiDAR <cit.> or IMU <cit.> data instead of using an image for relocalization in an end-to-end manner.\n\nIn this paper, we leverage two sensors equipped with a commercial serving robot to improve relocalization accuracy. From the perspective of the extension of previous studies, this paper proposes a fusion DNN taking both an RGB image and a 2D LiDAR point cloud as inputs to regress the 3-DoF pose of the serving robot. We call the proposed architecture FusionLoc.\nTo our best knowledge, this work is the first study to address the end-to-end relocalization of mobile robots based on the fusion of camera and LiDAR in two-dimensional planar space.\nThe proposed network extracts features by adopting the AtLoc <cit.> architecture and the PointLoc <cit.> architecture from RGB images and 2D LiDAR point clouds, respectively.\nThese features are combined through a concatenation operation. Then, the information captured from each sensor interacts together within multi-head self-attention (MHSA) layers <cit.>.\nFinally, FusionLoc outputs the position and orientation of the serving robot. Furthermore, this study introduces a new dataset consisting of tuples of RGB image, 2D LiDAR point cloud, and 3-DoF pose. The data was collected using a Polaris3D serving robot named ereon.\nExperiments on the dataset show that the proposed network outperforms the previous methods taking a single image or 2D LiDAR point cloud only.\nThe contributions of this paper are summarized as follows:\n\n    \n  * A fusion deep neural network leveraging multi-head self-attention layers is proposed to take an image and a 2D LiDAR point cloud as input.\n    \n  * A new dataset, collected using a commercial serving robot, is introduced.\n\n\nThis paper is organized as follows.\nSection <ref> briefly introduces the previous DNN-based studies for relocalization.\nSection <ref> describes the proposed DNN architecture for camera-2D LiDAR fusion in detail.\nThe new dataset is explained, and the experiments based on the dataset are shown in Section <ref>. Finally, Section <ref> concludes the paper and provides future work.\n\n\n\n\u00a7 RELATED WORKS\n\n\n\n\n\n \u00a7.\u00a7 Deep learning-based camera relocalizaton\n\nPoseNet <cit.> was a breaking ground work that tried to directly regress 6-DoF camera pose from a single image using a CNN.\nIt was noted that it was robust to motion blur, darkness, and unknown camera intrinsics compared to conventional methods based on the scale-invariant feature transform (SIFT <cit.>).\nProbabilistic PoseNet <cit.>, an extension of PoseNet, adopted a Bayesian CNN with Monte Carlo dropout sampling to handle uncertainty in pose estimation.\nIn <cit.>, LSTM was presented as a substitute for the fully-connected layer before the pose regression layer to prevent overfitting and to perform a structured dimensionality reduction.\nIn <cit.>, loss function was also a consideration to improve relocalization performance.\nTo this end, two alternatives were presented and tested in the study.\nOne was a learnable balancing parameter between position and orientation, and the other was the reprojection error between predicted and actual camera poses.\nIn <cit.>, VidLoc was proposed.\nIt leveraged a bidirectional LSTM to utilize temporal information in successive images.\nThe trial led to some reduction in the relocalization error.\nIn <cit.>, the authors employed an encoder-decoder CNN architecture, which was called hourglass, for fine-grained information restoration.\nIn the encoder, ResNet-34 <cit.> was adopted instead of GoogLeNet <cit.> used in PoseNet.\nIn <cit.>, MapNet was proposed using an additional loss term related to the relative poses between image pairs together with the loss term of the absolute poses of images.\nIt was meaningful concerning encoding geometric constraints between consecutive poses into the loss function.\nMoreover, in the study, the logarithm of the unit quaternion was also proposed as the representation of the camera orientation instead of the unit quaternion.\nThe logarithm of the unit quaternion has been popular in most follow-up studies.\nIn <cit.> and <cit.>, a Siamese architecture was presented to reduce the relocalization error with the relative poses of the image pairs.\nViPNet <cit.> utilized the squeeze-and-excitation blocks and a Bayesian CNN with ResNet-50 to deal with uncertainty in predicting a camera pose like Probabilistic PoseNet <cit.>.\nAtLoc <cit.> adopted a self-attention module to focus on geometrically rigid objects rather than dynamic ones.\nVMLoc <cit.> encoded the features of depth images (or projection of sparse LiDARs to RGB images) and RGB images in each CNN branch.\nThen, the two types of features were fused by Product-of-Experts.\n\n\n\n\n \u00a7.\u00a7 Deep learning-based other sensors relocalizaton\n\nThere have been a few studies on deep learning for relocalization with other sensors.\nCompared to camera relocalization, those studies have recently been published.\nThe 3D LiDAR point clouds have more geometric information because they can see with a wide angle of 360 degrees and vertical field of view.\nIn <cit.>, a point cloud odometry method was proposed, which took two consecutive 3D point clouds as input and predicted the transformation between them.\nEach point cloud was first encoded to a panoramic depth image, and then the two depth images were stacked.\nThe DeepPCO network estimated 3D translation and 3D orientation from the stacked depth image using its two sub-networks.\nDifferent from <cit.>, PointLoc <cit.> regressed a 6-DoF pose of a 3D LiDAR sensor directly from a point cloud.\nIn PointLoc, the PointNet++ <cit.> architecture was employed to extract features from unordered and unstructured 3D point clouds, and a self-attention module was also employed to remove outliers.\nUnlike DeepPCO and PointLoc, StickyLocalization <cit.> relocalized current 3D point clouds within a pre-built map.\nIn the pillar layer where PointPillar <cit.> method was utilized, local and global key points were extracted from a current point cloud and the global point cloud corresponding to a map, respectively.\nThen, self-attention and cross-attention modules in the graph neural network layer aggregate context information to improve robustness.\nThe final optimal transport layer outputs the pose of the current point cloud by a matching process between the outputs of the graph neural network layer.\nAlthough its task was not to estimate the pose of a mobile robot, 2DLaserNet <cit.> processed 2D laser scan data with a neural network developed from PointNet++ <cit.> to classify the location of the mobile robot as one of room, corridor, and doorway.\n\nIn addition to camera and LiDAR, IMU has also been used for relocalization. \nRoNIN <cit.> utilized the backbone networks such as ResNet, LSTM, and temporal convolutional network to estimate human 3-DoF poses from a sequence of IMU sensor data.\nIDOL <cit.> regressed 5-DoF poses with a two-stage procedure consisting of an orientation module using extended Kalman filters (EKFs) and LSTMs and a position module using bidirectional LSTMs.\nIn NILoc <cit.>, a neural inertial navigation technique was presented to convert IMU sensor data to a sequence of velocity vectors.\nThe methods leveraged a Transformer-based neural network architecture to reduce high uncertainty in IMU data.\nMoreover, in <cit.>, a neural network framework was proposed to handle laser scan data and IMU sensor data together for mobile robot localization.\nFor feature extraction in the method, a stack of two laser scans and a IMU data sequence between the two laser scans were passed through CNN and LSTM, respectively.\nThen, another LSTM regressed the 3-DoF robot pose from the fused features.\n\n\n\n\n\n\u00a7 METHOD\n \n\n\n\n \u00a7.\u00a7 Self-attention\n \nSince Transformer <cit.> made a great success in the literature on natural language process, its self-attention, one of the essential elements in Transformer, has been utilized in many studies for computer vision applications.\nIn particular, it was shown in <cit.> that a self-attention on CNN features was effective in camera pose regression.\nWe use the same self-attention, but its input consists of image features computed from an input image and point cloud features computed from an input 2D point cloud.\nThe input of the self-attention \ud835\udc1f_i, which is a column vector, is first projected to generate query \ud835\udc16_q\ud835\udc1f_i, key \ud835\udc16_k\ud835\udc1f_i, and value \ud835\udc16_v\ud835\udc1f_i by three learnable projections.\nThen, the value is weighted based on the normalized correlations between the query and the key.\nThe correlations are calculated using the softmax function \u03d5, and these procedures can be represented as\n\n    \ud835\udc1f_Att = \u03d5( \ud835\udc1f_i^T\ud835\udc16_q^T\ud835\udc16_k\ud835\udc1f_i)\ud835\udc16_v\ud835\udc1f_i.\n\nHere, \ud835\udc1f_Att is the output of scaled dot-product attention in <cit.>. \nThe output of the self-attention \ud835\udc1f_o is computed based on a linear projection of \ud835\udc1f_Att and a residual connection as\n\n    \ud835\udc1f_o = \ud835\udc16_p\ud835\udc1f_Att + \ud835\udc1f_i.\n\nAccording to <cit.>, it was illustrated that this self-attention makes activation more intense in fixed objects like buildings and furniture in its input image than in dynamic ones like moving vehicles and pedestrians, which leads to robust relocalization.\nIt was also shown that the self-attention is effective in capturing the correlations among the elements of \ud835\udc1f_i in camera relocalization as well as in other computer vision applications <cit.>, <cit.>.\n\n\n\n\n\n \u00a7.\u00a7 Camera-LiDAR fusion for relocalization with multi-head self-attention\n \nThe proposed network consists of two feature extraction modules, a MHSA module, and a regression module as shown in Fig. <ref>.\nIt receives two types of data obtained from different sensors, an image captured by a camera and a point cloud captured by a 2D LiDAR.\nWe assume that they are synchronized in time.\n\nAt first, in the feature extraction modules, the image and point cloud features are computed from the image and the point cloud, respectively.\nThe image feature \ud835\udc1f_I is computed by the feature extractor of AtLoc <cit.>, which consists of a CNN based on ResNet-34 <cit.> followed by the self-attention module described above.\nOn the other hand, the point cloud feature \ud835\udc1f_P is computed by the feature extractor of the PointLoc <cit.>, which consists of the set abstraction layers, the self-attention layer, and the group all layer.\nExcept for the self-attention layers, the other layers in the network were presented in PointNet++ <cit.>, which was proposed for 3D point cloud classification and semantic segmentation.\nSince the input point cloud is two-dimensional, not three-dimensional, we modified the feature extraction module so that it receives a 2D point cloud as input.\nAs in a CNN where a basic block consisting of convolution, nonlinear activation, and pooling operations is consecutively performed, several successive set abstraction layers extract features from a 2D LiDAR scan to give a feature matrix.\nThe self-attention layer in PointLoc is slightly different from the one in AtLoc described above.\nIn the layer, a weight matrix is computed by passing the feature matrix through a shared multi-layer perceptron (MLP) layer, a sigmoid function, and a broadcasting operation.\nThen, the output of the self-attention layer is obtained by multiplying the feature matrix and the weight matrix element-wise.\nNote that the input and output of the self-attention layer in PointLoc also have the same dimension as in AtLoc.\nThe group all layer takes the output of the self-attention layer as its input and provides the point cloud feature \ud835\udc1f_P by conducting an MLP and max-pooling operations.\nMore details are referred to as <cit.>.\n\n\nFrom the perspective of sensor fusion, it may be important to make data measured from different sensors interact with each other.\nFor the relocalization using both camera and 2D LiDAR, the most straightforward way to use their data in a network is to concatenate their features and to perform a pose regression using the concatenated feature, which is called the fusion features below.\nHowever, we figured out from many experiments that those simple concatenation is not enough to effectively fuse different information obtained from the two sensors.\nTo overcome this limitation, we propose to apply additional MHSA as shown in Fig. <ref> to the fusion features. \nSince the self-attention effectively captures the correlations between its input elements, it can allow different information contained in \ud835\udc1f_I and \ud835\udc1f_P to interact with each other.\nIn other words, we utilize the MHSA module for information fusion.\n\n\nIn <cit.>, MHSA was presented to capture different correlations among input elements by performing several scaled dot-product attentions in parallel.\nTo this end, the outputs of all attentions are integrated and a linear projection is performed as\n\n    \ud835\udc16_p[ \ud835\udc1f_Att_1^T, \u2026, \ud835\udc1f_Att_j^T, \u2026, \ud835\udc1f_Att_N_h^T]^T,\n\nwhere \ud835\udc1f_Att_j is the output of the j-th scaled dot-product attention, N_h is the number of the attention heads, and [ \ud835\udc1a_1^T, \u2026, \ud835\udc1a_n^T] is the concatenation of {\ud835\udc1a_i^T}_i=1^n.\nIn this operation, each attention is scaled by a scaling factor N_h so that its output has the same dimension as the input.\nLike the Transformer encoder in <cit.>, a normalization layer is applied before MHSA, and a residual connection is attached after MHSA.\nWe employ batch normalization (BN, <cit.>) instead of layer normalization (LN, <cit.>) different from <cit.>.\nIt was demonstrated in <cit.> that LN is more effective than BN for recurrent networks.\nHowever, we find out from experiments that BN is more effective than LN in this work.\nAlso, we do not use the positional encoding, another input of the Transformer encoder, because the order of elements in the fusion features is not important in this task, unlike a sequence.\nThis MHSA block with identical architecture repeats N_l times as in <cit.>.\nIt will be demonstrated in experiments that the repetition of MHSA improves the accuracy of the relocalization based on the camera-2D LiDAR fusion.\n\nFinally, the regression module predicts the pose, the position \ud835\udc29 = [ x, y ]^T and the orientation \ud835\udc2a = [ cos\u03b8, sin\u03b8]^T from the output of the MHSA module.\nIt consists of a position branch and an orientation branch as in <cit.>, <cit.>.\nEach branch is composed of consecutive MLPs.\nIn <cit.>, a leaky ReLU activation function was used after each MLP except for the last one in its regression head, but we replace it with the ReLU activation function in our network.\nDifferent from most of the previous studies for end-to-end relocalization, both the position and the orientation are two-dimensional under the assumption that typical serving robots move on planar space.\nTo take into account the continuity of the rotation angle <cit.>, we present the rotation \ud835\udc2a as [ cos\u03b8, sin\u03b8]^T rather than \u03b8.\nTo our best knowledge, this work is the first study addressing the end-to-end relocalization for a serving robot based on the camera-2D LiDAR fusion in two-dimensional planar space.\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Dataset\n\n\nIn order to train and evaluate the neural networks for serving robot relocalization, we constructed a dataset using a commercial serving robot Polaris3D ereon as shown in Fig. <ref>.\nThe dataset contains two sets, Set-01 and Set-02.\nWe gathered the data samples in Set-01 by moving the robot around an area with tables and chairs in our testbed as shown in the left of Fig. <ref>.\nSet-01 originally consisted of a single sequence of 3,964 lengths.\nWe split it into ten shorter ones as in the right of Fig. <ref>.\nSeven sequences and the others among them were used for training and evaluation, respectively.\nIn addition, we collected the data samples for Set-02 by operating the robot in a relatively wider area with long corridors as shown in the left of Fig. <ref>. \nSet-02 consists of three sequences with lengths of 4,820, 7,805, and 6,377 as in the right of Fig. <ref>.\nTwo sequences and the other one among them were used for training and evaluation, respectively.\nTable <ref> summarizes the ereon dataset described above.\n\nAs shown in Fig. <ref>, ereon has two cameras, Intel RealSense D435 and one 2D LiDAR, SLAMTEC RPLiDAR A1M8.\nThe cameras are installed at the side of the upper and lower serving tray, and the LiDAR is mounted in the center of the drive unit located under the lower serving tray.\nIn order to capture the whole body of people around the robot, the lower and upper cameras face upwards and downwards, respectively, rather than facing straight ahead.\nWe gathered images and point clouds obtained from the lower camera and the LiDAR because the upper camera was affected by vibration during robot movement.\nThe obtained RGB images have the size of 420\u00d7240 pixels with a frequency of 1.5 Hz.\nOn the other hand, the LiDAR sensor captures 2D point clouds with a range radius of up to 12 meters (m) and a field of view of 360 degrees (^\u2218).\nAlthough it can measure point clouds at a frequency of 8 Hz, we acquired only the point clouds synchronized with images.\nSince the angular resolution of the LiDAR sensor is equal to or greater than 0.313 degrees, the number of 2D points in a point cloud is up to about 1,150.\nFig. <ref> shows the example RGB images and 2D point clouds captured by our sensors.\nTogether with the sensor data, the robot poses corresponding to images and point clouds are necessary to train and evaluate relocalization algorithm.\nWe first constructed a map of our testbed using a SLAM technique.\nThen, the poses on the constructed map were measured by the localization mode of the technique.\nThe localization mode provides a 6-DoF pose with the quaternion representation for orientation.\nHowever, we used the x- and y-axis values for position and the yaw angle value for orientation under the consideration that typical serving robots operate in a flat environment.\nGiven a quaternion [ q_x, q_y, q_z, q_w], the yaw angle \u03b8 is calculated as\n\n    \u03b8 = arctan( 2(q_xq_y + q_wq_z), 1 - 2(q_y^2 + q_z^2) ).\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Training details\n\n\nWe implemented and trained our proposed network and other networks presented for the same task under the setting below.\nThe Adam method was employed as the solver or optimizer. \nThe learning rate was set to 0.0001, and the weight decay was determined to be the same value.\nThe networks were trained up to 1000 epochs using the ereon dataset with a batch size of 256 on a single GPU of NVIDIA GeForce RTX 3090.\nAs in AtLoc <cit.> and PointLoc <cit.>, we adopted \u2112_1 distances to measure the dissimilarity between the ground truth pose and the estimated pose in our loss function as\n\n    \u2016\ud835\udc29 - \ud835\udc29\u2016_1 e^-\u03b2 + \u03b2 + \u2016\ud835\udc2a - \ud835\udc2a\u2016_1 e^-\u03b3 + \u03b3,\n\nwhere \u03b2 and \u03b3 are learnable parameters to balance the position and orientation loss terms.\nTheir initial values \u03b2_0 and \u03b3_0 were set to 0.0 and -3.0, respectively, as in <cit.> and <cit.>. \nIn the above loss function, we employed the \u2112_1 distance between the predicted angle and the ground truth angle instead of the \u2112_1 distance between two logarithms of their unit quaternions.\nThis replacement came from the dimension of the space in which our robot operates.\n\n\n\n\n\nFor the purpose of comparison, AtLoc and PointLoc were selected as the image- and the point cloud-based baseline methods for end-to-end serving robot relocalization, respectively.\nWe also utilized their networks as backbones for image and point cloud feature extraction.\nFor image feature extraction, as in AtLoc, we scaled the short side of the image to have 256 pixels, then randomly cropped it to 256 \u00d7 256 pixels and normalized the cropped image when training. \nThe random cropping was replaced by the center cropping when testing.\nFor data augmentation, the color jittering method was also conducted by setting the brightness, contrast, and saturation values to 0.7 and the hue value to 0.5. \nThe pre-trained ResNet34 with the ImageNet dataset was chosen as the backbone for image feature extraction.\nWe also applied a dropout operation with a probability of 0.5 whereas no dropout was applied when using BN.\nTable <ref> shows the parameter values of the set abstraction layers in the point cloud feature extractor used in our experiments.\nWe reduced the numbers of layers and neurons in MLP compared to the original PointLoc setting because the 2D point cloud has fewer input points than the 3D point cloud.\nWe also employed ReLU instead of LeakyReLU as the activation function in the fully connected layers.\nAfter the feature extractions mentioned above, the image and the point cloud features were concatenated into the fusion features.\nThen, the fusion features were fed into the several MHSA blocks described in the previous section and the final regression head to provide the pose estimate.\nIn these procedures, both the image and the point cloud features could be set to 256-, 512-, 1024-, and 2048-dimensional giving 16 combinations of fusion.\nAlso, we conducted experiments applying MHSA with 1, 2, 4, and 8 heads and 1, 2, 4, and 6 layers.\n\n\n\n\n\n\n \u00a7.\u00a7 Evaluation results on Set-01\n \n\n\nBaselines. In the original AtLoc and PointLoc, the image and point cloud features were 2048- and 1024-dimensional, respectively.\nWe first tried to determine the dimensions of the image and point cloud features represented as d_I and d_P, respectively.\nTo do this, we trained the two methods using the data in the seven training sequences in Set-01 in Table <ref>.\nThen, we measured the median and mean of the position (m) and orientation errors (^\u2218) using the data in each evaluation sequence in the same set for each value of the d_I and d_P.\nThe previous studies on the end-to-end relocalization methods reported only the median errors.\nHowever, we report the mean errors with the median errors since the latter can not reflect a few large errors.\nWe computed the averages of the median and mean errors, and Table <ref> shows the average median and mean errors of position and orientation.\nWe can see from the table that AtLoc gives lower position errors than PointLoc whereas PointLoc yields lower orientation errors than AtLoc.\nAnother interesting point is that the tendency between the error values and their feature dimension is opposite, i.e., the error values of AtLoc and PointLoc decrease and increase as their dimension increases, respectively.\nAlso, in the case of position error, AtLoc provided similar error values varying the dimension of the image features, and the difference was up to 0.04m and 3.58^\u2218 for position and orientation, respectively.\nHowever, the error values of PointLoc had a large difference, the maximum of which was 0.51m and 3.24^\u2218.\n\n\nA straightforward concatenation. We present experiments with the fusion features corresponding to the concatenation of the image and point cloud features extracted by AtLoc and PointLoc without MHSA.\nTable <ref> shows the average median and mean errors of position and orientation depending on the fusion combinations, which have different dimensions of the image and point cloud features.\nWe can see from the table that the position and orientation errors overall tend to decrease as the dimensions of the image and point cloud features increase.\nNote that the position error decreased to less than 1m by fusing the image and point cloud features in terms of the average median error. \nThe orientation error also maximally decreased to 2.38\u00b0, which was less than the PointLoc's one.\nThis result clearly shows the benefit of fusing different sensor data.\nHowever, the fusion features still provided a large average mean error of orientation compared to the point cloud features.\nWe analyzed these results more by representing the error value as color.\nIn Fig. <ref>, the location of the points corresponds to the ground truth position, and their color means the error value. \nThe error values were measured using the data in seq-03, one of the evaluation sequences.\nComparing Fig. <ref> to Fig. <ref>, we can find the points at which orientation error decreased.\nHowever, we can also see that some points, e.g., on the top right and the bottom middle of Fig. <ref>, still have somewhat large orientation errors compared to the corresponding points in Fig. <ref>.\n\n\n\n\n\n\n\n\nFusionLoc. In addition to AtLoc, PointLoc, and the straightforward concatenation, we finally conducted relocalization experiments by adopting repetitive MHSAs taking the fusion features as input.\nAs mentioned in the previous section, we applied BN instead of LN to each MHSA block based on the fact that the range of the image feature values was different from that of the point cloud feature values. \nMoreover, it was mentioned in <cit.> that LN is not as effective in CNN as in recurrent networks.\nFig. <ref> shows the trajectories of the estimated positions by FusionLoc with LN and BN using the training sequences seq-01 and seq-10, respectively.\nThis result indicates that BN is more effective than LN to train the proposed network for relocalization. \nTable <ref> shows the results of some ablation studies on the number of attention heads N_h and the number of MHSA layers N_l in the MSHA module. \nFor efficiency, the position and orientation errors were measured under only the settings of (d_I, d_P)=(256, 256) and (d_I, d_P)=(2048, 2048).\nWhen (d_I, d_P)=(2048, 2048), we set the batch size to 64 due to memory limitation.\nWe can find from the table that the errors generally decrease as each of N_l and N_h increases in the case of (d_I, d_P)=(256, 256).\nEspecially, most average mean orientation errors become smaller than those of PointLoc when the number of layers is equal to or greater than 4.\nThis result could not be obtained without the MHSA module as shown in Table <ref>.\nWe can also see that the position and orientation errors are minimized when N_h=2 and N_l=6 in terms of the average median and when N_h=8 and N_l=6 in terms of the average mean in the case of (d_I, d_P)=(256, 256).\nCompared to the (d_I, d_P)=(256, 256) without the MHSA module in Table <ref>, the minimum error values get smaller by (0.23m, 6.24\u00b0) and (0.3m, 21.54\u00b0), respectively.\nThe improvement can also be found by comparing Fig. <ref> and Fig. <ref>.\nAnd, they are lower than those of (d_I, d_P)=(2048, 2048) in Table <ref> though using the lower numbers of image and point cloud features.\nMoreover, the minimum errors decreased more as the MHSA module was applied to the case of (d_I, d_P)=(2048, 2048).\nUsing the MHSA module made differences by (0.24m, 1.99\u00b0) in terms of average median error and (0.27m, 20.97\u00b0) in terms of average mean error when (d_I, d_P)=(2048, 2048).\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Evaluation results on Set-02\n\nIn order to validate the proposed method in a larger place, we compared the position and orientation errors using Set-02 data.\nFor efficiency, we set to d_I = d_P = 256.\nAs in the above experiments using the data in Set-01, we trained every neural network aforementioned using the training sequences in Set-02, and we measured the median and mean errors of position and orientation using the evaluation sequence.\nFig. <ref> shows the median and mean errors of AtLoc, PointLoc, the straightforward concatenation method, FusionLoc using LN, and FusionLoc using BN, which were computed at every 100 up to 1000 epochs.\nDifferent from the results of Set-01 in Table <ref> that AtLoc was better than PointLoc in terms of the position error whereas PointLoc was better than AtLoc in terms of the orientation error, we can see in Fig. <ref> that AtLoc provided lower position and orientation errors than PointLoc. \nThis inconsistency may come from the fact that i) Set-02 was collected by making the robot move similar paths multiple times unlike Set-01 as shown in Fig. <ref>, and ii) the maximum range of the used LiDAR sensor is relatively lower than the area of the place where Set-02 was gathered.\nNote that using only the fusion features could decrease the position and the orientation errors if the learning progresses enough.\nActually, it yielded a more accurate result than AtLoc by 0.24m and 1.09^\u2218 in terms of the mean errors at 1000 epochs.\nWe can also see that FusionLoc using LN provided higher errors than the straightforward concatenation method.\nHowever, FusionLoc using BN gave the minimum errors in position and orientation, which were lower than the concatenation method by 0.1m and 0.5^\u2218 in terms of the mean error at 1000 epochs.\nIt corresponds to 15% and 18% reductions in the mean position and orientation errors.\nFig. <ref> shows a visualization of the position and orientation errors represented as color using Set-02.\nWe also figure out from the figure that FusionLoc is more effective than the other methods in the relocalization task.\nIn summary, our experimental results demonstrate that MHSA can be an effective solution to fuse the features captured by different sensors, and BN is more appropriate than LN in MHSA for robot relocalization based on the camera-2D LiDAR fusion.\n\n\n\n\n\n\u00a7 CONCLUSIONS\n\n\nIn this paper, we proposed FusionLoc, an end-to-end relocalization method for serving robots based on the fusion of RGB images and 2D LiDAR point clouds.\nThe proposed network performs the pose regression through AtLoc and PointLoc feature extractors, the MHSA module, and the regression module.\nTo evaluate the proposed network, we constructed a dataset by collecting images, 2D point clouds, and robot poses using a commercial serving robot.\nConducting relocalization experiments using the dataset, FusionLoc showed better performances than the previous relocalization approaches taking only an image or a 2D point cloud as their input.\nWe observed from the experiments that images and point clouds play a role in complementing the lack of information in each modality.\nIn particular, MHSA was an effective way to make the interaction between different information contained in the image and point cloud.\nOur fusion method using MHSA can help the serving robot find its current pose with less error when it lost its location based on conventional methods such as adaptive Monte Carlo localization.\nIn the future, we will consider adopting Transformer to reduce the relocalization errors more.\nWe would also like to collect more datasets with different places and realistic scenarios for serving robots.\n\n\n\n\n\n\u00a7 ACKNOWLEDGMENTS\n\nThis work was mainly supported by Electronic and Telecommunications Research Institute (ETRI) grant funded by the Korean government [23ZD1130, Development of ICT Convergence Technology for Daegu-Gyeongbuk Regional Industry]. The first author (J. Lee) was supported by the Industrial Strategic Technology Development Program (20009396) funded By the Ministry of Trade, Industry & Energy (MOTIE, Korea).\n\n\nIEEEtranS\n\n\n\n\n[\n    < g r a p h i c s >\n]Jieun Lee received the B.E., M.E., and Ph.D degrees in department of electrical and computer engineering from Ajou University, Korea, in 2009, 2011, and 2019, respectively.\n\nFrom September to December in 2019, she was a Researcher in Advanced Institute of Convergence Technology, Korea. Since October 2021, She has been a Post Doctoral Researcher in Electronics and Telecommunications Research Institute (ETRI), Korea. Her research interests include computer vision, machine learning, robot perception, and their applications.\n\n\n[\n    < g r a p h i c s >\n]Hakjun Lee received the B.S. degree in electrical engineering from Chungbuk National University, Cheongju, SouthKorea and the M.S. and Ph.D. degrees in electrical engineering from the Pohang University of Science and Technology (POSTECH), Pohang, South Korea, in 2014, 2016, and 2020, respectively.\n\nHe was a Post Doctoral Researcher in POSTECH from Sep. 2020 to Apr. 2021. He is currently a Senior Researcher with Polaris3D Company, Ltd., Pohang, South Korea. His research interests include service robot, robust control, and navigation system.\n\n\n\n[\n    < g r a p h i c s >\n]Jiyong Oh (M'08) received the B.S. degree from the School of Electronic Engineering, Ajou University, Korea in 2004 and the M.S. and Ph.D. degrees from the School of Electrical Engineering and Computer Science, Seoul National University, Korea in 2006 and 2012, respectively.\n\nHe was a Post Doctoral Researcher in Sungkyunkwan and Ajou University, Korea in 2012 and 2013, respectively.\nFrom Sept. 2013 (March 2015) to March 2015 (May 2016), he was a Research Fellow (BK Assistant Professor) in the Graduate School of Convergence Science and Technology, Seoul National University, Korea.\nSince June 2016, he has been a Senior Researcher in Electronics and Telecommunications Research Institute (ETRI), Korea.\nHis research interests include computer vision, machine learning, robot perception, and their applications.\n\n\n"}