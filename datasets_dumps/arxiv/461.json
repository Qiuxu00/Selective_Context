{"entry_id": "http://arxiv.org/abs/2303.06673v1", "published": "20230312142656", "title": "SSGD: A smartphone screen glass dataset for defect detection", "authors": ["Haonan Han", "Rui Yang", "Shuyan Li", "Runze Hu", "Xiu Li"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI"], "text": "\n\n\nUniversal Instance Perception as Object Discovery and Retrieval\n    Bin Yan^1This work was performed while Bin Yan worked as an intern at\nByteDance. Email: mailto:yan_bin@mail.dlut.edu.cnyan_bin@mail.dlut.edu.cn. ^\u2020 Corresponding authors: mailto:jiangyi.enjoy@bytedance.comjiangyi.enjoy@bytedance.com, mailto:wdice@dlut.edu.cnwdice@dlut.edu.cn.,\nYi Jiang^2, \u2020,\nJiannan Wu^3, \nDong Wang^1, \u2020, \n\nPing Luo^3,\nZehuan Yuan^2,\nHuchuan Lu^1,4\n\n^1 School of Information and Communication Engineering, Dalian University of\nTechnology, China \n\n^2 ByteDance ^3 The University of Hong Kong ^4 Peng Cheng Laboratory\n\n    March 30, 2023\n===========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\nInteractive devices with touch screen have become commonly used in various aspects of daily life, which raises the demand for high production quality of touch screen glass.\n\nWhile it is desirable to develop effective defect detection technologies to optimize the automatic touch screen production lines, the development of these technologies suffers from the lack of publicly available datasets.\n\nTo address this issue, we in this paper propose a dedicated touch screen glass defect dataset which includes seven types of defects and consists of 2504 images captured in various scenarios.\nAll data are captured with professional acquisition equipment on the fixed workstation.\n\nAdditionally, we benchmark the CNN- and Transformer-based object detection frameworks on the proposed dataset to demonstrate the challenges of defect detection on high-resolution images.\n\nDataset and related code will be available at <https://github.com/VincentHancoder/SSGD>.\n\n\n\ntouch-screen-glass, dataset, defect detection\n\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\n\n\n\nNowadays, smart terminals are increasingly crucial to the intelligence process of contemporary society. Almost everyone has at least one smartphone in their hands. As the essential accessories, the production quality of the smartphone screen directly determines the display effect, service life, and user evaluation of smartphones. To this end, almost all screens must pass a production quality inspection before they leave the factory. However, defect detection via human beings only is labor-intensive and inefficient, which is insufficient for the vast market of smartphones. In this case, the use of data-driven computer vision inspection methods can significantly improve efficiency and reduce judgment errors brought on by human factors.\n\n\nThere have been several attempts to apply deep learning techniques to the task of detecting defects in industry. In the metal generic surface defect detection area, Bao et al.\u00a0<cit.> presented a dedicated dataset called NEU-Dataset, and Song et al.\u00a0<cit.> raised a matching method with a set of data used for defect detection of silicon steel strip micro surface. Similarly, a railway surface defect dataset\u00a0<cit.> was made for detection in 2017. Besides metal surface detection tasks, increasing number of industrial products dataset\u00a0<cit.> have been widely collected for specific detection scenarios. \nSome works focused on fabric regions\u00a0<cit.> have been produced and extended to a challenging competition to encourage contestants to approach higher detection accuracy.\n\nRecently more in-depth, increasing dimensions of the electronics industry production have been using neural network technologies to detect the quality of products and defect distribution. \nPramerdorfer et al.\u00a0<cit.> announced a dataset of Printed circuit board (PCB) which was made to facilitate the computer vision tasks in the challenge of PCB deficiency in producing process.\nIn 2019, Deitsch et al.\u00a0<cit.> presented a dataset presented a dataset about the solar panels damage situation.\n\n\nHowever, to the best of our knowledge, there is no publicly available dataset for defect detection of smartphone screens.\u00a0This seriously hinders the application of computer vision technology in the defects inspection of screens.\nTo solve this problem, we firstly propose an open-source Smartphone Screen Glass Dataset, dubbed as SSGD, which contains basically common types of defect occurring on the glass panels.\nSpecifically, the proposed SSGD is made up of 2504 images and contains seven types of defects commonly existing in the production process. All images are in an uniformed resolution of 1500\u00d7 1000 pixels.  Figure\u00a0<ref> shows some samples of SSGD. \nAfter the procedure of data collecting and annotating, we conduct extensive experiments based on the general platform to evaluate the performance of popular object detection on SSGD (Sec.\u00a0<ref>).\n\nOur main contributions can be summarized as follows:\n\n\n  * We collect and annotate a dataset for defect detection of smartphone screens, which possesses various annotations categories, relatively high image information quality, and public availability.\n\n  * We benchmark many popular object detectors on the proposed dataset, including CNN- and Transformer-based frameworks.\n\n\n\n\n\n\n\u00a7 DATASET CREATION PROCEDURE\n\n\n\n\nIn this section, we will introduce SSGD from three aspects:\n(1) the process of image capturing, (2) the overall properties of the dataset, and (3) the composition of the dataset. \n\n\n\n \u00a7.\u00a7 Data Collection\n\n\n\nBefore information acquisition, numerous samples with defects have been collected and selected as origin materials for the purpose of capturing. The background scenario is decorated with the color black to weaken the influence of other visible light in the environment on acquired image information. During the image collection process, smartphone touch screens are placed on a specific capture platform which has been calibrated by a leveler to ensure the correctness of the shooting angle. On the platform, the surrounding region of the screen is the ink area, and the middle is the visible area protected by specific film. The device used to collect images is a line-scan camera which is designed for industry-level information acquisition. After the unified collection and initial data cleaning process, we uniformly change the image resolution into 1500\u00d7 1000 pixels in order to ensure every training object provides the same size background information. The entire dataset is grouped by different capturing platforms and given specific file numbers according to groups. \n\nAfter collection, we employ the labelme[<https://github.com/wkentaro/labelme>] to annotate the seven kinds of defects and the corresponding locations (bounding box format)  on the screens and get an XML file for each image.\n\n\n\n \u00a7.\u00a7 Dataset Properties\n\n\n\nSSGD consists of 2504 images and includes seven common types of defects in actual smartphone screen process. Aiming to show the properties of SSGD clearly, we illustrate the causality in following perspectives:\n\nCategories: Seven types of defects including crack, broken, spot, scratch, light-leakage, blot, broken-membrane. Some samples are provided in Fig.\u00a0<ref>.\n\nWorkstation: Two workstations are used to capture images. SSGD is therefore divided into two parts, called Part I and Part II, respectively.\n\nWorkstation Content: Two workstations captured 1258 and 1246 images (corresponding to Part I and Part II).\n\n\nPart I Content: the possession situation of each type of defects is that crack: 988, broken: 304, spot: 175, scratch: 99, light-leakage: 63, blot: 18, broken-membrane: 10.\n\nPart II Content: the possession situation of each type of defects is that crack: 787, broken: 756, spot: 467, scratch: 163, light-leakage: 60, blot: 13, broken-membrane: 11.\n\n\n \u00a7.\u00a7 Dataset Distribution\n\n\n\nAs shown in Fig.\u00a0<ref> (a), there are two data bounding box characteristic distribution maps whose horizontal axis and vertical axis represent for the height and the width of each bounding box which are distinguished by different workstations. The position on the map of points in different colors show how their bounding boxes look like. There are two curves in blue and red symbolize thresholds to define small, middle and large target detection object. If the point is lower than red curve, it is a small target covering an area less than 32\u00d7 32 pixels. Similarly, it is thought as a large target when more distant from origin point than blue curve. And the target which is between blue and red curve will be thought as a middle size detection object.\n\nFor the purpose of showing quantity distribution of large, medium and small detection targets, we summarize the following related information that can be corroborated by the relevant information in the Fig.\u00a0<ref> (a) as well:\n\nPart I: the amount of different types detection object is  small: 241, middle: 378, large: 1038.\n\nPart II: the amount of different types detection object is  small: 783, middle: 441, large: 1033.\n\nHowever, there are some extreme points existing on the map such as the points gathering at the upper left area of the map, which means that bounding boxes represented by those points are at a nearly 7:1 width and length ratio. As the input of network, bounding box in a such extreme shape may influence final convergence direction.\n\nThe color intensity shows how gathering a type of defect points are. It is shown that on the both maps (Part I and Part II) category crack basically distributed above the blue line while category scratch mainly gather at the zone in middle. In other words, we get a pattern that most samples of above two categories are large size and middle size detection object. Obviously, there are still some categories, such as blot, that are unable to find the pattern of bounding box size. Such a phenomenon is acceptable as well.\n\n\n\n\n\n\n\n\u00a7 EXPERIMENT\n\n\n\n\n\n \u00a7.\u00a7 Settings\n\nWe benchmark most basic object detection models on the proposed SSGD, including Faster R-CNN\u00a0<cit.>, FCOS\u00a0<cit.>, and YOLO series\u00a0<cit.>. \nTo reduce random bias, we use 5-fold cross-validation to measure all models, and results are averaged over the five folds.\nIn the implementation, most experiments are based on MMDetection\u00a0<cit.>. YOLOv5 and YOLOX\u00a0<cit.> follow their official repositories[<https://github.com/ultralytics/yolov5>][<https://github.com/Megvii-BaseDetection/YOLOX>].\nBefore training, we initialize the model with the weight pre-trained on COCO\u00a0<cit.> dataset and new layers in the classification head with the Normal scheme. \nDuring training, for ResNet-50-based\u00a0<cit.> models, we utilize the SGD\u00a0<cit.> optimizer and the 2\u00d7 (24 epochs) schedules with a global batch size of 16 on 4 GPUs.\nWe also adopt the multi-scale training, where the short side of input images is randomly resized to [800, 1500] pixels, and the long side is at most 2250 pixels. \nFor Transformer-based models\u00a0<cit.>, we utilize the AdamW\u00a0<cit.> optimizer and the 2\u00d7 schedules with a global batch size of 8 on 8 GPUs. The resolution of input images is 1500\u00d7 1000 pixels. Other settings remain the same as MMDetection.\nFor YOLO series, the input with a resolution of 1500\u00d7 1000 pixels is padded to 1500\u00d7 1500 pixels. We train the model 100 epochs with a global batch size of 16 on 4 GPUs. Other settings remain the same as the original repositories.\nDuring testing, input images maintain their original resolution without any augmentation.\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Results\n\n\nResults on CNN-based methods. We evaluate mainstream anchor-based\u00a0<cit.> and anchor-free\u00a0<cit.> object detectors on SSGD. Besides YOLOv5 and YOLOX, all other models take ResNet-50 as the backbone. Due to an adaptive training sample selection strategy, ATSS obtains the best performance in all ResNet-50-based models. ATSS outperforms the two-stage Cascade R-CNN by 1.4 AP and 0.3 AP on SSGD Part I and Part II, respectively, while only possessing half the parameters. However, Cascade R-CNN achieves better performance on large objects. \n\n\n\nResults on Transformer-based methods. We evaluate Swin\u00a0<cit.>, PVT\u00a0<cit.>, ScalableViT\u00a0<cit.>, and UniFormer\u00a0<cit.> on the proposed SSGD using the Faster R-CNN framework. As reported in Table\u00a0<ref>, ScalableViT-S achieves 21.1 AP on SSGD Part I under a single-scale training strategy, which surpasses most ResNet-50-based methods using the multi-scale training strategy. However, on SSGD Part II, Swin-T obtains better performance (27.0 AP) than other Vision Transformer counterparts. When compared to ResNet-50, Swin-T gets 3.2 AP gains. Nevertheless, Transformer-based models have an obvious disadvantage in the speed that is required in industrial scenarios. Specifically, when input resolution is 1500 \u00d7 1000 pixels, Swin-T-based Faster R-CNN can only process 18 images per second, but ResNet-50-based ones can process 26 images. Moreover, PVT and ScalableViT are slower than Swin because the method that shrinks spatial tokens of Keys and Values may no longer be applicable in high-resolution images. Therefore, a Vision Transformer, friendly to high-resolution images and industrial scenes, needs to be developed with both higher accuracy and lower latency.\n\n\n\n\n\u00a7 DISCUSSION AND CONCLUSION\n\n\n\nIn this paper, we present the first publicly available Smartphone Screen Glass Dataset for defect detection.\n\nWe adapted professional capturing device and non-single workstations to collect images which involves various categories of defects commonly existing in actual producing procedure.\nThen, we elaborately analysed the object distribution of this dataset\n\nNext, abundant experiments are conducted to show the performance of popular methods on proposed dataset. \nBased on the comparison of experimental results, we find that the Vision Transformers perform worse and are much slower than their CNN counterparts at high resolution input. At the same time, a dynamic assignment strategy during training is very important to this dataset.\n\nIn the future, we will continue the investigation to develop more promising and approachable methods to improve the detection effect in the testing process. We hope this paper paves the way for the application of computer vision in the defect detection of screens.\n\n\n\n\n\n\n\u00a7 ACKNOWLEDGEMENT\n\n\nThis work was supported by the National Key R&D Program of China 505 (Grant No.2020AAA0108303), the Shenzhen Science and Technology Project (Grant No.JCYJ20200109143\n041798) and Shenzhen Stable Supporting Program (WDZC202\n00820200655001). Partial samples and analytical methods are provided by Shenzhen Zhihan Equipment Ltd., Li Xinghui and Wang Xiaohao.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIEEEbib\n\n\n"}