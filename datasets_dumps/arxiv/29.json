{"entry_id": "http://arxiv.org/abs/2303.07347v2", "published": "20230313175959", "title": "TriDet: Temporal Action Detection with Relative Boundary Modeling", "authors": ["Dingfeng Shi", "Yujie Zhong", "Qiong Cao", "Lin Ma", "Jia Li", "Dacheng Tao"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.MM"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTriDet: Temporal Action Detection with Relative Boundary Modeling\n    Dingfeng Shi^\u2217*: This work is done during an internship at JD Explore Academy.\n\n\nVRLab, Beihang University, China\n\n\nshidingfeng@buaa.edu.cn\nYujie Zhong\n\nMeituan Inc.\n\n\njaszhong@hotmail.com\nQiong Cao^\u2020^\u2020: Corresponding authors.\n\nJD Explore Academy\n\n\nmathqiong2012@gmail.com\n\nLin Ma\n\nMeituan Inc.\n\n\nforest.linma@gmail.com\nJia Li^\u2020\n\nVRLab, Beihang University, China\n\n\njiali@buaa.edu.cn\nDacheng Tao\n\nJD Explore Academy\n\n\ndacheng.tao@gmail.com\n\n\n    March 30, 2023\n============================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\nIn this paper, we present a one-stage framework TriDet for temporal action detection. \nExisting methods often suffer from imprecise boundary predictions due to the ambiguous action boundaries in videos. To alleviate this problem, we propose a novel Trident-head to model the action boundary via an estimated relative probability distribution around the boundary. In the feature pyramid of TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer to mitigate the rank loss problem of self-attention that takes place in the video features and aggregate information across different temporal granularities. \nBenefiting from the Trident-head and the SGP-based feature pyramid, TriDet achieves state-of-the-art performance on three challenging benchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational costs, compared to previous methods. \nFor example, TriDet hits an average mAP of 69.3% on THUMOS14, outperforming the previous best by 2.5%, but with only 74.6% of its latency. The code is released to https://github.com/dingfengshi/TriDethttps://github.com/dingfengshi/TriDet. \n\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\nTemporal action detection (TAD) aims to detect all start and end instants and corresponding action categories from an untrimmed video, which has received widespread attention. TAD has been significantly improved with the help of the deep learning. \n\nHowever, TAD remains to be a very challenging  task due to some unresolved problems.\n\nA critical problem in TAD is that action boundaries are usually not obvious. \nUnlike the situation in object detection where there are usually clear boundaries between the objects and the background, the action boundaries in videos can be fuzzy. A concrete manifestation of this is that the instants (\u00a0temporal locations in the video feature sequence) around the boundary have relatively higher predicted response value from the classifier. \n\n\n\n\n\n\n\n\nSome previous works attempt to locate the boundaries based on the global feature of a predicted temporal segment\u00a0<cit.>,\nwhich may ignore detailed information at each instant.\nAs another line of work, they directly regress the boundaries based on a single instant\u00a0<cit.>, potentially with some other features\u00a0<cit.>, which do not consider the relation between adjacent instants (\u00a0the relative probability) around the boundary.  \nHow to effectively utilize boundary information remains an open question. \n\n\n\nTo facilitate localization learning, we posit that the relative response intensity of temporal features in a video can mitigate the impact of video feature complexity and increase localization accuracy. \nMotivated by this, we propose a one-stage action detector with a novel detection head named Trident-head tailored for action boundary localization. Specifically, instead of directly predicting the boundary offsets based on the center point feature, the proposed Trident-head models the action boundary via an estimated relative probability distribution of the boundary (see fig:motivation). The boundary offset is then computed based on the expected values of neighboring locations (\u00a0bins).\n\nApart from the Trident-head, in this work, the proposed action detector consists of a backbone network and a feature pyramid. \n\nRecent TAD methods\u00a0<cit.> adopt the transformer-based feature pyramid and show promising performance. \n\nHowever, the video features of the video backbone tend to exhibit high similarities between snippets, \nwhich is further deteriorated by SA, leading to the rank loss problem\u00a0<cit.> (see fig:cosine).\n\nAdditionally, SA also incurs significant computational overhead.\n\n\n\n\nFortunately, we discover that the success of the previous transformer-based layers (in TAD) primarily relies on their macro-architecture, namely, how the normalization layer and feed-forward network (FFN) are connected, rather than the self-attention mechanism.\nWe therefore propose an efficient convolutional-based layer, termed Scalable-Granularity Perception (SGP) layer, to alleviate the two \nabovementioned problems of self-attention.\nSGP comprises two primary branches, which serve to increase the discrimination of features in each instant and capture temporal information with different scales of receptive fields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe resultant action detector is termed . \nExtensive experiments demonstrate that \u00a0surpasses all the previous detectors and achieves state-of-the-art performance across three challenging benchmarks: THUMOS14, HACS and EPIC-KITCHEN 100. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\nTemporal action detection.\nTemporal action detection (TAD) involves localizing and classifying all actions from an untrimmed video. The existing methods can be roughly divided into two categories, namely, two-stage methods and one-stage methods.\nThe two-stage methods\u00a0<cit.> split the detection process into two stages: proposal generation and proposal classification. Most of the previous works\u00a0<cit.> put emphasis on the proposal generation phrase. Concretely, some works\u00a0<cit.> predict the probability of the action boundary and densely match the start and end instants according to the prediction score. Anchor-based methods\u00a0<cit.> classify actions from specific anchor windows. However, two-stage methods suffer from a high complexity problem and can not be trained in an end-to-end manner. \n\nThe one-stage methods do the localization and classification with a single network. Some previous works\u00a0<cit.> build this hierarchical architecture with the convolutional network (CNN).\n\nHowever, there is still a performance gap between the CNN-based and the latest TAD methods.\n\n\nObject detection.\nObject detection is a twin task of TAD. General Focal Loss\u00a0<cit.> transforms bounding box regression from learning Dirac delta distribution to a general distribution function. Some methods\u00a0<cit.> use Depth-wise Convolution to model network structure and some branched designs\u00a0<cit.> show high generalization ability. They are enlightening for the architecture design of TAD.\n\nTransformer-based methods.\nInspired by the great success of the Transformer in the field of machine translation and object detection, some recent  works\u00a0<cit.> adopt the attention mechanism in TAD task, which help improve the detection performance. For example, some works<cit.> detect the action with the DETR-like Transformer-based decoder\u00a0<cit.>, which models action instances as a set of learnable. Other works\u00a0<cit.> extract a video representation with a Transformer-based encoder. However, most of these methods are based on the local behavior. Namely, they conduct attention operation only in a local window, which introduces an inductive bias similar to CNN but with a larger computational complexity and additional limitations (\u00a0The length of the sequence needs to be pre-padded to an integer multiple of the window size.). \n\n\n\n\n\n\u00a7 METHOD\n\nProblem definition. We first give a formal definition for TAD task. Specifically, given a set of untrimmed videos D={V_i}_i=1^n, we have a set of RGB (and optical flow) temporal visual features X_i={x_t}_t=1^T from each video V_i, where T corresponds to the number of instants, and K_i segment labels Y_i={s_k,e_k,c_k}_k=1^K_i with the action segment start instant s_k, the end instant e_k and the corresponding action category c_k. TAD aims at detecting all segments Y_i based on the input feature X_i. \n\n\n\n \u00a7.\u00a7 Method Overview\n\nOur goal is to build a simple and efficient one-stage temporal action detector. \nAs shown in fig:framework, the overall architecture of \u00a0consists of three main parts: a video feature backbone, a SGP feature pyramid, and a boundary-oriented Trident-head. \nFirst, the video features are extracted using a pretrained action classification network (\u00a0I3D\u00a0<cit.> or SlowFast\u00a0<cit.>). Following that, a SGP feature pyramid is built to tackle actions with various temporal lengths, similar to some recent TAD works\u00a0<cit.>.\nNamely, the temporal features are iteratively downsampled and each scale level is processed with a proposed Scalable-Granularity Perception () layer (Section\u00a0<ref>) to enhance the interaction between features with different temporal scopes. \nLastly, action instances are detected by a designed boundary-oriented Trident-head (Section\u00a0<ref>). \nWe elaborate on the proposed modules in the following. \n\n\n\n\n \u00a7.\u00a7 Feature Pyramid with \u00a0Layer\n\n\n\nThe feature pyramid is obtained by first downsampling the output features of the video backbone network several times via max-pooling (with a stride of 2). The features at each pyramid level are then processed using transformer-like layers (e.g. ActionFormer\u00a0<cit.>). \n\n\n\n\n\n\n\nCurrent Transformer-based methods for TAD tasks primarily rely on the macro-architecture of the Transformer (See supplementary material for details), rather than the self-attention mechanisms. Specifically, SA mainly encounters two issues: the rank loss problem across the temporal dimension and its high computational overhead.\n\n\n\n  \nLimitation 1: the rank loss problem.\nThe rank loss problem arises because the probability matrix in self-attention (\u00a0softmax(QK^T)) is non-negative and the sum of each row is 1, indicating the outputs of SA are convex combination for the value feature V. Considering that pure Layer Normalization\u00a0<cit.> projects feature onto the unit hyper-sphere in high-dimensional space, we analyze the degree of their distinguishability by studying the maximum angle between features within the instant features. We demonstrate that the maximum angle between features after the convex combination is less than or equal to that of the input features, resulting in increasing similarity between features (as outlined in the supplementary material), which can be detrimental to TAD. \n\n\n\n\n  \nLimitation 2: high computational complexity.\nIn addition, the dense pair-wise calculation (between instant features) in self-attention brings a high computational overhead and therefore decreases the inference speed.\n\n\n\n\n\n  \nThe SGP layer.\nBased on the above discovery, we propose a Scalable-Granularity Perception () layer to effectively capture the action information and suppress rank loss. The major difference between the Transformer layer and SGP layer is the replacement of the self-attention module with the fully-convolutional module SGP. The successive Layer Normalization<cit.> (LN) is changed to Group Normalization<cit.> (GN).\n\nAs shown in fig:module, \u00a0contains two main branches: an instant-level branch and a window-level branch. \nIn the instant-level branch, we aim to increase the feature discriminability between action and non-action instant by enlarging their feature distance with the video-level average feature. The window-level branch is designed to introduce the semantic content from a wider receptive field with a branch \u03c8 to help dynamically focus on the features of which scale.\n\n\nMathematically, the \u00a0can be written as:\n\n    f_SGP = \u03d5(x)FC(x) + \u03c8(x)(Conv_w(x) + Conv_kw(x))+ x,\n\nwhere FC and Conv_w denotes fully-connected layer and the 1-D depth-wise convolution layer\u00a0<cit.> over temporal dimension with window size w.\nAs a signature design of , k is a scalable factor aiming at capturing a larger granularity of temporal information.\n\n\nThe video-level average feature \u03d5(x) and branch \u03c8(x) are given as\n\n\n    \u03d5(x)    = ReLU(FC(AvgPool(x))),\n    \u03c8(x)    = Conv_w(x),\n\nwhere AvgPool(x) is the average pooling for all features over the temporal dimension. Here, both \u03d5(x) and \u03c8(x) perform the element-wise multiplication with the mainstream feature. \n\n\n\nThe resultant -based feature pyramid can achieve better performance than the transformer-based feature pyramid while being much more efficient.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Trident-head with Relative Boundary Modeling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \nIntrinsic property of action boundaries.\nRegarding the detection head, some existing methods directly regress the temporal length\u00a0<cit.> of the action at each instant of the feature and refine with the boundary feature<cit.>, or \u00a0<cit.> simply predict an actionness score (indicating the probability of being an action). These simple strategies suffer from a problem in practice: imprecise boundary predictions, due to the intrinsic property of actions in videos. Namely, the boundaries of actions are usually not obvious, unlike the boundaries of objects in object detection. Intuitively, a more statistical boundary localization method can reduce uncertainty and facilitate more precise boundaries.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \nTrident-head.\nIn this work, we propose a boundary-oriented Trident-head to precisely locate the action boundaries based on the relative boundary modeling, \u00a0considering the relation of features in a certain period and obtaining the relative probability of being a boundary for each instant in that period. The Trident-head consists of three components: a start head, an end head, and a center-offset head, which are designed to locate the start boundary, end boundary, and the temporal center of the action, respectively. The Trident-head can be trained end-to-end with the detector. \n\nConcretely, as shown in fig:head, given a sequence of features F\u2208R^T \u00d7 D output from the feature pyramid, we first obtain three feature sequences from the three branches (namely, F_s\u2208R^T, F_e\u2208R^T and F_c\u2208R^T \u00d7 2 \u00d7 (B+1)), where B is the number of bins for boundary prediction, F_s and F_e characterize the response value for each instant as the starting or ending point of an action, respectively. In addition, the center-offset head aims at estimating two conditional distributions P(b_st|t) and P(b_et|t). They represent the probability that each instant (in its set of bins) serves as a boundary when the instant t is the midpoint of an action. Then, we model the boundary distance by combining the outputs of the boundary head and center-offset head: \n\n    P_st   = Softmax(F_s^[(t-B):t] + F_c^t,0),\n    \n        d_st   = E_b\u223cP_st[b] \u2248\u2211_b=0^B(bP_stb),\n\nwhere F_s^[(t-B):t]\u2208R^B+1, F_c^t,0\u2208R^B+1 are the feature of the left adjacent bin set of instant t and the center offsets predicted by instant t only, respectively, and P_st is the relative probability which represents the probability of each instant as a start of the action within the bin set. Then, the distance between the instant t and the start instant of action instance d_st is given by the expectation of the adjacent bin set. Similarly, the offset distance of the end boundary d_et can be obtained by\n\n    P_et   = Softmax(F_e^[t:(t+B)] + F_c^t,1),\n    \n        d_et   = E_b\u223cP_et[b] \u2248\u2211_b=0^B(bP_etb)\n\n\nAll heads are simply modeled in three layers convolutional networks and share parameters at all feature pyramid levels to reduce the number of parameters.\n\nCombination with feature pyramid. \n\n\nWe apply the Trident-head in a pre-defined local bin set, which can be further improved by combining it with the feature pyramid.\nIn this setting, features at each level of the feature pyramid simply share the same small number of bins B (\u00a016) and then the corresponding prediction for each level l can be scaled by 2^l-1, which can significantly help to stabilize the training process. \n\nFormally, for an instant in the l-th feature level t^l,  \u00a0estimates the boundary distance d\u0302_st^l and d\u0302_et^l with the Trident-head described above, then the segments a=(\u015d_t, \u00ea_t) can be decoded by \n\n    \u015d_t    = (t-d\u0302_st^l)\u00d7 2^l-1,\n    \u00ea_t    = (t+d\u0302_et^l)\u00d7 2^l-1.\n\n\n\nComparison with existing methods that have explicit boundary modeling. Many previous methods improve boundary predictions. We divide them into two broad categories: the prediction based on sampling instants in segments\u00a0<cit.> and the prediction based on a single instant. The first category predicts the boundary \n\naccording to the global feature of the predicted instance segments.\nThey only consider global information instead of detailed information at each instant. The second category directly predicts the distance between an instant and its corresponding boundary based on the instant-level feature\u00a0<cit.>. Some of them refine the segment with boundary features\u00a0<cit.>. \n\nHowever, they do not take the relation (\u00a0relative probability of being a boundary) of adjacent instants into account.\nThe proposed Trident-head differs from these two categories and shows superior performance in precise boundary localization.\n\n\n\n\n\n \u00a7.\u00a7 Training and Inference\n\nEach layer l of the feature pyramid outputs a temporal feature F^l \u2208R^(2^l-1T)\u00d7 D, which is then fed to the classification head and the Trident-head for action instance detection. The output of each instant t in feature pyramid layer l is denoted as \u00f4_t^l = (\u0109_t^l, d\u0302_st^l, d\u0302_et^l). \n\n\nThe overall loss function is then defined as follows:\n\n    L   =1/N_pos\u2211_l,t1_{c^l_t>0}(\u03c3_IoUL_cls + L_reg)\n       + 1/N_neg\u2211_l,t1_{c^l_t=0}L_cls,\n\nwhere \u03c3_IoU is the temporal IoU between the predicted segment and the ground truth action instance, and L_cls, L_reg is focal loss\u00a0<cit.> and IoU loss\u00a0<cit.>. N_pos and N_neg denote the number of positive and negative samples.\nThe term \u03c3_IoU is used to reweight the classification loss at each instant, such that instants with better regression (\u00a0of higher quality) contribute more to the training. \nFollowing previous methods\u00a0<cit.>, center sampling is adopted to determine the positive samples. Namely, the instants around the center of an action instance are labeled as positive and all the others are considered as negative.\n\nInference. At inference time, the instants with classification scores higher than threshold \u03bb and their corresponding instances are kept. Lastly, Soft-NMS\u00a0<cit.> is applied for the deduplication of predicted instances.\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\nDatasets.\nWe conduct experiments on four challenging datasets: THUMOS14\u00a0<cit.>, ActivityNet-1.3<cit.>, HACS-Segment\u00a0<cit.> and EPIC-KITCHEN 100\u00a0<cit.>. THUMOS14 consists of 20 sport action classes and it contains 200 and 213 untrimmed videos with 3,007 and 3,358 action instances on the training set and test set, respectively. ActivityNet and HACS are two large-scale datasets and they share 200 classes of action. They have 10,024 and 37,613 videos for training, as well as 4,926 and 5,981 videos for test. The EPIC-KITCHEN 100 is a large-scale dataset in first-person vision, which have two sub-tasks: noun localization (\u00a0door) and verb localization (\u00a0open the door). It contains 495 and 138 videos with 67,217 and 9,668 action instances for training and test, respectively. The number of action classes for noun and verb are 300 and 97.  \n\nEvaluation. For all these datasets, only the annotations of the training and validation sets are accessible. Following the previous practice\u00a0<cit.>, we evaluate on the validation set. We report the mean average precision (mAP) at different intersection over union (IoU) thresholds. For THUMOS14 and EPIC-KITCHEN, we report the IoU thresholds at [0.3:0.7:0.1] and [0.1:0.5:0.1] respectively. For ActivityNet and HACS, we report the result at IoU threshold [0.5, 0.75, 0.95] and the avearge mAP is computed at [0.5:0.95:0.05]. \n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Implementation Details\n\nTriDet is trained end-to-end with AdamW\u00a0<cit.>\noptimizer. The initial learning rate is set to 10^-4 for THUMOS14 and EPIC-KITCHEN, and  10^-3 for ActivityNet and HACS. We detach the gradient before the start boundary head and end boundary head and initialize the CNN weights of these two heads with a Gaussian distribution N(0, 0.1) to stabilize the training process. The learning rate is updated with Cosine Annealing schedule\u00a0<cit.>. We train 40, 23, 19, 15 and 13 epochs for THUMOS14, EPIC-KITCHEN verb, EPIC-KITCHEN noun, ActivityNet and HACS (containing warmup 20, 5, 5, 10, 10 epochs).\n\nFor ActivityNet and HACS, the number of bins B of the Trident-head is set to 12, 14 and the convoluntion window w is set to 15, 11 and the scale factor k is set to 1.3 and 1.0, respectively. \nFor THUMOS14 and EPIC-KITCHEN, the number of bins B of the Trident-head is set to 16 and the convoluntion window w is set to 1 and the scale factor k is set to 1.5. We round the scaled windows size and take it up to the nearest odd number for convenience. We conduct our experiments on a single NVIDIA A100 GPU. \n\n\n\n\n\n\n \u00a7.\u00a7 Main Results\n\n\nTHUMOS14. We adopt the commonly used I3D<cit.> as our backbone feature and table:thumos14 presents the results. Our method achieves an average mAP of 69.3%, outperforming all previous methods including one-stage and two-stage methods. Notably, our method also achieves better performance than recent Transformer-based methods\u00a0<cit.>, which demonstrates that the simple design can also have impressive results. \n\nHACS. For the HACS-segment dataset, we conduct experiments based on two commonly used features: the official I3D<cit.> feature and the SlowFast\u00a0<cit.> feature. As shown in table:hacs, our method achieves an average mAP of 36.8% with the official features. It is the state-of-the-art and outperforms the previous best model TadTR by about 4.7% in average mAP. We also show that changing the backbone to SlowFast can further boost performance, resulting in a 1.8% increase in average mAP, which indicates that our method can benefit from a much more advanced backbone network. \n\n\nEPIC-KITCHEN. On this dataset, following all previous methods, SlowFast is adopted as the backbone feature. The method of our main comparison is ActionFormer<cit.>, which has demonstrated promising performance in EPIC-KITCHEN 100 dataset. We present the results in\u00a0table:epic. Our method shows a significant improvement in both sub-tasks: verb and noun, and achieves 25.4% and 23.8% average mAP, respectively. Note that our method outperforms ActionFormer with the same features by a large margin (1.9% and 1.9% average mAP in verb and noun, respectively). Moreover, our method achieves state-of-the-art performance on this challenging dataset. \n\n\n\n\n\n\n\n\nActivityNet. For the ActivityNet v1.3 dataset, we adopt the TSP R(2+1)D\u00a0<cit.> as our backbone feature. Following previous methods<cit.>, the video classification score predicted from the UntrimmedNet is adopted to multiply with the final detection score. table:activitynet presents the results. Our method still shows a promising result: \u00a0outperform the second best model\u00a0<cit.> with the same feature, only worse than TCANet\u00a0<cit.> which is a two-stage method and using the SlowFast as the backbone feature which is not available now. \n\n\n\n\n \u00a7.\u00a7 Ablation Study\n\nIn this section, we mainly conduct the ablation studies on the THUMOS14 dataset.\n\nMain components analysis.\nWe demonstrate the effectiveness of our proposed components in : SGP layer and Trident-head. To verify the effectiveness of our SGP layer, we use a baseline feature pyramid used by\u00a0<cit.> to replace our SGP layer. The baseline consists of two 1D-convolutional layers and shortcut. The window size of convolutional layers is set to 3 and the number of channels of the intermediate feature is set to the same dimension as the intermediate dimension in the FFN in our SGP layer. All other hyperparameters (\u00a0number of the pyramid layers, etc.) are set to the same as our framework.\n\nAs depicted in table:ablation, compared with the baseline model we implement (Row 1), the SGP layer brings a 6.2% absolute improvement in the average mAP. Secondly, we compare the SGP with the previous state-of-the-art method, ActionFormer, which adopts a self-attention mechanism in a sliding window behavior<cit.> with window size 7 (Row 2). We can see our SGP layer still has 1.5% improvement in average mAP, demonstrating that the convolutional network can also have excellent performance in TAD task. Besides, we compare our Trident-head with the normal instant-level regression head, which regresses the boundary distance for each instant. We can see that the Trident-head improves the average mAP by 1.0%, and the mAP improvement is more obvious in the case of high IoU threshold (\u00a01.6% average mAP improvement in IoU 0.7). \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputational complexity.\nWe compare the computational complexity and latency of \u00a0with the recent ActionFormer<cit.>, which brings a large improvement to TAD by introducing the Transformer-based feature pyramid.\n\nAs shown in table:time, we divide the detector into two parts: the main architecture and the detection heads (\u00a0classification head and regression head). We report the GMACs for each part and the inference latency (average over five times) on THUMOS14 dataset using an input with the shape 2304 \u00d7 2048, following the <cit.>. We also report our results using the Trident-head and the normal regression head, respectively. First, from the first row, we see that GMACs of our main architecture with SGP layer is only 47.1% of the ActionFormer (14.5 versus 30.8), and the overall latency is only 65.2% (146ms versus 224ms), but \u00a0still outperforms Actionformer by 1.5% average mAP, which shows that our main architecture is much better than the local Transformer-based method. Besides, we further evaluate our method with Trident-head. The experimental result shows that our framework can be improved by the Trident-head which further brings 1.0% average mAP improvement and the GMACs is still 1.6G smaller than ActionFormer, and the latency is still only 74.6% of it, proving the high efficiency of our method. \n\n\n\n\n\n\nAblation on the window size in SGP layer.\nIn this section, we study the effectiveness of the two hyper-parameters related to the window size in the SGP layer. Firstly, we fix k=1 and vary w. Secondly, we fix the value of w=1 and change k. Finally, we present the results in fig:statics on THUMOS14 datasets. We find that different choices of w and k produce stable results on both datasets. The optimal values are w=1, k=5 for THUMOS14.\n\nThe effectiveness of feature pyramid level.\nTo study the effectiveness of the feature pyramid and its relation with the number of Trident-head bin set, we start the ablation from the feature pyramid with 16 bins and 6 levels. We conduct two sets of experiments: a fixed number of bins or a scaled number of bins for each level in the feature pyramid. \nAs shown in table:fpn, we can see that the detection performance rises as the number of layers increases. With fewer levels (\u00a0level less than 3), more bins bring better performance. That is because the fewer the number of levels, the more bins are needed to predict the action with a long duration (\u00a0higher resolution at the highest level). We achieve the best result with a level number of 6.  \n\n\n\n\nAblation on the number of bins.\nIn this section, we present the ablation results for the choice of the number of bins on the THUMOS14 and HACS datasets in table:bin. We observe the optimal value is obtained at 16 and 14 on the THUMOS14 and the HACS, respectively. We also find that a small bin value leads to significant performance degradation on HACS but not on THUMOS14. That is because the THUMOS14 dataset aims at detecting a large number of action segments from a long video and a small bin value can meet the requirements, but on HACS, there are more actions with long duration, thus a larger number of bins is needed. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\nIn this paper, we aim at improving the temporal action detection task with a simple one-stage convolutional-based framework TriDet with relative boundary modeling. Experiments conducted on THUMOS14, HACS, EPIC-KITCHEN and ActivityNet demonstrate a high generalization capability of our method, which achieves state-of-the-art performance on the first three datasets and comparable results on ActivityNet. Extensive ablation studies are conducted to verify the effectiveness of each proposed component.\n\nAcknowledgement. \nThis work is supported by the National Natural Science Foundation of China under Grant 62132002.\n\n\n\nieee_fullname\n\n\n\n\n\n\n\u00a7 SUPPLEMENTARY MATERIAL\n\n\n\n \u00a7.\u00a7 Network Architecture in Feature Pyramid\n\n\n\n\n\n  \nFrom Transformer to CNN.\nTo be self-contained, we analyze the impact of module design on the detector. \nFor comparison, we build two baseline models: a convolutional baseline and a Transformer baseline. Firstly, we build the convolution baseline where the convolutional module is adopted from the previous one-stage detector\u00a0<cit.>. Secondly, the previous state-of-the-art detector\u00a0<cit.> with the local window self-attention\u00a0<cit.> is chosen as the Transformer baseline. Then, to analyze the importance of two common components: self-attention and normalization, in the Transformer\u00a0<cit.> macrostructure, we provide three variants of the convolutional-based structure: SA-to-CNN, LN-to-GN and LN-GN-Mix, as\u00a0fig:self2cnn shown, and validate their performance on THUMOS14. \n\n\n\n  \nResults.\nFrom the\u00a0table:self2cnn, we can see there is a large performance gap between the Transformer baseline and the CNN baseline (about 8.1% in average mAP), demonstrating that the Transformer holds a large advantage for TAD tasks. \nThen, we conduct the ablation study with the three variants with normal regression head and Trident-head, respectively.\n\nWe first simply replace the local self-attention with a 1D convolutional layer which has the same receptive field with\u00a0<cit.> (\u00a0kernel size is 19). This change brings a dramatic performance increase in average mAP compared with the CNN baseline (about 6.2%) but is still behind the Transformer baseline by about 1.9%. Next, we conduct experiments with different normalization layers (\u00a0Layer Normalization (LN)\u00a0<cit.> and Group Normalization (GN)\u00a0<cit.>) and we find that the hybrid structure of LN and GN (LN-GN-Mix) shows better performance comparing to the original form of the Transformer (65.7 versus 64.9). \nBy combining with the Trident-head, the LN-GN-Mix version achieves 66.0% in average mAP, which demonstrates the possibility of efficient convolutional modeling. These empirical results further motivate us to improve the feature pyramid with SGP layer (see Sec 3.2 of the main test for more details).\n\n\n\n\n\n\n\n \u00a7.\u00a7 The rank loss problem in Transformer.\n\nIn\u00a0<cit.>, the authors discuss how the pure self-attention operation causes the input feature to converge to a rank-1 matrix at a double exponential rate, while MLP and residual connections can only partially slow down this convergence. This phenomenon is disastrous for TAD tasks, as the video feature sequences extracted by pre-trained action recognition networks are often highly similar (see Section 1), which further aggravates the rank loss problem and makes the features at each instant indistinguishable, resulting in inaccurate detection of action.\n\nWe posit that the core reason for this issue lies in the softmax function used in self-attention. Namely, the probability matrix (\u00a0softmax(QK^T)) is non-negative and the sum of each row is 1, indicating the outputs of SA are convex combination for the value feature V. We will demonstrate that the largest angle between any two features in V' = SA(V) is always less than or equal to the largest angle between features in V.\n\n\nGiven a set of points S={x_1, x_2..., x_n}, a convex combination is a point of the form \u2211_na_nx_n, where a_n\u22650 and \u2211_na_n=1.\n\n\n\nThe convex hull H of a given set of points S is identical to the set of all their convex combinations. A Convex hull is a convex set.\n\n\n[Extreme point]\nAn extreme point p is a point in the set that does not lie on any open line segment between any other two points of the same set. For a point set S and its convex hull H, we have p\u2208 S.\n\n\n \n\nConsider the case of a convex hull that does not contain the origin.\nLet a, b \u2208\u211d^n and let S be the convex hull formed by them. Then, the angle between any two position vectors of points in S is less than or equal to the angle between the position vectors of the extreme points a\u20d7 and b\u20d7.\n\n\n\nConsider the objective function\n\n    f(x) = cos(x\u20d7,y\u20d7) = \u27e8x\u20d7,y\u20d7\u27e9/\u2016x\u20d7\u2016_2\u2016y\u20d7\u2016_2,\n\nwhere x\u20d7,y\u20d7 are the position vectors of two points x_1, x_2 within the convex hull S (a line segment with extreme points a and b). The angle between two vectors is invariant with respect to the magnitude of the vectors, thus, for simplicity, we define x\u20d7=a\u20d7+xb\u20d7, y\u20d7=a\u20d7+yb\u20d7, where x,y \u2208[0,+\u221e).\nMoreover, we have \n\n    f'(x) =   \u2016x\u20d7\u2016^-3_2 \u2016y\u20d7\u2016^-1_2 \u00d7\n       [\u27e8b,y\u20d7\u27e9||a\u20d7+xb\u20d7||_2^2-(||b\u20d7||^2_2x+\u27e8a\u20d7,b\u20d7\u27e9)\u27e8a\u20d7+xb\u20d7,y\u20d7\u27e9]\n\nWe consider \n\n    g(x)=   \u27e8b,y\u20d7\u27e9||a\u20d7+xb\u20d7||_2^2-(||b\u20d7||^2_2x+\u27e8a\u20d7,b\u20d7\u27e9)\u27e8a\u20d7+xb\u20d7,y\u20d7\u27e9\n    \n        =   \u27e8b\u20d7,y\u20d7\u27e9(||a\u20d7||_2^2+2\u27e8a\u20d7,b\u20d7\u27e9 x+||b\u20d7||_2^2x^2)-[\u27e8b\u20d7,y\u20d7\u27e9||b||_2^2x^2\n       +(\u27e8a\u20d7,b\u20d7\u27e9||b||_2^2+\u27e8a\u20d7,b\u20d7\u27e9\u27e8b\u20d7,y\u20d7\u27e9)x+\u27e8a\u20d7,y\u20d7\u27e9\u27e8a\u20d7,b\u20d7\u27e9]\n    \n        =   (\u27e8a\u20d7,b\u20d7\u27e9\u27e8b\u20d7,y\u20d7\u27e9-\u27e8a\u20d7,y\u20d7\u27e9\u27e8b\u20d7,b\u20d7\u27e9)x + \u27e8a\u20d7,a\u20d7\u27e9\u27e8b\u20d7,y\u20d7\u27e9-\u27e8a\u20d7,y\u20d7\u27e9\u27e8a\u20d7,b\u20d7\u27e9.\n\nSubstituting y\u20d7=a\u20d7+yb\u20d7 into the above equation, we have \n\n    g(x)=   (\u27e8a\u20d7,b\u20d7\u27e9\u27e8b\u20d7,a\u20d7+yb\u20d7\u27e9-\u27e8a\u20d7,a\u20d7+yb\u20d7\u27e9\u27e8b\u20d7,b\u20d7\u27e9)x + \n       \u27e8a\u20d7,a\u20d7\u27e9\u27e8b\u20d7,a\u20d7+yb\u20d7\u27e9-\u27e8a\u20d7,a\u20d7+yb\u20d7\u27e9\u27e8a\u20d7,b\u20d7\u27e9\n    \n        =   [\u27e8a\u20d7,b\u20d7\u27e9(\u27e8a\u20d7,b\u20d7\u27e9+y\u27e8b\u20d7,b\u20d7\u27e9)-(\u27e8a\u20d7,a\u20d7\u27e9+y\u27e8a\u20d7,b\u20d7\u27e9)\u27e8b\u20d7,b\u20d7\u27e9]x +\n        [\u27e8a\u20d7,a\u20d7\u27e9(\u27e8a\u20d7,b\u20d7\u27e9+y\u27e8b\u20d7,b\u20d7\u27e9)-(\u27e8a\u20d7,a\u20d7\u27e9+y\u27e8a\u20d7,b\u20d7\u27e9)\u27e8a\u20d7,b\u20d7\u27e9]\n    \n        =   (||\u27e8a\u20d7,b\u20d7\u27e9||_2^2-||a\u20d7||_2^2||b\u20d7||_2^2)x+(||a\u20d7||_2^2||b\u20d7||_2^2-||\u27e8a\u20d7,b\u20d7\u27e9||_2^2)y\n    \n        =   (||\u27e8a\u20d7,b\u20d7\u27e9||_2^2-||a\u20d7||_2^2||b\u20d7||_2^2)(x-y).\n\nAccording to the Cauchy-Schwarz inequality, we can obtain \n\n    ||\u27e8a\u20d7,b\u20d7\u27e9||_2^2-||a\u20d7||_2^2||b\u20d7||_2^2\u22640\n\nThen, we have\n\n    g(x){\n    >0       x<y\n    \n    =0       x=y \n    \n    <0       x>y.\n    .\n\nthus, for any position vector y\u20d7, when x=0 or x\u2192\u221e (x =a\u20d7 or x =b\u20d7), the angle formed between y and x\u20d7 is maximum.\n\nWithout loss of generality, given a specific y\u20d7, if its maximum vector x\u20d7=a\u20d7, we can then set y\u20d7 to a\u20d7 and find its maximum vector again, which yields\n\n    \u03b8(x\u20d7,y\u20d7)\u2264\u03b8(a\u20d7,y\u20d7)\u2264\u03b8(b\u20d7,a\u20d7)\n\nThe proof is completed.\n\n\n\n\nConsider the case of a convex hull that does not contain the origin. Let X = {x_1, x_2, \u2026, x_k} be a set of points and let S be its convex hull. Then, the maximum angle between the position vectors of any two points in S is formed by the position vectors of two extreme points of S.\n\n\n\nAssume that this case holds when k.\n\nWhen k=2, based on Lemma <ref>, the maximum angle is formed by the extreme points x\u20d7_\u20d71\u20d7 and x\u20d7_\u20d72\u20d7.\n\nWhen k\u22653, we can sort the elements of X such that for a point y in S, x\u20d7_\u20d7k\u20d7 maximizes the angle \u03b8(y\u20d7,x\u20d7_\u20d7k\u20d7). Besides, the points x in S are of the form:\n\n    \u03bb_1x\u20d7_\u20d71\u20d7+\u03bb_2x\u20d7_\u20d72\u20d7+...+\u03bb_kx\u20d7_\u20d7k\u20d7\n    \n        =   (\u03bb_1+...+\u03bb_k-1)(\u03bb_1 x\u20d7_\u20d71\u20d7/\u03bb_1+...+\u03bb_k-1+...+\u03bb_k-1x\u20d7_\u20d7k\u20d7-\u20d71\u20d7/\u03bb_1+...+\u03bb_k-1)\n       +\u03bb_kx\u20d7_\u20d7k\u20d7,\n\nwhere (\u03bb_1 x\u20d7_\u20d71\u20d7/\u03bb_1+...+\u03bb_n-1+...+\u03bb_k-1x\u20d7_\u20d7k\u20d7-\u20d71\u20d7/\u03bb_1+...+\u03bb_k-1) is a position vector of a point located within the convex hull induced by {x_1,x_2,...,x_k-1}. Through Lemma\u00a0<ref> and definition, we can obtain\n\n    \u03b8(x\u20d7,y\u20d7)\u2264\u03b8(x\u20d7_\u20d7k\u20d7,y\u20d7)\n\nFor any two points x and y in a convex hull S, by setting y\u20d7=x_k and using the above inequality twice, without loss of generality, we can assume that the vector x\u20d7_\u20d71\u20d7 makes the largest angle with x\u20d7_\u20d7k\u20d7. Then, we can obtain\n    \n    \u03b8(x\u20d7,y\u20d7)\u2264\u03b8(x\u20d7_\u20d7k\u20d7,y\u20d7)\u2264\u03b8(x\u20d7_\u20d71\u20d7,x\u20d7_\u20d7k\u20d7)\n\n\nBy definition, \u03b8(x\u20d7_\u20d71\u20d7,x\u20d7_\u20d7k\u20d7) is no greater than the maximum angle formed by any other two basis vectors.\n\nThe proof is completed.\n\n\n\nWhen the convex hull of the input set V does not contain the origin, the largest angle between any two features after self-attention V' = SA(V) is always less than or equal to the largest angle between features in V.\n\n\n\nIn the Temporal Action Detection (TAD) task, the temporal feature sequences extracted by the pre-trained video classification backbone often exhibit high similarity and the pure Layer Normalization\u00a0<cit.> projects the input features onto the hyper-sphere in the high-dimensional space. Consequently, the convex hull induced by these features often does not encompass the origin. As a result, self-attention operation causes the input features to become more similar, reducing the distinction between temporal features and hindering the performance of the TAD task.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Error Analysis\n\nIn this section, we analyse the detection results on THUMOS14 with the tool from \u00a0<cit.>, which analyze the results in three main directions: the False Positive (FP), the False Negative (FN) and the sensitivity of different length. For a further explanation of the analysis, please refer to \u00a0<cit.> for more details. \n\nSensitivity analysis.\nAs shown in fig:sensitivity (Left), three metrics are taken into consideration: coverage (the normalized length of the instance by the duration of the video), length (the actual length in seconds) and the number of instances (in a video). The results are divided into several length/number bins from extremely short (XS) to extremely long (XL). We can see that our method's performance is balanced over most of the action length, except for extremely long action instances which are significantly lower than the overall value (the dashed line). That's because extremely long action instances contain more complicated information, which deserves further exploration.\n\n\n\n\n\n\n\nAnalysis of the false positives.\nfig:fperror shows a chart of the percentage of different types of action instances in different k-G numbers, where G is the number of the ground-truth instances for each action category and the top k \u00d7 G predicted instances are kept for visualization. \n\nFrom the 1G column on left, we can see in the top G prediction, the true positive instances account for about 80% (at IoU=0.5), which indicates that our method has the power to estimate the right score of each instance. Moreover, on right, we can see the impact of each type of error: the regression error (\u00a0 localization error and background error, the IoU between prediction and ground truth is much lower than a threshold or equal to zero) is still the part that deserves the most attention. \n\n\nAnalysis of the false negatives.\nIn this section, we analyze the false negative (miss-detection) rate for our method. As depicted in fig:fnerror, only the extremely short and extremely long instances have a relatively higher FN rate (9.0% and 13.5%, respectively), which is consistent with intuition that they are more difficult to detect. Note that for a video with only one action instance (XS), TriDet can detect all of them without any miss-detection (0.0 in # Intances), demonstrating our advantage for single-action localization.\n\n\n\n\n\n\n \u00a7.\u00a7 Qualitative Analysis\n\nIn fig:viz, we show the visualization of a detection result on the THUMOS14 test set. It can be seen that our method accurately predicts the start and end instant of the action. Besides, we also visualize the predicted probability of the boundary in the Trident-head, where only the bin around the boundary has a relatively high probability while the others are low and smooth, indicating that the Trident-head can converge to a reasonable result.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}