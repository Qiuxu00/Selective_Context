{"entry_id": "http://arxiv.org/abs/2303.06818v1", "published": "20230313022559", "title": "Backdoor Defense via Deconfounded Representation Learning", "authors": ["Zaixi Zhang", "Qi Liu", "Zhicai Wang", "Zepu Lu", "Qingyong Hu"], "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CR", "cs.LG"], "text": "\n\n\n\n\n\nBackdoor Defense via Deconfounded Representation Learning\n    \n\tZaixi Zhang^1,2, Qi Liu^1,2Qi Liu is the corresponding author., Zhicai Wang^4, Zepu Lu^4, Qingyong Hu^3\n\n\t1: Anhui Province Key Lab of Big Data Analysis and Application,\n School of Computer Science and Technology, University of Science and Technology of China\n2:State Key Laboratory of Cognitive Intelligence, Hefei, Anhui, China\n3:Hong Kong University of Science and Technology 4: University of Science and Technology of China\n\n\tzaixi@mail.ustc.edu.cn, qiliuql@ustc.edu.cn\n {wangzhic, zplu}@mail.ustc.edu.cn, qhuag@cse.ust.hk\n\n    \n=====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\nDeep neural networks (DNNs) are recently shown to be vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by injecting a few poisoned examples into the training dataset. While extensive efforts have been made to detect and remove backdoors from backdoored DNNs, it is still not clear whether a backdoor-free clean model can be directly obtained from poisoned datasets. In this paper, we first construct a causal graph to model the generation process of poisoned data and find that the backdoor attack acts as the confounder, which brings spurious associations between the input images and target labels, making the model predictions less reliable. Inspired by the causal understanding, we propose the Causality-inspired Backdoor Defense (CBD), to learn deconfounded representations for reliable classification. Specifically, a backdoored model is intentionally trained to capture the confounding effects. The other clean model dedicates to capturing the desired causal effects by minimizing the mutual information with the confounding representations from the backdoored model and employing a sample-wise re-weighting scheme. Extensive experiments on multiple benchmark datasets against 6 state-of-the-art attacks verify that our proposed defense method is effective in reducing backdoor threats while maintaining high accuracy in predicting benign samples. Further analysis shows that CBD can also resist potential adaptive attacks. The code is available at <https://github.com/zaixizhang/CBD>. \n\n\n\n\n\n\u00a7 INTRODUCTION\n\nRecent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks <cit.>, where attackers inject stealthy backdoors into DNNs by poisoning a few training data. \nSpecifically, backdoor attackers attach the backdoor trigger (i.e., a particular pattern) to some benign\ntraining data and change their labels to the attacker-designated target label. The correlations between the trigger pattern and target label will be learned by DNNs during training.\nIn the inference process, the backdoored model behaves normally on benign data while its prediction will be maliciously altered when the backdoor is activated. The risk of backdoor attacks hinders the applications of DNNs to some safety-critical areas such as automatic driving <cit.> and healthcare systems <cit.>.\n\n\nOn the contrary, human cognitive systems are known to be immune to input perturbations such as stealthy trigger patterns induced by backdoor attacks <cit.>.\nThis is because humans are more sensitive to causal relations than the statistical associations of nuisance factors  <cit.>. In contrast, deep learning models that are trained to fit the poisoned datasets can hardly distinguish the causal relations and the statistical associations brought by backdoor attacks. Based on causal reasoning, we can identify causal relation <cit.> and build robust deep learning models <cit.>. Therefore, it is essential to leverage causal reasoning to analyze and mitigate the threats of backdoor attacks.\n\nIn this paper, we focus on the image classification tasks and aim to train backdoor-free models on poisoned datasets without extra clean data. We first construct a causal graph to model the generation process of backdoor data where nuisance factors (i.e., backdoor trigger patterns) are considered. With the assistance of the causal graph, we find that the backdoor attack acts as the confounder and opens a spurious path between the input image and the predicted label (Figure <ref>). If DNNs have learned the correlation of such a spurious path, their predictions will be changed to the target labels when the trigger is attached.\n\nMotivated by our causal insight, we propose Causality-inspired Backdoor Defense (CBD) to learn deconfounded representations for classification. \nAs the backdoor attack is stealthy and hardly measurable, we cannot directly block the backdoor path by the backdoor adjustment from causal inference <cit.>. Inspired by recent advances in disentangled representation learning <cit.>, we instead aim to learn deconfounded representations that only preserve the causality-related information. Specifically in CBD, two DNNs are trained, which focus on the spurious correlations and the causal effects respectively. The first DNN is designed to intentionally capture the backdoor correlations with an early stop strategy. The other clean model is then trained to be independent of the first model in the hidden space by minimizing mutual information. The information bottleneck strategy and sample-wise re-weighting scheme are also employed to help the clean model capture the causal effects while relinquishing the confounding factors.\nAfter training, only the clean model is used for downstream classification tasks.\nIn summary, our contributions are:\n\n    \n  * From a causal perspective, we find the backdoor attack acts as the confounder that causes spurious correlations between the input images and the target label.\n    \n  * With the causal insight, we propose a Causality-inspired Backdoor Defense (CBD), which learns deconfounded representations to mitigate the threat of poisoning-based backdoor attacks.\n    \n  * Extensive experiments with 6 representative backdoor attacks are conducted. The models trained using CBD are of almost the same clean accuracy as they were directly trained on clean data and the average backdoor attack success rates are reduced to around 1%, which verifies the effectiveness of CBD.\n    \n  * We explore one potential adaptive attack against CBD, which tries to make the backdoor attack stealthier by adversarial training. Experiments show that CBD is robust and resistant to such an adaptive attack.\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n \u00a7.\u00a7 Backdoor Attacks\n\nBackdoor attacks are emerging security threats to deep neural network classifiers. In this paper, we focus on the poisoning-based backdoor attacks, where the attacker can only inject poisoned examples into the training set while cannot modify other training components (e.g., training loss). Note that backdoor attacks could occur in other tasks (e.g., visual object tracking\u00a0<cit.>, graph classification\u00a0<cit.>, federated learning\u00a0<cit.>, and multi-modal contrastive learning\u00a0<cit.>). The attacker may also have extra capabilities such as modifying the training process\u00a0<cit.>. However, these situations are out of the scope of this paper.\n\nBased on the property of target labels, existing backdoor attacks can be divided into two main categories: dirty-label attacks\u00a0 <cit.> and clean-label attacks\u00a0<cit.>. Dirty-label attack is the most common backdoor attack paradigm, where the poisoned samples are generated by stamping the backdoor trigger onto the original images and altering the labels to the target label. BadNets\u00a0<cit.> firstly employed a black-white checkerboard as the trigger pattern. Furthermore, more complex and stealthier trigger patterns are proposed such as blending backgrounds\u00a0<cit.>, natural reflections\u00a0<cit.>, invisible noise\u00a0<cit.>  and sample-wise dynamic patterns <cit.>.\nOn the other hand, Clean-label backdoor attacks are arguably stealthier as they do not change the labels. For example, Turner et al. <cit.> leveraged deep generative models to modify benign images from the target class.\n\n\n\n\n \u00a7.\u00a7 Backdoor Defenses\n\nBased on the target and the working stage, existing defenses can be divided into the following categories: (1) Detection based defenses aim to detect anomalies in input data <cit.> or whether a given model is backdoored <cit.>. For instance, Du et al.\u00a0<cit.> applies differential privacy to improve the utility of backdoor detection.\n(2) Model reconstruction based defenses aim to remove backdoors from a given poisoned model. For example, Mode Connectivity Repair (MCR)\u00a0<cit.> mitigates the backdoors by selecting a robust model in the path of loss landscape, while Neural Attention Distillation (NAD)\u00a0<cit.> leverages attention distillation to remove triggers. (3) Poison suppression based defenses <cit.> reduce the effectiveness of poisoned examples at the training stage and try to learn a clean model from a poisoned dataset. For instance, Decoupling-based backdoor defense (DBD) <cit.> decouples the training of DNN backbone and fully-connected layers to reduce the correlation between triggers and target labels. \nAnti-Backdoor Learning (ABL)\u00a0<cit.> uses gradient ascent to unlearn the backdoored model with the isolated backdoor data. \n\nIn this paper, our proposed CBD is most related to the Poison suppression based defenses. Our goal is to train clean models directly on poisoned datasets without access to clean datasets or further altering the trained model. \nDifferent with ABL\u00a0<cit.>, CBD directly trains clean models on poisoned datasets without further finetuning the trained model (unlearning backdoor). In contrast with DBD, CBD does not require additional self-supervised pretraining stages and is much more efficient. In Sec. <ref>, extensive experimental results clearly show the advantages of CBD. \n\n\n\n \u00a7.\u00a7 Causal Inference\n\nCausal inference has a long history in statistical research <cit.>. The objective of causal inference is to analyze the causal effects among variables and mitigate spurious correlations. Recently, causal inference has also shown promising results in various areas of machine learning <cit.>. \nHowever, to date, causal inference has not been incorporated into the analysis and defense of backdoor attacks.\n\n\n\u00a7 PRELIMINARIES\n\n\n\n \u00a7.\u00a7 Problem Formulation\n\nIn this section, we first formulate the problem of poison suppression based defense, then provide a causal view on backdoor attacks and introduce our proposed Causality-inspired Backdoor Defense. Here, we focus on image classification tasks with deep neural networks.\n\nThreat Model. We follow the attack settings in previous works <cit.>. Specifically, we assume a set of backdoor examples has been pre-generated by the attacker and has been successfully injected into the training dataset. We also assume the defender has complete control over the training process but is ignorant of the distribution or the proportion of the backdoor examples in a given dataset. The defender\u2019s goal is to train a backdoor-free model on the poisoned dataset, which is as good as models trained on purely clean data. Robust learning strategies developed under such a threat model could benefit research institutes, companies, or government agencies that have the computational resources to train their models but rely on outsourced training data.\n\n\n\n \u00a7.\u00a7 A Causal View on Backdoor Attacks\n\nHumans' ability to perform causal reasoning is arguably one of the most important characteristics that distinguish human learning from deep learning <cit.>. The superiority of causal reasoning endows humans with the ability to recognize causal relationships while ignoring non-essential factors in tasks. On the contrary, DNNs usually fail to distinguish causal relations and statistical associations and tend to learn \u201ceasier\" correlations than the desired knowledge <cit.>. Such a shortcut solution could lead to overfitting to nuisance factors (e.g., trigger patterns), which would further result in the vulnerability to backdoor attacks. Therefore, here we leverage causal inference to analyze DNN model training and mitigate the risks of backdoor injection. \n\nWe first construct a causal graph as causal graphs are the keys to formalize causal inference. One\napproach is to use causal structure learning to infer causal graphs <cit.>,\nbut it is challenging to apply this kind of approach to high-dimensional data like images. Here, following previous works <cit.>, we leverage domain knowledge (Figure <ref> (a)) to construct a causal graph \ud835\udca2 (Figure <ref> (b)) to model the generation process of poisoned data.\n\nIn the causal graph, we denote the abstract data variables by the nodes (X as the input image, Y as the label, and B as the backdoor attack), and the directed links represent their relationships. As shown in Figure <ref>(b), besides the causal effect of X on Y (X \u2192 Y), the backdoor attacker can attach trigger patterns to images (B \u2192 X) and change the labels to the targeted label (B \u2192 Y). Therefore, as a confounder between X and Y, backdoor attack B opens the spurious path X \u2190 B \u2192 Y (let B=1 denotes the images are poisoned and B=0 denotes the images are clean). By \u201cspurious\u201d, we mean that the path lies outside the direct causal path from X to Y, making X and Y spuriously correlated and yielding an erroneous effect when the trigger is activated. DNNs can hardly distinguish between the spurious correlations and causative relations <cit.>. Hence, directly training DNNs on potentially poisoned dataset incurs the risk of being backdoored. \n\nTo pursue the causal effect of X on Y, previous works usually perform the backdoor adjustment in the causal intervention <cit.> with do-calculus: P(Y |do(X)) = \u2211_B\u2208{0,1} P(Y|X,B)P(B). \nHowever, since the confounder variable B is hardly detectable and measurable in our setting, we can not simply use the backdoor adjustment to block the backdoor path. \nInstead, since the goal of most deep learning models is to learn accurate embedding representations for downstream tasks <cit.>, we aim to disentangle the confouding effects and causal effects in the hidden space. The following section illustrates our method.\n\n\n\n\u00a7 CAUSALITY-INSPIRED BACKDOOR DEFENSE\n\nMotivated by our causal insight, we propose the Causality-inspired Backdoor Defense (CBD). \nIn practice, it may be difficult to directly identify the confounding and causal factors of X in the data space. We make an assumption that the confounding and causal factors will be reflected in the hidden representations. \nBased on the assumption, we illustrate our main idea in Figure <ref>.\nGenerally, two DNNs including f_B and f_C are trained, which focus on the spurious correlations and the causal relations respectively. We take the embedding vectors from the penultimate layers of f_B and f_C as R and Z. \nNote that R is introduced to avoid confusion with B.\nWithout confusion, we use uppercase letters for variables and lowercase letters for concrete values in this paper. To generate high-quality variable Z that captures the causal relations, we get inspiration from disentangled representation learning <cit.>.\nIn the training phase, f_B is firstly trained on the poisoned dataset to capture spurious correlations of backdoor. The other clean model f_C is then trained to encourage independence in the hidden space i.e., Z \u22a5 R with mutual information minimization and sample re-weighting. After training, only f_C is used for downstream classification tasks. In the rest of this section, we provide details on each step of CBD. \n\n\nTraining a backdoored model f_B. Firstly, f_B is trained on the poisoned dataset with cross entropy loss to capture the spurious correlations of backdoor. Since the poisoned data still contains causal relations, we intentionally strengthen the confounding bias in f_B with an early stop strategy. Specifically, we train f_B only for several epochs (e.g., 5 epochs) and freeze its parameters in the training of f_C. This is because previous works indicate that backdoor associations are easier to learn than causal relations <cit.>. \nExperiments in Appendix <ref> also verify that the losses on backdoor examples reach nearly 0 while f_B has not converged on clean samples after 5 epochs.\n\nTraining a clean model f_C.\nInspired by previous works <cit.>, we formulate the training objective of f_C with information bottleneck and mutual information minimization:\n\n    \u2112_C= min\u03b2 I(Z; X)_1- I(Z; Y)_2+I(Z; R)_3,\n\nwhere I(\u00b7; \u00b7) denotes the mutual information. Term 1 and 2 constitute the information bottleneck loss <cit.> that encourages the variable Z to capture the core information for label prediction (2) while constraining unrelated information from inputs (1). \u03b2 is a weight hyper-parameter. Term 3 is a de-confounder penalty term, which describes the dependency degree between the backdoored embedding R and the deconfounded embedding Z. It encourages Z to be independent of R by minimizing mutual information so as to focus on causal effects.\nHowever, \u2112_C in Equation <ref> is not directly tractable, especially for the de-confounder penalty term. \nIn practice, we relax Equation <ref> and optimize the upper bound of the term 1&2 and the estimation of the term 3. The details are shown below.\n\nTerm 1. Based on the definition of mutual information and basics of probability, I(Z; X) can be calculated as:\n\n    I(Z; X)   =\u2211_x \u2211_z p(z,x)  logp(z,x)/p(z)p(x)\n       =\u2211_x \u2211_z p(z|x)p(x)  logp(z|x)p(x)/p(z)p(x)\n       =\u2211_x \u2211_z p(z|x)p(x) log\u00a0 p(z|x)-\u2211_z p(z) log\u00a0p(z).\n\nHowever, the marginal probability p(z) = \u2211_x p(z|x)p(x) is usually difficult to calculate in practice. We use variational approximation to address this issue, i.e., we use a variational distribution q(z) to approximate p(z). According to Gibbs\u2019 inequality <cit.>, we know that the KL divergence is non-negative: D_ KL(p(z)||q(z))\u2265 0 \u21d2 -\u2211_z p(z)  log\u00a0p(z)\u2264 -\u2211_z\u00a0p(z)  log\u00a0q(z). By substitute such inequality into Equation <ref>, we can derive an upper bound of I(Z;X):\n\n    I(Z; X)   \u2264\u2211_x p(x)\u2211_z p(z|x) log\u00a0 p(z|x)-\u2211_z p(z) log\u00a0q(z)\n       =\u2211_x p(x)\u2211_z p(z|x)  logp(z|x)/q(z)\n       =\u2211_x p(x)D_ KL(p(z|x)||q(z)).\n\nFollowing previous work <cit.>, we assume that the posterior p(z|x)=\ud835\udca9(\u03bc(x), diag{\u03c3^2(x)}) is a gaussian distribution, where \u03bc(x) is the encoded embedding of variable x and diag{\u03c3^2(x)} = {\u03c3_d^2}_d=1^D is the diagonal matrix indicating the variance. On the other hand, the prior q (z) is assumed to be a standard Gaussian distribution, i.e., q(z) = \ud835\udca9(0, I). Finally, we can rewrite the above upper bound as:\n\n    D_ KL(p(z|x)||q(z))=1/2||\u03bc(x)||_2^2+1/2\u2211_d(\u03c3_d^2-  log\u03c3_d^2 -1).\n\nThe detailed derivation is shown in Appendix <ref>.\nFor ease of optimization, we fix \u03c3(x) to be an all-zero matrix.  \nThen z = \u03bc(x) becomes a deterministic embedding. The optimization of this upper bound is equivalent to directly applying the l_2-norm regularization on the embedding vector z.\n\nTerm 2. With the definition of mutual information, we have I (Z;Y) = H (Y)-H (Y|Z), where H(\u00b7) and H(\u00b7|\u00b7) denote the entropy and conditional\nentropy respectively. Since H (Y) is a positive constant and can be ignored, we have the following inequality,\n\n    -I(Z;Y)\u2264  H(Y|Z).\n\nIn experiments, H(Y|Z) can be calculated and optimized as the cross entropy loss (CE). To further encourage the independence between f_C and f_B,\nwe fix the parameters of f_B and train f_C with the samples-wise weighted cross entropy loss (\u2112_wce). The weight is calculated as:\n\n    w(x) = CE(f_B(x), y)/CE(f_B(x),y)+CE(f_C(x),y).\n\nFor samples with large losses on f_B, w(x) are close to 1; while w(x) are close to 0 when the losses are very small. The intuition of the re-weighting scheme is to let f_C focus on \u201chard\" examples for f_B to encourage independence. \n\nTerm 3. Based on the relationship between mutual information and Kullback-Leibler (KL) divergence <cit.>, the term I(Z; R) is equivalent to the KL divergence between the joint distribution p(Z, R) and the product of two marginals p(Z)p(X) as: I(Z; R)=D_ KL(p(Z, R)||p(Z) p(R)). Therefore, to minimize the de-confounder penalty term I(Z;R), we propose to use an adversarial process that minimizes the distance between the joint distribution p(Z,R) and the marginals p(Z) p(R). During the adversarial process, a discriminator D_\u03d5 is trained to\nclassify the sampled representations drawn from the joint p(Z,R) as the real, i.e., 1 and samples drawn from the marginals p(Z) p(R) as the fake, i.e., 0. \nThe samples from the marginal distribution p(Z) p(R) are obtained by shuffling the individual representations of samples (z,r) in\na training batch from p(Z,R). On the other hand, the clean model f_C tries to generate z that look like\ndrawn from p(Z) p(R) when combined with r from f_B. \nSpecifically, we optimize such adversarial objective similar to WGAN <cit.> with spectral normalization <cit.> since it is more stable in the learning process:\n\n    \u2112_adv = min_\u03b8_C max_\u03d5\ud835\udd3c_p(z,r)[D_\u03d5(z,r)] - \ud835\udd3c_p(z)p(r)[D_\u03d5(z,r)],\n\nwhere \u03b8_C and \u03d5 denote the parameters of f_C and D_\u03d5 respectively. To sum up, the training objective for f_C is:\n\n    \u2112_C = \u2112_wce+\u2112_adv+\u03b2||\u03bc(x)||_2^2.\n\nThe overall objective can then be minimized using SGD. f_B and f_C are trained for T_1 and T_2 epochs respectively. The pseudo algorithm of CBD is shown in Algorithm <ref>. \n\n\nFurther Discussions. \nAdmittedly, it is challenging to disentangle causal factors and confounding factors thoroughly. This is because f_B may still capture some causal relations. Moreover, encouraging independence between Z and R may result in loss of predictive information for f_C.\nHowever, with the well-designed optimization objectives and training scheme, CBD manages to reduce the confounding effects as much as possible while preserving causal relations. The following section shows the detailed verification.\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n \u00a7.\u00a7 Experimental Settings\n\n\nDatasets and DNNs. \nWe evaluate all defenses on three classical benchmark datasets, CIFAR-10 <cit.>, GTSRB <cit.> and an ImageNet subset <cit.>. As for model architectures, we adopt WideResNet (WRN-16-1) <cit.> for CIFAR-10 and GTSRB and ResNet-34 <cit.> for ImageNet subset. Note that our CBD is agnostic to the model architectures. Results with more models are shown in Appendix <ref>.\n\nAttack Baselines. \nWe consider 6 representative backdoor attacks. Specifically, we select\nBadNets\u00a0<cit.>, Trojan attack\u00a0<cit.>, Blend attack\u00a0<cit.>, Sinusoidal signal attack\u00a0(SIG)\u00a0<cit.>, Dynamic attack\u00a0<cit.>, and WaNet\u00a0<cit.>.  BadNets, Trojan attack are patch-based visible dirty-label attacks; Blend is an invisible dirty-label attack; SIG belongs to clean-label attacks; Dynamic and WaNet are dynamic dirty-label attacks. More types of backdoor attacks are explored in Appendix <ref>. The results when there is no attack and the dataset is completely clean is shown for reference.\n\nDefense Baselines.\nWe compare our CBD with 5 state-of-the-art defense methods: Fine-pruning (FP)\u00a0<cit.>, Mode Connectivity Repair (MCR)\u00a0<cit.>, Neural Attention Distillation (NAD)\u00a0<cit.>, Anti-Backdoor Learning (ABL)\u00a0<cit.>, and Decoupling-based backdoor defense (DBD) <cit.>. We also provide results of DNNs trained without any defense methods i.e., No Defense.\n\nAttack Setups.\nWe trained DNNs on poisoned datasets for 100 epochs using Stochastic Gradient Descent (SGD) with an initial learning rate of 0.1 on CIFAR-10 and the ImageNet subset (0.01 on GTSRB), a weight decay of 0.0001, and a momentum of 0.9. The target labels of backdoor attacks are set to 0 for CIFAR-10 and ImageNet, and 1 for GTSRB. The default poisoning rate is set to 10%.\n\nDefense Setups.\n\nFor FP, MCR, NAD, ABL, and DBD, we follow the settings specified in their original papers, including the available clean data. \nThree data augmentation techniques suggested in <cit.> including random crop, horizontal flipping, and cutout, are applied for all defense methods. The hyper-parameter T_1 and \u03b2 are searched in {3,5,8} and {1e^-5, 1e^-4, 1e^-3} respectively. Following the suggestion of the previous work <cit.>, we choose hyperparameters with 5-fold cross-validation on the training set according to the average classification accuracy on hold-out sets. T_2 is set to 100 in the default setting. \nAll experiments were run one NVIDIA Tesla V100 GPU.\nMore details of settings are shown in Appendix <ref>. \n\nMetrics. We adopt two commonly used performance metrics for the evaluation all methods: attack success rate (ASR) and clean accuracy (CA).\nLet \ud835\udc9f_test denotes the benign testing set and f indicates the trained classifier, we have ASR\u225c_(x,y) \u2208\ud835\udc9f_test{f(B(x))=y_t|y \u2260 y_t} and CA\u225c_(x,y) \u2208\ud835\udc9f_test{f(x)=y}, where y_t is the target label and B(\u00b7) is the adversarial generator to add triggers into images. \nOverall, the lower the ASR and the higher the BA, the better the defense.\n\n\n\n \u00a7.\u00a7 Effectiveness of CBD\n\nComparison to Existing Defenses. Table <ref> demonstrates the proposed CBD method on CIFAR-10,\nGTSRB, and ImageNet Subset. We consider 6 representative backdoor attacks and compare the performance of CBD with four other backdoor defense techniques. We omit some attacks on ImageNet dataset due to a failure of reproduction following their original papers.\nWe can observe that CBD achieves the lowest average ASR across all three datasets. Specifically, the average ASRs are reduced to around 1% (1.60% for CIFAR-10, 1.82% for GTSRB, and 0.91% for ImageNet). \nOn the other hand, the CAs of CBD are maintained and are close to training DNNs on clean datasets without attacks.\nWe argue that baseline methods which try to fine-prune or unlearn backdoors of backdoored models are sub-optimal and less efficient. For example, the ABL tries to unlearn backdoor after model being backdoored; DBD requires additional a self-supervised pre-training stage, which introduces around 4 times overhead <cit.>. On the contrary, CBD directly trains a backdoor-free model, which achieves high clean accuracy while keeping efficiency. \n\nWhen comparing the performance of CBD against different backdoor attacks, we find WaNet achieves higher ASR than most attacks consistently (4.24% for CIFAR-10, 3.41% for GTSRB). This may be explained by the fact that WaNet  as one of the state-of-the-art backdoor attacks adopts image warping as triggers that are stealthier than patch-based backdoor attacks <cit.>. Hence, the spurious correlations between backdoor triggers and the target label are more difficult to capture for f_B. Then in the second step, f_C struggles to distinguish the causal and confounding effects. We also notice that CBD is not the best when defending SIG on GTSRB and ImageNet Subset. We guess the reason is similar to WaNet discussed above. SIG produces poisoned samples by mingling trigger patterns with the background. Moreover, SIG belongs to clean-label attacks, which are stealthier than dirty-label attacks <cit.>. This is one limitation of our CBD to be improved in the future.\n\nEffectiveness with Different Poisoning Rate. In Table <ref>, we demonstrate that our CBD is robust and can achieve satisfactory defense performance with a poisoning rate ranging from 1% to 50%. Note that the results when poisoning rate equals 0% have been shown in Table <ref> (None). Here, we did experiments on CIFAR-10\nagainst 4 attacks including BadNets, Trojan, Blend, and WaNet. Generally, with a higher poisoning rate, CBD has lower CA and higher ASR. We can find that even with a poisoning rate of up to 50%, our CBD\nmethod can still reduce the ASR from 100% to 1.47%, 2.31%, 8.14%, and 8.75% for BadNets, Trojan, Blend, and WaNet, respectively. Moreover, CBD helps backdoored DNNs recover clean accuracies. For instance, the CA of CBD for Blend and WaNet improves from 69.67% and 67.25% to 85.56% and 70.43% respectively at 50% poisoning rate.\n\nVisualization of the Hidden Space.\nIn Figure <ref>, we show the t-SNE <cit.> visualizations of the embeddings to give more insights of our proposed method. We conduct the BadNets attack on CIFAR-10. First, in Figure <ref> (a)&(b), we show the embeddings of r and z when CBD is just initialized and when the training of CBD is completed. We observe that there is a clear separation between the confounding component r and the causal component z after training. Moreover, in Figure <ref> (c)&(d), we use t-SNE separately on r and z and mark samples with different labels with different colors. Interestingly, we find the embeddings of the poisoned samples form clusters in r, which indicates that the spurious correlation between backdoor trigger and the target label has been learned. In contrast, poisoned\nsamples lie closely to samples with their ground-truth label in deconfounded embeddings z, which demonstrates CBD can effectively defend backdoor attacks. \n\nComputational Complexity. Compared with the vanilla SGD to train a DNN model, CBD only requires additionally training a backdoored model f_B for a few epochs (e.g., 5 epochs) and a discriminator D_\u03d5, which introduces minimal extra overhead. \nHere, we report the training time cost of CBD on CIFAR-10 and the ImageNet\nsubset in Table <ref>. We also report the time costs of training vanilla DNNs for reference.\nThe extra computational cost is around 10%-20% of the standard training time on CIFAR-10 and the ImageNet subset.  This again shows the advantages of our method.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Resistance to Potential Adaptive Attacks\n\nWhile not our initial intention, our work may be used to help develop more advanced backdoor attacks. Here, we tentatively discuss the potential adaptive attacks on CBD. Typically, backdoor attacks are designed to be injected successfully in a few epochs even only a small portion of data is poisoned (e.g., less than 1%). Hence, the confounding bias of backdoor can be well captured by f_B and R. \nThe intuition of our adaptive attack strategy is to slow the injection process of backdoor attacks (i.e., increasing the corresponding training losses) by adding optimized noise into the poisoned examples, similar to recent works on adversarial training <cit.> and unlearnable examples <cit.>.\nIf the confounding effects is not captured in the first step, then our CBD becomes ineffective. \n\nAssumptions on Attacker's Capability. \nWe assume that attackers have the entire benign dataset. The attackers may have the knowledge of the DNN architecture but cannot interfere with the training process. Moreover, the attackers cannot further modify their poisoned data once the poisoned examples are injected into the training dataset.\n\nProblem Formulation. We formulate the adaptive attack as a min-max optimization problem. Let \ud835\udc9f'={(x_i, y_i)}_i=1^N' indicates the poisoned images by backdoor attacks,  \ud835\udc9f={(x_i, y_i)}_i=1^N denotes the benign images, and \u03b4_i is the added noise to be optimized. Given an DNN model f_\u03b8 with parameters \u03b8, the adaptive attack aims to optimize \u03b4_i by maximizing the losses of poisoned examples while minimizing the average cross entropy losses of all the samples, i.e.,\n\n    min_\u03b8[\u2211_x \u2208\ud835\udc9f\u2112(f_\u03b8(x),y)+\u2211_x \u2208\ud835\udc9f'max_\u03b4_i\u2112(f_\u03b8(x+\u03b4_i),y)],\n\nwhere the noise \u03b4_i is bounded by \u03b4_i_p \u2264\u03f5 with \u00b7_p denoting the L_p norm, and \u03f5 is set to be small such that the poisoned samples cannot be filtered by visual inspection. After optimization,  the poisoned examples attached with the optimized noises \u03b4_i are injected to the training dataset.\nWe adopt the first-order optimization method PGD <cit.> to solve the constrained inner maximization problem:\n\n    x_t+1 = \u03a0_\u03f5(x_t + \u03b1\u00b7\u2207_x\u2112(f_\u03b8(x_t), y)),\n\nwhere t is the current perturbation step (M steps in total), \u2207_x\u2112(f_\u03b8(x_t), y) is the gradient of the loss\nwith respect to the input, \u03a0_\u03f5 is a projection function that clips the noise back to the \u03f5-ball around\nthe original example x when it goes beyond, and \u03b1 is the step size. Pseudo codes of the adaptive attack are shown in Appendix <ref>.\n\nExperimental Settings.\nWe adopt the CIFAR-10 dataset and WRN-16-1 to conduct the experiments. According to previous studies in adversarial attacks, small L_\u221e-bounded noise within \u03b4_\u221e 8/255 on images are unnoticeable to human observers. Therefore, we consider the same constraint in our experiments.\nWe use the SGD to solve the above optimization problem for 10 epochs with the step size \u03b1 of 0.002 and M = 5 perturbation steps.\n\nResults.\nWith BadNets, the adaptive attack works well when there is no defense (CA=84.55%, ASR=99.62%). However, this attack still fails to attack our CBD (CA=84.19%, ASR=4.31%). More detailed results are shown in Appendix <ref>. We can conclude that our defense is resistant to this adaptive attack.\nThe most probable reason is that the optimized noise becomes less effective when the model is retrained and the model parameters are randomly initialized. In another word, the optimized perturbations are not transferable.\n\n\n\n\n\u00a7 CONCLUSION\n\nInspired by the causal perspective, we proposed Causality-inspired Backdoor Defense (CBD) to learn deconfounded representations for reliable classification. Extensive experiments against 6 state-of-the-art backdoor attacks show the effectiveness and robustness of CBD. Further analysis shows that CBD is robust against potential adaptive attacks. Future works include extending CBD to other domains such graph learning <cit.>, federated learning <cit.>, and self-supervised pertaining <cit.>. In summary, our work opens up an interesting research direction to leverage causal inference to analyze and mitigate backdoor attacks in machine learning.\n\n\n\n\u00a7 ACKNOWLEDGEMENTS\n\nThis research was partially supported by a grant from the National Natural Science Foundation of China (Grant No. 61922073).\n\n\nieee_fullname\n\n\n\n\n\n\n\n\u00a7 MORE EXPERIMENTAL SETTINGS\n\n\n\n\n \u00a7.\u00a7 Datasets and Classifiers\n\nThe datasets and DNN models used in our experiments are summarized in Table <ref>.\n\n\n \u00a7.\u00a7 Details of Baseline Implementations\n\nWe implemented the baselines including FP[https://github.com/kangliucn/Fine-pruning-defense], MCR[https://github.com/IBM/model-sanitization], NAD[https://github.com/bboylyg/NAD] ABL[https://github.com/bboylyg/ABL], and DBD [https://github.com/SCLBD/DBD] with their open-sourced codes.\nFor Fine-pruning (FP), we pruned the last convolutional layer of the model. For model connectivity repair (MCR),\nwe trained the loss curve for 100 epochs using the backdoored model as an endpoint and evaluated the\ndefense performance of the model on the loss curve. As for the Neural Attention Distillation (NAD), we finetuned the backdoored student network for 10 epochs with 5% of clean data. The distillation\nparameter for CIFAR-10 was set to be identical to the value given in the original paper. We cautiously selected the value of distillation\nparameter for GTSRB and ImageNet to achieve the best trade-off between ASR and CA. For ABL, we unlearned the backdoored model using the \u2112_GGA loss with 1% isolated backdoor examples and a learning rate of 0.0001. For our DBD, we adopt SimCLR as the self-supervised method and MixMatch as the semi-supervised method. The filtering rate is set to 50% as suggested by the original paper.\n\n\n\n \u00a7.\u00a7 Details of CBD Implementations\n\nIn CBD, f_C is trained on poisoned datasets for 100 epochs using Stochastic Gradient Descent (SGD) with an initial learning rate of 0.1 on CIFAR-10 and the ImageNet subset (0.01 on GTSRB), a weight decay of 0.0001, and a momentum of 0.9. The learning rate is divided by 10 at the 20th and the 70th epoch. D_\u03d5 is set as a MLP with 2 layers. The dimensions of the embedding r and z are set as 64.\n\n\n\n\u00a7 MORE EXPERIMENTAL RESULTS\n\n\n\n\n\n\n \u00a7.\u00a7 Results of Adaptive Attacks\n\n\nThe pseudo codes of adaptive attacks against CBD are shown in Algorithm <ref>. The results of adaptive attacks with different kinds of backdoor are shown in Table <ref>. We also show the curves of training losses on clean/backdoor examples in the optimization of added noise along with the vanilla training for reference. In Figure <ref>, we can observe that the training losses of backdoor examples reaches almost zero after several epochs of training (first line) while our adaptive attack strategy managed to increase the losses of backdoor examples in the optimization process (second line). \nThe above observation indicates that the backdoor examples are much easier to learn than clean examples in vanilla training. The adaptive attack can slow the injection of backdoor and try to make the backdoor attack stealthier to bypass CBD. \nHowever, our CBD can still defend the adaptive attack successfully (Table <ref>). The reason is most probably that the optimized noise becomes less effective when the model is retrained and the model parameters are randomly initialized. In another word, the optimized noise is not transferable.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Results with Different Model Architectures\n\nNote that our CBD is agnostic to the choice of model architectures. In the main text, we report the results with WideResNet-16-1 and ResNet-34. Here, in Table <ref> and <ref>, we show experimental results on CIFAR-10 with WideResNet-40-1 <cit.> and th T2T-ViT <cit.> under poisoning rate 10%. We can observe that CBD can still greatly reduce the attack success rate and keep clean accuracy with different model architectures. \n\n\n\n\n\n\n\n \u00a7.\u00a7 The computational time of other defenses.\n\nTable. <ref> shows the total computational time of defense methods against BadNets. As the methods belong to different categories,\nwe count the time to train backdoored models for FP, MCR, and NAD for a fair comparison. Generally, the time cost of CBD is acceptable. \n\n\n\n\n\u00a7 DETAILS OF DERIVATIONS\n\n\nHere we show the details of derivation with respect to Equ. <ref>. Since the p(z|x)= \ud835\udca9(\u03bc(x), diag{\u03c3^2(x)}) and p(x)= \ud835\udca9(0,1) are multivariate Gaussian distributions with independent components, we only need to derive the case with univariate Gaussian distributions. For the univariate case, we have:\n\n    D_ KL(\ud835\udca9(\u03bc, \u03c3^2)||\ud835\udca9(0,1))\n       =\u222b1/\u221a(2\u03c0\u03c3^2) e^-(x-\u03bc)^2/2\u03c3^2( loge^-(x-\u03bc)^2/2\u03c3^2/\u221a(2\u03c0\u03c3^2)/e^-x^2/2/\u221a(2\u03c0)) dx \n       =\u222b1/\u221a(2\u03c0\u03c3^2) e^-(x-\u03bc)^2/2\u03c3^2 log{1/\u03c3 exp{1/2[x^2-(x-\u03bc)^2/\u03c3^2]}} dx\n       =1/2\u222b1/\u221a(2\u03c0\u03c3^2) e^-(x-\u03bc)^2/2\u03c3^2[- log\u03c3^2+x^2-(x-\u03bc)^2/\u03c3^2]dx \n       =1/2 (- log\u03c3^2+\u03bc^2+\u03c3^2-1).\n\nThe final equation of Equ. <ref> holds because - log\u03c3^2 is a constant; the term x^2 is the second order moment of the Gaussian distribution and equals to \u03bc^2 + \u03c3^2 after integration; the (x-\u03bc)^2 in the third term calculates the variance and equals to \u03c3^2 after integration (-\u03c3^2/\u03c3^2 = -1). For the results of multivariate Gaussian distributions, we have:\n\n    D_ KL(p(z|x)||q(z))\n       =D_ KL(\ud835\udca9(\u03bc(x), diag{\u03c3^2(x)})||\ud835\udca9(0,1))\n       =1/2\u2211_d  (- log\u03c3_d^2+\u03bc_d^2+\u03c3_d^2-1)\n       =\n        1/2||\u03bc(x)||_2^2+1/2\u2211_d(\u03c3_d^2-  log\u03c3_d^2 -1).\n\nTherefore, Equ. <ref> in the main text has been proved.\n"}