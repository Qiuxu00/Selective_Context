{"entry_id": "http://arxiv.org/abs/2303.07123v1", "published": "20230313135611", "title": "Modality-Agnostic Debiasing for Single Domain Generalization", "authors": ["Sanqing Qu", "Yingwei Pan", "Guang Chen", "Ting Yao", "Changjun Jiang", "Tao Mei"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.LG"], "text": "\n\n\n\n\n\nModality-Agnostic Debiasing for Single Domain Generalization\n    Sanqing Qu^1This work was performed at JD.com , Yingwei Pan^2, Guang Chen^1Corresponding author , Ting Yao^2, Changjun Jiang^1, Tao Mei^2\n\n^1Tongji University, ^2JD.com\n\n{2011444, guangchen, cjjiang}@tongji.edu.cn, {panyw.ustc, tingyao.ustc}@gmail.com, tmei@live.com \n\n    March 30, 2023\n================================================================================================================================================================================================================================================================================\n\n\n\n\n\n\nDeep neural networks (DNNs) usually fail to generalize well to outside of distribution (OOD) data, especially in the extreme case of single domain generalization (single-DG) that transfers DNNs from single domain to multiple unseen domains. Existing single-DG techniques commonly devise various data-augmentation algorithms, and remould the multi-source domain generalization methodology to learn domain-generalized (semantic) features. Nevertheless, these methods are typically modality-specific, thereby being only applicable to one single modality (e.g., image).  In contrast, we target a versatile Modality-Agnostic Debiasing (MAD) framework for single-DG, that enables generalization for different modalities. Technically, MAD introduces a novel two-branch classifier: a biased-branch encourages the classifier to identify the domain-specific (superficial) features, and a general-branch captures domain-generalized features based on the knowledge from biased-branch. Our MAD is appealing in view that it is pluggable to most single-DG models. We validate the superiority of our MAD in a variety of single-DG scenarios with different modalities, including recognition on 1D texts, 2D images, 3D point clouds, and semantic segmentation on 2D images. More remarkably, for recognition on 3D point clouds and semantic segmentation on 2D images, MAD improves DSU by 2.82% and 1.5% in accuracy and mIOU.\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nDeep neural networks (DNNs) have achieved remarkable success in various tasks under the assumption that training and testing domains are independent and sampled from identical or sufficiently similar distribution\u00a0<cit.>. However, this assumption often does not hold in most real-world scenarios. When deploying DNNs to unseen or out-of-distribution (OOD) testing domains, inevitable performance degeneration is commonly observed.\nThe difficulty mainly originates from that the backbone of DNNs extracts more domain-specific (superficial) features together with domain-generalized (semantic) features. Therefore, the classifier is prone to paying much attention to those domain-specific features, and learning unintended decision rule\u00a0<cit.>. To mitigate this issue, several appealing solutions have been developed, including Domain Adaptation (DA)\u00a0<cit.> and Domain Generalization (DG)\u00a0<cit.>.  Despite showing encouraging performances on OOD data, their real-world applications are still limited due to the requirement to have the data from other domain (i.e., the unseen target domain or multiple source domains with different distributions). In this work, we focus on an extreme case in domain generalization: single domain generalization (single-DG), in which DNNs are trained with single source domain data and then required to generalize well to multiple unseen target domains.\n\n\nPrevious researches\u00a0<cit.> demonstrate that the specific local textures and image styles tailored to each domain are two main causes, resulting in domain-specific features for images. To alleviate this, recent works\u00a0<cit.> design a variety of data-augmentation algorithms to introduce diversified textures and image styles. The DG methodologies are then remolded with these data-augmentation algorithms to facilitate the learning of domain-generalized features. Nevertheless, such solution for single-DG is typically modality-specific and only applicable to the single modality inputs of images. When coming a new modality (e.g. 3D point clouds), it is difficult to directly apply these techniques to tackle single-DG problem. This is due to the fact that the domain shift in 3D point clouds is interpreted as the differences of 3D structural information among multiple domains, instead of the texture and style differences in 2D images\u00a0<cit.>. Figure\u00a0<ref> conceptually illustrates the issue, which has been seldom explored in the literature.\n\nIn this paper, we propose to address this limitation from the standpoint of directly strengthening the capacity of classifier to identify domain-specific features, and meanwhile emphasize the learning of domain-generalized features. Such way completely eliminates the need of modality-specific data augmentations, thereby leading to a versatile modality-agnostic paradigm for single-DG. Technically, to materialize this idea, we design a novel Modality-Agnostic Debiasing (MAD) framework, that facilitates single domain generalization under a wide variety of modalities. In particular, MAD integrates the basic backbone for feature extraction with a new two-branch classifier structure. One branch is the biased-branch that identifies those superficial and domain-specific features with a multi-head cooperated classifier. The other branch is the general-branch that learns to capture the domain-generalized representations on the basis of the knowledge derived from the biased-branch. It is also appealing in view that our MAD can be seamlessly incorporated into most existing single-DG models with data-augmentation, thereby further boosting single domain generalization.\n\nWe analyze and evaluate our MAD under a variety of single-DG scenarios with different modalities, ranging from recognition on 2D images, 3D point clouds, 1D texts, to semantic segmentation on 2D images. Extensive experiments demonstrate the superior advantages of MAD when being plugged into a series of existing single-DG techniques with data-augmentation (e.g., Mixstyle\u00a0<cit.> and DSU\u00a0<cit.>). More remarkably, for recognition on point cloud benchmark, MAD significantly improves DSU in the accuracy from 33.63% to 36.45%. For semantic segmentation on image benchmark, MAD advances DSU with mIoU improvement from 42.3% to 43.8%.\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n \u00a7.\u00a7 Domain Adaptation\n\nOver the last decade, many efforts have been devoted to domain adaptation (DA) to address the OOD issue\u00a0<cit.>. DA methods are developed to utilize the labeled source domain and the unlabeled out-of-distributed target domain in a transductive learning manner. Existing DA approaches can be briefly grouped into two paradigms, i.e., moment matching\u00a0<cit.> and adversarial alignment\u00a0<cit.>. DA methods have achieved significant progress in many applications, e.g., object recognition\u00a0<cit.>, semantic segmentation\u00a0<cit.>, and object detection\u00a0<cit.>. Nevertheless, the requirement of both source and target domain data during training significantly limits their practical deployment. Besides, in DA manner, DNNs are typically coupled with source and target domains, affecting their capacity to generalize to other domains. In this work, we focus on a more challenging scenario where DNNs are required to generalize well to multiple unseen domains.\n\n\n\n \u00a7.\u00a7 Domain Generalization\n\nDifferent from DA, domain generalization (DG) expects to learn generalized DNNs with the assistance of multiple source domains\u00a0<cit.>, without the access to target domain. Currently, DG methods can mainly be categorized into three dimensions, including domain alignment, data augmentation/generation, and ensemble learning. Most existing DG methods\u00a0<cit.> belong to the category of domain alignment. Their motivation is straightforward: features that are invariant to the source domain shifts should also be generalized to any unseen target domain shift. Data generation is another popular technique for DG\u00a0<cit.>. The goal is to generate diverse and rich data to boost the generalization ability of DNNs. Existing methods typically remould the Variational Auto-encoder (VAE)\u00a0<cit.>, and the Generative Adversarial Networks (GAN)\u00a0<cit.> to execute diversified data generation. Ensemble learning\u00a0<cit.> commonly learns multiple copies of the same model with different initialization and then utilizes their ensemble for final prediction. As the variant of ensemble learning, weight averaging\u00a0<cit.>, domain-specific neural networks\u00a0<cit.>, and domain-specific batch normalization\u00a0<cit.> have recently achieved promising results. Nevertheless, it is non-trivial to directly apply these DG techniques for single domain generalization.\n\n\n\n \u00a7.\u00a7 Single Domain Generalization\n\nSingle domain generalization (single-DG) is an extreme case of domain generalization, where DNNs are trained with only one source domain data and required to perform well to multiple unseen target domains. It is more challenging than DA and DG, yet indeed more realistic in practical applications. To address this challenging problem, several methods\u00a0<cit.> have designed various data augmentation algorithms to enhance the diversity and informativeness of training data. In\u00a0<cit.>, the authors propose a style-complement module to synthesize images from diverse distributions. In\u00a0<cit.>, synthesized feature statistics are introduced to model the uncertainty of domain shifts during training. To regulate single-DG training, <cit.> applies a variety of visual corruptions as augmentation and designs a new attention consistency loss. A novel image meta-convolution network is developed in\u00a0<cit.> for capturing more domain-generalized features. Nevertheless, most methods are modality-specific and only applicable to image inputs. When we encounter a new data modality, they are commonly not available to deploy. The reason behind is that for different data modalities, domain shifts tend to be different. For example, the differences in 3D geometry structure among multiple domains are the origin of domain shifts for point clouds, instead of style and texture differences in 2D images. Our work delves into this limitation and targets for proposing a general and versatile framework for single-DG that is agnostic to data modality.\n\n\n\n\u00a7 METHODOLOGY\n\n\n\n \u00a7.\u00a7 Preliminary\n\nWe consider an extreme case in generalization: single domain generalization (single-DG), where the goal is to train DNNs with single source domain \ud835\udc9f_S that perform well to multiple unseen target domains: {\ud835\udc9f_T^1, \ud835\udc9f_T^2,\u2026, \ud835\udc9f_T^Z}. In particular, we consider the K-way classification. We denote \ud835\udc9f_S = { (x_i, y_i)}^n_i=1, where x\u2208\ud835\udcb3\u2282\u211d^X, y\u2208\ud835\udcb4\u2282\u211d^K. The whole DNN architecture is represented as F = g \u2218 f, where f: \u211d^X\u2192\u211d^D denotes the feature extractor and g: \u211d^D\u2192\u211d^K is the classifier. This setting is guaranteed under a general assumption in domain generalization: There are domain-generalized features e_g in the domain \ud835\udc9f_S whose correlation with label is consistent across domains, and domain-specific features e_s whose correlation with label varies across domains. Classifiers that rely on domain-generalized features e_g perform much better on new unseen domains than those that depend on domain-specific features e_s.  In this setting, directly applying the vanilla empirical risk minimization (ERM)\u00a0<cit.> on \ud835\udc9f_S commonly results in a sub-optimal model that does not generalize well to unseen domains. The main reason originates from that the feature extractor f often extracts more domain-specific features e_s together with domain-generalized features e_g\u00a0<cit.>. DNNs trained with SGD often count on the simplest features\u00a0<cit.>, which leads to a tendency for the classifier g to overemphasize e_s and pay less attention to e_g, resulting in unintended decision rules.\n\nPrior methods\u00a0<cit.> have designed various data-augmentation algorithms to encourage the feature extractor g to learn more domain-generalized features e_g and suppress those domain-specific features e_s. However, these algorithms are typically modality-specific, and largely limited to images. Instead, we propose to mitigate this limitation by directly strengthening the capacity of classifier for identifying domain-specific features, and meanwhile emphasising the learning of domain-generalized features. That completely eliminates the requirement of modality-specific data augmentations, pursuing a versatile and modality-agnostic paradigm for single-DG. Technically, we present a novel modality-agnostic debiasing (MAD) framework. MAD integrates the basic backbone for feature extraction with a new two-branch classifier structure. One branch is the biased-branch that identifies those domain-specific features e_s with a multi-head cooperated classifier. The other is that learns to capture the domain-generalized features e_g with the knowledge derived from the biased-branch. Figure\u00a0<ref> illustrates the detailed architecture of our MAD.\n\n\n\n \u00a7.\u00a7 Identifying Domain-specific Features\n\nThere have been some efforts\u00a0<cit.> in domain generalization to realize domain-specific features e_s and domain-generalized features e_g separation.\nNevertheless, most of them require multiple training domains and pre-defined domain labels, making them inapplicable for single-DG. Moreover,\u00a0<cit.> has pointed out that given a trained classifier, it is non-trivial to uniquely decompose the classifier weight into domain-specific and domain-generalized terms, especially with only one source domain data.\n\nTo alleviate these issues, we propose a simple yet effective domain-specific feature identification strategy. Our motivation is straightforward: since the vanilla classifier trained with SGD will inadvertently focus more on those domain-specific features, the weights of the trained classifier can be considered as an indicator of those features.\n\nNevertheless, a single vanilla classifier is typically not effective to locate all domain-specific features. The reason is that there commonly exist multiple factors that contribute to domain-specific features. Taking the identification of \u201celephants\" and \u201ccats\" as an example, the hypotheses \u201celephants tend to be found in grasslands\", and \u201celephants tend to have wrinkled skin\" are both beneficial for classification. When we deploy classifiers to the real world, these hypotheses are domain-specific and superficial, and might result in severe performance degradation. For images, there are several factors typically correlated to domain-specific features, such as the background contexts\u00a0<cit.>, the texture of the objects\u00a0<cit.>, and high-frequency patterns that are almost invisible to the human eye\u00a0<cit.>. That motivates us to design a biased-branch that identifies more domain-specific features with a multi-head cooperated classifier g_bias: \u211d^D\u2192\u211d^K\u00d7 M.  Specifically, we apply the cooperation cross-entropy loss to learn this branch as:\n\n    \u2112_C-CE = \ud835\udd3c_x, y\u2211_k=1^K -1_[k=y]logexp(max(v_k(x)))/\u2211_j=1^Kexp(max(v_j(x))),\n\nwhere v_k(x)= g_bias(f(x))[k, :] \u2208\u211d^M denotes the logits of multi-head classifier for the k-th category of sample x, and M is the number of classification heads. Note that we do not enforce all heads of the biased-branch classifier to correctly predict each sample. Instead, we only need one of them to accurately identify it. That is, all heads are encouraged to cooperate with each other for classification. The spirit behind is that domain-specific features do not represent the truly domain-generalized semantics. Thereby, for a particular-type domain-specific features, they are not necessarily present in all samples. Since the max function is not differentiable in Eq.\u00a0(<ref>), we approximate this function with the log-sum-exp during our implementation.\n\nIn general, the sweet spot for M is set within the range from 1 to D//K - 1. Its value depends on the dimension and factors introduced domain-specific features. In our implementation, we perform cross validation to choose a good value for M, but it is worth noting that the performance is relatively stable with respect to this choice (see more discussions in Sec.\u00a0<ref>).\n\n\n\n \u00a7.\u00a7 Learning to Debias\n\nBased on the proposed biased-branch, we have an indicator to those domain-specific features. A follow-up question is how to suppress those domain-specific features in favor of focusing more on those desired domain-generalized features. Here, we introduce another general-branch classifier g_gen : \u211d^D\u2192\u211d^K to capture those domain-generalized features. Let W_bias\u2208\u211d^K\u00d7 M\u00d7 D and W_gen\u2208\u211d^K\u00d7 D be weights of the multi-head biased classifier and the domain-general classifier, respectively. An intuitive solution is to enforce orthogonality between W_bias and W_gen in Eq.\u00a0(<ref>) during learning the classifier g_gen in Eq.\u00a0(<ref>):\n\n    \u2112_CE = \ud835\udd3c_x, y\u2211_k=1^K -1_[k=y]logexp(u_k(x))/\u2211_j=1^Kexp(u_j(x)),\n\n\n\n    \u2112_Reg = 1/K\u2211_k=1^K W_bias[k,:] W_gen[k,:]^T _F^2.\n\nHere u_k(x)= g_gen(f(x))[k] \u2208\u211d represents the logit of classifier g_gen for the k-th category of input sample x. However, if we optimize the whole network (including the feature extractor f, biased-branch classifier g_bias, and general-branch classifier g_gen) simultaneously at the beginning, there is no guarantee that the classifier g_gen will pay more attention to those domain-general features. To address this issue, we introduce a two-stage learning mechanism to enable the interaction between the two branches. Technically, in the first stage, we only introduce Eq.\u00a0(<ref>) and Eq.\u00a0(<ref>) to optimize the network, encouraging the biased-branch classifier to learn those domain-specific features and expecting the weight of general-branch classifier W_gen to evade the territory of domain-specific features. Then, in the second stage, we apply Eq.\u00a0(<ref>), Eq.\u00a0(<ref>) and Eq.\u00a0(<ref>) together to optimize the entire network. Accordingly, the overall optimization objective is:\n\n    min_f, g_bias, g_gen\u2112_C-CE + \u2112_Reg + 1_[pro \u2265 T]\u00b7\u2112_CE,\n\nwhere all loss terms are equally weighted, pro denotes the overall training progress, T is a hyper-parameter that determines when to trigger the second stage learning. In general, the choice of T depends on the training dataset size and task difficulty. In our implementation, for recognition task, we typically set T = 3 epochs (50 epochs in total). As for semantic segmentation, we set T = 6% of the iterations in total. Algorithm.\u00a0<ref> presents the Pseudo-code of our MAD.\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\nWe evaluate the effectiveness of MAD for single domain generalization (single-DG) via various empirical evidences on a series of tasks, including recognition on images, point clouds, texts, and semantic segmentation on images. Here we include several single-DG methods as baselines for performance comparison: (1) ERM\u00a0<cit.> directly applies the vanilla strategy to train source model. (2) AugMix\u00a0<cit.> utilizes stochastic and diverse augmentations, and a formation to mix multiple augmented images to generate diverse samples. (3) pAdaIN\u00a0<cit.> swaps feature statistics between the samples applied with a random permutation of mini-batch, (4) Mixstyle\u00a0<cit.> adopts linear interpolation on feature statistics of two instances to generate synthesized samples. (5) DSU\u00a0<cit.> characterizes the feature statistics as uncertain distribution to model domain shift. (6) ACVC\u00a0<cit.> introduces more severe image augmentations, including image corruptions and Fourier transform. Recall that our MAD is able to directly strengthen the capability of classifier to identify domain-specific features, and meanwhile emphasize the learning of domain-generalized features. Therefore, MAD can be seamlessly incorporated into these methods to further boost performances. Note that MAD discards the additional biased-branch and only employs the feature extractor plus general-branch classifier at inference. That is, when plugging MAD into existing methods, there is no increase in computational cost.\n\n\n\n\n\n \u00a7.\u00a7 Single-DG on Image Recognition\n\nSetup and Implementation Details: We validate the proposed method on two image datasets: PACS\u00a0<cit.>, a widely-used benchmark for domain generalization with four domains: Photo (P), Art Painting (A), Cartoon (C), and Sketch (S). VLCS\u00a0<cit.>, another commonly adopted benchmark for domain generalization with four different domains: VOC2007 (V), LabelMe (L), Caltech101 (C), SUN09 (S). In our implementation, we adopt the ResNet-18\u00a0<cit.> pretrained on ImageNet\u00a0<cit.> as backbone. We apply the SGD optimizer with momentum 0.9. The batch size is set to 64. We set the learning rate to 2e-3/1e-3 for PACS/VLCS. Experiments are conducted on a Tesla P40 GPU with PyTorch-1.5. \nFollowing\u00a0<cit.>, we split the training domain into training and validation subsets, and select the best-performing model on validation set to report the OOD performances.\n\n\nExperiment Results: We first conduct experiments on PACS, shown in Table\u00a0<ref>. The main domain shift in this dataset is derived from style differences, \nand most data augmentation methods manifest higher performances than ERM baseline. Though these methods have achieved good performances, our MAD still manages to further improve their performance consistently. For example, MAD boosts up the overall accuracy of ACVC from 63.61% to 65.87%. Table\u00a0<ref> further summarizes the performance comparison on VLCS. The domain shift of this dataset mainly comes from background and view point changes. The scenes in VLCS vary from urban to rural, and the viewpoint tends to favor the side view or non-classical view. As a result, existing data augmentation methods which mainly introduce diverse styles obtain relatively smaller performance gains on VLCS dataset compared to those on PACS. Even in this case, MAD significantly improves the overall accuracy of ERM from 59.56% to 62.95%. Especially when taking \u201cLabelMe\" as source domain, our MAD leads to near 10% improvement in average accuracy. Similar to the observations on PACS, the consistent performance improvements are attained when integrating existing data augmentation approaches with MAD. In particular, MAD increases the accuracy of ACVC from 61.25% to 63.82%. \n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Single-DG on Point Cloud Recognition\n\nSetup and Implementation Details: Different from 2D vision, 3D vision has various modalities to represent data, such as voxel grid, 3D mesh and point cloud. Among them, point cloud is the most straightforward and representative modality, which consists of a set of points with 3D coordinates. To verify the generality of MAD, we conduct experiments on the 3D point cloud domain adaptation dataset PointDA-10\u00a0<cit.>, which consists of three domains: ShapeNet (SH), ScanNet (SC), and ModelNet (M). In our implementation, we adopt the PointNet\u00a0<cit.> as backbone, and apply the SGD optimizer with momentum 0.9. The batch size is set to 64. We set the learning rate to 1e-3. Experiments are executed on a Tesla P40 GPU with PyTorch-1.5. For model selection, similar to the experiments on images, we split the training domain into training and validation subsets, and choose the model with maximal accuracy on validation set to report the OOD performance.\n\nExperiment Results: \nTable\u00a0<ref> lists the performance comparison on PointDA-10. An observation is that the existing data augmentation methods on 2D images do not work well on 3D point clouds. The representative methods, e.g., Mixstyle and DSU, are even inferior to ERM. We speculate that this may be the results of the different types of domain shifts, which typically lie in geometric differences in point clouds rather than texture and style differences in 2D images. Moreover, there is no one-to-one correspondence and order between points, making it difficult to directly generate new point clouds by interpolating two point clouds. This somewhat reveals the weakness of data augmentation, when generalizing to different modalities. MAD, in comparison, benefits from decoupling domain-specific features and domain-generalized features, and constantly enhances these methods. In particular, MAD improves the overall accuracy of ERM/Mixstyle/DSU from 34.57%/33.78%/33.63% to 37.91%/38.16%/36.45%. The results basically indicate the advantage of MAD across different modalities.\n\n\n\n\n\n\n \u00a7.\u00a7 Single-DG on Text Classification\n\nSetup and Implementation Details: In addition to 2D images, and 3D point clouds, we further conduct experiments on cross-domain text classification. We choose the Amazon Reviews\u00a0<cit.> as the benchmark, which contains four different domains on product review, including DVDs (D), Kitchen appliance (K), Electronics (E), and Books (B). The dataset has already been pre-processed into a bag of features (unigrams and bigrams), losing all word order information. Following\u00a0<cit.>, we take the 5,000 most frequent features and represent each review as a 5,000-dimentional feature vector. \nFollowing\u00a0<cit.>, we employ an MLP as feature extractor. We apply the SGD optimizer with momentum 0.9. The batch size is set to 64. We set the learning rate to 1e-3. Experiments are conducted on a Tesla P40 GPU with PyTorch-1.5. We adopt the same model selection strategy as in image and point cloud recognition.\n\n\nExperiment Results: The results shown in Table\u00a0<ref> clearly verify the effectiveness of MAD in comparison to the existing methods. Similar to the observations on 2D images and 3D point clouds, MAD also exhibits performance improvement to existing approaches on text modality. For example, MAD boosts up the accuracy of ERM on Books domain by 3.09%, and leads to 0.94%, 0.89%, and 1.40% gain in overall accuracy to Mixup, Mixstyle, and DSU, respectively. The improvements empirically prove the impact of MAD on text modality.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Single-DG on Semantic Segmentation\n\nSetup and Implementation Details: The aforementioned experiments mainly focus on the single-DG recognition of 1D texts, 2D images and 3D point clouds. In this section, we experiment with 2D images segmentation. As a fundamental ability for autonomous driving, semantic segmentation models often encounter severe performance degeneration due to scenarios change. Here, we conduct experiments on GTA-5\u00a0<cit.> \u2192 Cityscape\u00a0<cit.> datasets, the most widely-used benchmark on semantic segmentation domain adaptation. The experiments are based on FADA released codes\u00a0<cit.>, using DeepLab-V2\u00a0<cit.> segmentation network with ResNet-101\u00a0<cit.> as backbone. We apply the SGD optimizer with momentum 0.9. The batch size is set to 8. We set the learning rate to 5e-4. Experiments are implemented on 4 Tesla P40 GPUs with PyTorch-1.5. Mean Intersection over Union (mIOU) and mean Accuracy (mAcc) for all objects categories are adopted as evaluation metric. \n\nExperiment Results: As a pixel-level classification task, semantic segmentation is much harder than image-level recognition. Table\u00a0<ref> details the results, demonstrating the superiority of MAD against baselines. Specifically, MAD contributes an mIOU increase of 1.9% and 1.5% to ERM and DSU, respectively. The results again verify the merit of MAD on semantic segmentation on 2D images.\n\n\n\n\n \u00a7.\u00a7 Experiments Analysis\n\n\n\n\nAblation Study: To examine the contribution of different components within MAD, we first conduct extensive ablation studies on texts, images, and point clouds recognition. Table\u00a0<ref> summarizes the results. Here, MAD (one-stage) refers to a degraded version of MAD without two-stage learning mechanism. That is, we optimize the biased-branch g_bias and the general-branch g_gen simultaneously. MAD (single-head) indicates that we only capitalize on a single-head classifier in the biased-branch to capture those domain-specific features. As shown in Table\u00a0<ref>, both the multi-head classifier design and the two-stage learning mechanism are effective. The two components complement to each other and both manage the general-branch classifier g_gen to focus more on those domain-generalized features.\n\n\n\nHyper-parameter Sensitivity: Next, we study the hyper-parameter sensitivity of M and T on text classification task. M is the number of the biased-branch classifiers, and T denotes the second-stage training threshold. \nAs shown in Figure\u00a0<ref>, the accuracies are relatively stable when each hyper-parameter varies. In our implementation, we set T to 3. Since M depends on the factors of the introduced domain-specific features, its value differs for different datasets. We set M to 3 for PointDA-10 and GTA-5, 5 for VLCS, and 7 for PACS and Amazon Review.\n\nLow-frequency Component vs High-frequency Component: As pointed out in\u00a0<cit.>, the low-frequency component (LFC) is much more generalizable than high-frequency component (HFC), i.e., LFC typically represents those domain-generalized (semantic) features, and HFC denotes those domain-specific (superficial) features. Here, we conduct experiments in the \u201cLabelMe\" domain of the VLCS benchmark to verify whether MAD encourages classifier to pay more attention to those domain-generalized features, i.e., the LFC. Specifically, for each instance in the validation subset, we decompose the data into LFC and HFC w.r.t different radius thresholds r via applying Fourier transform and inverse Fourier transform. Then, we train the vanilla ERM classifier, the ERM classifier equipped with MAD separately, and evaluate them on LFC and HFC. Figure\u00a0<ref> depicts the results, where r=12/16 low (solid line) denotes the LFC and r=12/16 high (dashed line) denotes the HFC. As shown in this figure, ERM w/ MAD performs much better on LFC than vanilla ERM, with an accuracy improvement of nearly 10%. The results confirm the effectiveness of MAD in improving single-domain generalization, and MAD indeed encourages classifiers to pay more attention to those domain-generalized features (LFC).\n\n\n\n\n \u00a7.\u00a7 Visualization\n\n\n\nIn addition to quantitative performance comparisons, we further present some qualitative illustrative results. Figure\u00a0<ref> (a) first visualizes the confusion matrix on PACS benchmark. The classification model is trained on \u201cCartoon\" domain and evaluated on unseen domain \u201cSketch\". The results show that ERM w/ MAD is less confusing for most categories when testing in unseen domains compared to vanilla ERM. Then, Figure\u00a0<ref> (b) illustrates an example for semantic segmentation. The visualization demonstrates that MAD can enhance the ERM baseline to achieve more precise segmentation results under domain shift, especially for the driveable areas.\n\n\n\n\u00a7 CONCLUSION\n\nIn this paper, we delve into the single domain generalization (single-DG) problem. Different from existing methods that introduce modality-specific data augmentation techniques, we propose a general and versatile modality-agnostic debiasing (MAD) framework for single-DG. MAD starts from the viewpoint of directly strengthening the capability of classifier for identifying domain-specific (superficial) features, and meanwhile emphasizing the learning of domain-generalized (semantic) features. Technically, we have devised a novel two-branch classifier, where a biased-branch is responsible for identifying those superficial features, while the general-branch is encouraged to focus more on those semantic features. MAD is appealing in view that it can be seamlessly incorporated into existing methods to further boost up performances. We have evaluated the effectiveness and superiority of MAD for single-DG via various empirical evidences on a series of tasks, including recognition on 1D texts, 2D images, 3D point clouds, and semantic segmentation on 2D images. In all tasks, MAD can facilitate the state-of-the-art methods to achieve better performance without bells and whistles. \n\nAcknowledgment: \nThis work was partially supported by Shanghai Municipal Science and Technology Major Project (No.2018SHZDZX01), ZJ Lab, and Shanghai Center for Brain Science and Brain-Inspired Technology, and the Shanghai Rising Star Program (No.21QC1400900).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 MORE DETAILS ABOUT DATASETS\n\nIn the main paper, we have validated the effectiveness of our Modality-Agnostic Debiasing (MAD) framework in a variety of single domain generalization (single-DG) scenarios with different modalities, including recognition on 1D texts, 2D images, 3D point clouds, and semantic segmentation on 2D images. Here we provide more details about the adopted datasets in the main paper. The statistics are listed in Table\u00a0<ref>.\n\nIn an effort to qualitatively show the domain shifts in different benchmarks, we further illustrate some examples in Figure\u00a0<ref>. One major observation is that the domain shifts vary a lot between benchmarks. For example, the domain shifts in images (Figure\u00a0<ref> (a), (b), (d)) mostly result from the changes for image contexts, styles, and viewpoints. In point clouds (Figure\u00a0<ref> (c)), the domain shifts primary correspond to geometric variations. Existing single-DG methods are commonly designed for images by devising various data augmentation algorithms to introduce various textures and image styles, making them modality-specific and only applicable to the single modality inputs of images. In contrast, MAD proposes to directly enhance the classifier's ability to identify domain-specific features while emphasizing the learning of domain-generalized features. In this way, a versatile modality-agnostic single-DG paradigm is established by completely eliminating the need for modality-specific data augmentations. MAD is also appealing due to the fact that it can be seamlessly incorporated into existing single-DG methods to further boost up performances.\n\n\n\n\n\u00a7 MORE RESULTS FOR LOW-FREQUENCY COMPONENT VS. HIGH-FREQUENCY COMPONENT\n\nFor images, Low-frequency component (LFC) is commonly considered as domain-generalized features, while High-frequency component (HFC) is regarded as domain-specific features\u00a0<cit.>. Here, we provide more results to support the capacity of MAD enforcing classifiers to pay more attention to domain-generalized features, i.e., LFC. Here we conduct additional experiments in the \u201cPhoto\" and \u201cArt\" domains on PACS benchmark. Implementation details are the same as in the main paper. That is, for each instance in the validation subset, we decompose the image into LFC and HFC w.r.t different radius threshold r via applying Fourier transform and inverse Fourier transform. Then, we train the ERM and the ERM w/ MAD, separately, and evaluate them on LFC and HFC. The results are summarized in Figure\u00a0<ref>, where r= 12/16 low represents the LFC and r=12/16 high depicts the HFC. As shown in this figure, we can conclude that MAD consistently encourages the classifier focus more on those domain-generalized features.\n\n\n\n\u00a7 MORE RESULTS FOR SEMANTIC SEGMENTATION VISUALIZATION\n\nSemantic segmentation models often suffer from performance degradation due to scenario changes.  We exhibit more visualization results of semantic segmentation in Figure \u00a0<ref>. These examples further demonstrate the effectiveness of MAD when integrated into existing data-augmentation based methods (e.g., DSU\u00a0<cit.>).\n\n\nieee_fullname\n\n\n\n"}