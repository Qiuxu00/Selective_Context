{"entry_id": "http://arxiv.org/abs/2303.07352v1", "published": "20230312174332", "title": "Sequential Spatial Network for Collision Avoidance in Autonomous Driving", "authors": ["Haichuan Li", "Liguo Zhou", "Zhenshan Bing", "Marzana Khatun", "Rolf Jung", "Alois Knoll"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI"], "text": "\n\n\n\n\n\nSequential Spatial Network for Collision Avoidance in Autonomous Driving\nThis research is supported by Sino German fond (5091331).\n\n    1st Haichuan Li\nChair of Robotics, Artificial  \n \nIntelligence and Real-Time Systems \n\nTechnical University of Munich\n\nGarching, Germany \n\nhaichuan.li@tum.de\n2nd Liguo Zhou\nChair of Robotics, Artificial  \n \nIntelligence and Real-Time Systems \n\nTechnical University of Munich\n\nGarching, Germany \n\nliguo.zhou@tum.de\n3rd Zhenshan Bing\nChair of Robotics, Artificial  \n \nIntelligence and Real-Time Systems \n\nTechnical University of Munich\n\nMunich, Germany \n\nzhenshan.bing@tum.de\n4th Marzana Khatun\nInstitute for Driver Assistance and \n\n Connected Mobility \n\nHochschule Kempten\n\nKempten, Germany \n\nmarzana.khatun@hs-kempten.de\n5th Rolf Jung\nInstitute for Driver Assistance and \n\n Connected Mobility \n\nHochschule Kempten\n\nKempten, Germany \n\nrolf.jung@hs-kempten.de\n6th Alois Knoll\nChair of Robotics, Artificial Intelligence \n \n and Real-Time Systems \n\nTechnical University of Munich\n\nGarching, Germany \n\nknoll@in.tum.de\n\n    March 30, 2023\n=====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\nSeveral autonomous driving strategies have been applied to autonomous vehicles, especially in the collision avoidance area. The purpose of collision avoidance is achieved by adjusting the trajectory of autonomous vehicles (AV) to avoid intersection or overlap with the trajectory of surrounding vehicles. A large number of sophisticated vision algorithms have been designed for target inspection, classification, and other tasks, such as ResNet, YOLO, etc., which have achieved excellent performance in vision tasks because of their ability to accurately and quickly capture regional features. However, due to the variability of different tasks, the above models achieve good performance in capturing small regions but are still insufficient in correlating the regional features of the input image with each other. In this paper, we aim to solve this problem and develop an algorithm that takes into account the advantages of CNN in capturing regional features while establishing feature correlation between regions using variants of attention. Finally, our model achieves better performance in the test set of L5Kit compared to the other vision models. The average number of collisions is 19.4 per 10000 frames of driving distance, which greatly improves the success rate of collision avoidance.\n\n\n\n\nCollision Avoidance, Computer Vision, Autonomous Driving, Trajectory Prediction, Sequential Spatial\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nIn the past many years, researchers have focused on how to turn vehicles from assisted driving to more intelligent autonomous driving. Due to the iteration of intelligent hardware and the improvement of chip computing power, a large amount of data collected by sensors can be quickly converted and fed into models to make decisions. In the driving process, the safety factor is the first consideration for users and researchers. Therefore, how AV should avoid collisions has become a top priority.  Concepts such as probabilistic methods (eg.: Markov chains\u00a0<cit.> and Monte \n\nCarlo\u00a0<cit.>), safety distance-based control methods\u00a0<cit.>, and trajectory prediction methods\u00a0<cit.> have been designed in recent years to cope with complex traffic conditions. In terms of vision, CNN\u00a0<cit.> has made outstanding contributions and has been applied to a large number of road condition inspection tasks due to its excellent regional feature extraction capabilities. The local feature information obtained by CNN will be used for obstacle detection. Secondly, because the motion trajectory is planned for AV, the relationship between each local feature of the image obtained by CNN needs to be established. Some strategies are based on CNN plus RNN\u00a0<cit.> so that they can deal with sequential graphs as input, eg.: STDN\u00a0<cit.>.\n\n\n\nAlthough the above strategies have performed well in a large number of vision tasks, their performances are still far inferior to similar-sized convolutional neural networks counterparts, such as EfficientNets\u00a0<cit.> and RepVGG\u00a0<cit.>. We believe this is due to the following aspects. First, the huge differences between the sequential tasks of NLP and the image tasks of CV are ignored. For example, when the local feature information acquired in a two-dimensional image is compressed into one-dimensional time series information, how to achieve accurate mapping becomes a difficult problem. Second, it is difficult to keep the original information of inputs since after RNN layers, we need to recover the dimension from one to three. Besides, due to the several transformations between different dimensions, that process becomes even harder, especially since our input size is 224\u00d7224\u00d75. Third, the computational and memory requirement of switching between layers are extremely heavy tasks, which also becomes a tricky point for the algorithm to run. Higher hardware requirements as well as more running time arise when running the attention part.\n\n\nIn this paper, we propose a new network structure based on CNN and attention to vision tasks in autonomous driving. The new network structure overcomes these problems by using Sequential Spatial Network (SSN) blocks. As shown in Fig.\u00a0<ref>, input images first go through the convolution stem for fine-grained feature extraction, and are then fed into a stack of SSN blocks for further processing. The Upsampling Convolutional Decreasing (UCD) blocks are introduced for the purpose of local information enhancement by deep convolution, and in SSN block of features generated in the first stage can be less loss of image resolution, which is crucial for the subsequent trajectory adjustment task. \n\nIn addition, we adopt a staged architecture design using five convolutional layers with different kernel sizes and steps gradually decreasing the resolution (sequence length) and flexibly increasing the dimensionality. Such a design helps to extract local features of different scales and, since the first stage retains high resolution, our design can effectively reduce the resolution of the output information in the first layer at each convolutional layer, thus reducing the computational effort of subsequent layers. The Reinforcement Region Unit (RRU) and the Fast MultiHead Self-Attention (FMHSA) in the SSN block can help obtain global and local structural information within the intermediate features and improve the normalization capability of the network. Finally, average pooling is used to obtain better trajectory tuning. \n\nExtensive experiments on the lykit dataset demonstrate the superiority of our SSN network in terms of accuracy. In addition to image classification, SSN block can be easily transferred to other vision tasks and serve as a versatile backbone.\n\n\n\n\u00a7 RELATED WORKS\n\n\nOver the past few decades, autonomous driving has flourished in the wave of deep learning, where a large number of solution strategies are based on computer vision, using images as the primary input. The prevailing visual neural networks are typically built on top of a basic block in which a series of convolutional layers are stacked sequentially to capture local information in intermediate features. However, the limited receptive field of the small convolution kernel makes it difficult to obtain global information, which hinders the high performance of the network on highly feature-dependent tasks (such as trajectory prediction and planning). In view of this dilemma, many researchers have begun to deeply study self-attention-based\u00a0<cit.> networks with the ability to capture long-distance information. Here, we briefly review traditional CNNs and recently proposed visual networks. Convolutional neural network. The first standard CNN was proposed by LeCun\u00a0<cit.> et al. was used for handwritten character recognition. Based on this foundation, a large number of visual models have achieved cross-generational success in a variety of tasks with images as the main input. Google Inception Net \u00a0<cit.> and DenseNet \u00a0<cit.> showed that deep neural networks consisting of convolutional and pooling layers can yield adequate results in recognition. SENet\u00a0<cit.> and MobileNetV3 \u00a0<cit.> demonstrate the effectiveness of multiple paths within a basic block.\n\nResNet\u00a0<cit.> is a classic structure that has a better generalization ability by adding shortcut connections to the underlying network. To alleviate the limited acceptance domain in previous studies, some studies used the attention mechanism as an operator for adapting patterns.\n\n\n\n\n\n\u00a7 APPROACH\n\n\n\n\n \u00a7.\u00a7 Network Structure Overview\n\n\nOur strategy is to take advantage of both CNN and attention by building a hybrid network. An overview of ResNet-50\u00a0<cit.>, RepVGG\u00a0<cit.>, ViT\u00a0<cit.> and our network are shown in Fig.\u00a0<ref> and Fig.\u00a0<ref>. \n\nResnet-50 consists of five stages, stage0 consists of a convolutional layer, a batch normalization layer and a maxpooling layer. stage1, stage2, stage3, and stage4 consist of bottleneck blocks, and the output is fed to a fully connected layer for classification. The advantage of this design is that resnet50 can efficiently handle the classification problem between different images. However, since the stage0 only uses one convolutional layer with a large convolutional kernel, the large field of view can quickly complete the initial processing of the input image, but the local information capture of the image is slightly insufficient. For this reason, we use the main input block to deal with this limitation. The main input block is composed of five different kernel sizes and steps convolutional layers, so that the purpose of stepping down the convolution kernel is to extract the input information quickly when the output size is large, and then use the small convolution kernel to extract the local information after the input size becomes smaller. \n\nAfter taking into account the fast processing and local information processing of the main input block, the input information is transferred to the subsequent blocks for subsequent processing. Furthermore, between each block, we add a UCD layer, which consists of a convolutional layer with 1\u00d71 kernel size and a downsampling layer which a sampling ratio is 0.5. The UCD layer allows us to speed up the network without reducing the amount of information in the input but maintaining the ratio between the information, and the size of the input is reduced to half of the original size after the UCD layer. Afterwards, the feature extraction is performed by a network layer composed of different numbers of SSN blocks, while maintaining the same resolution of the input. Due to the existence of the self-attention mechanism, SSN can capture the correlation of different local information features, so as to achieve mutual dependence between local information and global information. Finally, the results are output through an average pooling layer and a projection layer as well as a classifier layer. \n\nThrough the historical images of driving trips, we can obtain information such as position, yaw and environment. SSN is similar to a typical vision network and can adjust the stride size of the middle layer to obtain feature maps of different sizes according to the requirements, which can be applied to downstream tasks with different inputs, such as trajectory prediction and image classification.\n\n\n\n\n\n \u00a7.\u00a7 SSN blcok\n\nThe proposed SSN module consists of a Reinforcement Region Unit (RRU), a Fast Multi-Head Self-Attention (FMHSA) module and an Information Refinement Unit (IRU), as shown in Fig.\u00a0<ref>. We will describe these four components in the following.\n\nReinforcement region unit. In vision tasks, data augmentation is usually essential to improve model generalization effectively by training the augmented data. Common data augmentation methods such as flip, rotation and scaling, etc, but adding augmented data should not weaken the final performance of the model.\n\nIn other words, a good model should maintain effective operating output for similar but variant data as well so that the model has better input acceptability. However, the absolute position encoding used in the common attention was originally designed to exploit the order of the tokens, but it breaks the input acceptability because each patch adds a unique position encoding to it \u00a0<cit.>. Moreover, the concatenation between the local information obtained by the information capture module at the beginning of the model and the structural information inside the patch \u00a0<cit.> is ignored. In order to maintain input acceptability, the Reinforcement Region Unit (RRU) is designed to extract the local information from the input to the \"SSN\" module, defined as:\n\n    RRU(X)=Conv(Conv(X)).\n\n\n\nFast Multiple Head Self-Attention (FMHSA) module consists of one convolutional layer, one linear layer and one multi-head self-attention. With this scheme, we can build up the connection between different local information which results from RRU. By this way, the collision avoidance task is able to get an outstanding result since on each frame the trajectory is composed of continuously predicted positions. Moreover, these positions are sequentially related which means autonomous driving vehicles must arrive at the first target position then they can move to the next target position. Our FMHSA module is suitable to solve this problem because it can transfer local information between areas.\n\nThe Information Refinement Unit (IRU) is used to efficiently extract the local information obtained by FMHSA, and after processing by this unit, the extracted local information is fed into the pooling and classifier layers. The original FFN proposed in ViT consists of two linear layers separated by the GELU activation\u00a0<cit.>. First, expand the input dimension by 4 times, and then scale down the expanded dimension: \n\n    FFN(X) = GELU(XW1 + b1)W2 + b2.\n\nThis has the advantage of using a linear function for forward propagation before using the GELU() function, which greatly improves the efficiency of the model operation. However, this strategy leads to a certain performance sacrifice when the network is propagating fast in this region. Our design concept can be used to deal with this problem. First, we use the convolutional layer of a larger convolution kernel to obtain the characteristics of the input information with a large field of view, then use the linear function layer to conduct quickly, and finally use the convolutional layer of a small convolution kernel. The convolutional layer obtains refined information, thus taking into account both operational efficiency and model performance. The expression of the information refinement unit (IRU) can be written as\n\n    IRU(X)=Conv(L(Conv(X))),\n\nwhere L(X)=WX+b. After designing the above three unit modules, the SSN block can be formulated as: \n\n    A   =RRU(X) \n     B   =FMHSA(A) \n     C   =IRU(B)+B\n\nIn the experiment part, we will prove the efficiency of SSN network.\n\n\n\u00a7 EXPERIMENT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we investigate the effectiveness of the SSN architecture by conducting experiments on an autonomous driving obstacle avoidance task based on a driving map as the main input. We compare the proposed SSN with other popular models before showing in Fig.\u00a0<ref>, and then compare the experimental results to draw an analytical conclusion. We defined three different types of collisions which are front collision, rear collision and side collision. These situations are caused by different unsuitable physic parameters and in Fig.\u00a0<ref>.\n\n\n\n \u00a7.\u00a7 Dataset and description\n\n\nWe use l5kit dataset\u00a0<cit.> as our data source which contains over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles along a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the perception output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time. On top of this, the dataset contains a high-definition semantic map with 15,242 labeled elements and a high-definition aerial view over the area.\n\n\n\n \u00a7.\u00a7 Data preprocessing\n\n\nThe data mainly includes the following main concepts: Scenes, Frames, and Agents. A scene is identified by the host (i.e. which car was used to collect it) and a start and end time. It consists of multiple frames (=snapshots at discretized time intervals). The scene datatype stores reference to its corresponding frames in terms of the start and end index within the frames array (described below). The frames in between these indices all correspond to the scene (including the start index, excluding the end index.\n\nA frame captures all information that was observed at a time. This includes the timestamp, which the frame describes; data about the ego vehicle itself such as rotation and position; a reference to the other agents (vehicles, cyclists and pedestrians) that were captured by the ego\u2019s sensors; a reference to all traffic light faces (see below) for all visible lanes. An agent is an observation by the AV of some other detected object. Each entry describes the object in terms of its attributes such as position and velocity, and gives the agent a tracking number to track it over multiple frames (but only within the same scene!) and its most probable label. \n\nThe input of this dataset is images of Ego car, which is one of properties of Ego Dataset. And the output of our model are position and yaw which are properties of EgoDataset as well. By this way, we can simulate vehicles\u2019 driving as human driving actions. During human driving process, drivers control accelerator and driving wheels to move vehicles, accelerator is used for velocity and driving wheel for yaw. The output of our model is also velocity and yaw. Thus, we use this method to simulate the trajectories of vehicles so that we can change velocity and yaw to avoid collisions during driving.\n\n\n\n \u00a7.\u00a7 Result\n\n\nThe tables of test results which are processed by four\ndifferent network structures are shown in Tab.\u00a0<ref>. Compared with other transformer-based and convolution-based\ncounterparts, our model achieved better accuracy and\nfaster processing speed. In particular, our model achieves\n2.6 times on front collision which is 13.6 times less than\nRepVGG, 5.8 times less than ViT, and 12.6 times less\nthan ResNet50, indicating the benefit of SSN block for\ncapturing both local and global information. We can see\nthat SSN consistently outperforms other models by a large\nmargin.\n\n\n\n\n\n\u00a7 CONCLUSION\n\nThis paper proposes a novel hybrid architecture named SSN for vision-based autonomous driving tasks and other vision tasks. The designed SSN architectures take advantages of both CNNs and self-attention to capture local and global information, improving the ability of the sequentially related inputs. Extensive experiments on lykit dataset demonstrate the effectiveness and superiority of the proposed SSN architecture.\nIEEEtran\n\n\n"}