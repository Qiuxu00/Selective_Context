{"entry_id": "http://arxiv.org/abs/2303.07062v1", "published": "20230313123417", "title": "Quantile Online Learning for Semiconductor Failure Analysis", "authors": ["Bangjian Zhou", "Pan Jieming", "Maheswari Sivan", "Aaron Voon-Yew Thean", "J. Senthilnath"], "primary_category": "cs.LG", "categories": ["cs.LG"], "text": "\n\n\nMSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID\n    Jianyang Gu^1 Kai Wang^2 Hao Luo^3 Chen Chen^4 Wei Jiang^1*\n Yuqiang Fang^5 Shanghang Zhang^6 Yang You^2 Jian Zhao^7Co-corresponding authors \n\n^1Zhejiang University ^2National University of Singapore ^3Alibaba Group\n\n^4OPPO Research Institute ^5Space Engineering University ^6Peking University\n\n^7Institute of North Electronic Equipment \n\n{gu_jianyang, jiangwei_zju}@zju.edu.cn zhaojian90@u.nus.edu\n\n\n\n\n\n\n\n\n\n\n\n    March 30, 2023\n=============================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\nWith high device integration density and evolving sophisticated device structures in semiconductor chips, detecting defects becomes elusive and complex. Conventionally, machine learning (ML)-guided failure analysis is performed with offline batch mode training. However, the occurrence of new types of failures or changes in the data distribution demands retraining the model. During the manufacturing process, detecting defects in a single-pass online fashion is more challenging and favoured. This paper focuses on novel quantile online learning for semiconductor failure analysis. The proposed method is applied to semiconductor device-level defects: FinFET bridge defect, GAA-FET bridge defect, GAA-FET dislocation defect, and a public database: SECOM. From the obtained results, we observed that the proposed method is able to perform better than the existing methods. Our proposed method achieved an overall accuracy of 86.66% and compared with the second-best existing method it improves 15.50% on the GAA-FET dislocation defect dataset.\n\n\n\nOnline learning, single-pass, deep neural network, semiconductor failure analysis \n\n\n\n\n\u00a7 INTRODUCTION\n\n\nFor several decades the semiconductor industry follows Moore's law to drive development, and billions of well-designed semiconductor chips are put into use every year. Fine chips require more care about defects. Notably, as the devices are subjected to aggressive channel length scaling, the impact of bridge and dislocation defects on the transport properties of the transistor becomes aggravated. As a result, engineering efforts are essential for semiconductor failure analysis (FA) to detect defects on time and guarantee major components work rightly <cit.>.\n\nLately, Machine Learning (ML) has become increasingly important to accelerate semiconductor FA. There are mainly 2 kinds of the semiconductor dataset used in the ML model to learn and generalize, (1) Unstructured dataset, for example, wafer map imagery used to understand the wafer quality and identify the defects patterns <cit.> and (2) Structured dataset, for example, the transfer characteristics (drain current (I_d) vs. gate voltage (V_g)) of the transistor for different position of bridge defects <cit.>. The structured dataset is favoured by researchers as the features like slope and intercept (for example, sub-threshold swing and threshold voltage for transistor) represent healthy/unhealthy states more discriminatively. The structured datasets' features differ a lot in ranges, which is attributed to the operational principle of a transistor as a logic switch where the drain current varies across \u223c8 orders of magnitude, whereas drain voltage varies from 0-1V (for N-channel Metal-oxide Semiconductor).\n\nThe ML models used for the FA can be divided as traditional ML, such as tree-based model <cit.>, Neuro-fuzzy system (NFS) <cit.>, and Deep Neural Network (DNN) model <cit.>. The DNN models are affected by the high-range features that would take a dominant place, making other features insignificant. Hence, data normalization is necessary for DNN training. Meanwhile, as the number of semiconductor chips is increasing exponentially, the scenario has changed. The offline batch-mode DNN training fails to keep up pace with the large data stream, online single-pass mode DNN training is preferred. This makes the data normalization for the DNN difficult. Though we have many well-designed methods like min-max scaling, z-score normalization <cit.> and quantile normalization <cit.> for offline training, whereas in the online scenario, we lack global statistics requested by the normalization methods. Although some researchers have proposed using sliding windows to get statistics or designing equations/layers to approximate the statistics used in the normalization, most online normalization methods are still designed in batch-mode, whereas single-pass mode is rarely adopted.\n\nIn this paper, we propose a novel quantile online learning method for semiconductor FA. The main contributions of this study are summarized as follows:\n\n(1) Unlike quantile normalization averaging the feature values in batch/mini-batch setting with the same quantile value assigned for different samples, we use quantile numbers to replace raw features. Here, we observe the proposed approach is suitable for semiconductor datasets with varying feature ranges.\n(2) We trained the online DNNs in a purely single-pass mode including the input data normalization, there are very few methods performing semiconductor FA in such a way.\n(3) We propose a discount factor to control the proportion of local or global information in the statistics, which helps with defect detection. \n(4) Our experimental results demonstrate that the proposed method perform better than the existing methods on four semiconductor defect detection datasets, which exceeds the second-best existing method by 15.50%.\n\n\n\n\u00a7 RELATED WORK\n\n\nML-guided FA. Many traditional ML-based approaches have been applied to semiconductor FA. He et al., <cit.> utilized a k-nearest neighbor (KNN) rule-based algorithm to handle faults. Xie et al., <cit.> applied a support vector machine to detect defect patterns in the image. Nawaz et al., <cit.> developed a Bayesian network for etching equipment fault detection. After Lecun et al., <cit.> pointed out that DNN can improve the state-of-the-art methods in many fields, researchers also tried DNN methods, while the traditional methods are still common. Hsu et al., <cit.>, Kim et al., <cit.> and Chien et al., <cit.> proposed some variants of convolutional neural network for FA in image datasets. Pan et al., proposed a tree-based transfer learning model from FinFET to GAA-FET <cit.>. Kim et al., <cit.> devised a generative model to handle process drift in the defect detection scenario. Ferdaus et al., <cit.> devised an online NFS for semiconductor FA. However, most methods are focused on offline settings, this motivates us to design an efficient online learning methods for DNNs.\n\nInput data normalization. Commonly used offline input data normalization strategies are min-max scaling, z-score normalization and decimal scaling <cit.>. However, these techniques need to adapt to online scenarios and adaptive versions are few. To the best of our knowledge, the adaptive normalization methods can be divided into two categories: (1) Ogasawara et al., <cit.>, Gupta et al., <cit.> used sliding window to adapt statistics in streaming data. (2) Passalis et al., <cit.> proposed DAIN that uses additional linear layers to mimic the adaptive mean and standard deviation value for normalization. Tran et al., <cit.> and Shabani et al., <cit.> proposed improved variants of DAIN to handle time-series data. Though these methods were devised for online settings, they are designed in batch-mode. Our proposed method is a single-pass design, which means there is hardly any latency in getting predictions. And quantile normalization is another extremely popular method that could produce well-aligned distributions so all samples are from the same distribution, which is commonly seen in gene expression dataset <cit.>, <cit.>. Note, our proposed method is totally different from quantile normalization; as they would not use the quantile number to replace the feature value.\n\n\n\n\u00a7 QUANTILE ONLINE LEARNING\n\n\nThe proposed quantile online learning (QOL) classifies semiconductor FA datasets on the fly. The flow diagram of the proposed QOL is shown in Fig. <ref> and discussed in detail in the below subsections.\n\n\n\n \u00a7.\u00a7 Problem statement\n\n\nThe semiconductor structured input data stream i.e, X_i=[F_1,...,F_n] \u2208^1 \u00d7 n, i\u2208\ud835\udca9, which means i^th sample and n is the number of dimensions. Each input has a corresponding label Y_i \u2208\ud835\udca9 that represents the defect types; our objective is to address the classification problem of the defect types. To mimic real-world settings, a prequential test-then-train protocol is applied. This protocol would use the model to give predictions and then do the training.\n\n\n\n \u00a7.\u00a7 Buffer list initialization\n\n\nInitially, a small list of samples is collected i.e, C=[X_1,...,\n X_p]\u2208 ^p\u00d7 n, where p is the number of samples, set between 10 to 30. We collect the feature values in a 2D buffer list from samples C i.e, \nV = [[V_11,..,V_1p],\n..[V_n1,..,V_np]]. The V_ij means the feature value F_i from sample X_j. Next, we sort each single 1D list independently and define another 2D weight list W=[[1,..,1],..,[1,..,1]] with all ones. We treat the buffer list V={V_ij}^n\u00d7 p as cluster centres which capture the knowledge of the distribution of raw data, and weight list W= {W_ij}^n\u00d7 p as the number of samples in the cluster. During this phase, we wait for p samples to start, whereas the remaining steps use single-pass mode.\n\n\n\n \u00a7.\u00a7 Cluster centre choosing\n\n\nWe obtain the initial V and W and the raw input features X_i from the data stream. We choose the cluster centres for each feature by calculating distance dis(V_ij,F_i)=||V_ij-F_i||_2=\u221a((V_ij-F_i)^2) and find the index of minimum distance for the k^th feature using \n    idx_k={i|mindis(V_ki,F_k),1\u2264 i \u2264 p },\n \nThe index list idx =[idx_1,...,idx_n] is used as a reference to update later steps.\n\n\n\n \u00a7.\u00a7 Weight list updating\n\n\nThe main idea behind updating is to replace the old cluster centre with the new feature value and make the corresponding weight contribute to the closest cluster centres. We consider 2 cases in updating: (i) F_i \u2265 V_i,idx_i and (ii) F_i < V_i,idx_i. For case (i) in Fig <ref>, old_weight = W_i,idx_i represents the weight for V_i,idx_i before updating. We first calculate a percentage factor as below:\n\n    percentage = dis(V_i,idx_i,F_i)/dis(V_i,idx_i-1,F_i)\n\nThen we update W_i,idx_i, W_i,idx_i-1 using,\n\n    W_i,idx_i   = (1-percentage)\u00d7 old_weight+1\n      \n    \n      W_i,idx_i-1   = W_i,idx_i-1+percentage\u00d7 old_weight\n\nThese equations make sure the closer the value is to the cluster centres, the more weightage is assigned from the old weight. If the W_i,idx_i-1 does not exist, we give all weight to the new cluster using,\n\n    W_i,idx_i   = old_weight+1\n\nFor case (ii) in Fig.<ref>, we still use old_weight as above, and only give modifications to the above equations as follows:\n\n    percentage    = dis(V_i,idx_i,F_i)/dis(V_i,idx_i+1,F_i)\n    \n      W_i,idx_i   = (1-percentage)\u00d7 old_weight+1\n      \n    \n      W_i,idx_i+1   = W_i,idx_i+1+percentage\u00d7 old_weight\n\nFor the specific case W_i,idx_i+1 does not exist, then we use Eq.(<ref>) to update. Next, we introduce a discount factor \u03b7, which is used to control the forget rate. To make the buffer list capture a sense of local trend, we set \u03b7 between 0 to 1. After each sample's all features modification, we would update the whole weight list {W_ij}^n\u00d7 p=\u03b7\u00d7{W_ij}^n\u00d7 p. Finally, we assign V_i,idx_i=F_i as the new cluster centre.\n\n\n\n \u00a7.\u00a7 Quantile approximations\n\n\nAfter we find the suitable position for the current sample's feature values, we replace all the feature values F_i using,\n\n    F_i    =  \u2211_j=1^idx_iW_ij/\u2211_j=1^pW_ij\n\nThe weight for each cluster centres is used to approximate the number of feature values that are less than it so that we obtain an approximation of the quantile for each feature value. Then we feed the normalized data X_i=[New_1,...,New_n] to train the online DNN model.\n\n\n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n\n\nData description.\nIn this work, three real-world multi-class semiconductor fault detection datasets and a public dataset SECOM <cit.> from the UCI machine learning repository are considered. The real-world datasets include bridge defects in GAA-FET (GAA-FET-b), dislocation defects in GAA-FET (GAA-FET-d) and bridge defects in FinFET (FinFET-b). These FETs datasets were generated using a full three-dimensional technology computer aided design (TCAD) digital twin model and then were calibrated with the Current-Voltage curve in the actual device defects <cit.>. The SECOM dataset consists of complex signals for the engineers to analyze the defects. The detailed information of each dataset is shown in Table <ref>. Classifying imbalance classes in the dataset is more challenging as the minority class would only be 3\u223c20% of the majority class, say, SECOM is 104 fails to 1463 pass, GAA-FET-b is 27 to 764 and FinFET-b is 79 to 512. As a result, overall accuracy and class balanced accuracy were used as evaluation metrics.   \n\n\n\n\nExperiment setting. We chose the Adaptive Normalization (AN) from <cit.> and Deep Adaptive Input Normalization (DAIN) from <cit.> as baseline methods to compare the data normalization, and passing the raw data. As the baseline methods are variants of min-max and z-score normalization, which cannot handle feature values with outliers pretty well. Hence, we additionally pre-processed the data by logarithm transformation to make the feature range small, which is not adapted for our proposed method. We chose three online DNN models, which include a DNN with 2 layers fully connected feed-forward network, Deep Evolving Denoising Autoencoder (DEVDAN) from <cit.> and Autonomous Deep Learning (ADL) model from <cit.> to classify the normalized data in a single-pass mode. The ADL and DEVDAN are two evolving architectures designed for online settings, which would autonomously generate the needed hidden nodes. To make a fair comparison, we limited different models to all have 2 layers and the nearly same amount of parameters. We intended to prove our method's robustness by using these models. We applied the prequential test-then-train protocol to the whole datasets, except for GAA-FET-d as current online learning models rarely get good performance in limited sequences (90 samples). We split and augment the train set and evaluate the test set with the test-then-train protocol.  \n\nWe fine-tuned the hyper-parameters in the following range: [0.1,0.5] as lr_disc and [1e^-3,1e^-1] as lr_gen for DEVDAN <cit.>; [0.05,0.35] as lr for ADL <cit.>; [1e^-4,1e^-1] as lr for DNN; batch size 2 to 5 and threshold \u03b4 0.1 to 0.3 for AN <cit.>; mean_lr in [1e^-8,1e^-1], scale_lr in [1e^-8,1e^-1], gate_lr in [1e^-3,1] and batch size 5 for DAIN <cit.>. We select the batch size to be small so that it's a relatively fair comparison with our single-pass method. \n\n\n\n\nPerformance comparisons. We compare the performance of 3 data normalization approaches (AN, DAIN, QOL) and Raw data on semiconductor datasets that are combined with 3 DNN models separately. For the 3 multi-class semiconductor datasets, from Table <ref>, we observe that our QOL performs well for both overall and balanced accuracy while the AN was the second best, and even got better overall accuracy in GAA-FET-b. However, the DAIN method has low performance, due to (1) the data sequence being limited, and DAIN's converge rate being affected by more network parameters. (2) a small batch size may not be suitable for this method. Compared with the Raw data, AN and QOL's result is a strong demonstration of the need for data normalization. For the SECOM, we emphasize the balanced accuracy here as most methods can get high overall accuracy by predicting all as the majority. Our proposed QOL achieves the best balanced accuracy, while the DAIN, as well as AN, also performed better for this high-dimensional complex dataset. All the normalization methods can address the imbalance problems a bit, which may be due to normalized features being more discriminative. Among all the datasets, we find that our single-pass QOL is more stable in different settings and gets comparable or better performance than these batch-mode normalization methods.\n\n\n\n\u00a7 CONCLUSIONS\n\n\nIn this paper, we proposed a novel quantile online learning approach to address single-pass online classification for semiconductor FA. QOL utilizes the single-pass online framework where quantile representation is passed to DNN architecture to adapt to the change in the feature distribution of different semiconductor defects by prequential test-then-train protocol. This helps in achieving better classification performance in both overall accuracy and balanced accuracy. We compared the performance of our technique with the existing methods for a binary class and three multi-class semiconductor defect datasets, for all these datasets the proposed method performed better in comparison with the existing methods.\n\n\n\n\nIEEEbib\n\n\n"}