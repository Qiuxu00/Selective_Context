{"entry_id": "http://arxiv.org/abs/2303.07338v1", "published": "20230313175902", "title": "Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need", "authors": ["Da-Wei Zhou", "Han-Jia Ye", "De-Chuan Zhan", "Ziwei Liu"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRevisiting Class-Incremental Learning with Pre-Trained Models:\n\nGeneralizability and Adaptivity are All You Need\n    Da-Wei Zhou^1, Han-Jia Ye^1[2], De-Chuan Zhan^1, Ziwei Liu^2[2]\n\n\t^1 State Key Laboratory for Novel Software Technology, Nanjing University\n\t^2 \n\tS-Lab, Nanyang Technological University\n\n\t{zhoudw, yehj, zhandc}@lamda.nju.edu.cn,  ziwei.liu@ntu.edu.sg\n\n    March 30, 2023\n===============================================================================================================================================================================================================================================================\n\n\n\nempty\n\n[1]Work done when Da-Wei Zhou was a visiting scholar at NTU\n[2]Correspondence to: Han-Jia Ye and Ziwei Liu\n\n\n\t\n\tClass-incremental learning (CIL) aims to adapt to emerging new classes without forgetting old ones. \n\tTraditional CIL models are trained from scratch to continually acquire knowledge as data evolves.\n\tRecently, pre-training has achieved substantial progress, making vast pre-trained models (PTMs) accessible for CIL. \n\tContrary to traditional methods, PTMs possess generalizable embeddings, which can be easily transferred for CIL.\n\tIn this work, we revisit CIL with PTMs and argue that the core factors in CIL are adaptivity for model updating and generalizability for knowledge transferring. \n\t1) We first reveal that frozen PTM can already provide generalizable embeddings for CIL.\n\tSurprisingly, a simple baseline (SimpleCIL) which continually sets the classifiers of PTM to prototype features can beat state-of-the-art even without training on the downstream task. \n\t2) Due to the distribution gap between pre-trained and downstream datasets, PTM can be further cultivated with adaptivity via model adaptation. We propose ADapt And Merge (), which aggregates the embeddings of PTM and adapted models for classifier construction. \n\tis a general framework that can be orthogonally combined with any parameter-efficient tuning method, which holds the advantages of PTM's generalizability and adapted model's adaptivity. \n\t3) Additionally, we find previous benchmarks are unsuitable in the era of PTM due to data overlapping and propose four new benchmarks for assessment, namely ImageNet-A, ObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate the effectiveness of with a unified and concise framework. \n\tCode is available at: <https://github.com/zhoudw-zdw/RevisitingCIL>\n\t\n\n\n\n\u00a7 INTRODUCTION\n\n\nWith the advancement of deep learning, deep models have achieved impressive feats in many fields\u00a0<cit.>. However, most research focuses on recognizing a limited number of classes in static environments. In the real world, applications often deal with streaming data with incoming new classes\u00a0<cit.>. To address this issue, Class-Incremental Learning (CIL) has been proposed, which allows the model to learn from the evolving data and continuously build a unified classification model. Nevertheless, when new classes are added sequentially, the notorious catastrophic forgetting occurs<cit.>, which erases the previously learned knowledge. \nMany prior works\u00a0<cit.> are designed to continually build a holistic embedding without forgetting.\n\n\n\n\n\n\n\n\n While typical methods assume that the model is \u201ctrained from scratch,\u201d\n recent advancements in pre-training\u00a0<cit.> have made Pre-Trained Models (PTMs) more accessible for designing models in downstream tasks. These PTMs are often trained on massive corpus\u00a0<cit.> or countless images\u00a0<cit.> with handcrafted tricks\u00a0<cit.>, resulting in strong generalizability. Consequently, several methods\u00a0<cit.> propose to leverage PTM for better incremental learning. \n\n\nPowerful PTMs alleviate the burden of the learning process, substantially surpassing the performance upper bound of non-PTM-based methods\u00a0<cit.>. However, upon revisiting the objective of CIL, we find \nessential differences between these protocols.  Without PTMs, CIL models are trained to continually acquire the knowledge of new classes and build a unified embedding space, which requires the adaptivity for sequential updating. \nIn contrast, PTMs are trained with massive datasets, which makes it easier to achieve an ideal knowledge and embedding space with strong generalizability.\nTo use a human learning analogy, non-PTM methods aim to teach an infant to grow up and continually acquire knowledge through college, while PTM-based methods rely on a professor to do the same thing, which is as easy as pie.\n\nTo evaluate the generalizability of PTM, we formulate a CIL task using VTAB\u00a0<cit.> dataset and test the performance of state-of-the-art PTM-based methods\u00a0<cit.> with pre-trained ViT-B/16-IN1K in Figure\u00a0<ref>. \n As a comparison, we present a simple baseline SimpleCIL to evaluate the quality of the pre-trained feature. With the pre-trained embedding function frozen, SimpleCIL sets the classifier weights to the average embeddings\u00a0<cit.> of each new class for classification. If PTMs already possess generalizable features, directly matching the average pattern to each query instance could also achieve competitive results. To our surprise, we find that SimpleCIL outperforms the current SOTA by 5% even without any tuning on these downstream tasks, verifying its strong generalizability in knowledge transfer.\n \n \nAlthough PTMs are generalizable for CIL, a domain gap may still exist between pre-trained and incremental datasets\u00a0<cit.>.\nFor instance, the ImageNet pre-trained model may not generalize well to out-of-distribution\u00a0<cit.> or specialized tasks\u00a0<cit.>. Under such circumstances, freezing the embedding for knowledge transferring is not a \u201cpanacea.\u201d Accordingly, adaptivity becomes essential to enable the model to grasp task-specific features. Nevertheless, sequentially tuning the PTM will harm the structural information and weaken the generalizability\u00a0<cit.>, leading to the irreversible forgetting of previously learned knowledge. Is there a way to unify the generalizability of PTM with the adaptivity of the adapted model?\n\n\n\nIn this paper, we present ADapt And Merge (), which employs PTM to enhance generalizability and adaptivity in a unified framework. To improve adaptivity, we adapt the PTM in the first incremental stage via parameter-efficient tuning. Adapting the model helps to obtain task-specific features and fills the domain gap between PTM and incremental data.\nWe then concatenate the adapted model with the PTM to extract average embeddings as the classifier, thereby maintaining generalizability.\n restricts model tuning in the first stage, striking a balance between adaptivity and generalizability. \n Moreover, typical CIL benchmarks, such as ImageNet100/1000, are unsuitable for evaluation due to overlapping between pre-trained and downstream tasks.\n  Therefore, we benchmark PTM-based CIL with four new datasets that have large domain gaps with the pre-trained data. Extensive experiments under various settings demonstrate the effectiveness of .\n\n\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\nClass-Incremental Learning (CIL): enables a learning system to continually incorporate new concepts without forgetting old ones\u00a0<cit.>. Typical CIL methods can be roughly divided into four categories. The first group saves and replays exemplars from old classes to recover former knowledge\u00a0<cit.>. The second group utilizes knowledge distillation\u00a0<cit.> to align the outputs of old and new models, thereby maintaining knowledge of old concepts\u00a0<cit.>. \nThe third group rectifies the inductive bias in the incremental model through normalization and logit/feature adjustment\u00a0<cit.>.\nLastly, other works expand the network when needed to enhance representation ability. The network expansion techniques are further divided into neuron-wise\u00a0<cit.>, backbone-wise\u00a0<cit.>, and token-wise\u00a0<cit.>. \n\nCIL with PTM: is becoming a popular topic with the increasing prevalence of PTMs\u00a0<cit.>. The aim is to sequentially adjust the PTM to stream data with new classes. \nL2P\u00a0<cit.> applies visual prompt tuning\u00a0<cit.> to CIL based on the pre-trained Vision Transformer\u00a0<cit.> and learns a prompt pool to select the instance-specific prompt. \n DualPrompt\u00a0<cit.> extends L2P with two kinds of prompts, , general and expert prompts. Different from the key-value search in L2P, CODA-Prompt\u00a0<cit.> improves the prompt selection process with an attention mechanism. <cit.> explores the anchor-based energy self-normalization strategy to aggregate multiple pre-trained classifiers. When changing ViT into CLIP\u00a0<cit.>, <cit.> extend L2P by learning prompts for both text and image modalities\u00a0<cit.>.\n\nParameter-Efficient Tuning for PTM: aims to adapt the PTM to downstream tasks by tuning only a small number of (extra) parameters. Compared to fully finetuning, parameter-efficient tuning obtains competitive or even better performance at a much lower cost. VPT\u00a0<cit.> prepends tunable prefix tokens\u00a0<cit.> to the input or hidden layers. LoRA\u00a0<cit.> learns low-rank matrices to approximate parameter updates. <cit.> learn extra adapter\u00a0<cit.> modules with downsize and upsize projection. <cit.> merges the learned adapters with a fusion module. SSF\u00a0<cit.> addresses the scaling and shifting operation for model tuning. Apart from additional modules in the network, <cit.> proposes learning tunable parameters in the input space.\nFinally, <cit.> formulates these works in a unified framework, and NOAH\u00a0<cit.>  searches for the optimal design of prompt modules for downstream tasks.\n\n\n\n\n\u00a7 FROM OLD CLASSES TO NEW CLASSES\n\nThis section first introduces the setting of class-incremental learning and then discusses  naive baselines to utilize the pre-trained model for CIL.\n\n\n \u00a7.\u00a7 Class-Incremental Learning\n\nClass-incremental learning aims to learn from an evolving data stream with new classes to build a unified classifier\u00a0<cit.>. There is a sequence of B training tasks {^1, ^2, \u22ef, ^B}, where ^b={(_i^b, y_i^b)}_i=1^n_b is the b-th incremental step with n_b instances. Here, the training instance _i^b \u2208^D belongs to class y_i \u2208 Y_b, where Y_b is the label space of task b. Y_b  \u2229 Y_b^' = \u2205 for b\u2260 b^'. During the b-th training stage, we can only access data from ^b for model updating.\nThis paper focuses on the exemplar-free CIL setting\u00a0<cit.>, where no historical data can be fetched for rehearsal.\nThe goal of CIL is to incrementally build a unified model for all seen classes, , acquiring knowledge from new classes and meanwhile preserving knowledge from former ones. \nThe model's capability is evaluated over all seen classes \ud835\udcb4_b=Y_1 \u222a\u22ef Y_b after each incremental task. Formally, the target is to fit a model f(): X\u2192\ud835\udcb4_b that minimizes the empirical risk across all testing datasets:\n\n    \u2211_(\ud835\udc31_j, y_j) \u2208\ud835\udc9f_t^1\u222a\u22ef\ud835\udc9f_t^b\u2113(f\n    (_j), y_j)  ,\n\nwhere \u2113(\u00b7,\u00b7) measures the discrepancy between prediction and ground-truth label.  \ud835\udc9f_t^b denotes the testing set of task b. A good CIL model satisfying Eq.\u00a0<ref> has discriminability among all classes, which strikes a balance between learning new classes and remembering old ones.\n\n\nFollowing <cit.>, we assume the availability of a pre-trained model (, a ViT\u00a0<cit.> or ResNet\u00a0<cit.>) on ImageNet\u00a0<cit.>, which we use as the initialization of f(). For clarity, we decouple the deep model into two parts: f()=W^\u22a4\u03d5(), where \u03d5(\u00b7):\u211d^D\u2192\u211d^d is the embedding function and W\u2208\u211d^d\u00d7 |\ud835\udcb4_b| is the classification head. We denote the classifier for class k as _k: W=[_1,\u22ef,_|\ud835\udcb4_b|]. \nWe refer to the features after pooling as \u03d5() for convolutional networks.\nIn a plain ViT, the input encoding layer transforms the image into a sequence-like output features _e\u2208^L\u00d7 d, where L is the sequence length. We assume the first token in _e as the  token to simplify notation. _e is then fed into the subsequent layers (, multi-head self-attention and MLP) to produce the final embeddings. We treat the embedded  token as \u03d5() for ViT.\n\n\n\n\n\n \u00a7.\u00a7 Adaptivity and Generalizability in CIL\n\n\n\n\nCIL with Adaptivity: Before introducing PTMs into CIL, models are trained from scratch to gradually acquire knowledge of new classes. The naive idea is to update the incremental model with cross-entropy loss, which equips the model with adaptivity to adapt to new tasks: \n\n    \u2112=\t\u2211_(\ud835\udc31_i, y_i) \u2208\ud835\udc9f^b\u2113(f\n    \t(_i) , y_i)+ \u2112_reg ,\n\nwhere \u2112_reg stands for the regularization terms to resist forgetting, , knowledge distillation\u00a0<cit.>.\n\n\n\n\n \n\nCIL with Generalizability: PTMs are born with generalizability, which can be transferred to downstream tasks. \nSpecifically, we define a simple baseline, SimpleCIL, to transfer PTM for incremental tasks.\nWith the embedding function \u03d5(\u00b7) frozen throughout the learning process, we extract average embedding (, prototype\u00a0<cit.>) of each class:\n\n    _i=1/K\u2211_j=1^|\ud835\udc9f^b|\ud835\udd40(y_j=i)\u03d5(_j) ,\n\nwhere K=\u2211_j=1^|\ud835\udc9f^b|\ud835\udd40(y_j=i), \ud835\udd40(\u00b7) is the indicator function. The averaged embedding represents the most common pattern of the corresponding class. We set the prototype as the classifier, , _i=_i, to directly adjust the PTM for CIL. SimpleCIL demonstrates competitive performance in Figure\u00a0<ref>, confirming the strong generalizability of PTM.\n\n\nGeneralizability vs. Adaptivity:\nEq.\u00a0<ref> and Eq.\u00a0<ref> address different aspects of CIL models. The former aims to enhance the adaptivity by enabling the model to be gradually tuned. By contrast, the latter highlights the model's generalizability by freezing it throughout the learning process. \nTo understand their roles in CIL, we conduct an experiment on CIFAR100 with 20 incremental tasks, and compare the performance of finetuning versus SimpleCIL. These methods are based on pre-trained ViT-B/16-IN21K, and we separately report the performance of new (Y_b) and old (\ud835\udcb4_b-1) classes in Figure\u00a0<ref>. Specifically, SimpleCIL relies on the generalizability of PTM, which works competitively even without training on the target dataset. However, it can be further improved to grasp the task-specific features, and finetuning shows better performance in new classes with the help of adaptivity. However, finetuning suffers catastrophic forgetting of old classes since features are continually changing. To summarize, these characteristics are two core aspects of CIL \u2014 adaptivity enables the model to bridge the domain gap, while generalizability encourages knowledge transfer. Therefore, both of them should be cultivated to facilitate CIL.\n\n\n\n\u00a7 ADAM: ADAPT AND MERGE PTMS FOR CIL\n\n\nMotivated by the potential for enhancing both generalizability and adaptivity, can we achieve these characteristics in a unified framework? Specifically, we aim to achieve this goal from two aspects. On the one hand, to bridge the domain gap between the PTM and downstream datasets, model adaptation is essential to move the PTM towards incremental data. On the other hand, since the adapted model may lose the generalizability of high-level features, we attempt to merge the adapted model and PTM into a unified network for future tasks. The merged embedding function is kept frozen throughout the incremental learning process, transferring the generalizable embedding of model sets to incoming new classes. In this way, generalizability and adaptivity are achieved in the unified framework.\n\nWe first introduce the general framework of and then discuss the specific techniques for model adaptation.\n\n\n\n \u00a7.\u00a7 Training Procedure of \n\n\nAlthough PTMs have discriminating features, there may exist a significant domain gap between the pre-trained dataset and incremental data. For example, the PTM is optimized to capture the characteristics of classes in ImageNet, \nwhile the incremental data stream may correspond to specialized data that requires domain knowledge or has extensive concept drift from ImageNet.\nTo bridge this gap, an adapting process can be developed with the incremental data:\n\n    f^*()=\u2131(f(),,\u0398)  ,\n\nwhere the adapting algorithm \u2131 takes the current model f() and the dataset  as input. It optimizes the parameter set \u0398 and produces the adapted model f^*() that gains the domain-specific knowledge in the corresponding dataset. We introduce the variations of \u2131 in Section\u00a0<ref>.\nIf we could obtain all the incremental training sets at once, adapting the model via \u2131(f(),^1\u222a^2\u22ef\u222a^B,\u0398) can transfer the knowledge from PTM to the incremental dataset and grasp the task-specific features for better performance.\n\nHowever, since data in CIL arrive sequentially, we cannot hold the training sets at once. Continuously adapting the model would consequently result in catastrophic forgetting (as shown in Figure\u00a0<ref>). Hence, a naive solution is to adapt the model only in the first incremental stage:\n\n    f^*()=\u2131(f(),^1,\u0398)  .\n\nSince ^1 is a subset of the incremental data stream, it also possesses domain-specific knowledge that could facilitate model adaptation.\nThe tuning process enhances the adaptivity of the CIL model, and the next question is to ensure generalizability. Since  Eq.\u00a0<ref> forces the original generalizable feature to become more specialized to the downstream task, high-level features irrelevant to ^1 shall be overwritten and forgotten. Therefore, a better solution is to concatenate the features extracted by the PTM and the adapted model, , [\u03d5^*(),\u03d5()], where \u03d5^*() and \u03d5() stand for the adapted and pre-trained embedding functions, respectively.\n\n To maintain generalizability, we freeze the concatenated embedding functions [\u03d5^*(\u00b7),\u03d5(\u00b7)] after adaptation and extract prototypes for the following classes:\n \n    _i=1/K\u2211_j=1^|\ud835\udc9f^b|\ud835\udd40(y_j=i)[\u03d5^*(_j),\u03d5(_j)] ,\n\nwhere K=\u2211_j=1^|\ud835\udc9f^b|\ud835\udd40(y_j=i). In comparison to Eq.\u00a0<ref>, Eq.\u00a0<ref> contains additional information from the adapted model, which incorporates domain-specific features for better recognition. These prototypes reveal the most common patterns from the adapted and pre-trained models, ensuring both generalizability and adaptivity. We directly adopt the class prototype as the classifier weight, , _i=_i, and utilize a cosine classifier for classification: f(\ud835\udc31)=(W/W_2)^\u22a4([\u03d5^*(),\u03d5()]/[\u03d5^*(),\u03d5()]_2). Based on the similarity between instance embedding and class prototype, it assigns a higher probability to the class with a more similar prototype.\n \n\nEffect of Adapt and Merge: We give the visualizations of in Figure\u00a0<ref> (left). Although ^1 is a subset of the entire training set, adapting with it still helps transfer the PTM from the upstream dataset to the downstream task. The adapting process can be viewed as a further pre-training procedure, which adapts the PTM to the incremental dataset and bridges the domain gap. By merging the embedding functions of the PTM and the adapted model, the extracted features are more representative than any one of them alone. Additionally, since the model is only trainable in the first incremental task, the efficiency of is comparable to SimpleCIL, which does not require sequential tuning. On the other hand, since the model is frozen in the subsequent tasks, it does not suffer catastrophic forgetting of former concepts. We give the pseudo-code of in Algorithm\u00a0<ref>. \nIn the extreme case where the adaptation process in Line\u00a0<ref> does nothing to the PTM, will degrade to SimpleCIL, which guarantees the performance lower bound.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Adapting the PTM\n \n\nTo bridge the distribution gap between the pre-trained and incremental datasets, 's performance depends on the effective adapting algorithm \u2131.\nIn this section, we discuss six specializations of \u2131 in that can handle different types of PTMs, such as ViTs and CNNs.\n\n\n\nFully Finetune: is a naive idea when transferring the model to downstream tasks. It involves tuning all parameters in the adapting process, , \u0398=\u03b8_\u03d5\u222a\u03b8_W, and minimizing the discrepancy between model's output and the ground truth:\n\n    min_\u03b8_\u03d5\u222a\u03b8_W\u2211_(\ud835\udc31_j, y_j) \u2208\ud835\udc9f^1\u2113(f\n    \t(_j), y_j)  .\n\nHowever, the tuning cost could be relatively high for large-scale PTMs, , ViTs. Therefore, some parameter-efficient tuning techniques can alleviate the tuning cost.\n\nVisual Prompt Tuning (VPT)\u00a0<cit.>: is a lightweight tuning technique for adapting ViTs, which only prepends some learnable prompts \ud835\udc0f\u2208^p\u00d7 d to form the extended features [\ud835\udc0f,_e], where _e is the encoded features of the input image. The extended features are then fed into the subsequent layers of ViT to calculate the final embeddings. There are two variations of VPT: VPT-Deep, which prepends the prompts at every attention layer, and VPT-Shallow, which only prepends the prompts at the first layer. During optimization, it freezes the pre-trained weights in the embedding function and optimizes these prompts and classification head, , \u0398=\u03b8_\ud835\udc0f\u222a\u03b8_W.\n\nScale & Shift (SSF)\u00a0<cit.>: aims to adjust the feature activation by scaling and shifting. It  appends an extra SSF layer after each operation layer (, MSA and MLP) and adjusts the output of these operations. Given the input _i\u2208^L\u00d7 d, the output _o\u2208^L\u00d7 d is formulated as:\n\n    _o=\u03b3\u2297_i+\u03b2 ,\n\nwhere \u03b3\u2208^d and \u03b2\u2208^d are the scale and shift factors, respectively. \u2297 is Hadamard product (element-wise multiplication). The model optimizes the SSF layers and classifier, , \u0398=\u03b8_SSF\u222a\u03b8_W, to trace the features of new tasks.\n\nAdapter\u00a0<cit.>: is a bottleneck module which contains a down-projection W_down\u2208^d\u00d7 r to reduce the feature dimension, a non-linear activation function, and an up-projection W_up\u2208^r\u00d7 d to project back to the original dimension. We follow\u00a0<cit.> to equip the original MLP structure in ViT with the adapter. Denote the input of the MLP layer as _\u2113, the output of AdaptMLP is formatted as:\n\n    MLP(_\u2113)+ReLU(_\u2113 W_down)W_up .\n\nWith pre-trained weights frozen, it optimizes the adapter and classification head, , \u0398=\u03b8_W_down\u222a\u03b8_W_up\u222a\u03b8_W.\n\nBatch Normalization Tuning: If the PTM is a residual network, we can adjust the BN\u00a0<cit.> parameters. Since the running mean and variance in BN are compatible with the upstream data distribution, they could be unstable for downstream tasks. Correspondingly, we can zero the running statistics in BN and adapt to the current data via forward passing. No backpropagation is required.\n\nDiscussions: We visualize the adapting process of in Figure\u00a0<ref>.\nCompared to fully finetuning, parameter-efficient tuning adjusts the PTM towards the downstream task and preserves its generalizability. The adapted model can capture the specialized features in the incremental data, leading to better adaptivity. Since L2P and DualPrompt are based on pre-trained ViT, they cannot be deployed with CNN. In contrast, is a general framework that efficiently handles diverse structures. Specifically, can be combined with VPT/SSF/Adapter for ViT and SSF/BN Tuning for CNN.\nSince adopts the prototype-based classifier, the linear classifier W will be dropped after adaptation.\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\nIn this section, we compare with state-of-the-art methods on benchmark datasets to show the superiority. Due to the overlap between pre-trained datasets and traditional CIL benchmarks, we also advocate four new benchmarks for evaluating PTM-based methods. Ablations and visualizations verify the effectiveness of with new classes. We also explore the performance of different PTMs in CIL. \nMore details and extra results are included in the supplementary. \n\n\n\n\n \u00a7.\u00a7 Implementation Details\n\nDataset: Following\u00a0<cit.>, we evaluate the performance on CIFAR100\u00a0<cit.>, CUB200\u00a0<cit.>, and ImageNet-R\u00a0<cit.>. Since PTMs are often trained with ImageNet21K\u00a0<cit.>, evaluating PTM-based methods with ImageNet is meaningless. Hence, we advocate four new datasets that have large domain gap with ImageNet, namely ImageNet-A\u00a0<cit.>, ObjectNet\u00a0<cit.>, Omnibenchmark\u00a0<cit.> and VTAB\u00a0<cit.>. \nAmong them, ImageNet-A and ObjectNet contain challenging samples that ImageNet pre-trained models cannot handle, while\nOmnibenchmark and VTAB contain diverse classes from multiple complex realms.\nTo construct the CIL task, we sample 200 classes from ObjectNet and ImageNet-A, and 300 from Omnibenchmark. We sample 5 datasets from VTAB, each containing 10 classes, to construct the cross-domain CIL setting. More details are reported in the supplementary.\n\n\n\n\t\nDataset split: Following the benchmark setting\u00a0<cit.>, we adopt two types of dataset splits, , training from half and training from scratch. We unify them as `B/Base-m, Inc-n,' which means the first incremental dataset contains m classes, and each following dataset contains n classes. m=0 means the total classes are equally divided into each task. All classes are randomly shuffled with the same random seed\u00a0<cit.> before splitting for fair comparison.\n The testing set is the same as the original one to evaluate holistically.\n\n\n\n\nComparison methods: We first compare to\nthe state-of-the-art PTM-based CIL methods L2P\u00a0<cit.> and DualPrompt\u00a0<cit.>. We also modify classical CIL methods LwF\u00a0<cit.>, DER\u00a0<cit.>, FOSTER\u00a0<cit.>, MEMO\u00a0<cit.>, FACT\u00a0<cit.> to utilize the same PTM as the initialization. Apart from SimpleCIL, we also report the baseline, sequentially tuning the model, denoted as Finetune. All methods are initialized with the same PTM.\n\n\nTraining details:\nWe use PyTorch\u00a0<cit.> to deploy all models on Tesla V100 with the same network backbone.\nAs there are various PTMs publicly available\u00a0<cit.>, we follow\u00a0<cit.> to choose the most representative ones, \n denoted as ViT-B/16-IN1K and ViT-B/16-IN21K. Both are pre-trained on ImageNet21K, while the former is additionally finetuned on ImageNet1K. \n During adaptation, we train the model with a batch size of 48 for 20 epochs and use SGD with momentum for optimization.\n The learning rate starts from 0.01 and decays with cosine annealing. The prompt length p is 5 for VPT, and the projection dim r is 16 for Adapter.\n The source code will be publicly available upon acceptance. \n\n\n\nEvaluation protocol:  Following\u00a0<cit.>, we denote the Top-1 accuracy after the b-th stage as \ud835\udc9c_b. \nWe use \ud835\udc9c_B (the performance after the last stage) and \ud835\udc9c\u0305=1/B\u2211_b=1^B\ud835\udc9c_b (average performance along incremental stages) as measurements.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Benchmark Comparison\n\n\n\nWe report the incremental performance against SOTA methods in Table\u00a0<ref>, where all methods are based on the pre-trained ViT-B/16-IN21K. We also train these models with pre-trained ViT-B/16-IN1K and show the incremental trend in Figure\u00a0<ref>\u223c<ref>. These data splits include settings with large and small base classes for a holistic evaluation. \n\n\nFirstly, we can infer that the embeddings of PTMs are generalizable and can be directly applied for CIL to beat the SOTA. Specifically, the baseline SimpleCIL outperforms DualPrompt by 20% on CUB and 8% on ImageNet-A in terms of \ud835\udc9c_B. \nHowever, strong PTMs can be further improved if they are adapted by , as downstream tasks have a large domain gap with the pre-trained dataset. \nSpecifically, we find consistently outperforms SimpleCIL in seven benchmark datasets. \nIn contrast, sequentially finetuning the model suffers severe forgetting, which verifies the effectiveness of the adapt and merge protocol.\nSince only requires tuning the PTM in the first stage, it requires less training time and extra parameters than L2P and DualPrompt, as shown in Figure\u00a0<ref>.\nAmong the variations of adapting techniques, SSF and Adapter are more efficient than VPT. \nWe also compare to SOTA traditional CIL methods and modify their backbones into pre-trained ViT for a fair comparison. However, we can infer from Table\u00a0<ref> that these methods cannot work competitively without exemplars. \n\n\n\n\nApart from ViTs, also works well with pre-trained CNNs. We adopt the pre-trained ResNet18\u00a0<cit.> for evaluation and plot the incremental performance in Figure\u00a0<ref>,<ref>. Results show that consistently boosts the performance of pre-trained ViTs and CNNs.\n\nLastly, as shown in Table\u00a0<ref>, the performance on typical benchmarks is approaching saturation as they have a small domain gap with ImageNet. By contrast, due to the large domain gap between our newly established benchmarks and ImageNet, there is still space for improvement, indicating the effectiveness and necessity of these new benchmarks.\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Ablation Study\n\n\nDownscale features: Since the feature of is aggregated with PTM and adapted model, which is twice that of a PTM. We conduct an ablation with w/ SSF on CIFAR100 Base50 Inc5 to show whether these features are essential for CIL. Specifically, we train a PCA\u00a0<cit.> model in the first incremental stage to reduce embedding dimension for the following stages. Denote the target dimension as k, we train the PCA model PCA([\u03d5^*(),\u03d5()]):^d\u2192^k, and append it to the feature extractor. Hence, the features and prototypes are projected to k dimensions. We plot the performance with the change of k in Figure\u00a0<ref>. Specifically, obtains competitive performance to DualPrompt (with 768 dims) even if the features are projected to 50 dims. We also experiment by randomly sampling k features from the original feature space and report the results in Figure\u00a0<ref>. The conclusions are consistent with the former ones, showing that randomly sampling 200 dimensions of achieves the same  performance scale as DualPrompt. The accuracy-dimension curves are shown in Figure\u00a0<ref>.\n\n\n\nSub-modules: \nSince is concatenated with PTM and adapted model, we conduct ablations on ImageNet-A Base100 Inc5 with ViT-B/16-IN21K to compare w/ Finetune and its sub-modules.\nSpecifically, we build SimpleCIL with \u03d5(\u00b7) and \u03d5^*(\u00b7), respectively, denoted as SimpleCIL-PTM and SimpleCIL-Adapted. The former represents the capability of PTM, while the latter stands for the power of the adapted model. Both are compositional modules in . Besides, we  build SimpleCIL based on concatenated pre-trained ViT-B/16-IN21K and ViT-B/16-IN1K, denoted as SimpleCIL-21K+1K. It utilizes the aggregated features of two embedding functions, which has the same dimension as .\nAs shown in Figure\u00a0<ref>, SimpleCIL-Adapted outperforms SimpleCIL-PTM, indicating the importance of model adaptation. However, adapting the model also overwrites the high-level features, which reduces the model's generalizability. The adapted model suffers larger performance degradation than vanilla SimpleCIL, indicating the effect of generalizability in resisting forgetting. Finally, outperforms any of these sub-modules with the help of unified adaptivity and generalizability.\n\nDifferent PTMs: Observing the performance gap between ViT-B/16-IN21K and ViT-B/16-IN1K, we seek to explore different kinds of PTMs on ImageNet-R Base0 Inc20. We choose publicly available PTMs, , ResNet18/50/152\u00a0<cit.>, ViT-B/16-IN1K/21K, ViT-L/16-IN1K, ViT-B/16-DINO\u00a0<cit.>, ViT-B/16-SAM\u00a0<cit.>, ViT-B/16-MAE\u00a0<cit.>, ViT-B/16-CLIP\u00a0<cit.> (image encoder)  for a holistic evaluation, and report the results in Figure\u00a0<ref>. We can draw three main conclusions. 1) Pre-trained ViTs show better generalizability than ResNets. 2) Larger ViTs generalize better than small ones, and ViTs trained with supervised loss perform better than unsupervised ones. 3) Owing to the massive training corpus and the contrastive loss\u00a0<cit.>, CLIP performs better than ImageNet21K pre-trained ViTs. Finally, we find w/ Finetune consistently improves the performance of SimpleCIL for any PTM, thus validating its effectiveness.\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Visualization of Incremental Sessions\n\nIn this section, we visualize the learned decision boundaries with t-SNE\u00a0<cit.> on CIFAR100 dataset between two incremental stages, as shown in Figure\u00a0<ref>, <ref>. \nWe visualize the classes from the first and second incremental tasks with colorful dots and triangles. Correspondingly, the class prototypes are represented by squares. As we can infer from these figures, PTM works competitively, which well separates the instances into their corresponding classes. The class prototypes are situated at the center of each class, verifying their representativeness in recognition. When extending the model from the first to the second stage, we find performs well on both old and new classes. Visualizations verify the generalizability and adaptivity of .\n\n We also visualize the Grad-CAM\u00a0<cit.> results on OmniBenchmark dataset based on pre-trained ResNet18. Grad-CAM is utilized to highlight the critical regions in the image for predicting the corresponding concept. The results are shown in Figure\u00a0<ref> (bottom), indicating concentrates more on the task-specific features than vanilla PTMs. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nLearning with incremental classes is of great importance in real-world applications, which requires adaptivity for updating and generalizability for knowledge transfer. In this paper, we systematically revisit CIL with PTMs and draw three conclusions. Firstly, a frozen PTM can provide generalizable embeddings for CIL, enabling a prototype-based classifier to outperform the current state-of-the-art. Secondly, due to the distribution gap between pre-trained and downstream datasets, PTMs can be further harnessed to enhance their adaptivity. To this end, we propose , which can be orthogonally combined with any parameter-efficient tuning method to unify generalizability and adaptivity for CIL.\nLastly, due to data overlapping, traditional ImageNet-based benchmarks are unsuitable for evaluation in the era of PTM. Hence, we propose four new benchmarks to evaluate PTM-based CIL methods. \nExtensive experiments verify 's state-of-the-art performance. \nFuture work include exploring task-specific tuning methods and structures.\n\nLimitations: Possible limitations include the restriction of exemplars. It turns into exemplar-based CIL if sufficient instances from old classes are available, where adaptivity can be further addressed through exemplar replay.\n\n\n\n\n\n\n\nieee_fullname\n\t\n\n\n\n\n\n\n\n\n\nSupplementary Material\n\n\n\tClass-incremental learning (CIL) is of great importance to the machine learning community, and Pre-Trained Models (PTMs) have boosted its performance in recent years.\n\t In the main paper, we revisit PTM-based CIL and draw three conclusions. 1) We empirically prove that frozen PTM can  provide generalizable embeddings for CIL. To our surprise, a simple baseline\n\t (SimpleCIL) that continually sets the classifiers of PTM  to prototype features can beat state-of-the-art performance even without training on the downstream task.\n\t 2) We show that PTM can be further cultivated with adaptivity through model adaptation.\n\t We propose ADapt And Merge (), which holds the advantages of PTM\u2019s generalizability\n\t and adapted model\u2019s adaptivity. 3) previous benchmarks are unsuitable in the era of PTM due to data overlapping. Hence, we propose four new benchmarks,\n\t namely ImageNet-A, ObjectNet, OmniBenchmark,\n\t and VTAB that have a large domain gap with ImageNet pre-trained model for evaluation.\n\t \n\tIn the supplementary, we provide more details about the experimental results mentioned in the main paper, as well as additional empirical evaluations and discussions. The supplementary material is organized as follows:\n\n\n\n\t\n  * Section\u00a0<ref> provides  background information on vision transformer and parameter-efficient tuning methods. It also includes a discussion on model tuning in  ;\n\t\n  * Section\u00a0<ref> discusses the implementation details in the main paper, including the introduction of compared methods, hyper-parameters, pre-trained models, and datasets;\n\t\n  * Section\u00a0<ref> presents additional ablations of , including the influence of adapting stages, hyper-parameters and classifier types. It also contains extra experimental evaluations that could not be included in the main paper due to page limit, such as more visualizations of Grad-CAM; \n\t\n  * Section\u00a0<ref>  reports the full experimental results of 7 datasets and 45 splits, including the numerical results and performance curves. It also includes experiments with pre-trained ResNets.\n\n\n\n\n\n\u00a7 BACKGROUNDS ABOUT VIT AND PARAMETER-EFFICIENT TUNING\n \n\nIn this section, we provide background information on vision transformers and parameter-efficient tuning techniques adopted in the main paper.\n\n\n\n \u00a7.\u00a7 Vision Transformer\n\nThe concept of Vision Transformers (ViTs) is first introduced in\u00a0<cit.> to the computer vision field. In a plain ViT, an RGB image \u2208^3\u00d7 H\u00d7 W is first divided into non-overlapping patches, where (H, W) denotes the height and width of the input image. These patches are then appended with a class token  and  then fed into an embedding layer followed by the vision transformer blocks with self-attention\u00a0<cit.> as the core operation.\nFollowing the notations in the main paper, we denote the features after the embedding layer as _e\u2208^L\u00d7 d, and the first element in _e is the  token. L is the length of sequence, and d is the embedding dim. \n\nEach vision transformer block mainly consists of two modules, , a multi-head self-attention layer (MSA) and a two-layer MLP. In MSA, the tokens _e are first linearly projected into query Q\u2208^L\u00d7 d, key K\u2208^L\u00d7 d, and value V\u2208^L\u00d7 d. The self-attention is performed via:\n\n    Attention(Q, K, V)=Softmax(Q K^T/\u221a(d)) V  .\n\nThe output tokens are then sent to a LayerNorm\u00a0<cit.> and the MLP block. Denote the output of Eq.\u00a0<ref> as _\u2113, this process is formulated as:\n\n    _\u2113+ MLP(LN(_\u2113))  ,\n\nwhich aggregates the projected features and the original features for further information extraction. After the transformation of N cascaded transformer blocks, ViT takes the  token as the feature for final recognition. We refer the readers to the original work\u00a0<cit.> for more details about ViT.\n\n\n\n\n \u00a7.\u00a7 Parameter-Efficient Tuning\n\nIn this section, we introduce more details about the parameter-efficient tuning techniques adopted in the main paper, including visual prompt tuning (VPT)\u00a0<cit.>, adapters\u00a0<cit.>, scale & shift (SSF)\u00a0<cit.>, and batch normalization tuning. \nBefore addressing these methods, we revisit the target of parameter-efficient tuning, which aims to adapt the model with the least tunable parameters. Denote the tuning process as:\n\n    f^*()=\u2131(f(),^1,\u0398)  ,\n\nwhich freezes the parameters in the pre-trained model and only adjusts the parameters in \u0398. In the main paper, we optimizes the selected parameters via cross-entropy loss:\n\n    min_\u0398\u2211_(\ud835\udc31_j, y_j) \u2208\ud835\udc9f^1\u2113(f\n    \t(_j), y_j)  .\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Visual Prompt Tuning (VPT)\n\nVPT\u00a0<cit.> aims to prepend some learnable prompts \ud835\udc0f\u2208^p\u00d7 d to form the extended embedding features [\ud835\udc0f,_e], where _e is the encoded features of the input image. The extended features are then fed into the subsequent transformer blocks of ViT to calculate the final embeddings. Depending on where the prompts are inserted, VPT can be further divided into two types: VPT-Deep and VPT-Shallow.\n\nSpecifically, VPT-Shallow only learns the prompts in the first transformer block. Denote the function of k-th transformer block as L_k, the operations of VPT-Shallow can be denoted as:\n\n    [\ud835\udc19_1, \ud835\udc04_1]     =L_1([ \ud835\udc0f, _e]) \n    [ \ud835\udc19_i, \ud835\udc04_i]     =L_i([ \ud835\udc19_i-1, \ud835\udc04_i-1])    i=2,3, \u2026, N  ,\n\nwhere \ud835\udc19_i\u2208^p\u00d7 d denotes the encoded feature of prompts, N is the number of transformer blocks. Correspondingly, VPT-Deep learns the prompts in each transformer block:\n\n    [, \ud835\udc04_i]=L_i([\ud835\udc0f_i-1, \ud835\udc04_i-1])    i=1,2, \u2026, N  ,\n\nwhere \ud835\udc04_0=_e is the encoded feature of image patches. During optimization, VPT freezes the pre-trained weights in the embedding layer and only optimizes these learnable prompts and classification head[Note that the classification head W is only optimized in the adaptation process. Since we use prototype-based classifier for classification after model merge, W will be dropped after adaptation. ], , \u0398=\u03b8_\ud835\udc0f\u222a\u03b8_W.\n \n Specifically, the number of tunable parameters in VPT-Shallow is p\u00d7 d, and that of VPT-Shallow is p\u00d7 d \u00d7 N, where N is the number of transformer blocks. For example, we set the prompt length to 5 for the ViT-B/16 model. The tunable parameters are 5\u00d7 786=0.004 million in VPT-Shallow and 5\u00d7 786\u00d7 12=0.046 million for VPT-Deep, which is negligible compared to the ViT-B/16 with 86 million parameters.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Adapter\n\n\nAdapter\u00a0<cit.> is a bottleneck module that allows adjusting the output of ViT. Formally, it comprises a down-projection W_down\u2208^d\u00d7 r to reduce the feature dimension, a non-linear activation function, and an up-projection W_up\u2208^r\u00d7 d to project back to the original dimension. Following the implementation of AdaptFormer\u00a0<cit.>, we replace the original MLP structure in ViT with the AdaptMLP structure. Specifically, denote the input of the MLP layer as _\u2113, the output of AdaptMLP is formatted as:\n\n    MLP(_\u2113)+s \u00b7ReLU(_\u2113 W_down)W_up ,\n\nwhich is a residual structure. s is an optional learnable parameter for re-scaling the output. During adaptation, the model freezes the pre-trained weights and only optimizes the extra parameters, , \u0398=\u03b8_W_down\u222a\u03b8_W_up\u222a\u03b8_W.\nSpecifically, we set the hidden dim r to 16, and the number of tunable parameters in the Adapter is approximately 0.3 million, which is negligible compared to the ViT-B/16 with 86 million parameters.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Scale & Shift\n\nScale & Shift (SSF)\u00a0<cit.> aims to adjust the feature activation by scaling and shifting operations. SSF only appends an extra SSF layer after each operation layer (, MSA and MLP), and adjusts the output of these operations. Given the input _i\u2208^L\u00d7 d, the output _o\u2208^L\u00d7 d follows:\n\n    _o=\u03b3\u2297_i+\u03b2 ,\n\nwhere \u03b3\u2208^d and \u03b2\u2208^d are the scale and shift factors, respectively. \u2297 is Hadamard\nproduct (element-wise multiplication). The model optimizes the SSF layers and classification head, , \u0398=\u03b8_SSF\u222a\u03b8_W, to trace the features of new tasks. \nIn the implementation, we add the SSF layer after the MSA and MLP operations, and the number of tunable parameters in SSF is around 0.2 million, which is negligible compared to the ViT-B/16 with 86 million parameters.\n\nNote that the SSF layer can also be deployed for the pre-trained residual networks. We append SSF layers after the residual blocks to re-scale the output for better adaptation.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Batch Normalization Tuning\n\n If the PTM uses ResNet structures, we can also adjust the BN\u00a0<cit.> parameters, including the running mean and running variance. Specifically, BN is designed to normalize each feature dimension and overcome internal covariate shift. It records the running mean and variance in the training stage: \n \n    \u03bc_\u212c    = 1/m\u2211_i=1^m x_i \n    \u03c3_\u212c^2     = 1/m\u2211_i=1^m(x_i-\u03bc_\u212c)^2  ,\n\n where x_i denotes the feature of the i-th instance and m is the batch size. \u03bc_\u212c and \u03c3_\u212c^2 are then utilized to normalize the instances in the testing process. However, since the running mean and variance in PTM are compatible with the upstream data distribution, they could be unstable for the downstream tasks, leading to abnormal outputs\u00a0<cit.>.\n \n To handle the incompatible BN statistics in pre-trained and incremental datasets, we first zero the running statistics in BN and then forward pass the data in ^1 to record the  current data distribution. \n In other words, BN can be updated by feeding the model with new class instances, , f().\n These statistical parameters are then fit for the current data to alleviate the domain gap. Notably, BN tuning does not require backpropagation, making it an efficient solution.\n\n\n\n\n\n \u00a7.\u00a7 Summary of  PTM Tuning\n\nParameter-efficient tuning enables the model adaptation with least number of tunable parameters, which guarantees the model's adaptivity. Specifically, we also observe that fully finetuning could fail in cases where training data is rare. In such circumstances, parameter-efficient tuning could be a better solution for model adaptation. Accordingly, we notice w/ Finetune performs worse than SimpleCIL on CUB B0 Inc10, ObjectNet B0 Inc10, and OmniBenchmark B0 Inc30, while with other tuning methods perform better.\nOn the other hand, since parameter-efficient tuning freezes the pre-trained weights, the generalizability of the model is also maintained.\n It must be noted that the PTM in the model merge is the same as the PTM before the model adaptation.\n\n\nIn summary, is a general framework for class-incremental learning, which can be applied with different types of backbones and tuning techniques. Specifically, we can use VPT-Deep, VPT-Shallow, Scale & Shift, Adapter for ViT, and Scale & Shift and BN tuning for CNN. In the implementation, we only choose one specific tuning structure among these techniques, and do not consider the combination of multiple the tuning methods.\n\n\n\n\u00a7 IMPLEMENTATION DETAILS\n \nIn this section, we discuss the detailed implementation in , including the introduction of compared methods, hyper-parameters, selection of pre-trained models, and discussion about datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Compared Methods\n\n\nWe first introduce the compared methods in the main paper. These methods are as follows:\n\n\n\t\n  * Finetune: directly trains the model with new datasets incrementally, which leads to catastrophic forgetting;\n\t\n  * Finetune Adapter\u00a0<cit.>: freezes the pre-trained weights and sequentially optimizes the adapter module. To improve its performance, only the specific classifiers in the current dataset (, _i, i\u2208Y_b), are tuned, and the classifiers for former classes (, _i, i\u2208\ud835\udcb4_b-1) are frozen;\n\t\n  * LwF\u00a0<cit.>: utilizes knowledge distillation\u00a0<cit.> as regularization term to overcome forgetting, which relies on the supervision of old model to produce soft targets; \n\t\n  * DER\u00a0<cit.>: state-of-the-art class-incremental learning method, which creates a new backbone for each new incremental task. All these features are concatenated together to learn the unified classifier;\n\t\n  * FOSTER\u00a0<cit.>: state-of-the-art class-incremental learning method, which extends DER with a model compression stage to control the memory budget;\n\t\n  * MEMO\u00a0<cit.>: state-of-the-art class-incremental learning method. It creates new residual layers instead of the whole backbone to reduce memory cost. In the implementation, we change the residual layers into transformer blocks to keep consistent with the ViT structure;\n\t\n  * FACT\u00a0<cit.>: state-of-the-art class-incremental learning method, which also builds a prototype-based classifier. It reserves the embedding space for new classes in the incremental learning process;\n\t\n  * L2P\u00a0<cit.>: state-of-the-art PTM-based CIL method. It freezes the pre-trained model and optimizes a prompt pool to fit new patterns. To determine which prompt to use, it designs a `key-value' pair for prompt matching and utilizes an extra pre-trained model to provide the embeddings for prompt retrieval;\n\t\n  * DualPrompt\u00a0<cit.>:state-of-the-art PTM-based class-incremental learning method. It extends L2P with two kinds of prompts, namely general and expert prompts. Similar to L2P, it also relies on another pre-trained model for prompt retrieval.\n\n\nNote that all methods are based on the pre-trained ViT in the main paper. For methods requiring backbone expansion (, DER, MEMO, and FOSTER), we also use pre-trained ViT as the initialization of new backbones.\n\nDiscussion about total parameters: Since L2P and DualPrompt have prompt pools, they rely on another pre-trained ViT as the `retriever' to search for the instance-specific prompt. Hence, shares the same scale of total parameters as these methods. We list the total number of parameters of these methods in Figure\u00a0<ref>, which indicates that obtains better performance than the compared methods with the same scale or fewer parameters. \n\n\n\n\n\n \u00a7.\u00a7 Implementations and Hyper-Parameters\n\n\nFor compared methods, we adopt the PyTorch implementation[The original implementation <https://github.com/google-research/l2p> is based on JAX.] of L2P[<https://github.com/JH-LEE-KR/l2p-pytorch>]\u00a0<cit.> and DualPrompt[<https://github.com/JH-LEE-KR/dualprompt-pytorch>]\u00a0<cit.>. We follow the implementations in PyCIL[<https://github.com/G-U-N/PyCIL>]\u00a0<cit.> to re-implement other compared methods with ViT, , Finetune, Finetune Adapter, LwF, DER, FOSTER, MEMO, and FACT. \n\nSpecifically, for , the number of the prompt length p in VPT is set to 5, and the projected dimension of the Adapter r is set to 16. There are no other hyper-parameters for to set, which ensures the robustness of our proposed method. We conduct experiments to investigate the influence of these parameters in Section\u00a0<ref>. Following\u00a0<cit.>, we use the same data augmentation for all methods, , random resized crop and horizontal flip. Input images are resized to 224\u00d7224 before feeding into the model. Following\u00a0<cit.>, all classes are randomly shuffled with Numpy random seed 1993 before splitting into incremental tasks. A specific case is VTAB, where we force the classes to emerge from domain to domain, as discussed in Section\u00a0<ref> and <ref>.\n\n\n\n\n\n \u00a7.\u00a7 Pre-Trained Models\n\n\nSince there are various kinds of pre-trained models publicly available, we follow\u00a0<cit.> to choose the most commonly used ones in the main paper for evaluation, denoted as ViT-B/16-IN1K and ViT-B/16-IN21K. Specifically, both models are pre-trained on ImageNet21K\u00a0<cit.>, while ViT-B/16-IN1K is further finetuned with ImageNet1K. We follow timm\u00a0<cit.> implementation and report the details about these models in Table\u00a0<ref>.\nFor ResNet, we utilize the Pytorch\u00a0<cit.> pre-trained models. \nAdditionally, we report the backbones adopted in the ablation study, including ViT-L/16-IN1K, ViT-B/16-DINO\u00a0<cit.>, ViT-B/16-SAM\u00a0<cit.>, ViT-B/16-MAE\u00a0<cit.>, ViT-B/16-CLIP\u00a0<cit.> (image encoder), in the table. Among them, ViT-B/16-DINO and ViT-B/16-MAE are trained with self-supervised loss, and ViT-B/16-CLIP is trained on 400 million image-text pairs with contrastive loss.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Datasets\n \nIn this section, we provide an introduction to the datasets used in the main paper. We list the details of seven adopted datasets in Table\u00a0<ref>. Specifically, CIFAR100, CUB200, and ImageNet-R are benchmark CIL datasets widely adopted in\u00a0<cit.>. However, due to the data overlap between ImageNet-based benchmarks and the pre-trained dataset, ImageNet is unsuitable for evaluating PTM-based CIL methods\u00a0<cit.>. As a result, we introduce four new benchmarks for CIL that 1) do not overlap with the ImageNet dataset, 2) have a large domain gap with ImageNet, increasing the burden of PTM to generalize, and 3) contain large-scale datasets from multiple realms that can form the cross-domain class-incremental benchmark.\nWe list the detailed information below.\n\n\n\n\n\t\n  * CIFAR100\u00a0<cit.> contains 100 classes with 60,000 images, of\n\twhich 50,000 are training instances and 10,000 are testing ones, with 100 images per class. \n\t\n  * CUB200\u00a0<cit.> is a widely-used dataset for fine-grained visual categorization task. It contains 11,788 images of 200 subcategories belonging to birds, with 9,430 for training and 2,358 for testing.\n\t\n  * ImageNet-R\u00a0<cit.> is introduced into CIL by\u00a0<cit.>. It contains newly collected data of different styles, such as cartoon, graffiti, and origami, as well as hard examples\n\tfrom ImageNet that standard ImageNet pre-trained models fail to classify. Following\u00a0<cit.>, there are 24,000 training instances and 6,000 testing instances from 200 classes.\n\t\n\t\n  * ImageNet-A\u00a0<cit.> ImageNet-A is a dataset of real-world adversarially filtered images that fool current ImageNet pre-trained classifiers. It was exported from sites including iNaturalist,\n\tFlickr, and DuckDuckGo, and adversarially selected by removing examples that fail to fool  ResNet50. We select 5,981 training instances and 1,519 testing instances from 200 classes.\n\t\n  * ObjectNet\u00a0<cit.> is a large, real-world dataset for object recognition with controlled variations in object backgrounds, rotations, and imaging viewpoints, making finetuning a challenge due to only small performance increases.  \n\tWhen tested on ObjectNet, object detectors experience a 40\u223c45% drop in performance compared to their performance on other benchmarks. The original ObjectNet contains classes from 313 classes, and we select a subset of 200 classes for class-incremental learning. Among them, 26,509 instances are for training, and 6,628 are for testing.\n\t\n  * OmniBenchmark\u00a0<cit.> is a concise and diverse benchmark for evaluating pre-trained model generalization across semantic super-concepts/realms. It contains 21 semantic realm-wise datasets that have no overlapping concepts. The original OmniBenchmark is constituted of 7,372 classes, from which we sample 300 categories to construct the class-incremental learning dataset. The subset contains 89,697 training instances and 5,985 testing instances. As these selected classes are from multiple realms, it is harder to conduct incremental learning with OmniBenchmark than other datasets due to the domain gap among different classes.\n\n\n\t\n  * VTAB\u00a0<cit.>  includes 19 evaluation tasks spanning a variety of domains that can be grouped into three categories \u2014 natural, specialized, and structured. \n\tThe Natural group includes images of the natural world captured through standard cameras, representing generic objects, fine-grained classes, or abstract concepts. The Specialized group utilizes images captured using specialist equipment, such as medical images or remote sensing. The Structured group derives from artificial environments that target understanding of specific changes between images, such as predicting the distance to an object in a 3D scene, counting objects, or detecting orientation. Since the original VTAB contains 19 datasets, we select 5 to construct a cross-domain class-incremental learning setting, , Resisc45\u00a0<cit.>, DTD\u00a0<cit.>, Pets\u00a0<cit.>, EuroSAT\u00a0<cit.>, and Flowers\u00a0<cit.>. In VTAB, we do not shuffle the classes and make the classes emerge from domain to domain, which is a more realistic incremental learning setting.\n\n\n\n\n\nWe give the visualizations of our newly introduced datasets in Figure\u00a0<ref>. As shown in the figure, ImageNet-A and ObjectNet contain hard samples that could be misclassified  by the ImageNet pre-trained model. In contrast, OmniBenchmark and VTAB contain cross-domain instances, which increase the difficulty of incremental learning. Since the distribution gap from domain to domain has not been explored in former CIL tasks, exploring the cross-task incremental learning problem with OmniBenchmark and VTAB is very challenging. \n\n\n\n\n\n\n\u00a7 EXTRA EXPERIMENTAL EVALUATIONS\n \nIn this section, we conduct experiments to investigate the variations of . Specifically, we compared the results of model adaptation with different incremental stages to determine the best solution in model adaptation. We also explore the influence of hyper-parameters in model adaptation, , the prompt length in VPT and the projection dimension in Adapter. Additionally, we provide additional Grad-CAM results for better visualization.\n\n\n\n\n\n \u00a7.\u00a7 Influence of Adapting Stages\n\nAs discussed in the main paper, we only adapt the pre-trained model in the first incremental stage with ^1 in . There are two reasons: 1) Sequentially tuning the model will suffer catastrophic forgetting. 2) Since we utilize a prototype-based classifier, tuning the model with multiple stages will result in incompatible features between former and new prototypes.\n\nIn this section, we conduct an ablation to determine the influence of adapting stages and report the results in Figure\u00a0<ref>. We conduct the experiment on CIFAR100 Base0 Inc10  setting with pre-trained ViT-B/16-IN21K. There are 10 incremental stages in total. Wedenote the tuning stages as T, and train w/ Adapter for ablation. Specifically, we change the tuning stages among {0,1,2,\u22ef,10} to determine the influence on the final performance. \n In the first T stages, we adapt the PTM incrementally with adapter and replace the classifier with prototypes. Afterward the T-th stage, we freeze the encoding functions and only extract prototypes for the following stages. T=0 denotes vanilla SimpleCIL. To prevent forgetting, we freeze the classifier weight of former classes when learning new classes. \n\nAs shown in Figure\u00a0<ref>, tuning the model with the first stage achieves the best performance among all settings. Specifically, multi-stage tuning harms generalizability and results in the incompatible features of former and new classes. We also plot the trend of average/last accuracy with the change of tuning stages in Figure\u00a0<ref>, where T=1 achieves the best performance in both measures. \n\n\n\n\n \u00a7.\u00a7 Influence of Hyper-Parameters\n\n\nIn this section, we explore the influence of hyper-parameters in . Specifically, since is optimized with parameter-efficient tuning, the only hyper-parameters come from these tuning methods, , the prompt length p in VPT and the projection dim r in Adapter.\nWe train the model on CIFAR100 B50 Inc5 with pre-trained ViT-B/16-IN1K in this section.\n\nFirstly, we investigate the influence of the prompt length in Figure\u00a0<ref>. The figure shows that 's performance is robust with the change of the prompt length. Therefore, we use p=5 as a default parameter for all settings. Similarly, we observe similar phenomena in Figure\u00a0<ref>, where the performance is robust with the change of the projection dimension. As a result, we set r=16 as a default parameter for all settings.\n\n\n\n\n\n\n \u00a7.\u00a7 More Grad-CAM Results\n\n\nIn this section, we provide more Grad-CAM results of compared to PTM. As shown in Figure\u00a0<ref>,  concentrates\nmore on task-specific features than vanilla PTM, confirming the effectiveness of model adaptation in capturing task-specific features.\n\n\n\n\n\n \u00a7.\u00a7 Influence of Cosine Classifier\n\nIn this section, we verify the necessity of using a cosine classifier in . Specifically, we report the performance of SimpleCIL and and their variations without the cosine classifier in Table\u00a0<ref>. As we can infer from the table, replacing the cosine classifier with an unnormalizedd classifier will lead to a performance gap of approximately 5%. Therefore, the cosine classifier is essential in building PTM-based models with prototypical classifiers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 FULL EXPERIMENTAL RESULTS ON 7 DATASETS\n \n\nIn this section, we report the full experimental performance on seven benchmark datasets with the numerical results and accuracy curves. We also report the results of residual networks with . Specifically, there are 45 dataset splits from 7 datasets, and we report the performance of two backbones (ViT-B/16-IN1K and ViT-B/16-IN21K) on them.\n\n\n\n\n \u00a7.\u00a7 CIFAR100 Results\n\n\nFor CIFAR100, we design 6 dataset splits to divide these 100 classes, namely Base0 Inc5, Base0 Inc10, Base0 Inc20, Base 50 Inc5, Base50 Inc10, Base50 Inc25.\nWe report the results in Table\u00a0<ref>, <ref>, <ref>, <ref>, <ref>, <ref>, and plot the corresponding incremental performance in Figure\u00a0<ref>.\n\n\n\n\n\n\n\n \u00a7.\u00a7 CUB200 Results\n\n\nFor CIFAR100, we design 8 dataset splits to divide these 200 classes, namely Base0 Inc5, Base0 Inc10, Base0 Inc20, Base0 Inc40, Base 100 Inc5, Base100 Inc10, Base100 Inc20, Base100 Inc50.\nWe report the results in Table\u00a0<ref>, <ref>, <ref>, <ref>, <ref>, <ref>, <ref>, <ref> and plot the corresponding incremental performance in Figure\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 ImageNet-R Results\n\n\n\nFor ImageNet-R, we design 8 dataset splits to divide these 200 classes, namely Base0 Inc5, Base0 Inc10, Base0 Inc20, Base0 Inc40, Base 100 Inc5, Base100 Inc10, Base100 Inc20, Base100 Inc50.\nWe report the results in Table\u00a0<ref>, <ref>, <ref>, <ref>, <ref>, <ref>, <ref>, <ref> and plot the corresponding incremental performance in Figure\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 ImageNet-A Results\n\n\nFor ImageNet-A, we design 8 dataset splits to divide these 200 classes, namely Base0 Inc5, Base0 Inc10, Base0 Inc20, Base0 Inc40, Base 100 Inc5, Base100 Inc10, Base100 Inc20, Base100 Inc50.\nWe report the results in Table\u00a0<ref>, <ref>, <ref>, <ref>, <ref>, <ref>, <ref>, <ref> and plot the corresponding incremental performance in Figure\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 ObjectNet Results\n\n\nFor ObjectNet, we design 8 dataset splits to divide these 200 classes, namely Base0 Inc5, Base0 Inc10, Base0 Inc20, Base0 Inc40, Base 100 Inc5, Base100 Inc10, Base100 Inc20, Base100 Inc50.\nWe report the results in Table\u00a0<ref>, <ref>, <ref>, <ref>, <ref>, <ref>, <ref>, <ref> and plot the corresponding incremental performance in Figure\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 OmniBenchmark Results\n\n\nFor OmniBenchmark, we design 6 dataset splits to divide these 300 classes, namely Base0 Inc15, Base0 Inc30, Base0 Inc50, Base150 Inc15, Base 150 Inc30, Base150 Inc50.\nWe report the results in Table\u00a0<ref>, <ref>, <ref>, <ref>, <ref>, <ref>, and plot the corresponding incremental performance in Figure\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 VTAB Results\n \nSince VTAB is a complex dataset with multiple domains, we select five domains to construct a cross-domain class-incremental learning setting. Specifically, we fix the domain order to \u201cResisc45 \u2192 DTD \u2192 Pets \u2192 EuroSAT \u2192 Flowers\u201d. Each domain contains 10 classes, and we formulate the VTAB Base0 Inc10 setting.\nWe report the results in Table\u00a0<ref> and plot the corresponding incremental performance in Figure\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 with ResNet\n\n\nIn this section, we give the visualizations of to boost the performance of pre-trained ResNet. Specifically, we choose one split from each dataset and choose pre-trained ResNet18/50/101/152 as the backbone to evaluate the performance of .\nWe report the numerical results in Table\u00a0<ref>, <ref>, <ref>,  <ref>,  <ref>, <ref> and plot the incremental performance in Figure\u00a0<ref>.\nResults indicate also works competitively with ResNets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}