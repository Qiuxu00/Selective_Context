{"entry_id": "http://arxiv.org/abs/2303.08011v1", "published": "20230313030317", "title": "Large statistical learning models effectively forecast diverse chaotic systems", "authors": ["William Gilpin"], "primary_category": "cs.LG", "categories": ["cs.LG", "physics.comp-ph"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n wgilpin@utexas.edu\n\nDepartment of Physics, The University of Texas at Austin, Austin, Texas 78712, USA\n\n\nOden Institute for Computational Engineering and Sciences, The University of Texas at Austin, Austin, Texas 78712, USA\n\n\n\n\n\n             \n\n\nChaos and unpredictability are traditionally synonymous, yet recent advances in statistical forecasting suggest that large machine learning models can derive unexpected insight from extended observation of complex systems. Here, we study the forecasting of chaos at scale, by performing a large-scale comparison of 24 representative state-of-the-art multivariate forecasting methods on a crowdsourced database of 135 distinct low-dimensional chaotic systems. We find that large, domain-agnostic time series forecasting methods based on artificial neural networks consistently exhibit strong forecasting performance, in some cases producing accurate predictions lasting for dozens of Lyapunov times. Best-in-class results for forecasting chaos are achieved by recently-introduced hierarchical neural basis function models, though even generic transformers and recurrent neural networks perform strongly. However, physics-inspired hybrid methods like neural ordinary equations and reservoir computers contain inductive biases conferring greater data efficiency and lower training times in data-limited settings. We observe consistent correlation across all methods despite their widely-varying architectures, as well as universal structure in how predictions decay over long time intervals. Our results suggest that a key advantage of modern forecasting methods stems not from their architectural details, but rather from their capacity to learn the large-scale structure of chaotic attractors.\n\n\n\n\n\nLarge statistical learning models effectively forecast diverse chaotic systems\n    William Gilpin\n    March 30, 2023\n==============================================================================\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nChaos traditionally implies the butterfly effect: that a small change to a system's inputs grows exponentially over time, complicating efforts to reliably predict the system's long-term evolution. Yet recent efforts to generate long-term forecasts of real-world complex systems using statistical learning undermine this intuition, providing compelling examples of successful data-driven predictions of diverse systems such as cellular signaling pathways <cit.>, hourly precipitation forecasts <cit.>, and tokamak plasma disruptions <cit.>. These successes complement recent works showing that large, overparameterized statistical learning methods can learn latent representations of complex systems that implicitly linearize dynamics, raising the possibility of forecasting even strongly-chaotic systems using models with sufficient capacity <cit.>. Such results potentially elucidate the emergence of reservoir computers as best-in-class forecasting methods for dynamical systems <cit.>; these models use emergent properties of random networks to lift complex time series into a random feature space, substantially reducing training requirements by reducing learning to linear regression <cit.>. However, other recently-introduced hybrid models successfully forecast chaotic systems by encoding dynamical constraints within model formulation <cit.>; among these are neural ordinary differential equations <cit.>, physics-informed neural networks <cit.>, and recurrent neural networks with domain-specific architectures <cit.>. \n\n\n\n\nHowever, there is little consensus regarding the best choice of forecasting model for a given dynamical system, and whether dynamical constraints should be encoded, or learned from data via large domain-agnostic models\u2014the former provides an inductive bias that economizes training, while the latter allows greater flexibility and reduces model selection bias. Complicating comparisons among methods are the widely-varying contexts in which prior methods have been evaluated, which vary from domain-specific weather or medical time series to individual well-known chaotic systems like the Lorenz attractor <cit.>. A larger set of representative systems, and a controlled comparison among methods, is necessary to disentangle the relationship between chaoticity, predictability, and forecasting model architecture, as well as to understand how the properties of different black-box machine learning methods interact with the systems they predict.\n\n\n\nHere, we seek to systematically quantify the relationship between chaos and empirical predictability in large-scale, controlled experiments. We recently introduced a benchmark dataset containing 135 low-dimensional differential equations describing known chaotic attractors <cit.>. Initially curated from published works to include well-known systems such as the Lorenz, R\u00f6ssler, and Chua attractors, the dataset has grown through crowdsourcing to include examples spanning diverse domains such as climatology, neuroscience, and astrophysics. Each dynamical system is aligned with respect to its dominant timescale and integration timestep using surrogate significance testing <cit.>, and is annotated with calculations of its invariant properties such as the Lyapunov exponent spectrum, fractal dimension, and metric entropy.\n\n\nWhile each system has a different largest Lyapunov exponent (\u03bb_max), and thus putative chaoticity, some systems are more closely related than others. For example, the Sprott attractor subfamily (\u03bb_max\u2208 [0.01, 1.1]) exhibit similar qualitative structure such as paired lobes, owing to the presence of predominantly quadratic nonlinearities in the governing differential equations <cit.>. To identify such relationships across our dataset, we convert each dynamical system into a high-dimensional vector by first generating a long trajectory, and then computing 747 characteristic mathematical signal properties such as the metric entropy, power spectral coefficients, Hurst exponents, et al. that are invariant to the initial conditions and sampling rate <cit.>. We then use uniform manifold approximation and projection (UMAP) to visualize these high-dimensional vectors within a two-dimensional plane (Fig. <ref>) <cit.>. The resulting space of chaotic systems shows clear structure, with the Sprott and other scroll-like subfamilies clustering together, while other systems separate. These results suggest that our dataset contains high dynamical diversity beyond absolute chaoticity \u03bb_max, which correlates only weakly with the embedding (\u03c1 = 0.15 \u00b1 0.03, bootstrapped Spearman rank-order coefficient).\n\n\n\n\n\n\n\nOur dataset allows us systematically compare the forecasting ability of different statistical forecasting methods across diverse dynamical systems. Forecasting dynamical systems from observations is a well-established field <cit.>, and we structure our experiments as a standard long-term autoregressive forecasting task <cit.>. For each dynamical system, we define two time series Y_train, Y_test\u2208\u211d^T \u00d7 D corresponding to multivariate trajectories emanating from different initial conditions, and we define a splitting time such that T = T_past + T_fut, Y_train = Y_train^past\u222a Y_train^fut, etc. The model parameters are first fit using Y_train^past, and the accuracy of the resulting predictions on Y_train^fut are used for model selection and hyperparameter tuning. After completing model selection, the final model from each model class is then fit on Y_test^past, and the resulting predictions on Y_test^fut produce the error score \u03f5_ik(t) representing the performance of the i^th forecasting model on the k^th dynamical system at future time t \u2208 [0, T_fut] after the end of training data availability. We compute 17 different accuracy metrics, including root mean-squared error, pointwise correlation, mutual information, and Granger causality. We report our results in the main text in terms of the symmetric mean absolute percent error (sMAPE) \u03f5(t) \u2261 (2/N) \u2211_t'=1^t (| y\u030c(t') - \u0177\u030c\u0302(t')|) / (|y\u030c(t')| + |\u0177\u030c\u0302(t')|) due to its common use, conceptual simplicity, and correlation with other metrics <cit.>.\n\nEach forecasting method represents a class of possible models parametrized by choices made regarding architecture (model size, number of layers, activation function) or training (optimization epochs, batch sizes). Such choices can strongly affect the performance of different methods <cit.>, yet different methods do not necessarily have equivalent adjustable hyperparameters. In order to fairly compare different methods, we restrict hyperparameter tuning to the equivalent of the input time window for each model\u2014such as the input size for deep neural networks, the order of autoregressive models, or the number of time lags for state space models. We tune hyperparameters separately on each system and forecasting model pair. Our overall experiment design is characterized by several timescales: the lookback window hyperparameter corresponds to the number of past timepoints seen simultaneously by a model at a given time during training or prediction; the history length represents the total number of timepoints available to learn the model's parameters before prediction; the forecast horizon represents the number of unseen timepoints into the future that are predicted autoregressively; and the Lyapunov time \u03bb_max^-1 is an invariant property of each distinct dynamical system, representing the characteristic timescale over which forecasts are expected to lose accuracy due to the butterfly effect.\n\n\nWe evaluate 24 statistical forecasting models across all 135 dynamical systems. We choose forecasting methods representative of the broad diversity of methods available in the recent literature <cit.>. Traditional methods include standard linear regression, autoregressive moving averages (ARIMA), exponential smoothing, Kalman filtering, Fourier mode extrapolation, boosted random forest models <cit.>, and newly-introduced linear models that account for trends and distribution shift <cit.>. Current state-of-the-art models for general time series forecasting are based on deep neural networks: the transformer model <cit.>, long-short-term-memory networks (LSTM), block recurrent neural networks (RNN), temporal convolutional neural networks <cit.>, and neural basis expansion/neural hierarchical interpolation (NBEATS/NHiTS) <cit.>. The latter methods generates forecasts hierarchically by aggregating separate forecasts at distinct timescales (NBEATS), and can explicitly coarse-grain the time series to further reduce computational costs (NHiTS). We also consider hybrid physics-motivated methods such as neural ordinary differential equations <cit.>, which approximate the continuous-time differential equation underlying time series, as well as echo-state networks and their generalization, nonlinear vector autoregressive models (nVAR), which use a trainable linear recurrent structure with fixed \"reservoir\" of random nonlinearities <cit.>.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur main forecasting results are summarized in Figure <ref>. Our long-term forecasting experiments require \u223c10^18 floating point operations for training, model selection and hyperparameter tuning, a figure comparable to the scale of other recent large-scale machine learning benchmarks <cit.>. For brevity, we report here only results for the top 14 forecasting models in terms of the sMAPE error, and defer the full tabular results and accuracy metrics to the supplementary material and our open-source repository. \n\nWe observe that large, domain-agnostic forecasting methods successfully forecast diverse chaotic systems, with the strongest methods succeeding consistently across diverse systems and forecasting horizons. The strong relative performance of machine learning models is highlighted in Fig <ref>C, where NBEATS successfully forecasts the Mackey-Glass equation for \u223c 22 Lyapunov times without losing track of the global phase. This and other strongly-performing methods, such as the transformer and LSTM, comprise large models originally designed for generic sequential datasets, and which do not make any assumptions regarding whether observed time series data arises from a dynamical system. This observation suggests that more flexible, generic architectures may prove preferable for problems where some physical structure is present (e.g. analytic generating functions in the form of ordinary differential equations) but stronger domain knowledge (e.g. symmetries or symplecity) is unavailable to further constrain learning.\n\n\nThe strong performance of NBEATS and NHiTS suggest that these models may have structural features favoring the chaotic systems dataset. Because these methods compute forecasts hierarchically, they can flexibly integrate information across multiple timescales in a manner inaccessible to classical statistical models <cit.>. While chaotic systems exhibit continuous spectra and thus contain information relevant to forecasting at a variety of timescales, many systems exhibit topologically-preferred timescales such as unstable periodic orbits\u2014like the \"loops\" on either side of the Lorenz attractor\u2014that dominate the system's underlying measure <cit.>, and which therefore may represent higher priority motifs for learning. We thus argue that performant model architectures contain implicit inductive biases that advantage them on chaotic systems relative to other time series. Consistent with this finding, we note that the next strongest-performing model, nVAR, comprises a reservoir computing architecture <cit.>, which prior works have shown to perform particularly well on time series from dynamical systems <cit.>. Interestingly, the neural ordinary differential equation model performs poorly, despite putatively learning the underlying generator of the time series. We find that over long forecast horizons, neural ODE tend to settle into prolonged oscillations, favoring complex non-chaotic limit cycles over chaotic attractors given finite training data.\n\nWhile the utility of deep learning methods for forecasting general time series has been questioned <cit.>, our results agree with recent benchmarks suggesting that large models strongly outperform classical forecasting methods on long-horizon forecasting tasks <cit.>. We find that classical methods like exponential smoothing or ARIMA do not appear among the top 14 models, implying that the size and diversity of our chaotic systems dataset, as well as the long duration of the forecasting task, require larger models with greater intrinsic capacity to represent complex nonlinear systems. Relative performance among models remains stable across two orders of magnitude in Lyapunov time, indicating that strong models better approximate the underlying propagator for the flow even at small forecasting horizons. Given the autoregressive nature of forecasting, an initial accuracy advantage compounds over time due to the exponential sensitivity of chaotic systems to early errors. \n\n\n\n\n\n\n\n\nWhile the best forecasts are generated by domain-agnostic time series methods, we note that the different forecasting methods have different intrinsic model complexities and thus capacities. Fig. <ref>A shows the forecasting error at \u03bb_max^-1 versus the computational walltime required to train each model on one central processing unit. Training walltime measures model efficiency, and we interpret it as a loose proxy for model complexity because model size and parameter count are not directly quantifiable across highly-distinct and regularized architectures <cit.>. We find that error and training time exhibit negative correlation (\u03c1 = -0.21 \u00b1 0.04, bootstrapped Spearman coefficient), which persists within most method groups.  The best-performing machine learning models require considerable training times; in contrast, nVAR exhibits competitive performance with two orders of magnitude less training time due to its linear structure. The strong performance of reservoir computers on our chaotic systems task may indicate an inductive bias for learning complex dynamical systems, due to their disordered reservoir allowing them to more readily represent systems with continuous spectra <cit.>. \n\n\n\n\n\n\n\nThis general tradeoff between performance and training time motivates us to search for universal similarities across different forecasting methods. In Fig. <ref>B we compute the time-resolved Spearman correlation between each method's forecast error and the average error of that method, \u03c1\u0303_i(t) = \u03c1(\u03f5_ik(t), \u27e8\u03f5_ik\u27e9_t))_k. We find universal non-monotonic behavior, in which nearly all methods exhibit peak correlation at one Lyapunov time \u03bb_max^-1 (Fig <ref>B). This observation underscores that the largest Lyapunov exponent represents an appropriate timescale for comparing different dynamical systems, and that diverse forecasting models interact with this property in a shared manner. Like other disordered complex systems, the trained models exhibit peak correlation at peak variance <cit.>, coinciding with when average forecast errors rapidly increase in Fig <ref>B. The critical forecast horizon \u03bb_max^-1 is sufficiently long to distinguish different dynamical systems based on their invariant properties, but short enough that forecast methods do not accrue instabilities, large phase offsets, and other artifacts that saturate forecast error and mask intrinsic differences among systems.\n\nTo further investigate model complexity and performance, we next perform a series of experiments in which we titrate history length, which determines the the total amount of training data available to each method before generating a forecast (Fig. <ref>C). Unlike measuring training time or parameter count, these experiments seek to determine how effectively different models utilize additional observations. As expected, all models asymptotically improve given more training data. However, the three best-performing models improve more quickly and reach lower asymptotes as the available history increases, suggesting that they better leverage additional data than less performant models. These results match the intuition that these models combine high intrinsic capacity with inductive biases for chaotic dynamical systems in order to outperform other methods on our dataset.\n\n\n\n\n\n\n\n\nOur results show that recently-developed large, overparameterized statistical forecasting models efficiently leverage long-term observations of chaotic attractors, producing best-in-class forecasts that can remain accurate for dozens of Lyapunov times. Commonalities in predictions across highly distinct model classes suggest that performance arises primarily from model capacity and generalization ability, rather than specific architectural choices, and that performance at long prediction times is ultimately limited by a model's ability to learn long-term properties of a dynamical system's underlying attractor.  Our observations echo recent findings from other domains and represent an intuitive consequence of the \"no free lunch\" theorem for model selection <cit.>. Nonetheless, our results are practically informative for forecasting real-world time series driven by underlying dynamical systems. In the absence of restrictions on data availability or training resources, large domain-agnostic models are likely to produce high-quality forecasts without the need for system-specific knowledge. However, in restricted settings, models encoding strong domain knowledge, particularly recent reservoir computing architectures <cit.>, exhibit the strongest performance relative to their computational requirements.\n\n\n\nWhile NBEATS, NHiTS, and nVAR perform particularly well in our experiments, we refrain from endorsing these specific models to the detriment of other methods. Our results may be specific to our chaotic systems dataset and, more importantly, the recent literature contains a broad variety of new forecasting models, as well as infinite variations of each method due to hyperparameter and architectural choices, which could potentially exhibit strong performance. Rather, we have chosen representative set of forecasting models bridging different foci of the literature <cit.>, and aim to highlight general trends and the emerging strength of new models on the classical problem of forecasting chaos.\n\nIn future work, we plan to relate our empirical forecasting results to invariant properties of the different dynamical systems in our dataset, such as various measures of fractality and entropy <cit.>. Such characterization could improve the interpretability of machine learning-based forecasting models, which ostensibly provide less insight into a time series's structure than classical methods <cit.>. However, the strong empirical performance of machine learning suggests the potential for these methods to reveal new properties of nonlinear dynamics and, ultimately, new bounds on the intrinsic predictability and thus reducibility of chaotic systems.\n\n\n\n\n\n\n\n\n\n\n\u00a7 ACKNOWLEDGMENTS\n\n\nW. G. was supported by the University of Texas at Austin. Computational resources for this study were provided by the Texas Advanced Computing Center (TACC) at The University of Texas at Austin.\n\n\n\n\u00a7 CODE AVAILABILITY\n\n\nAll code used in this study is available online at <https://github.com/williamgilpin/dysts>\n\n\n\n \nnaturemag\n\n\n\n"}