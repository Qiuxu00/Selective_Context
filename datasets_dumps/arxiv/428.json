{"entry_id": "http://arxiv.org/abs/2303.06721v1", "published": "20230312180012", "title": "Knowledge-integrated AutoEncoder Model", "authors": ["Teddy Lazebnik", "Liron Simon-Keren"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.HC"], "text": "\n \nSTM observation of the hinge-states of bismuth nanocrystals\n    Herv\u00e9 Aubin\n    March 30, 2023\n===========================================================\n\n\n\n\nData encoding is a common and central operation in most data analysis tasks. The performance of other models, downstream in the computational process, highly depends on the quality of data encoding.\nOne of the most powerful ways to encode data is using the neural network AutoEncoder (AE) architecture. However, the developers of AE are not able to easily influence the produced embedding space, as it is usually treated as a black box technique, which makes it uncontrollable and not necessarily has desired properties for downstream tasks. In this paper, we introduce a novel approach for developing AE models that can integrate external knowledge sources into the learning process, possibly leading to more accurate results. The proposed  () model is able to leverage domain-specific information to make sure the desired distance and neighborhood properties between samples are preservative in the embedding space. The proposed model is evaluated on three large-scale datasets from three different scientific fields and is compared to nine existing encoding models. The results demonstrate that the  model effectively captures the underlying structures and relationships between the input data and external knowledge, meaning it generates a more useful representation. This leads to outperforming the rest of the models in terms of reconstruction accuracy. \n\n\nKeywords: data-driven encoding; biologically-inspired loss function; expert-driven model.\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nData encoding is a crucial step in many data-driven analysis models across various fields, including economics, physics, and biology <cit.>. Intuitively, the encoding process refers to the process of converting raw data into a standardized format that can be easily analyzed and interpreted <cit.>. This process typically involves transforming data into a numerical space, usually with a dimension much smaller than the original one <cit.>. The encoded representation is used to create models or perform statistical analyses. As such, data encoding plays a critical role in several applications, including machine learning, image and speech recognition, natural language processing, and genomic analysis, among others <cit.>.\n\nRecently, data-driven models are becoming increasingly complex, and as a result, the data used to train them is becoming more complex and large. As a result, there is a growing interest in the encoding space of the data and its properties <cit.>. In particular, latent spaces, in the context of deep learning, refer to the encoded representations of data samples that are learned by neural networks (NN) during training. The term \"latent\" is used because these representations are not directly observable, but rather inferred from the training data by the NN <cit.>. Latent spaces are an essential component of several deep learning models, including AutoEncoders (AEs), generative adversarial networks, and transformers.\n\nWhile providing promising results, current encoding methods leave the user (partially) blind to the properties of the encoded space that the model learned. This fact might result in unwanted behavior when known properties of the data or the domain are distributed. When focusing on the downstream tasks, users might be fine with such conditions when the results are promising but for debugging, optimization, and even explainability of the results, the current situation is sub-optimal.\n\nIn recent years, incorporating domain knowledge into machine learning models has gained significant momentum, as it has proven to be beneficial in many different applications. Incorporating domain knowledge, such as subject matter expertise or prior knowledge about the data or task, can lead to more accurate and interpretable results, as well as more efficient model designs. For example, in symbolic regression tasks, incorporating knowledge of physical laws or mathematical relationships, can reduce the computational resources required by minimizing the search space to only functions that fulfill a desired quality. Additionally, the added restraints over the search space aid in the discovery of a logical structure underlying the data <cit.>. Another area where domain knowledge is being used is in automated machine learning (AutoML). Similar to the case of symbolic regression, domain knowledge can be used to guide the design of AutoML algorithms and to constrain the search space for the best model <cit.>. Finally, domain knowledge can also be useful in the design of machine learning and deep learning models themselves <cit.>. By incorporating prior knowledge such as the structure of the input data or the relationships between different variables, models can be designed to be more efficient and accurate. This can be particularly useful in situations where data is limited, noisy, or difficult to collect.\n\nIn this paper, we present a novel approach for integrating domain knowledge into the Latent space of AEs named  (). With this approach, we propose to use an AutoEncoder-based architecture that preserves domain knowledge in the form of distance and neighborhood properties between labeled groups in the dataset, even if these properties are only partially known from outside of the dataset, as a piece of domain knowledge.\n\nWe demonstrate that  outperforms nine other AE models in three clustering tasks of datasets from the fields of economics, physics, and biology. Additionally, we demonstrate the drawback of the method, where, as in all other knowledge-informed models, the performance of the model is highly susceptible to the integration of incorrect knowledge. \n\nThe rest of this paper is organized as follows: Section <ref> reviews the state-of-the-art AE models with a focus on knowledge-integrated approaches. Section <ref> formally introduces  and outlines the experimental setup used to evaluate it. Section <ref> sets forth the experiments' results. Lastly, section <ref>, summarizes our conclusions and discusses opportunities for future work.\n\n\n\n\u00a7 RELATED WORK\n\n\nData encoding is the process of transforming raw data into a structured format that can be easily processed by a computer system <cit.>. There are two main approaches to data encoding: rule-based and data-driven. Traditional rule-based encoding methods involve using a fixed set of rules to transform data into a specific format <cit.>. For instance, geographical locations are represented using manually defined latitude and longitude values <cit.>. In contrast, data-driven encoding techniques employ statistical models to learn the encoding scheme from the data itself. This approach is particularly useful in scenarios where traditional encoding methods are not feasible or when the data has complex structures and patterns that are difficult to capture with fixed rules <cit.>. However, rule-based encoding has the advantage of being more interpretable compared to data-driven encoding <cit.>. Data-driven encoding techniques can also optimize data representation for specific tasks by learning encoding schemes that capture the most important features of the data <cit.>.\n\nIn recent years, AEs have gained popularity as powerful computational tools to achieve various goals. Specifically, AEs are a type of neural network that learn to encode and decode data in an unsupervised manner <cit.>. They consist of an encoder network that compresses the input data into a low-dimensional representation, also known as the latent space, and a decoder network that reconstructs the original data from the compressed representation. AEs have found widespread use in various tasks such as image and audio processing, anomaly detection, and data compression. AEs are particularly useful for tasks where labeled data is scarce or expensive to obtain <cit.>. \n\nDue to their usefulness, AEs have found widespread use in various fields such as economics, physics, and biology. For example, in the economic domain, <cit.> proposed a two-step electricity theft detection strategy that uses a convolutional autoencoder for electricity theft identification, where abnormal electricity consumption patterns are identified against the uniformity and periodicity of normal power consumption users. <cit.> proposed an anomaly detection model for smart farming using an AE model that reconstructs normal data with a low reconstruction loss and anomalous data with a high loss. In the physics domain, AEs have also gained much popularity. For instance, <cit.> investigated the usage of physics-constrained data-driven computing for material design using AEs. In addition, <cit.> studied the tagging of top jet images in a background of QCD jet images using AE architectures, with similar results obtained by <cit.>. Similarly, in the biological domain, <cit.> reviewed several variational AEs in the context of gene expression, showing they outperform rule-based encoding methods even with a small amount of data. <cit.> developed an algorithm that aids in the curation of gene annotations by automatically suggesting inaccuracies and predicting previously-unidentified gene functions, accelerating the rate of gene function discovery, which is based on AEs. The authors tested their AE model on gene annotation data from the Gene Ontology project, showing it outperforms many machine learning models.\n\nThe integration of domain knowledge into AEs is gaining popularity as an approach to enrich the dataset or direct the learning process <cit.>. For example, <cit.> proposed a Multi-view Factorization AutoEncoder (MAE) with network constraints that can seamlessly integrate multi-omics data and domain knowledge such as molecular interaction networks. This method learns feature and patient embedding simultaneously, using deep representation learning that constrains both feature representations and patient representations to specific regularization terms in the training objective. <cit.> proposed a method to incorporate domain knowledge explicitly in the generation process to achieve the Semantically Adversarial Generation (SAG), focusing on the driving scenes encoding task. They first categorize domain knowledge into two types; the property of objects and the relationship among objects. This approach is implemented with a tree-structured variational AutoEncoder (T-VAE) to learn hierarchical scene representation.\n\n\n\n\u00a7 METHODS AND MATERIALS\n\n\n\n\n \u00a7.\u00a7 Model definition\n\n\n is constructed from two components: a partial distance regressor and an LSTM-based AE with seven fully connected, size adaptive, layers (three as part of the encoder, three as part of the decoder, and one as a representation layer). Fig.\u00a0<ref> shows a schematic view of , presenting the inputs with domain-specific knowledge, the two components of , and the resulting outcome of a representative vector for each sample.\n\n\n\nUsing the two components, the proposed method operates in a training phase and an inference phase. During the training phase, the model obtains a tabular dataset of samples and a matrix of known distances between pairs of features (M_T). The matrix M_T is user-defined, where each value m_i,j\u2208 [0,\u221e) of the matrix represents an assumption of the relative distance between sample i and sample j, compared to the distance between other pairs of samples. If the user does not have sufficient theoretical knowledge to fill M_T completely, then the matrix is passed into the partial distance regressor component (DR, where the missing distance information is filled. Formally, DR is trained on the available distances of the pairs with a set of metrics provided by the user. Once DR is obtained, the empty entries are filled using an inference of the corresponding samples. Alternatively, the user can decide to leave the empty entries in M_T which will be ignored later on. Either way, once M_T is obtained, the AE model is trained to learn a lower-dimensional representation of the dataset. The loss function used for its training is a joint evaluation of the reconstruction capabilities of the model, and the capability to preserve all the distance properties portrayed in M_T in the Latent space. Thus, the loss function is defined as follows:\n\n    L({m_1, m_2, \u2026, m_n}) := 1/n^2 - n ( \u03c9_1 \u2211_i=1^n \u2211_i=j^n  ( ||m_i - m_i||  ) + \u03c9_2 \u2211_i=1^n   ( | ||R(m_i) - R(m_j)|| -  M_T(i, j)|  )   ),\n\nwhere m_i is a the i_th sample, m_i is the reconstructed i_th sample, R(m_i) is the representing vector of i_th sample, and \u03c9_1, \u03c9_2 \u2208 [0, 1] such that \u03c9_1 + \u03c9_2 = 1 are the weights of the domain-knowledge loss compared to the classical reconstruction loss of the AE. \n\nDuring the inference phase, as in other AE models, the decoder part of the AE is removed and the trained encoder is used to encode inputted data.\n\nThe AE used in  is constructed of an encoder network that uses a bidirectional LSTM to produce an initial embedding, either capturing temporal (ordered) data or not. The decoder network uses a unidirectional LSTM to reconstruct the input sequence using the latent embedding only. The fully connected (FC) layers are used to further reduce the representation layer size and learn high-order non-linear connections in the data. The choice to use three FC layers was motivated by the fact that a larger number of FC layers is able to capture more complex dynamics on the one hand, but it requires more time, data, and computational resources to train efficiently. All FC layers are followed by a ReLU activation function. The model is trained using the Adam optimizer <cit.>, batch size of 16 samples, and 10 epochs. \n\nThe AE's architecture can be easily altered to obtain better results for each specific task and dataset. Specifically, if the inputted sample's dimension, |m_i|, is larger than a pre-defined length |m_i| > L \u2208\u2115 than a sliding window of size L and a jump \u03c9\u2208\u2115^+ is used. At the reconstruction end, a majority vote is performed to obtain the final prediction for the reconstructed sample.\n\n\n\n \u00a7.\u00a7 Experimental setup\n\n\nTo determine the contribution of knowledge integration into an AE using the  model, we carried out three experiments on datasets from three distinctly different scientific fields: economics, physics, and biology. As seen in Table.\u00a0<ref>, each of these fields is characterized by fundamentally different characteristics of feature space and common samples count.\n\n\n\nSpecifically, to test out  on data from these scientific fields, we used three example datasets that are available online. The economic dataset\u00a0[<https://www.carrefour.com/en>] contains pricing and other properties describing 49,673 products sold in a large supermarket. The products are divided into four main categories: premium, semi-premium, regular, and under-priced <cit.>. The physics dataset contains different scales and ratios used to describe the mechanism of spherical particles settling in the air while experiencing aerodynamic drag <cit.>. The dataset contains samples of 2,500 different settling spheres, that are categorized by their density into two groups: light and heavy particles.\nFinally, for the biological dataset, we used 90 whole genome sequences equally divided between three species: Homosapien (humans), Rhesus macaque (Macaca mulatta), and Pan troglodytes (Chimpanzee), taken from NIH[<https://www.ncbi.nlm.nih.gov/>] and PGP-UK[<https://www.personalgenomes.org.uk/>]. The sequence indicators are provided as supplementary material. \n\nTo define M_T for each dataset, we integrated the following knowledge:\n\n    \n  * The distance between a sample to itself is \u03b1_i_i=0, by definition.\n    \n  * Samples of the same category are closer to one another than to samples from a different category. Therefore, the distance between two samples of the same category is randomly sampled from a uniform distribution between \u03b1_1 and \u03b1_2. We arbitrarily chose \u03b1_1 = 0 and \u03b1_2 = 1. \n    \n  * The distance between sample i and sample j, which are from two different groups x and y, respectively, are randomly sampled from a uniform distribution between \u03b3_xy and \u03b3_xy+1, such that \u2200 x, y: \u03b3_xy > \u03b1_2. The order of \u03b3_xy is determined per dataset as it reflects the domain-specific knowledge of relative distances between the categories of the data.\n    \n  * In the economic and physics datasets the order of \u03b3_ij between each pair of categories is set to \u03b3_ij=1. In the biology dataset, as Chimpanzees (group 1) are closest to Macaca mulattas (group 2), we set \u03b3_12=1. As humans (group 3) are more similar to Chimpanzees than to Macaca mulattas, we set \u03b3_13=2 and \u03b3_23=3.\n    \n  * For the Noisy , the matrix M_T is filled with values ranging between 0 and 1 at random with a uniform distribution. \n\n\nOf note, with this approach we were able to use classification data previously tagged by experts during the creation of these datasets, to approximate a domain expert's knowledge without any actual knowledge of these domains. In a more realistic scenario, the distances in M_T will be defined by more precise domain knowledge.\n\nEach dataset was used to train and evaluate five models: AE with domain knowledge (), AE without domain knowledge (AE),  with faulty domain knowledge (Noisy ), AE architecture obtained using the automatic machine learning library AutoKeras <cit.> (Auto-AE), and an AE commonly used in the specific type of problem (COMN-AE). Formally, for the COMN-AE, we trained seven common AE architectures from Ref.\u00a0<cit.>, and reported the best outcome. The Auto-AE is obtained by using AutoKeras as a black-box limited to 100 model training. This is meant to balance between the computational burden and the need to allow a thorough enough search process so that the Auto-AE can converge to a well-performing solution. For the  and Noisy  cases, we set \u03c9_1 = \u03c9_2 = 0.5. \n\nAfter obtaining the model for each of the experiments, the representation vector is computed for every sample in the dataset. Afterward, we computed the clustering using the Ward hierarchical clustering algorithm <cit.>. Using these clusters, we computed the misclassification metric <cit.>, calculated as the number of incorrect predictions divided by the total number of predictions. Since there are K! options to map for mapping a set of clusters to k groups (i.e., the possible ways to order k groups in a row), we tested all of them and reported the best score from all the options. \n\n\n\n\n\u00a7 RESULTS\n\n\n\nIn this section, we present the results of our experiments with different AE techniques, including the  method proposed in this paper. Table.\u00a0<ref> presents the misclassification rate score achieved by each model. Intuitively, the results show that introducing faulty distance and relation assumptions results in a poor ability to cluster the data, as seen by the high misclassification rates over all datasets and test cases. In all test cases, the  model resulted in the lowest misclassification rate, leading by a rate of 4%-19% over the rest of the models. Apart from the economic dataset, the Auto-AE outperforms the COMN-AE, which outperforms the simple AE used in . However, in the economic dataset, the ranking is slightly revered during the fit and train test, with the COMN-AE outperforming the Auto-AE by a rate of 2%-3%.\n\n\n \nFollowing these results, we visually explore how well the AE, , and Auto-AE were able to cluster the data. For that, we present Fig.\u00a0<ref>, which compares the performance of each technique (columns) on each dataset (rows). As the number of samples in the physics and economics datasets is too large to plot efficiently, we solved the set coverage task <cit.> with the greedy algorithm <cit.> to find n = 90 points with the smallest Euclidian distance to the obtained coverage points on the plot. This way, we ensure all the samples presented optimally represent the density and topology of the entire dataset, while keeping the visualized data small and consistent between the experiments <cit.>. Afterward, we computed the PCA <cit.> of each dataset, presenting the two eigenvectors PC_1 and PC_2 with the largest in absolute value eigenvalues. \n\n\n\nFig.\u00a0<ref> shows that the theoretical knowledge inserted to  (middle column) is preserved in the latent space. First, one can see that in the economic dataset (top), the centers of each cluster are set at roughly the same distance from one another, as was inserted. Second, in the physical dataset (middle), the centers of the two clusters are set the furthest apart, meaning that  was able to create a better distinction between the two categories of the data. Third, in the biological dataset (bottom), both the AE and  were able to preserve the theoretical distance ratios, while the Auto-AE failed (chimpanzees were found closest to humans rather than macaca mulattas).\n\n\n\n\n\n\n\n\n\n\u00a7 DISCUSSION\n\n\n\nIn this research, we proposed a novel approach for integrating domain knowledge into an AutoEncoder (AE) neural network architecture to control the distance and neighborhood properties of the obtained latent space, in order to increase the accuracy of the models. Our method, called , incorporates two novel ideas into the AE architecture: (a) a meta-regression task that takes a partially fulfilled domain-knowledge matrix and fulfills it by training a regression model from the available entries; (b) a loss function that preserves neighborhood and distance based on the domain-knowledge.\n\nThe results presented in Table\u00a0<ref> show that  outperforms a wide range of more advanced AE architectures by achieving significantly lower misclassification rates over three distinctly different scientific fields. The AE architectures examined and compared to  included the automatic search of optimal AE architectures suitable to each dataset.\n\nAdditionally, in Fig.\u00a0<ref> we demonstrate that  is able to capture both the distance and neighborhood properties of the samples in the latent space. To demonstrate that the better performance of  is associated with the new loss function rather than the AE architecture itself, we trained the architecture used for , which consistently showed worse results compared to . \n\nhowever, the results also show that the proposed method has some limitations. First, as in most knowledge-integrated models, the 's performance is highly dependent on the correctness and amount of domain knowledge provided to it. A poor quality (or even wrong) partial distance matrix can cause the model to obtain a mathematical result that complies with the inserted distance and neighborhood properties, yet harms the accuracy of the resulting clusters. To demonstrate this realistic scenario of inserting false relation assumptions, we compared in Table\u00a0<ref> the results obtained by  with faulty domain knowledge. We show that this inserts a false bias into the model <cit.>, which results in the worst results of all the models tested. Second, finding the right dimension of the representation is a challenging task in this work done via a manual trial-and-error process by the authors. The naive approach to tackle this challenge is to perform a grid search on this parameter, which can become computationally expensive even for a small set of configurations <cit.>. Future work can try to find the optimal representation dimension given a dataset and previous experience using the meta-learning approach <cit.>. Third, further improvements can be made by investigating how  can be extended to incorporate unlabeled data.\n\n\n\nunsrt\n\n\n\n\n\u00a7 DECLARATIONS\n\n\n\n \u00a7.\u00a7 Funding\n\nThis research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.\n\n\n\n \u00a7.\u00a7 Conflicts of interest/Competing interests\n\nThe authors have no financial or proprietary interests in any material discussed in this article.\n\n\n\n \u00a7.\u00a7 Materials availability\n\nThe data that has been used is presented in the manuscript with the relevant sources and available with the code on the project's page on GitHub.\n\n\n\n \u00a7.\u00a7 Author Contributions\n\nTeddy Lazebnik: Conceptualization, formal analysis, investigation, methodology, software, supervision, and writing - original draft. \n \nLiron Keren-Simon: Conceptualization, formal analysis, visualization, writing - original draft, and writing - review & editing.\n\n\n\n \u00a7.\u00a7 Acknowledgements\n\nThe authors wish to thank Walter Lutz, Alexander Liberzon, and Labib Shami for helping with the data gathering for the experiments and Stephan Beck and Ismail Moghul for helping with the research conceptualization.\n\n"}