{"entry_id": "http://arxiv.org/abs/2303.06881v1", "published": "20230313055636", "title": "OverlapNetVLAD: A Coarse-to-Fine Framework for LiDAR-based Place Recognition", "authors": ["Chencan Fu", "Lin Li", "Linpeng Peng", "Yukai Ma", "Xiangrui Zhao", "Yong Liu"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.RO"], "text": "\n\n[\n    [\n    March 30, 2023\n==================\n\nempty\nempty\n\n\n\n\nPlace recognition is a challenging yet crucial task in robotics. Existing 3D LiDAR place recognition methods suffer from limited feature representation capability and long search times. To address these challenges, we propose a novel coarse-to-fine framework for 3D LiDAR place recognition that combines Birds' Eye View (BEV) feature extraction, coarse-grained matching, and fine-grained verification. In the coarse stage, our framework leverages the rich contextual information contained in BEV features to produce global descriptors. Then the top-K most similar candidates are identified via descriptor matching, which is fast but coarse-grained. In the fine stage, our overlap estimation network reuses the corresponding BEV features to predict the overlap region, enabling meticulous and precise matching. Experimental results on the KITTI odometry benchmark demonstrate that our framework achieves leading performance compared to state-of-the-art methods. Our code is available at: <https://github.com/fcchit/OverlapNetVLAD>.\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nPlace recognition is a crucial task in mobile robots and autonomous driving, as it enables the recognition of previously visited places and provides a basis for loop closure detection. In particular, it plays a vital role in Simultaneous Localization and Mapping (SLAM), where identifying loop closures helps to correct drift and tracking errors. Visual place recognition (VPR) has been widely studied due to the low cost and rich texture information provided by cameras. In VPR, there are two main methods to describe places: local and global feature descriptors. Local descriptors are selectively extracted from parts of an image with a detection phase, while global descriptors are extracted from the entire image regardless of its content. However, long-term robot operations have revealed that changing appearance is a significant factor in visual place recognition failure<cit.>.\n\n \n\nRecently, LiDAR sensors have gained popularity for place recognition due to their ability to provide a wider field of view compared to cameras. Similar to visual place recognition methods, most LiDAR place recognition methods extract features from input point clouds to produce local or global descriptors<cit.>. However, local descriptors have limited representational power, and the detecting phase is time-consuming, while global descriptors may suffer from overfitting and are less robust to transformations. Inspired by Predator<cit.>, a unified BEV model<cit.>, later called BEVNet, has been proposed to jointly learn 3D local features and overlap estimation. The ability of BEVNet to predict overlap regions significantly enhances its performance on point cloud registration and loop closure. However, the overlap prediction process is time-consuming, and the exhaustive search time increases significantly with the size of the database.\n\nTo address these challenges, we propose a novel coarse-to-fine framework based on NetVLAD<cit.> and BEVNet<cit.> that efficiently uses BEV features to perform loop closure. Our framework comprises three parts: BEV feature extraction, coarse-grained matching, and fine-grained verification. Since BEV is a well-known natural and straightforward candidate view<cit.>, we first convert the input point cloud to a BEV representation, which is a voxel grid of multiple layers of BEVs. Then the backbone network extracts BEV features in 2D space, which is more efficient than using 3D convolutions due to the sparsity and irregularity of voxels in 3D space. In the coarse-grained matching stage, a global descriptor generation network produces a 1D global descriptor vector from encoded BEV features, and the query scan is matched against all other descriptors in the database to search for the top-K approximate nearest neighbors. Finally, in the fine-grained verification stage, the candidates are further validated by pairwise overlap prediction using an overlap estimation network, and the candidate with the highest overlap score is considered the final match. Our framework is efficient and maintains high place recognition ability. By utilizing the BEV representation, we extract useful information from point clouds and fully utilize it in our place recognition task. The proposed method significantly reduces the time-consuming overlap prediction process and is more effective in determining the matching place than existing methods. Fig.<ref> provides an illustration of our proposed framework.\n\nThe main contributions can be summarized as follows:\n\n\n    \n  * A coarse-to-fine place recognition framework that efficiently uses BEV features to perform loop closure.\n    \n  * A low-time-consuming coarse place candidate search method based on global descriptor matching.\n    \n  * An efficient method for validating selected place candidates through overlap detection without prior knowledge.\n    \n  * Comprehensive experiments and detailed ablation studies on the KITTI dataset<cit.> to validate the effectiveness of the proposed framework.\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n \u00a7.\u00a7 Point Cloud Description\n\nTo enable comparisons with other point clouds, descriptors are often utilized to provide a distinctive and reliable description of a point cloud feature. Descriptors can be classified into two categories: local descriptors and global descriptors. The difference between them lies in whether they describe the entire point cloud or not.\n\nLocal descriptors are usually generated by detecting keypoints. Guo et al.<cit.> design a local descriptor ISHOT by coupling the geometric and intensity information. Xia et al.<cit.> propose a self-attention and orientation encoding network called SOE-Net, which incorporates long-range context into point-wise local descriptors. Luo et al.<cit.> project 3D LiDAR scans to bird's-eye view images and design a descriptor called bird's-eye view feature transform (BVFT). However, local descriptors have limited representational power since they only capture local features and cannot capture the global context of the scene. In addition, local descriptors require the detection of feature points in the point cloud and the calculation of the correspondences, resulting in a higher computational cost. \n\nGlobal descriptors have emerged as an alternative to local descriptors, as they can capture the entire scene and provide a single feature vector for the whole point cloud. Several hand-crafted global descriptors have been proposed, including M2DP <cit.> and Scan Context <cit.>, which encode the scene into a compact feature representation. Inspired by Scan Context, SSC<cit.> uses semantic information to improve the representation ability of descriptors. \n\nIn recent years, learning-based 3D global descriptors have gained popularity in the research community. PointNetVLAD<cit.>, for example, combines PointNet<cit.> and NetVLAD<cit.> to extract global descriptors in an end-to-end manner. SpoxelNet<cit.> encodes voxelized point clouds into global descriptor vectors, while Locus<cit.> leverages topological and temporal information and aggregates multi-level features via second-order pooling. Other approaches focus on aggregating semantic information of point clouds. Kong et al.<cit.> propose a semantic graph representation for place recognition, and Li et al.<cit.> design a rotation-invariant global descriptor that combines semantic and geometric features and utilize a neural network to predict the similarity of descriptor pairs. There are also methods that explore new ways of encoding the features, such as DiSCO<cit.> which exploits the magnitude of the spectrum as a place descriptor, and MinkLoc3D<cit.> which uses a 3D Feature Pyramid Network<cit.> and GeM<cit.> pooling to produce global descriptors. FusionVLAD<cit.> generates multi-view representations from LiDAR scans and uses parallel encoders to fuse top-down and spherical features into translation-insensitive and rotation-invariant global descriptors. More recent works propose novel architectures to improve the representation ability of global descriptors. OverlapTransformer<cit.> utilizes a yaw-angle-invariant architecture to produce global descriptors using a transformer network, while LoGG3d-Net<cit.> leverages SparseConv U-Net to extract local feature embeddings, which are aggregated using second-order pooling and Eigen-value Power Normalization to generate global descriptors.\n\nConsidering the requirement of the place recognition task, we leverage global descriptors in the coarse searching of our framework. Since NetVLAD exhibits powerful feature representation capabilities and can be easily plugged into any other CNN architecture, we leverage it to generate global descriptors.\n\n\n\n \u00a7.\u00a7 Coarse-to-Fine Methods\n\nCoarse-to-fine methods have become increasingly popular in visual place recognition, as they allow for efficient searching of large-scale environments. BoCNF<cit.> is one such method, which uses BoW in the coarse stage to search for top-K candidate images, followed by a hash-based voting scheme in the fine stage to identify the best match. Similarly, Sarlin et al.<cit.> propose a hierarchical framework that first performs global retrieval using global descriptors and then matches local features within the candidate places.  Qi et al.<cit.> propose a progressive coarse-to-fine framework that employs global descriptors to discover visually similar references and local descriptors to filter irrelative patterns. They select candidates by global KNN retrieval and then re-rank them by region refinement and Spatial Deviation Index (SDI) spatial verification. Garg et al.<cit.> propose a semantic-geometric visual place recognition method that also uses the coarse-to-fine matching process for loop closure detection. They design a visual semantics-based descriptor and select the top N matching candidates by a cosine distance-based global place search, followed by keypoint correspondence and spatial layout consistency checks to select the best match. These methods demonstrate the effectiveness of coarse-to-fine strategies in visual place recognition.\n\n \n \nIn recent years, there has been a growing interest in developing more advanced coarse-to-fine methods for LiDAR-based place recognition. Cop et al.<cit.> design an intensity-based local descriptor DELIGHT and propose a two-stage framework to perform place recognition. They first search for N most similar DELIGHT descriptors and then perform the geometrical recognition on place candidates. The number N of place candidates is initialed as one and increases during validation until positioning succeeds or fails. Gong et al.<cit.> propose a two-level framework containing two phases, i.e., description and searching. The searching phase can be seen as a coarse-to-fine method. They first segment the point cloud into multiple clusters and extract the spatial relation descriptors (SRDs) between the clusters to form the spatial relation graph (SRG). They then utilize a two-level matching model to perform a coarse-to-fine matching of the SRGs. The upper-level searching model uses an incremental bag-of-works model to search for candidates. Then The lower-level matching model utilizes an improved spectral method to evaluate the similarities between SRGs.\n\nHowever, most of the existing methods rely on a single type of descriptor, which may not be able to fully capture the rich geometric information present in the point clouds. This limits their descriptive capability and can lead to a loss of significant information, resulting in unsatisfactory recognition performance. To address these limitations, we propose a novel framework that utilizes a powerful backbone network to extract contextual information for generating global descriptors and predicting overlapping regions. This framework combines the fast but coarse-grained matching of global descriptors with the slow but fine-grained evaluation of overlap regions, allowing for more accurate and efficient place recognition.\n\n \n\n\n\u00a7 METHODOLOGY\n\nIn this section, we describe the details of the proposed coarse-to-fine framework. We begin by introducing the BEV feature extraction process, which uses a backbone network to extract features from the voxel representation of the input point cloud in 2D space. Next, we describe the coarse-grained matching stage, where a global descriptor generation network aggregates the encoded feature map to produce a 1D global descriptor vector, and candidate scans are selected by matching the descriptors. Finally, we introduce the fine-verification stage, where corresponding pairwise BEV features are used to predict overlap regions, and the final match is obtained by calculating the overlap score. We also discuss additional design details, such as loss functions. An overview of our framework is shown in Fig.\u00a0<ref>.\n\n\n\n\n \u00a7.\u00a7 BEV Feature Extraction\n\nSince the raw point cloud is irregular and unstructured, we first create a voxel grid from the input point cloud. We adopt a multi-layer occupied BEV representation<cit.>, where each voxel is considered occupied if there is any point in it. By doing this, the point cloud P is divided into a  H_B\u00d7 W_B\u00d7 C_B  grid, resulting in a BEV representation  B_P\u2208\u211d^H_B\u00d7 W_B\u00d7 C_B.\n\n\n\n\nBackbone. The encoder module of the BEVNet<cit.> can effectively extract contextual information, so we leverage it as our backbone network. The backbone is a 2D convolution network with a solid ability to extract complex features. We feed the BEV grid into the network to extract deep features in 2D space. Considering the sparsity of the point cloud, 2D sparse convolutions are used to speed up the feature extraction. The final output feature map (BEV features) contains rich and useful contextual information, which is then stored in the database. The BEV feature map is as follows:\n\n\n    E_P = \u2130(B_P),\n\nwhere \u2130: B\u211d^H_E\u00d7 W_E\u00d7 D_E is the shared 2D BEV feature extractor, which maps the BEV grid B_P to a H_E \u00d7 W_E \u00d7 D_E dimensional BEV feature map E_P.\n\n\n\n \u00a7.\u00a7 Coarse-Grained Matching\n\nAfter extracting a BEV feature map from the input voxel grid, the feature E_P is passed through the global descriptor generation network. The network comprises two convolutional layers, a NetVLAD layer and a fully connected layer. The last two modules are followed by <cit.>. First, we leverage the convolutional layers to aggregate features, and the output feature E'_P is computed as follows:\n\n\n    E'_P = Con_3\u00d73(ReLU(Con_3\u00d73(E_P))),\n\nthen the feature E'_P is fed into the NetVLAD layer. The NetVLAD layer aggregates local descriptors x_i \u2208\u211b^D within the (N\u00d7 D)-dimension feature by summing the residuals between each descriptor and K cluster centers, which are weighted by soft-assignment of descriptors to multiple clusters. The output VLAD representation V is a D \u00d7 K matrix. \n\n\n    V(j, k) = \u2211_i=1^N a_k(x_i) (x_i(j)-c_k(j)),\n\nwhere a_k denotes the soft-assignment, x_i is the i^th descriptor in E'_P, and x_i(j) is the j^th element of the i^th descriptor, c_k is the k^th cluster center.\n\nTo reduce computational costs, we utilize a fully connected layer to reduce the dimension of the VLAD descriptors, followed by <cit.>. Finally, we obtain the global descriptor vector F(P) \u2208\u211d ^ \ud835\udcaa, where \ud835\udcaa\u226a (D \u00d7 K).\n\nThe coarse phase of our proposed framework involves searching for candidates close to the query place. Since matching 1D descriptor vectors is computationally efficient, we perform candidate selection by descriptor matching. First, we generate global descriptors for all point clouds and build up a descriptor database. We then compute the distance matrix between the descriptor of the query scan and the remaining descriptors in the database. To select the top-K approximate nearest neighbors as candidate matches, we sort the distances in ascending order based on the K closest distances. However, as the number of descriptors increases significantly with the expansion of the map in real-world autonomous driving scenarios, computation of the distance matrix may become prohibitively slow. To address this, we use a KD-tree implementation to accelerate feature matching. We compute pairwise Euclidean distances between the descriptor vectors of the point clouds P and Q.\n\n\n    D_PQ = \u2225 F(P)-F(Q) \u2225_2,\n\nwhere \u2225\u00b7\u2225_2 denotes the L_2 normalization and F(\u00b7) denotes the function that maps the input point cloud to a global descriptor vector.\n\n\n\n\n \u00a7.\u00a7 Fine-Grained Verification\n\nIn the fine-grained verification stage, we use overlap estimation to identify the most similar place among the top-K candidates. A cross-attention module is utilized to detect the overlap regions. The cross-attention mechanism can effectively interact information<cit.> and aggregate the contextual information to enhance feature representation. We reuse the previously stored features to reduce the overall computation time. Pairwise BEV features are fed into a cross-attention architecture to fuse features, similar to ImLoveNet<cit.>. We estimate overlap regions between pairs using the similarity score as an evaluation metric and select the point cloud with the highest overlap score as the final match. The overlap estimation process is illustrated in Fig.<ref>.\n\n\nCross-Attention. We use a cross-attention module that is similar to the one used in ImLoveNet <cit.> but with a slight modification. Instead of fusing deep triple features from 2D images and 3D point clouds, we fuse 2D features E_P and E_Q to generate relevant feature maps M_P and M_Q.\n\n\n    M_P    = E_P + MLP(cat(E_P, att(E_P, E_Q, E_Q))) \n    \n        M_Q    = E_Q + MLP(cat(E_Q, att(E_Q, E_P, E_P))),\n\nwhere MLP(\u00b7) denotes a three-layer fully connected network, cat(\u00b7,\u00b7) means concatenation and att(\u00b7,\u00b7,\u00b7) is the attention model.\n\nClassification Head. A binary classification is applied on the correlated feature maps M_P and M_Q to predict overlap region as follows:\n\n    \u03b3_t = Sigmoid(Conv_3\u00d73(ReLU(Conv_3\u00d73 (M_t)))), t\u2208{P,Q},\n\nThe classification head consists of two 3\u00d73 convolutional layers and a sigmoid layer with ReLU as the activation function.\n\nOverlap Score. The overlap score is used to evaluate the similarity of pairwise point clouds by quantifying the overlapping area. The similarity metric is as follows:\n\n    \u03c4 = 1/2(\u2211\u03b3_P/V_P + \u2211\u03b3_Q/V_Q),\n\nwhere V_P and V_Q denote the total number of non-zeros volumes of M_P and M_Q.\n\n\n\n \u00a7.\u00a7 Loss Function\n\n The training of the descriptor generation network is supervised by quadruplet loss, while the overlap detection network uses classification loss and circle loss<cit.> as supervisions.\n\nQuadruplet Loss. To train the global descriptor generation network, we use the quadruplet loss \u2112_quad. This loss helps the network learn how to accurately describe places so that the descriptors of close places are similar and the descriptors of distant places are significantly different. The quadruplet loss involves four inputs: an anchor, a positive input, and two negative inputs. So our training tuple is as \ud835\udcaf={ F(P_a), F(P_p), F(P_n), F(P_n^*) }, where P_a is an anchor point cloud, P_p is a positive point cloud close to P_a. P_n and P_n^* are two different point clouds far away from P_a, and these two point clouds are structurally dissimilar to each other as well. The quadruplet loss is as follows:\n\n\n    \u2112_quad=   [F(P_a)-F(P_p)_2 - F(P_a)-F(P_n)_2+\u03b1_1]_+ \n     \n        +    [F(P_a)-F(P_p)_2-F(P_n)-F(P_n^*)_2+\u03b1_2]_+,\n\nwhere [\u00b7]_+ is the hinge loss, \u03b1_1 and \u03b1_2 are the constant margins.\n\nClassification Loss. The classification loss adopts a binary cross entropy as\n\n\n    \u2112_bce=BCE(\u03b3_P,l_P)+BCE(\u03b3_Q,l_Q),\n\nwhere BCE(\u00b7, \u00b7) represents the binary cross entropy and l_P, l_Q are the ground truth of the overlap.\n\nCircle Loss. In <cit.>, it is found that circle loss can strengthen the network's ability to perform overlap region classification, so we use circle loss as supervision as well. To balance the distribution of positive and negative samples, random sampling is performed on BEV feature maps. Positive and negative sets of correspondences are then formed for each element E_P_i of the BEV feature map E_P. Specifically, the positive set \u03a9_p(E_P_i) is defined as points in E_Q that lie within a radius r_p around E_P_i, while the negative set \u03a9_n(E_P_i) consists of points outside a radius r_n. The circle loss of E_P is formulated as follows:\n\n\n    \u2112_cir^P = 1/n_P\u2211^n_P_i=1 log[ 1 + \u2211_j\u2208\u03a9_pe^\u03b2_p^j(d_i^j-\u0394_p)\u00b7\u2211_k\u2208\u03a9_ne^\u03b2_n^k(\u0394_n - d_i^k)],\n\nwhere d_i^j = \u2016 f'_E_P_i - f'_E_Q_j\u2016_2 denotes distance in the feature space, and \u0394_n, \u0394_p are negative and positive margins, the weights \u03b2_p^j and \u03b2_n^k are determined individually for each positive and negative example. The reverse loss \u2112_cir^Q is computed in the same way, and the total circle loss \u2112_cir = 1/2 (\u2112_cir^P + \u2112_cir^Q).\n\n\n\n\u00a7 EXPERIMENTS\n\nTo demonstrate the effectiveness and practicality of our proposed OverlapNetVLAD framework for LiDAR-based place recognition, we designed a series of experiments. Specifically, we evaluate our method on the widely-used KITTI odometry benchmark and compare its performance with other state-of-the-art place recognition methods. Furthermore, we conduct ablation studies to investigate the impact of each module on the final results.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Datasets and Implementation Details\n\nKITTI Odometry Benchmark. The KITTI odometry benchmark contains point clouds from various urban environments. The point clouds are collected using a Velodyne HDL-64E 3D laser scanner, which captures highly accurate 3D information. Moreover, the recording platform has a GPS/IMU location unit and RTK correction signals, providing ground truth poses<cit.>. The dataset includes 22 sequences, and we train our networks on the last 11 sequences with the ground truth trajectories provided by SemanticKITTI<cit.>. We evaluate the performance of our proposed method specifically on sequences 00, 02, 05, 06, 07, and 08, which contain revisited fragments, to detect loop closures.\n\nImplementation Details. In the preprocessing stage, we transform the input point cloud into a BEV representation with a shape of 256\u00d7256\u00d732. The backbone network then extracts a sparse 2D feature map with a shape of 32\u00d732\u00d7512 using the SpConv<cit.> for improved speed. These feature maps are saved in a database for subsequent use by the global descriptor generator and overlap estimator.\n \nFor global descriptor generation, we use a NetVLAD layer with an intermediate feature dimension of 512, a maximum number of pooled samples of 1024, and 32 clusters. The output feature dimension is 1024, resulting in a final global descriptor vector length of 1024. The overlap estimator predicts the pairwise overlap region using a 1\u00d732\u00d732 tensor and calculates the overlap score.\n \nTo train the descriptor generation network, we use point clouds less than 10m away from the query scan as positive examples and those more than 50m away as negative examples. We use a pre-trained model from <cit.> to extract BEV features and perform overlap estimation. During the evaluation, we exclude the previous 50 scans of the query scan, as they are too close. For candidate selection in the coarse search, we use K=1% (of the sequence length) as the number of candidates. We denote our method with only global descriptor matching as ours-NV and the complete coarse-to-fine method as ours-ONV.\n\n\n\n \u00a7.\u00a7 Loop Closure Detection\n\n In this section, we present the evaluation of the loop closure detection performance of our proposed framework and compare it with several state-of-the-art methods, including OverlapTransformer<cit.>, DiSCO<cit.>, Locus<cit.>, RINet<cit.> and BEVNet<cit.>. We use recall@1 and recall@1% as the evaluation metric. The inference is considered correct if the distance between the matched scan and the query scan is less than 10m, otherwise incorrect. As shown in Tab.\u00a0<ref>, our method (ours-ONV) achieves the highest recall@1, with a score 0.1% higher than BEVNet and significantly higher than the other methods. Although the recall@1 of BEVNet and our method is similar, our approach has the advantage of being much less time-consuming. This is because our method significantly reduces the number of candidates when searching for matches, which leads to using fewer pairwise overlap estimates. Interestingly, the recall@1 of BEVNet on sequences 06 and 07 is lower than ours, which we believe is due to the increased probability of misestimation when increasing the number of candidates. Notably, our method can effectively and robustly detect reverse loop closures in sequence 08, which has only reverse loop closures. As a result, our method can achieve comparable loop closure performance to BEVNet, while requiring fewer computational resources.\n \nIn terms of recall@1%, RINet, BEVNet, Ours-NV, and Ours-ONV achieve a score close to 100%, while other methods achieve around 96%. The recall@1% of ours-NV indicates that our candidate selection process provides valid and accurate candidates, which is the foundation of our coarse-to-fine approach.\n\nFurthermore, we evaluate the performance of the proposed method using recall@N on the KITTI dataset. As shown in Fig.\u00a0<ref>, our method outperforms most baseline methods. And the average recall on KITTI dataset is shown in Fig.\u00a0<ref>. The experimental results demonstrate the strong robustness and good generalization ability of our method for place recognition under challenging conditions.\n\n\n\n\n\n\n \u00a7.\u00a7 Ablation Study\n\nIn this section, we present our ablation studies on the proposed framework. We aim to evaluate the effectiveness of the coarse and fine stages of our approach independently. First, we investigate the impact of the global descriptor matching in the coarse stage. We choose the number of top references as N=1, 10, 15, 20, 25, 1% to test recall@N for place recognition using only global descriptor matching, which we refer to as ours-NV. As shown in Tab.\u00a0<ref>, we observe that recall@1 alone is not sufficient to achieve outstanding performance, and the actual match places are not well recognized in sequence 08. However, as the number of candidates increases, the recall also increases significantly. When we use 1% of the total place as reference candidates, recall@1% achieves 99.9%. Therefore we choose K=1% as the candidate number for a coarse search.\n \nNext, we evaluate the effect of candidate number K in the fine stage where the overlap estimation is applied. As shown in Tab.\u00a0<ref>, the recall@1 of only using global descriptor to detect loop closure (VLAD) is much lower than ours ONV-1%, which selects 1% of all scans as candidates by descriptor matching. Overall the recall@1 of our methods grows with the number of candidates, as we expected. Meanwhile, the recall of our framework is almost the same as BEVNet<cit.> (Overlap in Tab.\u00a0<ref>), while the latter is much more time-consuming. Comparing the results in Tab.\u00a0<ref>, we observe that the overlap estimation works well to detect loop closure using the proposed candidate selection approach.\n\nIn conclusion, the ablation studies validate that combining global descriptor matching and overlap estimation creates an effective and robust coarse-to-fine framework. \n\n\n\n\n\n\n\n \u00a7.\u00a7 Runtime\n\nIn this section, we conduct experiments to evaluate the efficiency of our method and compare it with other methods. The experiments are performed on a system equipped with an Intel Core i7-7700 CPU with 3.60GHz and an Nvidia GeForce GTX 1080 Ti with 11GB memory, using sequence 00 which contains 4541 scans. Tab.\u00a0<ref> shows that the candidate selection only takes 12.50ms, while the pairwise overlap estimation takes 7.13ms. We mainly compare the time consumption of ours-ONV with BEVNet. Since both methods use the same BEV feature maps, and they can be saved in the database without needing to be extracted each time, we mainly compare the time after feature extraction. For a query scan on sequence 00, ours-ONV costs 0.46\u00d74541+12.50+7.13\u00d74540\u00d71%=2425.06 ms, while BEVNet that performs overlap estimation for all references costs 7.13\u00d74540=32370.20 ms. Notably, descriptors can also be saved in a database, eliminating the need to generate them for each search. Overall, Our method saves much time and has similar place recognition performance.\n\nWe also evaluate the runtime of other methods to compare the efficiency of descriptor generation. As shown in Tab.\u00a0<ref>, the global descriptor generation in our framework is very efficient compared to the other methods.\n\nIn summary, the matching of global descriptors provides a coarse but fast candidate selection, which is the foundation of the proposed coarse-to-fine framework. The runtime evaluation validates the effectiveness of our framework for selecting multiple candidates to improve loop closure detection speed.\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\nIn this paper, we propose a novel coarse-to-fine framework that provides a robust and efficient solution for LiDAR-based place recognition. The combination of global descriptor matching and overlap estimation allows for fast candidate selection and accurate loop closure detection. Moreover, our method achieves leading performance on the KITTI odometry benchmark compared to the state-of-the-art methods. While the framework has some limitations, such as the bulky and memory-intensive BEV features, future work will address these challenges and explore the integration of multi-modal data for improved place recognition.\n\nieeetr\n\n\n\n\n"}