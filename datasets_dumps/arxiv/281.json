{"entry_id": "http://arxiv.org/abs/2303.06958v1", "published": "20230313095652", "title": "CPQR-based randomized algorithms for generalized CUR decompositions", "authors": ["Guihua Zhang", "Hanyu Li", "Yimin Wei"], "primary_category": "math.NA", "categories": ["math.NA", "cs.NA"], "text": "\n\n\n\n\n\n\n\n\n\n\nGuihua Zhang and Hanyu Li College of Mathematics and Statistics, Chongqing University, Chongqing, 401331, \n              \n              People\u2019s Republic of China.\n              lihy.hy@gmail.com; hyli@cqu.edu.cn\n\nYimin Wei  School of Mathematical Sciences and Shanghai Key Laboratory of Contemporary Applied Mathematics, Fudan University, Shanghai 200433, People\u2019s Republic of China.\n              ymwei@fudan.edu.cn; yimin.wei@gmail.com \n\n           \n           \n            \n\n\n\n\n\n\nCPQR-based randomized algorithms for generalized CUR decompositions\n\n\n\n    Guihua Zhang         Hanyu Li Yimin Wei\n         \n\n    Received: date / Accepted: date\n======================================================================\n\n\n\n\nBased on the column pivoted QR decomposition, we propose some randomized algorithms including pass-efficient ones for the generalized CUR decompositions of matrix pair and matrix triplet. \nDetailed error analyses of these algorithms are provided.  Numerical experiments are given to test the proposed randomized algorithms.\n\n\n 15A99 68W20 \n\n\n\n\n\u00a7 INTRODUCTION\n\nAs we know, the singular value decomposition (SVD) is the most popular tool for \nreducing dimension of a data matrix. However, \nthe final features \nwith the dimension reduction by SVD are usually difficult to explain and analyse <cit.>. To this end, some scholars proposed the CUR decomposition, \nwhich is composed of  subsets of the original columns and rows of the given matrix. Specifically, \nfor a matrix A, the CUR decomposition can be expressed as\n\n    A \u2248 C_AM_AR_A,\n\nwhere C_A and R_A are the subsets of columns and rows of A respectively, and M_A can be constructed by different ways; see Remark <ref> below. Recently, Gidisu and Hochstenbach <cit.> proposed the generalized CUR decompositions for matrix pair and matrix triplet; see Definitions <ref> and <ref> below. \n\nAt present, there are many works on applications (i.e., <cit.>), algorithms (i.e., <cit.>), and perturbation analyses (i.e., <cit.>) for (generalized) CUR decompositions. Here we mainly focus on their algorithms. For the case of CUR decomposition, Drineas et al. <cit.> presented the relative error guaranteed algorithm from the point of view of leverage-score sampling.  \nSubsequently, Wang and Zhang <cit.> proposed the adaptive sampling algorithm, \nand Boutsidis and Woodruff <cit.> combined the previous two sampling methods with BSS sampling <cit.> \nand obtained the optimal CUR decomposition.\n\nLater, Sorensen and Embree <cit.> introduced a novel algorithm by means of the discrete empirical interpolation method (DEIM), and Voronin and Martinsson <cit.> presented efficient algorithms based on the column pivoted QR decomposition (CPQR). Recently, Chen et al. <cit.> proposed an algorithm by using the truncated LU factorization, and Dong and Martinsson <cit.> compared the randomized algorithms based on CPQR <cit.> and LU decomposition with partial pivoting <cit.>.\nFor the case of generalized CUR decompositions of matrix pair and matrix triplet, by virtue of the DEIM, Gidisu and Hochstenbach <cit.> proposed the corresponding deterministic algorithms. \nVery recently, Cao et al. <cit.> presented some randomized algorithms based on L-DEIM <cit.>. \n\nIn this paper, we investigate the CPQR-based randomized algorithms for the generalized CUR decompositions of matrix pair and matrix triplet, and \n\npresent the corresponding expectation error bounds. \n\nThe rest of this paper is organized as follows. In Section <ref>, we introduce some preliminaries including the definitions of the generalized CUR decompositions for matrix pair and matrix triplet and the interpolative decomposition. The CPQR-based randomized algorithms and their error bounds are presented in Sections <ref> and <ref>. Finally, we provide some numerical results to test the proposed algorithms. \n\n\n\n\n\u00a7 PRELIMINARIES\n \nWe first introduce some notations used in this paper. Given a matrix A\u2208\u211d^m \u00d7 n, A^T, A^\u2020, \u2016 A\u2016 , \u2016 A\u2016_F and \u03c3_j(A) denote its transpose, Moore-Penrose inverse, spectral norm, Frobenius norm and jth largest singular value, respectively. \n\nAnd, we use A(I,J) to denote the part of A consisting of rows from the index set I and columns from the index set J. \nIn addition, let [n]={1,\u22ef,n}, I_n be the n\u00d7 n identity matrix, \ud835\udd3c be the expectation notation, \ud835\udd3c[ \u00b7|\u00b7] be the conditional expectation, \nP_A be the orthogonal projector on range(A), and P_A,B be the oblique projector on range(A) along range(B). \n\n\n\n \u00a7.\u00a7 CUR decomposition for matrix pair\n\n\n\n\n\tLet (A,B) be a matrix pair, where A\u2208\u211d^m \u00d7 n and B\u2208\u211d^d \u00d7 n. The CUR decomposition of (A,B) of rank k is\n\t\n    A_k=C_AM_AR_A,\n    \t\tB_k=C_BM_BR_B,\n\nwhere C_A=A(:,J)\u2208\u211d^m \u00d7 k, C_B=B(:,J)\u2208\u211d^d \u00d7 k, R_A=A(I_A,:)\u2208\u211d^k \u00d7 n and R_B=B(I_B,:)\u2208\u211d^k \u00d7 n.\n\n\n\n\t\n    The middle matrices M_A and M_B have some different ways to compute. For example, \n    we can construct M_A=A(I,J)^\u2020, or M_A=C_A^\u2020 AR_A^\u2020 <cit.>. In this paper, we use M_A=C_A^\u2020 AR_A^\u2020 to analyse the related algorithms. \n\n\nTo obtain the error analyses of our randomized algorithms of generalized CUR decompositions, \nwe  need \nthe generalized singular value decomposition (GSVD). \nFor the above matrix pair (A,B), \nthe GSVD can be expressed as\n\n    A=U\u03a3_1Y^T, B=\u1e7c\u03a3_2Y^T,\n\nwhere U\u2208\u211d^m \u00d7 m and \u1e7c\u2208\u211d^d \u00d7 d are column orthogonal, Y\u2208\u211d^n \u00d7 n is nonsingular, and \u03a3_1 \u2208\u211d^m \u00d7 n and \u03a3_2 \u2208\u211d^d \u00d7 n are diagonal with diagonal entries being in [0,1]. Here, we let the diagonal entries of \u03a3_1 be in nondecreasing order, while those of \u03a3_2 be in nonincreasing order. For the details of GSVD, please refer to <cit.>.\n\n\n\n\n \u00a7.\u00a7 CUR decomposition for matrix triplet\n\n\n \n\tLet (A,B,G) be a matrix triplet, where A\u2208\u211d^m \u00d7 n, B\u2208\u211d^m \u00d7 t and G\u2208\u211d^d \u00d7 n. The CUR decomposition of (A,B,G) of rank k is\n\t\n    A_k=C_AM_AR_A,\n    \t\tB_k=C_BM_BR_B,\n    \t\tG_k=C_GM_GR_G,\n\n\twhere C_A=A(:,J)\u2208\u211d^m \u00d7 k, C_B=B(:,J_B)\u2208\u211d^m \u00d7 k, \n\tC_G=G(:,J)\u2208\u211d^d \u00d7 k, R_A=A(I,:)\u2208\u211d^k \u00d7 n, R_B=B(I,:)\u2208\u211d^k \u00d7 t and \n\tR_G=G(I_G,:)\u2208\u211d^k \u00d7 n.\n\n\n\tThe decomposition shows that the selected column indices of A and G are the same, and the selected row indices of A and B are the same. For the middle matrices M_A, M_B and M_G, they can be constructed as discussed in Remark <ref>.\n\n\n\n \n\n\n \u00a7.\u00a7 Interpolative decomposition (ID)\n\n\nThe ID \nis very close to the CUR decomposition. Specifically, given A\u2208\u211d^m \u00d7 n, it has the following forms\n\n    A\u2248\u0108V\u0302^T, A\u2248\u0174R\u0302,\n\nwhere \u0108\u2208\u211d^m \u00d7 k and R\u0302\u2208\u211d^k \u00d7 n consist of k columns \nand rows of A, respectively, and \nV\u0302\u2208\u211d^n \u00d7 k and \u0174\u2208\u211d^k \u00d7 n are such that max_i,j|V\u0302(i,j)|\u2264 1 and max_i,j|\u0174(i,j)|\u2264 1, respectively. The above two IDs are one-sided IDs, i.e., column ID and row ID. Accordingly, there exists a two-sided ID: \n\n    A\u2248\u0174A_sV\u0302^T,\n\nwhere \u0174\u2208\u211d^m \u00d7 k, V\u0302\u2208\u211d^n \u00d7 k, and A_s\u2208\u211d^k \u00d7 k \nis the submatrix of A.\n\n\n\n\n\n\u00a7 RANDOMIZED ALGORITHMS FOR CUR DECOMPOSITION OF MATRIX PAIR\n \n\nWe first propose a randomized algorithm based on CPQR. Then,  \na pass-efficient version of the \nalgorithm is derived. Their  expectation error bounds are also presented correspondingly.\nMoreover, we also provide the alternative error analyses of algorithms on the basis of GSVD.\n\n\n\n\n \u00a7.\u00a7 CPQR-based randomized algorithm\n\n\nFor A\u2208\u211d^m \u00d7 n and B\u2208\u211d^d \u00d7 n, assume rank(A) > k and rank(B) > k. Using CPQR, \nwe have\n\n    [ A; B ]\u03a0\u0302=Q\u0302T\u0302=Q\u0302[ T\u0302_1, T\u0302_2 ],\n \nwhere Q\u0302\u2208\u211d^(m+d) \u00d7 (m+d) is orthogonal, T\u0302\u2208\u211d^(m+d) \u00d7 n is upper triangular with  T\u0302_1\u2208\u211d^(m+d) \u00d7 k and T\u0302_2\u2208\u211d^(m+d) \u00d7 (n-k), and \u03a0\u0302\u2208\u211d^n \u00d7 n is a permutation matrix. \nSimilar to <cit.>, C_A and C_B in the CUR decomposition of the matrix pair (A,B) can be obtained from \nQ\u0302T\u0302_1 directly. \nHowever, it \nis not very efficient on the computation cost. To tackle this problem, we consider the randomized CPQR \n<cit.>. \n\n\nLet \u03a9\u2208\u211d^l \u00d7 (m+d) be a Gaussian matrix, where l=k+p and p is the oversampling parameter. Then, using CPQR, we have\n\n    \u03a9[ A; B ]\u03a0=Q\u0303T=Q\u0303[ T_1, T_2 ],\n\nwhere Q\u0303\u2208\u211d^l \u00d7 l is orthogonal, T_1\u2208\u211d^l \u00d7 l is invertible and upper triangular, and \u03a0=I_n(:,J_n) with J_n being a permuted index vector. Based on the above decomposition, we can get the column index vector J appearing in the CUR decomposition of the matrix pair (A,B), \ni.e., J=J_n(1:l). \nThe specific algorithm is listed \nin Algorithm <ref>, from which, we can obtain\n\n    XI_n(:,J)=X\u03a0_C=Q\u0303T_1.\n\n\n\n\n\n\nTo investigate the theoretical analysis of Algorithm <ref>, we further consider the following column ID of the matrix pair (A,B): \n\n\n    [ A; B ]\u2248[ C_A; C_B ] V^T =[ C_AV^T; C_BV^T ],\n\nwhich \ncan be written as A \u2248 C_AV^T, B \u2248 C_BV^T.\n\n\t\n\nSolving one of them by the least squares method implies\n\n    V^T=C_A^\u2020A=C_B^\u2020B.\n\nBesides, noting that X\u03a0_C is invertible due to both Q\u0303 and T_1 being invertible, we define a rank-l oblique projector onto the row space of X as follows:\n\n    P_\u03a0_C,N_1=\u03a0_C(X\u03a0_C)^-1X=\u03a0_CT_1^-1Q\u0303^TX.\n\nSince we mainly focus on the range where the oblique projector projects on,  hereafter we just use N_x to denote the along range range(N_x) for oblique projectors. \nIt is clear that XP_\u03a0_C,N_1=X.\n\nNow, we present the\ntheoretical analysis of Algorithm <ref>. \n \n\tLet C_A\u2208\u211d^m \u00d7 l and C_B\u2208\u211d^d \u00d7 l be obtained by Algorithm <ref>, and V^T=C_A^\u2020A=C_B^\u2020B. Then \n\t\n    \u2016 A-C_AV^T \u2016\u2264\u2016 I_n-P_\u03a0_C,N_1\u2016\u2016 A(I_n-X^\u2020X) \u2016,  \n    \u2016 B-C_BV^T \u2016\u2264\u2016 I_n-P_\u03a0_C,N_1\u2016\u2016 B(I_n-X^\u2020X) \u2016.\n\n\n\n\tWe first consider the case for the matrix A. Since C_A=A(:,J)=A\u03a0_C is full column rank, we find that\n\t\n    C_AC_A^\u2020A=A\u03a0_C(C_A^TC_A)^-1C_A^TA=AP_\u03a0_C,N_2,\n\nwhere P_\u03a0_C,N_2=\u03a0_C(C_A^TC_A)^-1C_A^TA. Considering the relationship between P_\u03a0_C,N_2 and P_\u03a0_C,N_1, \n\n    P_\u03a0_C,N_2P_\u03a0_C,N_1   =\u03a0_C(C_A^TC_A)^-1C_A^TA\u03a0_C(X\u03a0_C)^-1X \n       =\u03a0_C(X\u03a0_C)^-1X=P_\u03a0_C,N_1,\n\nwe get\n\n    A(I_n-P_\u03a0_C,N_2)   =A(I_n-P_\u03a0_C,N_2)(I_n-P_\u03a0_C,N_1) \n       =(I_m-C_AC_A^\u2020)A(I_n-P_\u03a0_C,N_1).\n\nHence, \n\n    \u2016 A-C_AV^T \u2016   =\u2016 (I_m-C_AC_A^\u2020)A \u2016 = \u2016 A(I_n-P_\u03a0_C,N_2) \u2016\n       \u2264\u2016 I_m-C_AC_A^\u2020\u2016\u2016 A(I_n-P_\u03a0_C,N_1) \u2016\n       =\u2016 A(I_n-P_\u03a0_C,N_1) \u2016 \n    \t=\u2016 A(I_n-X^\u2020X)(I_n-P_\u03a0_C,N_1) \u2016\n       \u2264\u2016 A(I_n-X^\u2020X) \u2016\u2016 I_n-P_\u03a0_C,N_1\u2016,\n\nwhere the third equality holds because I_m-C_AC_A^\u2020 is an orthogonal projector and the last equality relies on \nXP_\u03a0_C,N_1=X.\n\nFor the case of the matrix B, we can similarly define \n\nP_\u03a0_C,N_3=\u03a0_C(C_B^TC_B)^-1C_B^TB\n\n\t\n\nand get \nP_\u03a0_C,N_3P_\u03a0_C,N_1=P_\u03a0_C,N_1.\n\n\t\n\nThus, along the similar line, (<ref>) can be obtained.\n\n\nNext, we \ndiscuss how to select the rows of A and B, respectively. The idea is to find \nthe row index vectors I_A\u2282 [m] and I_B\u2282 [d] \nfrom C_A and C_B. \n\nTo this end, we consider the exact CPQRs of the transposes of C_A and C_B:\n\n    l\u00d7 mC_A^T[ m\u00d7 l\u03a0_Ar, m\u00d7 (m-l)\u03a0_Ar^c ]   =l\u00d7 lQ_A[ l\u00d7 lT_A1, l\u00d7 (m-l)T_A2 ]=l\u00d7 lA_s^T[ l\u00d7 lI_l, l\u00d7 (m-l)T_A1^-1T_A2 ], \n    l\u00d7 dC_B^T[ d\u00d7 l\u03a0_Br, d\u00d7 (d-l)\u03a0_Br^c ]   =l\u00d7 lQ_B[ l\u00d7 lT_B1, l\u00d7 (d-l)T_B2 ]=l\u00d7 lB_s^T[ l\u00d7 lI_l, l\u00d7 (d-l)T_B1^-1T_B2 ],\n\nwhere \u03a0_Ar=I_m(:,I_A), \u03a0_Br=I_d(:,I_B), \nQ_A and Q_B are orthogonal, \nand T_A1 and T_B1 are invertible and upper triangular due to C_A and C_B being full column rank. \n\nFurther, the exact row IDs of C_A and C_B are given by\n\n    C_A=\u03a0_Ar(Q_AT_A1)^T:=W_AA_s,  C_B=\u03a0_Br(Q_BT_B1)^T:=W_BB_s.\n\nHere, A_s=(Q_AT_A1)^T and B_s=(Q_BT_B1)^T consist of the rows of C_A and C_B, respectively.\nBy denoting R_A=A_sV^T and R_B=B_sV^T, we have\n\n    W_AA_sV^T=C_AV^T=C_AC_A^\u2020A=W_AR_A=(\tW_AA_s)A_s^\u2020(A_sV^T)=C_AA_s^\u2020R_A, \n     \n    \tW_BB_sV^T=C_BV^T=C_BC_B^\u2020B=W_BR_B=(\tW_BB_s)B_s^\u2020(B_sV^T)=C_BB_s^\u2020R_B.\n\nThus, combining with the results that C_A and C_B are full column rank and R_A and R_B are full row rank implies \n\n    A_s^\u2020=C_A^\u2020AR_A^\u2020:=M_A,  B_s^\u2020=C_B^\u2020BR_B^\u2020:=M_B.\n\n\n\nTherefore, we can have \nthe randomized algorithm for the CUR decomposition of matrix pair, i.e., \nAlgorithm <ref>. \n\n\n\nTo present the expectation error bounds of Algorithm <ref>, the following lemma is necessary. \n\n <cit.> \n\tGiven a matrix A\u2208\u211d^m \u00d7 n, the target rank k and the oversampling parameter p satisfying k+p \u2264 min{m,n}, and a standard Gaussian matrix \u03a9\u2208\u211d^(k+p) \u00d7 m, the column sketch X=\u03a9 A satisfies\n\t\n    \ud835\udd3c\u2016  A(I_n-X^\u2020 X)  \u2016\u2264( 1+\u221a(k/p-1)) \u03c3_k+1(A) + e\u221a(k+p)/p( \u2211_j>k^\u03c3_j^2(A) )^1/2, \n       \ud835\udd3c\u2016  A(I_n-X^\u2020 X)  \u2016_F \u2264( 1+\u221a(k/p-1))^1/2( \u2211_j>k^\u03c3_j^2(A) )^1/2.\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\tLet C_A\u2208\u211d^m \u00d7 l, C_B\u2208\u211d^d \u00d7 l, R_A\u2208\u211d^l \u00d7 n, R_B\u2208\u211d^l \u00d7 n, M_A\u2208\u211d^l \u00d7 l, and M_B\u2208\u211d^l \u00d7 l be obtained by Algorithm <ref>. Let k be the target rank and p be the oversampling parameter. \n\t\n\tThen \n\n\n\n\n\n\n\n\n\n\n\n\n    max{\ud835\udd3c\u2016 A-   C_AM_AR_A \u2016, \ud835\udd3c\u2016 B-C_BM_BR_B \u2016}\u2264\u221a(1+(n-k-p)4^k+p-1)\n       \u00d7[ ( 1+\u221a(k/p-1)) \u03c3_k+1[ A; B ] + e\u221a(k+p)/p( \u2211_j>k^\u03c3_j^2[ A; B ])^1/2], \n     max{\ud835\udd3c\u2016 A-   C_AM_AR_A \u2016_F, \ud835\udd3c\u2016 B-C_BM_BR_B \u2016_F }\u2264\u221a(1+(n-k-p)4^k+p-1)\n       \u00d7( 1+\u221a(k/p-1))^1/2( \u2211_j>k^\u03c3_j^2[ A; B ])^1/2.\n\n\n\n\tBy virtue of  Lemma 3.2 in <cit.>, we have \n\t\n    \u2016 I_n-P_\u03a0_C,N_1\u2016\u2264\u221a(1+(n-k-p)4^k+p-1).\n\n\tThus, combining the fact \n\t\n    max{\u2016 A(I_n-X^\u2020X) \u2016, \u2016 B(I_n-X^\u2020X) \u2016}\u2264\u2016[ A; B ](I_n-X^\u2020X) \u2016\n\n\twith Theorem <ref> and Lemma <ref> \n\t\n\timplies the desired results. \n\n\n\n\tThe probability error bounds for Algorithm <ref> can be similarly obtained by using <cit.>. For brevity, we omit them.\n\n\n\n\n \u00a7.\u00a7 Pass-efficient randomized algorithm\n\nConsidering that \nthe matrix pair (A,B) may be very expensive to visit \nin practise, \nwe now investigate the pass-efficient randomized algorithm for CUR decomposition. The idea is to introduce an extra random matrix to find the row index sets I_A and I_B. Specifically, let \u03a9_1\u2208\u211d^l \u00d7 n be a Gaussian matrix and set Y_1=A\u03a9_1^T and Y_2=B\u03a9_1^T. Then, by the CPQRs of Y_1^T and Y_2^T, we have \n\n    Y_1^T\u03a0_1=Y_1^T [ \u03a0_1r, \u03a0_1r^c ]=\n    \tQ_1 [ T_11, T_12 ], \n    \n    Y_2^T\u03a0_2=Y_2^T [ \u03a0_2r,\n     \u03a0_2r^c ]=\n    Q_2 [ T_21, T_22 ],\n\nwhere \u03a0_1r=I_m(:,I_A) and  \u03a0_2r=I_d(:,I_B). The specific  algorithm is summarized in Algorithm <ref>.   \n\n\n\nIn Algorithm <ref>, two passes through (A,B) are required. One is used to obtain the sketches X, Y_1 and Y_2, and further to find the index sets I_A, I_B and J. The other is used for\nretrieving the factors of CUR decomposition. So, when only the column and row index sets are required, Algorithm <ref> only needs one pass and hence can be adapted to the streaming setting.  In contrast, implementing Algorithm <ref> needs to access (A,B) at least four times. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that Y_1^T\u03a0_1r = Q_1T_11 and Y_2^T\u03a0_2r = Q_2T_21 are invertible. Then, we can define two oblique projectors: \nP_Y_1,N_4=Y_1(\u03a0_1r^TY_1)^-1\u03a0_1r^T and P_Y_2,N_5=Y_2(\u03a0_2r^TY_2)^-1\u03a0_2r^T.\n\nWith them, we have the following results. \n\n \n\tLet C_A\u2208\u211d^m \u00d7 l, C_B\u2208\u211d^d \u00d7 l, R_A\u2208\u211d^l \u00d7 n, R_B\u2208\u211d^l \u00d7 n, M_A\u2208\u211d^l \u00d7 l, and M_B\u2208\u211d^l \u00d7 l be obtained by Algorithm <ref>. Then \n\t\n    \u2016 A-C_AM_AR_A \u2016   \u2264 2\u2016 I_n-P_\u03a0_C,N_1\u2016\u2016 A(I_n-X^\u2020X) \u2016+ \u2016 I_m-P_Y_1,N_4\u2016\u2016 (I_m-Y_1Y_1^\u2020)A \u2016,  \n    \u2016 B-C_BM_BR_B \u2016   \u2264 2\u2016 I_n-P_\u03a0_C,N_1\u2016\u2016 B(I_n-X^\u2020X) \u2016+ \u2016 I_d-P_Y_2,N_5\u2016\u2016 (I_d-Y_2Y_2^\u2020)B \u2016.\n\n\n\n\tWe just consider the error bound (<ref>) for A. \n As the proof in <cit.>, we have\n\t\n    \u2016 A-C_AM_AR_A \u2016\u2264\u2016 A-C_AV^T \u2016 + \u2016 A-W_AR_A \u2016.\n\nSince the bound of the first term in the right-hand side of the above inequality has been deduced in Theorem <ref>, we next study the bound of the second term. \n\nOn one hand, we have \n\n    \u2016 A-W_AR_A \u2016    = \u2016 A-C_AC_A^\u2020AR_A^\u2020R_A \u2016\n        = \u2016 A-C_AC_A^\u2020A+C_AC_A^\u2020A-C_AC_A^\u2020AR_A^\u2020R_A \u2016\n       \u2264\u2016 A-C_AC_A^\u2020A \u2016+\u2016 C_AC_A^\u2020\u2016\u00b7\u2016 A-AR_A^\u2020R_A \u2016\n        =\u2016 A-C_AC_A^\u2020A \u2016+\u2016 A-AR_A^\u2020R_A \u2016,\n\nwhere the last equality holds because C_AC_A^\u2020 is an orthogonal projector. On the other hand, considering that R_A=\u03a0_1r^TA is full row rank, using the property of Moore-Penrose inverse, we obtain\n\n    AR_A^\u2020R_A=AR_A^T(R_AR_A^T)^-1\u03a0_1r^TA:=P_AR_A^T,N_6A,\n\nwhere \n\nP_AR_A^T,N_6=AR_A^T(R_AR_A^T)^-1\u03a0_1r^T.\nIt is easy to find that P_Y_1,N_4P_AR_A^T,N_6=P_Y_1,N_4.\n\n\t\n\nHence, \n\n    \u2016 A-AR_A^\u2020R_A \u2016   = \u2016  A-P_AR_A^T,N_6A \u2016 =\u2016  (I_m-P_Y_1,N_4)(I_m-P_AR_A^T,N_6)A \u2016\n       =\u2016  (I_m-P_Y_1,N_4)A(I_n-R_A^\u2020R_A) \u2016\n       \u2264\u2016  (I_m-P_Y_1,N_4)A\u2016\u2016 I_n -R_A^\u2020R_A \u2016  = \u2016  (I_m-P_Y_1,N_4)A\u2016\n       \u2264\u2016 I_m-P_Y_1,N_4\u2016\u2016 (I_m-Y_1Y_1^\u2020)A \u2016,\n\nwhere the last equality holds because I_n-R_A^\u2020R_A is an orthogonal projector and the last inequality relies on the following result\n\n\n    P_Y_1,N_4Y_1-Y_1=0.\n\nThus, (<ref>) can be obtained by combining  (<ref>) and (<ref>) with (<ref>). \n\n\nNow, we present the expectation error bounds for Algorithm <ref>.   For brevity, we only consider the case for spectral norm. \n \n\tWith the same setting as Theorem <ref>, let k be the target rank and p be the oversampling parameter, and let \n\t\n    \u03b1 = \u221a(1+(n-k-p)4^k+p-1)[ ( 1+\u221a(k/p-1)) \u03c3_k+1[ A; B ] + e\u221a(k+p)/p( \u2211_j>k^\u03c3_j^2[ A; B ])^1/2].\n\n\tThen \n\t\n    \ud835\udd3c\u2016 A-C_AM_AR_A \u2016   \u2264 2\u03b1 + \u221a(1+(m-k-p)4^k+p-1)\n       \u00d7[ ( 1+\u221a(k/p-1)) \u03c3_k+1(A) + e\u221a(k+p)/p( \u2211_j>k^\u03c3_j^2(A) )^1/2], \n    \ud835\udd3c\u2016 B-C_BM_BR_B \u2016   \u2264 2\u03b1 + \u221a(1+(d-k-p)4^k+p-1)\n       \u00d7[ ( 1+\u221a(k/p-1)) \u03c3_k+1(B) + e\u221a(k+p)/p( \u2211_j>k^\u03c3_j^2(B) )^1/2].\n\n\n\t\n\tBy virtue of  Lemma 3.2 in <cit.>, we have \n\t\n    \u2016 I_n-P_\u03a0_C,N_1\u2016\u2264\u221a(1+(n-k-p)4^k+p-1), \u2016 I_m-P_Y_1,N_4\u2016\u2264\u221a(1+(m-k-p)4^k+p-1), \n       \u2016 I_d-P_Y_2,N_5\u2016\u2264\u221a(1+(d-k-p)4^k+p-1).\n\n\tThus, combining the fact \n\t\n    max{\u2016 A(I_n-X^\u2020X) \u2016, \u2016 B(I_n-X^\u2020X) \u2016}\u2264\u2016[ A; B ](I_n-X^\u2020X) \u2016\n\n\twith Theorem <ref> and Lemma <ref>, the proof is completed.\n\n\n\n\n\n \u00a7.\u00a7 GSVD-based error analyses\n \nThe error bounds in Theorems <ref> and <ref> are controlled partly by the  singular values of [ A^T B^T ]^T. \nIn this subsection, we provide the alternatives error analyses by the GSVD of the matrix pair (A,B) in (<ref>).  \n\n \n\n\nLet the QR decomposition of Y^T be Y^T=QR. \nIt is clear that R\u2208\u211d^n\u00d7 n is nonsingular since Y^T\u2208\u211d^n\u00d7 n is nonsingular. Thus, from the proof of Theorem <ref> and the fact (I_m-C_AC_A^\u2020 )A=A-C_AM_AR_A, the following inequality holds  \n\n    \u2016 A-C_AM_AR_A  \u2016   \u2264\u2016 A(I_n-X^\u2020 X) (I_n-P_\u03a0_C,N_1)\u2016\n       = \u2016 A(I_n-X^\u2020 X)R^-1 R(I_n-P_\u03a0_C,N_1)\u2016\n       \u2264\u2016 A(I_n-X^\u2020 X)R^-1\u2016\u2016 R(I_n-P_\u03a0_C,N_1)\u2016.\n\nSimilarly, we also have \u2016 B-C_BM_BR_B  \u2016\u2264\u2016 B(I_n-X^\u2020 X)R^-1\u2016\u2016 R(I_n-P_\u03a0_C,N_1)\u2016. In the following, we bound \u2016 A(I_n-X^\u2020 X)R^-1\u2016 and \u2016 B(I_n-X^\u2020 X)R^-1\u2016. \n\n\n\t\n\n\nDenote\n\n    \u00c2=AR^-1, B\u0302=BR^-1, and X\u0302=XR^-1,\n\nand suppose that \nrange(R^-1(R^-1)^TX^T)\u2282 range(X^T). \nHence, combining with the equation\n\n    [ A; B ]=[ U  ;   \u1e7c ][ \u03a3_1; \u03a3_2 ]Y^T,\n\nwe have \n\n    X\u0302=\u03a9[ A; B ]R^-1=\u03a9[  \u00c2; B\u0302 ].\n\nTo continue the GSVD based error analyses, the following lemma is necessary. \n <cit.> \n    Given matrices D_1\u2208\u211d^t_1\u00d7 t_2 and D_2\u2208\u211d^t_2\u00d7 t_3, then \n    \n    (D_1D_2)^\u2020=D_2^\u2020D_1^\u2020\n if and only if \n    \n    range(D_1^TD_1D_2)\u2282 range(D_2),   range(D_2D_2^TD_1^T)\u2282 range(D_1^T).\n\n\n\nBy means of Lemma <ref>, and considering the assumption range(R^-1(R^-1)^TX^T)\u2282 range(X^T) and the fact range(X^TXR^-1)\u2282 range(R^T), we have (XR^-1)^\u2020=RX^\u2020. \n \n \n \n \nHence, \n\n    \u2016\u00c2  (I_n-X\u0302^\u2020X\u0302) \u2016   =\u2016 AR^-1 (I_n-(XR^-1)^\u2020(XR^-1)) \u2016=\u2016 AR^-1 (I_n-RX^\u2020XR^-1) \u2016\n       =\u2016 A(R^-1-X^\u2020XR^-1) \u2016 = \u2016 A(I_n-X^\u2020X)R^-1\u2016.\n\n\nFor brevity, here we just discuss the case of A and that of B can be derived similarly. From (<ref>), it suffices to bound \u2016\u00c2  (I_n-X\u0302^\u2020X\u0302) \u2016. \nIn the following, we tackle the problem using the idea \nfrom \n<cit.>. \n\nObserve that the matrix X\u0302 can be expressed as follows by partitioning  U= [ U_1 , U_2 ], \u1e7c=[ V_1 , V_2 ], \n \n    \u03a3_1=[ \u03a3_11     ;      \u03a3_12 ],  and \u03a3_2=[ \u03a3_21     ;      \u03a3_22 ]\n\n    with \u03a3_11\u2208\u211d^k\u00d7 k, \u03a3_12\u2208\u211d^(m-k)\u00d7 (n-k), \u03a3_21\u2208\u211d^k\u00d7 k and \u03a3_22\u2208\u211d^(d-k)\u00d7 (n-k): \n\n    X\u0302   =\u03a9[ U_1 U_2        ;         V_1 V_2 ][ \u03a3_11     ;      \u03a3_12; \u03a3_21     ;      \u03a3_22 ]Q  =\u03a9[ \u00db_\u03021\u0302, \u00db_\u03022\u0302, V\u0302_\u03021\u0302, V\u0302_\u03022\u0302 ][ \u03a3_11     ;      \u03a3_12; \u03a3_21     ;      \u03a3_22 ]Q \n       =\u03a9[ \u00db_\u03021\u0302\u03a3_11+V\u0302_\u03021\u0302\u03a3_21, \u00db_\u03022\u0302\u03a3_12+V\u0302_\u03022\u0302\u03a3_22 ]Q ,\n \nwhere \n\n    \u00db_\u03021\u0302=[ U_1;   0 ], \t\u00db_\u03022\u0302=[ U_2;   0 ], V\u0302_\u03021\u0302=[   0; V_1 ],  and V\u0302_\u03022\u0302=[   0; V_2 ].\n\nConsidering that the case we focus on is that both A and B are low rank, \u03a3_11 and \u03a3_22 may be nearly zero. \n\nThen \n\n    X\u0302\u2248\u03a9[ V\u0302_\u03021\u0302\u03a3_21, \u00db_\u03022\u0302\u03a3_12 ]Q=[ \u03a9V\u0302_\u03021\u0302\u03a3_21, \u03a9\u00db_\u03022\u0302\u03a3_12 ]Q=[ \u0393_1\u03a3_21, \u0393_2\u03a3_12 ]Q,\n\nwhere \u0393_1=\u03a9V\u0302_\u03021\u0302\u2208\u211d^(k+p)\u00d7 k and \u0393_2=\u03a9\u00db_\u03022\u0302. \n\nFurther, X\u0302Q^T\u2248[ \u0393_1\u03a3_21, \u0393_2\u03a3_12 ]. Let X\u0303=X\u0302Q^T, note that \u03a3_21 is invertible, and suppose that \u0393_1 is full column rank. \nThen\n\n    \u1e90:=\u03a3_21^-1\u0393_1^\u2020X\u0303\u2248[ I , F\u0302 ],\n\nwhere F\u0302=\u03a3_21^-1\u0393_1^\u2020\u0393_2\u03a3_12. It is obvious that range(\u1e90^T)\u2282 range(X\u0303^T) and \u1e90 is full row rank. \n\nTo continue to bound \u2016\u00c2  (I_n-X\u0302^\u2020X\u0302) \u2016, \nsome notations are necessary. Let\n\n    \u00c3=U\u03a3_1=\u00c2Q^T, B\u0303=\u1e7c\u03a3_2=B\u0302Q^T, P_\u1e90=\u1e90^T(\u1e90\u1e90^T)^-1\u1e90, \n       P_X\u0303=X\u0303^T(X\u0303X\u0303^T)^\u2020X\u0303=QX\u0302^T(X\u0302X\u0302^T)^\u2020X\u0302Q^T=QX\u0302^\u2020X\u0302Q^T,\n\nwhere the last equality holds since (X\u0302X\u0302^T)^\u2020=(X\u0302^T)^\u2020X\u0302^\u2020. \nNote that P_\u1e90 and P_X\u0303 are orthogonal projectors. Thus,\ncombining range(\u1e90^T)\u2282 range(X\u0303^T) with Proposition 8.5 in <cit.>, we have\n\n    \u2016\u00c2  (I_n-X\u0302^\u2020X\u0302) \u2016 =\n    \t\u2016\u00c3 Q (I_n-X\u0302^\u2020X\u0302)Q^T \u2016=\u2016\u00c3(I_n-P_X\u0303) \u2016\u2264\u2016\u00c3(I_n-P_\u1e90) \u2016.\n\nFurther, we can obtain\n\n    \u2016\u00c2  (I_n-X\u0302^\u2020X\u0302) \u2016^2    \u2264\u2016\u00c3(I_n-P_\u1e90) \u2016^2 =\u2016 U\u03a3_1 (I_n-P_\u1e90)\u03a3_1^TU^T \u2016\n       =\u2016\u03a3_1 (I_n-P_\u1e90)\u03a3_1^T \u2016,\n\nwhere the first equality holds since I_n-P_\u1e90 is an orthogonal projector. As done in the proof of <cit.>, by the similar algebraic computation, the following holds,  \n\n\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\t\n\t\n\t\n\t\t\n\t\t\n\t\n\n\n\n\t\n\t\t\n\t\t\n\t\n\n\n\n\n\n\n\n\n\n    \u03a3_1(I_n-P_\u1e90)\u03a3_1^T \u227c[                 \u03a3_11F\u0302F\u0302^T\u03a3_11^T   -\u03a3_11( I_k+F\u0302F\u0302^T )^-1F\u0302\u03a3_12^T; -\u03a3_12F\u0302^T( I_k+F\u0302F\u0302^T )^-1\u03a3_11^T                       \u03a3_12\u03a3_12^T ].\n\nNote that the matrices on the two sides of the above inequality are positive semi-definite. Hence,\nProposition 8.3 in <cit.> results in \n\n    \u2016\u03a3_1(I_n-P_\u1e90)\u03a3_1^T \u2016\u2264\u2016\u03a3_11F\u0302F\u0302^T\u03a3_11^T  \u2016 + \u2016\u03a3_12\u03a3_12^T \u2016 =\n    \t\u2016\u03a3_11F\u0302\u2016^2+\u2016\u03a3_12\u2016^2.\n\nThus, combining (<ref>) with the above inequality implies \n\n    \u2016\u00c2(I_n-X\u0302^\u2020X\u0302) \u2016^2 \u2264\u2016\u03a3_11F\u0302\u2016^2+\u2016\u03a3_12\u2016^2.\n\nWith the fact that for all a,b\u2265 0, c^2\u2264 a^2+b^2 results in c\u2264 a+b, we have \n\n    \u2016\u00c2(I_n-X\u0302^\u2020X\u0302) \u2016\u2264\u2016\u03a3_11F\u0302\u2016 +\u2016\u03a3_12\u2016\u2264\u2016\u03a3_11\u03a3_21^-1\u2016\u2016\u0393_1^\u2020\u0393_2\u03a3_12\u2016 +\u2016\u03a3_12\u2016 ,\n\nwhere F\u0302=\u03a3_21^-1\u0393_1^\u2020\u0393_2\u03a3_12. \n\nSimilarly, for the case of B, we have\n\n    \u2016B\u0302  (I_n-X\u0302^\u2020X\u0302) \u2016^2    \u2264\u2016\u03a3_2 (I_n-P_\u1e90)\u03a3_2^T \u2016,\n    \u03a3_2 (I_n-P_\u1e90)\u03a3_2^T    \u227c[                 \u03a3_21F\u0302F\u0302^T\u03a3_21^T   -\u03a3_21( I_k+F\u0302F\u0302^T )^-1F\u0302\u03a3_22^T; -\u03a3_22F\u0302^T( I_k+F\u0302F\u0302^T )^-1\u03a3_21^T                       \u03a3_22\u03a3_22^T ],\n    \u2016B\u0302  (I_n-X\u0302^\u2020X\u0302) \u2016^2    \u2264\u2016\u03a3_21F\u0302\u2016^2 + \u2016\u03a3_22\u2016^2 \u2264\u2016\u03a3_21\u03a3_21^-1\u0393_1^\u2020\u0393_2\u03a3_12\u2016^2 + \u2016\u03a3_22\u2016^2 \n        = \u2016\u0393_1^\u2020\u0393_2\u03a3_12\u2016^2 + \u2016\u03a3_22\u2016^2,\n\nand \n\n    \u2016B\u0302  (I_n-X\u0302^\u2020X\u0302) \u2016\u2264\u2016\u0393_1^\u2020\u0393_2\u03a3_12\u2016 + \u2016\u03a3_22\u2016.\n\n\nTherefore,\n\n    \u2016 A(I_n-X^\u2020 X)R^-1\u2016   \u2264\u2016\u03a3_11\u03a3_21^-1\u2016\u2016\u0393_1^\u2020\u0393_2\u03a3_12\u2016 +\u2016\u03a3_12\u2016, \n    \u2016 B(I_n-X^\u2020 X)R^-1\u2016   \u2264\u2016\u0393_1^\u2020\u0393_2\u03a3_12\u2016 + \u2016\u03a3_22\u2016.\n\nFurthermore, it is easy to see that (<ref>) and (<ref>) also hold for Frobenius norm.\n\nNote that, in the above discussions, two necessary assumptions are used. We list them  uniformly as follows. \n \n\t(1) range(R^-1(R^-1)^TX^T)\u2282 range(X^T); \n\t\n\t(2) \u0393_1=\u03a9V\u0302_1 \n \n is full column rank.\n\n\n    The last assumption is satisfied in high probability when \u03a9 is a Gaussian matrix; see e.g., <cit.>.\n\n\nIn the following, we present the expectation error bounds of Algorithm <ref>. Two necessary lemmas are listed firstly as follows. \n<cit.> \n\tFor the given matrices S and N, draw a standard Gaussian matrix \u0393. Then\n\t\n    (\ud835\udd3c\u2016 S\u0393 N \u2016_F^2)^1/2=\u2016 S\u2016_F\u2016 N\u2016_F, \ud835\udd3c\u2016 S\u0393 N \u2016\u2264\u2016 S \u2016\u2016 N \u2016_F+\u2016 S \u2016_F\u2016 N \u2016.\n\n\n\n<cit.> \n\tLet \u0393\u2208\u211d^(k+p)\u00d7 k be a standard Gaussian matrix. Then \n\t\n    (\ud835\udd3c\u2016\u0393^\u2020\u2016_F^2)^1/2=\u221a(k/p-1), \ud835\udd3c\u2016\u0393^\u2020\u2016\u2264 e\u221a(k+p)/p.\n\n\n\n \nWith the same setting as Theorem <ref> and Assumption <ref>,\n\t\n let \n \n    \u03b7 =\u2016 Y \u2016\u221a(1+(n-k-p)4^k+p-1)\n\n with Y being defined in (<ref>).\n Then \n\t\n    \ud835\udd3c\u2016 A-C_AM_AR_A \u2016\u2264\u03b7[ \u2016\u03a3_11\u03a3_21^-1\u2016(  e\u221a(k+p)/p\u2016\u03a3_12\u2016_F + \u221a(k/p-1)\u2016\u03a3_12\u2016) + \u2016\u03a3_12\u2016], \n       \ud835\udd3c\u2016 B-C_BM_BR_B \u2016\u2264\u03b7[ (  e\u221a(k+p)/p\u2016\u03a3_12\u2016_F + \u221a(k/p-1)\u2016\u03a3_12\u2016)+ \u2016\u03a3_22\u2016],\n       \ud835\udd3c\u2016 A-C_AM_AR_A \u2016_F \u2264\u03b7[ \u221a(1+k/p-1\u2016\u03a3_11\u03a3_21^-1\u2016)\u2016\u03a3_12\u2016_F  ], \n       \ud835\udd3c\u2016 B-C_BM_BR_B \u2016_F \u2264\u03b7[ k/p-1\u2016\u03a3_12\u2016_F^2+\u2016\u03a3_22\u2016_F^2 ]^1/2,\n\n where \u03a3_11, \u03a3_12, \u03a3_21, and \u03a3_22 are defined as in  (<ref>).\n\n\n\n\tNote that Y^T=QR, then \u2016 Y^T \u2016=\u2016 Y \u2016=\u2016 R \u2016. Thus, noting  (<ref>), \n we have \n\t\n    \u2016 A-C_AM_AR_A \u2016\u2264\u2016 Y \u2016\u2016 I_n-P_\u03a0_C,N_1\u2016\u2016 A(I_n-X^\u2020 X)R^-1\u2016, \n       \u2016 A-C_AM_AR_A \u2016_F \u2264\u2016 Y \u2016\u2016 I_n-P_\u03a0_C,N_1\u2016\u2016 A(I_n-X^\u2020 X)R^-1\u2016_F.\n \n\tSimilarly, for the case of B, \n\t\n    \u2016 B-C_BM_BR_B \u2016\u2264\u2016 Y \u2016\u2016 I_n-P_\u03a0_C,N_1\u2016\u2016 B(I_n-X^\u2020 X)R^-1\u2016, \n       \u2016 B-C_BM_BR_B \u2016_F \u2264\u2016 Y \u2016\u2016 I_n-P_\u03a0_C,N_1\u2016\u2016 B(I_n-X^\u2020 X)R^-1\u2016_F.\n\n\tBy virtue of Lemma 3.2 in <cit.>, we get\n\t\n    \u2016 I_n-P_\u03a0_C,N_1\u2016\u2264\u221a(1+(n-k-p)4^k+p-1).\n\n\tNext, it suffices to bound the expectations of \u2016 A(I-X^\u2020 X)R^-1\u2016, \u2016 B(I-X^\u2020 X)R^-1\u2016, \u2016 A(I-X^\u2020 X)R^-1\u2016_F, and \u2016 B(I-X^\u2020 X)R^-1\u2016_F. \n\tObserving (<ref>) and (<ref>), we need to take expectation on \u2016\u0393_1^\u2020\u0393_2\u03a3_12\u2016 and \u2016\u0393_1^\u2020\u0393_2\u03a3_12\u2016_F^2. On one hand,\n\t\n    \ud835\udd3c\u2016\u0393_1^\u2020\u0393_2\u03a3_12\u2016   = \ud835\udd3c( \ud835\udd3c[\u2016\u0393_1^\u2020\u0393_2\u03a3_12\u2016|\u0393_1 ] ) \u2264\ud835\udd3c( \u2016\u0393_1^\u2020\u2016\u2016\u03a3_12\u2016_F+\u2016\u0393_1^\u2020\u2016_F \u2016\u03a3_12\u2016) \n       \u2264\u2016\u03a3_12\u2016_F \ud835\udd3c\u2016\u0393_1^\u2020\u2016 +\u2016\u03a3_12\u2016(\ud835\udd3c\u2016\u0393_1^\u2020\u2016_F^2 )^1/2\n       \u2264 e\u221a(k+p)/p\u2016\u03a3_12\u2016_F + \u221a(k/p-1)\u2016\u03a3_12\u2016,\n\n\twhere the first equality relies on the fact that \u0393_1\u2208\u211d^(k+p) \u00d7 k and \u0393_2 are independent, the first inequality relies on Lemma <ref>, the second inequality relies on H\u00f6lder\u2019s inequality, and the third inequality relies on Lemma <ref>. On the other hand,\n\t\n    \ud835\udd3c\u2016\u0393_1^\u2020\u0393_2\u03a3_12\u2016_F^2    = \ud835\udd3c( \ud835\udd3c[\u2016\u0393_1^\u2020\u0393_2\u03a3_12\u2016_F^2 |\u0393_1 ] ) =\ud835\udd3c( \u2016\u0393_1^\u2020\u2016_F^2 \u2016\u03a3_12\u2016_F^2 ) \n       = \u2016\u03a3_12\u2016_F^2 \ud835\udd3c\u2016\u0393_1^\u2020\u2016_F^2 =k/p-1\u2016\u03a3_12\u2016_F^2,\n\n\twhere the second equality relies on Lemma <ref> and the last equality relies on Lemma <ref>.\n\tThus, taking the expectation of both sides of (<ref>) and (<ref>) implies \n\t\n    \ud835\udd3c\u2016 A(I_n-X^\u2020 X)R^-1\u2016\u2264\u2016\u03a3_11\u03a3_21^-1\u2016\u00b7\ud835\udd3c\u2016\u0393_1^\u2020\u0393_2\u03a3_12\u2016 + \u2016\u03a3_12\u2016, \n       \ud835\udd3c\u2016 B(I_n-X^\u2020 X)R^-1\u2016\u2264\ud835\udd3c\u2016\u0393_1^\u2020\u0393_2\u03a3_12\u2016 + \u2016\u03a3_22\u2016.\n\n\tSimilarly, take the expectation of both sides of (<ref>) and (<ref>) leading to\n\t\n    \ud835\udd3c\u2016 A(I_n-X^\u2020 X)R^-1\u2016_F    \u2264( \ud835\udd3c\u2016 A(I_n-X^\u2020 X)R^-1\u2016_F^2 )^1/2\n       \u2264( \u2016\u03a3_11\u03a3_21^-1\u2016^2 \u00b7\ud835\udd3c\u2016\u0393_1^\u2020\u0393_2\u03a3_12\u2016_F^2 +\u2016\u03a3_12\u2016_F^2 )^1/2 , \n    \ud835\udd3c\u2016 B(I_n-X^\u2020 X)R^-1\u2016_F    \u2264( \ud835\udd3c\u2016 B(I_n-X^\u2020 X)R^-1\u2016_F^2 )^1/2\n       \u2264( \ud835\udd3c\u2016\u0393_1^\u2020\u0393_2\u03a3_12\u2016_F^2 +\u2016\u03a3_22\u2016_F^2 )^1/2.\n\n\tFinally, taking the expectation of both sides of (<ref>), (<ref>), (<ref>), and (<ref>), and then combining them with (<ref>), (<ref>), (<ref>), (<ref>), (<ref>), (<ref>), and (<ref>)  complete the proof.\n\n\nSimilarly, combining Theorem <ref> with Lemma <ref>, we can obtain the error analysis for \nAlgorithm <ref>.\n\n\tWith the same setting  as Theorems <ref> and <ref>, we have  \n\t\n    \ud835\udd3c\u2016 A-C_AM_AR_A \u2016\u2264 2\u03b7[ \u2016\u03a3_11\u03a3_21^-1\u2016(  e\u221a(k+p)/p\u2016\u03a3_12\u2016_F + \u221a(k/p-1)\u2016\u03a3_12\u2016) + \u2016\u03a3_12\u2016]  \n       +\u221a(1+(m-k-p)4^k+p-1)[ ( 1+\u221a(k/p-1)) \u03c3_k+1(A) + e\u221a(k+p)/p( \u2211_j>k^\u03c3_j^2(A) )^1/2], \n       \ud835\udd3c\u2016 B-C_BM_BR_B \u2016\u2264 2\u03b7[ (  e\u221a(k+p)/p\u2016\u03a3_12\u2016_F + \u221a(k/p-1)\u2016\u03a3_12\u2016)+ \u2016\u03a3_22\u2016] + \n       \u221a(1+(d-k-p)4^k+p-1)[ ( 1+\u221a(k/p-1)) \u03c3_k+1(B) + e\u221a(k+p)/p( \u2211_j>k^\u03c3_j^2(B) )^1/2].\n\n\n\n\n\n\n\n\u00a7 RANDOMIZED ALGORITHMS FOR CUR DECOMPOSITION OF MATRIX TRIPLET\n\n\n\nWe first consider a randomized algorithm based on randomized CPQR. \nLet A\u2208\u211d^m \u00d7 n, B\u2208\u211d^m \u00d7 t, G\u2208\u211d^d \u00d7 n, rank(A) > k,  rank(B) > k, and rank(G) > k, \nand let \u03a9_2 \u2208\u211d^l \u00d7 (m+d) and \u03a9_3 \u2208\u211d^l \u00d7 (n+t) be Gaussian matrices. Define \n\n    X_1=\u03a9_2[ A; G ], Y_3=[ A, B ]\u03a9_3^T.\n\nBy virtue of the CPQRs of X_1 and Y_3^T, we have\n \n    l\u00d7 nX_1n\u00d7 n\u03a0_3=X_1[ n\u00d7 l\u03a0_31,  n\u00d7 (n-l)\u03a0_31^c ]=l\u00d7 lQ_3[ l\u00d7 lT_31, l\u00d7 (n-l)T_32 ], \n    l\u00d7 mY_3^Tm\u00d7 m\u03a0_4=Y_3^T[ m\u00d7 l\u03a0_41,  m\u00d7 (m-l)\u03a0_41^c ]=l\u00d7 lQ_4[ l\u00d7 lT_41, l\u00d7 (m-l)T_42 ],\n\nwhere \u03a0_3 and \u03a0_4 are permutation matrices, Q_3 and Q_4 are orthogonal, and T_31 and T_41 are invertible and upper triangular. With this step, we can select the columns of A and G, and the rows of A and B. So, it suffices to consider how to select the rows of G and the columns of B. To this end, \nsimilar to the discussion on the row IDs of C_A and C_B in Section <ref>, we perform the exact IDs for the columns of G and the rows of B to obtain them. \nThe specific algorithm \nis concluded in Algorithm <ref>.\n\n\n\nNoting that X_1\u03a0_31=Q_3T_31 and Y_3^T\u03a0_41=Q_4T_41 are invertible, we can define two oblique projectors: \n\nP_\u03a0_31,N_7=\u03a0_31(X_1\u03a0_31)^-1X_1 and P_Y_3,N_8=Y_3(\u03a0_41^TY_3)^-1\u03a0_41^T.\nUsing them, the following results hold.\n \n\tLet C_A\u2208\u211d^m \u00d7 l, C_B\u2208\u211d^m \u00d7 l, C_G\u2208\u211d^d \u00d7 l, R_A\u2208\u211d^l \u00d7 n, R_B\u2208\u211d^l \u00d7 t, R_G\u2208\u211d^l \u00d7 n, M_A\u2208\u211d^l \u00d7 l, M_B\u2208\u211d^l \u00d7 l and M_G\u2208\u211d^l \u00d7 l be obtained by Algorithm <ref>. Then\n \n\t\n    \u2016 A-C_AM_AR_A \u2016   \u2264\u2016 I_n-P_\u03a0_31,N_7\u2016\u2016 A(I_n-X_1^\u2020X_1) \u2016\n       + \u2016 I_m-P_Y_3,N_8\u2016\u2016 (I_m-Y_3Y_3^\u2020)A \u2016, \n    \u2016 B-C_BM_BR_B \u2016   \u2264\u2016 I_m-P_Y_3,N_8\u2016\u2016 (I_m-Y_3Y_3^\u2020)B \u2016, \n    \u2016 G-C_GM_GR_G \u2016   \u2264\u2016 I_n-P_\u03a0_31,N_7\u2016\u2016 G(I_n-X_1^\u2020X_1) \u2016.\n\n\n\n\tThe proof is similar to those of Theorems <ref> and  <ref>. We first consider the case for the matrix A. \nSince X_1P_\u03a0_31,N_7=X_1 and P_Y_3,N_8Y_3=Y_3, we have \n\n    \u2016 A-C_AC_A^\u2020A \u2016   \u2264\u2016 I_n-P_\u03a0_31,N_7\u2016\u2016 A(I_n-X_1^\u2020X_1) \u2016,\n    \u2016 A-AR_A^\u2020R_A \u2016   \u2264\u2016 I_m-P_Y_3,N_8\u2016\u2016 (I_m-Y_3Y_3^\u2020)A \u2016.\n\n Considering the fact that C_AC_A^\u2020 is an orthogonal projector, we get\n\n    \u2016 A-C_AM_AR_A \u2016   = \u2016 A-C_AC_A^\u2020AR_A^\u2020R_A \u2016\n       = \u2016 (I_m-C_AC_A^\u2020)A+C_AC_A^\u2020A(I_n-R_A^\u2020R_A) \u2016\n       \u2264\u2016 (I_m-C_AC_A^\u2020)A \u2016+ \u2016 C_AC_A^\u2020\u2016\u2016 A(I_n-R_A^\u2020R_A) \u2016\n       = \u2016 (I_m-C_AC_A^\u2020)A \u2016+ \u2016 A(I_n-R_A^\u2020R_A) \u2016.\n\nThus, (<ref>) can be obtained. Similarly, for the cases of the matrices B and G, the following results hold, \n\n    \u2016 G-C_GC_G^\u2020G \u2016   \u2264\u2016 I_n-P_\u03a0_31,N_7\u2016\u2016 G(I_n-X_1^\u2020X_1) \u2016,\n    \u2016 B-BR_B^\u2020R_B \u2016   \u2264\u2016 I_m-P_Y_3,N_8\u2016\u2016 (I_m-Y_3Y_3^\u2020)B \u2016.\n\nNote that the CPQRs of C_G^T and R_B can imply the exact row IDs. Then \n\n    \u2016 B-C_BM_BR_B \u2016 = \u2016 B-BR_B^\u2020R_B \u2016, \u2016 G-C_GM_GR_G \u2016 = \u2016 G-C_GC_G^\u2020G \u2016.\n\nAs a result, (<ref>) and (<ref>) are derived.\n \n\nNow we present the pass-efficient algorithm, i.e., Algorithm <ref>, for the CUR decomposition of matrix triplet, whose deduction is  like that of matrix pair. \n\n\n\nNext, we present the expectation error bounds  for Algorithm <ref>.  For brevity, we only present the results of spectral norm.\n\nWith the same setting as Theorem <ref>, let k be the target rank and p be the oversampling parameter, and let\n\t\n\t\n    \u03b2   = ( 1+\u221a(k/p-1)) \u03c3_k+1[ A; G ] + e\u221a(k+p)/p( \u2211_j>k^\u03c3_j^2[ A; G ])^1/2, \n    \u03b8   = ( 1+\u221a(k/p-1)) \u03c3_k+1[ A , B ] + e\u221a(k+p)/p( \u2211_j>k^\u03c3_j^2[ A , B ])^1/2.\n\n\tThen \n\t\n    \ud835\udd3c\u2016 A-C_AM_AR_A \u2016   \u2264\u221a(1+(n-k-p)4^k+p-1)\u00d7\u03b2\n       + \u221a(1+(m-k-p)4^k+p-1)\u00d7\u03b8, \n    \ud835\udd3c\u2016 B-C_BM_BR_B \u2016   \u2264\u221a(1+(m-k-p)4^k+p-1)\u00d7\u03b8, \n    \ud835\udd3c\u2016 G-C_GM_GR_G \u2016   \u2264\u221a(1+(n-k-p)4^k+p-1)\u00d7\u03b2.\n\n\n\n\tBy virtue of  Lemma 3.2 in <cit.>, we have \n\t\n    \u2016 I_n-P_\u03a0_31,N_7\u2016\u2264\u221a(1+(n-k-p)4^k+p-1), \u2016 I_m-P_Y_3,N_8\u2016\u2264\u221a(1+(m-k-p)4^k+p-1).\n\n\tThus, combining the fact \n\t\n    max{\u2016 A(I_n-X_1^\u2020X_1) \u2016, \u2016 G(I_n-X_1^\u2020X_1) \u2016}\u2264\u2016[ A; G ](I_n-X_1^\u2020X_1) \u2016, \n        max{\u2016 (I_m-Y_3Y_3^\u2020)A \u2016, \u2016 (I_m-Y_3Y_3^\u2020)B\u2016}\u2264\u2016 (I_m-Y_3Y_3^\u2020) [ A, B ]\u2016\n\nwith Theorem <ref> and Lemma <ref> implies the desired results.\n\n\nSimilar to the error analysis of the CUR decomposition for matrix pair, i.e., Theorem <ref>, we now consider \nthe alternative  expectation error bounds of \nAlgorithm <ref>.\n\nLet the GSVDs of (A,G) and (A^T,B^T) be\n\n    [ A; G ]= [ W_1\u03a3_3H_1^T; W_2\u03a3_4H_1^T ], [ A^T; B^T ]= [ W_3\u03a3_5H_2^T; W_4\u03a3_6H_2^T ],\n\nwhere H_1\u2208\u211d^n\u00d7 n and H_2\u2208\u211d^m\u00d7 m are nonsingular, W_1\u2208\u211d^m\u00d7 m, W_2\u2208\u211d^d\u00d7 d, W_3\u2208\u211d^n\u00d7 n, and W_4\u2208\u211d^t\u00d7 t are orthogonal, \u03a3_3\u2208\u211d^m\u00d7 n, \u03a3_4\u2208\u211d^d\u00d7 n, \u03a3_5\u2208\u211d^n\u00d7 m, and \u03a3_6\u2208\u211d^t\u00d7 m are diagonal. \nBy using the similar partition method of matrix pair, i.e., \nW_1=[ W_11 , W_12 ],  W_2=[ W_21 , W_22 ], W_3=[ W_31 , W_32 ], W_4=[ W_41 , W_42 ],  \n\n    \u03a3_3 =[ \u03a3_31     ;      \u03a3_32 ], \u03a3_4 =[ \u03a3_41     ;      \u03a3_42 ], \u03a3_5 =[ \u03a3_51     ;      \u03a3_52 ], and \u03a3_6 =[ \u03a3_61     ;      \u03a3_62 ]\n\nwith \u03a3_31, \u03a3_41, \u03a3_51 and \u03a3_61\u2208\u211d^k\u00d7 k, \nwe have \n\n    \u03a9_2 [ A; G ]   = \u03a9_2 [ \u0174_11\u03a3_31+\u0174_21\u03a3_41, \u0174_12\u03a3_32+\u0174_22\u03a3_42 ]H_1^T \n       \u2248[ \u03a9_2\u0174_21\u03a3_41, \u03a9_2\u0174_12\u03a3_32 ]H_1^T, \n    \u03a9_3 [ A^T; B^T ]   = \u03a9_3 [ \u0174_31\u03a3_51+\u0174_41\u03a3_61, \u0174_32\u03a3_52+\u0174_42\u03a3_62 ]H_2^T \n       \u2248[ \u03a9_3\u0174_41\u03a3_61, \u03a9_3\u0174_32\u03a3_52 ]H_2^T,\n\nwhere \n\n    \u0174_11=[ W_11;    0 ], \u0174_12=[ W_12;    0 ], \u0174_21=[    0; W_21 ], \u0174_22=[    0; W_22 ], \n    \u0174_31=[ W_31;    0 ], \u0174_32=[ W_32;    0 ], \u0174_41=[    0; W_41 ], \u0174_42=[    0; W_42 ].\n\n\n\n\t\n\t\n\t\n\t\t\n\t\n\t\t\n\t\n\t\n\t\t\n\t\n\t\t\n\t\n\n\n\n\t\n\t\t\n\t\n\t\t\n\t\n\t\t\n\t\n\t\t\n\t\t\n\t\n\t\t\n\t\n\t\n\t\t\n\t\n\t\t\n\t\n\nLet the QR decompositions of H_1^T and H_2^T be H_1^T=Q_H_1R_H_1 and H_2^T=Q_H_2R_H_2 respectively.\nThus, under the following \nassumptions: \n\n\t\n\t    \n  1. \n     range(R_H_1^-1(R_H_1^-1)^TX_1^T)\u2282 range(X_1^T)\n     and range(R_H_2^-1(R_H_2^-1)^TY_3)\u2282 range(Y_3);\n \n \n  2. \u03a9_2\u0174_21 and \u03a9_3\u0174_41 are full column rank;\n\n\n\ncombining the GSVD-based error analyses in Section <ref> with Theorem <ref>, we can obtain the following theorem.\n\n\n\tWith the same setting as Theorem <ref> and the above assumptions, let k be the target rank and p be the oversampling parameter,  \n \n and let \n \n    \u03b7_1=\u2016 H_1 \u2016\u221a(1+(n-k-p)4^k+p-1), \u03b7_2=\u2016 H_2 \u2016\u221a(1+(m-k-p)4^k+p-1).\n\n Then\n\t\n\t\n \n\t\n\t\n\t\n\t\n \n    \ud835\udd3c\u2016 A-C_AM_AR_A \u2016   \u2264\u03b7_1 [ \u2016\u03a3_31\u03a3_41^-1\u2016(  e\u221a(k+p)/p\u2016\u03a3_32\u2016_F+\u221a(k/p-1)\u2016\u03a3_32\u2016) +\u2016\u03a3_32\u2016] \n         +\u03b7_2 [ \u2016\u03a3_51\u03a3_61^-1\u2016(  e\u221a(k+p)/p\u2016\u03a3_52\u2016_F+\u221a(k/p-1)\u2016\u03a3_52\u2016) +\u2016\u03a3_52\u2016] , \n    \ud835\udd3c\u2016 B-C_BM_BR_B \u2016   \u2264\u03b7_2 [ (  e\u221a(k+p)/p\u2016\u03a3_52\u2016_F+\u221a(k/p-1)\u2016\u03a3_52\u2016) + \u2016\u03a3_62\u2016], \n    \ud835\udd3c\u2016 G-C_GM_GR_G \u2016   \u2264\u03b7_1 [ (  e\u221a(k+p)/p\u2016\u03a3_32\u2016_F+\u221a(k/p-1)\u2016\u03a3_32\u2016)+ \u2016\u03a3_42\u2016].\n\n \n\n    Similar to Theorem <ref>, Theorem <ref> and Corollary <ref>, we can also obtain two alternative expectation error bounds of Algorithm <ref>.  For brevity, we omit them here.\n\n\n\n\n\n\n\u00a7 NUMERICAL EXPERIMENTS\n \nIn this section, we \ncompare numerically the CUR algorithms of matrix pair and matrix triplet whose abbreviations are summarized in Table <ref>, among which \nLDEIM-PCUR is obtained by combining the L-DEIM index selection (i.e., <cit.>) with DEIM-PCUR (i.e., <cit.>).\n These algorithms \n are performed five times, and the numerical results on relative errors and runtime are presented as their average. For a matrix A, the relative error of its low rank approximation is defined by \u2016 A-C_AM_AR_A \u2016/ \u2016 A \u2016. \n\n\n\n\n\n\n\n\n \u00a7.\u00a7 On the matrix pair\n\n\n\n\n\t\n\n\n\n\n\t\n\n\n \n \t\n \n\n\n\n\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\n\n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\nExperiment 1 \nAs done in <cit.>, we generate the matrix pair (A,B) as follows, \n\n    A=A_1A_2,   B=B_1B_2,\n\nwhere A_1\u2208\u211d^10000\u00d7 100, A_2\u2208\u211d^100\u00d7 5000, B_1\u2208\u211d^8000\u00d7 100 and B_2\u2208\u211d^100\u00d7 5000 are randomly generated by using the MATLAB build-in function randn. Note that rank(A)= rank(B)=100 with probability one. \n\nWe \nimplement the algorithms in Table <ref> on the above matrix pair (A,B), and present the relative errors and runtime in Fig. <ref>. \n\n \n\nFrom the first two figures in Fig. <ref>, it is seen that if the target rank is less than 100,  all the algorithms have almost the same performance in relative errors. When the target rank approaches 100, the relative errors of RPCUR and PE-PCUR are smaller than those of DEIM-PCUR,  LDEIM-PCUR and RLDEIM-PCUR. This mainly because the oversampling parameter p=5 is used in RPCUR and PE-PCUR. Therefore, all the algorithms actually perform almost the same in relative errors.\n\nFrom the last figure in Fig. <ref>,\n\nwe can find that RPCUR needs the least  runtime, followed by PE-PCUR. They are much cheaper than RLDEIM-PCUR, which in turn is cheaper than DEIM-PCUR and LDEIM-PCUR.\n\n\n\n\n\n\n \u00a7.\u00a7 On the matrix triplet\n\n\n\n\n\t\n\t\n\n\n\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\n\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n\n\n\t\n\t\n\t\n\t\n\n\n\n\t\n\t\n\t\n\t\n\n\nExperiment 2 As done in Experiment 1, we construct the matrix triplet (A,B,G) as follows, \n\n    A=A_1A_2,  B=B_1B_2,  G=G_1G_2,\n\nwhere A_1\u2208\u211d^5000\u00d7 100, A_2\u2208\u211d^100\u00d7 5000, B_1\u2208\u211d^5000\u00d7 100, B_2\u2208\u211d^100\u00d7 10000, G_1\u2208\u211d^10000\u00d7 100 and G_2\u2208\u211d^100\u00d7 5000 are generated by the MATLAB function randn. It is clear that rank(A)= rank(B)= rank(G)=100 with probability one. \nThen, the algorithms in Table <ref> on the above matrix triplet (A,B,G) are  performed, and the relative errors and  runtime are reported in Fig. <ref>. \n\n\n\nFrom Fig. <ref>, it follows that the conclusions are similar to those obtained in Experiment 1 except that, in approximating A and B, \nthe relative errors of RLDEIM-TCUR are barely falling when k> 100. \n\n\n\n\n\n\u00a7 CONCLUSIONS\n\n    This paper presents the CPQR-based randomized algorithms for the generalized CUR decompositions of matrix pair and matrix triplet. Their pass-efficient versions are also obtained. \n    For these algorithms, we present two alternative expectation error analyses, among which \n    the second one is mainly inspired by \n    the method in <cit.>. \n    Numerical experiments demonstrate that our algorithms can calculate the generalized CUR decompositions as accurate as the existing methods, but need much less  runtime. \n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n99\n\n\n\n\n\n\n\n\n\n\n\nmahoney2009cur\nMahoney, M.W., Drineas, P.: CUR matrix decompositions for improved\ndata analysis. Proc. Natl. Acad. Sci. USA 106(3),\n697\u2013702 (2009)\n\ngidisu2022generalized\nGidisu, P.Y., Hochstenbach, M.E.: A generalized CUR decomposition for matrix pairs. SIAM J. Math. Data Science 4(1), 386\u2013409 (2022)\n\ngidisu2022rsvd \nGidisu, P.Y., Hochstenbach, M.E.: RSVD-CUR decomposition for matrix\ntriplets. arXiv:2204.02113 (2022)\n\ncai2021robust\nCai, H., Hamm, K., Huang, L., Needell, D.: Robust CUR decomposition:\nTheory and imaging applications. SIAM J. Imaging Sci. 14(4), 1472\u20131503 (2021)\n\nlei2022exemplar\nLei, H., Liu, J., Yu, Y.: Exemplar-based large scale low-rank matrix\ndecomposition for collaborative prediction. Int. J. Comput. Math. 1-26 (2022)\n\nwang2013improving\nWang, S., Zhang, Z.: Improving CUR matrix decomposition and the\nnystr\u00f6m approximation via adaptive sampling. J. Mach. Learn. Res. 14(1), 2729\u20132769 (2013)\n\nsorensen2016deim\nSorensen, D.C., Embree, M.: A DEIM induced CUR factorization. SIAM J. Sci. Comput. 38(3), 1454\u20131482 (2016)\n\nvoronin2017efficient\nVoronin, S., Martinsson, P.-G.: Efficient algorithms for CUR and interpolative matrix decompositions. Adv. Comput. Math. 43(3), 495\u2013516 (2017)\n\nchen2020efficient\nChen, C., Gu, M., Zhang, Z., Zhang, W., Yu, Y.: Efficient spectrum revealing CUR matrix decomposition. In: Proceedings of the 23rd International Conference on\nArtificial Intelligence and Statistics (AISTATS), 108, 766\u2013775 (2020)\n\ndong2021simpler\nDong, Y., Martinsson, P.-G.: Simpler is better: a comparative study of\nrandomized algorithms for computing the CUR decomposition. arXiv:2104.05877 (2021)\n\nboutsidis2014optimal\nBoutsidis, C., Woodruff, D.P.: Optimal CUR matrix decompositions. SIAM J. Comput. 46(2), 543-589 (2017) \n\ndrineas2008relative\nDrineas, P., Mahoney, M.W., Muthukrishnan, S.: Relative-error CUR\nmatrix decompositions. SIAM J. Matrix Anal. Appl. 30(2), 844\u2013881 (2008)\n\nhamm2021perturbations\nHamm, K., Huang, L.: Perturbations of CUR decompositions. SIAM J. Matrix Anal. Appl. 42(1), 351\u2013375 (2021)\n\nchen2022tensor\nChen, J., Wei, Y., Xu, Y.: Tensor CUR decomposition under T-product\nand its perturbation. Numer. Funct. Anal. Optim. 43(6), 698-722 (2022)\n\nche2022perturbations\nChe, M., Chen, J., Wei, Y.: Perturbations of the TCUR decomposition for\ntensor valued data in the tucker format. J. Optim. Theory Appl. 194(3), 852\u2013877 (2022)\n\nboutsidis2014near\nBoutsidis, C., Drineas, P., Magdon-Ismail, M.: Near-optimal column-based matrix reconstruction. SIAM J.  Comput. 43(2), 687\u2013717 (2014)\n\ncao2023randomized\nCao, Z., Wei, Y., Xie, P.: Randomized GCUR decompositions.  arXiv:2301.13163 (2023)\n\nGidisu2022hybrid\nGidisu, P.Y., Hochstenbach, M.E.: A hybrid DEIM and leverage scores based method for CUR index selection. Progress in Industrial Mathematics at ECMI 2021, 147-153 (2022)\n\nvan1976generalizing\nVan Loan, C.F.: Generalizing the singular value decomposition. SIAM J. Numer. Anal. 13(1), 76\u201383 (1976)\n\nvan1985computing\nVan Loan, C.: Computing the CS and the generalized singular value decompositions. Numer. Math. 46(4), 479\u2013491 (1985)\n\nduersch2017randomized\nDuersch, J.A., Gu, M.: Randomized QR with column pivoting. SIAM J. Sci. Comput. 39(4), 263\u2013291 (2017)\n\nduersch2020randomized\nDuersch, J.A., Gu, M.: Randomized projection for rank-revealing matrix\nfactorizations and low-rank approximations. SIAM Rev. 62(3), 661\u2013682\n(2020)\n\nmartinsson2017householder\nMartinsson, P.-G., Quintana Ort\u00cd, G., Heavner, N., van de Geijn, R.:\nHouseholder QR factorization with randomization for column pivoting\n(HQRRP). SIAM J. Sci. Comput. 39(2), 96\u2013115 (2017)\n\nhalko2011finding\nHalko, N., Martinsson, P.-G., Tropp, J.A.: Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix\ndecompositions. SIAM Rev. 53(2), 217\u2013288 (2011)\n\n\n\n\nBen2003generalized\nBen-Israel, A., Greville, T.N.E.: Generalized Inverses: Theory and Applications, 2nd edn. Springer Verlag, New York (2003)\n\nwei2021randomized\nWei, W., Zhang, H., Yang, X., Chen, X.: Randomized generalized singular value decomposition. Commun. Appl. Math. Comput. 3(1), 137\u2013156 (2021)\n\n\n\n"}