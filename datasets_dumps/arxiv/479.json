{"entry_id": "http://arxiv.org/abs/2303.06654v1", "published": "20230312130328", "title": "Twice Regularized Markov Decision Processes: The Equivalence between Robustness and Regularization", "authors": ["Esther Derman", "Yevgeniy Men", "Matthieu Geist", "Shie Mannor"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "text": "\n\n\n\n\n\n\n\n\nTwice Regularized Markov Decision Processes: \n The Equivalence between\nRobustness and Regularization\n    Esther Derman \n\n        Technion \n\n        estherderman@campus.technion.ac.il \n        Yevgeniy Men  \n\n       Technion \n\n       yevgenimen@campus.technion.ac.il\n       Matthieu Geist \n\n       Google Research, Brain Team \n\n       mfgeist@google.com\n       Shie Mannor \n\n       Technion & Nvidia Research\n\n       shie@technion.ac.il\n       \n    \n======================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\nRobust Markov decision processes (MDPs) aim to handle changing or partially known system dynamics. To solve them, one typically resorts to robust optimization methods. However, this significantly increases computational complexity and limits scalability in both learning and planning. On the other hand, regularized MDPs show more stability in policy learning without impairing time complexity. Yet, they generally do not encompass uncertainty in the model dynamics.  In this work, we aim to learn robust MDPs using regularization. We first show that regularized MDPs are a particular instance of robust MDPs with uncertain reward. We thus establish that policy iteration on reward-robust MDPs can have the same time complexity as on regularized MDPs. We further extend this relationship to MDPs with uncertain transitions: this leads to a regularization term with an additional dependence on the value function. We then generalize regularized MDPs to twice regularized MDPs  (MDPs), MDPs with both value and policy regularization. The corresponding Bellman operators enable us to derive planning and learning schemes with convergence and generalization guarantees, thus reducing robustness to regularization. We numerically show this two-fold advantage on tabular and physical domains, highlighting the fact that preserves its efficacy in continuous environments.   \n\n\n\n  reinforcement learning, robust Markov decision processes, robust optimization, regularization, Fenchel-Rockafellar duality\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n \nMDPs provide a practical framework for solving sequential decision problems under uncertainty <cit.>. However, the chosen strategy can be very sensitive to sampling errors or inaccurate model estimates. This can lead to complete failure in common situations where the model parameters vary adversarially or are simply unknown <cit.>. Robust MDPs aim to mitigate such sensitivity by assuming that the transition and/or reward function (P,r) varies arbitrarily inside a given uncertainty set  <cit.>. In this setting, an optimal solution maximizes a performance measure under the worst-case parameters. It can be thought of as a dynamic zero-sum game with an agent choosing the best action while Nature imposes it the most adversarial model. As such, solving robust MDPs involves max-min problems, which can be computationally challenging and limits scalability.\n\nIn recent years, several methods have been developed to alleviate the computational concerns raised by robust reinforcement learning (RL).   \nApart from <cit.> who consider specific types of coupled uncertainty sets, all rely on a rectangularity assumption without which the problem can be NP-hard <cit.>. This assumption is key to deriving tractable solvers of robust MDPs such as robust value iteration <cit.> or  more general robust modified policy iteration (MPI) <cit.>. Yet, reducing time complexity in robust Bellman updates remains challenging and is still researched today <cit.>.\n\n \nAt the same time, the empirical success of regularization in policy search methods has motivated a wide range of algorithms with diverse motivations such as improved exploration <cit.> or stability <cit.>. <cit.> proposed a unified view from which many existing algorithms can be derived. Their regularized MDP formalism opens the path to error propagation analysis in approximate MPI <cit.> and leads to the same bounds as for standard MDPs. Nevertheless, as we further show in Sec.\u00a0<ref>, policy regularization accounts for reward uncertainty only: it does not encompass uncertainty in the model dynamics.\nDespite a vast literature on how regularized policy search works and convergence rates analysis <cit.>, little attention has been given to understanding why it can generate strategies that are robust to external perturbations <cit.>.\n\nTo our knowledge, the only works that relate robustness to regularization in RL are \n<cit.>. <cit.> employ a distributionally robust optimization approach to regularize an empirical value function. Unfortunately, computing this empirical value necessitates several policy evaluation procedures, which is quickly unpractical. <cit.> provide a dual relationship with robust MDPs under uncertain reward. Their duality result applies to general regularization methods and gives a robust interpretation of soft-actor-critic <cit.>. Although these two works justify the use of regularization for ensuring robustness, they do not enclose any algorithmic novelty. Similarly, <cit.> specifically focus on maximum entropy methods and relate them to either reward or transition robustness. We shall further detail on these most related studies in Sec.\u00a0<ref>.\n\nThe robustness-regularization duality is well established in statistical learning theory <cit.>, as opposed to RL theory.\nIn fact, standard setups such as classification or regression may be considered as single-stage decision-making problems, one-step MDPs, a particular case of RL setting. Extending this robustness-regularization duality to RL would yield cheaper learning methods with robustness guarantees. As such, we introduce a regularization function \u03a9_ that depends on the uncertainty set  and is defined over both policy and value spaces, thus inducing a twice regularized Bellman operator (see Sec.\u00a0<ref>). We show that this regularizer yields an equivalence of the form v_\u03c0,  = v_\u03c0, \u03a9_, where v_\u03c0, is the robust value function for policy \u03c0 and v_\u03c0, \u03a9_ the regularized one. This equivalence is derived through the objective function each value optimizes. More concretely, we formulate the robust value function v_\u03c0, as an optimal solution of the robust optimization problem:    \n\n    max_v\u2208^v, \u03bc_0 s. t.  v\u2264inf_(P,r)\u2208T_(P,r)^\u03c0v,\n        RO\n \nwhere T_(P,r)^\u03c0 is the evaluation Bellman operator <cit.>. Then, we show that  v_\u03c0, is also an optimal solution of the convex (non-robust) optimization problem: \n\n    max_v\u2208^v, \u03bc_0 s. t.  v\u2264 T_(P_0,r_0)^\u03c0v -\u03a9_(\u03c0, v),\n            CO\n\nwhere (P_0,r_0) is the nominal model. This establishes equivalence as the two problems admit the same optimum for any policy. \nMoreover, the inequality constraint of (<ref>) enables to derive a twice regularized () Bellman operator defined according to \u03a9_, a policy and value regularizer. For ball-constrained uncertainty sets, \u03a9_ has an explicit form and under mild conditions, the corresponding Bellman operators are contracting. The equivalence between the two problems (<ref>) and (<ref>) together with the contraction properties of Bellman operators enable to circumvent robust optimization problems at each Bellman update. As such, it alleviates robust planning and learning algorithms by reducing them to regularized ones, which are known to be as complex as classical methods. \n\nTo summarize, we make the following contributions: \n(i)\u00a0We show that regularized MDPs are a specific instance of robust MDPs with uncertain reward. Besides formalizing a general connection between the two settings, our result enables to explicit the uncertainty sets induced by standard regularizers. (ii)\u00a0We extend this duality to MDPs with uncertain transition and provide the first regularizer that recovers robust MDPs with s-rectangular balls and arbitrary norm. (iii)\u00a0We introduce twice regularized MDPs (MDPs) that apply both policy and value regularization to retrieve robust MDPs. We establish contraction of the corresponding Bellman operators. This leads us to proposing an MPI algorithm with similar time complexity as vanilla MPI. (iv)\u00a0We also introduce a model-free algorithm, q-learning, that solves MDPs, and for which we establish theoretical convergence. (v)\u00a0With the aim of extending q-learning to large state-spaces, we provide an easy method for estimating the value regularization term when a tabular representation is no longer available. Experiments on tabular and continuous domains prove the efficiency of for both planning and learning, thus opening new perspectives towards practical and scalable robust RL.[This paper extends the conference article Twice regularized MDPs and the equivalence between robustness and regularization published in Advances in Neural Information Processing Systems 2021 by . This manuscript extends the theory by introducing q-learning and proving its convergence. We additionally scale this approach to deep RL by proposing our new algorithm, double DQN. Experiments on both tabular and physical domains evaluate the performance of learning, thus confirming our previous findings on planning.]\n\n\n\n\u00a7 PRELIMINARIES\n\n\n \nThis section describes the background material that we use throughout our work. Firstly, we introduce some notations. Secondly, we recall useful properties in convex analysis. Thirdly, we address classical discounted MDPs and their linear program (LP) formulation. Fourth, we briefly detail regularized MDPs and the associated operators and lastly, we focus on the robust MDP setting. \n\n\n\n \u00a7.\u00a7 Notations\n\n\n=-1\nWe designate the extended reals by := \u222a{-\u221e, \u221e}.\nGiven a finite set , the class of real-valued functions (resp. probability distributions) over  is denoted by ^ (resp. \u0394_), while the constant function equal to 1 over  is denoted by 1_. Similarly, for any set \ud835\udcb3, \u0394_^\ud835\udcb3 denotes the class of functions defined over  and valued in \u0394_. The inner product of two functions , \u2208^ is defined as , := \u2211_z\u2208(z)(z), which induces the \u2113_2-norm := \u221a(,). The \u2113_2-norm coincides with its dual norm, = max_\u2264 1,  =:_*.\nLet a function f: ^\u2192. The Legendre-Fenchel transform (or convex conjugate) of f is \nf^*() := max_\u2208^{, - f()}. Given a set \u2128\u2286^, \nthe characteristic function \u03b4_\u2128: ^\u2192 is  \u03b4_\u2128() = 0 if \u2208\u2128; +\u221e otherwise. The Legendre-Fenchel transform of  \u03b4_\u2128 is the support function \n\u03c3_\u2128() = max_\u2208\u2128, <cit.>. \n\n \n\n\n\n\nLet C\u2282^ be a convex set and \u03a9:C\u2192 a strongly convex function. \nThroughout this study, the function \u03a9 plays the role of a policy and/or value regularization function.\nIts Legendre-Fenchel transform \u03a9^* satisfies several smoothness properties, hence its alternative name smoothed max operator <cit.>.\n\nOur work makes use of the following result <cit.>. \n\n\n\n\nGiven \u03a9:C\u2192 strongly convex, the following properties hold:\n\n\n    \n  (i) \n    \u2207\u03a9^* is Lipschitz and satisfies \n    \u2207\u03a9^*() = _\u2208 C, - \u03a9(), \u2200\u2208^.\n\n    \n  (ii) For any c\u2208, \u2208^, \u03a9^*( + c1_) = \u03a9^*() +c.\n\n    \n  (iii) The Legendre-Fenchel transform \u03a9^* is non-decreasing. \n\n\n\n \n\n\n \u00a7.\u00a7 Discounted MDPs and LP formulation\n\n\n\tConsider an infinite horizon MDP (, ,  \u03bc_0, \u03b3, P, r) with  and   finite state and action spaces respectively, 0< \u03bc_0 \u2208\u0394_ an initial state distribution and \u03b3\u2208 (0,1) a discount factor. Denoting \n\t:= \u00d7, \n\tP\u2208\u0394_^ is a transition kernel \n\tmapping each state-action pair to a probability distribution over  and r\u2208^ is a reward function. \n    A policy \u03c0\u2208\u0394_^ maps any state s\u2208 to an action distribution \u03c0_s\u2208\u0394_, and we evaluate its performance through the following measure:\n\n    \u03c1(\u03c0) := \ud835\udd3c[\u2211_t = 0^\u221e\u03b3^tr(s_t, a_t) | \u03bc_0, \u03c0, P] = v_(P,r)^\u03c0, \u03bc_0.\n\nHere, the expectation is conditioned on the process distribution determined by \u03bc_0, \u03c0 and P,\nand for all s\u2208, v_(P,r)^\u03c0(s) = \ud835\udd3c[\u2211_t = 0^\u221e\u03b3^tr(s_t, a_t) | s_0=s, \u03c0, P] is the value function at state s.\nMaximizing (<ref>) defines the standard RL objective,\nwhich can be solved thanks to the Bellman operators:\n\n    T^\u03c0_(P,r)v    := r^\u03c0 + \u03b3 P^\u03c0v   \u2200 v\u2208\u211d^, \u03c0\u2208\u0394_^,\n    \n        T_(P,r)v    := max_\u03c0\u2208\u0394_^T^\u03c0_(P,r)v   \u2200 v\u2208\u211d^,\n    \ud835\udca2_(P,r)(v)    := {\u03c0\u2208\u0394_^: T^\u03c0_(P,r)v = T_(P,r)v}  \u2200 v\u2208\u211d^,\n\nwhere r^\u03c0 := [\u03c0_s, r(s,\u00b7)]_s\u2208 and P^\u03c0 = [P^\u03c0(s'| s)]_s',s\u2208 with  P^\u03c0(s'| s) :=   \u03c0_s, P(s' | s,\u00b7).\nBoth T^\u03c0_(P,r) and T_(P,r) are \u03b3-contractions with respect to (w.r.t.) the supremum norm, so each admits a unique fixed point v^\u03c0_(P,r) and v^*_(P,r), respectively. The set of greedy policies w.r.t. value v defines \ud835\udca2_(P,r)(v), and any policy \u03c0\u2208\ud835\udca2_(P,r)(v^*_(P,r)) is optimal <cit.>. For all v\u2208^, the associated function q\u2208^ is given by q(s,a) = r(s,a) + \u03b3P(\u00b7|s,a),v  \u2200 (s,a)\u2208. In particular, the fixed point v^\u03c0_(P,r) satisfies v^\u03c0_(P,r) = \u03c0_s, q^\u03c0_(P,r)(s,\u00b7) where q^\u03c0_(P,r) is its associated q-function.\n\nThe problem in (<ref>) can also be formulated as an LP. \nGiven a policy \u03c0\u2208\u0394_^, we characterize its performance \u03c1(\u03c0) by the following v-LP <cit.>:\n\n    min_v\u2208^v, \u03bc_0 subject to (s.t.)  v \u2265 r^\u03c0 + \u03b3 P^\u03c0v. P^\u03c0\n\nThis primal objective provides a policy view on the problem. Alternatively, one may take a state visitation perspective by studying the dual objective instead:\n\n    max_\u03bc\u2208^r^\u03c0,\u03bc s. t. \u03bc\u2265 0  and  (\ud835\udc08\ud835\udc1d_^ - \u03b3 P_*^\u03c0)\u03bc = \u03bc_0, D^\u03c0\n\nwhere P_*^\u03c0 is the adjoint policy transition operator[It is the adjoint operator of P^\u03c0 in the sense that  P^\u03c0v, v' = v, P_*^\u03c0v'  \u2200 v, v'\u2208^.]:\n[P_*^\u03c0\u03bc](s):= \u2211_s\u0305\u2208P^\u03c0(s|s\u0305)\u03bc(s\u0305)   \u2200\u03bc\u2208^,\nand \ud835\udc08\ud835\udc1d_ is the identity function in ^.\n\nLet \ud835\udc08(s'| s,a) := \u03b4_s'=s  \u2200 (s,a,s')\u2208\u00d7 the trivial transition matrix and\ndefine its adjoint transition operator as \ud835\udc08_*\u03bc(s) := \u2211_(s\u0305, a\u0305)\u2208\ud835\udc08(s| s\u0305, a\u0305)\u03bc(s\u0305, a\u0305)  \u2200 s\u2208.\nThe correspondence between occupancy measures and policies lies in the one-to-one mapping\n\u03bc\u21a6\u03bc(\u00b7, \u00b7)/\ud835\udc08_*\u03bc(\u00b7)=:\u03c0_\u03bc\nand its inverse \u03c0\u21a6\u03bc_\u03c0 given by\n\n    \u03bc_\u03c0(s,a):= \u2211_t = 0^\u221e\u03b3^t\u2119(s_t=s, a_t = a |\u03bc_0, \u03c0, P)  \u2200 (s,a)\u2208.\n\nAs such, one can interchangeably work with the primal LP (<ref>) or the dual (<ref>). \n\n\n\n \u00a7.\u00a7 Regularized MDPs\n\n\nA regularized MDP is a tuple (, ,  \u03bc_0, \u03b3, P, r, \u03a9) with (, ,  \u03bc_0, \u03b3, P, r) an infinite horizon MDP as above, and \u03a9:= (\u03a9_s)_s\u2208 a finite set of functions such that for all s\u2208, \u03a9_s: \u0394_\u2192 is strongly convex. Each function \u03a9_s plays the role of a policy regularizer \u03a9_s(\u03c0_s). With a slight abuse of notation, we shall denote by \u03a9(\u03c0):= (\u03a9_s(\u03c0_s))_s\u2208 the family of state-dependent regularizers.[In the formalism of <cit.>, \u03a9_s is initially constant over . However, later in the paper <cit.>, it changes according to policy iterates. Here, we alternatively define a family \u03a9 of state-dependent regularizers, which accounts for state-dependent uncertainty sets (see Sec.\u00a0<ref> below).] The regularized Bellman evaluation operator is given by \n\n    [T^\u03c0, \u03a9_(P,r)v](s) := T^\u03c0_(P,r)v(s) - \u03a9_s(\u03c0_s)   \u2200 v\u2208^, s\u2208,\n\nand the regularized Bellman optimality operator by T^*,\u03a9_(P,r)v:= max_\u03c0\u2208\u0394_^T^\u03c0, \u03a9_(P,r)v  \u2200 v\u2208^ <cit.>. The unique fixed point of T^\u03c0, \u03a9_(P,r) (respectively T^*,\u03a9_(P,r)) is denoted by v^\u03c0, \u03a9_(P,r) (resp. v^*,\u03a9_(P,r)) and defines the regularized value function (resp. regularized optimal value function). \nAlthough the regularized MDP formalism stems from the aforementioned Bellman operators in <cit.>, it turns out that regularized MDPs are MDPs with modified reward. Indeed, for any policy \u03c0\u2208\u0394_^, the regularized value function is v^\u03c0, \u03a9_(P,r) = (\ud835\udc08_ - \u03b3 P^\u03c0)^-1(r^\u03c0 - \u03a9(\u03c0)), which corresponds to a non-regularized value with expected reward r\u0303^\u03c0:= r^\u03c0 - \u03a9(\u03c0). Note that the modified reward r\u0303^\u03c0(s) is no longer linear in \u03c0_s because of \u03a9_s being strongly convex. Also, this modification does not apply to the reward function r but only to its expectation r^\u03c0, as we cannot regularize the original reward without making it policy-independent. \n\n\n\n \u00a7.\u00a7 Robust MDPs\n\n\nIn general, the MDP model is not explicitly known but rather estimated from sampled trajectories. As this may result in over-sensitive outcome <cit.>, robust MDPs reduce such performance variation. Formally, a robust MDP (, ,  \u03bc_0, \u03b3, ) is an MDP with uncertain model belonging to :=\u00d7, uncertain transition P\u2208\u2286\u0394_^ and\n reward r\u2208\u2286^ <cit.>.\nThe uncertainty set  typically controls the confidence level of a model estimate, which in turn determines the agent's level of robustness.\nIt is given to the agent, who seeks to maximize performance under the worst-case model (P,r)\u2208. Although intractable in general, this problem can be solved in polynomial time for rectangular uncertainty sets, when = \u00d7_s\u2208_s = \u00d7_s\u2208(_s\u00d7_s) <cit.>. For any policy \u03c0\u2208\u0394_^\nand state s\u2208, the robust value function at s is v^\u03c0, (s) := min_(P,r)\u2208  v_(P,r)^\u03c0(s)\nand the robust optimal value function v^*, (s):= max_\u03c0\u2208\u0394_^v^\u03c0, (s). \nEach of them is the unique fixed point of the respective robust Bellman operators:\n\n    [T^\u03c0, v](s)    := min_(P, r)\u2208 T_(P,r)^\u03c0v(s)   \u2200 v\u2208^, s\u2208, \u03c0\u2208\u0394_^,\n    \n    [T^*, v](s)    := max_\u03c0\u2208\u0394_^[T^\u03c0, v](s)   \u2200  v\u2208^, s\u2208,\n\nwhich are \u03b3-contractions. For all v\u2208^, the associated robust q-function is given by \nq(s,a) = min_(P, r)\u2208{r(s,a) + \u03b3P(\u00b7|s,a),v}  \u2200 (s,a)\u2208, so that v^\u03c0,  = \u03c0_s, q^\u03c0,(s,\u00b7) where q^\u03c0, is the robust q-function associated to v^\u03c0,.\n\n \n\n\n\u00a7 REWARD-ROBUST MDPS\n\n\n \nThis section focuses on reward-robust MDPs, robust MDPs with uncertain reward but known transition model. We first show that regularized MDPs represent a particular instance of reward-robust MDPs, as both solve the same optimization problem. This equivalence provides a theoretical motivation for the heuristic success of policy regularization. Then, we explicit the uncertainty set underlying some standard regularization functions, thus suggesting an interpretable explanation of their empirical robustness. \n\nWe first show the following proposition <ref>, which applies to general robust MDPs and random policies. It slightly extends <cit.>, as Lemma\u00a03.2 there focuses on uncertain-transition MDPs and deterministic policies. For completeness, we provide a proof of Prop.\u00a0<ref> in Appx.\u00a0<ref>.\n\n\n\nFor any policy \u03c0\u2208\u0394_^, the robust value function v^\u03c0, is the optimal solution of the robust optimization problem:\n\n    max_v\u2208^v, \u03bc_0 s. t.  v\u2264 T_(P,r)^\u03c0v  for all    (P,r)\u2208. P_\n\n\n\nIn the robust optimization problem (<ref>), the inequality constraint must hold over the whole uncertainty set . As such, a function v\u2208^ is said to be robust feasible for (<ref>) if v\u2264 T_(P,r)^\u03c0v for all (P,r)\u2208 or equivalently, if max_(P,r)\u2208{v(s)  - T_(P,r)^\u03c0v(s) }\u2264 0 for all s\u2208. Therefore, checking robust feasibility requires to solve a maximization problem. For properly structured uncertainty sets, a closed form solution can be derived, as we shall see in the sequel. \nAs standard in the robust RL literature <cit.>, the remaining of this work focuses on uncertainty sets centered around a known nominal model. \nFormally, given P_0 (resp. r_0) a nominal\ntransition kernel (resp. reward function), we consider uncertainty sets of the form\n(P_0 + )\u00d7 ( r_0+ ). The size of \u00d7 quantifies our level of uncertainty or alternatively, the desired degree of robustness. \n\n\n\n \u00a7.\u00a7 Reward-robust and regularized MDPs: an equivalence\n\n\n\nWe now focus on reward-robust MDPs, robust MDPs with = {P_0}\u00d7  (r_0+ ). Thm.\u00a0<ref> establishes that reward-robust MDPs are in fact regularized MDPs whose regularizer is given by a support function. Its proof can be found in Appx.\u00a0<ref>. This result brings two take-home messages: (i) policy regularization is equivalent to reward uncertainty; (ii) policy iteration on reward-robust MDPs has the same convergence rate as regularized MDPs, which in turn is the same as standard MDPs <cit.>.  \n\n\n\nAssume that = {P_0}\u00d7  (r_0+ ). Then, for any policy \u03c0\u2208\u0394_^, the robust value function v^\u03c0, is the optimal solution of the convex optimization problem:\n\n    max_v\u2208^v, \u03bc_0 s. t.  v(s)\u2264 T_(P_0,r_0)^\u03c0v(s) - \u03c3__s(-\u03c0_s)  for all  s\u2208,\n\nwhere \u03c3__s is the support function of the reward uncertainty set (see definition in Sec.\u00a0<ref>).\n\n\nThm.\u00a0<ref> clearly highlights a convex regularizer \u03a9_s(\u03c0_s):= \u03c3__s(-\u03c0_s)  \u2200 s\u2208. We thus recover a regularized MDP by setting \n[T^\u03c0,\u03a9v](s) = T_(P_0,r_0)^\u03c0v(s) - \u03c3__s(-\u03c0_s)  \u2200 s\u2208. In particular, when _s is a ball of radius \u03b1_s^r, the support function (or regularizer) can be written in closed form as \u03a9_s(\u03c0_s):= \u03b1_s^r\u03c0_s, which is strongly convex. We formalize this below (see proof in Appx.\u00a0<ref>). \n\n\n\nLet \u03c0\u2208\u0394_^ and = {P_0}\u00d7  (r_0+ ). Further assume that for all s\u2208, the reward uncertainty set at s is _s:= {r_s\u2208^: r_s\u2264\u03b1_s^r}. Then,  the robust value function v^\u03c0, is the optimal solution of the convex optimization problem:\n\n    max_v\u2208^v, \u03bc_0 s. t.  v(s)\u2264 T_(P_0,r_0)^\u03c0v(s) - \u03b1_s^r\u03c0_s for all  s\u2208.\n\n\n\nWhile regularization induces reward-robustness,\nThm.\u00a0<ref> and Cor.\u00a0<ref> suggest that, on the other hand, specific reward-robust MDPs recover well-known policy regularization methods. In the following section, we explicit the reward-uncertainty sets underlying some of these regularizers. \n\n\n\n\n \u00a7.\u00a7 Related Algorithms\n\n\n \nConsider a reward uncertainty set of the form := \u00d7_(s,a)\u2208_s,a. This defines an (s,a)-rectangular  (a particular type of s-rectangular ) whose rectangles _s,a are independently defined for each state-action pair. For the regularizers below, we derive appropriate _s,a-s that recover the same regularized value function. Detailed proofs are in Appx.\u00a0<ref>. There, we also include a summary table that reviews the properties of some RL regularizers, as well as our function which we shall introduce later in Sec.\u00a0<ref>. Note that the reward uncertainty sets here depend on the policy. This is due to the fact that standard regularizers are defined over the policy space and not at each state-action pair. It similarly explains why the reward modification induced by regularization does not apply to the original reward function, as already mentioned in Sec.\u00a0<ref>.\n\nNegative Shannon entropy:\nLet _s,a^NS(\u03c0):= [ln(1/\u03c0_s(a)), +\u221e)  \u2200 (s,a)\u2208.\nThe associated support function enables to write:\n\n    \u03c3__s^NS(\u03c0)(-\u03c0_s) \n         = max_r(s,\u00b7):  r(s,a')\u2208_s,a'^NS(\u03c0), a'\u2208\u2211_a\u2208 -r(s,a)\u03c0_s(a) \n        =  \u2211_a\u2208\u03c0_s(a)ln(\u03c0_s(a)),\n\nwhere the last equality comes from maximizing -r(s,a) over [ln(1/\u03c0_s(a)), +\u221e) for each a\u2208. \nWe thus recover the negative Shannon entropy \u03a9(\u03c0_s)= \u2211_a\u2208\u03c0_s(a)ln(\u03c0_s(a)) <cit.>.\n\nKullback-Leibler divergence: Given an action distribution 0<d\u2208\u0394_, let  _s,a^KL(\u03c0):= ln(d(a)) + _s,a^NS(\u03c0)  \u2200 (s,a)\u2208. It amounts to translating the interval\n_s,a^NS by the given constant. Writing the support function yields \u03a9(\u03c0_s)= \u2211_a\u2208\u03c0_s(a)ln(\u03c0_s(a)/d(a)), which is exactly the KL divergence <cit.>.\n\nNegative Tsallis entropy: Letting _s,a^T(\u03c0):= [(1-\u03c0_s(a))/2, +\u221e)  \u2200 (s,a)\u2208, we recognize the negative Tsallis entropy \u03a9(\u03c0_s) = 1/2(\u03c0_s^2-1) <cit.>.\n\nThe worst-case rewards derived for the KL divergence and the Tsallis entropy are consistent with those obtained by <cit.>. Indeed, in both cases, taking the finite endpoint of the interval recovers the same worst-case reward. Yet, the dual view adopted there yields larger reward uncertainty, which may yield more conservative solutions when using approximate solvers. \n\n\n\n \u00a7.\u00a7 Policy-gradient for reward-robust MDPs\n\n\nThe equivalence between reward-robust and regularized MDPs leads us to wonder whether we can employ policy-gradient  <cit.> on reward-robust MDPs using regularization. The following result establishes that a policy-gradient theorem can indeed be established for reward-robust MDPs (see proof in Appx.\u00a0<ref>). \n\n\n\nAssume that = {P_0}\u00d7  (r_0+ ) with _s= {r_s\u2208^: r_s\u2264\u03b1_s^r}. Then, the gradient of the reward-robust objective J_(\u03c0):= v^\u03c0, , \u03bc_0 is given by\n\n    \u2207 J_(\u03c0) = \ud835\udd3c_(s,a)\u223c\u03bc_\u03c0[ \u2207ln\u03c0_s(a)(q^\u03c0, (s,a) - \u03b1_s^r\u03c0_s(a)/\u03c0_s)],\n\nwhere \u03bc_\u03c0 is the occupancy measure under the nominal model P_0 and policy \u03c0. \n\n\nAlthough Prop.\u00a0<ref> is an application of <cit.> for a specific regularized MDP, its reward-robust formulation is novel and suggests another simplification of robust methods. Indeed, previous works that exploit policy-gradient on robust MDPs involve the occupancy measure of the worst-case model <cit.>, whereas our result sticks to the nominal. In practice, Prop.\u00a0<ref> enables to learn a robust policy by sampling transitions from the nominal model instead of all uncertain models. This has a twofold advantage: (i) it avoids an additional computation of the minimum as done in <cit.>, where the authors sample next-state transitions and rewards based on all parameters from the uncertainty set, then update a policy based on the worst outcome; (ii) it releases from restricting to finite uncertainty sets. In fact, our regularizer accounts for robustness regardless of the sampling procedure, whereas the parallel simulations of <cit.> require the uncertainty set to be finite. Technical difficulties are yet to be addressed for generalizing our result to transition-uncertain MDPs, because of the interdependence between the regularizer and the value function (see Secs.\u00a0<ref>-<ref>). We detail more on this issue in Appx.\u00a0<ref>. Recently, <cit.> established a robust policy gradient for transition-uncertain MDPs, but their setting focuses on a fixed mixture between the nominal kernel and an arbitrary transition matrix, = ((1-R)P_0 + R\u0394_^ )\u00d7{r_0}. By design, this yields an (s,a)-rectangular uncertainty set included in an \u2113_\u221e-ball of size R around the nominal, whereas our setup considers more general s-rectangular uncertainty. \n\n\n\n\u00a7 GENERAL ROBUST MDPS\n\n\n \n\nNow that we have established policy regularization as a reward-robust problem, we would like to study the opposite question: can any robust MDP with both uncertain reward and transition be solved using regularization instead of robust optimization? If so, is the regularization function easy to determine? In this section, we answer positively to both questions for properly defined robust MDPs. This  greatly facilitates robust RL, as it avoids the increased complexity of robust planning algorithms  while still reaching robust performance.  \n\nThe following theorem establishes that similarly to reward-robust MDPs, robust MDPs can be formulated through regularization (see proof in Appx.\u00a0<ref>). Although the regularizer is also a support function in that case, it depends on both the policy and the value objective, which may further explain the difficulty of dealing with robust MDPs. \n\n\n\nAssume that =   (P_0+ )\u00d7(r_0+ ). Then, for any policy \u03c0\u2208\u0394_^, the robust value function v^\u03c0, is the optimal solution of the convex optimization problem:\n\n    max_v\u2208^v, \u03bc_0 s. t.  v(s)\u2264 T_(P_0,r_0)^\u03c0v(s)  -\u03c3__s(-\u03c0_s) -\u03c3__s(-\u03b3 v\u00b7\u03c0_s)  for all  s\u2208,\n\nwhere [v\u00b7\u03c0_s](s',a):=v(s')\u03c0_s(a)  \u2200 (s',a)\u2208.\n\n\n \nThe upper-bound in the inequality constraint (<ref>) is of the same spirit as the regularized Bellman operator: the first term is a standard, non-regularized Bellman operator on the nominal model (P_0, r_0) to which we subtract a policy and value-dependent function playing the role of regularization. \nThis function reminds that of <cit.> also coming from conjugacy. This is the only similarity between both regularizers: in <cit.>, the Legendre-Fenchel transform is applied on a different type of function and results in a regularization term that has no closed form but can only be bounded from above. Moreover, the setup considered there is different since it studies distributionally robust MDPs. As such, it involves general convex optimization, whereas we focus on the robust formulation of an LP. \n\nThe support function further simplifies when the uncertainty set is a ball, as shown below. Yet, the dependence of the regularizer on the value function prevents us from readily applying the regularized MDP tool-set. We shall study the properties of this new regularization function in Sec.\u00a0<ref>. \n\n\n\nAssume that =   (P_0+ )\u00d7(r_0+ ) with _s:= {P_s\u2208^: P_s\u2264\u03b1_s^P} and _s:= {r_s\u2208^: r_s\u2264\u03b1_s^r} for all s\u2208. Then, the robust value function v^\u03c0, is the optimal solution of the convex optimization problem:\n\n    max_v\u2208^v, \u03bc_0 s. t.  v(s)\u2264 T_(P_0,r_0)^\u03c0v(s) -\u03b1_s^r\u03c0_s - \u03b1_s^P\u03b3v\u03c0_s for all  s\u2208.\n\n\n\nWe restrict our statement to the \u2113_2-norm for notation convenience only, the dual norm of \u2113_2 being \u2113_2 itself. In fact, one can consider two different norms for reward and transition uncertainties. Thus, Cor.\u00a0<ref> can be rewritten with an arbitrary norm, which would reveal a dual norm \u00b7_* instead of \u00b7 in Eq.\u00a0(<ref>) (see proof in Appx.\u00a0<ref>). As a result, our regularization function recovers a robust value function independently of the chosen norm, which extends previous results from <cit.>. Indeed, <cit.> reduce the complexity of robust planning under the \u2113_1-norm only, while <cit.> focus on KL and \u2113_2 ball-constrained uncertainty sets. Both works rely on the specific structure induced by the divergence they consider to derive more efficient robust Bellman updates. Differently, our method circumvents these updates using a generic, problem-independent regularization function while still encompassing s-rectangular uncertainty sets as in <cit.>.     \n\n \n\n\n\u00a7 MDPS\n\n\n \nIn Sec.\u00a0<ref>, we showed that for general robust MDPs, the optimization constraint involves a regularization term that depends on the value function itself. This adds a difficulty to the reward-robust case where the regularization only depends on the policy. Yet, we provided an explicit regularizer for general robust MDPs that are ball-constrained. In this section, we introduce MDPs, an extension of regularized MDPs that combines policy and value regularization. The core idea is to further regularize the Bellman operators with a value-dependent term that recovers the support functions we derived from the robust optimization problems of Secs.\u00a0<ref>-<ref>. \n\n \n\nFor all v\u2208^, define \u03a9_v, : \u0394_\u2192 as \n\u03a9_v, (\u03c0_s):= \u03c0_s (\u03b1_s^r + \u03b1_s^P \u03b3v). The Bellman evaluation and optimality operators are defined as \n\n    [T^\u03c0,v](s)    := T_(P_0,r_0)^\u03c0v(s)-\u03a9_v, (\u03c0_s)    \u2200 s\u2208,\n        \n    \n        [T^*, v](s)    := max_\u03c0\u2208\u0394_^ [T^\u03c0,v](s)= \u03a9_v, ^*(q_s)   \u2200 s\u2208.\n\nFor any function v\u2208^, the associated unique greedy policy is defined as \n\n    \u03c0_s =  _\u03c0_s\u2208\u0394_ T^\u03c0,v(s)= \u2207\u03a9_v, ^*(q_s),   \u2200 s\u2208,\n\nthat is, in vector form, \u03c0 = \u2207\u03a9_v, ^*(q) =: \ud835\udca2_\u03a9_(v)  T^\u03c0,v = T^*, v. \n\n\nThe Bellman evaluation operator is not linear because of the functional norm appearing in the regularization function. Yet, under the following assumption, it is contracting and we can apply Banach's fixed point theorem to define the value function. \n \n[Bounded radius]\n\nFor all s\u2208, there exists \u03f5_s > 0 such that \n    \u03b1_s^P\u2264min( 1-\u03b3-\u03f5_s/\u03b3\u221a(); min_\ud835\udc2e_\u2208^_+, \ud835\udc2e_=1\n    \ud835\udc2f_\u2208^_+, \ud835\udc2f_=1\ud835\udc2e_^\u22a4 P_0(\u00b7| s,\u00b7)\ud835\udc2f_).\n \n\n\n\n\nAsm.\u00a0<ref> requires to upper bound the ball radius of transition uncertainty sets. The first term in the minimum is needed for establishing contraction of Bellman operators (item (iii) in Prop.\u00a0<ref>), while the second one is used for ensuring monotonicity (item (i) in Prop.\u00a0<ref>). We remark that the former depends on the original discount factor \u03b3: radius \u03b1_s^P must be smaller as \u03b3 tends to 1 but can arbitrarily grow as \u03b3 decreases to 0, without altering contraction. Indeed, larger \u03b3 implies longer time horizon and higher stochasticity, which explains why we need tighter level of uncertainty then. Otherwise, value and policy regularization seem unable to handle the mixed effects of parametric and stochastic uncertainties. The additional dependence on the state-space size comes from the \u2113_2-norm chosen for the ball constraints. In fact, for any \u2113_p-norm of dual \u2113_q, ^1/q replaces \u221a() in the denominator, so the bound becomes independent of  as (p,q) tends to (1,\u221e) (see Appx.\u00a0<ref>). Although we recognize a generalized Rayleigh quotient-type problem in the second minimum <cit.>, its interpretation in our context remains unclear. Asm.\u00a0<ref> enables the Bellman operators to admit a unique fixed point, among other nice properties. We formalize this below (see proof in Appx.\u00a0<ref>).\n\n\n\nSuppose that Asm.\u00a0<ref> holds. Then, the following properties hold:\n\n\n    \n  (i) Monotonicity: For all v_1, v_2\u2208^ such that v_1\u2264 v_2, we have T^\u03c0,v_1 \u2264 T^\u03c0,v_2 and \n\n    T^*, v_1 \u2264 T^*, v_2.  \n\n    \n  (ii) Sub-distributivity: For all v_1\u2208^, c\u2208, we have T^\u03c0,(v_1 + c1_)\u2264 T^\u03c0,v_1 + \u03b3 c1_ and T^*, (v_1 + c1_)\u2264 T^*, v_1 + \u03b3 c1_, \u2200 c\u2208. \n\n    \n  (iii) Contraction: Let \u03f5_*:= min_s\u2208\u03f5_s>0. Then, for all v_1, v_2\u2208^, we have\n\n    T^\u03c0,v_1 -  T^\u03c0,v_2_\u221e\u2264 (1-\u03f5_*)v_1-v_2_\u221e and\n    T^*, v_1 -  T^*, v_2_\u221e\u2264 (1-\u03f5_*)v_1-v_2_\u221e.\n\n\n\nWe emphasize that the contracting coefficient 1-\u03f5^* from Prop.\u00a0<ref> is different from the original discount factor \u03b3. Yet, as Asm.\u00a0<ref> suggests it, an intrinsic dependence between \u03b3 and \n\u03f5^* makes the Bellman updates similar to the standard ones: when \u03b3 tends to 0, the value of \u03f5^* required for Asm.\u00a0<ref> to hold increases, which makes the contracting coefficient 1-\u03f5^* tend to 0 as well, the two contracting coefficients behave similarly. \nThe contracting feature of both Bellman operators finally leads us to introduce value functions.\n\n\n\n\n    \n  (i) The value function v^\u03c0, is defined as the unique fixed point of the Bellman evaluation operator: v^\u03c0,  = T^\u03c0,v^\u03c0,. The associated q-function is q^\u03c0,(s,a) = r_0(s,a) + \u03b3 P_0(\u00b7|s,a), v^\u03c0,.\n    \n  (ii) The optimal value function v^*, is defined as the unique fixed point of the Bellman optimal operator: v^*,  = T^*,v^*,. The associated q-function is q^*,(s,a) = r_0(s,a) + \u03b3 P_0(\u00b7|s,a), v^*,.\n\n\n\n\n\nThe monotonicity of Bellman operators plays a key role in reaching an optimal policy, as we show in the following. A proof can be found in Appx.\u00a0<ref>.\n\n\n\nThe greedy policy \u03c0^*,  = \ud835\udca2_\u03a9_(v^*, ) is the unique optimal policy, for all \u03c0\u2208\u0394_^, v^\u03c0^*,  = v^*, \u2265 v^\u03c0,. \n\n\n  \n\n\nAn optimal policy may be stochastic. This is due to the fact that our MDP framework builds upon the general s-rectangularity assumption. Robust MDPs with s-rectangular uncertainty sets may similarly yield an optimal robust policy that is stochastic <cit.>. Nonetheless, the MDP formulation recovers a deterministic optimal policy in the more specific (s,a)-rectangular case, which is in accordance with the robust MDP setting (see proof in Appx.\u00a0<ref>). [The stochasticity of an optimal entropy-regularized policy as in the examples of Sec.\u00a0<ref> is not contradicting. Indeed, even though the corresponding uncertainty set is (s,a)-rectangular there, it is policy-dependent. ]\n\n\n\n\n\u00a7 PLANNING IN MDPS\n\n\n\n \u00a7.\u00a7 Modified Policy Iteration\n\n \nR0.33\n\n\n\n \n\n\n\n\nAll of the results above ensure convergence of MPI in MDPs. We call that method MPI and provide its pseudo-code in Alg.\u00a0<ref>. The convergence proof follows the same lines as in <cit.>. Moreover, the contracting property of the Bellman operator ensures the same convergence rate as in standard and robust MDPs, a geometric convergence rate. On the other hand, MPI reduces the computational complexity of robust MPI by avoiding to solve a max-min problem at each iteration, as this can take polynomial time for general convex programs. Advantageously, the only optimization involved in MPI lies in the greedy step: it amounts to projecting onto the simplex, which can efficiently be performed in linear time <cit.>. Still, such projection is not even necessary in the (s,a)-rectangular case: as mentioned in Rmk.\u00a0<ref>, it then suffices to choose a greedy action in order to eventually achieve an optimal value function. \n \n\n\n \u00a7.\u00a7 Planning on a Maze\n\n\n \nWe aim to compare the computing time of MPI with that of MPI <cit.> and robust MPI <cit.>. The code is available at <https://github.com/EstherDerman/r2mdp>. To do so, we run experiments on an Intel(R) Core(TM) i7-1068NG7 CPU @ 2.30GHz machine, which we test on a 5\u00d75 grid-world domain. In that environment, the agent starts from a random position and seeks to reach a goal state in order to maximize reward. Thus, the reward function is zero in all states but two: one provides a reward of 1 while the other gives 10.  An episode ends when either one of those two states is attained.  \n\nThe partial evaluation of each policy iterate is a building block of MPI. As a sanity check, we evaluate the uniform policy through both and robust policy evaluation (PE) sub-processes, to ensure that the two value outputs coincide.\nFor simplicity, we focus on an (s,a)-rectangular uncertainty set and take the same ball radius \u03b1^r (resp.\u00a0\u03b1^P) at each state-action pair for the reward function (resp. transition function). Parameter values and other implementation details are deferred to Appx.\u00a0<ref>. We obtain the same value for PE and robust PE, which numerically confirms Thm.\u00a0<ref>. On the other hand, both are strictly smaller than their non-robust, non-regularized counterpart, but as expected, they converge to the standard value function when all ball radii tend to 0 (see Appx.\u00a0<ref>). More importantly, PE converges in 0.02 seconds, whereas robust PE takes 54.8 seconds to converge, 2740 times longer. This complexity gap comes from the minimization problems being solved at each iteration of robust PE, something that PE avoids thanks to regularization. PE still takes 2.5 times longer than its standard, non-regularized counterpart, because of the additional computation of regularization terms. Table <ref> shows the time spent by each algorithm until convergence. \n \nWe then study the overall MPI process for each approach. We know that in vanilla MPI, the greedy step is achieved by simply searching over deterministic policies <cit.>. Since we focus our experiments on an (s,a)-rectangular uncertainty set, the same applies to robust MPI <cit.> and to MPI, as already mentioned in Rmk.\u00a0<ref>. We can see in Table <ref> that the increased complexity of robust MPI is even more prominent than its PE thread, as robust MPI takes 3953 (resp. 3270) times longer than MPI when m=1 (resp. m=4). Robust MPI with m=4 is a bit more advantageous than m=1, as it needs less iterations (31 versus 67), less optimization solvers to converge. Interestingly, for both m\u2208{1,4}, progressing from PE to MPI did not cost much more computing time to either the vanilla or the version: both take less than one second to run.   \n\n\n\n\n\n\u00a7 LEARNING IN MDPS\n\n\nIn general, we do not know the nominal model (P_0,r_0) and can only interact with the underlying system. Therefore, in this section, we are interested in devising a model-free method that (i) achieves a robust optimal policy (ii) has low time complexity. We show that when the uncertainty set is (s,a)-rectangular, an q-learning scheme provably converges to the optimal robust q-value. In the remaining part of this work, we thus assume that = \u00d7_(s,a)\u2208_s,a so there exists a deterministic policy which is optimal. We first formalize q-learning and its convergence guarantees in Sec.\u00a0<ref>. Then, in Sec.\u00a0<ref>, we numerically evaluate its properties on a tabular environment. We finally propose to extend q-learning to a deep variant in Sec.\u00a0<ref>. In particular, we introduce an easy method for estimating the norm of the value regularizer in non-tabular settings. The source code for q-learning and its deep extension is available at <https://github.com/yevgm/r2rl>.   \n\n\n\n \u00a7.\u00a7 q-learning\n\n\n\nq-learning is an variant of vanilla q-learning <cit.> aiming to learn a robust optimal policy. Its pseudo-code can be found in Alg.\u00a0<ref>. The only variation with standard q-learning is that we update an temporal difference (TD) to target an Bellman recursion. On the other hand, differently than robust q-learning <cit.>, q-learning does not involve any optimization problem in updating TDs. Indeed, robust q-learning requires computing the support function of the current value, which demands a linear program solver. Therefore, the additional time complexity is of \ud835\udcaa(^3) at least, the time required for multiplying square matrices using standard methods <cit.>. Instead, q-learning computes the norm of the current value, thus involving \ud835\udcaa() additional operations at most compared to vanilla q-learning.\n\n\n\nWe set the convergence of q-learning below. To prove it, we first construct an Bellman operator over state-action values, then establish the robust q-function as its fixed point. The rest of the proof relies on stochastic approximation theory <cit.> \u2013 see Appx.\u00a0<ref>. Bellman operators for q-values are similarly defined in <cit.>. However, the robust q-value is established as a fixed point without proof there, whereas we show a formal equivalence in Appx.\u00a0<ref>. Moreover, the q-learning algorithm derived in <cit.> is essentially a value iteration method that relies on a known nominal model. Here, q-learning is model-free and guaranteed to converge to the robust q-function, as stated below. \n\n\n\nFor any (s,a)\u2208, let a sequence of step-sizes (\u03b2_t(s,a))_t\u2208 satisfying 0\u2264\u03b2_t(s,a)\u2264 1, \u2211_t\u2208\u03b2_t(s,a) = \u221e and \u2211_t\u2208\u03b2_t^2(s,a)< \u221e. Then, the q-learning algorithm as given in Alg.\u00a0<ref> converges almost surely to the optimal robust q-function. \n\n\nThe assumption \u2211_t\u2208\u03b2_t(s,a) = \u221e, \u2200 (s,a)\u2208, implicitly requires that we must visit all state-action pairs infinitely often, which is a standard conjecture for convergence proofs <cit.>. Also, we emphasize that even in the (s,a)-rectangular case, the q-function q^\u03c0, from Def.\u00a0<ref> associated with v^\u03c0, is generally not the same as the robust q-function q^\u03c0, obtained from the robust Bellman operators on state-action values (see proof in Appx.\u00a0<ref>). This is similar to the regularized MDP setting where in general, the regularized value v^\u03c0,\u03a9 does not equal q^\u03c0,\u03a9\u00b7\u03c0 or equivalently, q^\u03c0,\u03a9\u00b7\u03c0 does not necessarily correspond to the fixed point of a regularized Bellman operator <cit.>. Instead, the regularized action-value function is solely defined from the regularized value, itself being a fixed point of the regularized Bellman operator. The same phenomenon arises in the robust setting under s-rectangular uncertainty, and in the case regardless of rectangularity. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Learning on a grid\n\n\nAs proof of concept, we perform experiments in a tabular environment. Here, our goals are the following: (i) numerically illustrate the convergence of q-learning; (ii) highlight its computational advantage over robust q-learning concurrently with its robustness properties.\n\nWe consider a Mars Rover domain as in <cit.>. The objective is to find the shortest path to a goal state in a 10\u00d7 10 grid. However, taking a shorter path implies higher risk: the rover has a greater chance to hit a rocket and get a negative reward. The transition function is stochastic: the agent moves to the chosen direction with probability 1-\u03f5, and randomly otherwise. At each step, it receives a small penalty r_step. An episode terminates whenever the rover reaches the goal state or hits a rock. The two scenarios yield a reward of r_success and r_fail respectively. We thus have r_success > 0 > r_step > r_fail. We compare our q-learning algorithm with two baselines: vanilla and robust q-learning. Vanilla is the standard method that ignores model uncertainty and assumes the reward and dynamics are fixed. Robust q-learning trains a robust optimal policy using robust Bellman updates as in Eq.\u00a0(<ref>), thus requiring solving an optimization problem at each iteration <cit.>. \n\nFig.\u00a0<ref> shows the convergence plot across iteration steps for the three agents: vanilla, robust and . All of them have similar sample complexity and fulfill the task within 100 iteration steps. The difference between them arises when we look at the time complexity of each algorithm. As we can see in Fig.\u00a0<ref>, robust q-learning takes more than 2 minutes to converge, whereas vanilla and q-learning achieve the highest reward within 4 seconds (see also Fig.\u00a0<ref> in Appx.\u00a0<ref>). Similarly, we calculated the average time necessary to perform one learning step in each algorithm: one update takes 7.7\u00b1 5.9 \u00d7 10^-6 seconds to run, which is slightly slower than vanilla with 1.24\u00b1 0.89\u00d7 10^-6 seconds. On the other hand, a robust q-update takes 3 \u00b1 0.9 \u00d710^-2 seconds, thus representing 10^4 higher cost than the other two approaches. This highlights the clear advantage of over robust q-learning in terms of computational cost. To check robustness, after training, we evaluate each policy under varying dynamics. In particular, we increase the value of \u03f5 to make the environment more adversarial. Fig.\u00a0<ref> shows that the policy performs similarly to the robust one under more adversarial transitions when \u03f5 tends to 1, both being less sensitive than vanilla.\n\n\n  \n\n\n\n \u00a7.\u00a7 Scaling q-learning\n\n\nThe current expression of TD (line 6 of Alg.\u00a0<ref>) suggests that we have access to the whole q-table for computing the current value's norm. This no longer applies when the state space is infinite or even continuous. Instead, we need to estimate the norm based on sampled observations. We thus keep track of a replay buffer that memorizes and updates past information online. At each iteration, we randomly extract a batch \u212c_t\n\nof samples thanks to which we derive an empirical estimate of the norm. Formally, v_t_\u212c_t^2 := \u2211_s\u2208\u212c_tv_t(s)^2, \nwhere the index in \u00b7_\u212c_t indicates the empirical nature of the norm. Finally, our approximate setting motivates us to stabilize value norm estimates. Thus, in the same spirit as <cit.>, we use a moving average mixing the previous estimate with the current one, at iteration t+1, the value norm squared is given by \u03b2v_t_\u212c_t^2 + (1-\u03b2)v_t+1_\u212c_t+1^2.  \nTo evaluate the reliability of our norm estimate, we compared it to an oracle. Practically, in Mars Rover, we stored the tabular value function obtained after convergence of q-learning, then measured how the norm estimated from batches evolves across iteration steps.[To avoid state duplicates in the norm expression (which may apply when q-learning starts converging), we further re-normalized each state by its number of occurrences: v_t_\u212c_t = \u2211_s\u2208\u212c_tn_s/*\u212c_tv_t(s)^2, where n_s is the number of instances of state s\u2208 in the batch.] In Fig.\u00a0<ref>, we see that decreasing \u03b2 improves stability at the expense of convergence speed. This trade-off similarly occurs in <cit.>, although in the different context of conservative policy iteration. We could also think of a time-adaptive parameter \u03b2 to take the best of both worlds, but leave this for future work.\n\n\n\n \n\n\n\u00a7 DEEP LEARNING\n\n\n\n\n\n \nWe are now able to scale tabular q-learning to a deep variant we name double DQN (DDQN) and compare it to vanilla and robust baselines. DDQN (resp. robust DDQN) is similar to DDQN <cit.>, except that it minimizes an TD (resp. robust TD) when updating the q-network q_\u03b8. Defining the loss as:\n\n    l_\u212c_t(\u03b8_t) = 1/\u212c_t\u2211_(s_j,a_j,s'_j,r_j)\u2208\u212c_t(y_j^baseline - q_\u03b8_t(s_j, a_j))^2,\n\nthe target variable is:\n\n    y_j^=r_j - \u03b1^r+\u03b3 q_\u03b8_t-1(s'_j,_b\u2208q_\u03b8'_t-1(s_j',b)) \n        -\u03b3\u03b1^P*q_\u03b8_t-1(\u00b7,_b\u2208q_\u03b8'_t-1(\u00b7,b))_\u212c_t\n\nand the robust target variable is:\n\n    y_t^robust=   r_j-\u03c3_\u211b(s_j,a_j)(-1)+\u03b3 q_\u03b8_t-1(s_j',_b\u2208q_\u03b8'_t-1(s'_j,b))\n       -\u03b3\u03c3_\ud835\udcab(s_j,a_j)(-q_\u03b8_t-1(\u00b7,_b\u2208q_\u03b8'_t-1(\u00b7,b))),\n\nwhere \u03b8 and \u03b8' denote the weights of the q-network and the target q-network respectively. Recall that we follow the method described in Sec.\u00a0<ref> to estimate the norm in Eq.\u00a0(<ref>).\nFor the three algorithmic variants, DDQN <cit.>, robust DDQN <cit.> and DDQN, we use a fully connected q-network with an input size corresponding to the dimension of the state space, 2 hidden layers of size 256, and an output size corresponding to the dimension of the action space. \n\nWe selected three physical environments from OpenAI Gym: Cartpole, Acrobot, and Mountaincar <cit.>. In Cartpole, the longer the episode, the higher the cumulative reward: at each step, the agent receives a reward of 1 if it maintains a small inclination angle and stays close to the starting x-position, and 0 otherwise. Oppositely, in Acrobot and MountainCar, the longer the episode, the lower the cumulative reward: the agent incurs a negative reward of -1 if it did not reach the goal area, and 0 otherwise\u2013in which case the episode terminates. In each environment, the underlying transition model is directly affected by the physical properties assigned to the agent. Therefore, changing the environment properties implicitly introduces transition uncertainty into the MDP. \n\nWe train the three agents on one nominal environment and five different seeds. For a fair comparison, each seed set is taken to be the same for vanilla, robust and DDQN. Robust and DDQN are trained under the same uncertainty level, namely, \u03b1^P=\u03b1^r=10^-4. Fig.\u00a0<ref> shows that all three agents converge to similar performance, except in Mountaincar where DDQN outperforms both vanilla and robust DDQN. \n\n\n\n\n\nTo check the computational advantage of DDQN over robust DDQN, we calculate the average time each algorithm takes to perform one update of the q-network. As we see in Tab.\u00a0<ref>, one learning step of robust DDQN is slower than one update by an order of magnitude. This is expected, as robust DDQN must solve an optimization problem at each step. On the other hand, one update is approximately four times slower than vanilla because of the additional computations it requires. This confirms the results we obtained previously for MPI and q-learning: robust updates take much longer than updates, themselves being slightly slower than standard, non-robust updates.  \n\n\n\n\n \n\n\nWe finally aim to check the robustness of each algorithm to unseen or changing dynamics. After training, we select two environment parameters across a range of values and evaluate the average performance over several episodes run under the corresponding dynamics. We repeat the same procedure for all seeds and environments. Fig.\u00a0<ref> displays the performance obtained by each agent undergoing such treatment. In all environments, achieves similar performance as robust DDQN. Both exhibit more robust behavior than vanilla DDQN except for mass pole changes in Cartpole, where the three algorithms demonstrate similar results. We also notice that in Mountaincar, is much more robust to changing gravity than the other two agents. To additionally evaluate their robustness to shock, we conduct the following experiment: each agent starts the task under the same nominal parameters as those it has been trained on but at time step t=20, the dynamics abruptly change and remain in this tweaked environment until the episode ends. As we can see in Fig.\u00a0<ref>, and robust DDQN achieve higher performance than vanilla DDQN as the change becomes more abrupt or equivalently, as we look further away from the cell corresponding to the nominal parameters. \n\n\n \n \n\n\n\u00a7 RELATED WORK\n\n\n\nIn statistical learning, regularization has long been used as a computationally low-cost tool for reducing over-fitting  <cit.>. Later on, the theory of robust optimization <cit.> has enabled establishing formal connections between regularization and robustness in standard learning settings such as support vector machines <cit.>, logistic regression <cit.> or maximum likelihood estimation <cit.>. As stated in Sec.\u00a0<ref>, these represent particular RL problems as they concern a single-stage decision-making process. In that regard, the generalization of robustness-regularization duality to sequential decision-making has seldom been studied in the RL literature. \n\nIt is only recently that policy regularization started being viewed from a robustness perspective <cit.>. In <cit.>, regularization is applied to the dual objective instead of the primal. This has two shortcomings: (i) It prevents the formulation of regularized Bellman operators and dynamic programming methods; (ii) The feasible set is that of occupancy measures, so the connection with standard policy regularization remains unclear. Whilst <cit.> address this shortcoming by providing relevant reward sets, their focus on the dual problem yields larger uncertainty. As mentioned in Sec.\u00a0<ref>, although it is consistent with our results at optimality, using approximate solvers may result in overly conservative performance. Moreover, the optimization problem studied there is unrelated to robust dynamic programming, which hinders the facilitation of robust RL. Also, both of these works focus on reward robustness. Differently, <cit.> address reward and transition uncertainty by showing that policies with maximum entropy regularization solve a particular type of robust MDP. Yet, their analysis separately treats the uncertainty on P and r, which questions the robustness of the resulting policy when the whole model (P,r) is adversarial. Moreover, the dual relation they establish between entropy regularization and transition-robust MDPs is weak and applies to specific uncertainty sets. Both of these works treat robustness as a side-effect of regularization more than an objective on its own, whereas we aim to do the opposite, namely, use regularization to solve robust RL problems. \n\nTwo works that do use regularization as a tool for achieving robust policies are <cit.>. Through distributionally robust MDPs, <cit.> show upper and lower bounds between transition-robustness and regularization. There again, duality is weak and reward uncertainty is not addressed. Moreover, since the exact regularization term has no explicit form, it is usable through its upper bound only. Finally, regularization is applied on the mean of several value functions v^\u03c0_(P\u0302_i,r), where each P\u0302_i is a transition model estimated from an episode run. Computing this quantity requires as many policy evaluations as the number of model estimates available, which results in a linear complexity blowup at least. <cit.> extend the conference version of this work <cit.> by additionally constraining all kernels in P_0+ to reside in a probability simplex. This yields less conservative policies at the expense of additional computational cost. Moreover, similarly as <cit.>, <cit.> focus on model-based methods, whereas we provably extend regularization to model-free learning. \n\nPrevious studies analyze robust planning algorithms to provide convergence guarantees. The works <cit.> propose robust value iteration, while <cit.> introduce robust policy iteration. <cit.> generalize both schemes by proposing a robust MPI and determine the conditions under which it converges. The polynomial time within which all these works guarantee a robust solution is often insufficient, as the complexity of a Bellman update grows cubically in the number of states <cit.>. \n\nIn order to reduce the time complexity of robust planning algorithms, <cit.> propose faster methods able to compute robust Bellman updates in \ud835\udcaa(log()) operations for \u2113_1-constrained uncertainty sets, while the algorithms from <cit.> require  \ud835\udcaa(log()) operations for \u2113_\u221e-robust updates. Advantageously, our regularization approach reduces each such update to its standard, non-robust complexity of \ud835\udcaa().\nMoreover, although <cit.> address both (s,a) and s-rectangular uncertainty sets, sets that are independently defined over each state-action pair or state only, they focus on transition uncertainty, whereas we tackle both reward and transition uncertainties in the general s-rectangular case. Finally, their contribution relies on LP formulations that necessitate restricting to one predefined norm while our method applies to any norm. This may come from the fact that our main Theorems (Thms.\u00a0<ref> and <ref>) use Fenchel-Rockafellar duality <cit.>, a generalization of LP duality (see Appx.\u00a0<ref> and <ref>). More recently, <cit.> proposed a first-order method to accelerate robust value iteration under s-rectangular uncertainty sets that are either ellipsoidal or KL-constrained, while <cit.> provide an algorithmic speedup for general f-divergence robust MDPs. To our knowledge, our study is the first one reducing the time complexity of robust to standard planning when the uncertainty set is s-rectangular and constrained with an arbitrary norm. \n\nDespite its theoretical guarantees, robust RL has been sparingly applied to deep settings. We believe this is due to its computational challenges, which this work aims to address. As mentioned in Sec.\u00a0<ref>, previous methods require samples from several models to train a robust policy <cit.>. Besides their increased sample complexity, these parallel simulations bind the learning process to finite uncertainty sets. Differently, the robust fitted value iteration method of <cit.> learns a robust policy based on the nominal system's observations only. It derives the worst-case model in closed form, so it can iterate over the value function in a standard way. Yet, the dynamics should be deterministic and satisfy structural assumptions while the nominal model needs to be known. In contrast, we study a general MDP setting without structure in the environment and propose a model-free algorithm that is guaranteed to converge to a robust optimal policy. \n\nRecently, <cit.> leveraged the regularization technique to derive robust policies. There, as in <cit.>, an Bellman equation is provided for q-values without a formal equivalence to its robust analog being established. In fact, the uncertainty they consider here is not rectangular: the transition model is parameterized by uncertain parameters (like in <cit.>), but these lie in a ball regardless of the state or action. Finally, while several notions of robustness may be addressed in deep RL methods, action perturbation <cit.>, execution delay <cit.> or adversarial observations <cit.>, these are outside the scope of our study which focuses on robustness w.r.t. the transition and/or reward functions. \n \n\n\n\u00a7 CONCLUSION AND FUTURE WORK\n\n\n \nIn this work, we established a strong duality between robust MDPs and twice regularized MDPs. This revealed that the regularized MDPs of <cit.> are in fact robust MDPs with uncertain reward, which enabled us to derive a policy-gradient theorem for reward-robust MDPs. When extending this robustness-regularization duality to general robust MDPs, we found that the regularizer depends on the value function besides the policy. We thus introduced MDPs, a generalization of regularized MDPs with both policy and value regularization. The related Bellman operators lead us to propose a converging MPI (resp. q-learning) algorithm that achieves the optimal robust value function within a similar computing time as standard MPI (resp. q-learning), as confirmed by our tabular experiments. Then, we devised a scalable method to estimate the value regularizer in an approximate setting. Finally, we tested the resulting DDQN algorithm on physical domains. \n\nThis study settles the theoretical foundations for scalable robust RL. We should note that our results naturally extend to continuous but compact action spaces in the same manner as standard MDPs do <cit.>. Theoretical extension to infinite state space would be more involved because of the state-dependent regularizer in MDPs. In fact, it would be interesting to study the MDP setting under function approximation, as such approximation would have a direct effect on the regularizer. Similarly, one could analyze approximate dynamic programming for MDPs in light of its robust analog <cit.>. \nApart from its practical effect, we believe our work opens the path to more theoretical contributions in robust RL. For example, extending MPI to the approximate case <cit.> would be an interesting problem to solve because of the evaluation operator being non-linear. So would be a sample complexity analysis for MDPs with a comparison to robust MDPs <cit.>. Another line of research is to extend policy-gradient to MDPs, as this would avoid parallel learning of adversarial models <cit.> and be very useful for continuous control.\n\n\n\nWe would like to thank Raphael Derman for his useful suggestions that improved the clarity of the text, and Navdeep Kumar for noticing a mistake in the proof of Prop.\u00a0<ref> from <cit.>. Although it has no effect on our results, this manuscript fixes it. Thanks to Roee Ben-Shlomo for taking part in the numerical experiments. Thanks also to Stav Belogolovsky for reviewing a previous version of this paper. Funding in direct support of this work: ISF grant. \n\n\n\n\n\n\n\nThis appendix provides proofs for all of the results stated in the paper. \nWe first recall the following theorem used in the sequel and referred to as Fenchel-Rochafellar duality <cit.>.\n\n[Fenchel-Rockafellar duality]\n\nLet X, Y two Euclidean spaces, f:X\u2192 and g: Y\u2192 two proper, convex functions, and A: X\u2192 Y a linear mapping such that \n0\u2208core(dom(g) - A(dom(f))).[Given C\u2286^, we say that x\u2208core(C) if for all d\u2208^ there exists a small enough t\u2208 such that x+td\u2208 C <cit.>.]\nThen, it holds that\n\n    min_x\u2208 Xf(x) + g(A x) = max_y\u2208 Y -f^*(-A^*y) - g^*(y).\n\n\n\n\n\n\n\n\u00a7 REWARD-ROBUST MDPS\n\n\n\n \u00a7.\u00a7 Proof of Proposition\u00a0<ref>\n\n\n\n\nFor any policy \u03c0\u2208\u0394_^, the robust value function v^\u03c0, is the optimal solution of the robust optimization problem:\n\n    max_v\u2208^v, \u03bc_0 s. t.  v\u2264 T_(P,r)^\u03c0v  for all    (P,r)\u2208. P_\n\n\n\n\n\n\nLet v^* an optimal point of (<ref>). By definition of the robust value function, v^\u03c0,  = T^\u03c0, v^\u03c0,  =  min_(P,r)\u2208 T_(P,r)^\u03c0v^\u03c0,. In particular, v^\u03c0, \u2264 T_(P,r)^\u03c0v^\u03c0, for all (P,r)\u2208, so the robust value is feasible and by optimality of v^*, we get v^*,\u03bc_0\u2265v^\u03c0, , \u03bc_0. Now, we aim to show that any feasible v\u2208^ satisfies v\u2264 v^\u03c0, . \nLet an arbitrary \u03f5 > 0. By definition of T^\u03c0,, there exists (P_\u03f5,r_\u03f5)\u2208 such that\n\n    T^\u03c0, v^\u03c0,  +\u03f5  > T_(P_\u03f5,r_\u03f5)^\u03c0v^\u03c0, .\n\nThis yields:\n\n    v - v^\u03c0,    = v - T^\u03c0, v^\u03c0,    [v^\u03c0,  = T^\u03c0, v^\u03c0, ]\n       <  v + \u03f5 - T_(P_\u03f5,r_\u03f5)^\u03c0v^\u03c0,    [By Eq.\u00a0(<ref>)]\n       \u2264 T^\u03c0, v+ \u03f5 - T_(P_\u03f5,r_\u03f5)^\u03c0v^\u03c0,    [v  is feasible for (<ref>)]\n       \u2264 T_(P_\u03f5,r_\u03f5)^\u03c0v+ \u03f5 - T_(P_\u03f5,r_\u03f5)^\u03c0v^\u03c0,    [T^\u03c0, v \u2264 T_(P,r)^\u03c0v  for all  (P,r)\u2208]\n       = T_(P_\u03f5,r_\u03f5)^\u03c0(v-v^\u03c0, )+ \u03f5.    [By linearity of T_(P_\u03f5,r_\u03f5)^\u03c0]\n\nThus, v - v^\u03c0, \u2264 T_(P_\u03f5,r_\u03f5)^\u03c0(v-v^\u03c0, ) + \u03f5, which we iteratively apply as follows:\n\n    v - v^\u03c0,    \u2264 T_(P_\u03f5,r_\u03f5)^\u03c0(v-v^\u03c0, ) + \u03f5\n       \u2264 T_(P_\u03f5,r_\u03f5)^\u03c0(T_(P_\u03f5,r_\u03f5)^\u03c0(v-v^\u03c0, ) + \u03f5) + \u03f5   [u\u2264 w T_(P_\u03f5,r_\u03f5)^\u03c0u\u2264 T_(P_\u03f5,r_\u03f5)^\u03c0w]\n       =(T_(P_\u03f5,r_\u03f5)^\u03c0)^2(v-v^\u03c0,  ) + \u03b3\u03f5 + \u03f5\n       \u2264 (T_(P_\u03f5,r_\u03f5)^\u03c0)^2(T_(P_\u03f5,r_\u03f5)^\u03c0(v-v^\u03c0,  ) + \u03f5)  + \u03b3\u03f5 + \u03f5\n         \u22ee\n       \u2264 (T_(P_\u03f5,r_\u03f5)^\u03c0)^n+1(v-v^\u03c0,  )  + \u2211_k = 0^n\u03b3^k\u03f5\n       = (T_(P_\u03f5,r_\u03f5)^\u03c0)^n+1(v-v^\u03c0,  )  + 1-\u03b3^n+1/1-\u03b3\u03f5.\n\nBy definition of the sup-norm and applying the triangular inequality we obtain:\n\n    v - v^\u03c0,    \u2264*(T_(P_\u03f5,r_\u03f5)^\u03c0)^n+1(v-v^\u03c0,  )_\u221e + 1-\u03b3^n+1/1-\u03b3\u03f5\n       \u2264\u03b3^n+1v - v^\u03c0, _\u221e + 1-\u03b3^n+1/1-\u03b3\u03f5   [T_(P_\u03f5,r_\u03f5)^\u03c0 is \u03b3-contracting]\n\nSetting n\u2192\u221e yields v - v^\u03c0, \u2264\u03f5/1-\u03b3. Since both \u03f5>0 and v were taken arbitrarily, v^* - v^\u03c0, \u2264 0, while we have already shown that v^*,\u03bc_0\u2265v^\u03c0, , \u03bc_0.\nBy positivity of the probability distribution \u03bc_0, it results that v^*,\u03bc_0 = v^\u03c0, , \u03bc_0, and since \u03bc_0 >0, v^\u03c0,  = v^*.  \n\n\n\n\n \u00a7.\u00a7 Proof of Theorem <ref>\n\n\n\n[Reward-robust MDP]\nAssume that = {P_0}\u00d7  (r_0+ ). Then, for any policy \u03c0\u2208\u0394_^, the robust value function v^\u03c0, is the optimal solution of the convex optimization problem:\n\n    max_v\u2208^v, \u03bc_0 s. t.  v(s)\u2264 T_(P_0,r_0)^\u03c0v(s) - \u03c3__s(-\u03c0_s)  for all  s\u2208.\n\n\n\n\nFor all s\u2208, define: F(s):= max_(P, r)\u2208{v(s) - r^\u03c0(s) - \u03b3 P^\u03c0v(s) }.\nIt corresponds to the robust counterpart of (<ref>) at s\u2208. Thus, the robust value function v^\u03c0, is the optimal solution of:\n\n    max_v\u2208^v, \u03bc_0 s. t.  F(s)\u2264 0  for all    s\u2208.\n\nBased on the structure of the uncertainty set = {P_0}\u00d7  (r_0+ ), we compute the robust counterpart:\n\n    F(s)    = max_r'\u2208 r_0 + {v(s) - r'^\u03c0(s) - \u03b3 P_0^\u03c0v(s) }\n       =max_r': r' = r_0 + r, r\u2208{v(s) - r'^\u03c0(s) - \u03b3 P_0^\u03c0v(s) }\n       =max_ r\u2208{v(s) - (r_0^\u03c0(s)  + r^\u03c0(s))- \u03b3 P_0^\u03c0v(s) }   [(r_0 + r)^\u03c0 = r_0^\u03c0 + r^\u03c0  \u2200\u03c0\u2208\u0394_^]\n       = max_ r\u2208{v(s) - r^\u03c0(s)-r_0^\u03c0(s) -  \u03b3 P_0^\u03c0v(s) }\n       = max_ r\u2208{v(s) - r^\u03c0(s)-T_(P_0,r_0)^\u03c0v(s) }   [T_(P_0,r_0)^\u03c0v(s)  = r_0^\u03c0(s) +  \u03b3 P_0^\u03c0v(s)]\n       = max_r\u2208{- r^\u03c0(s)} + v(s) - T_(P_0,r_0)^\u03c0v(s)\n       = max_r\u2208^{- r^\u03c0(s) - \u03b4_(r')}+ v(s) - T_(P_0,r_0)^\u03c0v(s)\n       = -min_r\u2208^{ r^\u03c0(s) +\u03b4_(r)}+ v(s) - T_(P_0,r_0)^\u03c0v(s)\n       = -min_r\u2208^{r_s, \u03c0_s +\u03b4_(r)}+ v(s) - T_(P_0,r_0)^\u03c0v(s).    [r^\u03c0(s) = r_s, \u03c0_s]\n\nBy the rectangularity assumption, = \u00d7_s\u2208_s and for all r:= (r_s)_s\u2208\u2208^, we have\n\u03b4_(r) = \u2211_s'\u2208\u03b4__s'(r_s'). As such, \n\n    F(s)\n           = -min_r\u2208^{r_s, \u03c0_s +\u2211_s'\u2208\u03b4__s'(r_s')}+ v(s) - T_(P_0,r_0)^\u03c0v(s)\n       = -min_r\u2208^{r_s, \u03c0_s +\u03b4__s(r_s)}+ v(s) - T_(P_0,r_0)^\u03c0v(s),\n\nwhere the last equality holds since the objective function is minimal if and only if r_s\u2208_s. \n\nWe now aim to apply Fenchel-Rockafellar duality to the minimization problem. Let the function\nf :  ^\u2192 defined as r_s  \u21a6r_s, \u03c0_s, and consider \nthe support function \u03b4__s: ^\u2192 together with\nthe identity mapping \ud835\udc08\ud835\udc1d_: ^\u2192^. \nClearly, dom(f) = ^, dom(\u03b4__s) = _s, and dom(\u03b4__s) - \ud835\udc08\ud835\udc1d_(dom(f)) = _s - ^ = ^. Therefore, core(dom(\u03b4__s) - A(dom(f))) = core(^) = ^ and 0\u2208^. We can thus apply Fenchel-Rockafellar duality: noting that \ud835\udc08\ud835\udc1d_ = (\ud835\udc08\ud835\udc1d_)^* and (\u03b4__s)^*(y) = \u03c3__s(y), we get\n\n    min_r_s\u2208^{ f(r_s) + \u03b4__s(r_s)}   = -min_y\u2208^{f^*(-y)+(\u03b4__s)^*(y)} = -min_y\u2208^{f^*(-y)+\u03c3__s(y)}.\n\nIt remains to compute\n\n    f^*(-y) \n        = max_r_s\u2208^ - r_s, y -r_s, \u03c0_s \n        = max_r_s\u2208^r_s, -y-\u03c0_s\n        =\n        0  if  -y-\u03c0_s = 0\n    \n        +\u221e otherwise,\n\nand obtain\n\n    F(s)    = min_y\u2208^{f^*(-y)+\u03c3__s(y)} + v(s) - T_(P_0,r_0)^\u03c0v(s)\n        = \u03c3__s(-\u03c0_s) + v(s) - T_(P_0,r_0)^\u03c0v(s).\n\nWe can thus rewrite the optimization problem (<ref>) as:\n\n    max_v\u2208^v, \u03bc_0 s. t. \u03c3__s(-\u03c0_s) + v(s) - T_(P_0,r_0)^\u03c0v(s)\u2264 0  for all    s\u2208,\n\nwhich concludes the proof.\n\n\n\n\n \u00a7.\u00a7 Proof of Corollary <ref>\n\n\n\n\nLet \u03c0\u2208\u0394_^ and = {P_0}\u00d7  (r_0+ ). Further assume that for all s\u2208, the reward uncertainty set at s is _s:= {r_s\u2208^: r_s\u2264\u03b1_s^r}. Then,  the robust value function v^\u03c0, is the optimal solution of the convex optimization problem:\n\n    max_v\u2208^v, \u03bc_0 s. t.  v(s)\u2264 T_(P_0,r_0)^\u03c0v(s) - \u03b1_s^r\u03c0_s for all  s\u2208.\n\n\n\n\nWe evaluate the support function:\n\n    \u03c3__s(-\u03c0_s)    =  max_r_s\u2208^: r_s\u2264\u03b1_s^r r_s,-\u03c0_s(1)=\u03b1_s^r -\u03c0_s = \u03b1_s^r \u03c0_s,\n\nwhere equality (1) holds by definition of the dual norm. \nApplying Thm.\u00a0<ref>, the robust value function v^\u03c0, is the optimal solution of:\nmax_v\u2208^v, \u03bc_0 s. t. \u03b1_s^r \u03c0_s + v(s) - T_(P_0,r_0)^\u03c0v(s)\u2264 0  for all  s\u2208,\nwhich concludes the proof. \n\nBall-constraint with arbitrary norm.  In the case where reward ball-constraints are defined according to an arbitrary norm \u00b7_a with dual norm \u00b7_a^*, the support function becomes:\n\n    \u03c3__s(-\u03c0_s)    =  max_r_s\u2208^: r_s_a\u2264\u03b1_s^r r_s,-\u03c0_s=\n       \u03b1_s^r -\u03c0_s_a^* = \u03b1_s^r \u03c0_s_a^*.\n\n\n\n\n\n\n \u00a7.\u00a7 Related Algorithms: Uncertainty sets from regularizers\n\n \n\n\n\nNegative Shannon entropy. Each (s,a)-reward uncertainty set is _s,a^NS(\u03c0):= [ ln(1/\u03c0_s(a)), +\u221e).\nWe compute the associated support function:\n\n    \u03c3__s^NS(\u03c0)(-\u03c0_s)    = max_r_s\u2208_s^NS(\u03c0)r_s, -\u03c0_s\n       = max_r(s,a'):  r(s,a')\u2208_s,a'^NS(\u03c0), a'\u2208\u2211_a\u2208 -r(s,a)\u03c0_s(a) \n       = max_r(s,a'):  r(s,a')\u2265ln(1/\u03c0_s(a)), a'\u2208\n        -\u2211_a\u2208\u03c0_s(a)r(s,a) \n       =  \u2211_a\u2208\u03c0_s(a)ln(\u03c0_s(a)),\n\nwhere the last equality results from the fact that \u03c0_s\u2265 0, and -r(s,a)\u03c0_s(a) is maximal when r(s,a) is minimal. \nWe thus obtain the negative Shannon entropy.\n\nKL divergence. Similarly, given d\u2208\u0394_, let _s,a^KL(\u03c0):= ln(d(a)) + _s,a^NS(\u03c0)  \u2200 (s,a)\u2208. Then\n\n    \u03c3__s^KL(\u03c0)(-\u03c0_s) \n           =max_r(s,a'):  r(s,a')\u2208_s,a'^KL(\u03c0), a'\u2208\u2211_a\u2208 -r(s,a)\u03c0_s(a)\n       =max_r(s,a')+ ln(d(a)):\n      r(s,a')\u2208_s,a'^NS(\u03c0), a'\u2208\u2211_a\u2208 -r(s,a)\u03c0_s(a)\n       = max_r(s,a'): \n    \n        r(s,a')\u2208_s,a'^NS(\u03c0), a'\u2208\u2211_a\u2208 - (r(s,a) + ln(d(a))\u03c0_s(a)\n       = max_r(s,a'): \n    \n        r(s,a')\u2208_s,a'^NS(\u03c0), a'\u2208{-\u2211_a\u2208\u03c0_s(a) r(s,a) }-\u2211_a\u2208\u03c0_s(a) ln(d(a))\n       = \u2211_a\u2208\u03c0_s(a)ln(\u03c0_s(a)) - \u2211_a\u2208\u03c0_s(a)ln(d(a)),\n\nwhere the last equality uses Eq.\u00a0(<ref>). \nWe thus recover the  KL divergence \u03a9(\u03c0_s)= \u2211_a\u2208\u03c0_s(a)ln(\u03c0_s(a)/d(a)).\n\nNegative Tsallis entropy. Given _s,a^T(\u03c0):= [1-\u03c0_s(a)/2, +\u221e)  \u2200 (s,a)\u2208, we compute:\n\n    \u03c3__s^T(\u03c0)(-\u03c0_s)   =  \n        max_r(s,a'):  r(s,a')\u2208_s,a'^T(\u03c0), a'\u2208\u2211_a\u2208 -r(s,a)\u03c0_s(a) \n       = max_r(s,a'):  r(s,a')\u2208[1-\u03c0_s(a')/2, +\u221e), a'\u2208\u2211_a\u2208 -r(s,a)\u03c0_s(a)\n       =  \u2211_a\u2208 -1-\u03c0_s(a)/2\u03c0_s(a)\n       = -1/2\u2211_a\u2208\u03c0_s(a) + 1/2\u2211_a\u2208\u03c0_s(a)^2\n        = -1/2+ 1/2\u03c0_s^2,\n\nwhere Eq.\u00a0(<ref>) also comes from the fact that \u03c0_s\u2265 0, and -r(s,a)\u03c0_s(a) is maximal when r(s,a) is minimal. \nWe thus obtain the negative Tsallis entropy \u03a9(\u03c0_s) = 1/2(\u03c0_s^2-1).\n\nThe reward uncertainty sets associated to both KL and Shannon entropy are similar, as the former amounts to translating the latter \nby a negative constant (translation to the left). As such, both yield reward values that can be either positive or negative. This is not the case of the negative Tsallis, as its minimal reward is 0, attained for a deterministic action policy, when \u03c0_s(a) = 1.\n\nTable <ref> summarizes the properties of each regularizer. \nFor the Tsallis entropy, we denote by \u03c4: ^\u2192 the function q_s\u21a6\u2211_a\u2208\ud835\udd04(q_s)q_s(a)-1/\ud835\udd04(q_s), where \ud835\udd04(q_s)\u2286 is a subset of actions:\n\ud835\udd04(q_s) = {a\u2208: 1 + i q_s(a_(i)) > \u2211_j=0^iq_s(a_(j)), i \u2208{1,\u22ef, }}, and a_(i) is the action with the  i-th maximal value<cit.>.\n\n \n\n\n\n\n \u00a7.\u00a7 Proof of Proposition\u00a0<ref>\n\n\n\n\nAssume that = {P_0}\u00d7  (r_0+ ) with _s= {r_s\u2208^: r_s\u2264\u03b1_s^r}. Then, the gradient of the reward-robust objective J_(\u03c0):= v^\u03c0, , \u03bc_0 is given by\n\n    \u2207 J_(\u03c0) = \ud835\udd3c_(s,a)\u223c\u03bc_\u03c0[ \u2207ln\u03c0_s(a)(q^\u03c0, (s,a) - \u03b1_s^r\u03c0_s(a)/\u03c0_s)],\n\nwhere \u03bc_\u03c0 is the occupancy measure under the nominal model P_0 and policy \u03c0. \n\n\nWe prove the following more general result. To establish Prop.\u00a0<ref>, we then set \u03b1_s^P = 0 and apply Thm.\u00a0<ref> to replace v^\u03c0,  = v_\u03c0, and q^\u03c0,  = q^\u03c0,.\n\n \n\nSet \u03a9_v, (\u03c0_s):= \u03c0_s (\u03b1_s^r + \u03b1_s^P \u03b3v). Then, the gradient of the objective J_(\u03c0):= v_\u03c0, , \u03bc_0 is given by\n\n    \u2207 J_(\u03c0) = \ud835\udd3c_s\u223c d_\u03bc_0,\u03c0[ \u2211_a\u2208\u03c0_s(a)\u2207ln\u03c0_s(a)q^\u03c0, (s,a) - \u2207\u03a9_v, (\u03c0_s)],\n\nwhere d_\u03bc_0,\u03c0:=\u03bc_0^\u22a4(\ud835\udc08_ - \u03b3 P_0^\u03c0)^-1, with \u03bc_0\u2208^\u00d7 1 the initial state distribution.\n\n\n\nBy linearity of the gradient operator, \u2207 J_(\u03c0) = \u2207 v^\u03c0, , \u03bc_0. We thus need to compute \u2207 v^\u03c0, . Using the fixed point property of v^\u03c0, w.r.t. the Bellman operator yields:\n\n    \u2207 v^\u03c0, (s)\n    \n        =   \u2207(r_0^\u03c0(s) + \u03b3 P_0^\u03c0v^\u03c0, (s) - \u03a9_v, (\u03c0_s))\n    \n        =   \u2207(\u2211_a\u2208\u03c0_s(a)(r_0(s, a) + \u03b3P_0(\u00b7| s,a), v^\u03c0, ) - \u03a9_v, (\u03c0_s))\n    \n        =   \u2211_a\u2208\u2207\u03c0_s(a) (r_0(s, a) + \u03b3P_0(\u00b7| s,a), v^\u03c0, )\n            + \u03b3\u2211_a\u2208\u03c0_s(a)P_0(\u00b7| s,a), \u2207 v^\u03c0,  - \u2207\u03a9_v, (\u03c0_s)   [Linearity of gradient & product rule]\n    \n        =   \u2211_a\u2208\u2207\u03c0_s(a)q^\u03c0, (s,a)+ \u03b3\u2211_a\u2208\u03c0_s(a)P_0(\u00b7| s,a), \u2207 v^\u03c0,  - \u2207\u03a9_v, (\u03c0_s) \n                                                 [q^\u03c0, (s,a) = r_0(s, a) + \u03b3P_0(\u00b7| s,a), v^\u03c0, ]\n    \n        =   \u2211_a\u2208\u03c0_s(a)( \u2207ln\u03c0_s(a)q^\u03c0, (s,a)+ \u03b3P_0(\u00b7| s,a), \u2207 v^\u03c0, ) - \u2207\u03a9_v, (\u03c0_s)\n          [\u2207\u03c0_s = \u03c0_s\u2207ln(\u03c0_s)]\n    \n        =   \u2211_a\u2208\u03c0_s(a)( \u2207ln\u03c0_s(a)q^\u03c0, (s,a) - \u2207\u03a9_v, (\u03c0_s) + \u03b3P_0(\u00b7| s,a), \u2207 v^\u03c0, ).\n\nThus, the components of \u2207 v^\u03c0, are the non-regularized value functions corresponding to the modified reward R(s,a):=  \u2207ln\u03c0_s(a)q^\u03c0, (s,a) - \u2207\u03a9_v, (\u03c0_s). By the fixed point property of the standard Bellman operator, it results that:\n\n    \u2207 v^\u03c0, (s)    =  (\ud835\udc08_ - \u03b3 P_0^\u03c0)^-1(\u2211_a\u2208\u03c0_\u00b7(a)(\u2207ln\u03c0_\u00b7(a)q^\u03c0, (\u00b7,a) - \u2207\u03a9_v, (\u03c0_\u00b7)))(s)\n\nand\n\n    \u2207 J_(\u03c0)    = \u2211_s\u2208\u03bc_0(s)\u2207 v^\u03c0, (s)\n       = \u2211_s\u2208\u03bc_0(s)(\ud835\udc08_ - \u03b3 P_0^\u03c0)^-1(\u2211_a\u2208\u03c0_\u00b7(a)(\u2207ln\u03c0_\u00b7(a)q^\u03c0, (\u00b7,a) - \u2207\u03a9_v, (\u03c0_\u00b7)))(s)\n       = \u2211_s\u2208 d_\u03bc_0, \u03c0(s)(\u2211_a\u2208\u03c0_s(a)\u2207ln\u03c0_s(a)q^\u03c0, (s,a) - \u2207\u03a9_v, (\u03c0_s)),\n\nby definition of d_\u03bc_0, \u03c0.\n\nThe subtraction by \u2207\u03a9_v, (\u03c0_s) also appears in <cit.>. However, here, the gradient includes partial derivatives that depend on both the policy and the value itself. Let's try to compute the gradient of the double regularizer \u03a9_v, (\u03c0_s) = \u03c0_s (\u03b1_s^r + \u03b1_s^P \u03b3v^\u03c0, ). By the chain-rule we have that:\n\n    \u2207\u03a9_v, (\u03c0_s)    = \u2211_a\u2208\u2202\u03a9_v, /\u2202\u03c0_s(a)\u2207\u03c0_s(a)\n       + \u2211_s\u2208\u2202\u03a9_v, /\u2202 v^\u03c0, (s)\u2207 v^\u03c0, (s)\n       =\u2211_a\u2208(\u03b1_s^r + \u03b1_s^P \u03b3v^\u03c0, )\u03c0_s(a)/\u03c0_s\u2207\u03c0_s(a)\n       + \u2211_s\u2208\u03b1_s^P \u03b3\u03c0_sv^\u03c0, (s)/v^\u03c0, \u2207 v^\u03c0, (s).\n\nWe remark here an interdependence between \u2207\u03a9_v, (\u03c0_s) and \u2207 v^\u03c0, (s): computing the gradient \u2207\u03a9_v, (\u03c0_s) requires to know \u2207 v^\u03c0, (s) and vice versa. There may be a recursion that still enables to compute these gradients, which we leave for future work.  \n \n\n\n\n\n\u00a7 GENERAL ROBUST MDPS\n\n\n\n \u00a7.\u00a7 Proof of Theorem <ref>\n\n\n\n[General robust MDP]\nAssume that =   (P_0+ )\u00d7(r_0+ ). Then, for any policy \u03c0\u2208\u0394_^, the robust value function v^\u03c0, is the optimal solution of the convex optimization problem:\n\n    max_v\u2208^v, \u03bc_0 s. t.  v(s)\u2264 T_(P_0,r_0)^\u03c0v(s)  -\u03c3__s(-\u03c0_s) -\u03c3__s(-\u03b3 v\u00b7\u03c0_s)  for all  s\u2208,\n\nwhere [v\u00b7\u03c0_s](s',a):=v(s')\u03c0_s(a)  \u2200 (s',a)\u2208.\n\n\n\nThe robust value function v^\u03c0, is the optimal solution of:\n\n    max_v\u2208^v, \u03bc_0 s. t.  F(s)\u2264 0  for all    s\u2208,\n\nwhere F(s):= max_(P, r)\u2208{v(s) - r^\u03c0(s) - \u03b3 P^\u03c0v(s) } is the robust counterpart of (<ref>) at s\u2208. Let's compute it based on the structure of the uncertainty set  =   (P_0+ )\u00d7(r_0+ ): \n\n    F(s)   = max_(P',r')\u2208 (P_0+ )\u00d7(r_0+ ){v(s) - r'^\u03c0(s) - \u03b3 P'^\u03c0v(s) }\n       = max_P': P' = P_0 + P, P\u2208\n     r': r' = r_0 + r, r\u2208{v(s) - r'^\u03c0(s) - \u03b3 P'^\u03c0v(s) }\n       = max_P\u2208 , r\u2208{v(s) - (r_0^\u03c0(s) +r^\u03c0(s)) - \u03b3 (P_0^\u03c0 + P^\u03c0 )v(s) }  [(P_0 + P)^\u03c0 = P_0^\u03c0 + P^\u03c0,\n                                                           (r_0+r)^\u03c0 = r_0^\u03c0+r^\u03c0]\n       = max_P\u2208, r\u2208{v(s) - r_0^\u03c0(s) -r^\u03c0(s) - \u03b3 P_0^\u03c0v(s) -\u03b3 P^\u03c0 v(s) }\n       = max_P\u2208, r\u2208{v(s) - T_(P_0,r_0)^\u03c0v(s)-r^\u03c0(s) -\u03b3 P^\u03c0 v(s) }  [T_(P_0,r_0)^\u03c0v(s)  = r_0^\u03c0(s) +  \u03b3 P_0^\u03c0v(s)]\n       = max_P\u2208{-\u03b3 P^\u03c0 v(s)} + max_r\u2208{-r^\u03c0(s)} +v(s) - T_(P_0,r_0)^\u03c0v(s) \n       = -min_P\u2208{\u03b3 P^\u03c0 v(s) }- min_r\u2208{r^\u03c0(s)}+v(s) - T_(P_0,r_0)^\u03c0v(s)\n       = -min_P\u2208^\u00d7{\u03b3 P^\u03c0 v(s) + \u03b4_(P)} - min_r\u2208^{r^\u03c0(s) + \u03b4_(r)}\n           +v(s) - T_(P_0,r_0)^\u03c0v(s)\n       = -min_P\u2208^\u00d7{\u03b3P^\u03c0_s, v + \u03b4_(P)  }\n        -min_r\u2208^{r_s, \u03c0_s +\u03b4_(r)}\n           +v(s) - T_(P_0,r_0)^\u03c0v(s).                    [P^\u03c0 v(s) = P^\u03c0_s, v, r^\u03c0(s) = r_s, \u03c0_s]\n\nAs shown in the proof of Thm.\u00a0<ref>, min_r\u2208^{r_s, \u03c0_s +\u03b4_(r)} = min_r_s\u2208^{r_s, \u03c0_s + \u03b4__s(r_s)} thanks to the rectangularity assumption. Similarly, by rectangularity of the transition uncertainty set, for all P:= (P_s)_s\u2208\u2208^, we have\n\u03b4_(P) = \u2211_s'\u2208\u03b4__s'(P_s'). As such, \n\n    min_P\u2208^\u00d7{\u03b3P^\u03c0_s, v + \u03b4_(P)  }   = min_P\u2208^\u00d7{\u03b3P^\u03c0_s, v +  \u2211_s'\u2208\u03b4__s'(P_s')}\n       = min_P_s\u2208^{\u03b3P^\u03c0_s, v +  \u03b4__s(P_s)},\n\nwhere the last equality holds since the objective function is minimal if and only if P_s\u2208_s. \nFinally,\n\n    F(s)    = - min_P_s\u2208^{\u03b3P^\u03c0_s, v +  \u03b4__s(P_s)} - min_r_s\u2208^{r_s, \u03c0_s + \u03b4__s(r_s)}+v(s) - T_(P_0,r_0)^\u03c0v(s).\n\nReferring to the proof of Thm.\u00a0<ref>, we know that\n-min_r\u2208^{r_s, \u03c0_s +\u03b4_(r)} = \u03c3__s(-\u03c0_s), so \n\n    F(s)    = - min_P_s\u2208^{\u03b3P^\u03c0_s, v +  \u03b4__s(P_s)} + \u03c3__s(-\u03c0_s)+v(s) - T_(P_0,r_0)^\u03c0v(s).\n\nLet the matrix v\u00b7\u03c0_s\u2208^ defined as\n[v\u00b7\u03c0_s](s',a):=v(s')\u03c0_s(a) for all (s',a)\u2208. Further define\n \u03c6(P_s) := \u03b3P^\u03c0_s, v, which we can rewrite as\n\u03c6( P_s) =  \u03b3P_s, v\u00b7\u03c0_s.\nThen, we have that:\n\n    min_P_s\u2208^{\u03b3P^\u03c0_s, v +  \u03b4__s(P_s)} = min_P_s\u2208^{\u03c6( P_s) + \u03b4__s(P_s)} = -min_\u2208^{\u03c6^*( -) + \u03c3__s()},\n\nwhere the last equality results from Fenchel-Rockafellar duality and the fact that (\u03b4__s)^* = \u03c3__s.\nIt thus remains to compute the convex conjugate of \u03c6:\n\n    \u03c6^*(-)    = max_P_s\u2208^{P_s, - - \u03c6( P_s) }\n       =  max_P_s\u2208^{P_s,-  -\u03b3P_s, v\u00b7\u03c0_s}\n       =max_P_s\u2208^P_s, - - \u03b3 v\u00b7\u03c0_s\n       = \n       0  if  - - \u03b3 v\u00b7\u03c0_s = 0 \n    \n       +\u221e otherwise,\n\nwhich yields min_\u2208^{\u03c6^*( -) + \u03c3__s()} = \u03c3__s(- \u03b3 v\u00b7\u03c0_s). \nFinally, the robust counterpart rewrites as: F(s) = \u03c3__s(-\u03b3 v\u00b7\u03c0_s)+ \u03c3__s(-\u03c0_s) + v(s) - T_(P_0,r_0)^\u03c0v(s),\nand plugging it into the optimization problem\u00a0(<ref>) yields the desired result.\n\n\n\n\n \u00a7.\u00a7 Proof of Corollary <ref>\n\n\n\n\nAssume that =   (P_0+ )\u00d7(r_0+ ) with _s:= {P_s\u2208^: P_s\u2264\u03b1_s^P} and _s:= {r_s\u2208^: r_s\u2264\u03b1_s^r} for all s\u2208. Then, the robust value function v^\u03c0, is the optimal solution of the convex optimization problem:\n\n    max_v\u2208^v, \u03bc_0 s. t.  v(s)\u2264 T_(P_0,r_0)^\u03c0v(s) -\u03b1_s^r\u03c0_s - \u03b1_s^P\u03b3v\u03c0_s for all  s\u2208.\n\n\n\n\nAs we already showed in Cor.\u00a0<ref>, the support function of the reward uncertainty set is\n\u03c3__s(-\u03c0_s)  = \u03b1_s^r \u03c0_s. For the transition uncertainty set, we similarly have:\n\n    \u03c3__s(-\u03b3 v\u00b7\u03c0_s)    = max_P_s\u2208^: \n    P_s\u2264\u03b1_s^P P_s,-\u03b3 v\u00b7\u03c0_s\n       = \u03b1_s^P -\u03b3 v\u00b7\u03c0_s\n       = \u03b1_s^P \u03b3 v\u00b7\u03c0_s\n       = \u03b1_s^P \u03b3 v\u03c0_s.    [ v\u00b7\u03c0_s^2 = \u2211_(s',a)\u2208(v(s')\u03c0_s(a))^2 \n          = \u2211_s'\u2208v(s')^2\u2211_a\u2208\u03c0_s(a)^2 =  v^2\u03c0_s^2]\n\nNow we apply Thm.\u00a0<ref> and replace each support function by their explicit form to get that the robust value function v^\u03c0, is the optimal solution of:\n\n    max_v\u2208^v, \u03bc_0 s. t.  v(s)\u2264 T_(P_0,r_0)^\u03c0v(s) - \u03b1_s^r\u03c0_s - \u03b1_s^P\u03c0_s\u00b7\u03b3v for all  s\u2208.\n\nBall-constraints with arbitrary norms.  As seen in the proof of Thm.\u00a0<ref> and Cor.\u00a0<ref>, \nfor ball-constrained rewards defined with an arbitrary norm \u00b7_a of dual \u00b7_a^*, the corresponding support function is \u03c3__s(-\u03c0_s) = \u03b1_s^r \u03c0_s_a^*. Similarly, for ball-constrained transitions based on a norm \u00b7_b of dual \u00b7_b^*, we have:\n\n    \u03c3__s(-\u03b3 v\u00b7\u03c0_s)    = max_P_s\u2208^: \n    P_s_b\u2264\u03b1_s^P P_s,-\u03b3 v\u00b7\u03c0_s= \u03b1_s^P -\u03b3 v\u00b7\u03c0_s_b^*,\n\nin which case the robust value function v^\u03c0, is the optimal solution of:\n\n    max_v\u2208^v, \u03bc_0 s. t.  v(s)\u2264 T_(P_0,r_0)^\u03c0v(s) - \u03b1_s^r\u03c0_s_a^* - \u03b1_s^P -\u03b3 v\u00b7\u03c0_s_b^* for all  s\u2208.\n\n\n\n\n\n\n\u00a7 MDPS\n\n\n\n \u00a7.\u00a7 Proof of Proposition <ref>\n\n\n\n\nSuppose that Asm.\u00a0<ref> holds. Then, we have the following properties:\n\n\n    \n  (i) Monotonicity: For all v_1, v_2\u2208^ such that v_1\u2264 v_2, we have T^\u03c0,v_1 \u2264 T^\u03c0,v_2 and T^*, v_1 \u2264 T^*, v_2.  \n\n    \n  (ii) Sub-distributivity: For all v_1\u2208^, c\u2208, we have T^\u03c0,(v_1 + c1_)\u2264 T^\u03c0,v_1 + \u03b3 c1_ and T^*, (v_1 + c1_)\u2264 T^*, v_1 + \u03b3 c1_, \u2200 c\u2208. \n\n    \n  (iii) Contraction: Let \u03f5_*:= min_s\u2208\u03f5_s>0. Then, for all v_1, v_2\u2208^,\n\n    T^\u03c0,v_1 -  T^\u03c0,v_2_\u221e\u2264 (1-\u03f5_*)v_1-v_2_\u221e and\n    T^*, v_1 -  T^*, v_2_\u221e\u2264 (1-\u03f5_*)v_1-v_2_\u221e.\n\n\n\n\nProof of (i).  Consider the evaluation operator and let v_1,v_2\u2208^ such that v_1\u2264 v_2.\n For all s\u2208,\n\n    [T^\u03c0,v_1 -  T^\u03c0,v_2](s) \n       =  T_(P_0,r_0)^\u03c0v_1(s) - \u03b1_s^r\u03c0_s - \u03b1_s^P \u03b3v_1\u03c0_s\n           - (T_(P_0,r_0)^\u03c0v_2(s) - \u03b1_s^r\u03c0_s - \u03b1_s^P \u03b3v_2\u03c0_s)\n       = T_(P_0,r_0)^\u03c0v_1(s) - T_(P_0,r_0)^\u03c0v_2(s) + \u03b1_s^P \u03b3\u03c0_s (v_2 - v_1)\n       = \u03b3 P_0^\u03c0(v_1 - v_2)(s) + \u03b1_s^P \u03b3\u03c0_s (v_2 - v_1)\n       = \u03b3\u03c0_s, P_0_s(v_1 - v_2)+ \u03b1_s^P \u03b3\u03c0_s (v_2 - v_1)\n        [\u2200 v\u2208^, P_0^\u03c0v(s) = \u2211_(s',a)\u2208\u03c0_s(a)P_0(s'|s,a)v(s') \n                                                   = \u2211_a\u2208\u03c0_s(a)[P_0_s v](a) =\u03c0_s, P_0_sv]\n       =  \u03b3\u03c0_s( *\u03c0_s/\u03c0_s, P_0_s(v_1 - v_2)  + \u03b1_s^P  (v_2 - v_1))\n       \u2264\u03b3\u03c0_s( *\u03c0_s/\u03c0_s, P_0_s(v_1 - v_2)  + \u03b1_s^P  (v_2- v_1))   [\u2200 v, w\u2208^, v - w\u2264*v - w\u2264v-w].\n\nBy Asm.\u00a0<ref>, we also have\n\n    \u03b1_s^P   \u2264min_\ud835\udc2e_\u2208^_+, \ud835\udc2e_=1\n    \ud835\udc2f_\u2208^_+, \ud835\udc2f_=1\ud835\udc2e_^\u22a4 P_0(\u00b7| s,\u00b7)\ud835\udc2f_   =min_\ud835\udc2e_\u2208^_+, \ud835\udc2e_=1\n    \ud835\udc2f_\u2208^_+, \ud835\udc2f_=1*\ud835\udc2e_, P_0(\u00b7| s,\u00b7)\ud835\udc2f_   \u2264*\u03c0_s/\u03c0_s, P_0(\u00b7| s,\u00b7)(v_2-v_1)/v_2-v_1,\n\nso that \n\n    [T^\u03c0,v_1 -  T^\u03c0,v_2](s) \n        \u2264\u03b3\u03c0_s( *\u03c0_s/\u03c0_s, P_0_s(v_1 - v_2)  + *\u03c0_s/\u03c0_s, P_0(\u00b7| s,\u00b7)(v_2-v_1)/v_2-v_1v_2- v_1)\n       = \u03b3\u03c0_s( *\u03c0_s/\u03c0_s, P_0_s(v_1 - v_2)  + *\u03c0_s/\u03c0_s, P_0(\u00b7| s,\u00b7)(v_2-v_1)) =0,\n\nwhere we switch notations to designate P_0(\u00b7| s,\u00b7) = P_0_s \u2208^\u00d7. This proves monotonicity. \n\nProof of (ii).  We now prove the sub-distributivity of the evaluation operator. Let v\u2208^, c\u2208. For all s\u2208,\n\n    [T^\u03c0,(v + c1_)](s) \n    \n        =    [T_(P_0,r_0)^\u03c0(v + c1_)](s) - \u03b1_s^r\u03c0_s - \u03b1_s^P \u03b3v + c1_\u03c0_s\n    \n        =    T_(P_0,r_0)^\u03c0v(s) + \u03b3 c - \u03b1_s^r\u03c0_s - \u03b1_s^P \u03b3v + c1_\u03c0_s   [T_(P_0,r_0)^\u03c0(v + c1_) = T_(P_0,r_0)^\u03c0v + \u03b3 c1_]\n    \u2264     T_(P_0,r_0)^\u03c0v(s) + \u03b3 c - \u03b1_s^r\u03c0_s - \u03b1_s^P \u03b3\u03c0_s (v + c1_)\n    \n        =    T_(P_0,r_0)^\u03c0v(s)+ \u03b3 c - \u03b1_s^r\u03c0_s - \u03b1_s^P \u03b3\u03c0_sv\n           - \u03b1_s^P \u03b3\u03c0_s c1_\n    \n        =    [T^\u03c0,v ](s) + \u03b3 c- \u03b1_s^P \u03b3\u03c0_s c1_\n    \u2264    [T^\u03c0,v ](s) + \u03b3 c.    [\u03b3 > 0, \u03b1_s^P > 0,  \u00b7\u2265 0]\n\n\nProof of (iii). \nWe prove the contraction of a more general evaluation operator with \u2113_p regularization, p\u2265 1. This will establish contraction of the operator T^\u03c0, by simply setting p=2. \nDefine as q the conjugate value of p, such that 1/p + 1/q =1. As seen in the proof of Thm.\u00a0<ref>, \nfor balls that are constrained according to the \u2113_p-norm \u00b7_p, the robust value function v^\u03c0, is the optimal solution of:\n\n    max_v\u2208^v, \u03bc_0 s. t.  v(s)\u2264 T_(P_0,r_0)^\u03c0v(s) - \u03b1_s^r\u03c0_s_q - \u03b1_s^P -\u03b3 v\u00b7\u03c0_s_q for all  s\u2208,\n\nbecause \u00b7_q is the dual norm of \u00b7_p, and we can define the operator accordingly:\n\n    [T^\u03c0,_qv](s) := T_(P_0,r_0)^\u03c0v(s) - \u03b1_s^r\u03c0_s_q - \u03b1_s^P \u03b3v\u00b7\u03c0_s_q  \u2200 v\u2208^, s\u2208.\n\nWe make the following assumption:\n[A_q]\nFor all s\u2208, there exists \u03f5_s > 0 such that \u03b1_s^P\u22641-\u03b3-\u03f5_s/\u03b3^1/q.\n\n\nLet v_1,v_2\u2208^. For all s\u2208,\n\n    *[T_q^\u03c0,v_1](s) -  [T_q^\u03c0,v_2](s) \n    \n         =   | T_(P_0,r_0)^\u03c0v_1(s) - \u03b1_s^r\u03c0_s_q - \u03b1_s^P \u03b3v_1\u00b7\u03c0_s_q\n           - (T_(P_0,r_0)^\u03c0v_2(s) - \u03b1_s^r\u03c0_s_q - \u03b1_s^P \u03b3v_2\u00b7\u03c0_s_q)|\n    \n         =   *T_(P_0,r_0)^\u03c0v_1(s)   - T_(P_0,r_0)^\u03c0v_2(s)  + *\u03b1_s^P \u03b3 (v_2 \u00b7\u03c0_s_q  -  v_1\u00b7\u03c0_s_q)\n    \n        =   *T_(P_0,r_0)^\u03c0v_1(s)   - T_(P_0,r_0)^\u03c0v_2(s)  + \u03b1_s^P \u03b3*v_2 \u00b7\u03c0_s_q  -  v_1\u00b7\u03c0_s_q\n    \u2264   *T_(P_0,r_0)^\u03c0v_1(s)  - T_(P_0,r_0)^\u03c0v_2(s) + \u03b1_s^P \u03b3v_2 \u00b7\u03c0_s  -  v_1\u00b7\u03c0_s_q\n                                           [\u2200, \u2208^, *_q - _q\u2264-_q] \n    \u2264   \u03b3*v_1 - v_2_\u221e + \u03b1_s^P \u03b3v_2 \u00b7\u03c0_s  -  v_1\u00b7\u03c0_s_q\n                                           [T_(P_0,r_0)^\u03c0v_1  - T_(P_0,r_0)^\u03c0v_2_\u221e\u2264\u03b3*v_1 - v_2_\u221e]\n     \n         =   \u03b3*v_1 - v_2_\u221e + \u03b1_s^P \u03b3(v_2 -v_1)\u00b7\u03c0_s _q\n                                           [\u2200 v, w\u2208^, v\u00b7\u03c0_s - w\u00b7\u03c0_s = (v-w)\u00b7\u03c0_s]\n    \u2264   \u03b3*v_1 - v_2_\u221e + \u03b1_s^P \u03b3v_2 -v_1_q        [\u2200 v\u2208^, v\u00b7\u03c0_s_q\u2264v_q]\n    \u2264   \u03b3*v_1 - v_2_\u221e + \u03b1_s^P \u03b3^1/q*v_1 - v_2_\u221e  [\u2200 v, w\u2208^, v-w_q \u2264^1/qv-w_\u221e]\n    \n         =   \u03b3 (1 + \u03b1_s^P^1/q)*v_1 - v_2_\u221e\n    \u2264   \u03b3(1 + 1-\u03b3- \u03f5_s/\u03b3)v_1-v_2_\u221e        [\u03b1_s^P \u22641-\u03b3-\u03f5_s/\u03b3^1/q by Asm.\u00a0(A_q)]   \n    \n         =    (1- \u03f5_s )v_1-v_2_\u221e\n    \u2264   (1- \u03f5_* )v_1-v_2_\u221e,\n\nwhere \u03f5_*:= min_s\u2208\u03f5_s. Setting q=2 and remarking that: (i) the first bound in Asm.\u00a0<ref> recovers Asm.\u00a0(A_q); (ii) T_2^\u03c0, = T^\u03c0,, establishes contraction of the evaluation operator. For the optimality operator, the proof is exactly the same as that of <cit.>, using Prop.\u00a0<ref>. \n\n\n \n\n \u00a7.\u00a7 Proof of Theorem <ref>\n\n \n\n\n[optimal policy]\nThe greedy policy \u03c0^*,  = \ud835\udca2_\u03a9_(v^*, ) is the unique optimal policy, for all \u03c0\u2208\u0394_^, v^\u03c0^*,  = v^*, \u2265 v^\u03c0,. \n\n\n\nBy strong convexity of the norm, the function\n\u03a9_v,  : \u03c0_s \u21a6\u03c0_s (\u03b1_s^r + \u03b1_s^P \u03b3v) is strongly convex in \u03c0_s. As such, we can invoke Prop.\u00a0<ref> to state that the greedy policy \u03c0^*, is the unique maximizing argument for v^*,.  \nMoreover, by construction, \n\n    T^\u03c0^*, ,v^*,  = T^*, v^*, = v^*, .\n\nSupposing that Asm.\u00a0<ref> holds, the evaluation operator T^\u03c0^*, , is contracting and has a unique fixed point v^\u03c0^*, ,. Therefore, v^*, being also a fixed point, we have v^\u03c0^*, , = v^*,. It remains to show the last inequality: the proof is exactly the same as that of <cit.>, and relies on the monotonicity of the operators.\n\n\n\n\n \u00a7.\u00a7 Proof of Remark <ref>\n\n\n\n\nAn optimal policy may be stochastic. This is due to the fact that our MDP framework builds upon the general s-rectangularity assumption. Robust MDPs with s-rectangular uncertainty sets similarly yield an optimal robust policy that is stochastic <cit.>. Nonetheless, the MDP formulation recovers a deterministic optimal policy in the more specific (s,a)-rectangular case, which is in accordance with the robust MDP setting.\n\n\n\nIn the (s,a)-rectangular case, the uncertainty set is structured as = \u00d7_(s,a)\u2208(s,a), where (s,a):= P_0(\u00b7|s,a)\u00d7 r_0(s,a)+ (s,a)\u00d7(s,a). The robust counterpart of problem (<ref>) is:\n\n    F(s)   = max_(P, r)\u2208{v(s) - r^\u03c0(s) - \u03b3 P^\u03c0v(s) }\n       =max_(P(\u00b7|s,a), r(s,a))\u2208(s,a)\u00d7(s,a){v(s) - r_0^\u03c0(s) - r^\u03c0(s)- \u03b3 P_0^\u03c0v(s) - \u03b3 P^\u03c0v(s) }\n       =max_(P(\u00b7|s,a), r(s,a))\u2208(s,a)\u00d7(s,a){ - r^\u03c0(s) - \u03b3 P^\u03c0v(s) } + v(s) - r_0^\u03c0(s)- \u03b3 P_0^\u03c0v(s)\n       = max_ r(s,a)\u2208(s,a){-r^\u03c0(s) }\n        +\u03b3max_P(\u00b7|s,a)\u2208(s,a){ - P^\u03c0v(s) } + v(s)  - T_(P_0,r_0)^\u03c0v(s)\n       = max_ r(s,a)\u2208(s,a){ -\u2211_a\u2208\u03c0_s(a)r(s,a) }\n        +\u03b3max_P(\u00b7|s,a)\u2208(s,a){ - \u2211_a\u2208\u03c0_s(a) P(\u00b7|s,a), v}\n           + v(s)  - T_(P_0,r_0)^\u03c0v(s)\n       = \u2211_a\u2208\u03c0_s(a)(max_ r(s,a)\u2208(s,a){ -r(s,a) }\n        +\u03b3max_P(\u00b7|s,a)\u2208(s,a){P(\u00b7|s,a), -v})\n           + v(s)  - T_(P_0,r_0)^\u03c0v(s).\n\nIn particular, if we have ball uncertainty sets (s,a):= {P(\u00b7|s,a)\u2208^: P(\u00b7|s,a)\u2264\u03b1_s,a^P} and (s,a):= {r(s,a)\u2208: r(s,a)\u2264\u03b1_s,a^r} for all (s,a)\u2208, then we can explicitly compute the support functions:\n\n    max_r(s,a): r(s,a)\u2264\u03b1_s,a^r -r(s,a) = \u03b1_s,a^r  and max_P(\u00b7|s,a): P(\u00b7|s,a)\u2264\u03b1_s,a^PP(\u00b7|s,a), -v =  \u03b1_s,a^Pv.\n\nTherefore, the robust counterpart rewrites as:\n\n    F(s)    = \u2211_a\u2208\u03c0_s(a)(\u03b1_s,a^r + \u03b3\u03b1_s,a^Pv)+ v(s)  - T_(P_0,r_0)^\u03c0v(s),\n\nand the robust value function v^\u03c0, is the optimal solution of the convex optimization problem:\n\n    max_v\u2208^v, \u03bc_0 s. t.   v(s)  \u2264  T_(P_0,r_0)^\u03c0v(s) - \u2211_a\u2208\u03c0_s(a)(\u03b1_s,a^r + \u03b3\u03b1_s,a^Pv)  for all  s\u2208.\n\nThis derivation enables us to derive an Bellman evaluation operator for the (s,a)-rectangular case. Indeed, the regularization function now becomes\n\n    \u03a9_v, (\u03c0_s):= \u2211_a\u2208\u03c0_s(a)(\u03b1_s,a^r + \u03b3\u03b1_s,a^Pv),\n \nwhich yields the following operator:\n\n    [T^\u03c0,v](s) := T_(P_0,r_0)^\u03c0v(s)-\u03a9_v, (\u03c0_s) ,     \u2200 s\u2208.\n\nWe aim to show that we can find a deterministic policy \u03c0^d\u2208\u0394_^ such that [T^\u03c0^d,v](s) = [T^*,v](s) for all s\u2208. Given an arbitrary policy \u03c0\u2208\u0394_^, we first rewrite:\n\n    [T^\u03c0,v](s)    = r_0^\u03c0(s) + \u03b3 P_0^\u03c0v(s) -  \u03a9_v, (\u03c0_s)\n       = \u2211_a\u2208\u03c0_s(a)r_0(s,a) + \u03b3\u2211_a\u2208\u03c0_s(a)P_0(\u00b7|s,a), v - (\u2211_a\u2208\u03c0_s(a)(\u03b1_s,a^r + \u03b3\u03b1_s,a^Pv))\n       = \u2211_a\u2208\u03c0_s(a)(r_0(s,a)- \u03b1_s,a^r + \u03b3 (P_0(\u00b7|s,a), v - \u03b1_s,a^Pv))\n\nBy <cit.>, we have that:\n\n    \u2211_a\u2208\u03c0_s(a)(r_0(s,a)- \u03b1_s,a^r + \u03b3 (P_0(\u00b7|s,a), v - \u03b1_s,a^Pv)) \n       \u2264max_a\u2208{r_0(s,a)- \u03b1_s,a^r + \u03b3 (P_0(\u00b7|s,a), v - \u03b1_s,a^Pv)},\n\nand since the action set is finite, there exists an action a^*\u2208 reaching the maximum. Setting \u03c0^d(a^*)=1 thus gives the desired result. \nWe just derived a regularized formulation of robust MDPs with (s,a)-rectangular uncertainty set and ensured that the corresponding Bellman operators yield a deterministic optimal policy. In that case, the optimal Bellman operator becomes:\n\n    [T^*,v](s) = max_a\u2208{r_0(s,a)- \u03b1_s,a^r + \u03b3 (P_0(\u00b7|s,a), v - \u03b1_s,a^Pv)}.\n\n\n\n\n\n\u00a7 Q-LEARNING\n\n\n\n\n\n \u00a7.\u00a7 The q-function\n\n\n\nAssume that = ({P_0} + ) \u00d7 ({r_0} + ) and  is (s,a)-rectangular. Then, its corresponding robust action-value function is an optimal solution of:\n\n    max_q\u2208^q, \u03bc_0\u00b7\u03c0 s.t.  q(s,a)\u2264 T^\u03c0_(P_0, r_0)q(s,a) - \u03c3_(s,a)(-1) - \u03c3_(s,a)(-\u03b3 q\u00b7\u03c0)  for all  (s,a)\u2208,\n\nwhere [q\u00b7\u03c0](s'):= \u2211_a'\u2208\u03c0_s'(a')q(s', a'), \u2200 s'\u2208. \n\n\n\nIt is known from <cit.> that the robust action-value function is an optimal solution of:\n\n    max_q\u2208^q, \u03bc_0\u00b7\u03c0 s.t.  q(s,a)\u2264 T^\u03c0_(P, r)q(s,a)  for all  (s,a)\u2208, (P(\u00b7|s,a), r(s,a))\u2208(s,a),\n \nwhich can be rewritten as:\n\n    max_q\u2208^q, \u03bc_0\u00b7\u03c0 s.t.  q(s,a)   \u2264 T^\u03c0_(P_0, r_0)q(s,a) +r(s,a) + \u03b3P(\u00b7|s,a)\u00b7\u03c0, q\n        for all  (s,a)\u2208, (P(\u00b7|s,a), r(s,a))\u2208(s,a),\n\nwith [P(\u00b7|s,a)\u00b7\u03c0](s',a'):= \u03c0_s'(a')P(s'|s,a), \u2200 (s',a'\u2208). \nMore synthetically, the robust action-value function is an optimal solution of:\n\n    max_q\u2208^q, \u03bc_0\u00b7\u03c0\n        s.t. max_(P(\u00b7|s,a), r(s,a))\u2208(s,a){\n    q(s,a) - T^\u03c0_(P_0, r_0)q(s,a) - r(s,a) - \u03b3P(\u00b7|s,a)\u00b7\u03c0, q}\u2264 0\n        for all \n    (s,a)\u2208.\n\n\nWe now compute the robust counterpart. For any (s,a)\u2208 and policy \u03c0\u2208\u0394_^, denote by:\n\n    F^\u03c0(s,a):= max_(P(\u00b7|s,a), r(s,a))\u2208(s,a){\n    q(s,a) - T^\u03c0_(P_0, r_0)q(s,a) - r(s,a) - \u03b3P(\u00b7|s,a)\u00b7\u03c0, q}.\n\nRemoving the constant terms from the maximization and using the indicator function yields:\n\n    F^\u03c0(s,a)    = q(s,a) - T^\u03c0_(P_0, r_0)q(s,a) + \n        max_(P(\u00b7|s,a), r(s,a))\u2208(s,a){- r(s,a) - \u03b3P(\u00b7|s,a)\u00b7\u03c0, q}\n       =q(s,a) - T^\u03c0_(P_0, r_0)q(s,a) - min_(P(\u00b7|s,a), r(s,a))\u2208(s,a){ r(s,a) + \u03b3P(\u00b7|s,a)\u00b7\u03c0, q}\n       = q(s,a) - T^\u03c0_(P_0, r_0)q(s,a) - min_r(s,a)\u2208(s,a)r(s,a) - min_P(\u00b7|s,a)\u2208(s,a)\u03b3P(\u00b7|s,a)\u00b7\u03c0, q\n       =  q(s,a) - T^\u03c0_(P_0, r_0)q(s,a) - min_r(s,a)\u2208{r(s,a) + \u03b4_(s,a)(r(s,a))}\n            -min_P(\u00b7|s,a)\u2208^{\u03b3P(\u00b7|s,a)\u00b7\u03c0, q +\u03b4_(s,a)(P(\u00b7|s,a))}.\n\nApplying Fenchel-Rockafellar duality to both minimization problems yields the desired result. \n\n\n\n\nIf, additionally, _sa is a ball of radius \u03b1_sa^P w.r.t. some norm \u00b7 and _sa an interval of radius \u03b1_sa^r, then the\nrobust action-value function is an optimal solution of:\n\n    max_q\u2208^q, \u03bc_0\u00b7\u03c0 s.t.  q(s,a)\u2264 T^\u03c0_(P_0, r_0)q(s,a) - \u03b1_sa^r -\u03b3\u03b1_sa^P q\u00b7\u03c0_*  for all  (s,a)\u2208.\n\n\n\nThe upper-bound in the optimization problem enables to define the Bellman operator on q-functions as:\n\n    [T^\u03c0,q](s,a):= T^\u03c0_(P_0, r_0)q(s,a) - \u03b1_sa^r -\u03b3\u03b1_sa^P q\u00b7\u03c0_*\n\n\n\n\n \u00a7.\u00a7 Distinguishing between and robust q-functions\n\n\nWe aim to show that although we can interchangeably optimize an q-function or a robust q-value under (s,a)-rectangularity, \n\nthe q-function obtained from the value v is not the same as the q-function obtained from the original robust optimization problem. This nuance is reminiscent of the regularized MDP setting, where defining the regularized q-function w.r.t. the regularized value v is not equivalent to taking v as the expected q-function over a policy. \n\nLet thus assume that the uncertainty set is (s,a)-rectangular. Then, by Sec.\u00a0<ref>, the value function v^\u03c0, is the unique fixed point of the Bellman operator as below: \n\n    [T^\u03c0,v](s) := T_(P_0,r_0)^\u03c0v(s)-\u2211_a\u2208\u03c0_s(a)(\u03b1_s,a^r + \u03b3\u03b1_s,a^Pv),     \u2200 s\u2208.\n\nThis rewrites as: \n\n    v^\u03c0, (s)    = \u2211_a\u2208\u03c0_s(a)(r_0(s,a) +\u03b3P_0(\u00b7|s,a), v^\u03c0,  -  \u03b1_s,a^r - \u03b3\u03b1_s,a^Pv^\u03c0, )\n       = \u2211_a\u2208\u03c0_s(a)(q^\u03c0,(s,a) -  \u03b1_s,a^r - \u03b3\u03b1_s,a^Pv^\u03c0, ),\n\nwhere the last equality holds by definition of the q-function associated with v^\u03c0, (Def.\u00a0<ref>). As a result,\n\n    q^\u03c0,\u00b7\u03c0(s) = v^\u03c0, (s) + [\u03b1^r + \u03b3\u03b1^Pv^\u03c0, ] \u00b7\u03c0 (s)\n\n\nAlternatively, by optimizing w.r.t. q\u2208^ instead of v\u2208^ and applying Cor.\u00a0<ref>, the robust action-value function q^\u03c0, satisfies: \n\n    q^\u03c0, (s,a) = T^\u03c0_(P_0, r_0)q^\u03c0, (s,a) - \u03b1_sa^r -\u03b3\u03b1_sa^P q^\u03c0, \u00b7\u03c0_*  for all  (s,a)\u2208.\n\nTaking the expectation over policy \u03c0 yields:\n\n    q^\u03c0, \u00b7\u03c0 = r_0^\u03c0 + P_0^\u03c0 (q^\u03c0, \u00b7\u03c0)  - \u2211_a\u2208\u03c0_s(a)(\u03b1_sa^r -\u03b3\u03b1_sa^P q^\u03c0, \u00b7\u03c0_*)  for all  (s,a)\u2208,\n\nso that q^\u03c0, \u00b7\u03c0 is a fixed point of the Bellman operator. By unicity of its fixed point, we obtain that q^\u03c0, \u00b7\u03c0 = v^\u03c0,. \nAs a result:\n\n    q^\u03c0, \u00b7\u03c0   = v^\u03c0, \n    \n           = (q^\u03c0, - \u03b1^r-\u03b3v^\u03c0, \u03b1^P)\u00b7\u03c0\n       = q^\u03c0,\u00b7\u03c0  - [\u03b1^r + \u03b3\u03b1^Pv^\u03c0, ]\u00b7\u03c0\n\nTaking deterministic policies on each possible action, we end up with an element-wise identity: \n\n    q^\u03c0, (s,a)    = q^\u03c0,(s,a) - \u03b1^r_s,a-\u03b3v^\u03c0, \u03b1^P_s,a\n\n\n\n\n\n \u00a7.\u00a7 Convergence of q-learning\n\n\nFor any (s,a)\u2208, let a sequence of step-sizes (\u03b2_t(s,a))_t\u2208 satisfying 0\u2264\u03b2_t(s,a)\u2264 1, \u2211_t\u2208\u03b2_t(s,a) = \u221e and \u2211_t\u2208\u03b2_t^2(s,a)< \u221e. Then, the q-learning algorithm as given in Alg.\u00a0<ref> converges almost surely to the optimal q-function. \n\n\n\nWe will use the convergence result from <cit.>.\nThe update rule is given by:\n\n    q_t+1(s_t,a_t)    = q_t(s_t,a_t) \n           + \u03b2_t(s_t,a_t)(\n      r_t+1 + \u03b3max_b\u2208q_t(s_t+1,b) - \u03b1_s_t a_t^r -\u03b3\u03b1_s_t a_t^Pmax_b\u2208q_t(\u00b7,b)_*  - q_t(s_t,a_t))\n\nwhich we rewrite as:\n\n    q_t+1(s_t,a_t)    = (1-\u03b2_t(s_t,a_t))q_t(s_t,a_t)\n           + \u03b2_t(s_t,a_t)(\n      r_t+1 + \u03b3max_b\u2208q_t(s_t+1,b) - \u03b1_s_t a_t^r -\u03b3\u03b1_s_t a_t^Pmax_b\u2208q_t(\u00b7,b)_* ).\n\nFurther let \u0394_t(s,a):= q_t(s,a) - q^*,(s,a), \u2200 (s,a)\u2208. Then Eq.\u00a0(<ref>) rewrites as:\n\n    \u0394_t+1(s,a)    = (1-\u03b2_t(s_t,a_t))\u0394_t(s_t,a_t)\n       + \u03b2_t(s_t,a_t)(\n      r_t+1 + \u03b3max_b\u2208q_t(s_t+1,b) - \u03b1_s_t a_t^r -\u03b3\u03b1_s_t a_t^Pmax_b\u2208q_t(\u00b7,b)_* - q^*,(s_t,a_t) ).\n\nWe introduce the following random variable:\n\n    G_t(s,a):= r(s,a) + \u03b3max_b\u2208q_t(X(s,a),b) - \u03b1_sa^r -\u03b3\u03b1_s a^Pmax_b\u2208q_t(\u00b7,b)_* - q^*,(s,a),\n\nso that \n\n    \ud835\udd3c[G_t(s,a) | \u2131_t ] \n           = \ud835\udd3c[r(s,a) + \u03b3max_b\u2208q_t(X(s,a),b) - \u03b1_sa^r -\u03b3\u03b1_s a^Pmax_b\u2208q_t(\u00b7,b)_* - q^*,(s,a) | \u2131_t]\n       = r(s,a)+\u03b3\u2211_s'\u2208max_b\u2208P(s'|s,a)q_t(s',b)- \u03b1_sa^r -\u03b3\u03b1_s a^Pmax_b\u2208q_t(\u00b7,b)_* - q^*,(s,a) \n       = [T^*,q_t](s,a)- q^*,(s,a) \n       = [T^*,q_t](s,a)- [T^*,q^*,](s,a).\n\nBy contraction property of the Bellman operator, we thus obtain:\n\n    *\ud835\udd3c[G_t(s,a) | \u2131_t ]_\u221e   = *[T^*,q_t](s,a)- [T^*,q^*,](s,a)_\u221e\n       \u2264 (1-\u03f5^*)*q_t(s,a)- q^*,(s,a)_\u221e = (1-\u03f5^*)*\u0394_t(s,a)_\u221e\n\n\n\n\n\n\n\n\u00a7 PLANNING ON A MAZE\n\n\n\n\n\nIn the following experiment, we play with the radius of the uncertainty set and analyze the distance of the robust/value function to the vanilla one obtained after convergence of MPI. Except for the radius parameters of Table <ref>, all other parameters remain unchanged. In both figures <ref> and <ref>, we see that the distance norm converges to 0 as the size of the uncertainty set gets closer to 0: this sanity check ensures an increasing relationship between the level of robustness and the radius value. As shown in Fig.\u00a0<ref>, the plots for robust MPI and MPI coincide in the reward-robust case, but they diverge from each other as the transition model gets more uncertain. This does not contradict our theoretical findings from Thms.\u00a0<ref>-<ref>. In fact, each iteration of robust MPI involves an optimization problem which is solved using a black-box solver and yields an approximate solution. As such, errors propagate across iterations and according to Fig.\u00a0<ref>, they are more sensitive to transition than reward uncertainty. This is easy to understand: as opposed to the reward function, the transition kernel interacts with the value function at each Bellman update, so errors on the value function also affect those on the optimum and vice versa. Moreover, the gap grows with the radius level because of the simplex constraint we ignored when computing the support function of the transition uncertainty set. The work <cit.> accounts for this additional constraint to derive a regularization function that recovers the robust value under transition uncertainty. \n\n\n0.2in\n\n\n\n\n\u00a7 LEARNING EXPERIMENTS\n\n\n\nIn this section, we provide additional details and plots regarding our q-learning algorithm. \n\n\n \n\n\n\n\n\n\n"}