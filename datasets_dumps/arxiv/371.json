{"entry_id": "http://arxiv.org/abs/2303.06819v2", "published": "20230313022745", "title": "TranSG: Transformer-Based Skeleton Graph Prototype Contrastive Learning with Structure-Trajectory Prompted Reconstruction for Person Re-Identification", "authors": ["Haocong Rao", "Chunyan Miao"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\nTranSG: Transformer-Based Skeleton Graph Prototype Contrastive Learning with Structure-Trajectory Prompted Reconstruction for Person Re-Identification\n    Haocong Rao  Chunyan MiaoCorresponding author\n\nLILY Research Center, Nanyang Technological University, Singapore\n\nSchool of Computer Science and Engineering, Nanyang Technological University, Singapore\n\n{haocong001, ascymiao}@ntu.edu.sg\n\n\n\n\n\n\n\n\n\n\n\n\n    \n============================================================================================================================================================================================================================================================\n\n\n\n\nPerson re-identification (re-ID) via 3D skeleton data is an emerging topic with prominent advantages. Existing methods usually design skeleton descriptors with raw body joints or perform skeleton sequence representation learning. However, they typically cannot concurrently model different body-component relations, and rarely explore useful semantics from fine-grained representations of body joints. In this paper, we propose a generic Transformer-based Skeleton Graph prototype contrastive learning (TranSG) approach with structure-trajectory prompted reconstruction to fully capture skeletal relations and valuable spatial-temporal semantics from skeleton graphs for person re-ID. Specifically, we first devise the Skeleton Graph Transformer (SGT) to simultaneously learn body and motion relations within skeleton graphs, so as to aggregate key correlative node features into graph representations. Then, we propose the Graph Prototype Contrastive learning (GPC) to mine the most typical graph features (graph prototypes) of each identity, and contrast the inherent similarity between graph representations and different prototypes from both skeleton and sequence levels to learn discriminative graph representations. Last, a graph Structure-Trajectory Prompted Reconstruction (STPR) mechanism is proposed to exploit the spatial and temporal contexts of graph nodes to prompt skeleton graph reconstruction, which facilitates capturing more valuable patterns and graph semantics for person re-ID. \nEmpirical evaluations demonstrate that TranSG significantly outperforms existing state-of-the-art methods. We further show its generality under different graph modeling, RGB-estimated skeletons, and unsupervised scenarios. \nOur codes are available at https://github.com/Kali-Hac/TranSGhttps://github.com/Kali-Hac/TranSG.\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nPerson re-identification (re-ID) is a challenging task of retrieving and matching a specific person across varying views or scenarios, which has empowered many vital applications such as security authentication, human tracking, and robotics <cit.>. Recently driven by economical, non-obtrusive and accurate skeleton-tracking devices like Kinect <cit.>, person re-ID via 3D skeletons has attracted surging attention in both academia and industry <cit.>. Unlike conventional methods that rely on visual appearance features (e.g., colors, silhouettes), skeleton-based person re-ID methods model unique body and motion representations with 3D positions of key body joints, which typically enjoy smaller data sizes, lighter models, and more robust performance under scale and view variations <cit.>.\n\n\n\nMost existing methods manually extract anthropometric descriptors and gait attributes from body-joint coordinates <cit.>, or leverage sequence learning paradigms such as Long Short-Term Memory (LSTM) <cit.> to model body and motion features with skeleton sequences <cit.>. However, these methods rarely explore the inherent body relations within skeletons (e.g., inter-joint motion correlations), thus largely ignoring some valuable skeleton patterns. \nTo fill this gap, a few recent works <cit.> construct skeleton graphs to model body-component relations in terms of structure and action. These methods typically require multi-stage non-parallel relation modeling, e.g., <cit.> models collaborative relations conditioned on the structural relations, while they cannot simultaneously mine different underlying relations. On the other hand, they usually leverage sequence-level instances such as averaged features of sequential graphs <cit.> for representation learning,\n\nwhich inherently limits their ability to exploit richer graph semantics from fine-grained (e.g., node-level) representations. For example, there usually exist strong correlations between nearby body-joint nodes within a local spatial-temporal graph context, which can prompt learning unique and recognizable skeleton patterns for person re-ID. \n\nTo address the above challenges, we propose a general Transformer-based Skeleton Graph prototype contrastive learning (TranSG) paradigm with structure-trajectory prompted reconstruction as shown in Fig. <ref>, which integrates different relational features of skeleton graphs and contrasts representative graph features to learn discriminative representations for person re-ID. Specifically, we first model 3D skeletons as graphs, and propose the Skeleton Graph Transformer (SGT) to perform full-relation learning of body-joint nodes, so as to simultaneously aggregate key relational features of body structure and motion into effective graph representations.\nSecond, a Graph Prototype Contrastive learning (GPC) approach is proposed to contrast and learn the most representative skeleton graph features (defined as \u201cgraph prototypes\u201d) of each identity. By pulling together both sequence-level and skeleton-level graph representations to their corresponding prototypes while pushing apart representations of different prototypes, we encourage the model to capture discriminative graph features and high-level skeleton semantics (e.g., identity-associated patterns) for person re-ID. Last, motivated by the inherent structural correlations and pattern continuity of body joints <cit.>, we devise a graph Structure-Trajectory Prompted Reconstruction (STPR) mechanism to exploit the spatial-temporal context (i.e., graph structure and trajectory) of skeleton graphs to prompt the skeleton graph reconstruction, which facilitates learning richer graph semantics and more effective graph representations for the person re-ID task.\n\nOur key contributions can be summarized as follows:\n\n    \n  * We present a generic TranSG paradigm to learn effective representations from skeleton graphs for person re-ID. To the best of our knowledge, TranSG is the first transformer paradigm that unifies skeletal relation learning and skeleton graph contrastive learning specifically for skeleton-based person re-ID.\n    \n  * We devise a skeleton graph transformer (SGT) to fully capture relations in skeleton graphs and integrate key correlative node features into graph representations.\n    \n  * We propose the graph prototype contrastive learning (GPC) to contrast and learn the most representative graph features and identity-related semantics from both skeleton and sequence levels for person re-ID.\n    \n  * We devise the graph structure-trajectory prompted reconstruction (STPR) to exploit spatial-temporal graph contexts for reconstruction, so as to capture more key graph semantics and unique features for person re-ID.\n\n\n Extensive experiments on five public benchmarks show that TranSG prominently outperforms existing state-of-the-art methods and is highly scalable to be applied to different graph modeling, RGB-estimated or unlabeled skeleton data.\n\n\n\n\n\u00a7 RELATED WORKS\n\nPerson Re-identification Using Skeleton Data. \nEarly methods manually design skeleton descriptors (e.g., anthropometric and gait attributes) from raw body joints. In <cit.>, seven Euclidean distances between certain joints are computed to perform distance metric learning for person re-ID. An further extension with 13 (D_13) and 16 skeleton descriptors (D_16) in <cit.> and <cit.> are utilized to identify different persons. \n\nRecent methods employ deep neural networks to perform representation learning with skeleton sequences or graphs. In <cit.>, a CNN-based model PoseGait is devised to encode body-joint sequences and hand-crafted pose features for human recognition. \n<cit.> proposes the AGE model to encode recognizable gait features from 3D skeleton sequences, while its extension SGELA <cit.> further combines self-supervised semantics learning (e.g., sequence sorting) and sequence contrastive learning to improve discriminative feature learning. The SimMC framework <cit.> is proposed to encode prototypes and intra-sequence relations of masked skeleton sequences for person re-ID. MG-SCR <cit.> and SM-SGE <cit.> perform multi-stage body-component relation learning based on multi-scale graphs to learn person re-ID representations.\n\n<cit.> proposes a general skeleton feature re-ranking mechanism for skeleton-based person re-ID.\n\n\nContrastive Learning.\n The aim of contrastive learning is to pull closer homogeneous or positive representation pairs while pushing farther negative pairs in a certain feature space. It has been broadly applied to various areas to learn effective feature representations <cit.>. An instance discrimination paradigm with exemplar tasks is designed in <cit.> for visual contrastive learning. A contrastive predictive coding (CPC) approach based on the context auto-encoding and probabilistic contrastive loss <cit.> is proposed to learn different-domain representations. \n\nPCL <cit.> combines k-means clustering and contrastive learning for image representation learning. Skeleton contrastive paradigms based on consecutive sequences or randomly masked sequences are devised in <cit.> and <cit.> for unsupervised skeleton representation learning and person re-ID.\n\n\nPrompt Learning.\n\nThe function of prompts is to provide additional knowledge, instruction, or context for the input of models, such that they can be prompted to give more reliable outputs for different tasks <cit.>. \nIn <cit.>, CLIP leverages language-based prompts to generalize the pre-trained visual representations to many tasks. CoOp <cit.> is further devised to automatically model task-relevant prompts with continuous representations to improve the downstream task performance. \nAs far as we know, this work for the first time explores graph prompts (defined as structure and trajectory contexts) for skeleton graph reconstruction, so as to encourage capturing more key features and graph semantics (e.g., pattern continuity) for person re-ID. \n\n\n\n\n\n\n\n\n\n\u00a7 THE PROPOSED APPROACH\n\nGiven a 3D skeleton sequence X=(x_1,\u22ef,x_f)\u2208\u211d^f \u00d7 J \u00d7 3, where x_t\u2208\u211d^J \u00d7 3 denotes the t^th skeleton with 3D coordinates of J body joints.\n Each skeleton sequence X corresponds to a person identity y, where y\u2208{1, \u22ef, C} and C is the number of different classes (i.e., identities). We use \u03a6_T={X^T_i}_i=1^N_1, \u03a6_P={X^P_i}_i=1^N_2, and \u03a6_G={X^G_i}_i=1^N_3 to denote the training set, probe set, and gallery set that contain N_1, N_2, and N_3 skeleton sequences of different persons in different scenes and views. Our aim is to learn an encoder to map skeleton sequences into effective representations, such that the encoded skeleton sequence representations (denoted as {S^P_i}_i=1^N_2) in the probe set can be matched with the representations (denoted as {S^G_i}_i=1^N_3) of the same identity in the gallery set.\n\nThe overview of our TranSG approach is shown in Fig. <ref>: Firstly, we represent each skeleton sequence x_1,\u22ef,x_f as skeleton graphs \ud835\udca2^1,\u22ef,\ud835\udca2^f (see Sec. <ref>), and integrate the graph positional information into their node representations h_i. Then, they are fed into the skeleton graph transformer (SGT) to fully capture body-component relations with multiple full-relation (FR) heads (see Sec. <ref>). We employ multiple SGT layers and take the l^th layer output h^(l+1)_i as the input of (l+1)^th layer. Next, the centroids of graph features belonging to different identities (ID) are utilized to generate graph prototypes, and we enhance the similarity of both skeleton-level (s^t) and sequence-level graph representations (S) to their corresponding prototypes, while maximizing their dissimilarity to other prototypes by optimizing \u2112_GPC (see Sec. <ref>). Meanwhile, we randomly mask skeleton graph structure and node trajectory, and exploit correspondingly masked graph representations as contexts to prompt reconstruction by minimizing \u2112_STPR (see Sec. <ref>). The skeleton graph representations learned from our approach are exploited for person re-ID (see Sec. <ref>).\n\n\n\n \u00a7.\u00a7 Skeleton Graph Construction\n\n\nThe human body with joints can be naturally modeled as graphs to characterize rich structural and positional information <cit.>. We construct skeleton graphs with the body joints as nodes and the structural connections between adjacent joints as edges. Each graph \ud835\udca2^t(\ud835\udcb1^t, \u2130^t) (corresponding to the t^th skeleton x_t)  consists of nodes \ud835\udcb1^t={v^t_1, v^t_2, \u22ef,v^t_J}, v^t_i\u2208\u211d^3, i\u2208{1,\u22ef,J} and edges \u2130^t={e^t_i,j | v^t_i, v^t_j\u2208\ud835\udcb1^t}, e^t_i,j\u2208\u211d. Here \ud835\udcb1^t and \u2130^t denote the set of nodes corresponding to J different body joints and the set of their internal connection relations, respectively. \nThe adjacency matrix of \ud835\udca2^t is denoted as \ud835\udc00^t\u2208\u211d^J \u00d7 J to represent the relations among J nodes. \ud835\udc00^t is initialized based on the connections of adjacent body joints.\n\n\n\n\n \u00a7.\u00a7 Skeleton Graph Transformer\n\n\nAs our goal is to capture discriminative skeleton features for person re-ID, it is crucial to consider two unique properties of human skeletons: (1) Body structural features, which can be inferred from the relations between adjacent body joints; (2) Skeleton actional patterns (e.g., gait <cit.>), which are typically characterized by the relations among different body components <cit.>. From the perspective of graphs, we regard each body-joint node as a basic body component, and propose to combine the above relation learning as a full-relation learning of body-joint nodes, so as to fully aggregate key body and motion features from skeleton graphs.\nFor this purpose, we devise the skeleton graph transformer (SGT) as follows (shown in Fig. <ref>).\n\n\nGiven a skeleton graph \ud835\udca2^t(\ud835\udcb1^t, \u2130^t) and its adjacency matrix \ud835\udc00^t, we first exploit the pre-defined graph structure to generate the positional encoding for graph nodes with:\n\n    \u0394=\ud835\udc08-\ud835\udc03^-1 / 2\ud835\udc00\ud835\udc03^-1 / 2=\ud835\udc14^T \u039b\ud835\udc14,\n\nwhere \ud835\udc00, \ud835\udc03 are the adjacency matrix and degree matrix of the skeleton graph \ud835\udca2, respectively, and \u039b,\ud835\udc14 denote the matrices of Laplacian eigenvalues and eigenvectors, respectively. \ud835\udc14^T \u039b\ud835\udc14 is the factorization of the graph Laplacian matrix. For convenience, we use \ud835\udc00 to represent \ud835\udc00^t as skeleton graphs in the same dataset share the identical initialized adjacency matrix. We follow <cit.> to adopt the K smallest non-trivial eigenvectors as the node positional encoding, denoted as \u03bb_i\u2208\u211d^K for the node v_i. They are mapped into feature spaces of the same dimension d with the affine transformation, which are then added by:\n\n    h_i=(W_vv_i+b_v)+(W_p\u03bb_i + b_p),\n\nwhere h_i\u2208\u211d^d denotes the i^th position-encoded node representation, W_v\u2208\u211d^d \u00d7 3, W_p\u2208\u211d^d \u00d7 K, b_v, b_p\u2208\u211d^d are the learnable parameters of the feature transformation for the i^th node v_i and its corresponding positional encoding. \nIntuitively, the addition of positional encoding in Eq. (<ref>) helps preserve the unique positional information of nodes based on the graph structure, i.e., structurally nearby nodes are endowed with similar positional features while the farther nodes possess more dissimilar positional features, so as to encourage more effective node representation learning <cit.>.\n\n\nThen, given the body-joint node representations, we capture their inherent relations using multiple independent full-relation (FR) heads (see Fig. <ref>), and update node representations by aggregating corresponding relational features:\n\n    w_i, j^k,l=Softmax_j((Q^k, lh_i^(l)) \u00b7 (K^k, lh_j^(l))/\u221a(d_k)),\n\n\n    \u0125_i^(l)=O^l_k=1^H(\u2211_j \u2208\ud835\udca9_iw_i,j^k, lV^k, lh_j^(l)),\n\nwhere Q^k,l,K^k,l,V^k,l\u2208\u211d^d_k\u00d7 d are the parameter matrices for query, key, and value transformations in the k^th FR head of the l^th SGT layer, O^l\u2208\u211d^d \u00d7 d is the parameter matrix for output transformation of the l^th SGT layer.\n1/\u221a(d_k) is the scaling factor for the scaled dot-product similarity, w_i,j^k,l denotes the normalized relation value between the i^th and j^th node captured by the k^th FR head in the l^th layer, \n represents the concatenation operation, and H is the number of FR heads. For clarity, we use \u0125_i^(l)\u2208\u211d^d denotes the i^th node representation that concatenates node features learned from different heads in the l^th layer. \n\nIt is worth noting that SGT naturally generalizes the self-attention <cit.> to the full-relation learning of graph nodes, and can be viewed as a general paradigm that simultaneously captures structural and actional relations from both adjacent and non-adjacent body-component nodes.\n\nThe multiple FR heads enable the model to jointly attend to node relations from different feature subspaces and integrate more key correlative node features into final node representations.\nWe follow <cit.> to apply a Feed Forward Network (FFN) with residual connections <cit.> and batch normalization <cit.> by:\n\n    h_i^(l)=Norm(h_i^(l)+\u0125_i^(l)),\n\n\n    h_i^(l+1)=Norm(h_i^(l)+W_2^l \u03c3(W_1^lh_i^(l))).\n\nIn Eq. (<ref>) and (<ref>), Norm(\u00b7) denotes the batch normalization operation, W_1^l\u2208\u211d^2d \u00d7 d, W_2^l\u2208\u211d^d \u00d7 2d are the learnable parameters of FFN, \u03c3(\u00b7) is the ReLU activation function, h_i^(l) and h_i^(l+1) represent the intermediate and output node representations of the l^th SGT layer, respectively. We average the node features in each skeleton graph as the corresponding graph representation, and then integrate f consecutive graph representations into the final sequence-level graph representation S with:\n\n    S=1/f\u2211^f_t=1s^t=1/f\u2211^f_t=11/J\u2211^J_i=1h^t_i,\n\nwhere S, s^t\u2208\u211d^d are the sequence-level and skeleton-level graph representations, corresponding to a skeleton sequence X and the t^th skeleton x_t in the sequence, respectively. For simplicity of presentation, we use h^t_i to denote the encoded representation of i^th node in the t^th skeleton graph. Here we assume that each node representation with aggregated relational features (see Eq. (<ref>), (<ref>)) contributes equally to the graph representation, and each skeleton graph has the same importance in representing patterns of an individual.\n\n\n\n \u00a7.\u00a7 Graph Prototype Contrastive Learning\n\n\nEach individual's skeletons usually share the same anthropometric features (e.g., skeletal lengths), while their continuous sequence can characterize identity-specific walking patterns (i.e., gait) <cit.>. In this context, it is desirable to mine the most representative skeleton features (defined as \u201cprototypes\u201d) of each individual to learn distinguishable patterns. A straightforward way is to cluster sequence representations to mine prototypes for contrastive learning like <cit.>, while they can only generate identity-agnostic (i.e., pseudo-labeled) prototypes with large uncertainty or unreliability, e.g., when existing large intra-class variation, two same-class sequences with diverse patterns might be clustered to two prototype groups.\nTo encourage the model to generate representatives graph features more reliably, we propose to exploit the graph feature centroid of each ground-truth identity as graph prototypes, which are contrasted with both sequence-level and skeleton-level graph features to learn discriminative representations for person re-ID.\n\nGiven the encoded graph representations {S^T_i}_i=1^N_1 of training skeleton sequences {X^T_i}_i=1^N_1, we group them by ground-truth classes as {\ud835\udd4a_k}_k=1^C, where \ud835\udd4a_k={S_k,j}_j=1^n_k denotes the set of graph representations belonging to the k^th identity, S_k,j is the j^th sequence-level graph representation, and n_k represents the number of k-class sequences. Then, the graph prototype is generated by averaging the graph features of the same class with:\n\n    c_k=1/n_k\u2211^n_k_j=1S_k,j ,\n\nwhere c_k\u2208\u211d^d denotes the graph prototype of the k^th identity. To focus on the representative graph prototype of each identity to learn discriminative identity-related semantics from both skeleton and sequence levels, we propose the graph prototype contrastive (GPC) loss as:\n\n    \u2112_GPC=\u03b1\u2112^seq_GPC + (1-\u03b1)\u2112^ske_GPC,\n\nwhere\n\n    0.8\u2112^seq_GPC=1/N_1\u2211_k=1^C\u2211_j=1^n_k-logexp(S_k, j\u00b7c_k / \u03c4_1)/\u2211_m=1^Cexp(S_k, j\u00b7c_m / \u03c4_1),\n\n\n    0.8\u2112^ske_GPC=1/fN_1\u2211_k=1^C\u2211_j=1^n_k\u2211_t=1^f-logexp(\u2131_1(s^t_k, j) \u00b7\u2131_2(c_k) / \u03c4_2)/\u2211_m=1^Cexp(\u2131_1(s^t_k, j) \u00b7\u2131_2(c_m) / \u03c4_2).\n\nIn Eq. (<ref>), \u03b1 is the weight coefficient to fuse sequence-level (\u2112^seq_GPC) and skeleton-level graph prototype contrastive learning (\u2112^ske_GPC). In Eq. (<ref>) and (<ref>), c_m represents the m-class graph prototype, s^t_k, j denotes the graph representation of the t^th skeleton in the sequence that corresponds to S_k, j belonging to the k^th identity, \u03c4_1, \u03c4_2 are the temperatures for contrastive learning, and \u2131_1(\u00b7), \u2131_2(\u00b7) are linear projection heads to transform skeleton-level graph representations and graph prototypes into the same contrastive space \u211d^d. It should be noted that the graph prototypes are generated from higher level (i.e., sequence-level) representations and the learnable linear projection in Eq. (<ref>) can be viewed as integrating related graph features from both levels for contrastive learning. \n\nThe proposed GPC loss is essentially a generalized skeleton prototype contrastive loss that combines joint-level relation encoding (see Sec. <ref>), skeleton-level and sequence-level graph learning (see Eq. (<ref>)). Its objective can be theoretically modeled as an Expectation-Maximization (EM) solution (see Appendix I).\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Graph Structure-Trajectory Prompted Reconstruction Mechanism\n\n\nTo exploit more valuable graph features and high-level semantics (e.g., pattern consistency) from both spatial and temporal contexts of skeleton graphs, we propose a self-supervised graph Structure and Trajectory Prompted Reconstruction (STPR) mechanism. Motivated by the structural correlations and local motion continuity <cit.> of body components, we devise two graph context based prompts (see Fig. <ref>), namely (1) partial node positions of the same graph and (2) partial node trajectory among continuous graphs, to reconstruct the graph structure and dynamics.\n\n\nGraph Structure Prompted Reconstruction. Given the t^th skeleton graph representation with J encoded nodes (h^t_1,\u22ef,h^t_J) encoded by SGT (see Eq. (<ref>)-(<ref>)), we first randomly mask node positions to generate the masked graph representation as:\n\n    \u015d^t=1/J-a\u2211^J_i=1m_ih^t_i,\n\nwhere a is the number of masks, m_i is the binary mask value (i.e., 0 for masking and 1 for unmasking) applied on the i^th node representation h^t_i, and we have \u2211_i=1^Jm_i=J-a. With both spatial positional information (Eq. (<ref>), (<ref>)) and relational features (Eq. (<ref>), (<ref>)) integrated into each node, the masked graph representation \u015d^t\u2208\u211d^d in Eq. (<ref>) retains the context of graph structure, which is then exploited to prompt the skeleton reconstruction by:\n\n    x\u0302^t=f_s(\u015d^t),\n\nwhere f_s(\u00b7) is an MLP network with one hidden layer for skeleton reconstruction with the structure prompt, and x\u0302^t\u2208\u211d^J\u00d73 is the predicted skeleton. It is worth noting that Eq. (<ref>) not only utilizes the unmasked node representations to reconstruct their corresponding positions, but also exploits them as the context to predict the masked node positions. For conciseness, we denote the predicted i^th training skeleton sequence as X\u0302_i=(x\u0302^1,\u22ef,x\u0302^f) \u2208\u211d^f\u00d7 J\u00d73.\n\n\n\n\nGraph Trajectory Prompted Reconstruction. To encourage the model to capture more unique temporal patterns from skeleton graphs, we propose to reconstruct graph trajectories based on their partial dynamics. In particular, we randomly mask the trajectory of each node with:\n\n    T_i=1/f-b\u2211^f_t=1u^th^t_i,\n\nwhere T_i\u2208\u211d^d is the masked representation of i-node trajectory (i.e., h^1_i, \u22ef, h^f_i), b is the number of trajectory masks, u^t is the binary mask value applied to the t^th position in the i-node trajectory (i.e., i^th node representation of the t^th skeleton graph), and we have \u2211_t=1^fu^t=f-b. The masked trajectory representation T_i preserves partial graph dynamics of different body-joint nodes, which are then used to prompt the skeleton reconstruction by: \n\n    x_i=f_t(T_i).\n\nIn Eq. (<ref>), x_i\u2208\u211d^f\u00d73 denotes the temporal trajectory of the i^th body joints predicted by an MLP network f_t(\u00b7) with one hidden layer. Based on the unmasked node representations over the temporal dimension, this reconstruction facilitates capturing key temporal dynamics and semantics (e.g., continuous patterns) to infer the node positions missed on the trajectory. We represent the predicted i^th training sequence as X_i\u2208\u211d^f\u00d7 J\u00d73 by transposing the original predicted position matrix (x_1,\u22ef,x_J) \u2208\u211d^J\u00d7 f\u00d73.\n\nWe propose the STPR objective to combine both graph structure and trajectory prompted reconstruction as follows:\n\n    \u2112_STPR=\u03b2\u2112_STPR^st+(1-\u03b2)\u2112_STPR^tr,\n\nwhere\n\n    \u2112_STPR^st=1/N_1\u2211_i=1^N_1X\u0302_i-X^T_i_1,\n\n\n    \u2112_STPR^tr\n    = 1/N_1\u2211_i=1^N_1X_i-X^T_i_1.\n\nIn Eq. (<ref>), \u03b2 is the weight coefficient to fuse structure-prompted (\u2112_STPR^st) and trajectory-prompted reconstruction (\u2112_STPR^tr). X^T_i\u2208\u211d^f\u00d7 J \u00d7 3 denotes the ground-truth positions of i^th training skeleton sequence, and \u00b7_1 represents the \u2113_1 norm. We employ \u2113_1 reconstruction loss following <cit.>, as it can alleviate gradient explosion of large losses while providing sufficient gradients for the positions with small losses to facilitate more precise reconstruction. \n\n\n\n \u00a7.\u00a7 The Entire Approach\n\n\nWe combine the proposed GPC (Sec. <ref>) and STPR (Sec. <ref>) to perform skeleton representation learning with:\n\n    \u2112=\u03bb\u2112_GPC+(1-\u03bb) \u2112_STPR ,\n\nwhere \u03bb is the weight coefficient to fuse two losses for training.\nFor person re-ID, we leverage the learned SGT to encode skeleton sequences of the probe set \u03a6_P into sequence-level graph representations, {S^P_i}_i=1^N_2 (see Eq. (<ref>)), which are matched with the representations, {S^G_i}_i=1^N_3, of the same identity in the gallery set \u03a6_G based on Euclidean distance.\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n \u00a7.\u00a7 Experimental Setups\n\nDatasets. Our approach is evaluated on four skeleton-based person re-ID benchmark datasets: IAS <cit.>, KS20 <cit.>, BIWI <cit.>, KGBD <cit.>, which contain 11, 20, 50, and 164 different persons. We also verify the generality of TranSG on the RGB-estimated skeleton data from a large-scale multi-view gait dataset CASIA-B <cit.> with 124 individuals under three conditions (Normal (N), Bags (B), Clothes (C)). We adopt the commonly-used probe and gallery settings following <cit.> for a fair comparison. \n\n\n\n\n\nImplementation Details. The number of body joints is J=20 in KGBD, IAS, BIWI, J=25 in KS20, and J=14 in the estimated skeleton data of CASIA-B.\nThe sequence length is set to f=6 for the Kinect-based skeleton datasets (IAS, KS20, BIWI, and KGBD) and f=40 for the RGB-estimated skeleton data (CASIA-B), following existing methods for a fair comparison. The embedding size of each node representation is d=128. We empirically employ 2 SGT layers with H=8 FR heads and d_k=16 for each layer. Each component in our approach is equally fused with \u03b1=\u03b2=\u03bb=0.5. For models trained with RGB-estimated skeletons, we set \u03b1=1.0. \u03c4_1=0.07 and \u03c4_2=14 are empirically used for contrastive learning. The numbers of random masks are set to a=10 and b=2. We use the Adam optimizer with a learning rate of 3.5\u00d7 10^-4 for model optimization, and the batch size is set to 256 for all datasets. More details are provided in Appendix II.\n\n\nEvaluation Metrics.\nWe compute the Cumulative Matching Characteristics (CMC) curve and adopt Rank-1 accuracy (R_1), Rank-5 accuracy (R_5), and Rank-10 accuracy (R_10) as performance metrics.  Mean Average Precision (mAP) <cit.> is also used to evaluate the overall performance.\n\n\n\n\n \u00a7.\u00a7 Comparison with State-of-the Art Methods\n\n\nWe compare our approach with state-of-the-art graph-based methods, hand-crafted methods, and sequence learning methods on BIWI, KS20, IAS, and KGBD in Table <ref>.\n\n\n\n\nComparison with Graph-based Methods:\n\n\nAs shown in Table <ref>, the proposed TranSG significantly outperforms two state-of-the-art graph-based methods, MG-SCR <cit.> and SM-SGE <cit.>, by 11.7-36.7% for mAP and 12.8-48.6% for Rank-1 accuracy on different datasets. As these methods rely on multi-stage relation modeling and sequence-level context learning, the results demonstrate that the proposed full-relation learning model (SGT) with fine-grained (i.e., graph and node level) semantics learning (STPR) is able to learn more unique skeleton features for person re-ID. Compared with the latest SPC-MGR model <cit.> that utilizes sequence-level graph representations for contrastive learning, our model achieves superior performance with a large margin of 7.5% to 24.5% for mAP and 7.3% to 34.6% for Rank-1 accuracy on all datasets.\n\n\nThis suggests the higher effectiveness of our approach combining both sequence-level and skeleton-level graph prototype contrastive learning. We will also show the generality of our model under different-scale skeleton graph modeling in Sec. <ref>.\n\n\nComparison with Hand-crafted and Sequence Learning Methods:\n\nIn contrast to the methods using hand-crafted anthropometric descriptors (D_13 <cit.>, D_16 <cit.>) or 3D pose features (PoseGait <cit.>), our approach consistently achieves higher performance by up to 27.3% for mAP and 54.7% for Rank-1 accuracy on all datasets.\n\n\nTranSG also achieves a remarkable improvement over existing sequence representation learning models (AGE <cit.>, SGELA <cit.>, SimMC <cit.>) in terms of mAP (7.0-37.3%), Rank-1 (4.1-56.1%), Rank-5 (3.2-67.5%), and Rank-10 accuracy (3.2-70.7%). This demonstrates the superiority of our graph-based model, as it can fully capture body relations and discriminative patterns from skeleton graphs for the person re-ID task.\n\n\n\n\n\n \u00a7.\u00a7 Ablation Study\n\n\nWe conduct ablation study to verify the effectiveness of each component in our approach. The skeleton sequences of concatenated joints are adopted as the baseline, and we include the na\u00efve prototype contrastive learning (PC) using original sequences <cit.> for comparison. As shown in Table <ref>, compared with using raw skeleton sequences or PC without graph modeling, applying SGT achieves significantly better performance in most cases, regardless of using GPC or not.\nThis demonstrates the effectiveness of the skeleton graph learning with SGT, as it can fully capture relations within skeletons to learn unique body structure and motion features for person re-ID. \nThe SGT employing GPC achieves superior results than \u201cSGT + DS\u201d that uses direct supervised learning (i.e., cross-entropy loss) with an improvement of 4.4-14.9% for mAP and 5.5-24.2% for Rank-1 accuracy, which verifies the key role of the graph prototype contrastive learning in capturing more representative discriminative graph features from different identities. Finally, adding STPR consistently boosts model performance by 1.4-3.7% for mAP on all dataset, which further demonstrates its effectiveness on capturing more valuable graph semantics and discriminative patterns for person re-ID.\n\n  \n\n\n\n\n\n\n\u00a7 FURTHER ANALYSIS\n\n\n\n\nEvaluation on RGB-estimated Skeletons.\nTo verify the generality of our skeleton-based model on RGB-estimated skeletons, we utilize pre-trained pose estimation models to extract skeleton data from RGB videos of CASIA-B <cit.>, and evaluate the performance of TranSG under different conditions. As presented in Table <ref>, our approach not only outperforms many existing state-of-the-art skeleton-based models with a prominent improvement in most conditions, but also achieves superior performance to representative classical appearance-based methods that utilize RGB-based features (e.g., textures, silhouettes) or/and visual metric learning <cit.>. \nThis demonstrates the stronger ability of TranSG on capturing more discriminative features from the estimated skeletons, and also suggests its great potential to be applied to more general RGB-based scenarios.\n\n\n\nApplication to Skeleton Graphs with Varying Scales. \nTo validate the effectiveness of our approach under different graph modeling, we follow <cit.> to construct different-scale graphs for model learning. As shown in Table <ref>, TranSG significantly outperforms the state-of-the-art framework SM-SGE <cit.> when utilizing the original skeleton graphs (corresponding to J joints) or higher level graphs with less nodes.\n This demonstrates that our model is compatible with different-level skeletal structures and can learn more effective features even under different graph modeling.\n\n\n\n\n\n\nTransfer to Unsupervised Scenarios. To apply our approach in an unsupervised manner without using ground-truth labels, we follow <cit.> to perform DBSCAN clustering <cit.> of graph representations, and leverage their pseudo classes to generate graph prototypes for contrastive learning. With only unlabeled skeletons as inputs, TranSG can still achieve superior performance than the latest graph-based method SPC-MGR <cit.> in most cases, as shown in Table <ref>. \nThis further demonstrates the generality of our approach, which could be promisingly transferred to more related tasks such as unsupervised open-set person re-ID.\n\n\n\n\n\n\n\n\n\n \n\n\n\nDiscussions. As presented in Fig. <ref>, we show effects of different hyper-parameters on our approach. An appropriate fusion of different components can encourage better model performance, while setting \u03b1, \u03b2, and \u03bb to 0.5 achieves slightly better results. Adding too many FR heads or SGT layers could slightly reduce the performance, as it might expand the model scale and learn more redundant information. Our model is not sensitive to the variation of some parameters such as temperature \u03c4_1, while setting a moderate value for mask numbers benefits model performance. \nThe t-SNE visualization <cit.> in Fig. <ref> shows that our learned skeleton representations possess more discriminative inter-class separation than other methods <cit.>, which indicates that TranSG may capture richer class-related semantics. \nMore empirical and theoretical analyses are in the appendices.\n\n\n\n\u00a7 CONCLUSION\n\nIn this paper, we propose TranSG to learn effective representations from skeleton graphs for person re-ID. We devise a skeleton graph transformer (SGT) to perform full-relation learning of body-joint nodes to aggregate key body and motion features into graph representations. A graph prototype contrastive learning (GPC) approach is proposed to learn discriminative graph representations by contrasting their inherent similarity with the most representative graph features. Furthermore, we design a graph structure-trajectory prompted reconstruction (STPR) mechanism to encourage learning richer graph semantics and key patterns for person re-ID. TranSG outperforms existing state-of-the-art models, and can be scalable to be applied to different scenarios.\n\n\n\n\n\n\n\u00a7 ACKNOWLEDGEMENTS\n\nThis research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD/2022-01-034[T]).\n\n\n\n\nieeetr\n\n\n"}