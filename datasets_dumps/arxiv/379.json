{"entry_id": "http://arxiv.org/abs/2303.06806v1", "published": "20230313012855", "title": "Neural Diarization with Non-autoregressive Intermediate Attractors", "authors": ["Yusuke Fujita", "Tatsuya Komatsu", "Robin Scheibler", "Yusuke Kida", "Tetsuji Ogawa"], "primary_category": "eess.AS", "categories": ["eess.AS", "cs.CL", "cs.SD"], "text": "\n\nBoosting Source Code Learning with Data Augmentation: An Empirical Study\n    Jianjun Zhao\n    \n========================================================================\n\n\n\n\nEnd-to-end neural diarization (EEND) with encoder-decoder-based attractors (EDA) is a promising method to handle the whole speaker diarization problem simultaneously with a single neural network.\nWhile the EEND model can produce all frame-level speaker labels simultaneously, it disregards output label dependency.\nIn this work, we propose a novel EEND model that introduces the label dependency between frames.\nThe proposed method generates non-autoregressive intermediate attractors to produce speaker labels at the lower layers and conditions the subsequent layers with these labels.\nWhile the proposed model works in a non-autoregressive manner, the speaker labels are refined by referring to the whole sequence of intermediate labels.\nThe experiments with the two-speaker CALLHOME dataset show that the intermediate labels with the proposed non-autoregressive intermediate attractors boost the diarization performance.\nThe proposed method with the deeper network benefits more from the intermediate labels, resulting in better performance and training throughput than EEND-EDA.\n\n\n\n\nend-to-end neural diarization, intermediate objective, self-conditioning, attractor, speaker diarization\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\n\n\n\n\n\n\n\nSpeaker diarization is the task of detecting multi-speaker speech activity in audio recordings.\nIt has been actively studied as an essential component for conversational speech understanding <cit.>.\nThe task has been evaluated in telephone conversations (CALLHOME <cit.>), meetings (ICSI <cit.>, AMI <cit.>), web videos (VoxConverse <cit.>) and various hard scenarios (DIHARD Challenges <cit.>).\n\nA standard approach to speaker diarization is speaker embedding clustering <cit.>, which first extracts speaker-discriminative embeddings like x-vectors <cit.> and d-vectors <cit.> for fixed-length speech segments, and then merges homogeneous segments to the same speaker by applying a clustering algorithm such as spectral clustering <cit.> and agglomerative hierarchical clustering <cit.>.\nFor speaker embedding clustering, voice activity detection should be done in advance to determine speech/non-speech boundaries, and overlapping speech segments have to be eliminated with some pre- or post-processing methods<cit.>.\n\nAs an alternative to speaker embedding clustering, end-to-end neural diarization (EEND) <cit.> has been proposed.\nEEND learns a neural network to directly produce full speaker diarization labels containing speech/non-speech boundaries and overlapping speech segments, as well as speaker assignments for the detected speech segments.\nSince the speaker diarization labels have permutation ambiguity due to the arbitrary order of speaker indices, the EEND network is trained with permutation-invariant training objectives.\nVarious network architectures have been investigated for EEND including Transformer <cit.>, time-dilated convolutional neural network <cit.>, and Conformer <cit.>.\nEncoder-decoder-based attractor (EDA) <cit.> is a promising network architecture for EEND, which first generates an attractor vector for each speaker from an embedding sequence, and then generates the speaker's activity by measuring the similarity between the embedding sequence and the attractor vector.\n\nAlthough the EEND-EDA model is found to be effective compared with speaker embedding clustering, there are two points of improvement.\nFirst, the model assumes conditional independence between frame-level speaker labels, which limits their performance.\nThe EEND-EDA model runs in a non-autoregressive manner and produces all frame-level speaker labels in parallel.\nHowever, such non-autoregressive models disregard the potential benefit of the output label dependency between frames.\nSecond, the long-short term memory (LSTM)-based encoder in EDA receives the frame-level embeddings recursively.\nThe well-known vanishing gradient problem of LSTM hinders the optimization of the lower layers.\n\nIn related fields, researchers have studied the use of \u201cintermediate\u201d labels to relax the conditional independence assumption in such non-autoregressive models.\nFor non-autoregressive automatic speech recognition (ASR) based on connectionist temporal classification (CTC), Intermediate CTC <cit.> is proposed to introduce auxiliary tasks of predicting labels inside the network by inserting the same CTC losses to the intermediate predictions.\nSelf-conditioned CTC <cit.> further utilizes the intermediate labels at the lower layer as conditions for enhancing the predictions at the upper layers.\nThis self-conditioning technique achieves the best performance among the non-autoregressive ASR systems <cit.>.\nFor EEND, a similar intermediate speaker label prediction technique <cit.> is proposed, which uses the same permutation-invariant training objectives for the intermediate speaker labels.\nHowever, the intermediate speaker labels are not utilized for the self-conditioning features.\nThese prior studies motivate us to introduce the self-conditioning technique in EEND models.\n\nIn this paper, we propose a novel network architecture for EEND that uses intermediate speaker labels to condition the subsequent network layers.\nFor producing the intermediate speaker labels, the proposed method extracts intermediate attractors at every encoder layer.\nThe auxiliary permutation-invariant training losses are introduced for optimizing the intermediate labels.\nFor conditioning the subsequent network layers, the proposed method adds the weighted intermediate attractors to the frame-level embeddings. While the proposed network still works in a non-autoregressive manner, the speaker labels are iteratively refined by referring to the whole sequence of intermediate speaker labels.\nFor the vanishing gradient problem in the EDA's LSTM, we adopt the attention mechanism <cit.>.\nUnlike LSTM, it does not suffer from vanishing gradients, thus facilitating optimization of the lower layers.\nAnother advantage of the attention mechanism is that it is non-autoregressive, so training throughput is much higher than LSTM when interacting with the intermediate training objectives.\nThe experimental results with the two-speaker CALLHOME dataset show that the intermediate labels with the proposed non-autoregressive intermediate attractors boost the diarization performance while the original EDA cannot get benefit from the intermediate labels.\nThe proposed method with the deeper network benefits more from the intermediate labels, resulting in better performance and training throughput than EEND-EDA.\n\n\n\n\u00a7 METHOD\n\n\nThis section briefly introduces EEND-EDA as our baseline system, followed by our proposed method: intermediate attractors.\n\n\n\n \u00a7.\u00a7 End-to-end Neural Diarization\n\n\nEEND formulates the speaker diarization problem as a frame-wise multi-label classification task <cit.>.\nIn this paper, we denote X \u2208\u211d^D\u00d7 T as a T-length sequence of D-dimensional audio features.\nA neural network accepts X and produces the same-length sequence of speaker label posteriors Y \u2208 [0,1]^C\u00d7 T, where C is the number of speakers and [Y]_c,t is the probability that c-th speaker is speaking at time t. \nThe network is trained to minimize the binary cross-entropy loss between the ground-truth speaker labels Y^* \u2208{0,1}^C\u00d7 T and the estimated label posteriors Y:\n\n    \u2112_\ud835\uddaf\ud835\udda8\ud835\uddb3(Y^*, Y) = 1/CTmin_\u03d5\u2208\ud835\udcab(C)\u2211_c=1^C \u2211_t=1^T \ud835\udda1\ud835\udda2\ud835\udda4([Y^*]_\u03d5_c,t, [Y]_c,t),\n\nwhere \ud835\udda1\ud835\udda2\ud835\udda4(y^*,y) = - y^*log y - (1-y^*)log(1-y), \ud835\udcab(C) is the set of all permutations of a sequence {1,\u2026,C}.\nThis permutation-invariant training scheme <cit.> correctly handles the label ambiguity caused by the arbitrary order of speaker indices.\n\n\n\n \u00a7.\u00a7 EEND with Encoder-decoder-based Attractors\n\n\nThe neural network for EEND adopted in <cit.> comprises of a stack of Transformer encoders:\n\n    E_l = \ud835\udda4\ud835\uddc7\ud835\uddbc\ud835\uddc8\ud835\uddbd\ud835\uddbe\ud835\uddcb\ud835\uddab\ud835\uddba\ud835\uddd2\ud835\uddbe\ud835\uddcb_l(E_l-1) \u2208\u211d^D\u00d7 T     (1 \u2264 l \u2264 L),\n\nwhere L is the number of Transformer layers and E_0 = X [In this paper, we assume X is subsampled beforehand.].\nWhereas the vanilla EEND <cit.> simply transforms E_L to Y by a linear layer followed by a sigmoid function, EDA <cit.> first generates speaker-wise attractor vectors A = [a_1, \u2026, a_C] \u2208\u211d^D\u00d7 C:\n\n    A = \ud835\udda4\ud835\udda3\ud835\udda0(E_L).\n\nHere, this EDA function is implemented using long short-term memory (LSTM) layers:\n\n    (h_t, c_t) = \ud835\uddab\ud835\uddb2\ud835\uddb3\ud835\uddac_\ud835\uddbe\ud835\uddc7\ud835\uddbc(h_t-1, c_t-1, [E_L]_:,t)    (1 \u2264 t \u2264 T), \n    \n        (a_c, d_c) = \ud835\uddab\ud835\uddb2\ud835\uddb3\ud835\uddac_\ud835\uddbd\ud835\uddbe\ud835\uddbc(a_c-1, d_c-1, 0)    (1 \u2264 c \u2264 C),\n\nwhere \ud835\uddab\ud835\uddb2\ud835\uddb3\ud835\uddac_\ud835\uddbe\ud835\uddc7\ud835\uddbc() is an unidirectional LSTM layer that sequentially reads an embedding vector for time t, [E_L]_:,t is the embedding vector from the t-th column of E_L, h_t \u2208\u211d^D is a hidden state, c_t \u2208\u211d^D is a cell state.\n\ud835\uddab\ud835\uddb2\ud835\uddb3\ud835\uddac_\ud835\uddbd\ud835\uddbe\ud835\uddbc() is another unidirectional LSTM layer with the initial hidden state a_0 = h_T and the initial cell state d_0 = c_T. The decoder LSTM receives zero vector C times to produce the speaker-wise attractor vector a_c for C speakers.\n\nEEND-EDA estimates the speaker label by comparing the embedding sequence E_L with the speaker-wise attractors A:\n\n    Y = \ud835\uddb2\ud835\uddc2\ud835\uddc0\ud835\uddc6\ud835\uddc8\ud835\uddc2\ud835\uddbd(A^\u22a4E_L).\n\nAs the number of attractor vectors can vary with the number of iterations C in Eq.\u00a0<ref>, EDA can handle an unknown number of speakers by jointly estimating the number of iterations. In this paper, we fix the number of iterations to two since we only evaluate the method on two-speaker conversations.\n\nWith EEND-EDA described above, the speaker labels at all frames are estimated in parallel.\nThis parallel estimator lacks a mechanism handling the label dependency between frames.\nIn the next subsection, we consider the label dependency between frames by using intermediate speaker labels.\n\n\n\n \u00a7.\u00a7 Intermediate Attractors\n\n\n\n\n\nConsidering the label dependency between frames, we extract \u201cintermediate\u201d speaker labels and feed them into the subsequent network layers.\nBecause the upper layer can refer to the whole sequence of intermediate speaker labels from the lower layer, conditional independence between frames is relaxed.\nThe overview of the proposed method with the four-layer EEND-EDA model is depicted in Fig. <ref>.\n\nFor producing the intermediate speaker labels, intermediate attractors are generated with EDA.\nUsing the same EDA components in Eq.\u00a0<ref>, intermediate attractors for the l-th layer are calculated as follows:\n\n    A_l = \ud835\udda4\ud835\udda3\ud835\udda0(E_l).\n\nThen, intermediate speaker labels Y_l for each layer are estimated using the intermediate attractors A_l, similar to Eq. <ref>:\n\n    Y_l = \ud835\uddb2\ud835\uddc2\ud835\uddc0\ud835\uddc6\ud835\uddc8\ud835\uddc2\ud835\uddbd(A_l^\u22a4E_l)      (1 \u2264 l \u2264 L-1)\n\nThe auxiliary permutation-invariant training loss is introduced for the intermediate speaker labels, and the model is trained with the summation of the original loss (Eq. <ref>) and the intermediate losses [We can take the weighted loss by introducing a mixing ratio. However, we ignore the hyperparameter in this work.]:\n\n    \u2112_\ud835\uddc2\ud835\uddc7\ud835\uddcd\ud835\uddbe\ud835\uddcb = \u2112_\ud835\uddaf\ud835\udda8\ud835\uddb3(Y^*, Y) + 1/L-1\u2211_l=1^L-1\u2112_\ud835\uddaf\ud835\udda8\ud835\uddb3(Y^*, Y_l).\n\n\nFor conditioning the subsequent network layers, the weighted intermediate attractors are added to the input embeddings.\nEq.\u00a0<ref> is modified by inserting the conditioning function for the input embedding:\n\n    E_l = \ud835\udda4\ud835\uddc7\ud835\uddbc\ud835\uddc8\ud835\uddbd\ud835\uddbe\ud835\uddcb\ud835\uddab\ud835\uddba\ud835\uddd2\ud835\uddbe\ud835\uddcb_l(\ud835\udda2\ud835\uddc8\ud835\uddc7\ud835\uddbd\ud835\uddc2\ud835\uddcd\ud835\uddc2\ud835\uddc8\ud835\uddc7(E_l-1)), \n    \ud835\udda2\ud835\uddc8\ud835\uddc7\ud835\uddbd\ud835\uddc2\ud835\uddcd\ud835\uddc2\ud835\uddc8\ud835\uddc7(E_l) = E_l + W A_l Y_l,\n\nwhere W \u2208\u211d^D\u00d7 D is learnable parameters that control the weights of intermediate predictions.W is the only additional parameter introduced for the proposed method and is shared among L-1 layers.\nA_l Y_l in Eq.\u00a0<ref> can be interpreted as a T-length sequence of weighted averages of attractor vectors, and the weights are determined by the intermediate speaker labels. Through the intermediate attractors, the subsequent layers are conditioned on the intermediate speaker labels.\n\n\n\n \u00a7.\u00a7 Non-autoregressive Attractor Extraction\n\n\n\n\n\nFor training efficiency with the intermediate speaker labels, we propose a non-autoregressive extraction of speaker-wise attractors instead of the LSTM-based autoregressive extractor used in EDA <cit.>. The difference is depicted in Fig.\u00a0<ref>.\nWe prepare query vectors Q \u2208\u211d^C\u00d7 D for the attractors as learnable parameters through training.\nA cross-attention module extracts the attractors by using the query vector and the frame-wise embeddings E_L as keys and values:\n\n    A = \ud835\udda0\ud835\uddcd\ud835\uddcd\ud835\uddc7(Q, E_L, E_L),\n\nwhere \ud835\udda0\ud835\uddcd\ud835\uddcd\ud835\uddc7 is a multi-head attention layer used in Transformer decoders <cit.>.\nThe intermediate attractors are extracted by using the same attention layer:\n\n    A_l = \ud835\udda0\ud835\uddcd\ud835\uddcd\ud835\uddc7(Q, E_l, E_l)    (1 \u2264 l \u2264 L-1).\n\nSpeaker labels and their intermediates are estimated using Eqs.\u00a0<ref> and <ref>, respectively.\nThe intermediate labels are utilized to condition the subsequent layers using Eqs.<ref> and <ref>.\n\nNote that the original EDA can work with an unknown number of speakers, while our non-autoregressive extraction generates a fixed number of attractors.\nOur method can be extended to an unknown number of speakers by adding a speaker-counting objective for the attractors, similar to EDA <cit.>.\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n \u00a7.\u00a7 Data\n\n\nThe statistics of the datasets are listed in Table <ref>.\nWe followed the family of EEND works <cit.> to prepare test data for two-speaker telephone conversations extracted from CALLHOME [The data preparation code is available at <https://github.com/hitachi-speech/EEND>].\nWe call CH-adapt for the adaptation dataset and CH-test for the test dataset.\n\nSimulated two-speaker audio mixtures were used for a training set.\nThe source audio corpora were Switchboard-2 (PhaseI, II, III), Switchboard Cellular(Part1, 2), and the NIST Speaker Recognition Evaluation (2004, 2005, 2006, 2008). MUSAN corpus <cit.> was used for adding noise.\nWith a recently proposed mixture simulation algorithm <cit.>, simulated conversations (SimConv) were prepared using the statistics of the CH-adapt dataset.\nNote that reverberation was not applied, which was the best configuration in <cit.>.\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Model Configurations\n\n\nEEND-EDA:\nWe used PyTorch-based implementation of EEND-EDA [<https://github.com/BUTSpeechFIT/EEND>] used in <cit.>.\nOur baseline EEND-EDA model was exactly the same configuration as described in <cit.>, which is also the same as the original EEND-EDA paper <cit.>.\nAudio features were 23-dimensional log-scale Mel-filterbanks computed every 10 msec.\nFor input to the neural network, 15 consecutive audio features were stacked to form 345-dimensional vectors every 100 msec.\nWe used four Transformer encoder layers with 256 attention units containing four heads.\nTraining configuration was also the same as <cit.>.\nThe training batch was a collection of 50-sec segments in the training set.\nThe batch size was 32.\nAdam optimizer was used with the Noam learning scheduler and 200k linear warm-up steps.\nThe training was run for 100 epochs with the training set.\nFor adaptation, we run 100 epochs using the adaptation set with a learning rate of 10^-5.\nAfter training, the last 10 model checkpoints were averaged.\n\nEEND-EDA+InterLoss:\nThe model is built with additional computations (Eqs.\u00a0<ref>-<ref>) introducing intermediate losses to the EEND-EDA model as described in Section\u00a0<ref>.\n\nEEND-EDA+SelfCond is built with additional computations  (Eqs.\u00a0<ref>-<ref>) to apply the self-conditioning technique with the intermediate attractors.\n\nEEND-NA:\nWe built a model named EEND-NA as EEND with the proposed non-autoregressive attractor extraction as described in Sec.\u00a0<ref>.\nAn +InterLoss model and a +SelfCond model were built on top of the EEND-NA model, similar to the EEND-EDA models.\n\nEEND-EDA-deep and EEND-NA-deep:\nWe investigated the effect of deeper networks by increasing the number of Transformer blocks from four to eight.\n\n\n\n \u00a7.\u00a7 Metrics\n\n\nThe diarization error rates (DERs) were evaluated with a forgiveness collar of 0.25s.\nWe also showed DER breakdown into miss, false alarm (FA), and confusion (CF) error rates, followed by speech activity detection (SAD) miss and false alarm errors.\nThe number of parameters (#Params) and training throughput (Tp) for each model were shown to discuss the results with training efficiency. The training throughput was calculated on a Tesla V100 32G GPU.\n\n\n\n \u00a7.\u00a7 Results\n\n\nTable <ref> shows the DERs on CH-test.\nThe first three rows show that the conventional EEND-EDA model could not benefit from the proposed intermediate labels and the self-conditioning technique.\nTraining throughput was down to one-third.\nThe results indicate that the LSTM-based autoregressive attractors cannot optimize the intermediate frame-level embeddings.\n\nOn the other hand, the proposed EEND-NA model showed performance improvement with the intermediate labels and the self-conditioning technique.\nThe training throughput of the proposed model was higher than that of the EEND-EDA model, and the slowdown by introducing the intermediate labels and self-conditioning was in an acceptable range.\nThe EEND-NA+SelfCond model reached a similar performance to the EEND-EDA model, while the proposed model has less number of parameters and higher throughput than the EEND-EDA model.\nThe results suggest that the non-autoregressive attractors can help optimize the intermediate frame-level embeddings, unlike EDA.\nThe EEND-NA model itself was worse than EEND-EDA, although the intermediate labels reduce the difference.\nThe disadvantage of the non-autoregressive attractors may come from the conditional independence between speakers.\nUsing conditional inference on previously estimated speakers, like in the decoder part of EDA, may improve performance.\n\nTable\u00a0<ref> also shows the results of deeper (eight-layer) models.\nThe proposed EEND-NA-deep+SelfCond model achieved the best performance among the evaluated models.\nWhile the conventional EEND-EDA-deep model achieved better performance than the original four-layer EEND-EDA model, we could not get the results with intermediate labels because of the slow training.\nWe expect that the EEND-EDA-deep model cannot benefit from the intermediate labels as with the baseline EEND-EDA models.\n\n\n\n \u00a7.\u00a7 Results with Intermediate Speaker Labels\n\n\nTable\u00a0<ref> shows diarization error rates with intermediate speaker labels produced by the EEND-NA-deep+SelfCond model.\nThe errors were consistently reduced layer by layer.\nThe results indicate that optimizing the speaker label at the lower layers improves the final diarization performance.\nUnexpectedly, the seventh-layer results were better than the last eighth-layer results.\nWe think that conditioning with the intermediate labels could be enhanced by selecting the set of intermediate layers: not all the layers.\n\n\n\n\n\u00a7 CONCLUSION\n\n\nWe proposed an end-to-end diarization model that uses intermediate speaker labels to condition the subsequent network layers.\nThe experiments showed that the intermediate labels with the non-autoregressive intermediate attractors boosted the diarization performance.\nThe proposed method with the deeper network benefits more from the intermediate labels, resulting in better performance and training throughput than EEND-EDA.\n\n\nIEEEbib\n\n\n"}