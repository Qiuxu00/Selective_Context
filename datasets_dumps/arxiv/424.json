{"entry_id": "http://arxiv.org/abs/2303.06726v1", "published": "20230312184429", "title": "Global Optimality of Elman-type RNN in the Mean-Field Regime", "authors": ["Andrea Agazzi", "Jianfeng Lu", "Sayan Mukherjee"], "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "text": "\n\n\nWe analyze Elman-type Recurrent Reural Networks (RNNs) and their training in the mean-field regime. Specifically, we show convergence of gradient descent training dynamics of the RNN to the corresponding mean-field formulation in the large width limit. We also show that the fixed points of the limiting infinite-width dynamics are globally optimal, under some assumptions on the initialization of the weights. Our results establish optimality for feature-learning with wide RNNs in the mean-field regime.\n\n\nProbing the Origin of Changing-look Quasar Transitions with Chandra\n    [\n    \n===================================================================\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nDuring the last decade, artificial intelligence and in particular deep leaning have achieved a significant series of groundbreaking successes, partly due to the unprecedented increase of data and computational power at our disposal. Notably, the range of disciplines that have recently been revolutionized by machine learning is virtually unlimited: from medicine <cit.> to finance <cit.>, from games <cit.> to image analysis <cit.>, to the point where almost no domain has remained unaltered by the emergence of these technologies.\n\nThis revolution would have been unthinkable without the advent of deep neural networks. This extremely flexible family of function approximators has outperformed classical methods in almost every domain where it has been applied.\nA discipline that has been profoundly revolutionized by these models is the analysis of time series and, more generally, the problem of learning dynamical systems. For instance, Recurrent Neural Networks (RNNs) <cit.> and more specifically Long-Short Term Memory (LSTM) RNNs <cit.> and Gated Recurrent Units <cit.> have dramatically increased the predictive performance of machine learning in this context. These models take as input temporal sequences of data and act iteratively on the elements of such sequences, storing the information about previous timepoints into the hidden state of the network. This structure allows to learn datasets with strong time-correlations using relatively few parameters, and has provided benchmarks for state-of-the-art time-series learning algorithms for over a decade.\n\nHowever, despite the groundbreaking success of these models in practice, the theoretical underpinnings of such success remain elusive to the computer science community. More specifically, many questions about the theoretical reasons for the performance of these models applied far into the overparametrized regime, such as for example explanations for their optimal behavior and their generalization error, remain open.\n\n\nOnly recently, a theory of neural\nnetwork learning has started to emerge in the context of wide, single-layer neural networks. The two main theoretical frameworks are based on either understanding mean-field training dynamics\n<cit.> or based on linearized dynamics in the overparametrized regime <cit.>.\nThese two frameworks provide contrasting explanations for the success of neural networks. \u00e5On one hand the linearized dynamics gives strong convergence guarantees for the training process but fails to explain the feature-learning properties of neural networks. On the other hand the the mean-field framework is better at  accurately\ncapturing the\nhighly nonlinear dynamics arguably resulting in feature-learning <cit.>, but the resulting training process is typically harder to analyze. Consequently, it remains a challenge to extend the mean-field results listed above to more realistic structures such as RNNs.\n\nThis paper aims to extend the mean-field framework to Elman-type RNNs. Specifically, we aim to establish optimality of the fixed points of the training dynamics for wide RNNs trained with classical gradient descent. This provides an explanation of the outstanding performance of these models, in a certain idealized regime.\n\n\n\n \u00a7.\u00a7 Previous results\n\n\nThe training dynamics of neural networks in the mean-field infinite width limit was pioneered by the series of papers <cit.>. Here, the authors proved that the training dynamics of infinitely wide, single layer neural networks in the mean-field regime can be studied by representing the parametric state of the network as a probability distribution in the space of weights. Using this representation it was possible to prove that\nthe limiting points of the training dynamics are global optimizers of the loss function.\n\nThese ideas have been extended to the reinforcement learning setting <cit.>, to non-differeniable network nonlinearities such as ReLU <cit.> and to the deep ResNet architecture <cit.>. A recent series of papers <cit.> has bypassed the difficulties related to the representation of the network state as a distribution by introducing the neuronal embedding framework. This framework allows for the investigation of the mean-field dynamics of deep, purely feedforward neural networks. However, none of these results can be directly applied to the RNN setting: \u00e5the presence of weight-sharing in  the RNN structure and the interaction of the unrolled network with the input violate fundamental assumptions in these analyses. \n\nThe performance of RNNs in the infinite width limit was studied in <cit.>. Here, the authors explored the performance of the network in the so-called Neural Tangent Kernel (NTK) regime, arising under a particular scaling of the weights at initialization. This scaling linearizes the training dynamics of the network, which behaves essentially like a kernel method. It is therefore widely believed that in this regime feature learning is not possible. A recent paper <cit.> considers a similar scaling to <cit.> in combination with more general architectures. In contrast to these works, the mean-field scaling we consider retains the nonlinear training dynamics.\n\nFinally, a seemingly related result about mean-field theory for RNNs has been presented in <cit.>. That work, however, uses dynamical mean-field theory to explain the role of gating in RNN architectures and thus our proof techniques differ greatly from that paper. \u00e5The scope of the results is also significantly different, as their results aim to explore forward propagation of signal\nthrough vanilla RNNs, and do not aim to establish optimality of the fixed points after training.\n\n\n\n \u00a7.\u00a7 Contributions\n\n\nThis paper adapts the neuronal embedding analysis framework developed in <cit.> to unrolled Elman-type RNNs <cit.>. We prove optimality of the fixed points of the training\ndynamics in the mean-field regime under some  assumptions \u00e5on the expressive power of the\nnetwork at initialization. Specifically we prove:\n\n\n\n  * Convergence of the dynamics of the finite-width RNN to its infinite-width limit. To do so we adapt the coupling formulation presented in <cit.> in the context of fully-connected feedforward networks to the RNN framework, thereby extending it to networks with weight-sharing.\n\n  * Gradient descent trains these networks to optimal fixed points given infinite training time. This optimality result holds in the feature-learning regime, as opposed to previous results that hold in the NTK regime.\n\n  * \u00e5To prove the above results, we show universal approximation for deep neural networks with uniformly bounded hidden weights. This result extends classical universal approximation theorems, where weights are critically assumed to be in a vector space and, as such, to be unbounded.\n\nA standard initialization assumption in feedforward neural networks, for example 3-layer networks, with a large number of nodes is to initialize the weights randomly and independently. In this paper, we further observe that feedback in an RNN requires stronger assumptions on the weights of the network at initialization to achieve a comparable level of expressivity as a 3-layer feedforward network. We examine this issue in some detail through our analysis.\n\n\n\n\nThe paper is organized as follows: in Section\u00a0<ref> we introduce the notation and the model being investigated, together with its mean-field limit. Then, in Section\u00a0<ref> we outline our main results. The results are exemplified with some numerical experiments in Section\u00a0<ref>, and conclusions follow in Section\u00a0<ref>. The proofs of our main theorems are given in the appendix.\n\n\n\n\n\u00a7 NOTATION\n\n\n\n\n \u00a7.\u00a7 Predictors\n\nTo put the data-generation process in an abstract framework for dynamical systems, we consider as predictors subsets of a bi-infinite observation sequence \ud835\udc31\u2208  (\u211d^)^\u2124. For a given subshift :\u00a0(\u211d^)^\u2124\u2192 (\u211d^)^\u2124, we generate the elements of  as \ud835\udc31_k+1 = (\ud835\udc31)_k.\n We make the following assumption on the underlying dynamical system\n\n\n \n\n\n\n\n\n  There exists a continuous function T\u00a0:\u00a0\u211d^\u2192\u211d^ such that \ud835\udc31_k+1 = T(\ud835\udc31_k)  for all k \u2208\u2124. We further assume that this map is uniquely ergodic \u00e5(upon possibly restricting it to a forward invariant set \ud835\udd4f) and that the corresponding invariant measure has finite fourth moments. For the definition of unique ergodicity, see <cit.>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe denote by \u03a0^0\u00a0:\u00a0(\u211d^)^\u2124\u2192\u211d^ the projection of a bi-infinite sequence on its 0-th element. We further define \u03bd\u2208\u2133_+^1(\u211d^\u2124) as the invariant measure of the map \ud835\udcaf, which exists and is unique by the above assumption. The marginal \u03bd_0\u2208\u2133_+^1(\u211d) on the 0-th component of \u03bd  is the invariant measure of T, and \u03a0_#^0 \u03bd = \u03bd_0.\n\n\n\n\n \u00a7.\u00a7 Loss function\n\nWe assume that we have access to an infinite-length sample from the invariant measure \u03bd, from a dynamical system satisfying  a:01a to train the RNN.\nOur objective is to learn a map F^*\u00a0:\u00a0(\u211d^)^\u2115\u2192\u211d  from sequences of arbitrary length to reals. We restrict our attention to functions with a fixed, finite memory L \u2208\u2115.\n\nThe function F^* only depends on {\ud835\udc31_-L, \u2026, \ud835\udc31_0}, for a fixed L \u2208\u2115.\n\nOur objective is to learn an estimate of F^*\nby minimizing the\nsample Mean Squared Error (MSE) between the target function F^* and a parametric family of estimators {F( \u00b7 ;W)}_W indexed by the parameter vector W on an observation sequence of length K. In other words, we aim to find\nthe minimizer F\u0302\u2208{F( \u00b7 ;W)}_W of the empirical risk\n\n\u2112_K(F^*, F\u0302) := 1/K \u2211_k = 1^K 1/2(F^*(^k(\ud835\udc31)) - F\u0302(^k(\ud835\udc31))^2 .\n\nIn the large sample limit K \u2192\u221e, the above loss function can be rewritten as the population risk\n\n\u2112(W)  = lim_K \u2192\u221e \u2112_K(F^*( \u00b7 ), F\u0302( \u00b7 ;W)) =   1/2 \u222b(F^*(\ud835\udc31) - F\u0302(\ud835\udc31;W))^2 \u03bd(x) ,\n\nexpressed above as a function of the parameters of the estimator. While our analysis extends to more general loss functions, for concreteness and ease of exposition we restrict our discussion to the MSE.\n\n\n\n \u00a7.\u00a7 RNN structure\n The family of models we consider are Elman-type Recurrent Neural Networks  of hidden width n \u2208\u2115. Such a neural network can be written as\n\n\nF\u0302(\ud835\udc31;\ud835\udc16)     = \n\ud835\udc07_hy(\ud835\udc31) \n\n\ud835\udc07_hy(\ud835\udc31)     =  1/n \u03c3_h(\ud835\udc07_hh(\ud835\udc31,0) + \ud835\udc07_xh(\ud835\udc31_0))\n\n\ud835\udc07_hh(\ud835\udc31, k)     =  1/n  \u03c3_h(\ud835\udc07_hh(\ud835\udc31,k+1) + \ud835\udc07_xh(\ud835\udc31_-(k+1)))\n\n\ud835\udc07_hh(\ud835\udc31, L)     = 0\n\n\ud835\udc07_xh(\ud835\udc31_k)     = \u00b7\ud835\udc31_k\n\n\nwhere we assume that \ud835\udc31_k \u2208\u211d^, \u2208\u211d^n \u00d7, \u2208\u211d^ n, \u2208\u211d^n \u00d7 n and activation function \u03c3_h\u00a0:\u00a0\u211d\u2192\u211d\nis applied component-wise.   The structure of the network is represented in f:structure.\n\n\n\nTo investigate the convergence properties of RNNs as n\u2192\u221e, we will apply the neuronal embedding formalism from <cit.>.\nThis formalism lifts the labeling of the neurons of the network to an abstract probability space (, \u2131_h, ), and the neural network weights are interpreted as a function of these abstract indices. This lifting allows for the representation of\nany network as a specific choice of labelings, and\n equivalent relabelings of the neural network weights are different realizations of an abstract (random) labeling process.\nIn this formalism, the weight functions can then be written as\n\n(\u03b8)\u2208\u211d^d\n    (\u03b8,\u03b8')\u2208\u211d\n    (\u03b8')\u2208\u211d\n\nfor \u03b8, \u03b8' \u2208. A precise definition of \u03b8, \u03b8' and of the coupling procedure to identify the neuronal embedding with the infinite width limit of the network e:finitewidth is given in the next section. The mean-field representation is the  continuous version of the network introduced above, representing matrix multiplications as integral kernels and can be written as\n\n\nF\u0302(\ud835\udc31;W)  =    \nH_hy(\ud835\udc31) \n\nH_hy(\ud835\udc31)  =    \u222b(\u03b8) \u03c3_h(H_hh(\u03b8; \ud835\udc31,0)+H_xh(\u03b8; \ud835\udc31_0)) (\u03b8\u0323)\n\nH_hh(\u03b8; \ud835\udc31,k)  =    \u222b(\u03b8, \u03b8')\n\u03c3_h(H_hh(\u03b8';\ud835\udc31,k+1)+ H_xh(\u03b8';\ud835\udc31_-(k+1))) (\u03b8\u0323')\n\nH_hh(\u03b8;\ud835\udc31, L)  \u2261      0\n\nH_xh(\u03b8; \ud835\udc31_k)  =     (\u03b8) \u00b7\ud835\udc31_k\n\n\n\nAs the next example shows, any finite-width RNN F\u0302(;) can be embedded into the mean-field representation.\n(Finite-width RNN) For any choice of parameters = {, , } for a width-n network for n<\u221e and assuming = 1 we can set\n  := {1,2,\u2026, n} .\nThe measure  can be chosen as the uniform measure on {1,2,\u2026, n}. Then, it is readily seen that, setting (i,j) := ()_ij, (i) := ()_i and (j) := ()_j for i,j \u2208{1,\u2026, n} we have that e:mfrnn gives the same output as e:finitewidth.\n\n\n\n\n \u00a7.\u00a7 Initialization and Coupling procedure\n We now introduce the coupling procedure that connects the evolution of finite-width neural networks e:finitewidth to their mean-field representation e:mfrnn. This coupling procedure is performed at initialization, before training starts. We will respectively denote the  weights of the finite-width network and of the mean-field limit at initialization by \ud835\udc16^0 and W^0.\nInstrumental to introducing the coupling procedure between the finite-width and the infinite-width neural network is the notion of neuronal embedding. Given a family I of initialization laws indexed by the width n of the hidden layer,\n\n  I = {\u03c1_n\u00a0:\u00a0\u03c1_n  is the law of \ud835\udc16^0 for a network of width n}\n\nwe consider the parameters  \ud835\udc16^0\nof the width-n network as samples from the corresponding distribution \u03c1_n \u2208 I.\n\nWe call (, , W) a neuronal embedding for the neural network with initialization laws in I if for every \u03c1_n there exists a sampling rule P\u0305_n such that\n\n    \n    \n  * P\u0305_n is a distribution on ^n (not necessarily a product distribution) with marginals given by \n    \n  * The mean-field weights W = (, , ) are such that, if (\u03b8(j))_j \u223cP\u0305_n,  then for every n with i,j \u2208{1,\u2026,n}:\n    \n      Law((\u03b8(i)), (\u03b8(i), \u03b8(j)), (\u03b8(j)))=\u03c1_n.\n    \n\nThe above definition decomposes the concept of neural network weights to two parts: the first part is a deterministic function of possibly continuous arguments and the second part consists of a random map \u03b8 transforming the index i to a (random) argument of the weight function W. A finite-width network is then seen as a choice of the map \u03b8 and weight function W. The evolution of the weights is captured, for a choice of \u03b8, by the dependence of W in time (the time evolution will be detailed in the next section). Specifically, we couple \ud835\udc16^0 and W^0 as follows:\n\n    \n  * Given a family of initialization laws I, we choose (, , W^0) to be a neuronal embedding of I and initialize the dynamical quantities W^0(\u00b7).\n    \n  * Given n \u2208\u2115 and the sampling rule P\u0305_n, we sample (\u03b8(1),\u2026, \u03b8(n))\u223cP\u0305_n and set ^0(i,j) = ^0(\u03b8(i), \u03b8(j)), ^0(i) = ^0(\u03b8(i)) and ^0(j) = ^0(\u03b8(j)) for j\u2208{1,\u2026 , n}.\n\n\n  The key property of the neuronal embedding construction is the decomposition of the probability space generating an instance of the neural network into a product space over different layers. This decomposition captures the symmetry of the neural network's output under certain permutations of the indices of the neurons, thereby generalizing the representation as an empirical measure used in <cit.>. The following example helps clarify this analogy.\n\n\nIn the case of the finite-width network discussed in Example\u00a0<ref>, the sampling rule \u03b8(i) = i + \u03c9 with \u03c9\u2208 common to the whole layer and distributed uniformly on  satisfies the above conditions. We further notice that \u03b8(i) = \u03b9(i) for any (random) permutation \u03b9 of {1,\u2026, n} realizes the same neural network, a neural network with the same weights , up to permutation of the indices of its neurons.\n\nWhile the example above illustrates the connection between neuronal embeddings and finite-width neural networks by using finite probability spaces, the same connection can be established more abstractly in the case of iid initializations for arbitrary and infinite-width networks by means of the Kolmogorov extension theorem.\n\n\nIn the case of iid initialization, the neuronal embedding acquires a more explicit formulation. For a given probability space (\u039b, \ud835\udca2, P_0) we define p_xh(c), p_hh(c,c') and p_hy(c) which are respectively \u211d^-valued,  \u211d-valued and \u211d-valued random processes indexed by (c,c') \u2208 [0,1]\u00d7 [0,1]. For any n and any collection of indices {c^(i),(c')^(i)\u00a0:\u00a0i \u2208{1,\u2026,n}} let S be the set of indices and R be the set of pairs of indices, we let {p_xh(c)\u00a0:\u00a0c \u2208 S}, {p_hh(c,c')\u00a0:\u00a0(c,c')\u2208 R}, {p_hy(c)\u00a0:\u00a0c \u2208 S} be independent. Then we let\nLaw(p_xh(c)) = \u03c1_xh,  Law(p_hh(c,c')) = \u03c1_hh, Law(p_hy(c)) = \u03c1_hy\nfor all c \u2208 S, (c,c')\u2208 R. This space exists by Kolmogorov extension theorem. The desired neuronal embedding is obtained by  taking \u03a9_h = \u039b\u00d7 [0,1], equipped with the measure P_hh = P_0\u00d7Unif([0,1]) and we define the weight functions as\n\n  ((\u03bb_1, c))    = p_xh(c)(\u03bb_1)\n\n   ((\u03bb_1, c),(\u03bb_2,c'))    = p_hh(c,c')(\u03bb_1,\u03bb_2)\n\n      ((\u03bb_2, c))    =  p_hy(c)(\u03bb_2).\n\n\nIn order to state our results we assume that the dependence of \u03b8(i) and \u03b8(j) for i\u2260 j is sufficiently weak, as stated in a:indep below. While this condition is trivially satisfied by iid initialization introduced above, it makes our analysis applicable to more general \u2013 not fully necessarily iid \u2013 initialization procedures.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Training dynamics\n\nA popular algorithm to minimize the MSE  e:mse1 is given by gradient descent: starting from an initial condition\n(0), \nwe update the parameters  in the direction of steepest descent of the loss function:\n\n\ud835\udc16(j+1) := \ud835\udc16(j) - \u03b2D_\ud835\udc16 \u2112(\ud835\udc16) ,\n\nwhere D_ represents the Fr\u00e9chet derivative with respect to , j \u2208\u2115_0 indexes the timesteps of the algorithm and \u03b2 denotes the stepsize of the discrete-time update.\n\nIn this work, we consider the  regime of asymptotically small constant\nstep-sizes, \u03b2\u2192 0. In this continuum limit, the stochastic\ncomponent of the dynamics is averaged before the parameters of the\nmodel can change significantly.  This allows us to consider the parametric update as a deterministic dynamical system emerging from the\naveraging of the underlying stochastic algorithm corresponding to the limit of infinite sample sizes.\nThis is known as the ODE\nmethod <cit.> for analyzing stochastic approximations. We focus on the analysis\nof this deterministic system to highlight the core dynamical properties of our training algorithm.\n\nWe denote\n\n    \ud835\udc16(t) := {(t; \u00b7), (t; \u00b7,  \u00b7), (t; \u00b7)}\n\nas the continuous-time, averaged trajectory of the finite-width weights with initial conditions\n(0; \u00b7)=^0(\u00b7), (0; \u00b7,  \u00b7)=^0(\u00b7,\u00b7), (0; \u00b7)=^0(\u00b7). The gradient descent dynamics for these quantities can be written as the following\nodes\n\n    \u2202_t \ud835\udc16(t) = - D_ \u2112(\ud835\udc16(t)) .\n\nWhile the dynamics of both  and  will be described by the above equation, we truncate the evolution of  in an interval of width R>0 as follows:\n\n  \u2202_t \ud835\udc16_hh(t) =    - \u03c7_R((t)) \u2299D__hh \u2112(\ud835\udc16(t))\n\nwhere \u2299 denotes the Hadamard product and  \u03c7_R\u00a0:\u00a0\u211d\u2192\u211d is a smooth indicator function acting component-wise on its argument and such that \u03c7_R(w) = w if w\u2264 R/2 and \u03c7_R(w) \u2261 0 if w\u2265 R. \u00e5We comment on the reasons for this truncation in r:truncation.\n\nAnalogously, we denote\n\n    W(t) := {(t; \u00b7), (t; \u00b7,  \u00b7), (t; \u00b7)} ,\n\nas the continuous-time trajectory of the mean-field weights with initial condition (0; \u00b7)=^0(\u00b7), (0; \u00b7,  \u00b7)=^0  (\u00b7,\u00b7), (0; \u00b7)=^0(\u00b7), obeying the set of\nodes\n\n \u2202_t (t;\u03b8) =    - \u03b4/\u03b4 \u2112(W(t))\n\n  \u2202_t (t;\u03b8, \u03b8') =    - \u03c7_R((t;\u03b8, \u03b8')) \u03b4/\u03b4 \u2112(W(t))    \n\n    \u2202_t (t;\u03b8) =     - \u03b4/\u03b4 \u2112(W(t))\n\nwhere \u03b4/\u03b4 W denotes the variational derivative (Fr\u00e9chet derivative) with respect to W. While the explicit expressions for these dynamics are derived in Appendix\u00a0<ref>, we give here the update for the last layer of mean-field weights:\n\n\u2202_t  (t;\u03b8)  =    - \u222b(F\u0302(\ud835\udc31;W(t)) - F^*(\ud835\udc31))\n\u03c3_h(H_hh(\u03b8;\ud835\udc31,0) +  H_xh(\u03b8;\ud835\udc31_0)) \u03bd(x) .\n\nIn the next section we will leverage the fact that this quantity must be 0 at stationarity to establish the desired optimality result.\n\n\n\u00a7 OPTIMALITY RESULTS\n\n\nTo state the main results of this paper, \u00e5denoting by L_R^\u221e() whe set of functions on \u03a9_h that are essentially bounded by R>0, we formulate the following assumption:\n  Consider a neuronal embedding (,, W) and consider a mean-field limit associated with the neuronal ensemble (, ) with initialization W(0) = W^0. We assume that there exists K>R such that\n  \n    \n  a) Regularity of \u03c3:\n    \u03c3_h is bounded, differentiable, \u03c3_h(0) = 0, \u03c3_h'(0)\u2260 0 and D\u03c3_h is K-bounded and K-Lipschitz.\n    \n  b) Universal approximation: The span of {\u03c3_h(\u00b7_0) : \u2208\u211d^} is dense in L^2(\u03bd_0).\n\n    \n  c) Diversity at initialization: The support of the weight functions ^0, ^0 at initialization satisfies\n\n    (^0(\u03b8),^0(\u00b7,\u03b8),^0(\u03b8,\u00b7)) = \u00d7 L_R^\u221e() \u00d7 L_R^\u221e() .\n\nThroughout the paper we denote by (\u00b7,\u03b8) the random (in \u03b8) mapping \u03b8' \u21a6(\u03b8',\u03b8).\n\n  d) Regularity at initialization: \nThe weight functions ^0, ^0, ^0 at initialization satisfy \u00e5sup_\u03b8, \u03b8' |^0(\u03b8, \u03b8')| \u2264 R and given E_1(m) = \ud835\udd3c(|^0(\u03b8)^m|)^1/m and\nE_2(m) =\n\ud835\udd3c(|^0(\u03b8)^m|)^1/m then\n\n    sup_m \u22651 1/\u221a(m) E_1(m)\n    \u2228E_2(m)< K .\n\n  \n  \n\n Most of the assumptions made above are standard in the literature on mean-field limits of neural networks, and were first formulated in similar terms in <cit.> and <cit.>. a:1a) gives technical conditions on the regularity of the nonlinearities, ensuring that the training dynamics are well-behaved. \u00e5The condition on the nonvanishing derivative at the preimage of 0, which without loss of generality is assumed to be at 0 itself, is required to preserve expressivity of the network while allowing for uniform in time boundedness of the hidden weights. a:1b) demands sufficient expressivity of the activation function, required to approximate any function of a finite list of inputs {_-L,\u2026,\ud835\udc31_0}. This condition replaces the convexity assumption from <cit.>, and is satisfied by any nonlinearity for which the universal approximation theorem holds <cit.>, tanh. a:1c) guarantees that the initial condition is such that the expressivity from b) can actually be exploited. This property, which as we shall show is preserved by the network throughout training, ensures that the argument of the nonlinearity at each layer is sufficiently varied, and was first introduced in <cit.>. Combining this with a:1b) ensures, by induction, that there is no information bottleneck throughout the depth of the unrolled network and that the model is highly expressive throughout training. Finally, a:1d) is a technical assumption on the data and on the weights guaranteeing the well-posedness of the training dynamics.\n\n \n  We note that a:1c) is significantly stronger than the analogous \u201csufficient support\u201d assumption from <cit.>. In particular, this assumption is not satisfied if the weights of each layer are sampled iid from any initialization law \u03bc. As we comment in the proof of our results, relaxing this assumption to include iid initialization would significantly reduce the expressivity of the untrained infinite-width network with respect to predictors _k at timesteps k<0. More specifically, an iid initialization of the weights combined with the infinite width limit we are considering results in a highly degenerate hidden state of the network. Because of the intrinsic depth of RNN structures, this generates in turn a bottleneck effect preventing information from values of the predictors in the distant past to propagate through the network.\n \n\n  \u00e5The truncation of the dynamics of the hidden layer weights e:truncation1 e:mfpde was introduced in order to guarantee existence and uniqueness of the solution to both the finite-width and the mean-field equations. Indeed, in the absence of this cutoff, weight-sharing in this class of RNNs would result in a non-Lipschitz RHS for the dynamical equation e:gradientdescent, as shown explicitly in  Appendix\u00a0<ref>. Given this lack of regularity, existence of the solution cannot be guaranteed by standard analytical tools. However, in practice the weights are stored using a floating-point representation which is intrinsically bounded, and we argue that in this sense the truncation of their trajectories is a relatively natural assumption.\n\n\n\nWe now proceed to present the main results of the paper, which we divide into two parts:\n\n\n\n \u00a7.\u00a7 Convergence\n\n\nThe main result in this section is the convergence of the finite-width network trajectories to the mean-field limit, analogously to\nThm.\u00a018 in <cit.>. More specifically, for a given neuronal ensemble (\u03a9, ) and sample \ud835\udc16 from  we define the following distance or error metric\n\ud835\udc9f_\u03c4(W, \ud835\udc16) for any \u03c4 >0 as\n\n     \ud835\udc9f_\u03c4(W, \ud835\udc16):= sup_t \u2208(0,\u03c4)(1/n^2 (t;\u03b8(i), \u03b8(j)) - (t;i,j)_2 .\u22281/n (t;\u03b8(j))-(t; j)_2\n\n                          \u2228. 1/n (t;\u03b8(j))-(t; j)_2)\n\nwhere \u00b7 _2 denotes, depending on its argument, the Frobenius norm or the classical \u2113_2 norm. \n\n\nFor any R>0, let Assumptions\u00a0<ref>, <ref>, <ref> and <ref> hold. There exist constants c,c'>0 such that, for any \u03b4>0, any L \u2208\u2115 and \u03c4 >0, there exists n^*\u2208\u2115 such that for any n>n^* with probability at least 1-\u03b4-K\u0305 nexp(-K\u0305 n^c') we have\n\n  \ud835\udc9f_\u03c4(W, \ud835\udc16) \u2264K\u0305 n^-c \u221a(logn^2/\u03b4+e)\n\nwhere K\u0305 is a constant that depends on  L and R.\n\n\n\nThe proof of the above result mimics the one in <cit.> and is provided in the appendix for completeness. The main argument of the proof is similar to classical propagation of chaos results <cit.>. The first step of the argument establishes sufficient regularity of the gradient dynamics and guarantees existence and uniqueness of the solution to e:mfpde. Then, one bounds the difference in differential updates for the particle system and the mean-field dynamics as a function of the distance \ud835\udc9f_t(W, ).\nThe proof is concluded by an application of Gr\u00f6nwall's inequality.\n\n\n\n\n \u00a7.\u00a7 Optimality\n\n\nThe main optimality result is presented in the following theorem.\n\n  For any R>0 let Assumption\u00a0<ref>, <ref> and <ref> hold and assume that the trajectory W(t) solving (<ref>) converges to W\u0305 \u00e5in the following sense: for all i \u2208{1, \u2026, L} the following quantities vanish in the limit t \u2192\u221e,\n  \n        \u2219  ess-sup_\u03b8\u2208supp() |\u2202_t W_hy(t;\u03b8)|  \n\n       \u2219\u222b|W\u0305_hy(\u03b8) - W_hy(t;\u03b8)|^2 (d\u03b8) \n\n       \u2219\u222bW\u0305_hy(\u03b8^(0))^2 \u220f_j=1^i-1 W\u0305_hh(\u03b8^(j-1),\u03b8^(j))^2 W\u0305_hh(\u03b8^(i-1),\u03b8^(i))- W_hh(t;\u03b8^(i-1),\u03b8^(i))^2P_hh^\u2297i+1(\u03b8\u0323^(0),..., \u03b8\u0323^(i))   \n\n       \u2219\u222bW\u0305_hy(\u03b8^(0))^2 \u220f_j=1^i-1 W\u0305_hh(\u03b8^(j-1),\u03b8^(j))^2W\u0305_xh(\u03b8^(L-1))- W_xh(t;\u03b8^(L-1))^2^\u2297i+1(\u03b8\u0323^(0),..., \u03b8\u0323^(i))\n    \n    \n    Then lim_t \u2192\u221e\u2112(W(t)) = 0 .\n\n\nThis result asserts that if the gradient descent dynamics e:mfpde converges to a stationary point W\u0305, \nthat point must be a global minimizer, it must approximate the underlying function to arbitrary accuracy.\nWe prove the above result in three steps. First,\nwe show in p:spanning that if the weights at initialization are sufficiently varied (a:1c)) then the network enjoys a high level of expressivity, inherited from the properties of \u03c3_h a:1a) and b). Such expressivity in turn implies that the mean-field vector fields evaluated at a suboptimal fixed point of the dynamics e:mfpde cannot vanish everywhere\nin neuronal embedding space.\n In other words, a network whose weights have sufficient support cannot correspond to a suboptimal stationary point of the gradient dynamics.\n\nWe then show in l:support that such sufficient notion of support (a:1c)) is preserved by the gradient descent dynamics e:mfpde throughout training.\nFor any finite time, this is true by topological arguments: the full support property cannot be altered by a continuous vector field such as e:mfpde.\n\nFinally, we show that the gradient descent dynamics cannot converge to a spurious fixed point by combining the two partial results above.\nIn particular, we show that by the preserved expressivity of the network throughout the dynamics proven above, the fact that the time derivative of W_hy(t) e:deltaw must vanish almost-everywhere as t \u2192\u221e implies that the difference between the approximator and the target function F^* must also vanish almost everywhere in the limit. In other words, combining the assumption on convergence of W_hy(t) with the nondegeneracy of the W_hy-Jacobian of the network (following from expressivity) imples that the limiting point must be optimal.\n\nThere are multiple technical challenges that need to be addressed in this proof with respect to the proof techniques used in previous results. The most important one stems from the fact that the input structure of the (unrolled) network is different from a standard feedforward network or ResNet. The additive combination of the input with the hidden state of the previous \u201clayer\u201d, together with weight sharing, results in possible degeneracies of the dynamics that need to be taken into account in the proof.\nBy studying the risk minimization problem in equation e:mse1 and considering exclusively the dynamics of  e:deltaw, we bypass the problem of weight sharing in the unrolled network by leveraging the expressivity a:1c).\n\n\u00e5Finally, we note that the boundedness of  prevents us from using any of the classical expressivity results leveraging the vector space structure of the space of admissible weights. For this reason, we adapt our proof to bypass the boundedness of the hidden weights resulting from the truncation in e:mfpde. To do so, we leverage the fact that, by a:1a), the image under \u03c3_h of a function whose supremum is close to 0 is close to the identity. Combining this with the possibility of choosing arbitrarily small hidden weights results in the network being able to propagate information throughout its layers. Finally, the unboundedness of  allows to recover this information and therefore to realize the expressive potential of the network.\n\n\n\n\n\u00a7 NUMERICAL EXPERIMENTS\n\n\nIn this section we numerically validate the\ntheoretical\noptimality and convergence results in our paper with some simulations.\n\n\n\n\n\n\n\n\n\n  \nNetwork architecture  and model\n\nThe network model we consider is a wide a RNN in the mean-field regime in a teacher-student scenario. For the optimality results we set the width to be\nn_s = 1000 and for the convergence results we  train the student RNN with increasing size (n_s \u2208{20,60,100,140,\u2026, 300}).\n\nWe train a so-called student many-to-one RNN F\u0302 with d = 1 and hidden layer width n_s to learn the output of a teacher many-to-one RNN F^*, with the same input and output size and hidden width n_t = 15. Both neural networks have hidden activation \u03c3_h(\u00b7) = tanh(\u00b7)\nand their weights are initialized iid as follows:\n\n  teacher:                    student:\n\ud835\udc16_xh \u223c\ud835\udca9(1,1)\n \ud835\udc16_hh \u223c\ud835\udca9(0,n_t^-2)\n \ud835\udc16_hy \u223c\ud835\udca9(0,n_t^-2)     \n   \ud835\udc16_xh \u223c\ud835\udca9(0,5)\n \ud835\udc16_hh \u223c\ud835\udca9(0,10 \u00b7n_s^-2)\n \ud835\udc16_hy \u223c\ud835\udca9(0,10 \u00b7n_s^-2).\n\n\n\n\n  \nSimulated data\n\n        The predictors for our \n        simulation are generated as samples of length L = 10 from the stationary trajectories of the shift map T(x) = x+1 acting on the sphere \ud835\udd4f = S^1 = [0, 2\u03c0). To do so, we sample the initial point \ud835\udc31_-L^(j) iid from the invariant measure \u03bd_0(x\u0323) = 1/2\u03c0x\u0323 of T supported on \ud835\udd4f and generate the corresponding input sequence as\n        \ud835\udc31_-k+1^(j) := T(\ud835\udc31_-k^(j))\n        for k \u2208 (1,,\u2026, L).\n\n \n\n  \nTraining specifications The training of the student RNN is performed using the  package in pytorch <cit.>. We train the parameters \ud835\udc16\u0302 to minimize the empirical  Mean Squared Error \u2112\u0303(\ud835\udc16\u0302):=1/m\u2211_j = 1^m (F^*(\ud835\udc31^(j)) - F\u0302(\ud835\udc31^(j),\ud835\udc16 ))^2 where m = 2^13\u2248 10^4\n denotes the size of the database. Combining this with our sampling of \ud835\udc31^(j) results in iid samples from the invariant measure \u03bd(\ud835\udc31) of T, and therefore in the finite-sample equivalent of the population risk e:mse1. The optimization is performed using stochastic gradient descent (), which is called with a stepsize \u03b3 = 3 10^-3 and batch size of m, the full database size, thereby resulting in full-fledged gradient descent. The results of the simulation are shown in f:numerics. Code is available at <cit.>.\n\n\n\n\n\n   \n\n  \nInitialization for the convergence results\n\n    For the convergence results we need to\n    initialize the family of student networks in a consistent way. Our procedure for enforcing consistency draws the weights without replacement from a reference student network of width 300.\n\n\n\n\n\n\u00a7 CONCLUSIONS\n\n\nThis work shows that, despite the increased complexity,  RNNs  share common optimality properties with simpler single-layer neural networks <cit.>. Specifically we show that, under some conditions on the expressivity of the network at initialization, the fixed points of wide Elman-type RNNs' with gradient descent training dynamics in the mean-field regime are globally optimal, that the neural network will perfectly learn the given function of the dynamical system's trajectories. In this sense, while extending previous results on the optimality properties of shallow and deep neural networks to novel architectures, this work contributes to the understanding of deep learning applied to dynamical systems data. The proof is carried out by unrolling the RNN structure and showing that the fixed points of the training dynamics, which preserve a certain notion of support in parameter space, can only be globally optimal.\n\nPossible future developments include relaxing the assumption about the support of the weights at initialization a:1c) to a condition that is simpler to realize in practice. Drawing an analogy with autoregressive processes, a promising insight towards solving this problem consists in injecting, at each iteration of the RNN, new directions in the function space spanned by the model by means of, the network biases, so as to reduce the hidden layer's null space. Another possible avenue of future research consists of relaxing the adiabaticity assumption, considering the stochastic approximation problem resulting from the finite number of samples and the finite gradient stepsize. We note that, because of this assumption, our analysis is immune to the exploding gradients problem <cit.>. To prevent this problem to affect a finite timestep analysis, another important extension of the present work is to establish similar results for different RNN architectures, such as the LSTM, which given its extensive use in practice is of great interest.\n\nFrom the theoretical standpoint, the most important\nopen question concerns establishing quantitative convergence of mean-field dynamics of neural networks: even in the single-layer, supervised setting, despite recent results in specific settings <cit.>, these guarantees still elude the community's research efforts.\n\n\n\n  \u00a7.\u00a7.\u00a7 Acknowledgments.\n\n We thank the anonymous referee for pointing out a gap in our previous proof of t:eandu. All authors acknowledge partial support of the TRIPODS NSF grant CCF-1934964. AA acknlwledges partial support of the University of Pisa, through project PRA 2022_85. JL acknowledges the partial support of the NSF grant DMS-2012286. SM acknowledges partial support of HFSP RGP005, NSF\nDMS 17-13012, NSF BCS 1552848, NSF DBI 1661386, NSF IIS 15-46331, NSF DMS 16-\n13261, as well as high-performance computing partially supported by grant 2016-IDG-1013 from the North Carolina Biotechnology Center. AA and SM thank Katerina Papagiannouli and Andrea Aveni for insightful discussions and acknowledge the hospitality of the Max Planck Institute for Mathematics in the Sciences and of the ScaDS institute of the University of Leipzig and Technical University of Dresden during the final part of this project.\n\nplain\n\n\n\n\n\n\n\nequationsection\n\n\n\n\n\u00a7 COMPUTATION OF MEAN-FIELD ODES\n\n\nIn this section we explicitly compute the RHS of the mean-field ODEs e:mfpde. Recall the definitions of the population risk \u2112(W) from e:mse1 and of the mean-field approximator\n\n\nF\u0302(\ud835\udc31;W)     = \nH_hy(\ud835\udc31) \n\nH_hy(\ud835\udc31)     = \u222b(\u03b8) \u03c3_h(H_hh(\u03b8; \ud835\udc31,0) + H_xh(\u03b8; \ud835\udc31_0)) (\u03b8\u0323)\n\nH_hh(\u03b8; \ud835\udc31,k)     = \u222b(\u03b8, \u03b8') \u03c3_h(H_hh(\u03b8';\ud835\udc31,k+1) + H_xh(\u03b8';\ud835\udc31_-(k+1))) (\u03b8\u0323')\n\nH_xh(\u03b8; \ud835\udc31_-k)     = (\u03b8) \u00b7\ud835\udc31_-k\n\n\nFurther, for notational convenience, we define throughout the argument of the nonlinearity as\n\n  (\u03b8; , k) := H_hh(\u03b8;\ud835\udc31,k) + H_xh(\u03b8;\ud835\udc31_-k)\n\nand, when necessary, we will slightly abuse notation and  explicitly write the set of weights generating the hidden state  in its argument as [W](\u03b8; , k).\nFurthermore, we define\n\n  \u0394F(W, ) := F\u0302(\ud835\udc31;W(t)) - F^*(\ud835\udc31)\n\nso that we can write\n\n\u03b4/  \u03b4\u2112[W](\u03b8)      = \u222b\u0394F(W, )  \n\u03c3_h(  [W](\u03b8; , 0)) \u03bd(x)\n\n\nWe proceed to compute the derivative wrt  :\n\n\u03b4/  \u03b4\u2112[W](\u03b8)     = \u222b\u0394F(W, )  \u03b4/\u03b4 \u222b(\u03b8') \u03c3_h(H_hh(\u03b8';\ud835\udc31,0) + H_xh(\u03b8';\ud835\udc31_0))(\u03b8\u0323')(\u03b8)  \u03bd(x)\n\n   = \u222b\u0394F(W, )\n \u222b(\u03b8') \u039e_0(\u03b8;\u03b8', ) (\u03b8\u0323') \u03bd()\n\nwhere, denoting here and throughout by \u03b4(\u03b8) the Dirac delta distribution,  we define recursively\n\n  \u039e_i[W](\u03b8;\u03b8', )   := \u03b4/\u03b4\u03c3_h(H_hh(\u03b8';\ud835\udc31,i) + H_xh(\u03b8';\ud835\udc31_-i))(\u03b8)\n\n     = \u03c3_h'(  (\u03b8'; , i))\u03b4/\u03b4H_hh(\u03b8';\ud835\udc31,i)(\u03b8) + _i\u03b4(\u03b8'-\u03b8)\n\n     = \u03c3_h'(  (\u03b8'; , i))\u222b(\u03b8', \u03b8\u201d)\u039e_i-1(\u03b8; \u03b8\u201d, ) (\u03b8\u0323\u201d) + _i \u03b4(\u03b8'-\u03b8)\n\nand throughout we slightly abuse notation by suppressing the dependency of \u039e_i on W when clear from the context.\nTherefore,  we obtain\n\n\u03b4/  \u03b4\u2112[W](\u03b8)\n   = \u222b\u0394F(W, )\n (\n \u2211_i=0^L\n\u0393_i(W, \u03b8, )_-i)\u03bd(x)\n\nwhere for i \u2208{0,1,\u2026, L} we define\n\n  \u0393_i(W, \u03b8,)    = \u222b(\u03b8_0) \u03c3_h'((\u03b8_0; , 0))\u222b(\u03b8_0,\u03b8_1)\u03c3_h'((\u03b8_1; , 1))\n\n             \u2026\u222b(\u03b8_i,\u03b8)\u03c3_h'((\u03b8; , i)) ^\u2297i+1(\u03b8_0,\u2026, \u03b8_i)\n\nAnalogously, we proceed to compute the derivative wrt :\n\n\u03b4/  \u03b4\u2112[W](\u03b8, \u03b8')     = \u222b\u0394F(W, )  \u03b4/\u03b4 \u222b(\u03b8_0) \u03c3_h([W](\u03b8_0;\ud835\udc31,0) )(\u03b8\u0323_0)(\u03b8,\u03b8')  \u03bd(x)\n\n   = \u222b\u0394F(W, )\n \u222b(\u03b8_0) \u039e_0'[W](\u03b8, \u03b8';\u03b8_0, ) (\u03b8\u0323_0) \u03bd()\n\nwhere we define recursively\n\n      \u039e_i'[W](\u03b8, \u03b8';\u03b8_i , )   := \u03b4/\u03b4\u03c3_h(H_hh[W](\u03b8_i ;\ud835\udc31,i) + H_xh[W](\u03b8_i;\ud835\udc31_-i))(\u03b8, \u03b8')\n\n     = \u03c3_h'([W](\u03b8_i; , i))\u03b4/\u03b4H_hh[W](\u03b8_i;\ud835\udc31,i)(\u03b8, \u03b8') \n\n     = \u03c3_h'([W](\u03b8_i; , i))(.\u222b(\u03b8_i, \u03b8_i+1)\u039e_i+1'[W](\u03b8, \u03b8'; \u03b8_i+1, ) (\u03b8\u0323_i+1). \n\n                     . . + \u222b\u03c3_h([W](\u03b8_i+1; , i+1)) \u03b4(\u03b8_i-\u03b8)\u03b4(\u03b8_i+1-\u03b8') (\u03b8\u0323_i+1))\n\nand throughout we slightly abuse notation by suppressing the dependency of \u039e_i' on W when clear from the context.\nTherefore, we obtain\n\n\u03b4/  \u03b4\u2112[W](\u03b8,\u03b8')\n= \u222b\u0394F(W, )\n (\u2211_i=0^L\u0393_i(W, \u03b8, )\u03c3_h((\u03b8'; , i+1)))\u03bd(x)\n\nfor \u0393_i defined in e:gammai.\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXISTENCE AND UNIQUENESS OF SOLUTIONS TO ODES\n\n\nWe now proceed to sketch the proof of existence and uniqueness of the solutions to the mean-field ODEs, stated below. To this aim, fixing throughout a value of the cutoff R>0 for e:mfpde, we define the sub-Gaussian norm\n\n  W_hh  _\u03c8,t    := \u221a(50) sup_m \u22651 1/\u221a(m)\u222bsup_s<t |W_hh(s,\u03b8,\n  \u03b8')|^mP_hh^\u22972(d\u03b8, d\u03b8')^1/m\n\n  W_hy  _\u03c8,t    := \u221a(50) sup_m \u22651 1/\u221a(m) \u222bsup_s<t |W_hy(s,\u03b8)|^m (d\u03b8)^1/m\n\n  W_xh  _\u03c8,t    := \u221a(50) sup_m \u22651 1/\u221a(m) \u222bsup_s<t |W_xh(s,\u03b8)|^m (d\u03b8)^1/m\n\ninducing the norm on the weights W\n\n  W _\u03c8,t := maxW_hh_\u03c8,t, W_xh_\u03c8,t, W_hy  _\u03c8,t .\n\nFrom these definitions we have that W_hh_\u03c8,t\u2265W_hh_t, W_hy_\u03c8,t\u2265W_hy_t,  W_xh_\u03c8,t\u2265W_xh_t\nwhere\n\n   _t    =\u222bsup_s\u2264t |(s,\u03b8,\u03b8')|^50^\u22972(\u03b8\u0323, \u03b8\u0323')^1/50\n\n  _t    = \u222bsup_s<t |(s,\u03b8)|^50(\u03b8\u0323)^1/50\n\n  _t    = \u222bsup_s<t |(s,\u03b8)|^50(\u03b8\u0323)^1/50\n\n so that\n \n   W  _\u03c8,t \u2265W_t\n \nfor\n\nW_t := _t \u2228_t\u2228_t .\n\nNote that by a:1 we have _t \u2264  R < K uniformly in t\u2265 0.\n\nFurthermore, for a pair of mean-field weights W, W', we define the analogous norm on differences (note the different exponent):\n\nW- W'_t := -'_t \u2228- '_t\u2228- '_t\n\nfor\n\n - '_t    :=\u222bsup_s\u2264t |(s,\u03b8,\u03b8') - '(s,\u03b8,\u03b8')|^2^\n \u22972\n (\u03b8\u0323, \u03b8\u0323')^1/2\n\n- '_t    := \u222bsup_s<t |(s,\u03b8)- '(s,\u03b8)|^2(\u03b8\u0323)^1/2 \n\n- '_t    := \u222bsup_s<t |(s,\u03b8)-'(s,\u03b8)|^2(\u03b8\u0323)^1/2\n\n\nThroughout this section we fix an initialization W^0 for the mean-field weights of the network.\n\n  Assume that the initialization of the MF ODEs satisfies W^0  _\u03c8,0 < K. Then under a:1 there exists a unique solution to the MF ODEs e:mfpde.\n  \n  Analogously to <cit.>, the proof pivots on the use of Picard's iteration. In order to apply this strategy, we define the trajectory of the weights where the RHS of the MF ODEs  is obtained by \u201cplugging in\u201d the evolution of the weights at the previous iteration with initial condition W(0):\n  \n    F_xh[W'](t,\u03b8)     := W_xh(0,\u03b8) - \u222b_0^t \u222b_X  \u0394F(W'(s), )\n     \u222b_\u03a9_h '(\u03b8') \u039e_0[W'(s)](\u03b8;\u03b8', ) (\u03b8\u0323') \u03bd() ds\n\n    F_hh[W'](t,\u03b8, \u03b8')    :=  W_hh(0,\u03b8, \u03b8') - \u222b_0^t \u222b_X  \u0394F(W'(s), )\n     \u222b_\u03a9_h '(\u03b8_0) \u039e_0'[W'(s)](\u03b8, \u03b8';\u03b8_0, ) (\u03b8\u0323_0) \u03bd() ds\n\n    F_hy[W'](t,\u03b8)    := W_hy(0,\u03b8') - \u222b_0^t \u222b_X  \u0394F(W'(s), ) \u03c3_h(  (\u03b8; , 0)) \u03bd(x) ds\n  We now present a preparatory lemma, estimating the growth of\n  \n    F[W']-F[W\u201d]_t := F_xh[W']-F_xh[W\u201d]_t \u2228F_hh[W']-F_hh[W\u201d]_t \u2228F_hy[W']-F_hy[W\u201d]_t\n  \n  in terms of W'-W\u201d_t in order to prove contraction of the map F. This result holds provided that the growth of the weight trajectories W', W\u201d is bounded in an appropriate sense.\n  To state these necessary growth bounds, we introduce the key functional\n  \n     K_0(t) :=  K^2L+5 (1+t^2)(1+ W^0 _\u03c8,0)\n     \n   \n   that depends on a large constant K>0 to be chosen later.\n   For any T>0, we also define the maximal operator\n   \n     max_T(W) := sup_s\u2264T |W_hh(s; \u03b8, \u03b8')|\u2228| W_hy(s;\u03b8)|\u2228| W_hx(s;\u03b8)|\n   \n\n   Let a:1 hold and W^0_0. \u03c8<\u221e. For any T>0 and any B>0, consider two collections of mean-field parameters W' = {W'(t)}_t \u2264 T, W\u201d = {W\u201d(t)}_t \u2264 T,\n     assume that W'_T \u2228 W\u201d_T < K_0(T) and\n    \n      \u2119(max_T(W') > K_0(T)B )\u2228\u2119(max_T(W\u201d) > K_0(T)B )\u22642Le^1-K_1B^2\n      \n      for a choice of K, K_1>0. Then we have\n    \n      F[W']-F[W\u201d]_t \u2264k_1 (1+B) \u222b_0^t W'-W\u201d_s ds + k_2 e^-k_3B^2\n    \n    where k_1 = (K K_0(T))^3L+3, k_2 = T \u221a(L) (K K_0(T))^3L+3, k_3 = K_1/2.\n  \n  Based on the definition of K_0(t) from e:K0T we define the spaces \ud835\udcb2_T, \ud835\udcb2_T^0 as the set of mean-field weight trajectories W' satisfying that there exists K>0 such that, respectively,\n  \n    W'_T \u2264K_0(T) \n    and\n    \n       W'(0) = W^0,        W'_T,\u03c8 \u2264K_0(T) ,      \u2119(max_T(W') > K_0(T)B )\u22642Le^1-K_1B^2  \u2200B > 0\n  \n  so that \ud835\udcb2_T^0 \u2286\ud835\udcb2_T by e:supboundnorm.\n\n  .\n\n\n\n\nFix an arbitrary finite time T>0. By the fact that F is an endomorphism in \ud835\udcb2_T^0 (Lemma\u00a08 in <cit.>), we can apply l:9 (for every B with K, K_1 fixed) and iterating the above estimate to obtain\n\n  F^(m)[W'] -    F^(m)[W\u201d]_T  \u2264k_1 (1+B) \u222b_0^T F^(m-1)[W']-F^(m-1)[W\u201d]_t_2 dt_2 + k_2 e^-k_3B^2\n\n     \u2264k_1^2 (1+B)^2 \u222b_0^T \u222b_0^t_2 F^(m-2)[W']-F^(m-2)[W\u201d]_t_3 dt_3 dt_2 \n\n             + k_2\u2211_\u2113=1^2(T k_1k_2 (1+B))^\u2113-1/\u2113! e^-k_3B^2\n\n     \u2026\n\n     \u2264k_1^m (1+B)^m \u222b_0^T \u222b_0^t_2\u2026\u222b_0^t_m W'-W\u201d_t_m+1 dt_m+1\u2026dt_2 \n\n             + k_2\u2211_\u2113=1^m(T k_1k_2 (1+B))^\u2113-1/\u2113! e^-k_3B^2\n\n     \u2264k_1^m (1+B)^m T^m 1/m!   W'-W\u201d_T  + k_2e^(T k_1k_2 (1+B))-k_3B^2\n\n\nSetting B = \u221a(m) and choosing W\u201d = F[W'], from the above estimate we obtain\n\n  \u2211_m=1^\u221eF^(m+1)[W'] - F^(m)[W']_T =  \u2211_m=1^\u221eF^(m)[W\u201d] - F^(m)[W']_T < \u221e\nshowing that F^(m)[W'] is a Cauchy sequence and hence converges (the proof of completeness of the spaces \ud835\udcb2_T, \ud835\udcb2_T^0 is similar to <cit.> and is omitted). The uniqueness of the limit point is obtained by contradiction: Assume that W', W\u201d with W'- W\u201d_T > 0 are fixed points of F. Then, again choosing B = \u221a(m), for every m>0 we have\n\n  W' - W\u201d_T    = F^(m)[W'] - F^(m)[W\u201d]_T \n\n      \u2264k_1^m (1+\u221a(m))^m T^m 1/m!   W'-W\u201d_T  + k_2e^(T k_1k_2 (1+\u221a(m)))-k_3m\n\nwhich vanishes as m \u2192\u221e contradicting the assumption. Since the above argument goes through for every T> 0 we have existence and uniqueness for every T>0.\n\n\n\nWe now introduce some more compact notation for the time differential of the mean-field weight trajectories:\n\n   \u0394^xh(, \u03b8, W'(s))     :=  \u0394F(W'(s), )\n   \u222b'(\u03b8') \u039e_0[W'(s)](\u03b8;\u03b8', ) (\u03b8\u0323') \n\n  \u0394^hh(, \u03b8, \u03b8', W'(s))    :=   \u0394F(W'(s), )\n   \u222b'(\u03b8_0) \u039e_0'[W'(s)](\u03b8, \u03b8';\u03b8_0, ) (\u03b8\u0323_0) \n\n       = \u0394F(W, )\n    (\u2211_i=0^L\u0393_i(W,\u03b8, )\u03c3_h((\u03b8'; , i+1)))\n\n  \u0394^hy(, \u03b8,  W'(s))    :=  \u0394F(W'(s), ) \u03c3_h(  (\u03b8; , 0))\n\nand defining\n\n  \u0394_i^H(, \u03b8,  W) := \u0394F(W, )\u0393_i(W,\u03b8, )\n\nwe can write\n\n  \u0394^hh(, \u03b8, \u03b8', W'(s)) = \u2211_i=0^L \u0394_i^H(, \u03b8,  W'(s))\u03c3_h((\u03b8'; , i+1))\n\n\nWe proceed to establish the necessary a-priori growth and Lipschitz estimates to obtain the above result, defining throughout \ud835\udd3c_X[\u00b7] := \u222b_X \u00b7\u03bd(d ).\n\n\n  Under a:1, given an initialization W(0), a solution W to the MF ODEs e:mfpde must satisfy that for any t>0\n  \n    W_t \u2228max_i \u222b_\u03a9_h sup_s \u2264t  \ud835\udd3c_X[|\u0394_i^H(, \u03b8; W(s))|]^50^1/50 \u2264K^2L+5(1+t^2)(1+W_0)\n  \n  for a constant K > 0 large enough.\n  Similarly for W _\u03c8,t,\n  there exists\n  K>0 large enough such that W _\u03c8,t < K_0(t) for all t >0. Furthermore, for any B>0\n  \n    \u2119(max_t(W)\u2265K_0(t) B) \u22642 L e^1-K_1B^2\n  \n  for a universal constant K_1.\n\n\n\n\n\n  Consider two collections of mean-field parameters W', W\u201d\u2208\ud835\udcb2_T. Under a:1 for any t<T and any 1\u2264 k \u2264 L we have\n  \n    \u222bsup_s\u2264t H_\u03c3[W'(s)](\u03b8;, k ) - H_\u03c3[W\u201d(s)](\u03b8; , k)^1/2    \u2264K^2L  W'-W\u201d_t\n\n    sup_s\u2264t \ud835\udd3c_X[ |F\u0302(; W'(s)) - F\u0302(; W\u201d(s))|]    \u2264K^2L K_0(T) W'-W\u201d_t\n  \n\n\n\n  For a given B>0 consider two collections of mean-field parameters W', W\u201d\u2208\ud835\udcb2_T such that\n  \n    \u2119(max_T(W')> K_0(T)B)\u2264e^1-K_1B^2,\n \u2119(max_T(W\u201d)> K_0(T)B)\u2264e^1-K_1B^2\n  \n  Then under a:1, for any t \u2208 [0,T] the following holds:\n\n  \u222bsup_s\u2264t \u0394^hh(, \u03b8, \u03b8', W'(s))- \u0394^hh(, \u03b8, \u03b8', W\u201d(s)) P_hh^\u22972(d\u03b8, d\u03b8')^1/2 \u2264D(t, W', W\u201d)\n\nwhere\n\n  D(t, W', W\u201d) := (K K_0(t))^3L+3 (1+B) W'-W\u201d_t + \u221a(L) e^- K_1 B^2/2\n\n\n The proof of this lemma is performed as in Lemma 9 in <cit.>\n  combining l:10 and l:11, corresponding respectively to Lemma 10 in <cit.> and\n  Lemma 11 in <cit.>.\n\n\n\n\n  The proof of this lemma is analogous to the one of Lemma 6 in <cit.>.\n  In the following we highlight the main differences with Lemma 6 in <cit.>, to which we refer the reader for the details of the proof. We define\n  \n    W_hh _m,t := \u221a(50/m) \u222bsup_s\u2264t |W_hh(s,\u03b8,\u03b8')|^m ^\u22972(d\u03b8, d\u03b8') ^1/m\n  \n  and analogously for  and .\n\nStarting at the output layer, we have by Cauchy-Schwarz inequality and the fact that the mean-field ODE dynamics decrease the population risk that\n\n  sup_s\u2264t \ud835\udd3c_X[\u0394F(W(s), )^2] \u2264\ud835\udd3c_X[\u0394F(W(0), )^2] = \u221a([W(0)])< K\n\nfor a K>0 large enough.\nConsequently, we can bound the RHS of the equation for \u2202_t using Cauchy-Schwarz and the boundedness of \u03c3_h<K as\n\n|\u2202_t | = |\u222b\u0394F(W, ) \u03c3_h(  (\u03b8; , 0)) \u03bd(x)| \u2264K^2\n\nso that\n\n  W_hy _m,t \u2264W_hy _m,0 + K^2 t\n\nas desired.\n\nThe boundedness result for  trivially holds by the truncation introduced by \u03c7_R upon choosing K > R as in a:1, so that W_hh(\u00b7, \u00b7)_\u221e\u2264 R < K uniformly in t.\n\nFinally, for W_xh we have again by Cauchy-Schwarz\n\n  _m,t     \u2264_m,0 + \u221a(50/m)\u222b_\u03a9_htsup_s\u2264t |\u222b\u0394F(W, )\n   (\u2211_i=0^L\u0393_i(W, \u03b8, )_-i)\u03bd(x)|^m (\u03b8\u0323) ^1/m\n\n      \u2264_m,0 + \u221a(50/m)\u221a([W(0)]) \u2211_i=0^L \u222b|_-i|^2\u03bd(x)^1/2\u222b_\u03a9_ht sup_s\u2264t sup_|\u0393_i(W, \u03b8, )|^m (\u03b8\u0323) ^1/m \n\n      \u2264_m,0 + \u221a([W(0)])L  _0_\u03bd K^2L  t W_hy _m,t  \n\n      \u2264_m,0 + L  K^2L+2  t W_hy  _m,t\n\n      \u2264_m,0 + L  K^2L+2   t(W_hy _m,0 + K^2 t)\n\nwhere in the second upper bound we have used that |\u0393_i(W, \u03b8, )| \u2264 K^2L uniformly in i \u2208{1, \u2026, L},  and \u03b8\u2208\u0398 by boundedness of  and \u03c3_h, \u03c3_h' from a:1. From this follows that\n\n  W_xh _m,t \u2264(1+ W _m,0) K^2L+5 (1+t^2) .\n\nThe probability bound follows directly from the fact that max_T (W) is K_0(t) sub-Gaussian by the bounds established above.\n\n\n This proof is analogous to the one of Lemma 10 in <cit.>.\n  Recalling the definition of H_\u03c3  from e:hh\n  \n    (\u03b8; , k) := H_hh(\u03b8;\ud835\udc31,k) + H_xh(\u03b8;\ud835\udc31_-k)\n  \n   and slightly abusing that notation by H_\u03c3[W] (and similarly for ,) to highlight the set of weights with respect to which the hidden state is computed, we define\n  \nD_k^H(t):= \u222b_\u03a9_h sup_s\u2264t  \ud835\udd3c_X|H_\u03c3[W'(s)](\u03b8;, k) - H_\u03c3[W\u201d(s)](\u03b8;, k) |^2(d\u03b8)^1/2\n    \n    where we recall that \ud835\udd3c_X[\u00b7] = \u222b_X \u00b7\u03bd(d ).\n    Proceeding to bound the above for decreasing values of k we have\n    \n      D_L^H(t)     =  \u222b_\u03a9_h sup_s\u2264t   \ud835\udd3c_X|H_xh[W'](\u03b8;\ud835\udc31_-L) -H_xh[W\u201d](\u03b8;\ud835\udc31_-L) |^2(d\u03b8)^1/2\n\n          = \u222b_\u03a9_h sup_s\u2264t   \ud835\udd3c_X|'(\u03b8;s)\ud835\udc31_-L -\u201d(\u03b8;s)\ud835\udc31_-L |^2(d\u03b8)^1/2\n\n          \u2264\ud835\udd3c_X[|_-L|] \u222b_\u03a9_h sup_s\u2264t |'(\u03b8;s) -\u201d(\u03b8;s)|^2 (d\u03b8)^1/2 \n\n          \u2264K d_t(W', W\u201d)\n    \nwhere we define\n\n  d_t(W', W\u201d)    := max{\u222b_\u03a9_h^2 sup_s\u2264t |'(t;\u03b8, \u03b8')- \u201d(t;\u03b8, \u03b8')|^2 ^\u22972(d\u03b8, d\u03b8')^1/2, .\n\n                  \u222b_\u03a9_h sup_s\u2264t |'(t;\u03b8)- \u201d(t;\u03b8)|^2 (d\u03b8)^1/2,\n\n                  . \u222b_\u03a9_h sup_s\u2264t |'(t;\u03b8)- \u201d(t;\u03b8)|^2 (d\u03b8)^1/2}\n\nFor i < L we have, by triangle inequality and the Lipschitz and boundedness properties on \u03c3_h from a:1\n\nD_i^H(t)      =(\u222b_\u03a9_h sup_s\u2264t   \ud835\udd3c_X[|([W'](\u03b8;\ud835\udc31_-i) -[W\u201d](\u03b8;\ud835\udc31_-i)) ..\n\n                . . + ([W'](\u03b8;,i)-[W\u201d](\u03b8;,i))|^2](d\u03b8))^1/2\n\n      \u2264\u222b_\u03a9_h sup_s\u2264t   \ud835\udd3c_X|'(\u03b8;s)\ud835\udc31_-L -\u201d(\u03b8;s)\ud835\udc31_-L |^2(d\u03b8)^1/2\n\n                  + \u222b_\u03a9_h sup_s\u2264t   \ud835\udd3c_X|[W'(s)](\u03b8;,i)-[W\u201d(s)](\u03b8;,i))|^2(d\u03b8)^1/2\n\n      \u2264K d_t(W', W\u201d) + K^2 D_i+1^H(t) + K d_t(W', W\u201d) \n\n      \u2264K^2 (d_t(W', W\u201d)+ D_i+1^H(t)) \n\nThis implies that max_i \u2208{0, \u2026, L}D_i^H(t) \u2264 K^2L d_t(W', W\u201d), proving the first claim.\n\nThe second claim follows from a similar bound:\n\nsup_s\u2264t \ud835\udd3c_X F\u0302(; W'(s)) - F\u0302(; W\u201d(s))    \u2264K \u222b_\u03a9_hsup_s\u2264t |'(\u03b8;s)-\u201d(\u03b8;s) |^2 (d\u03b8)^1/2 + K '_t D_0(t)\n\n    \u2264K d_t(W', W\u201d) + K K_0(t) D_0(t)\n\nyielding the desired estimate.\n\n\n Again by similarity with the original reference we simply sketch this proof highlighting the differences with the present framework.\n\nD\u0303\nWe start the proof establishing the a priori bound\n\n  \u222b_\u03a9_hsup_s\u2264t\ud835\udd3c_X[\u0394_i^H[W'(t)](\u03b8;)]^50 (\u03b8\u0323)^1/50 \u2264K^2L K_0(t)\n\nwhich is obtained immediately by the fact that \u222b_\u03a9_hsup_s\u2264 t\ud835\udd3c_X[\u0394_L^H[W'(t)](\u03b8;)]^50(d \u03b8)^1/50\u2264 K^2 K_0(T) as established above and by the recursion\n\n  \u222b_\u03a9_hsup_s\u2264t\ud835\udd3c_X[\u0394_i^H[W'(t)](\u03b8;)]^50(\u03b8\u0323)^1/50 \u2264K^2 \u222b_\u03a9_hsup_s\u2264t\ud835\udd3c_X[\u0394_i+1^H[W'(t)](\u03b8;)]^50(\u03b8\u0323)^1/50 .\n  \n  We now consider\n  \n    _i^H(t) := \u222b_\u03a9_h sup_s\u2264t \ud835\udd3c_X|\u0394_i^H[W'(t)](\u03b8;)- \u0394_i^H[W\u201d(t)](\u03b8;)|^2 (\u03b8\u0323)^1/2 .\n  \n  Starting from i = 0 we have\n  \n    _0^H(t)    \u2264_0^H,1(t)+_0^H,2(t)+_0^H,3(t)\n  \n  where\n  \n            _0^H,1(t)     = K W_hy\u201d_t sup_s\u2264t\ud835\udd3c_X[|F\u0302(;W'(s)) - F\u0302(;W\u201d(s))|] \u2264K_0(t)^2 K^2L+2 d_t(W', W\u201d)  \n\n     _0^H,2(t)     = K^2 \u222b_\u03a9_h sup_s\u2264t |'(t;\u03b8)- \u201d(t;\u03b8)|^2 (\u03b8\u0323)^1/2 \u2264K^2d_t(W', W\u201d)\n\n     _0^H,3(t)     =  \u222b_\u03a9_h  sup_s\u2264t |'(s;\u03b8)|^2\u0394F(W\u201d(s);)\u03c3_h(H_\u03c3[W'(s)](\u03b8;, 0 )) - \u03c3_h(H_\u03c3[W\u201d(s)](\u03b8; , 0))(\u03b8\u0323)^1/2\n     \u2264K^2L+2 K_0(t)(B d_t(W', W\u201d) + \u221a(\u039e(B)))\n  \n  for any B>0, where \u039e(B) = 2Le^-K_1B^2 and in the last bound we have separated the expectation in \u03a9_h using the indicator on the set max_t(W) > BK_0(t) and its complement.\n  We then proceed estimating _i^H(t)  from _i-1^H(t): using the boundedness of , \u03c3_h', \u03c3_h and the Lipschitz continuity of \u03c3_h we have\n  \n    _i^H(t) \u2264_i^H,1(t) + _i^H,2(t)+_i^H,3(t)\n  \n  where  we have,\n  \n    _i^H,1(t)     = K^2 \u222b_\u03a9_h sup_s\u2264t \ud835\udd3c_X|\u0394_i-1^H[W'(t)](\u03b8;)- \u0394_i-1^H[W\u201d(t)](\u03b8;)|^2 (\u03b8\u0323)^1/2 \u2264K^2 _i-1^H(t) \n\n     _i^H,2(t)     = K^2L \u222b_\u03a9_h^2 sup_s\u2264t |'(t;\u03b8, \u03b8')- \u201d(t;\u03b8, \u03b8')|^2 ^\u22972(\u03b8\u0323, \u03b8\u0323')^1/2 \u2264K^2Ld_t(W', W\u201d)\n\n     _i^H,3(t)      =K_0(t)K^2L + 2   \u222b_\u03a9_h sup_s\u2264t H_\u03c3[W'(s)](\u03b8;, k ) - H_\u03c3[W\u201d(s)](\u03b8; , k)(\u03b8\u0323)^1/2\n     \u2264K_0(t) K^4L + 3(B d_t(W', W\u201d) + \u221a(\u039e(B))) .\n  \n  Combining the above equations results in max_i \u2208{0,\u2026, L}_i^H(t) \u2264 K_0(t)^2K^6L + 2((1+B)d_t(W', W\u201d)+ \u221a(\u039e(B))).\n\n  This yields, again analogously to <cit.>, estimates on the quantities\n  \n    _hh^w(t)    := \u222b_\u03a9_h sup_s\u2264t \ud835\udd3c_X[\u0394^hh(, \u03b8, \u03b8', W'(s))-\u0394^hh(, \u03b8, \u03b8', W\u201d(s))]^2^\u22972(\u03b8\u0323, \u03b8\u0323')^1/2\n\n    _xh^w(t)    := \u222b_\u03a9_h sup_s\u2264t \ud835\udd3c_X[\u0394^xh(, \u03b8, W'(s))-\u0394^xh(, \u03b8,  W\u201d(s))]^2(\u03b8\u0323)^1/2\n\n    _hy^w(t)    := \u222b_\u03a9_h sup_s\u2264t \ud835\udd3c_X[\u0394^hy(, \u03b8, W'(s))-\u0394^hy(, \u03b8,  W\u201d(s))]^2 (\u03b8\u0323)^1/2\n  \n  We only perform these estimates explicitly on the first quantity, as the other ones are analogous. In this case we have, from e:needed1 by the Lipschitz continuity of \u03c3_h and the uniform boundedness of \u0394_i^H in L^2(\u03bd),\n  \n    _hh^w(t) \u2264L K^2(_hh^w,1(t) + _hh^w,2(t))\n  \n  for\n  \n    _hh^w,1(t)     =  max_i \u2208{1, \u2026, L}\u222b_\u03a9_h sup_s\u2264t \ud835\udd3c_X[\u0394_i^H(, \u03b8, W\u201d(s))-\u0394_i^H(, \u03b8,  W\u201d(s))]^2(\u03b8\u0323)^1/2 ,\n\n     _hh^w,2(t)     = max_i \u2208{1, \u2026, L}\u222b_\u03a9_h sup_s\u2264t \ud835\udd3c_X[H_\u03c3[W'(s)](\u03b8;,  i)-H_\u03c3[W\u201d(s)](\u03b8;,  i)]^2(\u03b8\u0323)^1/2 .\n  \n  Having bounded both terms by\n  \n    K_0(t)^2K^6L + 2((1+B)d_t(W', W\u201d)+ \u221a(\u039e(B))) \u2264K^3L + 2K_0(T)^3L + 2((1+B)d_t(W', W\u201d)+ \u221a(\u039e(B)))\n    \n     in the first part of this proof and in l:10 respectively concludes the argument.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 PROOF OF CONVERGENCE\n\n\nTo prove finite time convergence for the trajectories of the large-width neural network to the corresponding mean-field limit we bound the distance\n\n  \ud835\udc9f_\u03c4(W, \ud835\udc16)   =   sup_t \u2208(0,\u03c4)(1/n^2 (t;\u03b8(i), \u03b8(j)) - (t;i,j)_2 \u22281/n (t;\u03b8(j))-(t; j)_2.\n\n                      \u2228. 1/n (t;\u03b8(j))-(t; j)_2)\n\nThis proof, given for completeness, adapts the steps of Proposition 25 in <cit.> to the present setting, and we therefore give it as a sketch. We require the following additional assumption\n\n  Let \u03b7 = n^0.501 and consider a family of initialization laws I. For each n \u2208\u2115 the sampling rule P\u0305_n satisfies that (\u03b8(j))_j =1^n \u223cP\u0305_n are \u03b7-independent, i.e. for all 1-bounded f\u00a0:\u00a0\u2192\u210b where \u210b is a separable Hilbert space we have\n\n  \ud835\udd3c[f(\u03b8(j))|{\u03b8(j')\u00a0:\u00a0j'<j}]- \ud835\udd3c[f(\u03b8(j))]_\u210b \u2264\u03b7    for all  j \u2208{1,\u2026,n}\n\n  \n\nWe recall the main theorem, stated together with the above assumption\n  \n  For any R>0, let Assumptions\u00a0<ref>, <ref>, <ref> and <ref> hold. There exist constants c,c'>0 such that, under a:1, for any \u03b4>0, any L \u2208\u2115 and \u03c4 >0, there exists n^*\u2208\u2115 such that for any n>n^* with probability at least 1-\u03b4-K\u0305 nexp(-K\u0305 n^c') we have\n  \n    \ud835\udc9f_\u03c4(W, \ud835\udc16) \u2264K\u0305 n^-c \u221a(logn^2/\u03b4+e)\n  \n  where K\u0305 is a constant that depends on  L and R.\n  \n\nWe will consider the evolution of the truncated version W of the initialization W^0, which is obtained by evolving according to e:mfpde the initial condition\n\n  (0,\u03b8)    := \u03c7\u0303_B((0,\u03b8))\n\n  (0,\u03b8)    := \u03c7\u0303_B((0,\u03b8))\n\n\nand respectively for \n\n  (0,i)    := \u03c7\u0303_B((0,\u03b8(i)))\n\n  (0,i)    := \u03c7\u0303_B((0,\u03b8(i)))\n\n\nwhere \u03c7\u0303_B(u) = u 1(|u|<B) + B (u)1(|u|\u2265 B) and 1 is the indicator function. Note that the  weights were not truncated as they are bounded by assumption.\nThen, analogously to Proposition 27 in <cit.> one can show that with probability at least 1 -KLnexp(-K e^-KB^2 n^1/52) we have\n\n  - _T \u2228W - W_T \u2264K exp-KB^2+K^2L+5(1+T^2)(1+B)\n\nWe define for any t>0, analogously to e:tnorm\n\n  - '_t    := -'_t \u2228-'_t\u2228- '_t\n\n\nfor\n\n  -'_t    := 1/n^2 \u2211_j_1, j_2=1^n sup_s\u2208(0,t) |(s,j_1,j_2)-'(s,j_1,j_2)|^2^1/2 \n\n  -'_t    := 1/n \u2211_j_1=1^n sup_s\u2208(0,t) |(s,j_1)-'(s,j_1)|^2^1/2 \n\n  -'_t    := 1/n \u2211_j_1=1^n sup_s\u2208(0,t) |(s,j_1)-'(s,j_1)|^2^1/2\n\nDefining throughout\n\n  K_t := K^\u03ba(1+t^\u03ba)\n  \nfor a choice of K, \u03ba that can change from line to line, we proceed to show that with probability at least 1- \u03b4 -KLnexp(-K  n^1/52)\n\n_t( W, ) \u2264\u221a(1/n log2 T L n^2/\u03b4+ e) exp(K_T(1+B))\n\n for every choice of \u03b4>0, B>0. Combining e:close and e:midassumption via triangle inequality we obtain that with probability 1 -\u03b4 - KLnexp(-K e^-KB^2 n^1/52) we have\n\n  _t(W,  )     \u2264_t(W, ) + - _T + W - W_T\n\n       \u2264\u221a(1/n log2 T L n^2/\u03b4+ e) + exp(-KB^2) exp(K_T(1+B))\n\nand\nchoosing B = c_0 \u221a(log n) for some suitable constant c_0>0 yields the claim of t:convergence.\n\nWe prove the missing result e:midassumption.\nTo do so, in the remainder of the section we slightly abuse notation and denote by W, the truncated W,.\nThen, for the newly defined W,,\nusing that\n\n|^0(\u03b8, \u03b8')| \u2264K ,\n\nwe define the norms\n\n  _t    := 1/n^2 \u2211_j_1, j_2=1^n sup_s\u2208(0,t) |(s,j_1,j_2)|^50^1/50 \n\n  _t    := 1/n \u2211_j_1=1^n sup_s\u2208(0,t) |(s,j_1)|^50^1/50 \n\n  _t    := 1/n \u2211_j_1=1^n sup_s\u2208(0,t) |(s,j_1)|^50^1/50\n\nand for a given realization of the sampling P\u0305_n,\n\n  W_samp,t\n      = 1/n \u2211_i=1^n sup_s<t |(s,\u03b8(i))|^50^1/50 \u22281/n \u2211_i=1^n\u222bsup_s<t |(s,\u03b8(i),\u03b8')|^50( \u03b8\u0323')^1/50\n   \n  \u22281/n \u2211_i=1^n\u222bsup_s<t |(s,\u03b8',\u03b8(i))|^50( \u03b8\u0323')^1/50\n   \n  \u22281/n^2 \u2211_i,j=1^n sup_s<t |(s,\u03b8(i),\u03b8(j))|^50^1/50\n  \u22281/n \u2211_i=1^n sup_s<t |(s,\u03b8(i))|^50^1/50\n\n Then, analogously to l:globalstability and in turn Lemma 30 in <cit.> one can show that for each W_0, W_samp,0 there exists \u03ba such that, respectively,\n\ud835\udc16_t \u2264 K_t and W_samp,t\u2264 K_t.\n\nFurther, we define \u2130 as the event\n\n  \u2130 := {_0 \u2228W_samp,0 < K}\n\nwhich holds with a probability of at least 1-KLnexp(-K n^1/52) by Lemma 29 in <cit.>\nsince by assumption W_0< K. This directly implies that _t \u2228W_samp,t < K_t by Lemma 30 in <cit.>.\n\nWe start by decomposing, for any \u03be > 0,\n\n  _t(W, )    \u2264K \u222b_0^t (^w(\u230as/\u03be\u230b\u03be) + ^w(\u230as/\u03be\u230b\u03be) +  ^w(\u230as/\u03be\u230b\u03be)  ) \u1e63 \n\n         + Kt sup_s \u2208(0, T-\u03be)sup_\u03be'\u2208(0,\u03be)max_V \u2208{W, }^\u03be[V](s,\u03be')\u2228^\u03be[V](s,\u03be') \u2228^\u03be[V](s,\u03be')\n\nwhere\n\n  ^w(t)    := 1/n^2 \u2211_j,k =1^n |\u2202_t (t;j,k) - \u2202_t (t;\u03b8(j);\u03b8(k))|^2^1/2\n\n      ^w(t)    := 1/n \u2211_j =1^n |\u2202_t (t;j) - \u2202_t (t;\u03b8(j))|^2^1/2\n\n        ^w(t)    := 1/n \u2211_j=1^n |\u2202_t (t;j) - \u2202_t (t;\u03b8(j))|^2^1/2\n      \n      and\n      \n        ^\u03be[](t, \u03be')    := 1/n^2 \u2211_j,k =1^n |\u2202_t (t;j,k) - \u2202_t (t+\u03be';j,k)|^2^1/2\n\n            ^\u03be[](t, \u03be')    := 1/n \u2211_j =1^n |\u2202_t (t;j) - \u2202_t (t+\u03be';j)|^2^1/2\n\n              ^\u03be[](t, \u03be')    := 1/n \u2211_j=1^n |\u2202_t (t;j) - \u2202_t (t+\u03be';j)|^2^1/2\n\n              ^\u03be[W](t, \u03be')    := 1/n^2 \u2211_j,k =1^n |\u2202_t (t;\u03b8(j),\u03b8(k)) - \u2202_t (t+\u03be';\u03b8(j),\u03b8(k))|^2^1/2\n\n                  ^\u03be[W](t, \u03be')    := 1/n \u2211_j =1^n |\u2202_t (t;\u03b8(j)) - \u2202_t (t+\u03be';\u03b8(j))|^2^1/2\n\n                    ^\u03be[W](t, \u03be')    := 1/n \u2211_j=1^n |\u2202_t (t;\u03b8(j)) - \u2202_t (t+\u03be';\u03b8(j))|^2^1/2\n\nThe following lemma, proven at the end of the section, bounds the error resulting from the time-disctretization in \u03be:\n For any \u03be\u2208 [0,T] we have that almost surely on the event \u2130\n  \n    sup_s \u2208(0, T-\u03be)sup_\u03be'\u2208(0,\u03be)max_V \u2208{W, }^\u03be[V](s,\u03be')\u2228^\u03be[V](s,\u03be') \u2228^\u03be[V](s,\u03be') \u2264K_T (1+B)\u03be\n\n\nWe now proceed to bound the terms on the first line of e:firstlineDD. To do so we  define for \u2113\u2208{1,\u2026, L} M_\u03c3\n\n  ^\u2113(t)    := 1/n \u2211_j=1^n \ud835\udd3c_X|^(,j,(t),\u2113) - ^H(,j,W(t),\u2113)|^2^1/2\n\n  ^\u2113(t)    := 1/n \u2211_j=1^n \ud835\udd3c_X|(,j,(t),\u2113) - (,j,W(t),\u2113)|^2^1/2\n\n  ^\u2113(t)    := 1/n \u2211_j=1^n \ud835\udd3c_X|(,j,(t),\u2113) - (,j,W(t),\u2113)|^2^1/2\n\n  ^\u2113(t)    := 1/n \u2211_j=1^n \ud835\udd3c_X|\ud835\udc07_\u03c3(,j,(t),\u2113) - H_\u03c3(,j,W(t),\u2113)|^2^1/2\n\nwhere\n\n  (,j,W,\u2113)    := W_xh(\u03b8(j))\u00b7_-\u2113 \n\n  (,j,\ud835\udc16,\u2113)    := \ud835\udc16_j \u00b7_-\u2113\n\n  (,j,W,\u2113)    := [W](\u03b8(j),, \u2113)\n\n  (,j,\ud835\udc16,\u2113)    := [\ud835\udc16]( , \u2113)_j\n\n  (,j,W,\u2113)    := (,\u03b8(j),W,\u2113) + (,\u03b8(j),W,\u2113)\n\n  \ud835\udc07_\u03c3(,j,\ud835\udc16,\u2113)    := (,j,\ud835\udc16,\u2113) + (,j,\ud835\udc16,\u2113)\n\n  ^H(,j,W,\u2113)    := \u0394_\u2113^H(,\u03b8(j),W)\n\n^(,j,,\u2113)    := \u0394_\u2113^\ud835\udc07(,j,)\n\nfor \u0394_\u2113^H defined in e:Dih and\n\n  \u0394_\u2113^\ud835\udc07(, j,  ) := \u0394F(, )\u0393_\u2113(,j, )\n\nfor\n\n  \u0393_\u2113(, j,)    := 1/n \u2211_j_0=1^n (j_0) \u03c3_h'(\ud835\udc07_\u03c3(,j_0,\ud835\udc16,0)) 1/n \u2211_j_1=1^n (j_0, j_1) \u03c3_h'(\ud835\udc07_\u03c3(,j_1,\ud835\udc16,1))\n\n             \u20261/n \u2211_j_\u2113-1=1^n (j_\u2113-1, j) \u03c3_h'(\ud835\udc07_\u03c3(,j,\ud835\udc16,\u2113))\n\nFurther defining\n\n  ^w,\u2113(t)    := (1/n \u2211_j=1^n \ud835\udd3c_X1+|^(,j,(t),\u2113)|^2 + |^H(,j,W(t),\u2113)|^2)^1/2\n\n         \u00b7[( 1/n \u2211_j=1^n \ud835\udd3c_X|(,j,(t),\u2113) - (,j,W(t),\u2113)|^2)^1/2.\n\n             + .( 1/n \u2211_j=1^n \ud835\udd3c_X|(,j,(t),\u2113) - (,j,W(t),\u2113)|^2)^1/2]\n\nwe have that on the event \u2130 by Lemma 30 in <cit.>\n^w,\u2113(t) \u2264 K_T ^\u2113(t), so that on the same event we have\n\n  ^w(t)\u2264K_T \u2211_\u2113= 0^L-1 ^\u2113+1(t) + ^\u2113+1(t) \n\nand analogously\n\n  ^w(t)   \u2264K_T   ^0(t) + ^0(t) \n\n  ^w(t)    \u2264K_T \u2211_\u2113= 0^L  ^\u2113(t)\n\nCombining these bounds with l:xibound we obtain that on the event \u2130\n\n  _t(W, ) \u2264K_T \u222b_0^t\u2211_\u2113= 0^L ^\u2113(s) + 2^\u2113(s) + (1+B)\u03be \u1e63\n\n\nWe further proceed to bound ^\u2113(s) + 2^\u2113(s) in terms of _t(W, ) with high probability as follows:\n\n  For any sequence {\u03b3_j}_j=0^L with \u03b3_j > 0, for all k \u2208{0, \u2026 ,L } and t \u2208 (0, T) the event \u2130_t,k^ where\n  \n    ^\u2113(t) \u2264K_T^L-\u2113+1 _t(W, ) + (1+B)\u2211_j=\u2113^L-1 \u03b3_j    holds for all  \u2113\u2208{k, k+1, \u2026,L}\n    \n    has probability\n    \n      \u2119(\u2130_t,k^|\u2130) \u22651- \u2211_j =k^L  n/\u03b3_j exp(-n\u03b3_j^2/K_T) .\n    \n\n\n  For any sequence {\u03b2_j}_j =1^L with \u03b2_j > 0, for all k \u2208{0, \u2026 ,L } and t \u2208 (0, T) the event \u2130_t,k^\u0394 where\n  \n    ^\u2113(t) \u2264K_T^L+\u2113+1 (1+B)_t(W, ) + (1+B^2)\u2211_j=0^L-1 \u03b3_j + \u2211_j=1^\u2113 \u03b2_j    holds for all  \u2113\u2208{0, 2,\u2026,k}\n    \nsatisfies\n    \n      \u2119(\u2130_t,k^\u0394\u2229\u2130_t,0^|\u2130) \u2265\u2119(\u2130_t,0^|\u2130) - \u2211_j =1^k n/\u03b2_j exp(-n\u03b2_j^2/K_T) .\n    \n\n\nCombining the above lemmas with e:convergence2 and l:xibound yields that for every B>0 we have\n\n  _t(W, ) \u2264K_T^2L \u222b_0^t (1+B)_s(W, ) + (1+B^2)\u2211_j=1^L \u03b3_j+1 + \u2211_j=1^L \u03b2_j+1 + (1+\n  B)\u03be\u1e63\n\nwith probability at least\n\n  1- T/\u03be\u2211_j =1^L-1 n/\u03b3_j exp(-n\u03b3_j^2/K_T) - \u2211_j =2^L n/\u03b2_j exp(-n\u03b2_j^2/K_T) - KLnexp(-Kn^1/52)\n\nThe proof is concluded applying Gronwall's lemma with\n\n  \u03b3_j = \u03b2_j := \u221a(1/K_Tn log2 T L n^2/\u03b4+ e)    and     \u03be= 1/\u221a(n)\n\nwhich gives, for all t<T and for all \u03b4>0, B>0\n\n  _t(W, )    \u2264K_T (1+B^2)\u2211_j=1^L \u03b3_j+1 + \u2211_j=1^L \u03b2_j+1 +  (1+B)\u03beexp(K_T(1+B)T)\n\n      \u2264K_T \u2211_j=1^L \u03b3_j+1 + \u2211_j=1^L \u03b2_j+1+  \u03beexp(K_T(1+B))\n\n      \u2264K_T 2L\u221a(1/K_Tn log2 T L n^2/\u03b4+ e) exp(K_T(1+B))\n\nwith probability\n\n  \u2119(\u2130 \u2229\u2130_T,0^\u2229\u2130_T,L^\u0394)    = \u2119( \u2130_T,0^\u2229\u2130_T,L^\u0394|\u2130) \u2119(\u2130)\n\n      > 1- 2L \u221a(n) T n/\u03b3_1 exp(-n\u03b3_1^2/K_T)   - KLnexp(-Kn^1/52)\n\n     > 1- \u03b4- KLnexp(-Kn^1/52)\n\nthereby proving e:midassumption, as desired.\n\n\nWe now proceed with the verification of the claims that led to this conclusion. We limit ourselves to checking l:xibound and l:Fbound as the proof of l:Gbound is analogous.\n\n We show the claim by induction on the depth of the unrolled network. Starting from ^L we have that with probability 1\n  \n    (1/n \u2211_j=1^n \ud835\udd3c_X[|   _\u03c3(,j,(t),L)  - H_\u03c3(, \u03b8(j), W(t), L)|]^2)^1/2 \n\n       = 1/n \u2211_j=1^n\ud835\udd3c_X[|(t,j) _-L - (t,\u03b8(j)) _-L|]^2^1/2 \u2264K _t(W, )\n  \n  In other words, the base case holds with probability \u2119(\u2130_t,1^) = 1.\n\n  We now assume that the claim holds for ^\u2113+1 and prove it for ^\u2113. The proof for ^\u2113, ^\u2113 is analogous. To do so we decompose ^\u2113 in two parts: the first measures the distance between a randomly sampled, finite set of weights evolving according to W(t) and (t), while the second compares the approximation obtained by taking a finite sample from W(t) and the expectation wrt  on W(t).\nMore specifically we decompose\n\n  |(x,i,(t),\u2113)     - (x, \u03b8(i), (t), \u2113)| = \n\n     = |1/n \u2211_j=1^n (t,i,j)\u03c3_h( (,j, (t), \u2113+1)+(j, , (t)))\n\n           - \u222bW(t,\u03b8(i), \u03b8') \u03c3_h( (,\u03b8', W(t), \u2113+1)+[W(t)](\u03b8'; )) (d\u03b8')|\n\n       = Q_1,\u2113(t;i) + Q_2,\u2113(t;i)\n\nwhere\n\n  Q_1,\u2113(t;i)    =1/n \u2211_j = 1^n | _hh(t,i,j)\u03c3_h( (,j, (t), \u2113+1)+(j, , (t))) \n\n             - W_hh(t,\u03b8(i),\u03b8(j)) \u03c3_h( (,\u03b8(j), W(t), \u2113+1) + [W(t)](\u03b8(j), ))|\n\n  Q_2,\u2113(t;i)    = | 1/n \u2211_j=1^nW_hh(t,\u03b8(i),\u03b8(j)) \u03c3_h( (,\u03b8(j), W(t), \u2113+1)+[W(t)](\u03b8(j), ))\n\n              - \u222bW_hh(t,\u03b8(i),\u03b8') \u03c3_h( (,\u03b8', W(t), \u2113+1) + [W(t)](\u03b8', )) (d\u03b8')|\n\nand we can bound\n\n  ^\u2113(t) \u22641/n \u2211_i=1^n\ud835\udd3c_X[|Q_1,\u2113(t;i)| + |Q_2,\u2113(t;i)|]^2^1/2 + 1/n \u2211_j=1^n\ud835\udd3c_X[|(t,j) _-\u2113 - (t,\u03b8(j)) _-\u2113|]^2^1/2\n\nThe first term is then bounded by\n\n  \ud835\udd3c_X|Q_1,\u2113(t;i)|^2    \u2264K/n \u2211_j=1^n1+|_hh(t,j,i)|^2 +|W_hh(t,\u03b8(j),\u03b8(i))|^2\n\n              \u00b71/n \u2211_j=1^n  \ud835\udd3c_X _\u03c3(,j, (t), \u2113+1) - H_\u03c3(,\u03b8(j), W(t), \u2113+1)^2\n\n        + K/n \u2211_j=1^n |_hh(t,i,j)-W_hh(t,\u03b8(i),\u03b8(j))|^2\n\nand therefore,  under the event \u2130_t,\u2113+1^ and \u2130 we have\n\n1/n \u2211_i=1^n\ud835\udd3c_X|Q_1,\u2113(t;i)|^2^1/2 \u2264K_T ^\u2113+1(t) + K _t(W, )\n\nWe proceed to bound Q_2,\u2113(t). Defining\n\n  Z_\u2113^H(t,\u03b8, \u03b8') = (t, \u03b8, \u03b8' ) \u03c3_h((, \u03b8',\u2113+1)+ (t, \u03b8) _- (\u2113+1))\n\nUsing independence  of \u03b8, \u03b8', we have that the conditional expectation wrt  is trivial\n\n  \ud835\udd3c_[Z_\u2113^H(t,\u03b8(i), \u03b8(j))| \u03b8(i)] = \ud835\udd3c_ [Z_\u2113^H(t,\u03b8(j),\u03b8')]\n\nand we have that, for almost every  almost surely by assumption\n\n  Z_\u2113^H(t,\u03b8(i), \u03b8(j)) \u2264K_T(1+B)\n\nThen, by Lemma 28 in <cit.>,\nsince \u03b3_\u2113\u2265 0 we have that \n\n  \u2119\ud835\udd3c_X[ Q_2,\u2113(t) ]\u2265K_T (1+B)\u03b3_\u2113 \u22641/\u03b3_\u2113 exp(-n \u03b3_\u2113^2/K_T) .\n\nThe proof is concluded by combinging the bound on H_hh with the one on H_xh to yield an analogous one on H_\u03c3 and taking an union bound over i \u2208{1,\u2026, n}, resulting in the fact that on the events \u2130 and \u2130_t,\u2113+1^\n\n  ^\u2113(t) \u2265K_T ^\u2113+1 (t) + 2 K _t(W, ) + K_T (1+B) \u03b3_\u2113\u2265K_T^L-\u2113+1  _t(W, ) + (1+B)\u2211_k=\u2113^L-1\u03b3_k\n\nwith probability at most (n/\u03b3_\u2113) exp(-n\u03b3_\u2113^2/K_T).\nTherefore we get by union bound\n\n  \u2119((\u2130_t,\u2113^)^c| \u2130) \u2264\u2119((\u2130_t,\u2113+1^)^c| \u2130) +(n/\u03b3_\u2113) exp(-n\u03b3_\u2113^2/K_T)  \u2264\u2211_k=\u2113^L-1 n/\u03b3_k+1 exp(-n\u03b3_k+1^2/K_T)\n\nproving the desired claim.\n\n\n\n\nWe again only sketch this proof for the term ^\u03be[W](t, \u03be') as the other cases follow analogously.\n\nWe see that since W_0, W_samp,t\u2264 K on the event \u2130, we have\n\n1/n^2\u2211_i,j=1^n sup_s\u2208(0,t)\u2202_t W_hh(s,\u03b8(i), \u03b8(j))^50^1/50\u2264K + K 1/n\u2211_j=1^n sup_s\u2208(0,t)\ud835\udd3c_x[|\u0394_i^H(,\u03b8(j),W(s))|]^50^1/50\u2264K_T\n\nfor any t\u2264 T. Consequently we have\n\n  1/n^2\u2211_i,j=1^n sup_s\u2208(0,T-\u03be)sup_\u03be'\u2208(0,\u03be)W_hh(s+\u03be',\u03b8(i), \u03b8(j))-W_hh(s,\u03b8(i), \u03b8(j))^2^1/2 \u2264K_T \u03be\nThe desired bound results from the application of an adapted version of l:11 to the paths W'(t) := W_hh(t,\u03b8(i), \u03b8(j)), W\u201d(t) := W_hh(t+\u03be,\u03b8(i), \u03b8(j)) replacing e^-K_1B^2\u2192 0 by the assumed trunctaion of W. This yields almost surely on \u2130\n\n  sup_t\u2208(0,T-\u03be)sup_\u03be'\u2208(0,\u03be) ^\u03be(t,\u03be') \u2264K_T (1+B) W'-W\u201d_T-\u03be \u2264K_T (1+B) \u03be\nas desired. Analogous bounds on ^\u03be[], ^\u03be[W], ^\u03be[],^\u03be[W], ^\u03be[] prove the lemma.\n\n\n\n\n\n\n\n\n\n\u00a7 GLOBAL OPTIMALITY\n\n\n\nRecall the definition of the preactivation between the first and the second layer:\n\n(\u03b8;\ud835\udc31,L) = \u222b(\u03b8, \u03b8') \u03c3_h(W_xh(\u03b8')\u00b7\ud835\udc31_-L) (\u03b8\u0323')\n\nand define recursively the corresponding \u2113-preactivaton for \u2113\u2264 L-1\n\n(\u03b8;\ud835\udc31,\u2113) := \u222b(\u03b8, \u03b8') \u03c3_h((\u03b8';\ud835\udc31, \u2113+1)+(\u03b8')\ud835\udc31_-(\u2113+1)) (\u03b8\u0323')\n\n\nFor notational convenience, we define \u03bd_L,\u2113:= \u03a0_#^(-L, -L+\u2113)\u03bd, where \u03a0^(a,b) is projection on coordinates ranging from a to b.\n\n\n\n \u00a7.\u00a7 Expressivity at initialization\n\n\n\nIn this section we prove our main expressivity result. Defining throughout \u0398 :=  () we we state the result as follows:\n\n  Fix L>0, for any t>0 let W = W(t) satisfy a:1b) and c), let \u03c3_h satisfy a:1a) and let Assumptions\u00a0<ref> and <ref> hold. Then\n  \n    \u03c3_h(H_hh(\u03b8;\ud835\udc31,0)+(\u03b8)\ud835\udc31_0)\u00a0:\u00a0\u03b8\u2208\u0398 = L^2(\u03bd_L,0)\n    \n  \n  The above result can readily be rephrased in the following, more explicit form:\n   Under the conditions of Proposition\u00a0<ref> above, the map\n\nF\u0302(W; \ud835\udc31 ) = \u222b( \u03b8) \u03c3_h(H_hh(\u03b8;\ud835\udc31,0)+(\u03b8)\ud835\udc31_0) (\u03b8\u0323)\n\nintended as a functional of W_hy\u2208 L^2() is dense in the space L^2(\u03bd_L,0).\n\np:spanning above proves that the network can express any function in L^2(\u03bd_L,0) provided that the support of the weights W(t) is sufficiently varied as codified in  a:1b). We will show in the next subsection that this condition, if satisfied at initialization,  is also satisfied at every finite time throughout the dynamics.\n\n\n\nTo prove the above result we first state the following\n\n  Let \ud835\udcb3\u2286\u211d^d' for d' \u2208\u2115 and let \u03bc be a probability measure on \ud835\udcb3. Assume that \u03c3_h\u00a0:\u00a0\u211d\u2192\u211d satisfies a:1, \n  that the set \u03a6 := {\u03d5_\u03b8\u00a0:\u00a0\u03b8\u2208\u0398}\u2286 L^2(\u03bc)\u2229 L^\u221e(\u03bc) is star-shaped at 0 \u2208 L^2(\u03bc) and \u03d5_\u03b8\u00a0:\u00a0\u03b8\u2208\u0398 is dense in L^2(\u03bc), then so is\n  \u03c3_h(\u03d5_\u03b8) \u00a0:\u00a0\u03b8\u2208\u0398.\n\n\n\n  Assume towards a contradiction that there exists f^* \u2208 L^2(\u03bc) such that for any sequence {\u03d5_n}_n with \u03d5_n \u2208 L^2(\u03bc) we have \u222b f^* \u03c3_h(\u03d5_n) \u03bc(d x) = 0 for all n \u2208\u2115.   By the spanning assumption there exists \u03d5^* \u2208\u03a6 with  \u03b4^*:=|\u222b\u03d5^* f^* \u03bc(dx)|>0.  We now consider the sequence of functions \u03d5_n = n\u03d5^*_\u221e^-1\u03d5^*. By assumption on the star-like structure of \u03a6, \u03d5_n \u2208\u03a6 for all n>0.\n  The result of the lemma follows by the Taylor expansion of \u03c3_h around the point 0:\n  \n    \u03c3_h(\u03d5_\u03b8(x) ) = 0 + \u03c3_h'(0) \u03d5_\u03b8(x) + R[\u03d5_\u03b8](x)\n  \n  where, denoting by \u212c_\u03c1^\u221e(0) the ball of radius \u03c1 in the L^\u221e(\u03bc) norm around 0, there exists a constant C>0 such that the remainder term satisfies |R[\u03d5](x)| < C \u03d5(x)^2 uniformly in \u03d5\u2208\u212c_\u03c1^\u221e(0) and x for \u03c1 small enough. Then, along the sequence {\u03d5_n}_n we have\n  \n    \u222bf^*(x) \u03c3_h( \u03d5_n(x) ) \u03bc(d x) \u2265\u03c3_h'(0) \u222bf^*(x) \u03d5_n(x) \u03bc(dx) -  \u222bf^*(x)R[ \u03d5_n](x) \u03bc(dx)\n  \n  We notice that for n \u2208\u2115 sufficiently large we have,\n  \n    \u03c3_h'(0) \u222bf^*(x) \u03d5_n(x) \u03bc(dx)     = \u03c3_h'(0)/n\u03d5^*_\u221e \u03b4^*\n\n     \u222bf^*(x)R[\u03d5_n](x) \u03bc(dx) \u2264f^*_2 R[\u03d5_n](x)_2    \u2264C(\u03d5^*)^2_2/n^2\u03d5^*_\u221e^2  f^*_2 \u22641/2 \u03c3_h'(0)/n\u03d5^*_\u221e \u03b4^*\n  \n  so that the first term in the expansion dominates the second. Combining this with e:triangle implies that there exists n large enough such that\n  \n    \u222bf^*(x) \u03c3_h( \u03d5_n(x)) \u03bc(d x) >1/2 \u03c3_h'(0)/n\u03d5^*_\u221e \u03b4^* > 0\n  \n  contradicting the fact that \u222b f^* \u03d5_n \u03bc (d x) = 0 for all n \u2208\u2115.\n\n\n \n  We want to show that, for any \u2113\u2208{0,\u2026, L},\n    \n      {\u03c3_h((\u03b8; ,\u2113) + _-\u2113)\u00a0:\u00a0\u03b8\u2208()} = L^2(\u03bd_-L,-\u2113)\n    \n   By the deterministic nature of the dynamical system T the measure \u03bd_-L,-\u2113 can be written, in the sense of distributions, as\n  \n    \u03bd_-L,-\u2113()    = \u03bd(_-\u2113|_-\u2113)\u2026\u03bd(_-L+1|_-L)\u03bd_0(_-L)\n\n        = \u03bd_0(_-L) \u220f_j=1^\u2113\u03b4(_L-j - T^j(_-L))\n    \n  so that, integrating on _-L, \u2026, _-\u2113 we write \n    (\u03b8; x,\u2113) := (\u03b8;(x, T(x),\u2026, T^L-\u2113+1(x)),\u2113) .\n  Then, condition e:induction can be written as\n  \n      {\u03c3_h((\u03b8; x,\u2113) + T^L-\u2113(x))\u00a0:\u00a0\u03b8\u2208()} = L^2(\u03bd_0)\n    \n  which, since {0}\u00d7 L_R^\u221e() \u2282{(t;\u03b8), (t;\u03b8,\u00b7)\u00a0:\u00a0\u03b8\u2208\u0398} by l:support follows if\n  \n    _\u03b8\u222b\u03c3(\u03b8, \u03b8')\u03c3(\u03b8', \u03b8\u201d) \u03c3\u2026\u03c3_h((\u03b8^(L))x) ^\u2297L(d \u03b8',\u2026, d \u03b8^(L)) = L^2(\u03bd_0)\n  \nWe prove e:toshow by induction on the depth of the unrolled network.\n\nBase case \u2113=L: In this case we simply need to show that {\u03c3_h((\u03b8) _-L)\u00a0:\u00a0\u03b8\u2208\u0398} is dense in L^2(\u03bd_-L) = L^2(\u03bd_0). This, however, is immediately true by the global approximation property a:1b).\n\nInduction step  \u2113\u2192\u2113-1:\nBy l:above it is sufficient to show that\n\n  '(\u03b8; x, \u2113) := \u222b(\u03b8, \u03b8')\u03c3_h(\u03b8', \u03b8\u201d) \u03c3_h \u2026\u03c3_h((\u03b8^(L))x) ^\u2297(L-\u2113+1)(d \u03b8^(\u2113),\u2026, d \u03b8^(L))\n  \nspans the desired space.\n  This claim is true if having\n\n \u222bg\u0305(x)  '(\u03b8; x, \u2113-1)  \u03bd_0(x\u0323) =  \u222bg\u0305(x) \u222b(\u03b8, \u03b8') \u03c3_h((\u03b8'; x,\u2113)\n ) (\u03b8\u0323') \u03bd_0(x\u0323)=0\n\nfor almost all \u03b8\u2208 implies that the function g\u0305\u00a0:\u00a0\u211d^\u2192\u211d must satisfy g\u0305(x) \u2261 0.\nUsing l:support to establish that {(t;\u03b8,\u00b7)}_\u03b8 is dense in \u00e5L_R^\u221e() we can rewrite the above condition as\n\n \u222bg\u0305(x) '(\u03b8; x,\u2113-1)  \u03bd_0(x\u0323)\n    =\u222bg\u0305(  x) \u222bf(\u03b8') \u03c3_h('(\u03b8'; x,\u2113)\n ) (\u03b8\u0323') \u03bd_0(x\u0323)\n\n   =\u222bf(\u03b8') \u222bg\u0305(x) \u03c3_h('(\u03b8'; x,\u2113)\n) \u03bd_0(x\u0323) (\u03b8\u0323')=0\n\nfor all \u00e5f \u2208 L_R^\u221e(), where \nin the last line we have applied Fubini's theorem. This is true only if\n\n\u222bg\u0305( x) \u03c3_h('(\u03b8'; x,\u2113)\n) \u03bd_0(x\u0323)=0    for -almost all  \u03b8'\u2208 .\n\nwhich, by the induction assumption, is only true if g\u0305(x) \u2261 0, showing e:toshow and therefore the claim.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Preservation of expressivity during training\n\n\n\nRecalling the definition of L_R^\u221e() = {f \u2208 L^2()\u00a0:\u00a0sup_\u0398 |f| \u2264 R} we have\n\nLet (t;\u00b7,\u00b7), (t;\u00b7) be the mean-field parameter functions solving e:mfpde with initial condition ^0(\u00b7,\u00b7), ^0(\u00b7). If a:1 holds, then at any time t>0 we have that\n\n((t;\u03b8),(t;\u00b7,\u03b8),(t;\u03b8,\u00b7)\u00a0:\u00a0\u03b8\u2208\u0398)    = \u00d7\u00e5L_R^\u221e()\u00d7L_R^\u221e()\n\n\n\n\nTo prove the bidirectional diversity result we will consider the flow induced by e:mfpde on any value of the (parametric) initial condition. From now on we denote by f,g the inner product in L^2().\n\n Consider a MF trajectory W(t) and a triple u = (u_1,u_2,u_3) \u2208\u211d^d\u00d7 L_R^\u221e()\u00d7 L_R^\u221e(), representing respectively values of ((\u03b8), (\u00b7, \u03b8), (\u03b8, \u00b7)). To characterize the evolution of a triple u we consider the flow\n\n  \u2202/\u2202t ^+ (t; u)    = - \u03b2(t) \u222b\u0394F(W(t), )\n       \u2211_i = 1^L+1 \u0393_i-1(W(t), \u00b7, ), ^+ (t, \u00b7; u) \u03c3_h'([W](,^+ (t,\u00b7; u),^+ (t ; u),i)) _i  \u03bd()\n\n  \u2202/\u2202t ^+ (t, \u03b8; u)    = - \u03b2(t) \u03c7_R(^+ (t, \u03b8; u)) \u222b\u0394F(W(t),) \n       \u2211_i = 1^L+1  \u0393_i-1(W(t),\u00b7, ), ^+ (t, \u00b7; u) \u03c3_h([W](,^+ (t,\u00b7; u),^+ (t ; u),i)) \u03bd()\n\n  \u2202/\u2202t ^+ (t, \u03b8'; u)    = - \u03b2(t) \u03c7_R(^+ (t, \u03b8'; u))\u222b\u0394F(W(t),)\n       \u2211_i = 1^L+1  \u0393_i-1(W(t),\u00b7, ), ^+ (t, \u00b7; u) \u03c3_h([W](\u03b8',,W,i+1)) \u03bd()\n\n  \n  with initial conditions (0; u)^+ = u_1, (0,\u00b7; u)^+ = u_2, (0,\u00b7; u)^+ = u_3, where \u0393_i(W,\u03b8, ), \u0394 F(W,) were defined in Appendix\u00a0<ref> and\n\n  [W](,^+ (t,\u00b7; u),^+ (t ; u),i) := ^+ (t,\u00b7; u) , \u03c3_h([W](\u00b7, , i+1)) + ^+ (t ; u) _-i\n\nThese flows track the evolution of mean-field parameters in the space where their evolution is naturally embedded: we see that the MF trajectory solving  e:mfpde satisfies\n\n  (t,\u03b8)    = ^+(t;(0;\u03b8), (0;\u03b8, \u00b7),(0;\u00b7, \u03b8))\n (t,\u03b8, \u00b7)    = ^+(t;(0;\u03b8), (0;\u03b8, \u00b7),(0;\u00b7, \u03b8))\n\n  (t,\u00b7, \u03b8)    = ^+(t;(0;\u03b8), (0;\u03b8, \u00b7),(0;\u00b7, \u03b8))\n\n\nWe proceed construct, for all finite T>0 and every u^+ = (u_1^+,u_2^+,u_3^+) \u2208\u211d^d\u00d7 L_R^\u221e()\u00d7 L_R^\u221e() an initial condition u^- = (u_1^-,u_2^-,u_3^-) \u2208\u211d^d\u00d7 L_R^\u221e()\u00d7 L_R^\u221e() that reaches u^+ after time T, such that\n\n  ^+(T;u^-) = u_1^+     ^+(T, \u00b7;u^-) = u_2^+     ^+(T, \u00b7;u^-) = u_3^+\n\nTo do so we consider the reverse-time dynamics on the interval (0,T), described by the flow\n\n  \u2202/\u2202t ^- (t; u)\n     = - \u03b2(T-t) \u222b\u0394F(W(T-t), )\n  \n\n          \u2211_i = 1^L+1 \u0393_i-1(W(T-t), \u00b7, ), ^- (t, \u00b7; u) \u03c3_h'((,^- (t,\u00b7; u),^- (t ; u),i)) _i  \u03bd()\n  \n\n  \u2202/\u2202t ^- (t, \u03b8; u)\n     = - \u03b2(T-t) \u03c7_R(^- (t, \u03b8; u)) \u222b\u0394F(W(T-t),)\n  \n\n          \u2211_i = 1^L+1  \u0393_i-1(W(T-t),\u00b7, ), ^- (t, \u00b7; u) \u03c3_h((,^- (t,\u00b7; u),^- (t ; u),i)) \u03bd()\n  \n\n  \u2202/\u2202t ^- (t, \u03b8'; u)\n     = - \u03b2(T-t) \u03c7_R(^- (t, \u03b8'; u)) \u222b\u0394F(W(T-t),)\n  \n\n         \u2211_i = 1^L+1  \u0393_i-1(W(T-t),\u00b7, ), ^- (t, \u00b7; u) \u03c3_h((\u03b8',,W,i)) \u03bd()\n\n  \n  initialized at ^- (0; u) = u_1, ^- (0, \u00b7; u) = u_2 and ^- (0, \u00b7; u) = u_3. Note that, by construction, ^-(t) = ^- (T-t, \u03b8; u), ^-(t) = ^- (T-t, \u03b8; u) and ^-(t) = ^- (t; u) solve the same equation as ^+(t; u), ^+(t,\u00b7; u), ^+(t,\u00b7; u) with initial condition ^-(0, \u00b7 ) = ^-(T,\u00b7; u^+), ^-(0, \u00b7 ) = ^-(T,\u00b7; u^+), ^-(0 ) = ^-(T; u^+).\nBy existence and uniqueness of the solution of this system of ODEs both forward and backward in time proven in s:existenceuniqueness, we must have that, setting u^- = (u_1^-,u_2^-, u_3^-) := (^-(T,\u00b7; u^+),^-(T,\u00b7 ; u^+), ^-(T,\u00b7; u^+)) as the initial condition of\n e:aflow, the endpoint of the trajectory of satisfies e:endpoint as desired. \u00e5Finally, we show that the point u^- is in \u211d\u00d7 L_R^\u221e()\u00d7 L_R^\u221e(). This follows immediately upon showing that the set \u211d\u00d7 L_R^\u221e()\u00d7 L_R^\u221e() is invariant with respect to the flow maps (^+,^+, ^+), (^-,^-, ^-) induced by the ODEs. The forward invariance of \u211d for  under both forward and backward flow maps follows from the Lipschitz bounds on the RHS of the corresponding ODEs, established in s:existenceuniqueness. It remains to prove forward invariance of L_R^\u221e(), which we now do by contradiction. Assuming that L_R^\u221e() is not invariant with respect to (^+,^+, ^+), (^-,^-, ^-), then by the continuity of the flow maps, there must exist \u03b8, \u03b8' \u2208 with |(t;\u03b8, \u03b8')| = K such that \u2202_t |(t;\u03b8, \u03b8')| >0, which is impossible given that \u2202_t |(t;\u03b8, \u03b8')| = 0, since \u03c7_R((\u03b8, \u03b8')) = 0, for all such \u03b8, \u03b8' .  \n\n\nBy continuity of the solution map u \u21a6 (^+(T; u), ^+(T, \u00b7; u), ^+(T, \u00b7; u)), for any \u03f5 > 0 there exists a neighborhood U of u^- \u2208\u211d\u00d7 L_R^\u221e() \u00d7 L_R^\u221e() such that\n\n  (^+(T;u), ^+(T,\u00b7;u), ^+(T,\u00b7;u)) - u^+ < \u03f5\nfor all u \u2208 U.\nThis finally implies, by  a:1c), that ((T;\u03b8), (T; \u03b8, \u00b7), (T; \u00b7, \u03b8)) has full support in \u211d^d\u00d7 L_R^\u221e()\u00d7 L_R^\u221e(), which in turn proves the claim.\n\n\n\n\n\n \u00a7.\u00a7 Proof of t:optimality\n\n\n\nThe proof of  t:optimality is carried out by adapting the argument from  Theorem 50 in <cit.>, to the present setting. We recall that, writing \u0394 F[W]() := F\u0302( ;W(t)) - F^*( ) and using the definition e:hh we have\n\n  \u2202_t  (t;\u03b8)  = - \u222b\u0394F[W]()  \n  \u03c3_h((\u03b8; , 0)) \u03bd()\n\nso that, by the convergence assumption, we have that for every \u03f5>0 there exists a T>0 such that for almost every \u03b8\u2208()\n\n   | \u222b\u0394F[W]()  \n  \u03c3_h((\u03b8; , 0)) \u03bd()| \u2264\u03f5 .\n\nWe proceed to prove that \u0394 F[W] converges in L^2(\u03bd) to \u0394 F[W\u0305] as t\u2192\u221e. To do so we define\n\n  \u03b4_i(t,,\u03b8) = \u03c3_h([W\u0305](\u03b8; , i)) - \u03c3_h([W(t)](\u03b8; , i))\n\nfor which by boundedness and Lipschitz continuity of \u03c3_h we have\n\n  \u03b4_L(t, , \u03b8)    \u2264K |W\u0305_xh(\u03b8)_-L - W_xh(t;\u03b8)_-L|\n\n  \u03b4_i(t,,\u03b8)    \u2264K (|W\u0305_xh(\u03b8)_-L -  W_xh(t;\u03b8)_-L| + K \u222b|W\u0305_hh(\u03b8, \u03b8') -  W_hh(t;\u03b8,\u03b8' )|(\u03b8\u0323' ) . \n                     .+ \u222b|W\u0305_hh(\u03b8, \u03b8')\u03b4_i+1(t,,\u03b8')| (\u03b8\u0323'))\n\nTherefore, denoting by \u03b8 the differential \u03b8\u0323^(0),\u2026, \u03b8\u0323^(L) we have that\n\n  \u222b|   \u0394F[W\u0305]() - \u0394F[W(t)]()|^2 \u03bd() \n\n     = \u222b|F\u0302(W\u0305; ) - F\u0302(W(t);)|^2\u03bd()\n\n      \u2264\u222bK \u222b|W\u0305_hy(\u03b8) - W_hy(t;\u03b8)| (\u03b8\u0323) + \u222bW\u0305_hy(\u03b8) \u03b4_0(t, , \u03b8) (\u03b8\u0323)^2 \u03bd()\n\n      \u2264K^2L \u2211_i=0^L \u222b|W\u0305_hy(\u03b8^(0))|^2 \u220f_j=1^i-1 |W\u0305_hh(\u03b8^(j-1),\u03b8^(j))|^2W\u0305_hh(\u03b8^(i-1),\u03b8^(i))- W_hh(t;\u03b8^(i-1),\u03b8^(i))^2^\u2297L+1(\u03b8)\n   \n    +  K^2L \u2211_i=0^L \u222b|W\u0305_hy(\u03b8^(0))|^2 \u220f_j=1^i-1 |W\u0305_hh(\u03b8^(j-1),\u03b8^(j))|^2W\u0305_xh(\u03b8^(i-1))- W_xh(t;\u03b8^(i-1))^2^\u2297L+1(d \u03b8) \ud835\udd3c_X[^2]\n\n        + K^2 \u222b|W\u0305_hy(\u03b8) - W_hy(t;\u03b8)|^2 (d\u03b8)\n\nand by a:1 we have that the above goes to 0 as t \u2192\u221e.\n\n\n\nHaving proven the convergence of \u0394 F[W(t)] to  \u0394 F[W\u0305] we proceed to prove the claim of the theorem. By boundedness of \u03c3_h we have that for every \u03b8\u2208()\n\n  | \u222b\u0394F[W\u0305]  \n \u03c3_h     ((\u03b8; , 0)) \u03bd()|\n   \n \u2264K | \u222b(\u0394F[W\u0305]()  - \u0394F[ W]()) \u03bd()| + | \u222b\u0394F[ W]()  \n\u03c3_h     ((\u03b8; , 0)) \u03bd()|\n   \n\u2264K \u03f5\nBy continuity of \u03c3_h  we have that for every \u03f5 > 0\n\n  |\u222b\u0394F[W\u0305]()  \n f() \u03bd()| \u2264K \u03f5\nuniformly over f() \u2208 S where S = {\u03c3_h((\u03b8; , 0))\u00a0:\u00a0\u03b8\u2208}, implying that\n|\u222b\u0394 F[W\u0305]()  \n f() \u03bd()|= 0 for all  f \u2208 S.\nSince from p:spanning we have that (\u03c3_h((\u03b8; , 0))) = L^2(\u03bd), the above result immediately yields that for \u03bd-almost every , \u0394 F[W\u0305]() = 0, so that \u2112(W\u0305) = 0.\n\nFinally, we prove the desired result by connecting \u2112(W\u0305) and \u2112(W(t)):\n\n|\u2112(W\u0305) - \u2112(W(t))|    = |\u222b\u0394F[W\u0305]()^2 - \u0394F[W(t)]()^2 \u03bd()|\n    \u22642K F\u0302(W\u0305;\u00b7) - F\u0302(W(t);\u00b7)_\u03bd ,\n\nwhich by e:lastbound goes to 0 with t \u2192\u221e. Combining the above we have\n\n  lim_t \u2192\u221e \u2112(W(t)) \u2264\u2112(W\u0305) + lim_t \u2192\u221e |\u2112(W\u0305) - \u2112(W(t))| = 0\n\nwhich proves the claim.\n\n\nAuthor affiliations:\n\n\n\n\n\n"}