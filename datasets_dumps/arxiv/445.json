{"entry_id": "http://arxiv.org/abs/2303.06699v1", "published": "20230312162840", "title": "PageRank Nibble on the sparse directed stochastic block model", "authors": ["Sayan Banerjee", "Prabhanka Deka", "Mariana Olvera-Cravioto"], "primary_category": "math.PR", "categories": ["math.PR", "0580, 05C81, 05C85, 60J80"], "text": "\n\n\n\n\n\n\n\n\n\nBanerjee, Deka and Olvera-Cravioto\n\n\n\nUniversity of North Carolina at Chapel Hill, Chapel Hill NC 27514, USA \n\n\n\n\n{sayan,deka,molvera}@email.unc.edu\n\nPageRank Nibble on the sparse directed stochastic block modelSupported in part by the NSF RTG award (DMS-2134107) and in part by the NSF-CAREER award (DMS-2141621).\n    Sayan Banerjee  Prabhanka Deka Mariana Olvera-Cravioto 0000-0003-3335-759\n    \n March 2023 \n\n====================================================================================================================================================================\n\n\n\n\nWe present new results on community recovery based on the PageRank Nibble algorithm on a sparse directed stochastic block model (dSBM). Our results are based on a characterization of the local weak limit of the dSBM and the limiting PageRank distribution. This characterization allows us to estimate the probability of misclassification for any given connection kernel and any given number of seeds (vertices whose community label is known). The fact that PageRank is a local algorithm that can be efficiently computed in both a distributed and asynchronous fashion, makes it an appealing method for identifying members of a given community in very large networks where the identity of some vertices is known. \n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nMany real-world networks exhibit community structure, where members of the same community are more likely to connect to each other than to members of different communities.  Stochastic block models are frequently used to model random graphs with a community structure, and there are many problems where the goal is to identify the members of a given community, often based only on the graph structure, i.e,, on the vertices and the existing edges among them. A two community symmetric SBM is described by two parameters \u03b1 and \u03b2, which determine the edge probabilities, with \u03b1 corresponding to the probability that two members from the same community connect to each other, and \u03b2 to the probability that two members from different communities connect to each other.  In <cit.>, the authors work on the semi-sparse regime \u03b1 = a log n /n and \u03b2 = b log n /n, where n is the number of vertices in the graph,  and show that the exact recovery of communities is efficiently possible if | \u221a(a) - \u221a(b)| > 2 and impossible otherwise. When recovery is possible, the authors use spectral methods to get an initial guess of the partition and fine tune it to retrieve the communities. Similar work has been done in the sparse regime, where \u03b1 = a/n and \u03b2 = b/n. In <cit.>, the authors show that recovery is impossible when (a-b)^2 < 2 (a+b). In <cit.>, it was proved that recovery is efficiently possible when (a-b)^2 > 2 (a+b) through the use of the spectral properties of a modified adjacency matrix B that counts the number of self avoiding paths of a given length l between two vertices in the graph. Further, the authors of <cit.> show that it is possible to recover a fraction (1-\u03b3) of the vertices of community 1 if a and b are sufficiently large and satisfy (a-b)^2 > K_1 log(\u03b3^-1) (a+b) for some constant K_1 . The clustering methods in <cit.> all rely on finding eigenvectors of the adjacency matrix (or a modified adjacency matrix), which is computationally expensive for large networks. \n\n\n\nAlthough the literature on community detection is vast, and there are in fact many methods that work remarkably well, many of those methods become computationally costly for very large networks. In some important cases like the web graph and social media networks, the networks of interest are so large and constantly changing that it becomes difficult to implement some of these methods. Moreover, in many cases, one has more information about the network than just its structure, e.g., vertex attributes that tell us the community to which certain vertices belong to. The question is then whether one can leverage knowledge of such vertices to help identify other members of their community using a computationally efficient method that does not require information about the entire network.  One such problem was studied in <cit.>, where the authors consider community detection in a dense (average degree of vertices scale linearly with the size of the network) SBM in which information about the presence or absence of each edge was hidden at random. Here, we will analyze a setting where the labels of some prominent members of the community of interest are known. \n\nThe PageRank Nibble algorithm was introduced in <cit.> as a modification of the Nibble algorithm described in <cit.> that uses personalized PageRank. This algorithm provides a cheap method for identifying the members of one community when a number of individuals in that community have been identified.  PageRank based clustering methods were also proposed in <cit.> for the two-commmunity SBM, as a special case of a more general method of combining random walk probabilities using a \u201cdiscriminant\" function. \n\nThe intuition behind PageRank Nibble is that random walks that start with the individuals that are known to belong to the community we seek will tend to visit more often members of that same community. PageRank Nibble works by choosing the personalization parameter of the known individuals, which we refer to as the \u201cseeds\", to be larger than for all other vertices in the network, and then choosing a damping factor c sufficiently far from either 0 or 1. This choice of the personalization values makes the PageRanks of  close neighbors of the seeds to be larger, compared to those of individuals outside the community. Once the ranks produced by PageRank Nibble have been computed, a simple threshold rule can be used to identify the likely members of the community of interest. PageRank based methods can generally be executed quickly due to the availability of fast, distributed algorithms <cit.>.\n\nPageRank Nibble on the undirected SBM was studied in <cit.> under regimes where personalized PageRank (PPR) concentrates around its mean field approximation. The idea proposed there was to use the mean field approximation to identify vertices belonging to the same community as the seeds. In particular, the authors of <cit.> show that concentration occurs provided the average degrees grow as a(n) log n for some a(n) \u2192\u221e as n \u2192\u221e, and is impossible for the sparse regime where average degrees remain constant as the network size grows.  Our present work focuses on the directed stochastic block model (dSBM) in the sparse regime, and our results are based on the existence of a local weak limit and, therefore, of a limiting PageRank distribution.  Once we have this characterization, we can compute the probability that an individual will be correctly or incorrectly classified, and choose the threshold that minimizes the misclassification probability.\n\n\n\n\n\n\n\n\u00a7 MAIN RESULTS\n\n\nLet \ud835\udca2_n = G(V_n, E_n) be a dSBM on the vertex set V_n = {1, \u2026, n } with two communities. To start, each vertex v \u2208 V_n is assigned a latent label C_v \u2208{1, 2} identifying its community. We assume that these labels are unknown to us. Denote by _1 and _2 the subsets of vertices in communities 1 and 2 respectively. Then, each possible directed edge is sampled independently according to:  \n\n    p^(n)_vw := ((v,w) \u2208 E_n | C_v, C_w ) = \n        a/n\u2227 1    if  C_v = C_w\n    b/n\u2227 1    if  C_v \u2260 C_w.\n\nThe edge probabilities can be written as p_vw^(n) = (n^-1\u03ba_C_v, C_w) \u2227 1, where \n    \u03ba = [ a b; b a ]\n is called the connection probability kernel for the dSBM. \n\nFor i=1, 2, we define \n    \u03c0_i^(n) = 1/n\u2211_v=1^n 1 (C_v = i)\n to be the proportion of vertices belonging to each community. We focus specifically on the case where \u03c0_1^(n) = \u03c0_2^(n) = 1/2, but the techniques used here can be applied to more general dSBMs.\n\nTo describe the setting for our results, start by fixing a constant 0 < s < 1, and  assume there exists a subset \u2286_1, with || = n \u03c0_1^(n) s, for which the community labels are known. In other words, we assume that we know the identities of a fixed, positive proportion of the vertices in community 1. We refer to the vertices in  as the seeds. In a real-world social network one can think of the seeds as famous individuals whose community label or affiliation is known or easy to infer.  Given the seed set , the goal is to identify the vertices v \u2208\ud835\udc9e_1 \u2216, i.e., to recover the remaining members of community 1. \n\nIn order to describe the PageRank Nibble algorithm, we start first with the definition of personalized PageRank. On a directed graph G = (V,E), the PageRank of vertex v \u2208 V is given by:\n\n    r_v = c \u2211_w \u2208 V : (w,v) \u2208 E1/D_w^+  r_w  + (1-c) q_v,\n\nwhere D_w^+ is the out-degree of vertex w \u2208 V, q_v is the personalization value of vertex v, and c \u2208 (0,1) is a damping factor. \n\nPageRank is one of most popular measures of network centrality, due to both its computational efficiency (it can be computed in a distributed and asynchronous way), and its ability to identify relevant vertices. When \ud835\udc2a = (q_v: v \u2208 V) is a probability vector, the PageRank vector \ud835\udc2b = (r_v: v\u2208 V) is known to correspond to the stationary distribution of a random walk that, at each time step, chooses, with probability c, to follow an outbound edge uniformly chosen at random, or with probability 1-c, chooses its next destination according to \ud835\udc2a  (if the current vertex has no outbound edges, the random walk always chooses its next destination according to ). PageRank is known to rank highly vertices that either have a large in-degree, or that have close inbound neighbors whose PageRanks are very large <cit.>, hence capturing both popularity and credibility. Since on large networks the PageRank scores will tend to be very small, it is often convenient to work with the scale-free (graph-normalized) PageRank vector \ud835\udc11 = |V| \ud835\udc2b instead. \n\nFor the two community dSBM G_n = (V_n, E_n) described above, let Q_v = nq_v and define \n    \u03bc_n( B) = 1/n\u2211_v=1^n 1((C_v, Q_v) \u2208 B)\n for any measurable set B. We assume that there exists a limiting measure \u03bc with \u03c0_i := \u03bc({i}\u00d7_+) > 0 for i = 1,2 such that \n \n    \u03bc_n \u21d2\u03bc\n in probability. Here and in the sequel, \u21d2 denotes weak convergence. Further, for any measurable A, let \n\n    \u03c3_i^(n) (A) = 1/n \u03c0_i^(n)\u2211_v \u2208 V_n 1(C_v = i, Q_v \u2208 A),      i = 1,2,\n \ndenote the empirical distribution of Q_v conditionally on C_v = i for i = 1,2. Due to assumption (<ref>), we get the existence of limiting distributions \u03c3_i, given by \n\n    \u03c3_i(A) = \u03bc({i}\u00d7 A)/\u03c0_i,      i = 1,2,\n such that \u03c3_i^(n)\u21d2\u03c3_i in probability as n \u2192\u221e. \n\nAs mentioned in the introduction, our analysis is based on the existence of a local weak limit for the dSBM, and the fact that if we let I be uniformly chosen in V_n, independently of G(V_n, E_n), and let R_I denote the scale-free PageRank of vertex I, then R_I converges weakly to a random variable \u211b as n \u2192\u221e. In order to characterize the distribution of \u211b, first define R^(1) and R^(2) to be random variables satisfying\n\n    \u2119( R^(i)\u2208\u00b7) = \u2119( . R_I \u2208\u00b7| C_I = i ),      i = 1,2.\n\nOur first result establishes the weak convergence of R^(i) for i = 1,2 and characterizes the limiting distributions as the solutions to a system of distributional fixed-point equations.\n\n\n \nLet G_n = (V_n, E_n) be a sequence of dSBM as described above such that (<ref>) holds. Then, there exist random variables \u211b^(1) and \u211b^(2) such that for any x \u2208\u211d that is a point of continuity of the limit,\n\n    R^(i)\u21d2\u211b^(i)    and    2/n\u2211_v\u2208 V_n 1( R_v \u2264 x,   C_v = i) ( \u211b^(i)\u2264 x )  ,\n\nas n \u2192\u221e, i = 1,2. Moreover, the random variables \u211b^(1) and \u211b^(2) satisfy:\n\n    \u211b^(1)d= c \u2211_j=1^\ud835\udca9^(11)\u211b^(1)_j /\ud835\udc9f_j^(1) + c \u2211_j=1^\ud835\udca9^(12)\u211b^(2)_j /\ud835\udc9f_j^(2) + (1-c) \ud835\udcac^(1)\n    \u211b^(2)d= c \u2211_j=1^\ud835\udca9^(21)\u211b^(1)_j /\ud835\udc9f_j^(1) + c \u2211_j=1^\ud835\udca9^(22)\u211b^(2)_j /\ud835\udc9f_j^(2) + (1-c) \ud835\udcac^(2)\n\nwhere ^(1) and ^(2) are random variables distributed according to \u03c3_1 and \u03c3_2 respectively, \ud835\udca9^(kl) are Poisson random variables with means \u03c0_l \u03ba_lk, (_j^(i) - 1 : j \u2265 1), i = 1,2, are i.i.d.\u00a0sequences of Poisson random variables with mean \u03c0_1\u03ba_i1 + \u03c0_2 \u03ba_i2, and (\u211b_j^(i): j \u2265 1), i=1,2, are i.i.d.\u00a0copies of \u211b^(i), with all random variables independent of each other. \n\n\n    Note that the (\ud835\udc9f_j^(i)) are size-biased Poisson random variables that represent the out-degrees of the inbound neighbors of the explored vertex I.    \n\n\n\nThe above result holds in more generality for a degree-corrected dSBM with k-communities, but for the purposes of this paper, we restrict ourselves to the k=2 case. We will only outline a sketch of the proof, and focus our attention instead on the following theorem about the classification of the vertices.\n\nEquations (<ref>) and (<ref>) are the key behind our classification method. Observe that in the PageRank equations (<ref>), the parameters within our control are the damping factor c and the personalization vector \ud835\udc10 = (Q_v: v \u2208 V_n). If we choose \ud835\udc10 that results in ^(1)\u2265_s.t.^(2), we can identify vertices in community 1 as the ones having higher PageRank scores. With that in mind, we set Q_v = 1(v \u2208), choose an appropriate cutoff point x_0 (which might depend on c, s and \u03ba), and classify as a member of community 1 any vertex v \u2208 V_n such that its scale-free PageRank, R_v, satisfies R_v > x_0.  The algorithm requires that we choose c sufficiently bounded away from both zero and one, since from the random walk interpretation of PageRank, it is clear that we want to give the random surfer time to explore the local neighborhood, while at the same time ensuring that it returns sufficiently often to the seed set. In practice, a popular choice for the damping factor is c=0.85. In the context of the dSBM, we have that when a >> b, the random surfer ends up spending more time exploring the vertices in community 1, and the probability that it escapes to community 2 before jumping back to the seeds is much smaller. As a result, the stationary distribution ends up putting more mass on the community 1 vertices, and the proportion of misclassified vertices diminishes when a+b is large and b/a is close to zero. We formalize this in the theorem below. Note that Theorem\u00a0<ref> gives that the miscalssification probabilities satisfy:\n\n    \u2119( . R_v \u2264 x_0  | v \u2208\ud835\udc9e_1 ) \u2248\u2119( \u211b^(1)\u2264 x_0 )   and\n\n\n    \u2119( . R_v > x_0  | v \u2208\ud835\udc9e_2 ) \u2248\u2119( \u211b^(2) > x_0 ) .\n\n\nOur local classification algorithm with input parameters c and x_0 is then described as follows:\n\n    \n  * Set Q_v = 1 if v \u2208, and zero otherwise. \n    \n  * Fix the damping factor c \u2208 (0,1) and compute the personalized scale-free PageRank vector \ud835\udc11.     \n    \n  * For a threshold x_0, the estimated members of \ud835\udc9e_1 are the vertices in the set _1(x_0,c) = { v \u2208 V_n: R_v > x_0 }.\n\n\nThe theorem below can be used to quantify the damping factor c and the classification threshold x_0, and the corollary that follows shows that the proportion of misclassified vertices becomes small with high probability as n \u2192\u221e. \n\n \n    Let G_n = (V_n, E_n) be a 2-community dSBM with \n    \u03ba = [ a b; b a ]\n and \u03c0_1 = \u03c0_2 = 1/2. Assume a,b satisfy 8b/(a+b) < 1/2 and e^-(a+b)/2 < b/4a. Let Q_v = 1(v \u2208) for v \u2208 V_n, and take any c \u2208 (1/2, 1 - 8b/(a+b)]. Then, for x_0 = 5s/8, we have \n    \n    (^(1) < 5s/8)    \u2264256 c^2/(a+b)(1-c^2) + 64(1-c)(1-s)/(1+c)s,\n    (^(2) > 5s/8)    \u2264256 c^2/(a+b)(1-c^2)(1 + (1-c)(1-s)/2(1+c)s).\n\n    \n    \n     \n      \n    \n\n\n\n\nNaturally, the misclassification errors get smaller as s increases, i.e., as more members of community 1 are known. Also, we get better bounds for the misclassification errors when a+b is large (strong connectivity within a community) and b/(a+b) is small (equivalently, a/(a+b) close to one), i.e., when the network is strongly assortative. \n\nNote that the assumptions in Theorem <ref> do not involve s (proportion of seeds). As the proof indicates, our classification errors involve Chebychev bounds which crucially depend on: (i) the mean PageRank scores of the two communities being sufficiently different, and (ii) the ratio of the variance of the PageRank scores of vertices in each community to the square of the mean community PageRank being small. By Lemma <ref> below, the ratio of the mean community PageRank scores is independent of s and hence their separation required by (i) is ensured by conditions involving a,b but not s. Moreover, as seen in Lemma <ref>, the scaled fluctuations in (ii) depend more significantly on the `sparsity' of the underlying network, quantified by a+b (expected total degree of a vertex), than s. Thus, the dependence on s arises mainly through the choice of the threshold x_0 in our classification algorithm (see Corollary <ref>).\n\nAs a direct corollary to Theorem <ref>, we have\n\nLet x_0 = 5s/8, c \u2208 (1/2, 1 - 8b/(a+b)], \n    \u03b4_1 = 256 c^2/(a+b)(1-c^2) + 64(1-c)(1-s)/(1+c)s\n and \n    \u03b4_2 = 256 c^2/(a+b)(1-c^2)(1 + (1-c)(1-s)/2(1+c)s).\n Then, under the hypothesis of Theorem 2, for \u03b4 = \u03b4_1 + \u03b4_2 and any \u03f5 > 0, we have \n\n    lim_n \u2192\u221e( | _1 _1(x_0,c)| > (\u03b4 + \u03f5) n/2) = 0.\n\n\n\n    For notational convenience, we drop the dependence of \ud835\udc9e\u0302_1 on x_0 and c. Observe that | _1 _1| = |_1 \\_1| + | _1 \u2229_2|, and we have _1 \\_1 = {v \u2208_1 : R_v < 5s/8 } and _1 \u2229_2 = { v \u2208_2 : R_v > 5s/8 }. So we get that for x_0 = 5s/8,\n    \n    (| _1 _1| > (\u03b4+\u03f5) n/2) = ( 2/n\u2211_v \u2208_1 1(R_v < x_0) + 2/n\u2211_v \u2208_2 1(R_v > x_0) > \u03b4 +\u03f5).\n Then the result follows since \n    \n    2/n\u2211_v \u2208_1 1(R_v < x_0) +  2/n\u2211_v \u2208_2 1(R_v > x_0) \n       (^(1) < x_0)+ (^(2) > x_0) = \u03b4\n\n    as n \u2192\u221e.\n\n\n\nOur proof of Theorem <ref> uses Chebyshev\u2019s inequalities based on mean and variance bounds for the limiting (scale-free) personalized PageRank distribution obtained from the distributional fixed-point equations in Theorem <ref>. The choice of x_0 above is rather ad hoc and mainly for simplicity of the associated misclassification error bounds. One can check that the choice of x_0 which minimizes the sum of the Chebyshev error bounds is given by x_0^* = (r_1 v_2^1/3 + r_2v_1^1/3)/(v_1^1/3 + v_2^1/3), where r_1,r_2 are the expected limiting PageRank values obtained in Lemma <ref> and v_1,v_2 are the corresponding variances obtained in Lemma <ref>. Further, x_0 = 5s/8 is independent of the kernel parameters a and b, which are often unknown in practice. Moreover, although the range of c depends on a,b, the results above hold for any c in the given range. Hence, in practice, when a,b are not known, then any c > 1/2 which is not too close to one should work provided the network is not too sparse (b/(a+b) is sufficiently small). \n\n\n\n\n\n\n\n\n\u00a7 PROOFS\n\n\nAs mentioned earlier, Theorem\u00a0<ref> holds in considerably more generality than the one stated here, so we will only provide a sketch of the proof that suffices for the simpler dSBM considered here. The proof of Theorem\u00a0<ref> is given later in the section.\n\nTheorem\u00a0<ref> (Sketch).\nThe proof consists of three main steps. \n\n\n  * Establish the local weak convergence of the dSBM: For the 2-community dSBM considered here, one can modify the coupling in <cit.> (which works for an undirected SBM) to the exploration of the in-component of a uniformly chosen vertex. The coupled graph is a 2-type Galton-Watson process, with the two types corresponding to the two communities in the dSBM, and all edges directed from offspring to parent. The number of offspring of type j that a node of type i has is a Poisson random variable with mean m_ij^- = \u03c0_j \u03ba_ji for j=1,2. For each node  in the coupled tree, denote by C_ its type, and assign it a mark _ = ( \ud835\udc9f_, Q_), where (\ud835\udc9f_ - 1 )| C_ = j is a Poisson random variable with mean m_j^+ = \u03c0_1 \u03ba_j1 + \u03c0_2 \u03ba_j2, and Q_ | C_ = j has distribution \u03c3_j as defined in (<ref>). The construction of the coupling follows a two step exploration process similar to the one done for inhomogeneous random digraphs in <cit.>. First the outbound edges of a vertex are explored, followed by the exploration of its inbound neighbors, assigning marks to a vertex once we finish exploring both its inbound and outbound one-step neighbors. This establishes the local weak convergence in probability of the dSBM to the 2-type Galton-Watson process. \n\n\n  * Establish the local weak convergence of PageRank: Once we have the local weak convergence of the dSBM, let \u211b^* denote the personalized PageRank of the root node of the 2-type Galton-Watson process in the coupling. The local weak convergence in probability of the PageRanks on the dSBM to \u211b^*, i.e., \n\n    1/n\u2211_v \u2208 V_n 1( R_v \u2264 x)  (\u211b^* \u2264 x)\n\nas n \u2192\u221e, follows from Theorem\u00a02.1 in <cit.>. Note that the random variables \u211b^(1) and \u211b^(2) correspond to the conditional laws of \u211b^* given that the root has type 1 or type 2, respectively. And since the two communities are assumed to have the same size, the probability that the root has type 1 is 1/2, hence,\n\n    1/n\u2211_v \u2208 V_n 1( R_v \u2264 x,   C_v = i) (\u211b^(i)\u2264 x ) 1/2,\n\nas n \u2192\u221e. The weak convergence result follows from the bounded convergence theorem.\n\n\n  * Derive the distributional fixed point equations: If the nodes in the first generation of the 2-type Galton-Watson process are labeled 1 \u2264 j \u2264\ud835\udca9, where \ud835\udca9 denotes the number of offspring of the root node, then \n\n    \u211b^* = c \u2211_j=1^\ud835\udca9\u211b_j/\ud835\udc9f_j + (1-c) \ud835\udcac,\n\nwhere \ud835\udcac denotes the personalization value of the root, (\ud835\udc9f_j: j \u2265 1) correspond to the out-degrees of the offspring, and the (\u211b_j: j \u2265 1) correspond to their PageRanks. Conditioning on the type of the root, as well as on the types of its offspring, gives the two distributional fixed-point equations (<ref>) and (<ref>). In particular, conditionally on the root having type i,  \ud835\udca9^(ik) corresponds to the number of offspring of type k, \ud835\udcac^(i) has distribution \u03c3_i, and \ud835\udc9f_1^(k) and \u211b_1^(k) are independent random variables having the distribution of \ud835\udc9f_1 and \u211b_1 conditionally on node 1 having type k.\n\n\n\nWe prove Theorem <ref> through the second moment method. First we prove the following lemmas establishing bounds on the mean and variance of ^(1) and ^(2).\n\n \nLet r_i = [ ^(i)], \u03bb = 1 -  e^-(a+b)/2 and \n    \u0394 = (1 - c \u03bb a/a+b)^2 - ( c \u03bb b/a+b)^2 .\n Then, we have \n\n    r_1    = ( 1 - c \u03bb  a/a+b)s(1-c)/\u0394\n    \n        r_2    = ( c \u03bb b/a+b)s(1-c)/\u0394.\n\nFurther, if 1 - \u03bb =  e^-(a+b)/2\u2264 b/4a and c >1/2, we have the bounds\n\n    r_1    \u2265 s (1 - 2b/(1-c) (a+b)) ,\n    \n        r_2    \u2264s/2.\n\n\n\nRecall the distributional equations satisfied by ^(1) and ^(2) from Theorem\u00a0<ref>. Taking expectation on both sides gives us\n\n    [^(1)]    = c [  \u2211_j=1^\ud835\udca9^(11)_j^(1)/_j^(1) + \u2211_j=1^\ud835\udca9^(12)_j^(2)/_j^(2)]  + (1-c)  [^(1)],\n    [^(2)]    = c [ \u2211_j=1^\ud835\udca9^(21)_j^(1)/_j^(1) + \u2211_j=1^\ud835\udca9^(22)_j^(2)/_j^(2)]   + (1-c) [^(2)].\n\nFirst, note that with our choice of \ud835\udcac, [^(1)] = s  and [^(2)] = 0. Further (_j^(i), _j^(i))_j \u2265 1 (resp. (_j^(i), _j^(i))_j \u2265 1) are independent of \ud835\udca9^(1i) (resp. \ud835\udca9^(2i)), and of each other, for i = 1, 2. So the above expressions can be simplified to \n\n    r_1    = c ( [ \ud835\udca9^(11)] [1/^(1)] r_1 + [ \ud835\udca9^(12) ] [1/^(2)] r_2 ) + (1-c)s,\n    \n        r_2    = c ( [ \ud835\udca9^(21)] [1/^(1)] r_1 + [ \ud835\udca9^(22)] [1/^(2)] r_2 ),\n\nwhere \ud835\udca9^(ij) and (^(i) - 1) are Poisson random variables with means as described in Theorem\u00a0<ref>. Therefore we can further reduce the equations to\n\n    r_1    = c( a/2\u00b7(1 -  e^-(a+b)/2)/(a+b)/2\u00b7 r_1 + b/2\u00b7(1 -  e^-(a+b)/2)/(a+b)/2\u00b7 r_2 ) + (1-c)s,\n    \n        r_2    = c (b/2\u00b7(1 -  e^-(a+b)/2)/(a+b)/2\u00b7 r_1 + a/2\u00b7(1 -  e^-(a+b)/2)/(a+b)/2\u00b7 r_2 ),\n\nor in matrix form, and after substituting \u03bb = (1 -  e^-(a+b)/2), \n\n    [ 1 -  ca \u03bb/a+b      -cb\u03bb/a+b;      -cb\u03bb/a+b   1 - ca\u03bb/a+b ][ r_1; r_2 ]\n        = [ (1-c)s;      0 ].\n\nSolving (<ref>), we get \n\n    [  r_1 \n     r_2 ]    = 1/\u0394[ ( 1 - c\u03bb a/(a+b) ) s(1-c) \n     c\u03bb b s(1-c)/(a+b) ],\n\nwhere  \n    \u0394 = (1 - c \u03bb a/a+b)^2 - ( c \u03bb b/a+b)^2\n as required. Note that since c \u03bb < 1, we have \u0394 > 0, and so the above quantities are well defined. From here, the bound for r_2 is a straightforward calculation.\n\n    r_2    = c \u03bb b/a+b  (1-c)s/\u0394\u2264b/a+b (1-c)s/(1 - c \u03bb)(1 - c \u03bba-b/a+b)\u2264sb/(a+b)1/( 1 - a-b/a+b) = s/2.\n\nTo get the bound for r_1, we proceed as follows\n\n    r_1    = ( 1 - c \u03bb a/a+b) (1-c)s/\u0394\n       \u2265s(1-c)/1 - c \u03bb a/a+b = s (1-c)/a+b/a+b - c\u03bb a/a+b = s (1-c) /b/a+b + a/a+b( 1 - \u03bb + \u03bb (1-c) )\n       \u2265s (1-c) /b/a+b + a/a+b( e^-(a+b)/2 + (1-c) ) = s (1-c) /(1-c) + cb/a+b + a/a+b e^-(a+b)/2\n       \u2265s(1-c)/1-c + 2bc/a+b\u2265 s ( 1 - 2b/(1-c)(a+b)),\n\nwhere for the last inequality we used fact that e^-(a+b)/2a/(a+b) \u2264 b/4(a+b) \u2264 cb/(a+b) due to our assumptions on \u03bb and c, and 1 - x^2 \u2264 1 for all x \u2208. This completes the proof.\n\nThe next lemma provides a result for the variances. \n\n \nDefine v_i = (\u211b^(i)) for i=1,2, then, if we let \ud835\udc2f = (v_1, v_2)', and \ud835\udc2b^2 = (r_1^2, r_2^2)', then\n\n    \ud835\udc2f  = 1/2(1-g_1)(1-g_2)( K \ud835\udc2b^2 + (1-c)^2 s(1-s) \ud835\udc24),\n\nwhere g_1 = c^2 [1/(^(1))^2] (a-b)/2, g_2 = c^2 [1/(^(1))^2] (a+b)/2, \n\n    K  = [  g_1 + g_2 - 2g_1 g_2 ,     g_2  - g_1  \n     g_2  - g_1 ,     g_1 + g_2 - 2g_1 g_2],   and  \ud835\udc24 =  [  2- g_1-g_2  \n     g_2- g_1  ] .\n\nFurthermore,\n\n    v_1    \u22644c^2s^2/(a+b)(1-c^2) + 1-c/1+cs(1-s), \n    \n        v_2    \u22644c^2s^2/(a+b)(1-c^2)(1 + (1-c)(1-s)/2s(1+c)).\n\n    \n      \n      \n    \n\n\n    To calculate the variance of ^(1) and ^(2), we will rely on the law of total variances, i.e., for any two random variables X and Y, \n    (X) =  [(X|Y)] +  [ (X|Y)].\n Applying this to equation (<ref>), along with the fact that r_i < 1 for i = 1, 2, we get the following bound for (^(1)):\n    \n    (^(1)) =    c^2 ( r_1 \ud835\udca9^(11)[ 1/^(1)] + r_2 \ud835\udca9^(12)[ 1/^(2)] ) \n       + c^2 [ \ud835\udca9^(11)( ^(1)/^(1)) + \ud835\udca9^(12)( ^(2)/^(2)) ] + (1-c)^2 (^(1)) .\n\nNow use the observation that (\ud835\udca9^(11)) = [\ud835\udca9^(11)] = a/2, (\ud835\udca9^(12)) = [ \ud835\udca9^(12)] = b/2, \n\nand (\ud835\udcac^(1)) = s(1-s), to obtain that for v_i = (\u211b^(i)),\n\n    v_1    = c^2 ( r_1^2 \u00b7a/2\u00b7([1/^(1)])^2  + r_2^2 \u00b7b/2\u00b7([1/^(2)])^2  ) \n        + c^2 ( a/2( ^(1)/^(1)) + b/2( ^(2)/^(2))  ) + (1-c)^2 s(1-s) \n       = c^2 ( r_1^2 \u00b7a/2\u00b7([1/^(1)])^2  + r_2^2 \u00b7b/2\u00b7([1/^(2)])^2  ) + (1-c)^2 s(1-s) \n        + c^2 a/2(  (  1/^(1)  r_1 ) +  [ 1/(^(1))^2 (\u211b^(1))  ]  ) \n        + c^2 b/2(  (  1/^(2)  r_2 ) +  [ 1/(^(2))^2 (\u211b^(2))  ]  ) \n       = c^2 ( r_1^2 \u00b7a/2\u00b7([1/^(1)])^2  + r_2^2 \u00b7b/2\u00b7([1/^(2)])^2  ) + (1-c)^2 s(1-s) \n        + c^2 a/2( r_1^2 (  1/^(1)) +  [ 1/(^(1))^2 ] v_1  ) \n        + c^2 b/2( r_2^2 (  1/^(2)) +  [ 1/(^(2))^2 ] v_2  ) \n       = (1-c)^2 s(1-s)    + c^2 a/2[ 1/(^(1))^2 ] (r_1^2 + v_1)   + c^2 b/2[ 1/(^(2))^2 ] (r_2^2 + v_2 ).\n\nSimilarly, using \ud835\udcac^(2)\u2261 0 and \n\n    v_2    = c^2 ( r_1 \ud835\udca9^(21)[ 1/^(1)] + r_2 \ud835\udca9^(22)[ 1/^(2)] ) \n        + c^2 [ \ud835\udca9^(21)( ^(1)/^(1)) + \ud835\udca9^(22)( ^(2)/^(2)) ] + (1-c)^2 (^(2)) \n    \n    \n       = c^2 b/2[ 1/(^(1))^2 ] (r_1^2 + v_1)   + c^2 a/2[ 1/(^(2))^2 ] (r_2^2 + v_2 ).\n\nWriting the above in matrix notation we obtain for \ud835\udc2f = (v_1, v_2)' and \ud835\udc2b^2 = (r_1^2, r_2^2)',\n\n    \ud835\udc2f   = c^2 M (\ud835\udc2f + \ud835\udc2b^2) + \ud835\udc21,\n\nwhere (note that ^(1)d=^(2)), \n\n    M =  [ 1/(^(1))^2 ]/2[  a       b  \n    \n    b       a  ]     and    \ud835\udc21 = [  (1-c)^2 s(1-s) \n     0 ].\n\nMoreover, use the observation that\n\n    M = B A B,    A = [1/(^(1))^2]/2[  (a-b)     0 \n     0     (a+b) ],    B = 1/\u221a(2)[  -1     1 \n     1     1 ],\n\nso the maximum eigenvalue of M is [1/(^(1))^2] [^(1)-1]. Since for a Poisson random variable N with mean \u03bc we have that\n\n    [1/(N+1)^2] E[N] = \u2211_n=1^\u221ee^-\u03bc\u03bc^n/n \u00b7 n! = [ 1/N \u2228 1] - e^-\u03bc\u2264 1,\n\nthen the matrix I - c^2 M is invertible, and we obtain\n\n    \ud835\udc2f   = (I - c^2 M)^-1 ( c^2 M \ud835\udc2b^2 + \ud835\udc1b) = \u2211_k=0^\u221e c^2k M^k (c^2 M \ud835\udc2b^2 + \ud835\udc1b) \n    \n       = B [ c^2 A_11/1- c^2 A_11    0 \n     0    c^2 A_22/1- c^2 A_22] B \ud835\udc2b^2 + B [ 1/1-c^2 A_11    0 \n     0    1/1-c^2 A_22] B \ud835\udc1b.\n\nSetting g_i = c^2 A_ii for i = 1,2, and computing the product of matrices gives:\n\n    \ud835\udc2f  = 1/2(1-g_1)(1-g_2)( K \ud835\udc2b^2 + (1-c)^2 s(1-s) \ud835\udc24),\n\nfor K and \ud835\udc24 defined in the statement of the lemma.\n\nFurther, if we let \u0394_2 = 2(1-g_1)(1-g_2) and expand the above equation, we obtain \n\n    = 1/\u0394_2([ (g_1 + g_2 - 2g_1g_2)r_1^2 + (-g_1 + g_2) r_2^2; (-g_1 + g_2) r_1^2 + (g_1 + g_2 - 2g_1g_2)r_2^2 ] + (1-c)^2s(1-s)[ (2 - (g_1+g_2));    (-g_1 + g_2) ]).\n\nFrom equations (<ref>) and (<ref>) we also get that r_i \u2264 s for i=1,2, so we can reduce this to\n\n    \u22641/\u0394_2[ 2g_2(1-g_1)s^2 + (2 - (g_1+g_2))(1-c)^2s(1-s);    2g_2(1-g_1)s^2 + (-g_1 + g_2)(1-c)^2s(1-s) ].\n\nPlugging in \u0394_2 = 2(1-g_1)(1-g_2), and noting that g_2 \u2265 g_1, we get\n\n    v_1    \u2264g_2 s^2/1 - g_2 + 1/2( 1/1- g_2 + 1/1 - g_1)(1-c)^2s(1-s)\n       \u2264g_2 s^2/1 - g_2 + 1/1 - g_2(1-c)^2s(1-s),\n\nand\n\n    v_2    \u2264g_2 s^2/1-g_2 + 1/2( 1/1- g_2 - 1/1 - g_1)(1-c)^2s(1-s) \n       \u2264g_2 s^2/1-g_2 + g_2/2(1-g_1)(1-g_2)(1-c)^2s(1-s).\n\nFinally, using [1 / (^(1))^2 ] \u2264 8/(a+b)^2 and (<ref>), we have g_2 \u2264min{c^2,4c^2/(a+b)}, and so\n\n    v_1    \u22644c^2s^2/(a+b)(1-c^2) + 1-c/1+cs(1-s), \n    \n        v_2    \u22644c^2s^2/(a+b)(1-c^2)(1 + (1-c)(1-s)/2s(1+c)).\n\n\n\n\n\nWe are now ready to prove Theorem <ref>.\n For any z > 0, Chebyshev's inequality gives\n\n    (^(1)\u2264 r_1 - z)    = (^(1) - r_1 \u2264 -z) \n       \u2264v_1/z^2\n       \u22641/z^2(4c^2s^2/(a+b)(1-c^2) + 1-c/1+cs(1-s) ) .\n\nA similar application of Chebyshev's inequality for any w > 0 with ^(2) gives \n\n    (^(2) > s/2 + w )    \u2264(^(2) > r_2 + w) \u2264v_2/w^2\n       \u22641/w^24c^2s^2/(a+b)(1-c^2)(1 + (1-c)(1-s)/2s(1+c)) ,\n    \nwhere the first inequality follows from equation (<ref>).\nChoosing c \u2208 (1/2, 1 - 8b/(a+b)] results in r_1 \u2265 3s/4, so choosing z = w = s/8 and plugging into the bounds from equations (<ref>) and (<ref>) gives\n\n    (^(1) < 5s/8)    \u2264256 c^2/(a+b)(1-c^2) + 64(1-c)/1+c\u00b71-s/s, \n    (^(2) > 5s/8)    \u2264256 c^2/(a+b)(1-c^2)(1 + (1-c)(1-s)/2s(1+c)).\n\n \n\n\n\n\u00a7 RESULTS FROM SIMULATIONS\n\nWe illustrate the algorithm with some simulation experiments. First, we calculated the personalized PageRank scores for a 2-community dSBM with n=20000 vertices,  a=150, b=10, s = .2 and c = .85. The plot shows a clear separation of the PPR scores of the seeds, the rest of community 1 and the vertices in community 2.\n\n\nWe also investigated the role of the damping factor c and the best way to choose it. One natural way of doing so is to find the value of c that maximizes the difference between the mean PPR scores for the two communities. Note that r_1 - r_2 is strictly monotone in c, but if we let r\u0302_1 to be the mean of the non-seeded members of community 1, we see in Fig. <ref> that r\u0302_1 - r_2 is strictly convex with a maximum attained at c=.86. We have a description for the optimal c^* as follows.\n\n\nLet r\u0302_1 and r_2 be as described above. Then \n    c^* :=  argmax_c {r\u0302_1(c) - r_2(c) } = 1 - \u221a(1 - E)/E,\n where \n    E = a-b/a+b(1 -  e^-(a+b)/2).\n \n\n\n\nTo calculate r\u0302_\u03021\u0302, we consider the dSBM to have 3 communities, where we separate the seeds and the rest of the vertices in community 1. Then, Theorem\u00a0<ref> gives us a system of 3 distributional fixed-point equations. Using those, and calculations similar to the ones we did for Lemma\u00a0<ref>, we get\n\n    r\u0302_\u03021\u0302   = (1-c)s ( 1 - c \u03bb a/a+b/(1 - c\u03bb) (1 - c \u03bb(a-b/a+b) ) - 1 )\n    \n    \tr_2    = (1-c)s ( c \u03bb b/a+b/(1 - c \u03bb) (1 - c \u03bba-b/a+b)).\n\nNow substitute E = \u03bb (a-b)/(a+b) to obtain that \n\n    r\u0302_1 - r_2 = (1-c)s/1-cE,\n\nand use calculus to compute the optimal \n    c^* = 1 - \u221a(1-E)/E.\n\n\n\nNote that the value E is the second eigenvalue of the matrix on the left hand side of equation (<ref>). \n\n\n\n\n\n\u00a7 REMARKS AND CONCLUSIONS\n\nIn the sparse regime, we have proposed a cutoff level to identify vertices of community 1 based on their personalized PageRank scores and provided theoretical bounds on the probability of misclassifying a vertex. Our bounds are not tight, and simulations indicate that we might be able to use a lower threshold to further reduce the error (see also Remark <ref>). Another possible threshold option in the case of the symmetric SBM (\u03c0_1=\u03c0_2) is the median of PageRank scores. We also believe that the proposed method should work for asymmetric dSBMs with \u03c0_1 \u2260\u03c0_2, but the expressions for the mean and variance of PageRank become too complicated to compute clean bounds. Possible future work could \ninclude trying to show that the \u03c0_1-th quantile of the limiting PageRank distribution is a good threshold in the case \u03c0_1 \u2260\u03c0_2, or trying to find a threshold independent of \u03c0 so that we can recover communities even when we do not have information about their sizes.\nAnother interesting direction would be to investigate whether the inference can be strengthened if the seed set contains members from both communities and/or the connectivity structure of the subgraph spanned by the seeds is fully or partially known.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\n\nref_meanfield\nAvrachenkov, K., Kadavankandy, A., & Litvak, N. (2018). Mean field analysis of personalized pagerank with implications for local graph clustering. Journal of statistical physics, 173(3), 895-916.\n\nref_weakconvpr\nGaravaglia, A., van der Hofstad, R., & Litvak, N. (2020). Local weak convergence for PageRank. The Annals of Applied Probability, 30(1), 40-79.\n\nref_dcsbm\nGulikers, L., Lelarge, M., & Massouli\u00e9, L. (2018). An impossibility result for reconstruction in the degree-corrected stochastic block model. The Annals of Applied Probability, 28(5), 3002-3027.\n\nref_heatsbm\nKloumann, I. M., Ugander, J., & Kleinberg, J. (2017). Block models and personalized PageRank. Proceedings of the National Academy of Sciences, 114(1), 33-38.\n\n\n\n\nref_prinhomogeneous\nLee, J., & Olvera-Cravioto, M. (2020). PageRank on inhomogeneous random digraphs. Stochastic Processes and their Applications, 130(4), 2312-2348.\n\nref_mns_dense\nMossel, E., Neeman, J., & Sly, A. (2014). Consistency thresholds for binary symmetric block models. arXiv preprint arXiv:1407.1591, 3(5).\n\nref_mns_sparse\nMossel, E., Neeman, J., & Sly, A. (2015). Reconstruction and estimation in the planted partition model. Probability Theory and Related Fields, 162(3), 431-461.\n\nref_massoulie_sparse\nMassouli\u00e9, L. (2014, May). Community detection thresholds and the weak Ramanujan property. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing (pp. 694-703).\n\nref_partialrecovery\nChin, P., Rao, A., & Vu, V. (2015, June). Stochastic block model and community detection in sparse graphs: A spectral algorithm with optimal rate of recovery. In Conference on Learning Theory (pp. 391-423). PMLR.\n\nref_localclustering\nSpielman, D. A., & Teng, S. H. (2013). A local clustering algorithm for massive graphs and its application to nearly linear time graph partitioning. SIAM Journal on computing, 42(1), 1-26.\n\n\n\n\nref_prnibble\nAndersen, R., Chung, F., & Lang, K. (2007, December). Local partitioning for directed graphs using pagerank. In International Workshop on Algorithms and Models for the Web-Graph (pp. 166-178). Springer, Berlin, Heidelberg.\n\nref_censor\nDhara, S., Gaudio, J., Mossel, E., & Sandon, C. (2022). Spectral recovery of binary censored block models*. In Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA) (pp. 3389-3416). Society for Industrial and Applied Mathematics.\n\nref_distributedpr\nDas Sarma, A., Molla, A. R., Pandurangan, G., & Upfal, E. (2013, January). Fast distributed pagerank computation. In International Conference on Distributed Computing and Networking (pp. 11-26). Springer, Berlin, Heidelberg.\n\nref_olv22corrPR\nOlvera\u2013Cravioto, M. (2021). PageRank\u2019s behavior under degree correlations. The Annals of Applied Probability, 31(3), 1403-1442.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}