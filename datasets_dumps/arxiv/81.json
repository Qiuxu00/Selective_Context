{"entry_id": "http://arxiv.org/abs/2303.07259v3", "published": "20230313163526", "title": "A novel approach of empirical likelihood with massive data", "authors": ["Yang Liu", "Xia Chen", "Wei-min Yang"], "primary_category": "stat.ME", "categories": ["stat.ME"], "text": "\n \n \n  \nplain\n0.5mm\n\n\n\n\n\n\n\n\n\n  \n\n\n\n \tsemicolon,aysep=,yysep=,,notesep=:\n\n\n\n\n\n\n\n\n\n\n\nmythm1.5ex plus 1ex minus .2ex1.5ex plus 1ex minus .2ex\n    0.5em\nmythm\n\nlemLemma A.\nthmTheorem\nremRemark\ncrlCorollary\nexExample\n\nthmsection\ncrlsection\n\nremsection\nexsection\n\n\n\n\u03b5\n\u03c9\n\u03b3\n\n\u2202\n\n\u03c6\n\n\n    \u27f6 FH\u030b\u03b5dp\u2192\u030a T\n    \n    \u00a7 \n    [hang]  0.0ex\n    \n     \u00a7.\u00a7 \n    [hang]  0.0ex*\n    \n    \u00a7 \n    0pt10pt4pt\n    \n    \n      A novel approach of empirical likelihood with massive data 4mm\n    \n     Yang Liu, Xia Chen[Corresponding author. E-mail: xchen80@snnu.edu.cn], Wei-min Yang School of Mathematics and Statistics, Shaanxi Normal University, Xi'an 710119, China\n    \n    \n     4mm\n    \n    \n     Abstract  \n    Statistical analysis of large datasets is a challenge because of the limitation of computing devices' memory and excessive computation time.  Divide and Conquer (DC) algorithm is an effective solution path, but the DC algorithm still has limitations for statistical inference. Empirical likelihood is an important semiparametric and nonparametric statistical method for parameter estimation and statistical inference, and the estimating equation builds a bridge between empirical likelihood and traditional statistical methods, which makes empirical likelihood widely used in various traditional statistical models. In this paper, we propose a novel approach to address the challenges posed by empirical likelihood with massive data, which is called split sample mean empirical likelihood(SSMEL), our approach provides a unique perspective for sovling big data problem. We show that the SSMEL estimator has the same estimation efficiency as the empirical likelihood estimator with the full dataset, and maintains the important statistical property of Wilks' theorem, allowing our proposed approach to be used for statistical inference. The effectiveness of the proposed approach is illustrated using simulation studies and real data analysis.\n    \n    \n     Keywords  \n    Empirical likelihood; Massive data; Divide and Conquer; Parameter estimation; Statistical inference\n    \n    \n    \n    \u00a7 INTRODUCTION\n    \n    \n    With the development of science and technologies, the growth in the size of datasets is accelerating, large datasets are not unusual. The presence of massive data has presented new challenges for classical statistical methods, which can offer excellent theoretical properties for such data, yet are often difficult to implement in practice due to computation time and memory constraints. Additionally, when data is stored in a distributed manner, it may be infeasible to perform statistical analysis on the full dataset due to communication costs and privacy concerns. Thus, there is a growing need for a new statistical method to handle the challenges posed by massive data. In recent years, Divide and Conquer(DC) has been widely used to address the problems caused by massive data. The DC algorithms can be broadly classified into two categories, one is the one-shot approach<cit.>.\n    Its basic idea is that one can randomly partition the full dataset into multiple subsets, perform the parallel computation of the relevant statistics on each, and eventually produce the estimator via a suitable aggregation function. <cit.> studied penalized regression models utilizing the one-shot approach; <cit.> developed the one-shot approach for estimation and hypothesis testing in high-dimensional sparse models; <cit.> discussed the statistical inference of symmetric statistics under the one-shot approach; <cit.> examined the properties of the one-shot estimators of empirical likelihood and exponentially tilted empirical likelihood; <cit.> proposed a novel statistical inference for the one-shot estimator by empirical likelihood.\n    \n    The one-shot approach has the advantage of easy implementation and low communication cost, however, it requires sufficiently large subsets to ensure the accuracy of the final estimator. In practice, it will limit to call of more computational resources to reduce the computation time. While the first-order error of the one-shot estimator is equivalent to that of the full dataset, the second-order error of the nonlinear statistic is not negligible<cit.>. To address this challenge, the iterative approach<cit.> has been proposed. It aims to generate a final estimator by the iterative comunication of the gradient information of subsets, this approach requires more communication cost compared to the one-shot approach. <cit.> achieved the one-step estimator through the Newton-Raphson algorithm to update the one-shot estimator. <cit.> proposed a general framework called Communication-efficient Surrogate Likelihood (CSL) that aggregates gradient information from subsets to construct a CSL function, which is then used to obtain the final estimator through multiple iterative and optimization procedures. The framework also establishes the theoretical properties of CSL for statistical inference. <cit.> incorporated a strictly convex quadratic term that can be adaptively adjusted at each iteration step. This addition helps the CSL converge faster, resulting in improved performance. The DC algorithms can effectively handle the challenges of parameter estimation under large datasets, but the statistical inference is still a complex task. General statistical inference methods construct test statistics based on the asymptotic distribution of the estimator, but the covariance matrix is difficult to estimate under the full dataset. For more details on the distributed statistics can refer to <cit.>.\n    \n    Empirical likelihood is a significant nonparametric and semiparametric statistical method, it inherits Wilks' theorem of parametric likelihood<cit.>. Therefore, it produces confidence regions with data-driven shapes and constructs test statistics without estimating the covariance matrix. <cit.> demonstrated that empirical likelihood resembles parametric likelihood with Bartlett correction. Due to these advantageous properties, and empirical likelihood can easily incorporate side information, so it has gained significant attention and has been extensively investigated and utilized, e.g. regression models<cit.>, estimating euqations<cit.>, partially linear models<cit.>, bayesian settings<cit.>, quantile regression models<cit.>, U-statistics<cit.>, time series models<cit.>, high-dimensional statistical inference<cit.>.\n    \n    \n    Empirical likelihood is computationally intensive when the size of the dataset is large, which can limit the application of empirical likelihood to the large dataset. Because empirical likelihood is well adapted to traditional statistical models, it is necessary to address the challenges of empirical likelihood with massive data. Recently, <cit.> and <cit.> proposed split sample empirical likelihood (SSEL) and distributed empirical likelihood (DEL), respectively, to solve this problem. <cit.> constructed the empirical likelihood function with respect to each subset and defined the SSEL estimator as the maximizer of the product of these empirical likelihood functions. <cit.> obtained estimators for each subset and then averaged these estimators across all subsets to generate the DEL estimator. Both methods utilize parallel computing to tackle the challenges of massive data on empirical likelihood. Modern parallel computing structures have the potential to significantly reduce computation time. However, for large values of K (n=O(\u221a(K))), the accuracy of estimation, particularly for non-linear statistics, cannot be ensured. Consequently, there are stringent limitations on the value of K required to obtain reliable estimators, and each parallel pool remains computationally expensive. On the other hand, the DEL is failing to meet Wilks' theorem, which eliminates the benefits of empirical likelihood and necessitates the exploration of alternative statistical inference methods.\n    \n    To address these issues, we propose a novel approach, which called split sample mean empirical likelihood (SSMEL). Under mild regularity conditions, we show that the SSMEL estimator retains the same asymptotic efficiency as that of the full dataset. Additionally, the SSMEL preserves the important property of Wilks' theorem, making it suitable for statistical inference. Different from other methods, the SSMEL does not require parallel computing and can be implemented efficiently with limited computational resources, making it more widely practical applicable. For distributed dataset, we designed the corresponding distributed algorithm for the SSMEL based on the algorithm in <cit.> which is a variant of the iterative approach. Simulations and real data analysis further demonstrate the efficiency of our proposed method.\n    \n    The remainder of this paper is organized as follows. In Section 2, we briefly review empirical likelihood and present the methodology of SSMEL. Section 3 investigates the theoretical properties of SSMEL. Section 4 designs a new algorithm applicable to the SSMEL. Sections 5 and 6 examine the performance of the proposed approach on simulated and real data analysis. Section 7 concludes the paper and discusses future work.\n    \n    \n    \n    \n    \u00a7 METHODOLOGY\n    \n    Suppose that \ud835\udcb3 = {x_1,\u22ef,x_n} are d-variate independent and identically distributed samples with common distribution function F. Let \u03b8\u2208\u211d^p be a vector of the unknown parameter of interest, and \u03b8_0 is the true value. For the sake of completeness, we first briefly review empirical likelihood.\n    \n    \n    \n     \u00a7.\u00a7 Empirical likelihood\n    \n    \n    Assume that true value \u03b8_0 satisfies constraints in the form of the r\u2265 p unbias estimating equation, i.e.\n    \n        \ud835\udd3cg(X,\u03b8_0)=0,\n    \n    where g(X,\u03b8_0)=(g_1(X,\u03b8_0),\u22ef,g_r(X,\u03b8_0)). Then, the empirical likelihood ratio function evaluated at \u03b8 be defined as\n    \n    \n    R(\u03b8)=sup{\u220f_i=1^nnp_i:p_i\u22650,\u2211_i=1^np_i=1,\u2211_i=1^np_ig(x_i,\u03b8)=0}\n    By the Lagrange multiplier method, we have\n    \n        p_i=1/n\u00b71/1+\u03bb^Tg(x_i,\u03b8) where \u03bb(\u03b8) solves 1/n\u2211_i=1^ng(x_i,\u03b8)/1+\u03bb^Tg(x_i,\u03b8)=0.\n    \n    Thus, the empirical log-likelihood ratio defined as\n    \n    \n    l(\u03b8)=\u2211_i=1^nlog[1+\u03bb^Tg(x_i,\u03b8)]\n    \n    The maximum empirical likelihhod estimate \u03b8\u0302_EL is calculated by\n    \u03b8\u0302_EL=argmin_\u03b8\u2208\u0398max_\u03bb\u2208\u039b\u0302_n(\u03b8)\u2211_i=1^nlog[1+\u03bb^Tg(x_i,\u03b8)]\n    \n    where \u039b\u0302_n(\u03b8)={\u03bb\u2208\u211d^r:\u03bb^Tg(x_i,\u03b8)\u2208\ud835\udcb1,i=1,\u22ef,n} for \u03b8\u2208\u0398 and \ud835\udcb1 is an open interval containing zero, and \u0398 is the convex hull of {g(x_i,\u03b8),i=1\u22ef,n)}. Under mild regularity conditions, <cit.> showed  that as n\u2192\u221e,\n    \n        \u221a(n)(\u03b8\u0302_EL-\u03b8_0)d\u27f6N(0,\u03a3),\n    \n    where \u03a3=[\ud835\udd3c(\u2202 g(X,\u03b8_0)/\u2202\u03b8^T)^T(\ud835\udd3cg(X,\u03b8_0)g^T(X,\u03b8_0))^-1\ud835\udd3c(\u2202 g(X,\u03b8_0)/\u2202\u03b8^T)]^-1. Moreover, if Var(g(X,\u03b8_0)) is finite and the rank p>0, then Wilks theorem is holded, i.e.\n    \n        2l(\u03b8_0)-2l(\u03b8\u0302_EL)d\u27f6\u03c7_p^2, as  n\u2192\u221e.\n    \n    \n     \u00a7.\u00a7 Split sample mean empirical likelihood\n    \n    \n    Empirical likelihood cannot generally be written in a closed form, so a numerical optimization algorithm is required for the solution, resulting in computational obstacles from massive data and distributed datasets. To address these challenges, we introduce our proposed SSMEL. Assume the size of full dataset n is very large, and randomly partition the full dataset \ud835\udcb3={x_1,\u22ef,x_n} into K subsets of size m=n/K. We denote S_k={x_i^(k),i=1,\u22ef,m} as kth subset, which x_i^(k) means ith sample in kth subset. Obviously, \u22c3_k=1^KS_k=\ud835\udcb3 and S_k\u22c2 S_t=\u2205, for any k\u2260 t.\n    \n    For each subset S_k,k=1\u22ef,K, we consider the following steps:\n    \n      *  By inputting the samples x_i^(k) from subset S_k into the estimating equation, we can obtain the sequence of estimating equations g(x_i^(k),\u03b8),i=1,\u22ef,m.\n    \n    \n      *  Taking the mean of estimating euqation sequence {g(x_1^(k),\u03b8),\u22ef,g(x_m^(k),\u03b8)}, i.e.\n    \n        g\u0305^(k)(\u03b8)=1/m\u2211_i=1^mg(x^(k)_i,\u03b8).\n     \n    \n    With the aforementioned steps, we can derive mean estimating equations g\u0305^(k)(\u03b8),k=1,\u22ef,K from K subsets. It can be easily seen that the mean estimating equation is still the estimating equation, i.e.\n    \n        \ud835\udd3cg\u0305(\u03b8_0)=\ud835\udd3c[1/m\u2211_i=1^mg(X_i,\u03b8_0)]=1/m\u2211_i=1^m\ud835\udd3cg(X_i,\u03b8_0)=0,\n    \n    \n    Thus, we can construct the split sample mean empirical likelihood(SSMEL) ratio function using the mean estimating equations,\n    \n    R_S(\u03b8)=sup{\u220f_k=1^KKp_k:p_k\u22650,\u2211_k=1^Kp_k=1,\u2211_k=1^Kp_kg\u0305^(k)(\u03b8)=0}\n    and the split sample mean empirical log-likelihood ratio function defined as\n    \n    l_S(\u03b8)=\u2211_k=1^Klog(1+\u03bb^Tg\u0305^(k)(\u03b8))\n    \n    The core approach of SSMEL is to compress the sample information and convert the large dataset into a small dataset. Therefore, the full dataset empirical likelihood is a special case of SSMEL, when K=n.\n    \n    Similar to Equation.(<ref>), the maximum split sample mean empirical likelihood estimator is\n    \u03b8\u0302_S=argmin_\u03b8\u2208\u0398\u0305max_\u03bb\u2208\u039b\u0302_K(\u03b8)\u2211_k=1^Klog(1+\u03bb^Tg\u0305^(k)(\u03b8))\n    \n    where \u039b\u0302_K(\u03b8)={\u03bb:\u03bb^Tg\u0305^(k)(\u03b8)\u2208\ud835\udcb1,k=1,\u22ef,K} for \u03b8\u2208\u0398\u0305 and \ud835\udcb1 is an open interval containing zero, and \u0398\u0305 is the convex hull of {g\u0305^(k)(\u03b8),k=1,\u22ef,K}. To solve Equation. (<ref>), a prerequisite is that \u0398\u0305 have the zero vector as an interior point. Lemma 11.1 in <cit.> states that if Var(g(X,\u03b8)) is finite and the rank p>0, then the zero vector must be contained in \u0398. Obviously, because of Var(g\u0305(\u03b8))=m^-1Var(g(X,\u03b8)), if Var(g(X,\u03b8)) satisfies this condition, then Var(g\u0305(\u03b8)) also satisfies it. Figure\u00a0<ref> shows the parameter space for partly subsets and the parameter space for SSMEL under the same segmentation. It can be seen from Figure\u00a0<ref>(a) that when the subset size m is small, the convex hull consisting of g(x_i^(k),\u03b8),x_i^(k)\u2208 S_k does not contain zero vectors with a higher probability, thus leading to poor estimation of SSEL and DEL when the value of K is taken to be large. In contrast, Figure\u00a0<ref>(b) shows that the SSMEL has a much smaller parameter space and always contains zero vectors.\n    \n    \n    \u00a7 ASYPTOTIC PROPERTIES\n    \n    \n    In this section, we establish the asymptotic properties of SSMEL. For the empirical likelihood, the critical aspect is to control the tail probabilities behavior of the estimating equation, i.e., to ensure \u2016 n^-1\u2211_i=1^ng(x_i,\u03b8)\u2016=O_p(n^-1/2). It is worth noting that K^-1\u2211_k=1^Kg\u0305^(k)(\u03b8)=K^-1\u2211_k=1^Km^-1\u2211_i=1^mg(x_i^(k),\u03b8)=n^-1\u2211_i=1^ng(x_i,\u03b8). Therefore, the SSMEL and empirical likelihood have the same assumptions. The following assumptions are made.\n    \n      A.1\u03b8_0\u2208 int(\u0398\u0305) is unique solution to \ud835\udd3cg(X,\u03b8)=0, where \u0398\u0305 is a compact set and int(\u0398\u0305) denotes the interior of \u0398\u0305.\n    \n      A.2g(x_i,\u03b8) is continuous with respect to \u03b8 at each \u03b8\u2208\u0398\u0305 with probability 1 and is continuously differentiable with respect to \u03b8 in a neighbourhood of \ud835\udca9 of \u03b8_0.\n    \n      A.3\ud835\udd3c[sup_\u03b8\u2208\u0398\u2016 g(X,\u03b8)\u2016^\u03b1]<\u221e for some \u03b1>2, where \u2016\u00b7\u2016 is the Euclidean norm for vector and the Frobenius norm for matrix.\n    \n      A.4\u03a9:=\ud835\udd3c[g(X,\u03b8_0)g(X,\u03b8_0)^T] is nonsingular.\n    \n      A.5\ud835\udd3c[sup_\u03b8\u2208\ud835\udca9\u2016\u2202 g(X,\u03b8)/\u2202\u03b8^T\u2016]<\u221e, denotes \ud835\udd3c(\u2202 g(X,\u03b8)/\u2202\u03b8^T)=G, rank(G)=p.\n    \n    A.1-A.5 guarantees the existence and asymptotic normality of \u03b8\u0302_S, further ensure Wilks' theorem holds. As discussed in the previous section, the assumption in A.1 of \u0398\u0305 can be relaxed to \u0398. These assumptions are similar to those found in <cit.>, which are the fundamental assumptions of empirical likelihood, and no additional assumptions are introduced in this paper.\n    \n    \n    Under assumptions A.1-A.5, we have\n    \n        \u221a(n)(\u03b8\u0302_S-\u03b8_0)d\u27f6N(0,\u03a3), as n\u2192\u221e\n    \n    where \u03a3=(G^T\u03a9 G)^-1.\n    \n    Under the assumption of Theorem <ref>, we have\n    \n        \ud835\udd3c[\u2016\u03b8\u0302_S-\u03b8_0\u2016^2]\u2264tr(\u03a3)/n+o(n^-1)\n    \n    where tr(\u00b7) represents the trace of the matrix.\n    \n    \n    Theorem <ref> shows that the asymptotic distribution of \u03b8\u0302_S is the same as for \u03b8\u0302_EL, and if the estimating equation g is the score function of the true parameter likelihood function, then the asymptotic distribution of \u03b8\u0302_S is same as maximum likelihood estimator. Corollary <ref> shows that the mean squared error(MSE) upper bound for the SSMEL estimator, which is the same as the full dataset empirical likelihood, therefore they have the same estimation efficiency. Finally, we give the asymptotic behavior of SSMEL test statistic. Theorem <ref> summarizes the general conclusions, while Corollary <ref> provides the asymptotic distribution in the presence of nuisance parameters.\n    \n    \n    The split sample mean empirical likelihood ratio test statistic for H_0:\u03b8=\u03b8_0 is\n    \n        \ud835\udcb2(\u03b8_0)=2[l_S(\u03b8_0)-l_S(\u03b8\u0302_S)]\n    \n    Under assumptions of Theorem <ref>, \ud835\udcb2(\u03b8_0)d\u27f6\u03c7^2_p as  n\u2192\u221e, when H_0 is ture.\n    \n    Let \u03b8^T=(\u03d5,\u03b3)^T, and \u03d5 is q\u00d71 vector, \u03b3 is (p-q)\u00d71 nuisance parameters. The profile split sample mean empirical likelihood ratio test statistic for H_0:\u03d5=\u03d5_0 is\n    \n        \ud835\udcb2(\u03d5_0)=2[l_S(\u03d5_0,\u03b3\u0302(\u03d5_0))-l_S(\u03d5\u0302_S,\u03b3\u0302_S)]\n    \n    where \u03b3\u0302(\u03d5_0) minimizes l_S(\u03d5_0,\u03b3) with respect to \u03b3. Under H_0, \ud835\udcb2(\u03d5_0)d\u27f6\u03c7^2_q as n\u2192\u221e.\n    \n    The theoretical results of SSMEL hold provided K\u2192\u221e as n\u2192\u221e , this means that m is a fixed positive integer.  Related to the choice of K, there are some practical considerations. As we formally use K samples, the computation time grows as K increases, therefore it is necessary to ensure that K is not excessively large. On the other hand, empirical likelihood can only be applied when K is larger than p (the parameter dimension). In the general setting, from our own experiences, we recommend 100 to be a suitable value for K, this is further demonstrated in the subsequent simulation and real data analysis.\n    \n    \n    \u00a7 ALGORITHM FOR DISTRIBUTED DATASET\n    \n    The algorithms for solving empirical likelihood can be applied to the SSMEL, making the implementation of SSMEL estimation feasible on a single computing device. To extend SSMEL to the distributed dataset, we generalize the two-layer coordinate descent algorithm in <cit.>. The algorithm is briefly reviewed in the context of SSMEL.\n    \n    First, we define\n    \n    f(\u03bb;\u03b8) = 1/K\u2211_k=1^Klog_*{1+\u03bb^Tg\u0305^(k)(\u03b8)}\n    f(\u03b8) = max_\u03bb\u2208\u039b\u0302_K(\u03b8)f(\u03bb;\u03b8) \n    \n    where log_*(x) is twice differentiable pseudo-logarithm function with bounded support adopted from <cit.>:\n    \n        log_*(x) = {[                     log(x)                     if x\u2265\u03b5; log(\u03b5)-1.5+2x/\u03f5-x^2/(2\u03b5^2)                     if x\u2264\u03b5 ].\n    \n    where \u03b5 is chosen as 1/K in this paper. The SSMEL estimaotr \u03b8\u0302_S is calculated by minimizing the following objective function:\n    \u03b8\u0302_S = argmin_\u03b8\u2208\u0398\u0305f(\u03b8)\n    \n    \n    We apply the two-layer coordinate decent algorithm in <cit.> to solve the problem. The inner layer of the algorithm is to find \u03bb by maximizing f(\u03b8) for a fixed \u03b8. The outer layer of the algorithm is to search for the optimal \u03b8\u0302_S, and both layers can be solved using coordinate descent.\n    \n    The inner-layer involves maximizing f(\u03bb,\u03b8) as defined in Equation (<ref>) for a fixed \u03b8. Assuming the initial value of \u03bb is \u03bb^(0), we fix the other coordinates and calculate the value of \u03bb_j, where j=1,2,\u22ef,r in the (M+1)th iteration, the \n    jth component of \u03bb is given by\n    \n    \u03bb\u0302_j^(M+1)=\u03bb\u0302_j^(M)-\u2211_k=1^Klog_*^'(t_k^(M))\u00b7g\u0305_j^(k)(\u03b8)/\u2211_k=1^Klog_*^\u201d(t_k^(M))\u00b7{g\u0305_j^(k)(\u03b8)}^2\n    where t_k^(M)=1+g\u0305^(k)(\u03b8)^T\u03bb\u0302^(M), \u03bb\u0302^(M)=(\u03bb\u0302_1^(M),\u22ef,\u03bb\u0302_r^(M))^T. The procedure cycles through all the r components of \u03bb and is repeated until convergence. It is important to ensure that the objective function is optimized at each step. If not, continue halving the step size until the objective function is driven in the right direction. The procedure in Equation (<ref>) can be viewed as an optimization of a univariate sequence.\n    \n    The outer layer can also be solved using a coordinate descent algorithm. At a given \u03bb, the algorithm updates \u03b8_t, t=1,\u22ef,p by minimizing f(\u03b8) defined in Equation (<ref>) with respect to \u03b8_t with other \u03b8_l is fixed. Assuming the initial value of \u03b8 is \u03b8\u0302^(0), the (M+1)th Newton update for \u03b8_t is given by\n    \n    \u03b8\u0302_t^(M+1)=\u03b8\u0302_t^(M)-\u2211_k=1^Klog_*^'(s_k^(M))[\u03bb^T(\u2202g\u0305^(k)(\u03b8\u0302^(M))/\u2202\u03b8_t)]/\u2211_k=1^K{log_*^\u201d(s_k^(M))[\u03bb^T(\u2202g\u0305^(k)(\u03b8\u0302^(M))/\u2202\u03b8_t)]^2+log_*^'(s_k^(M))[\u03bb^T(\u2202^2g\u0305^(k)(\u03b8\u0302^(M))/\u2202\u03b8_t^2)]}\n    where s_k^(M)=1+\u03bb^Tg\u0305^(k)(\u03b8\u0302^(M)), \u03b8\u0302^(M)=(\u03b8\u0302^(M)_1,\u22ef,\u03b8\u0302^(M)_p)^T. The SSMEL can be implemented following the pseudo-code in <cit.> on a single computing device. For the distributed dataset, we give the pseudo-code in Algorithm <ref>.\n    \n    \n    It can be seen that the SSMEL only requires optimizing a single objective function, while other distributed methods require processing multiple optimization procedures in parallel. Thus, our approach is suitable for general computing devices.\n    \n    Algorithm <ref> is a simple implementation of SSMEL applied to distributed data that differs from the traditional iterative approach. Traditional iterative approaches generally require multiple optimization procedures, while this algorithm accomplishes a single optimization procedure through interaction between multiple computing devices. It is worth noting that the efficiency of SSMEL is not limited by the number of devices. The data stored on each machine can still be randomly partitioned and compressed and then transferred between machines.\n    \n    \n    \u00a7 SIMULATIONS\n    \n    In this section, we demonstrate the behavior of the proposed approach through some simulations. The first example evaluates the behavior of SSMEL in reducing computation time for massive data and compares the estimation accuracy of SSMEL, DEL, and SSEL under different settings. The second example shows the estimation accuracy of SSMEL, DEL, and SSEL for increasing the dimensionality of the growth parameters under three different partitioning conditions. The third example provides the results of the hypothesis testing of SSMEL, DEL, and SSEL under different partition numbers K. The centralized empirical likelihood(CEL) in these examples represents the empirical likelihood under the full dataset. All simulations were implemented in R.\n    \n    Example 1 We generate data X_1,\u22ef,X_n from a normal distribution N(\u03bc,\u03c3^2), where the unknown parameters \u03bc and \u03c3 are randomly generated from the uniform distributions Unif(-2,2) and Unif(0.5,2), respectively. The random variable X of the normal distribution N(\u03bc,\u03c3^2) satisfies the following moment conditions:\n     \n        \ud835\udd3c[g(X,\u03b8_0)]=\ud835\udd3c[           \u03bc - X;     \u03c3^2-(X-\u03bc)^2; X^3-\u03bc(\u03bc^2+3\u03c3^2) ]\n        =0.\n    Case 1 To assess the efficacy of the proposed approach in terms of reducing computation time for massive data, we choose the full dataset size at n=200000 and vary K=[10,50,100,500,1000, 5000], with K=200000 being equivalent to the CEL. This procedure with 500 replications, the mean square error (MSE) for \u03bc and \u03c3, and total computation time (TCT) were recorded in Table <ref>.\n    \n    \n    \n    \n    Table <ref> shows that, with the proper K, our proposed approach can effectively reduce the computation time and achieve the estimation accuracy under the full dataset. This confirms the conclusion drawn in Corollary <ref>. A smaller value of K is not recommended, as this would over-compress the sample information, causing poor estimation accuracy and non-optimal computational efficiency.\n    \n    Case 2 In this case, the performance of CEL, SSMEL, DEL, and SSEL is compared by evaluating their accuracy in four different settings with 1000 repetitions. To further explore the sensitivity of each method with respect to K and m, we did not consider setting n extremely large, and the subsequent simulation settings were similarly based on this consideration. The logarithmic mean squared error (log-MSE) is shown in Figure <ref>.\n    \n    \n      (a) Fixing the subset size m, the split size K increases with the size of the full dataset n. We consider m=[50,100] and vary n=[1000,2000,3000,4000,5000].\n    \n      (b) Fixing the split size K, the subset size m increases with the size of full dataset n. We consider K=[10,50] and vary n=[1000,2000,3000,4000,5000].\n    \n      (c) Fixing the size of full dataset at n=12000 and vary K=[10,20,40,80,100,120].\n    \n      (d)  Fixing the size of full dataset at n=12000 and vary m=[100,200,400,800,1000,1200].\n    \n    \n    From the first and second rows of Figure <ref>, we can see that for fixed K and m, the log-MSE of SSMEL on \u03bc and \u03c3 are closer to the behavior of CEL when the total sample increases. The SSEL and DEL perform poorly for the nonlinear statistic \u03c3. With a fixed n, it can be seen from the third row of Figure <ref> that when K increases to 100, the estimation of SSMEL achieves the estimation efficiency under the full dataset. When m is large i.e. K is small, the log-MSE of SSMEL tends to increase due to excessive compression of information. Overall, the SSMEL performs more robustly compared to other methods.\n    \n    Case 3 In this case, we compare the performance of each method in terms of the reduction of computation time. Due to limited knowledge regarding parallel optimization of the likelihood components of SSEL, direct optimization of SSEL functions did not result in a significant reduction in computation time. We have restricted our comparison to the DEL and SSMEL. We consider n=12000 and 500 repetitions, and the results are presented in Table <ref>.\n    \n    \n    \n    From Table <ref>, we clearly see that the SSMEL reduces the computation time more efficiently than the DEL. It is worth highlighting that the SSMEL exhibits superior computational efficiency without relying on parallel computing devices. In contrast, the DEL may leverage parallel computing to decompose computation time, but its estimation accuracy may deteriorate as K increases.\n    \n    Example 2 In this example, we consider estimating the coefficients of a linear regression model with a varying number of parameters,\n    \n    Y_i =Z_i^T\u03b2+\u03b5_i,   i=1,\u22ef,n\n    \n    where \u03b2=(\u03b2_0,\u03b2_1,\u22ef,\u03b2_p)^T, \u03b5_i and Z_i=(1,X_i1,\u22ef,X_ip)^T are independent, \u03b5i.i.d\u223cN(0,1), and X_i=(1,X_i1,\u22ef,X_ip)^Ti.i.d\u223cN(0,\u03a3_p) with\n    \n        \u03a3_p=\n        [ 1 \u03c1 \u22ef \u03c1; \u03c1 1 \u22ef \u03c1; \u22ee \u22ee \u22f1 \u22ee; \u03c1 \u03c1 \u22ef 1;   ]_p\u00d7 p\n    \n    we refer to the setting in <cit.> that p=[4,8,18], \u03b2_0=(1,5,4,3,2,1_p-4^T)^T with 1_p-4^T=(1,\u22ef,1)^T for p>4, and \u03b2_0=(1,5,4,3,2) for p=4. \u03c1=[0,0.2,0.5,0.8] in \u03a3_p. We fix the full dataset size at n=20000 and K=[10,50,100]. We compare the performance of SSMEL, DEL, and SSEL estimators in terms of empirical MSE, i.e. the mean of \u2016\u03b2\u0302 - \u03b2_0 \u2016^2. Table <ref> summarizes these results base on 500 replications.\n    \n    \n    \n    \n    From Table <ref>, we can see that the MSE corresponding to SSMEL is very close to that of the CEL in most cases. The SSMEL exhibits poor performance when K is close to p due to the fact that the approach formally uses only K samples. When K is significantly greater than p, the SSMEL is computationally simple and the estimation is robust compared to other methods in various cases.\n    \n    \n    Due to the fact that a plane can only be formed by three points in two dimensions, the parameter space of SSMEL is the convex hull of {g\u0305^(k)(\u03b8), k=1,\u22ef, K}. Therefore, when K is close to p, the convex hull formed by K points is unable to effectively encompass p-dimensional vectors, resulting in poor estimation performance.\n    Example 3 In this example, we examine the performance of the hypothesis test for the SSMEL. We generate data (X,Y) from the bivariate normal distribution N(\u03bc_1,\u03c3_1^2,\u03bc_2,\u03c3_2^2,\u03c1). We use a dataset size of 10000 and 500 replications with parameter values \u03bc_1=\u03bc_2=0, \u03c3_1=\u03c3_2=1, and \u03c1=0.5. The random vector (X,Y) of the N(\u03bc_1,\u03c3_1^2,\u03bc_2,\u03c3_2^2,\u03c1) satisfies the following moment conditions:\n     \n        \ud835\udd3c[g(X,Y;\u03b8_0)]=\ud835\udd3c[                \u03bc_1 - X;                \u03bc_2 - Y;        \u03c3_1^2-(X-\u03bc_1)^2;        \u03c3_2^2-(Y-\u03bc_2)^2; (X-\u03bc_1)(Y-\u03bc_2)-\u03c1\u03c3_1\u03c3_2 ]\n    \n    The hypothesis testing of DEL is challenging due to the failure to meet Wilks' theorem. To address this problem, <cit.> recently presented a statistical inference method for the one-shot estimator of average aggregation. The primary objective is to test the mean of the estimators of each subset using empirical likelihood. We applied the method to the DEL for hypothesis testing and compared it with the SSMEL, SSEL, and CEL. We consider the following null hypothesis:\n    \n      1.H_0:\u03b8 = (\u03bc_1,\u03bc_2,\u03c3_1,\u03c3_2,\u03c1) = (0,0,1,1,0.5)\n      2.H_0:\u03c8 = (\u03bc_1,\u03c3_1) = (0,1)\n      3.H_0:\u03c1=0.5\n    \n    We choose K=[5,10,20,40,80,100], the false rejection rate at nominal levels \u03b1=0.05 for each method are recorded in Table\u00a0<ref> and  the empirical frequencies of \u03c1\u2209{\u03c1:\ud835\udcb2(\u03c1)=2(l_S(\u03c1)-l_S(\u03c1\u0302))\u2264\u03c7^2_1,0.95} for a sequence of \u03c1 values in Table\u00a0<ref>. From Table <ref>, we can see that the false rejection rate when using the SSMEL is also affected by the choice of K. The false rejection rate of SSMEL closes the nominal level and the results of the full dataset as K increases, and the change becomes subtle when K grows to a certain level (K=100). In contrast to the SSMEL, the SSEL is negatively impacted by larger values of K, as the false rejection rate deviates further away from the nominal level. The DEL is a more complex method compared to others, as it involves a trade-off between the one-shot estimation accuracy and the error of the asymptotic \u03c7^2 distribution of the empirical log-likelihood function. From Table\u00a0<ref> we can see that the test power of SSMEL has a consistent performance with the empirical likelihood under the full datasets. Therefore, the SSMEL is an effective alternative to the CEL to address the challenges of statistical inference with massive data.\n    \n    \n    \n    \u00a7 REAL DATA ANALYSIS\n    \n    \n     \u00a7.\u00a7 Protein dataset\n    \n    Physicochemical properties of protein tertiary structure dataset[<https://archive.ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure>] are taken from CASP 5-9. There are 45730 decoys and sizes varying from 0 to 21 armstrong, which aims to predict the size of the residue(RSMD). The explanatory variables are as follows: X_1, total surface area; X_2, non polar exposed area; X_3, fractional area of exposed non polar residue; X_4, fractional area of exposed non polar part of residue; X_5, molecular mass weighted exposed area; X_6, average deviation from standard exposed area of residue; X_7, euclidian distance; X_8, secondary structure penalty; X_9, spacial distribution constraints (N,K Value). We randomly partition the dataset into a training set and a test set according to 7:3, where the training set has 32,010 samples and the test set has 13,720 samples. In this example, we use the training set to fit a linear regression model and the test set to evaluate the prediction performance of SSMEL under the K=[50,100,500]. We recorded the mean squared prediction error (MSPE) and computation time(CT) of CEL, DEL, and SSMEL under the different number of splits in Table <ref>. From Table <ref> we can see that the MSPE of SSMEL is very close to the CEL when K is close to 100, and the computation time is significantly reduced. \n    \n    \n    \n     \u00a7.\u00a7 the United Stated airline dataset\n    \n    In this section, we use the SSMEL to analyze the United States airline dataset, which is publicly available on the American Statistical Association (ASA) website[<http://stat-computing.org/dataexpo/2009>]. This airline dataset is very large, with nearly 120 million records. Each record contains information on every commercial flight detail in the United States from October 1987 to April 2008. The dataset is partitioned into 22 files based on year, each file containing 13 continuous variables and 16 categorical variables. However, due to the massive size of the dataset, a typical personal computer may not have sufficient memory to load the full dataset for statistical analysis. In this paper, we concentrate on the analysis of the 13 continuous variables, and only 5 have missing rates less 10%: ActualElapsedTime(actual elapsed time), CRSElapsedTime(scheduled elapsed time), Distance, DepDelay(departure) and ArrDelay(arrival delay). Therefore, we study these 5 variables. For more detailed information on the variables, refer to the ASA official website.\n    \n    Due to these variables being so heavy-tailed that the existence of finite moments becomes questionable. Similar to <cit.>, we perform a signed-log-transformation: log|x|\u00b7 sign(x) on these variables. For each transformed variable, we examine the mean, standard deviation, skewness, and kurtosis, denoted as \u03bc, \u03c3, \u03be, and \u03ba. These statistics satisfy the following moment conditions:\n     \n        \ud835\udd3c[g(X;\u03b8_0)]=\ud835\udd3c[           \u03bc - X;     \u03c3^2-(X-\u03bc)^2; \u03be - (X-\u03bc)^3/\u03c3^3;   \u03ba-(X-\u03bc)^4/\u03c3^4 ]\n        =0\n    \n    To handle the large dataset used in this study, we implemented a strategy of loading one year's worth of data at a time while retaining only the relevant variables in memory and discarding the irrelevant ones. Due to the size of the dataset, using the empirical likelihood method was not feasible due to its impractical computation time. As a solution, we partitioned the data into 5 subsets for each year, resulting in a total of 110 subsets, and calculated the DEL for comparison with the SSMEL. The estimators and computation time for each variable are presented in Table\u00a0<ref>. From Table\u00a0<ref>, we can see that the estimators of the two methods are consistent for most variables, while the SSMEL has a better computation time than the DEL.\n    \n    \n    \n    \u00a7 DISCUSSION\n    \n    In this paper, we propose a novel and straightforward methodology for calculating the empirical likelihood with massive data which we refer to as split sample mean empirical likelihood (SSMEL). The approach employs split and compression strategies to address the obstacles in empirical likelihood with massive data. We show that the SSMEL preserves the statistical properties of the original empirical likelihood, and thus can be used for parameter estimation and statistical inference. Our approach's efficacy is confirmed through extensive simulation and real data analysis. It is worth noting that our method does not involve parallel computation, making it applicable to a broad range of computing devices and more feasible for real-world applications. Furthermore, in order to facilitate the processing of distributed dataset, we have developed a corresponding distributed algorithm for the SSMEL.\n    \n    To conclude this paper, we discuss several interesting directions for future research. First, this paper has only studied the case of fixed dimensionality, high-dimensional massive data are prevalent in real-world applications, so extending the approach to the case where both n and p are large is an important direction. Second, it would be of interest to develop a similar idea to more general M estimators. Finally, although we have designed an algorithm for distributed dataset for the SSMEL, the algorithm is a simple extension in <cit.>, which has a high communication cost, so it would be an interesting direction to design a more efficient distributed algorithm for this approach.\n    \n    \n    cas-model2-names\n    \n    \u00a7 APPENDIX: PROOFS\n    \n    The following Lemmas are similar from <cit.>.\n    \n    \n    \n    Under the assumption A.1- A.3, for any \u03be with 1/\u03b1<\u03be<1/2 and \u039b_n={\u03bb:\u2016\u03bb\u2016\u2264 n^-\u03be}, sup_\u03b8\u2208\u0398\u0305,\u03bb\u2208\u039b_n,1\u2264 k\u2264 K|\u03bb^Tg\u0305^(k)(\u03b8)|P\u27f60, with probability tending to 1, \u039b_n\u2282\u039b\u0302_K(\u03b8) for all \u03b8\u2208\u0398\u0305.\n    \n    \n    By Assumptions A.3 and Markov inequality, we have\n    \n        sup_\u03b8\u2208\u0398\u0305\u2016 g(x_i,\u03b8)\u2016= O_p(n^1/\u03b1)\n    \n    And on the other hand,\n    \n        max_1\u2264 k\u2264 Ksup_\u03b8\u2208\u0398\u0305\u2016g\u0305^(k)(\u03b8)\u2016   =max_1\u2264 k\u2264 Ksup_\u03b8\u2208\u0398\u0305\u20161/m\u2211_i=1^mg(x_i^(k),\u03b8)\u2016\n           \u2264max_1\u2264 k\u2264 Ksup_\u03b8\u2208\u0398\u03051/m\u2211_i=1^m\u2016 g(x_i^(k),\u03b8)\u2016\n           \u2264max_1\u2264 k\u2264 Ksup_\u03b8\u2208\u0398\u03051/m\u2211_i=1^mmax_1\u2264 i\u2264 m\u2016 g(x_i^(k),\u03b8)\u2016\n           =max_1\u2264 i\u2264 nsup_\u03b8\u2208\u0398\u0305\u2016 g(x_i,\u03b8)\u2016\n    \n    So it is obvious that\n    \n        max_1\u2264 k\u2264 Ksup_\u03b8\u2208\u0398\u0305\u2016g\u0305^(k)(\u03b8)\u2016=O_p(n^1/\u03b1)\n    \n    Then by Cauchy-Schwarz inequality,\n    \n        sup_\u03b8\u2208\u0398\u0305,\u03bb\u2208\u039b_n,1\u2264 k\u2264 K|\u03bb^Tg\u0305^(k)(\u03b8)|\u2264 n^-\u03bemax_1\u2264 k\u2264 Ksup_\u03b8\u2208\u0398\u0305\u2016g\u0305^(k)(\u03b8)\u2016=O_p(n^-\u03be+1/\u03b1)p\u27f60\n    \n    Under Assumpitons A.1-A.4, if \u03b8\u0302\u2208\u0398\u0305, \u03b8\u0302p\u27f6\u03b8_0, and 1/K\u2211_k=1^Kg\u0305^(k)(\u03b8\u0302)=O_p(n^-1/2), then with probability tending to 1, \u03bb\u0302=argmax_\u03bb\u2208\u039b\u0302_n(\u03b8\u0302)l_S(\u03bb,\u03b8\u0302) exists, and max_\u03bb\u2208\u039b\u0302_n(\u03b8\u0302)l_S(\u03bb,\u03b8\u0302)\u2264 O_p(n^-1).\n    \n    \n    Then the existence of \u03bb\u0302\u2208\u039b_n follows the statement in <cit.> by noting from Lemma A.<ref>, max_1\u2264 k\u2264 K|\u03bb^Tg\u0305^(k)(\u03b8)|p\u27f60 for \u03bb\u2208\u039b_n.  Then by a Tyloy expansion around \u03bb=0, where \u03bb\u0307 satisfy \u2016\u03bb\u0307\u2016\u2264\u2016\u03bb\u0302\u2016,\n    \n        K\u00b7 l_S(\u03b8\u0302,\u03bb\u0302)=\u2211_k=1^K\u03bb\u0302^Tg\u0305^(k)(\u03b8\u0302)-1/2\u03bb\u0302^T[\u2211_k=1^K{1+\u03bb\u0307^Tg\u0305^(k)(\u03b8\u0302)}^-2g\u0305^(k)(\u03b8\u0302)g\u0305^(k)(\u03b8\u0302)^T]\u03bb\u0302\n    \n    By Lemma A.<ref>, (1+\u03bb\u0307g\u0305^(k)(\u03b8\u0302))^-2>1/2 for all k with probability tending to 1. In addition by uniform weak law of large numbers \n    \u20161/K\u2211_k=1^Kg\u0305^(k)(\u03b8\u0302)g\u0305^(k)(\u03b8\u0302)^T-1/m\u03a9\u2016p\u27f60, as n\u2192\u221e.\n    Because \u03bb\u0302 is the maximizer, we have with probability tending to 1,\n    \n    0=l_S(\u03b8\u0302,0)\u2264 l_S(\u03b8\u0302,\u03bb\u0302)\u2264\u2016\u03bb\u0302\u2016\u20161/K\u2211_k=1^Kg\u0305^(k)(\u03b8\u0302)\u2016-c/4\u2016\u03bb\u0302\u2016^2\n    \n    This concludes \u2016\u03bb\u0302\u2016=O_p(n^-1/2) because \u20161/K\u2211_k=1^Kg\u0305^(k)(\u03b8\u0302) \u2016 = O_p(n^-1/2). Since \u03be\u22641/2, we have \u03bb\u0302\u2208\u039b\u0302_K(\u03b8\u0302) with probability tending to 1. And this is easy to get max_\u03bb\u2208\u039b\u0302_K(\u03b8\u0302)l_S(\u03bb,\u03b8\u0302)\u2264 O_p(n^-1) from (<ref>).\n    \n    Under Assumpitons A.1-A.4, we have \u20161/K\u2211_k=1^Kg\u0305^(k)(\u03b8\u0302)\u2016=O_p(n^-1/2).\n    \n    It is worth noting that\n    \n        1/K\u2211_k=1^Kg\u0305^(k)(\u03b8\u0302)=1/K\u2211_k=1^K1/m\u2211_i=1^mg(x_i^(k),\u03b8\u0302)=1/n\u2211_i=1^ng(x_i,\u03b8\u0302)\n    \n    Let 1/K\u2211_k=1^Kg\u0305^(k)(\u03b8\u0302)=1/n\u2211_i=1^ng(x_i,\u03b8\u0302)=g\u0305(\u03b8\u0302), and for \u03be in Lemma A.<ref>, \u03bb\u0303=n^-\u03beg\u0305(\u03b8\u0302)/\u2016g\u0305(\u03b8\u0302)\u2016. By Lemma A.<ref>, max_1\u2264 k\u2264 K|\u03bb\u0303^Tg\u0305^(k)(\u03b8\u0302)|p\u27f60 and \u03bb\u0303\u2208\u039b\u0302_K(\u03b8\u0302) with probability tending to 1. Also, by Cauchy-Schwarz inequality and uniform weak law of largr numbers, \u2211_kg\u0305^(k)(\u03b8\u0302)g\u0305^(k)(\u03b8\u0302)^T/K\u2264(\u2211_k\u2016g\u0305^(k)(\u03b8\u0302)\u2016^2/K)Ip\u27f6CI, so that the largest eigenvalue of \u2211_kg\u0305^(k)(\u03b8\u0302)g\u0305^(k)(\u03b8\u0302)^T/K is bounded above with probability tending to 1. By Tylor expansion, where \u03bb\u0307 satisfy \u2016\u03bb\u0307\u2016\u2264\u2016\u03bb\u0303\u2016, it holds with probability tending to 1,\n    \n        l_S(\u03b8\u0302,\u03bb\u0303)\n           =1/K\u2211_k=1^K\u03bb\u0303^Tg\u0305^(k)(\u03b8\u0302)-1/2\u03bb\u0303^T[1/K\u2211_i=1^K{1+\u03bb\u0307^Tg\u0305^(k)(\u03b8\u0302)}^-2g\u0305^(k)(\u03b8\u0302)g\u0305^(k)(\u03b8\u0302)^T]\u03bb\u0303\n           \u2265 n^-\u03be\u2016g\u0305(\u03b8\u0302)\u2016-Cn^-2\u03be{1+o_p(1)}\n    \n    By the Lindeberg-L\u00e9vy central limit theorem the hypotheses of Lemma A.<ref> are satisfied by \u03b8\u0302=\u03b8_0. By \u03b8\u0302 and \u03bb\u0302 being a saddle point, this equation and Lemma A.<ref> give\n    \n    n^-\u03be\u2016g\u0305(\u03b8\u0302)\u2016-Cn^-2\u03be\u2264 l_S(\u03b8\u0302,\u03bb\u0303)\u2264max_\u03bb\u2208\u039b\u0302_K(\u03b8\u0302)l_S(\u03b8\u0302,\u03bb) \u2264 O_p(n^-1)\n    \n    Also, by \u03be<1/2, \u03be-1<-1/2<-\u03be. Solving Equation(<ref>) for \u2016g\u0305(\u03b8\u0302) \u2016 then gives\n    \n        \u2016g\u0305(\u03b8\u0302) \u2016\u2264 O_p(n^\u03be-1)+Cn^-\u03be = O_p(n^-\u03be)\n    \n    We consider any \u03b5_n\u21920. Let \u03bb^*=\u03b5_ng\u0305(\u03b8\u0302), then \u03bb^*=o_p(n^-\u03be), so that \u03bb^*\u2208\u039b_n with probability tending to 1. Using the same arguments above, we can obtain\n    \n        \u03f5_n\u2016g\u0305(\u03b8\u0302)\u2016^2-C\u03f5_n^2\u2264 O_p(n^-1)\n    \n    Then \u03f5_n\u2016g\u0305(\u03b8\u0302)\u2016^2=O_p(n^-1). Notice that we can select arbitrary slow \u03b5_n\u21920, following a standard result form probability theroy, that if \u03b5_nY_n=O_p(n^-1), for all \u03b5_n\u21920, then Y_n=O_p(n^-1). So, we have \u2016g\u0305(\u03b8\u0302)\u2016=O_p(n^-1/2).\n    \n    Under Assumpitons A.1-A.4, we have \u03b8\u0302_Sp\u27f6\u03b8_0, 1/K\u2211_k=1^Kg\u0305^(k)(\u03b8\u0302_S)=O_p(n^-1/2), \u03bb\u0302_S=argmax_\u03bb\u2208\u039b\u0302_K(\u03b8\u0302_S)l_S(\u03b8\u0302_S,\u03bb) exists with probability tending to 1, and \u03bb\u0302_S=O_p(n^-1/2).\n    \n    Let g(\u03b8)=\ud835\udd3c[g(x,\u03b8)], by Lemma A.<ref>, g\u0305(\u03b8\u0302_S)p\u27f60, and by uniform weak law of large numbers, sup_\u03b8\u2208\u0398\u0305\u2016g\u0305(\u03b8)-g(\u03b8)\u2016p\u27f60 and g(\u03b8) is continuous. By triangle inequlity,\n    \n        \u2016g\u0305(\u03b8\u0302_S)\u2016-\u2016 g(\u03b8\u0302_S)\u2016\u2264\u2016g\u0305(\u03b8\u0302_S)-g(\u03b8\u0302_S)\u2016\u2264sup_\u03b8\u2208\u0398\u0305\u2016g\u0305(\u03b8)-g(\u03b8)\u2016\n    \n    We have g(\u03b8\u0302_S)p\u27f60. Since g(\u03b8)=0 has a unique zero at \u03b8_0, \u2016 g(\u03b8)\u2016 must be bounded away from zero outside any neighborhood of \u03b8_0. Therefore, \u03b8\u0302_S must be inside any neighborhood of \u03b8_0 with probability tending to 1, i.e. \u03b8\u0302_Sp\u27f6\u03b8_0, giving the first conclusion. The second conclusion follows by Lemma A.<ref>. And by the first two conclusions, the hypotheses of Lemma A.<ref> are satisfied, so that the last conclusion follows from Lemma A.<ref>.\n    \n    Under Assumpitons A.1-A.4, \u03b8\u0302_S and \u03bb\u0302_S satisfy\n    \n        Q_1K(\u03b8\u0302_S,\u03bb\u0302_S)=0,   Q_2K(\u03b8\u0302_S,\u03bb\u0302_S)=0\n    \n    where\n    \n        Q_1K(\u03b8,\u03bb)   =1/K\u2211_k=1^K1/1+\u03bb^Tg\u0305^(k)(\u03b8)g\u0305^(k)(\u03b8)\n        \n        \t\tQ_2K(\u03b8,\u03bb)   =1/K\u2211_k=1^K1/1+\u03bb^Tg\u0305^(k)(\u03b8)(\u2202g\u0305^(k)(\u03b8)/\u2202\u03b8)^T\u03bb\n    \n    The conclusion can be obtained from Lemma A.<ref> and Lemma A.<ref>, more details refer the proof of Lemma 1 of <cit.> or the proof of Theorem 3.2 of <cit.>Proof of Theorem <ref>\n    By LemmaA.<ref>, we have\n    \n        Q_1K(\u03b8_S,\u03bb_S)   =1/K\u2211_k=1^K1/1+\u03bb_S^Tg\u0305^(k)(\u03b8_S)g\u0305^(k)(\u03b8_S)=0,\n        \n        \t\tQ_2K(\u03b8_S,\u03bb_S)   =1/K\u2211_k=1^K1/1+\u03bb^T_Sg\u0305^(k)(\u03b8_S)(\u2202g\u0305^(k)(\u03b8_S)/\u2202\u03b8)^T\u03bb_S=0\n    \n    As n\u2192\u221e,\n    \n    \n        \u2202 Q_1K(\u03b8_0,0)/\u2202\u03b8   =1/K\u2211_k=1^K\u2202g\u0305^(k)(\u03b8_0)/\u2202\u03b8^T\n        =1/K\u2211_k=1^K1/m\u2211_i=1^m\u2202 g(x_i^(k),\u03b8_0)/\u2202\u03b8^T\n           =1/n\u2211_i=1^n\u2202 g(x_i,\u03b8_0)/\u2202\u03b8^Tp\u27f6\ud835\udd3c(\u2202 g/\u2202\u03b8^T)=G\n    \n        \u2202 Q_1K(\u03b8_0,0)/\u2202\u03bb^T   =-1/K\u2211_k=1^Kg\u0305^(k)(\u03b8_0)g\u0305^(k)(\u03b8_0)^T\n           =-1/K\u2211_k=1^K1/m^2(\u2211_i=1^mg(x_i^(k),\u03b8_0)\u2211_i=1^mg(x_i^(k),\u03b8_0)^T)\n           =-1/m1/n\u2211_i=1^ng(x_i,\u03b8_0)g(x_i,\u03b8_0)^T\n           -1/n\u2211_k=1^K(2/m\u2211_i\u2260 s^mg(x_i^(k),\u03b8_0)g(x_s^(k),\u03b8_0)^T)\n           p\u27f6-1/m\ud835\udd3c(gg^T)+o_p(n^-1/2)=-1/m\u03a9+o_p(n^-1/2)\n    \n        \u2202 Q_2K(\u03b8_0,0)/\u2202\u03bb^T   =1/K\u2211_k=1^K(\u2202g\u0305^(k)(\u03b8_0)/\u2202\u03b8^T)^T=1/K\u2211_k=1^K1/m\u2211_i=1^m\u2202 g(x_i^(k),\u03b8_0)/\u2202\u03b8^T^T\n           =1/n\u2211_i=1^n\u2202 g(x_i,\u03b8_0)^T/\u2202\u03b8^Tp\u27f6\ud835\udd3c(\u2202 g/\u2202\u03b8^T)^T=G^T\n    \n    By Tylor expansion around (\u03b8_0,0), we can show\n    \n    \n        0   =Q_1K(\u03b8\u0302_S,\u03bb\u0302_S)=Q_1K(\u03b8_0,0)+\u2202 Q_1K(\u03b8_0,0)/\u2202\u03b8(\u03b8\u0302_S-\u03b8_0)+\u2202 Q_1K(\u03b8_0,0)/\u2202\u03bb^T\u03bb\u0302_S+o_p(\u03b4_K)\n        \n        0   =Q_2K(\u03b8\u0302_S,\u03bb\u0302_S)=Q_2K(\u03b8_0,0)+\u2202 Q_2K(\u03b8_0,0)/\u2202\u03b8(\u03b8\u0302_S-\u03b8_0)+\u2202 Q_2K(\u03b8_0,0)/\u2202\u03bb^T\u03bb\u0302_S+o_p(\u03b4_K)\n    \n    where both Q_2K(\u03b8_0,0) and \u2202 Q_2K(\u03b8_0,0)/\u2202\u03b8 are 0, \u03b4_K=\u2016\u03b8\u0302_S-\u03b8_0\u2016+\u2016\u03bb\u0302_S\u2016, so\n    \n    \n        0   =\u2202 Q_2K(\u03b8_0,0)/\u2202\u03bb^T\u00d7(-\u2202 Q_1K(\u03b8_0,0)/\u2202\u03bb^T)^-1\u00d7[Q_1K(\u03b8_0,0)+\u2202 Q_1K(\u03b8_0,0)/\u2202\u03b8(\u03b8\u0302_S-\u03b8_0)+o_p(\u03b4_K)]+o_p(\u03b4_K)\n           =G^T(1/m\u03a9)^-1Q_1K(\u03b8_0,0)+G^T(1/m\u03a9)^-1G(\u03b8\u0302_S-\u03b8_0)+o_p(mn^-1/2)\n    \n        \u03b8\u0302_S-\u03b8_0   =-\u03a3 G^T\u03a9^-1 Q_1K(\u03b8_0,0)+o_p(n^-1/2)\n    \n     Because -\u221a(n)\u03a9^-1/2Q_1K(\u03b8_0,0) converges to standard multivariate normal distribution, i.e. -\u221a(n)\u03a9^-1/2Q_1K(\u03b8_0,0)=-\u221a(n)\u03a9^-1/2g\u0305(\u03b8_0)d\u27f6N(0,I), therefore as n\u2192\u221e, we have\n    \n        \u221a(n)(\u03b8\u0302_S-\u03b8_0)d\u27f6N(0,\u03a3)\n    Proof of Corollary <ref>\n    See literature Lemma 1 of <cit.>.\n    Proof of Theorem <ref>\n    The split sample mean empirical likelihood ratio test statistic is\n    \n        \ud835\udcb2(\u03b8_0)=2{\u2211_k=1^Klog[1+\u03bb_0^Tg\u0305^(k)(\u03b8_0)]-\u2211_k=1^Klog[1+\u03bb\u0302_S^Tg\u0305^(k)(\u03b8\u0302_S)]}\n    \n    By LemmaA.<ref>, we have Q_1K(\u03b8\u0302_S,\u03bb\u0302_S)=0, and by Tylor expansion\n    \n        \u03bb\u0302_S   =[1/K\u2211_k=1^Kg\u0305^(k)(\u03b8\u0302_S)g\u0305^(k)(\u03b8\u0302_S)^T]^-1(\u2211_k=1^Kg\u0305^(k)(\u03b8\u0302_S))+o_p(1) \n           =(1/m\u03a9)^-1Q_1K(\u03b8\u0302_S,0)+o_p(1)\n    \n    Also by the Taylor expansion and  Euqation (<ref>),\n    \n    \n        Q_1K(\u03b8\u0302_S,0)   =Q_1K(\u03b8_0,0)+\u2202 Q_1K(\u03b8_0,0)/\u2202\u03b8^T(\u03b8\u0302_S-\u03b8_0)+o_p(1)\n           =Q_1K(\u03b8_0,0)+G(\u03b8\u0302_S-\u03b8_0)+o_p(1)\n           =Q_1K(\u03b8_0,0)-G\u03a3 G^T\u03a9^-1Q_1K(\u03b8_0,0)+o_p(1)\n    \n    \n    Further Taylor expansion for l_S(\u03b8\u0302_S,\u03bb\u0302_S), and by Equation (<ref>) and (<ref>) we have\n    \n    \n        2l_S(\u03b8\u0302_S,\u03bb\u0302_S)   =2\u2211_k=1^Klog[1+\u03bb\u0302_S^Tg\u0305^(k)(\u03b8\u0302_S)]\n           =2\u2211_k=1^K\u03bb\u0302^T_Sg\u0305^(k)(\u03b8\u0302_S)-2\u2211_k=1^K[\u03bb\u0302^T_Sg\u0305^(k)(\u03b8\u0305_S)]^2+o_p(1)\n           =KQ_1K^T(\u03b8\u0302_S,0)[1/K\u2211_k=1^Kg\u0305^(k)(\u03b8_S)g\u0305^(k)(\u03b8_S)^T]^-1Q_1K(\u03b8\u0302_S,0)+o_p(1)\n           =KQ_1K^T(\u03b8\u0302_S,0)(1/m\u03a9)^-1Q_1K(\u03b8\u0302_S,0)+o_p(1)\n           =nQ^T_1K(\u03b8_0,0)\u03a9^-1(I-G\u03a3 G^T\u03a9^-1)Q_1K(\u03b8_0,0)+o_p(1)\n    \n    Under H_0 is ture, similarly\n    \n        \u03bb_0=\u03a9^-1Q_1K(\u03b8_0,0)+o_p(1),   and   l_S(\u03b8_0,\u03bb_0)=nQ^T_1K(\u03b8_0,0)\u03a9^-1Q_1K(\u03b8_0,0)+o_p(1)\n    \n    Thus\n    \n        \ud835\udcb2(\u03b8_0)   =nQ^T_1K(\u03b8_0,0)[\u03a9^-1-\u03a9^-1(I-G\u03a3 G^T\u03a9^-1)]Q^T_1K(\u03b8_0,0)+o_p(1)\n           =nQ^T_1K(\u03b8_0,0)\u03a9^-1G\u03a3 G^T\u03a9^-1Q_1K(\u03b8_0,0)+o_p(1)\n           =[\u03a9^-1/2\u221a(n)Q_1K(\u03b8_0,0)]^T[\u03a9^-1/2G\u03a3 G^T\u03a9^-1/2][\u03a9^-1/2\u221a(n)Q_1K(\u03b8_0,0)]+o_p(1)\n    \n    Note that \u03a9^-1/2G\u03a3 G^T\u03a9^-1/2 converges to a standard multivariate normal distribution and that \u03a9^-1/2G\u03a3 G^T\u03a9^-1/2 is symmetric idempotent, with trace equal to p. Hence the SSMEL ratio test statistic \ud835\udcb2(\u03b8_0) converges to \u03c7_p^2.\n    Proof of Corollary <ref>\n    See literature Corollary 5 of <cit.>.\n"}