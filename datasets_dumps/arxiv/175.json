{"entry_id": "http://arxiv.org/abs/2303.07109v1", "published": "20230313134359", "title": "Transformer-based World Models Are Happy With 100k Interactions", "authors": ["Jan Robine", "Marc H\u00f6ftmann", "Tobias Uelwer", "Stefan Harmeling"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "stat.ML"], "text": "\n\nUpcycling Models under Domain and Category Shift\n    Sanqing Qu^1Equal Contribution , Tianpei Zou^1, Florian R\u00f6hrbein^2,\nCewu Lu^3, Guang Chen^1Corresponding author: guangchen@tongji.edu.cn ,\n Dacheng Tao^4,5, Changjun Jiang^1\n\n^1Tongji University, ^2Chemnitz University of Technology,\n\n^3Shanghai Jiao Tong University, ^4JD Explore Academy, ^5The University of Sydney\n\n\n    March 30, 2023\n=================================================================================================================================================================================================================================================================================================================================\n\n\n\n\nDeep neural networks have been successful in many reinforcement learning\nsettings. However, compared to human learners they are overly data hungry. To\nbuild a sample-efficient world model, we apply a transformer to real-world\nepisodes in an autoregressive manner: not only the compact latent states and the\ntaken actions but also the experienced or predicted rewards are fed into the\ntransformer, so that it can attend flexibly to all three modalities at different\ntime steps. The transformer allows our world model to access previous states\ndirectly, instead of viewing them through a compressed recurrent state. By\nutilizing the Transformer-XL architecture, it is able to learn long-term\ndependencies while staying computationally efficient. Our transformer-based\nworld model (TWM) generates meaningful, new experience, which is used to train a\npolicy that outperforms previous model-free and model-based reinforcement\nlearning algorithms on the Atari 100k benchmark.\nOur code is available at <https://github.com/jrobine/twm>. \n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep reinforcement learning methods have shown great success on many challenging\ndecision making problems. Notable methods include DQN <cit.>, PPO\n<cit.>, and MuZero <cit.>. However, most algorithms require\nhundreds of millions of interactions with the environment, whereas humans often\ncan achieve similar results with less than 1% of these interactions, i.e., they\nare more sample-efficient. The large amount of data that is necessary renders a\nlot of potential real world applications of reinforcement learning impossible.\n\nRecent works have made a lot of progress in advancing the sample efficiency of\nRL algorithms: model-free methods have been improved with auxiliary objectives\n<cit.>, data augmentation (, ), or both\n<cit.>. Model-based methods have been successfully applied to complex\nimage-based environments and have either been used for planning, such as\nEfficientZero <cit.>, or for learning behaviors in imagination,\nsuch as SimPLe <cit.>.\n\nA promising model-based concept is learning in imagination\n(; ; ;\n): instead of learning behaviors from the collected\nexperience directly, a generative model of the environment dynamics is learned\nin a  manner. Such a so-called world model can\ncreate new trajectories by iteratively predicting the next state and reward.\nThis allows for potentially indefinite training data for the reinforcement\nlearning algorithm without further interaction with the real environment. A\nworld model might be able to generalize to new, unseen situations, because of\nthe nature of deep neural networks, which has the potential to drastically\nincrease the sample efficiency. This can be illustrated by a simple example: in\nthe game of Pong, the paddles and the ball move independently. In the best case,\na successfully trained world model would imagine trajectories with paddle and\nball configurations that have never been observed before, which enables learning\nof improved behaviors.\n\nIn this paper, we propose to model the world with transformers\n<cit.>, which have significantly advanced the field of natural\nlanguage processing and have been successfully applied to computer vision tasks\n<cit.>. A transformer is a sequence model consisting of multiple\nself-attention layers with residual connections. In each self-attention layer\nthe inputs are mapped to keys, queries, and values. The outputs are computed by\nweighting the values by the similarity of keys and queries. Combined with causal\nmasking, which prevents the self-attention layers from accessing future time\nsteps in the training sequence, transformers can be used as autoregressive\ngenerative models. The Transformer-XL architecture <cit.> is\nmuch more computationally efficient than vanilla transformers at inference time\nand introduces relative positional encodings, which remove the dependence on\nabsolute time steps.\n\n\n\n\n\n  \nOur contributions:\nThe contributions of this work can be summarized as follows:\n\n\n  * We present a new autoregressive world model based on the Transformer-XL\n  <cit.> architecture and a model-free agent trained in latent\n  imagination. Running our policy is computationally efficient, as the\n  transformer is not needed at inference time. This is in contrast to related\n  works <cit.> that require the full world\n  model during inference.\n\n  * Our world model is provided with information on how much reward has\n  already been emitted by feeding back predicted rewards into the world model.\n  As shown in our ablation study, this improves performance.\n\n  * We rewrite the balanced KL divergence loss of <cit.> to allow\n  us to fine-tune the relative weight of the involved entropy and cross-entropy\n  terms.\n\n  * We introduce a new thresholded entropy loss that stabilizes the policy's\n  entropy during training and hereby simplifies the selection of hyperparameters\n  that behave well across different games.\n\n  * We propose a new effective sampling procedure for the growing dataset of\n  experience, which balances the training distribution to shift the focus\n  towards the latest experience. We demonstrate the efficacy of this procedure\n  with an ablation study.\n\n  * We compare our transformer-based world model (TWM) on the Atari 100k\n  benchmark with recent sample-efficient methods and obtain excellent results.\n  Moreover, we report empirical confidence intervals of the aggregate metrics as\n  suggested by <cit.>.\n\n\n\n\n\u00a7 METHOD\n\nWe consider a partially observable Markov decision process (POMDP) with discrete\ntime steps , scalar rewards , high-dimensional image observations  , and discrete actions ,\nwhich are generated by some policy , where  o_1:t and a_1:t-1 denote the sequences of\nobservations and actions up to time steps t and , respectively.\nEpisode ends are indicated by a boolean variable .\nObservations, rewards, and episode ends are jointly generated by the unknown\nenvironment dynamics . The goal is to find a policy \u03c0 that maximizes the expected sum\nof discounted rewards , where  is the discount factor. Learning in\nimagination consists of three steps that are repeated iteratively: learning the\ndynamics, learning a policy, and interacting in the real environment. In this\nsection, we describe our world model and policy, concluding with the training\nprocedure.\n\n\n\n \u00a7.\u00a7 World Model\n\nOur world model consists of an observation model and a dynamics model, which do\nnot share parameters. <ref> illustrates our combined world\nmodel architecture.\n\n\n\n  \nObservation Model:\nThe observation model is a variational autoencoder <cit.>, which encodes\nobservations o_t into compact, stochastic latent states z_t and reconstructs\nthe observations with a decoder, which in our case is only required to obtain a\nlearning signal for z_t:\n0.1em\n\n    3   Observation encoder:      z_t    \u223c(z_t  o_t) \n       Observation decoder:     \u00f4_t    \u223c(\u00f4_t  z_t).\n\nWe adopt the neural network architecture of DreamerV2\n<cit.> with slight modifications for our observation model. Thus, a\nlatent state z_t is discrete and consists of a vector of 32 categorical\nvariables with 32 categories. The observation decoder reconstructs the\nobservation and predicts the means of independent standard normal distributions\nfor all pixels. The role of the observation model is to capture only\nnon-temporal information about the current time step, which is different from\n<cit.>. However, we include short-time temporal information, since a\nsingle observation o_t consists of four frames (aka frame stacking, see also\n<ref>).\n\n\n\n  \nAutoregressive Dynamics Model:  The\ndynamics model predicts the next time step conditioned on the history of its\npast predictions. The backbone is a deterministic aggregation model \nwhich computes a deterministic hidden state h_t based on the history of the\n previously generated latent states, actions, and rewards.\nPredictors for the reward, discount, and next latent state are conditioned on\nthe hidden state. The dynamics model consists of these components:\n0.1em\n\n    3   Aggregation model:      h_t     = (z_t-:t,a_t-:t,r_t-:t-1) \n       Reward predictor:     r\u0302_t    \u223c(r\u0302_t  h_t) \n       Discount predictor:     \u03b3\u0302_t    \u223c(\u03b3\u0302_t  h_t) \n       Latent state predictor:       \u1e91_t+1   \u223c(\u1e91_t+1 h_t).\n\nThe aggregation model is implemented as a causally masked Transformer-XL\n<cit.>, which enhances vanilla transformers <cit.>\nwith a recurrence mechanism and relative positional encodings. With these\nencodings, our world model learns the dynamics independent of absolute time\nsteps. Following <cit.>, the latent states, actions, and\nrewards are sent into modality-specific linear embeddings before being passed to\nthe transformer. The number of input tokens is 3 - 1, because of\nthe three modalities (latent states, actions, rewards) and the last reward not\nbeing part of the input. We consider the outputs of the action modality as the\nhidden states and disregard the outputs of the other two modalities (see\n<ref>; orange boxes vs. gray boxes).\n\nThe latent state, reward, and discount predictors are implemented as multilayer\nperceptrons (MLPs) and compute the parameters of a vector of independent\ncategorical distributions, a normal distribution, and a Bernoulli distribution,\nrespectively, conditioned on the deterministic hidden state. The next state is\ndetermined by sampling from (\u1e91_t+1 h_t). The reward and\ndiscount are determined by the mean of (r\u0302_t  h_t) and\n(\u03b3\u0302_t  h_t), respectively.\n\nAs a consequence of these design choices, our world model has the following\nbeneficial properties:\n\n  \n  * The dynamics model is autoregressive and has direct access to its\n    previous outputs.\n  \n  * Training is efficient since sequences are processed in parallel\n    (compared with RNNs).\n  \n  * Inference is efficient because outputs are cached (compared with\n    vanilla Transformers).\n  \n  * Long-term dependencies can be captured by the recurrence mechanism.\n\n\nWe want to provide an intuition on why a fully autoregressive dynamics model is\nfavorable: First, the direct access to previous latent states enables to model\nmore complex dependencies between them, compared with RNNs, which only see them\nindirectly through a compressed recurrent state. This also has the potential to\nmake inference more robust, since degenerate predictions can be ignored more\neasily. Second, because the model sees which rewards it has produced previously,\nit can react to its own predictions. This is even more significant when the\nrewards are sampled from a probability distribution, since the introduced noise\ncannot be observed without autoregression.\n\n\n\n  \nLoss Functions:\nThe observation model can be interpreted as a variational autoencoder with a\ntemporal prior, which is provided by the latent state predictor. The goal is to\nkeep the distributions of the encoder and the latent state predictor close to\neach other, while slowly adapting to new observations and dynamics.\n<cit.> apply a balanced KL divergence loss, which lets them control\nwhich of the two distributions should be penalized more.  To control the\ninfluences of its subterms more precisely, we disentangle this loss and obtain a\nbalanced cross-entropy loss that computes the cross-entropy\n(z_t+1 o_t+1),(\u1e91_t+1 h_t) and\nthe entropy (z_t  o_t) explicitly. Our derivation can be\nfound in <ref>. We call the cross-entropy term for the\nobservation model the consistency loss, as its purpose is to prevent\nthe encoder from diverging from the dynamics model. The entropy regularizes the\nlatent states and prevents them from collapsing to one-hot distributions. The\nobservation decoder is optimized via negative log-likelihood, which provides a\nrich learning signal for the latent states. In summary, we optimize a\nself-supervised loss function for the observation model that is the expected sum\nover the decoder loss, the entropy regularizer and the consistency loss\n\n    _^Obs. = [][] _t=1^T -ln(o_t  z_t)_decoder - (z_t  o_t)_entropy regularizer + (z_t  o_t),(\u1e91_t  h_t-1)_consistency,\n\nwhere the hyperparameters , \u2265 0 control the\nrelative weights of the terms.\n\nFor the balanced cross-entropy loss, we also minimize the cross-entropy in the\nloss of the dynamics model, which is how we train the latent state predictor.\nThe reward and discount predictors are optimized via negative log-likelihood.\nThis leads to a self-supervised loss for the dynamics model\n\n    _^Dyn. = [][] _t=1^T (z_t+1 o_t+1),(\u1e91_t+1 h_t)_latent state predictor - ln(r_t  h_t)_reward predictor - ln(\u03b3_t  h_t)_discount predictor ,\n\nwith coefficients , \u2265 0 and where \u03b3_t = 0\nfor episode ends (d_t = 1) and \u03b3_t = \u03b3 otherwise.\n\n\n\n \u00a7.\u00a7 Policy\n \nOur policy \u03c0_\u03b8(a_t \u1e91_t) is trained on imagined trajectories\nusing a mainly standard advantage actor-critic <cit.> approach. We train\ntwo separate networks: an actor a_t \u223c(a_t \u1e91_t) with\nparameters  and a critic (\u1e91_t) with parameters\n. We compute the advantages via Generalized Advantage Estimation\n<cit.> while using the discount factors predicted by the world model\n\u03b3\u0302_t instead of a fixed discount factor for all time steps. As in\nDreamerV2 <cit.>, we weight the losses of the actor and the critic by\nthe cumulative product of the discount factors, in order to softly account for\nepisode ends.\n\n\n\n  \nThresholded Entropy Loss:  We penalize\nthe objective of the actor with a slightly modified version of the usual entropy\nregularization term <cit.>. Our penalty normalizes the entropy and only\ntakes effect when the entropy falls below a certain threshold\n\n    \u2112^Ent._\u03b8 =max(0,  - H(\u03c0_\u03b8)/ln(m)),\n\nwhere 0\u2264\u0393\u2264 1 is the threshold hyperparameter, H(\u03c0_\u03b8) is the\nentropy of the policy, m is the number of discrete actions, and ln(m) is\nthe maximum possible entropy of the categorical action distribution. By doing\nthis, we explicitly control the percentage of entropy that should be preserved\nacross all games independent of the number of actions. This ensures exploration\nin the real environment and in imagination without the need for\n\u03f5-greedy action selection or changing the temperature of the action\ndistribution. We also use the same stochastic policy when evaluating our agent\nin the experiments. The idea of applying a hinge loss to the entropy was first\nintroduced by <cit.> in the context of supervised learning. In\n<ref> we show the effect of this loss.\n\n\n\n  \nChoice of Policy Input:\n\nThe policy computes an action distribution \u03c0_\u03b8(a_t  x_t) given\nsome view x_t of the state. For instance, x_t could be o_t, z_t, or\n[z_t,h_t] at inference time, i.e., when applied to the real environment, or\nthe respective predictions of the world model \u00f4_t, \u1e91_t, or\n[\u1e91_t,h_t] at training time. This view has to be chosen carefully, since\nit can have a significant impact on the performance of the policy and it affects\nthe design choices for the world model. Using x_t = o_t (or \u00f4_t) is\nrelatively stable even with imperfect reconstructions \u00f4_t, as the\nunderlying distribution of observations p(o) does not change during training.\nHowever, it is also less computationally efficient, since it requires\nreconstructing the observations during imagination and additional convolutional\nlayers for the policy. Using x_t = z_t (or \u1e91_t) is slightly less\nstable, as the policy has to adopt to the changes of the distributions\n(z_t  o_t) and (\u1e91_t+1 h_t) during\ntraining. Nevertheless, the entropy regularizer and consistency loss in\n<ref> stabilize these distributions. Using  (or [\u1e91_t,h_t]) provides the agent with a summary of the\nhistory of experience, but it also adds the burden of running the transformer at\ninference time. Model-free agents already perform well on most Atari games when\nusing a stack of the most recent frames (e.g., ). Therefore, we\nchoose  and apply frame stacking at inference time in order to\nincorporate short-time information directly into the latent states. At training\ntime we use x_t = \u1e91_t, i.e., the predicted latent states, meaning no\nframe stacking is applied. As a consequence, our policy is computationally\nefficient at training time (no reconstructions during imagination) and at\ninference time (no transformer when running in the real environment).\n\n\n\n \u00a7.\u00a7 Training\n\nAs is usual for learning with world models, we repeatedly (i) collect experience\nin the real environment with the current policy, (ii) improve the world model\nusing the past experience, (iii) improve the policy using new experience generated\nby the world model.\n\nDuring training we build a dataset = [(o_1, a_1, r_1, d_1), \u2026,\n(o_, a_, r_, d_)] of the\ncollected experience. After collecting new experience with the current policy,\nwe improve the world model by sampling  sequences of length\n from  and optimizing the loss functions in\n<ref> using stochastic gradient descent.\nAfter performing a world model update, we select  of the\n\u00d7 observations and encode them into latent\nstates to serve as initial states for new trajectories. The dynamics model\niteratively generates these  trajectories of length \nbased on actions provided by the policy. Subsequently, the policy is improved\nwith standard model-free objectives, as described in <ref>. In\n<ref> we present pseudocode for training the world model and the\npolicy.\n\n\n\n  \nBalanced Dataset Sampling:  Since the dataset\ngrows slowly during training, uniform sampling of trajectories focuses too\nheavily on early experience, which can lead to overfitting especially in the low\ndata regime. Therefore, we keep visitation counts v_1, \u2026, v_,\nwhich are incremented every time an entry is sampled as start of a sequence.\nThese counts are converted to probabilities using the softmax function\n\n    (p_1, \u2026, p_T) = softmax(-v_1, \u2026, -v_),\n\nwhere >0 is a temperature hyperparameter. With our sampling\nprocedure, new entries in the dataset are oversampled and are selected more\noften than old ones. Setting = \u221e restores uniform sampling\nas a special case, whereas reducing  increases the amount of\noversampling. See <ref> for a comparison. We empirically show the\neffectiveness in <ref>.\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\nTo compare data-efficient reinforcement learning algorithms, <cit.>\nproposed the Atari 100k benchmark, which uses a subset of 26 Atari games from\nthe Arcade Learning Environment <cit.> and limits the number of\ninteractions per game to 100K.  This corresponds to 400K frames (because of\nframe skipping) or roughly 2 hours of gameplay, which is 500 times less than\nthe usual 200 million frames (e.g., ).\n\nWe compare our method with five strong competitors on the Atari 100k benchmark:\n(i) SimPLe <cit.> implements a world model as an action-conditional\nvideo prediction model and trains a policy with PPO <cit.>, (ii) DER\n<cit.> is a variant of Rainbow <cit.>\nfine-tuned for sample efficiency, (iii) CURL <cit.> improves representations\nusing contrastive learning as an auxiliary task and is combined with DER, (iv)\nDrQ <cit.> improves DQN by averaging Q-value estimates over multiple data\naugmentations of observations, and (v) SPR <cit.> forces representations to\nbe consistent across multiple time steps and data augmentations by extending\nRainbow with a self-supervised consistency loss.\n\n\n\n \u00a7.\u00a7 Results\n\n\n\n\nr0.45\n  \n\n  \n    < g r a p h i c s >\n\n  Performance profiles on the Atari 100k benchmark based on score\n  distributions <cit.>. It shows the fraction of runs across all\n  games (y-axis) above a human normalized score (x-axis). Shaded regions show\n  pointwise 95% confidence bands.\n  \n  \n\n\n\nWe follow the advice of <cit.> who found significant discrepancies\nbetween reported point estimates of mean (and median) scores and a thorough\nstatistical analysis that includes statistical uncertainty. Thus, we report\nconfidence interval estimates of the aggregate metrics median, interquartile\nmean (IQM), mean, and optimality gap in <ref> and\nperformance profiles in <ref>, which we created using\nthe toolbox provided by <cit.>. The metrics are computed on human\nnormalized scores, which are calculated as\n\n. We report the unnormalized scores per\ngame in <ref>. We compare with new scores for DER, CURL, DrQ, and\nSPR that were evaluated on 100 runs and provided by <cit.>. They\nreport scores for the improved DrQ(\u03b5), which is DrQ evaluated with\nstandard \u03b5-greedy parameters.\nWe perform 5 runs per game and compute the average score over 100 episodes\nat the end of training for each run. TWM shows a significant improvement over\nprevious approaches in all four aggregate metrics and brings the optimality gap\ncloser to zero.\n\n\n\n \u00a7.\u00a7 Analysis\n\n\nIn <ref> we show imagined trajectories of our world model. In\n<ref> we visualize an attention map of the transformer for an\nimagined sequence. In this example a lot of weight is put on the current action\nand the last three states. However, the transformer also attends to states and\nrewards in the past, with only past actions being mostly ignored. The two high\npositive rewards also get high attention, which confirms that the rewards in the\ninput sequence are used by the world model. We hypothesize that these rewards\ncorrespond to some events that happened in the environment and this information\ncan be useful for prediction.\n\nAn extended analysis can be found in <ref>, including\nmore imagined trajectories and attention maps (and a description of the\ngeneration of the plots), sample efficiency, stochasticity of the world model,\nlong sequence imagination, and frame stacking.\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Ablation Studies\n\n\n\n\n  \nUniform Sampling: \n\nTo show the effectiveness of the sampling procedure described in\n<ref>, we evaluate three games with uniform dataset sampling,\nwhich is equivalent to setting = \u221e in <ref>.\nIn <ref> we show that balanced dataset sampling\nsignificantly improves the performance in these games. At the end of training,\nthe dynamics loss from <ref> is lower when applying balanced\nsampling. One reason might be that the world model overfits on early training\ndata and performs bad in later stages of training.\n\n\n\n  \nNo Rewards: \n\nAs described in <ref>, the predicted rewards are fed back\ninto the transformer. In <ref> we show on three games that this\ncan significantly increase the performance. In some games the performance is\nequivalent, probably because the world model can make correct predictions solely\nbased on the latent states and actions.\n\nIn <ref> we perform additional ablation\nstudies, including the thresholded entropy loss, a shorter history length,\nconditioning the policy on [z,h], and increasing the sample efficiency.\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\nThe Dyna architecture <cit.> introduced the idea of training a model of\nthe environment and using it to further improve the value function or the\npolicy. <cit.> introduced the notion of a world model,\nwhich tries to completely imitate the environment and is used to generate\nexperience to train a model-free agent. They implement a world model as a VAE\n<cit.> and an RNN and learn a policy in latent space with an evolution\nstrategy. With SimPLe, <cit.> propose an iterative training procedure\nthat alternates between training the world model and the policy. Their policy\noperates on pixel-level and is trained using PPO <cit.>. <cit.>\npresent Dreamer and implement a world model as a stochastic RNN that splits the\nlatent state in a stochastic part and a deterministic part; this idea was first\nintroduced by . This allows their world model to capture the\nstochasticity of the environment and simultaneously facilitates remembering\ninformation over multiple time steps. <cit.> use a VQ-VAE to\nconstruct a world model with drastically lower number of parameters. DreamerV2\n<cit.> achieves great performance on the Atari 50M benchmark after\nmaking some changes to Dreamer, the most important ones being categorical latent\nvariables and an improved objective. \n\nAnother direction of model-based reinforcement learning is planning, where the\nmodel is used at inference time to improve the action selection by looking ahead\nseveral time steps into the future. The most prominent work is MuZero\n<cit.>, where a learned sequence model of rewards and values is combined\nwith Monte-Carlo Tree Search <cit.> without learning explicit\nrepresentations of the observations. MuZero achieves impressive performance on\nthe Atari 50M benchmark, but it is also computationally expensive and requires\nsignificant engineering effort. EfficientZero <cit.> improves\nMuZero and achieves great performance on the Atari 100k benchmark.\n\nTransformers <cit.> advanced the effectiveness of sequence models\nin multiple domains, such as natural language processing and computer vision\n<cit.>. Recently, they have also been applied to reinforcement learning\ntasks. The Decision Transformer <cit.> and the Trajectory\nTransformer <cit.> are trained on an offline dataset of\ntrajectories. The Decision Transformer is conditioned on states, actions, and\nreturns, and outputs optimal actions. The Trajectory Transformer trains a\nsequence model of states, actions, and rewards, and is used for planning.\n<cit.> replace the RNN of Dreamer with a transformer and outperform\nDreamer on Hidden Order Discovery tasks. However, their transformer has no\naccess to previous rewards and they do not evaluate their method on the Atari\n100k benchmark. Moreover, their policy depends on the outputs of the\ntransformer, leading to higher computational costs during inference time.\nConcurrent to and independent from our work, <cit.> apply a transformer to\nsequences of frame tokens and actions and achieve state-of-the-art results on\nthe Atari 100k benchmark.\n\n\n\n\u00a7 CONCLUSION\n\nIn this work, we discuss a reinforcement learning approach using\ntransformer-based world models. Our method (TWM) outperforms previous model-free\nand model-based methods in terms of human normalized score on the 26 games of\nthe Atari 100k benchmark. By using the transformer only during training, we were\nable to keep the computational costs low during inference, i.e., when running\nthe learned policy in a real environment. We show how feeding back the predicted\nrewards into the transformer is beneficial for learning the world model.\nFurthermore, we introduce the balanced cross-entropy loss for finer control over\nthe trade-off between the entropy and cross-entropy terms in the loss functions\nof the world model. A new thresholded entropy loss effectively stabilizes the\nentropy of the policy. Finally, our novel balanced sampling procedure corrects\nissues of naive uniform sampling of past experience.\n\n\n\n\niclr2023_conference\n\n\n\n\u00a7 APPENDIX\n\n\n\n\n \u00a7.\u00a7 Extended Experiments\n \n\n\n\n\n\n\n\n\n  \nAdditional Analysis: \n\n  \n\n  \n  * We provide more example trajectories in <ref>.\n  \n  * We present more attention plots in\n    <ref>. All attention\n    maps are generated using the attention rollout method by\n    <cit.>. Note that we had to modify the method slightly, in\n    order to take the causal masks into account.\n  \n  * Sample Efficiency: We provide the scores of our main\n    experiments after different amounts of interactions with the environment in\n    <ref>. After 50K interactions, our method already\n    has a higher mean normalized score than previous sample-efficient methods.\n    Our mean normalized score is higher than DER, CURL, and SimPLe after 25K\n    interactions. This demonstrates the high sample efficiency of our approach.\n  \n  * Stochasticity: The stochastic prediction of the next state\n    allows the world model to sample a variety of trajectories, even from the\n    same starting state, as can be seen in <ref>.\n  \n  * Long Sequence Imagination: The world model is trained using\n    sequences of length = 16, however, it generalizes well to\n    very long trajectories, as shown in <ref>.\n  \n  * Frame Stacking: In <ref> we visualize the\n    learned stacks of frames. This shows that the world model encodes and\n    predicts the motion of objects.\n\n\n\n\n  \nAdditional Ablation Studies: \n\n  \n\n  \n  * Thresholded Entropy Loss: In <ref> we\n    compare (i) our thresholded entropy loss for the policy (see\n    <ref>) with (ii) the usual entropy penalty. For (i)\n    we use the same hyperparameters as in our main experiments, i.e.,\n    = 0.01 and = 0.1. For (ii) we\n    set = 0.001 and = 1.0, which\n    effectively disables the threshold. Without a threshold, the entropy is more\n    likely to either collapse or diverge. When the threshold is used, the score\n    is higher as well, probably because the entropy is in a more sensible range\n    for the exploration-exploitation trade-off. This cannot be solved by\n    adjusting the penalty coefficient  alone, since it would\n    increase or decrease the entropy in all games.\n  \n  * History Length: We trained our world model with a shorter\n    history and set = 4 instead of = 16. This\n    has a negative impact on the score, as can be seen in\n    <ref>, demonstrating that more time steps into the past\n    are important.\n  \n  * Choice of Policy Input: In <ref> we explained\n    why the input to the policy is only the latent state, i.e., x = z. In\n    <ref> we show that using x = [z, h] can result in lower\n    final scores. We hypothesize that the policy network has a hard time keeping\n    up with the changes of the space of h during training and cannot ignore\n    this additional information.\n  \n  * Increasing the Sample Efficiency: To find out whether we can\n    further increase the sample efficiency shown in\n    <ref>, we train a random subset of games again on\n    10K, 25K, and 50K interactions with the full training budget that we\n    used for the 100K interactions. In <ref> we see\n    that this can lead to significant improvements in some cases, which could\n    mean that the policy benefits from more training on imagined trajectories,\n    but can even lead to worse performance in other cases, which could possibly\n    be caused by overfitting of the world model. When the performance stays the\n    same even with longer training, this could mean that better exploration in\n    the real environment is required to get further improvements.\n\n\n\n\n\n\n  \nWall-Clock Times:\nFor each run, we give the agent a total training and evaluation budget of\nroughly 10 hours on a single NVIDIA A100 GPU. The time can vary slightly,\nsince the budget is based on the number of updates. An NVIDIA GeForce RTX 3090\nrequires 12-13 hours for the same amount of training and evaluation. When using\na vanilla transformer, which does not use the memory mechanism of the\nTransformer-XL architecture <cit.>, the runtime is roughly\n15.5 hours on an NVIDIA A100 GPU, i.e., 1.5 times higher.\n\nWe compare the runtime of our method with previous methods in\n<ref>. Our method is more than 20 times faster than\nSimPLe, but slower than model-free methods. However, our method should be as\nfast as other model-free methods during inference. In\n<ref> we have shown that our method achieves a higher\nhuman normalized score than previous sample-efficient methods after 50K\ninteractions. This suggests that our method could potentially outperform\nprevious methods with shorter training, which would take less than 23.3 hours.\n\nTo determine how time-consuming the individual parts of our method are, we\ninvestigate the throughput of the models, with the batch sizes of our main\nexperiments. The Transformer-XL version is almost twice as fast, which again\nshows the importance of this design choice. The throughputs were measured on an\nNVIDIA A100 GPU and are given in (approximate) samples per second:\n\n  \n  * World model training: 16,800 samples/s\n  \n  * World model imagination (Transformer-XL): 39,000 samples/s\n  \n  * World model imagination (vanilla): 19,900 samples/s\n  \n  * Policy training: 700,000 samples/s\n\nWe also examine how fast the policy can run in an Atari game. We measured the\n(approximate) frames per second on a CPU (since the batch size is 1).\nConditioning the policy on [z, h] is about 3 times slower than z, since\nthe transformer is required:\n\n  \n  * Policy conditioned on z: 653 frames/s\n  \n  * Policy conditioned on [z,h]: 213 frames/s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Derivation of Balanced Cross-Entropy Loss\n \n\n<cit.> propose to use a balanced KL divergence loss to jointly\noptimize the observation encoder q_\u03b8 and state predictor p_\u03b8 with\nshared parameters \u03b8, i.e.,\n\n    \u03bb q_\u03b8p_\u03b8 + (1 - \u03bb)  q_\u03b8p_\u03b8,\n\nwhere \u00b7 denotes the stop-gradient operation and \u03bb\u2208 [0,1]\ncontrols how much the state predictor adapts to the observation encoder and vice\nversa. We use the identity qp = q,p - q, where q,p\nis the cross-entropy of distribution p relative to distribution q, and show\nthat our loss functions lead to the same gradients as the balanced KL objective,\nbut with finer control over the individual components:\n\n    \u2207_\u03b8[ \u03bb q_\u03b8p_\u03b8 + (1 - \u03bb)  q_\u03b8p_\u03b8] \n    \n      =   \u2207_\u03b8[ \u03bb(q_\u03b8,p_\u03b8 - q_\u03b8) + (1 - \u03bb) ( q_\u03b8,p_\u03b8 - q_\u03b8) ] \n    \n      =   \u2207_\u03b8[ \u03bb_1  q_\u03b8,p_\u03b8 + \u03bb_2  q_\u03b8,p_\u03b8 - \u03bb_3  q_\u03b8],\n\nsince \u2207_\u03b8q_\u03b8 = 0 and by defining \u03bb_1 =\n\u03bb and \u03bb_2 = 1 - \u03bb and \u03bb_3 = 1 - \u03bb. In this\nform, we have control over the cross-entropy of the state predictor relative to\nthe observation encoder and vice versa. Moreover, we explicitly penalize the\nentropy of the observation encoder, instead of being entangled inside of the KL\ndivergence.\n\nAs common in the literature, we define the loss function by omitting the\ngradient in <ref>, so that automatic differentiation\ncomputes this gradient. For our world model, we split the objective into two\nloss functions, as the observation encoder and state predictor have separate\nparameters, yielding <ref>.\n\n\n\n \u00a7.\u00a7 Additional Training Details\n\n\nIn <ref> we present pseudocode for training the world model and the\nactor-critic agent. We use the SiLU activation function <cit.> for all\nmodels. In <ref> we summarize all hyperparameters that we\nused in our experiments. In <ref> we provide the number of\nparameters of our models.\n\n\n\n  \nPretraining for Better Initialization:\nDuring training we need to correctly balance the amount of world model training\nand policy training, since the policy has to keep up with the distributional\nshift of the latent space. However, we can spend some extra training time on the\nworld model with pre-collected data (included in the 100K interactions) at the\nbeginning of training in order to obtain a reasonable initialization for the\nlatent states.\n\n\n\n\n\n\n\n"}