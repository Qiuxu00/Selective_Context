{"entry_id": "http://arxiv.org/abs/2303.06930v1", "published": "20230313085347", "title": "Twin Contrastive Learning with Noisy Labels", "authors": ["Zhizhong Huang", "Junping Zhang", "Hongming Shan"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwin Contrastive Learning with Noisy Labels\n\n    Zhizhong Huang^1    Junping Zhang^1    Hongming Shan^2,3Corresponding author\n\n\n^1 Shanghai Key Lab of Intelligent Information Processing, School of Computer Science,\n\nFudan University, Shanghai 200433, China\n\n^2 Institute of Science and Technology for Brain-inspired Intelligence and MOE Frontiers Center \nfor Brain Science,  Fudan University, Shanghai 200433, China\n\n^3 Shanghai Center for Brain Science and Brain-inspired Technology, Shanghai 200031, China\n\n{zzhuang19, jpzhang, hmshan}@fudan.edu.cn\n\n    March 30, 2023\n==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\nLearning from noisy data is a challenging task that significantly degenerates the model performance.\nIn this paper, we present , a novel twin contrastive learning model to learn robust representations and handle noisy labels for classification.\nSpecifically, we construct a Gaussian mixture model (GMM) over the representations by injecting the supervised model predictions into GMM to link label-free latent variables in GMM with label-noisy annotations.\nThen, detects the examples with wrong labels as the out-of-distribution examples by another two-component GMM, taking into account the data distribution.\nWe further propose a cross-supervision with an entropy regularization loss that bootstraps the true targets from model predictions\nto handle the noisy labels.\nAs a result, can learn discriminative representations aligned with estimated labels through mixup and contrastive learning.\nExtensive experimental results on several standard benchmarks and real-world datasets demonstrate the superior performance of . In particular, achieves 7.5% improvements on CIFAR-10 with 90% noisy label\u2014an extremely noisy scenario. \nThe source code is available at <https://github.com/Hzzone/TCL>.\n\n\n\n\n\u00a7 INTRODUCTION\n\nDeep neural networks have shown exciting performance for classification tasks\u00a0<cit.>. Their success largely results from the large-scale curated datasets with clean human annotations, such as CIFAR-10\u00a0<cit.> and ImageNet\u00a0<cit.>, in which the annotation process, however, is tedious and cumbersome. In contrast, one can easily obtain datasets with some noisy annotations\u2014from online shopping websites\u00a0<cit.>, crowdsourcing\u00a0<cit.>, or Wikipedia\u00a0<cit.>\u2014for training a classification neural network. Unfortunately, the mislabelled data are prone to significantly degrade the performance of deep neural networks. Therefore, there is considerable interest in training noise-robust classification networks in recent years\u00a0<cit.>.\n\nTo mitigate the influence of noisy labels, most of the methods in  literature propose the robust loss functions\u00a0<cit.>, reduce the weights of noisy labels\u00a0<cit.>, or correct the noisy labels\u00a0<cit.>.\nIn particular, label correction methods have shown great potential for better performance on the dataset with a high noise ratio. Typically, they correct the labels by using the combination of noisy labels and model predictions\u00a0<cit.>, which usually require an essential iterative sample selection process\u00a0<cit.>. \nFor example, Arazo\u00a0\u00a0<cit.> uses the small-loss trick to carry out sample selection and correct labels via the weighted combination.\nIn recent years, contrastive learning has shown promising results in handling noisy labels\u00a0<cit.>. They usually leverage contrastive learning to learn discriminative representations, and then clean the labels\u00a0<cit.> or construct the positive pairs by introducing the information of nearest neighbors in the embedding space. However, using the nearest neighbors only considers the label noise within a small neighborhood, which is sub-optimal and cannot handle extreme label noise scenarios, as the neighboring examples may also be mislabeled at the same time. \n\nTo address this issue, this paper presents , a novel twin contrastive learning model that explores the label-free unsupervised representations and label-noisy annotations for learning from noisy labels.\nSpecifically, we leverage contrastive learning to learn discriminative image representations in an unsupervised manner and construct a Gaussian mixture model\u00a0(GMM) over its representations. Unlike unsupervised GMM, links the label-free GMM and  label-noisy annotations by replacing the latent variable of GMM with the model predictions for updating the parameters of GMM.\nThen, benefitting from the learned data distribution, we propose to formulate label noise detection as an out-of-distribution\u00a0(OOD) problem, utilizing another two-component GMM to model the samples with clean and wrong labels. The merit of the proposed OOD label noise detection is to take the full data distribution into account, which is robust to the neighborhood with strong label noise.\nFurthermore, we propose a bootstrap cross-supervision with an entropy regulation loss to reduce the impact of wrong labels, in which the true labels of the samples with wrong labels are estimated from another data augmentation.\nLast, to further learn robust representations, we leverage contrastive learning and Mixup techniques to inject the structural knowledge of classes into the embedding space, which helps align the representations with estimated labels.\n\nThe contributions are summarized as follows:\n\n\n\n  * We present , a novel twin contrastive learning model that explores the label-free GMM for unsupervised representations and label-noisy annotations for learning from noisy labels. \n\n\n  * We propose a novel OOD label noise detection method by modeling the data distribution, which excels at handling extremely noisy scenarios.\n\n\n  * We propose an effective cross-supervision, which can bootstrap the true targets with an entropy loss to regularize the model.\n\n\n  * Experimental results on several benchmark datasets and real-world datasets demonstrate that our method outperforms the existing state-of-the-art methods by a significant margin. In particular, we achieve 7.5% improvements in extremely noisy scenarios.\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n  \nContrastive learning.\nContrastive learning methods\u00a0<cit.> have shown promising results for both representation learning and downstream tasks. The popular loss function is InfoNCE loss\u00a0<cit.>, which can pull together the data augmentations from the same example and push away the other negative examples. MoCo\u00a0<cit.> uses a memory queue to store the consistent representations. SimCLR\u00a0<cit.> optimizes InfoNCE within mini-batch and has found some effective training tricks, , data augmentation.\nHowever, as unsupervised learning, they mainly focus on inducing transferable representations for the downstream tasks instead of training with noisy annotations. Although supervised contrastive learning\u00a0<cit.> can improve the representations by human labels, it harms the performance when label noise exists\u00a0<cit.>.\n\n\n\n  \nLearning with noisy labels.\nMost of the methods in literature mitigate the label noise by robust loss functions\u00a0<cit.>, noise transition matrix\u00a0<cit.>, sample selection\u00a0<cit.>, and label correction\u00a0<cit.>. In particular, label correction methods have shown promising results than other methods.\nArazo\u00a0\u00a0<cit.> applied a mixture model to the losses of each sample to distinguish the noisy and clean labels, inspired by the fact that the noisy samples have a higher loss during the early epochs of training.\nSimilarly, DivideMix\u00a0<cit.> employs two networks to perform the sample selection for each other and applies the semi-supervised learning technique where the targets are computed from the average predictions of different data augmentations.\nDue to the success of contrastive learning, many attempts have been made to improve the robustness of classification tasks by combining the advantages of contrastive learning. Zheltonozhskii\u00a0\u00a0<cit.> used contrastive learning to pre-train the classification model. MOIT\u00a0<cit.> quantifies this agreement between feature representation and original label to identify mislabeled samples by utilizing a k-nearest neighbor (k-NN) search. RRL\u00a0<cit.> performs label cleaning by two thresholds on the soft label, which is calculated from the predictions of previous epochs and its nearest neighbors. Sel-CL\u00a0<cit.> leverages the nearest neighbors to select confident pairs for supervised contrastive learning\u00a0<cit.>.\n\nUnlike existing methods\u00a0<cit.> that detect the wrong labels within the neighborhood, formulates the wrong labels as the out-of-distribution examples by modeling the data distribution of representations learned by contrastive learning. In addition, we propose a cross-supervision with entropy regularization to better estimate the true labels and handle the noisy labels.\n\n\n\n\n\n\u00a7 THE PROPOSED \n\n\nEach image in dataset \ud835\udc9f={x_i}_i=1^N associates with an annotation y\u2208{1, 2,\u2026, K}. In practice, some examples may be mislabeled. We aim to train a classification network, p_\u03b8(y|x) = g(x;\u03b8) \u2208\u211d^K, that is resistant to the noisy labels in training data, and generalizes well on the clean testing data.\nFig.\u00a0<ref> illustrates the framework of our proposed . \n\n\n\n  \nOverview.\nIn the context of our framework, f(\u00b7) and g(\u00b7) share the same backbone and have additional individual heads to output representations and class predictions from two random and one mixup data augmentations. Afterward, there are four components in , including (i) modeling the data distribution via a GMM in Sec.\u00a0<ref> from the model predictions and representations; (ii) detecting the examples with wrong labels as out-of-distribution samples in Sec.\u00a0<ref>; (iii) cross-supervision by bootstrapping the true targets in Sec.\u00a0<ref>; and (iv) learning robust representations through contrastive learning and mixup in Sec.\u00a0<ref>.\n\n\n\n \u00a7.\u00a7 Modeling Data Distribution\n\n\nGiven the image dataset consisting of N images, we opt to model the distribution of x over its representation v = f(x) via a spherical Gaussian mixture model (GMM).\nAfter introducing discrete latent variables z\u2208{1, 2,\u2026, K} that determine the assignment of observations to mixture components, the unsupervised GMM can be defined as\n\n    p(v )    = \u2211_k=1^K p(v, z=k) \n       =\u2211_k=1^K p(z=k) \ud835\udca9(v | \u03bc_k, \u03c3_k).\n\nwhere \u03bc_k is the mean and \u03c3_k a scalar deviation.\nIf we assume that the latent variables z are uniform distributed, that is, p(z=k) = 1/K, we can define the posterior probability that assigns x_i to k-th cluster:\n\n    \u03b3_ik=p(z_i=k|x_i)\u221d\ud835\udca9(x_i|\u03bc_k,\u03c3_k).\n\nIn an ideal scenario where all the samples have clean labels y\u2208{1, 2,\u2026, K}, the discrete latent variables z would be identical to the annotation y, and the parameters \u03bc_k, \u03c3_k and latent variable z can be solved through a standard Expectation-Maximization (EM) algorithm\u00a0<cit.>.\n\nHowever, in practice, the labels are often noisy and the latent variable z, estimated in an unsupervised manner, has nothing to do with the label y. Therefore, we are interested in connecting latent variable z, estimated in an unsupervised fashion  (\u00a0label-free), and the available annotations y, label-noisy, for the task of learning from noisy labels. \n\nTo link them together, we propose to inject the model predictions p_\u03b8(y_i = k|x_i), learned from noisy labels, into the latent variables z. Specifically, we propose to replace the unsupervised assignment p(z_i=k|x_i) with noisy-supervised assignment p_\u03b8(y_i=k|x_i). As a result, we can connect the latent variable z with the label y, and thus use the noisy supervision to guide the update of the parameters of GMM. In particular, the update of the GMM parameters becomes\n\n    \u03bc_k    = norm(\u2211_i p_\u03b8(y_i = k|x_i) v_i/\u2211_i p_\u03b8(y_i = k|x_i)),\n         \n    \u03c3_k    = \u2211_i p_\u03b8(y_i = k|x_i) (v_i - \u03bc_k)(v_i - \u03bc_k)/\u2211_i p_\u03b8(y_i = k|x_i),\n\nwhere norm(\u00b7) is \u2113_2-normalization such that \u03bc_k_2=1.\n\n\n\n\n \u00a7.\u00a7 Out-Of-Distribution Label Noise Detection\n\n\n\nPrevious works\u00a0<cit.> typically detect the wrong labels within the neighborhood, that is, using the information from nearest neighbors. It is limited as the neighboring examples are usually mislabeled at the same time. To address this issue, we propose to formulate label noise detection as to detect the out-of-distribution examples.\n\nAfter building the connection between the latent variables z and labels y, we are able to detect the sample with wrong labels through the posterior probability in Eq.\u00a0(<ref>). We implement it as a  normalized version to take into account the intra-cluster distance, which allows for detecting the samples with likely wrong labels:\n\n    \u03b3_ik=exp(- (v_i-\u03bc_k)(v_i-\u03bc_k) / 2\u03c3_k)/\u2211_k exp(- (v_i-\u03bc_k)(v_i-\u03bc_k) / 2\u03c3_k).\n\nSince \u2113_2-normalization has been applied to both embeddings v and the cluster centers \u03bc_k, yielding (v-\u03bc_k)(v-\u03bc_k)=2-2v\u03bc_k. Therefore, we can re-write Eq.\u00a0(<ref>) as:\n\n    \u03b3_ik   =p(z_i=k|x_i)\n       =exp(v_i\u03bc_k / \u03c3_k) / \u2211_k exp(v_i\u03bc_k/\u03c3_k).\n\n\nOnce built the GMM over the distribution of representations, we propose to formulate the conventional noisy label detection problem  as out-of-distribution sample detection problem. Our idea is that the samples with clean labels should have the same cluster indices after linking the cluster index and class label. Specifically, given one particular class y=k, the samples within this class can be divided into two types: in-distribution samples with clean labels, and out-of-distribution samples with wrong labels. Therefore, we define the following conditional probability to measure the probability of one sample with clean label:\n\n    \u03b3_y=z|i   =p(y_i=z_i|x_i)\n       =exp(v_i\u03bc_z_i / \u03c3_z_i) / \u2211_k exp(v_i\u03bc_k/\u03c3_k).\n\nAlthough Eqs.\u00a0(<ref>) and\u00a0(<ref>) share similar calculations, they have different meanings. Eq.\u00a0(<ref>) calculates the probability of one example belonging to k-th cluster while Eq.\u00a0(<ref>) the probability of one example having clean label\u2014that is, y_i=z_i. Therefore, the probability of one example having the wrong label can be written as \u03b3_y\u2260 z|i= p(y_i\u2260 z_i|x_i) = 1 - p(y_i=z_i|x_i). \n\nFurthermore, instead of setting a human-tuned threshold for \u03b3_y=z|i, we opt to employ another two-component GMM following\u00a0<cit.> to automatically estimate the clean probability \u03b3_y=z|i for each example. Similar to the definition of GMM in Eq.\u00a0(<ref>), this two-components GMM is defined as follows:\n\n    p(\u03b3_y=z|i)=\u2211_c=0^1p(\u03b3_y=z|i, c) = \u2211_c=0^1 p(c)p(\u03b3_y=z|i| c),\n\nwhere c is the new introduced latent variable: c=1 indicates the cluster of clean labels with higher mean value and vice versus c=0. After modeling the GMM over the probability of one example having clean labels, \u03b3_y=z|i, we are able to infer the posterior probability of one example having clean labels through the two-component GMM.\n\n\n\n \u00a7.\u00a7 Cross-supervision with Entropy Regularization\n\n\nAfter the label noise detection, the next important step is to estimate the true targets by correcting the wrong label to reduce its impact, called label correction.\nPrevious works usually perform label correction using the temporal ensembling\u00a0<cit.> or from the model predictions\u00a0<cit.> before mixup augmentation without back-propagation.\n\nleverages a similar idea to bootstrap the targets through the convex combination of its noisy labels and the predictions from the model itself:\n\n    t^(1)_i = w_i y_i + (1 - w_i) g(x^(1)_i)\n    t^(2)_i = w_i y_i + (1 - w_i) g(x^(2)_i) \n        ,\n\nwhere g(x^(1)_i) and g(x^(2)_i) are the predictions of two augmentations, y_i the noisy one-hot label, and w_i\u2208[0, 1] represents the posterior probability as p(c=1|\u03b3_y=z|i) from the two-component GMM defined in Eq.\u00a0(<ref>). When computing Eq.\u00a0(<ref>), we stop the gradient from g to avoid the model predictions collapsed into a constant, inspired by\u00a0<cit.>.\n\nGuided by the corrected labels t_i, we swap two augmentations to compute the classification loss twice, leading to the bootstrap cross supervision, formulated as:\n\n    \u2112_cross = \u2113(g(x_i^(1)), t^(2)_i) + \u2113(g(x_i^(2)), t^(1)_i),\n\nwhere \u2113 is the cross-entropy loss. This loss makes the predictions of the model from two data augmentations close to corrected labels from each other. In a sense, if w_i=0, the model is encouraged for consistent class predictions between different data augmentations, otherwise w_i=1 it is supervised by the clean labels.\n\nIn addition, we leverage an additional entropy regularization loss on the predictions within a mini-batch \u212c:\n\n    \u2112_reg   = - \u210d(1/|\u212c|\u2211_x\u2208\u212c  g(x)) +1/|\u212c|\u2211_x\u2208\u212c\u210d(g(x)),\n\nwhere \u210d(\u00b7) is the entropy of predictions\u00a0<cit.>. The first term can avoid the predictions collapsing into a single class by maximizing the entropy of average predictions.\nThe second term is the minimum entropy regularization to encourage the model to have high confidence for predictions, which was previously studied in semi-supervised learning literature\u00a0<cit.>.\n\nAlthough both using the model predictions, we would emphasize that the cross-supervision in is different to\u00a0<cit.> in three aspects: (i) both x^(1)_i and x^(2)_i are involved in back-propagation; (ii) the strong augmentation\u00a0<cit.> used to estimate the true targets can prevent the overfitting of estimated targets; and (iii) employs two entropy regularization terms to avoid the model collapse to one class.\n\nThe final classification loss is given as follows:\n\n    \u2112_cls = \u2112_cross + \u2112_reg.\n\n\n\n\n \u00a7.\u00a7 Learning Robust Representations\n\n\nTo model the data distribution that is robust to noisy labels, we leverage contrastive learning to learn the representations of images. Specifically,\ncontrastive learning performs instance-wise discrimination\u00a0<cit.> using the InfoNCE loss\u00a0<cit.> to enforce the model outputting similar embeddings for the images with semantic preserving perturbations.\nFormally, the contrastive loss is defined as follows:\n\n    \u2112_ctr = -logexp( f(x^(1)) f(x^(2)) / \u03c4)/\u2211_x\u2208\ud835\udcaeexp(f(x^(1)) f(x) / \u03c4),\n\nwhere \u03c4 is the temperature and \ud835\udcae is the \u212c except x^(1). x^(1) and x^(2) are two\naugmentations of x.\nIntuitively, InfoNCE loss aims to pull together the positive pair (x^(1), x^(2)) from two different augmentations of the same instance, and push them away from negative examples of other instances. Consequently, it can encourage discriminative representations in a pure unsupervised, or label-free manner.\n\nAlthough beneficial in modeling latent representations, contrastive learning cannot introduce compact classes without using the true labels. Since the label y is noisy, we leverage Mixup\u00a0<cit.> to improve within-class compactness, which has been shown its effectiveness against label noise in literature\u00a0<cit.>. Specifically,\na mixup training pair (x^(m)_i, t\u0305^(m)_i) is linearly interpolated between (x_i, t\u0305_i) and (x_j, t\u0305_j) under a control coefficient \u03bb\u223cBeta(\u03b1,\u03b1):\n\n    x^(m)_i = \u03bbx_i + (1 - \u03bb) x_j,\n    t\u0305^(m)_i = \u03bbt\u0305_i + (1 - \u03bb) t\u0305_j,\n\nwhere x_j is randomly selected within a mini-batch, and t\u0305_i=(t^(1)_i+t^(2)_i)/2 is the average of estimated true labels of two data augmentations. Intuitively, we can inject the structural knowledge of classes into the embedding space learned by contrastive learning. This loss can be written as:\n\n    \u2112_align = \u2113(g(x^(m)_i), t\u0305^(m)_i) + \u2113(p(z|x^(m)_i), t\u0305^(m)_i),\n\nwhere the second term can align the representations with estimated labels. In a sense, \u2112_align regularizes classification network g and encourages f to learn compact and well-separated representations.\nFurthermore, we would point out two differences between and\u00a0<cit.>, although both using mixup to boost the representations. First,\u00a0<cit.> does not explicitly model the data distribution p(z|x^(m)_i) like . Second, has leveraged the full training dataset via the corrected label t\u0305^(m)_i instead of a subset of clean examples in\u00a0<cit.>, which leads to stronger robustness of over\u00a0<cit.> on extreme high label noise ratios.\n\n\n\n \u00a7.\u00a7 Training and inference\n\n\n\n\n\nThe overall training objective is to minimize the sum of all losses:\n\n    \u2112 = \u2112_cls + \u2112_ctr + \u2112_align.\n\nWe find that a simple summation of all losses works well for all datasets and noise levels, which indicates the strong generalization of the proposed method. During inference, the data augmentations are disabled and the class predictions are obtained by argmax_k p_\u03b8(k |\ud835\udc31).\n\nThe training algorithm of the proposed method is shown in Alg.\u00a0<ref>. In a sense, the architecture of our method leads to an EM-like algorithm: (1) the E-step updates {(\u03bc_k, \u03c3_k)}_k=1^K for , and {w_i}_i=1^N for each sample in \ud835\udc9f to form the true targets with the predictions from another data augmentations, and (2) the M-step optimizes the model parameters by Eq.\u00a0(<ref>) to better fit those estimated targets.\nTherefore, the convergence of can be theoretically guaranteed, following the standard EM algorithm. \n\n\n\n\u00a7 EXPERIMENTS\n\n\nIn this section, we conduct experiments on multiple benchmark datasets with simulated and real-world label noises. We strictly follow the experimental settings in previous literature\u00a0<cit.> for fair comparisons.\n\n\n\n \u00a7.\u00a7 Experiments on simulated datasets\n\n\n\n\n  \nDatasets. \nFollowing\u00a0<cit.>, we validate our method on CIFAR-10/100\u00a0<cit.>, which contains 50K and 10K images with size 32\u00d7 32 for training and testing, respectively. We leave 5K images from the training set as the validation set for hyperparameter tuning, then train the model on the full training set for fair comparisons.\nTwo types of label noise are simulated: symmetric and asymmetric label noise. Symmetric noise randomly assigns the labels of the training set to random labels with predefined percentages, a.k.a, noise ratio, which includes 20%, 50%, 80%, and 90% on two datasets in this paper. Asymmetric noise takes into account the class semantic information, and the labels are only changed to similar classes (, truck \u2192 automobile). Here, only experiments on the CIFAR-10 dataset with 40% noise ratio for asymmetric noise are conducted; otherwise, the classes with above 50% label noise cannot be distinguished.\n\n\n\n\n\n\n\n\n  \nTraining details. \nSame as previous works\u00a0<cit.>, we use a PreAct ResNet-18\u00a0<cit.> as the encoder. We adopt SGD optimizer to train our model with a momentum of 0.9, a weight decay of 0.001, and a batch size of 256 for 200 epochs. The learning rate are linearly warmed up to 0.03 for 20 epochs and decayed with the cosine schedule.\nThe data augmentations of\u00a0<cit.> are applied to two views\u00a0(ResizedCrop, ColorJitter, and etc). Only crop and horizontal flip are employed for mixup. Both projection and classification heads are a two-layer MLP with the dimension 128 and the number of classes. The temperature \u03c4 of contrastive loss and the \u03b1 of mixup are 0.25 and 1. The settings are shared for all experiments, which are significantly different from\u00a0<cit.> that adopt specific configurations for different datasets and even for different noise ratios/types.\n\n\n\n  \nQuantitative results.\nTable\u00a0<ref> presents the the comparisons with existing state-of-the-art methods. Our method yields competitive performance on low noise ratios, but promising improvements over recent methods on extreme noise ratios and the most challenging CIFAR-100 dataset with 100 classes. In particular, with 90% label noise, there are 7.5% and 5.7% improvements on for CIFAR-10 and CIFAR-100, respectively. We stress that the hyperparameters are consistent for different noise ratios/types. In practical scenarios, the noise ratio for a particular dataset is unknown, so it is hard to tune the hyper-parameters for better performance. Therefore, these results indicate the strong generalization ability of our method regardless of noise ratios/types.\n\nFor fair comparisons, following\u00a0<cit.>, we performed extra experiments on fine-tuning the classification network for 70 epochs with the detected clean samples and mixup augmentation, termed TCL+. Table\u00a0<ref> shows that under low label noise\u00a0(below 50%), TCL+ achieves significant improvements over TCL and outperforms the recent state-of-the-art methods. The benefits from the detected clean subset and longer training, which can fully utilize the useful supervision signals from labeled examples.\n\n\nIn Appendix\u00a0<ref>, we also perform the k-NN classification over the learned representations, which indicates that our method has maintained meaningful representations better than the pure unsupervised learning model.\nIn Appendix\u00a0<ref>, we provide more experimental results and analysis on asymmetric label noise and imbalance data.\n\n\n\n\n\n\n  \nQualitative results. Figs.\u00a0<ref>(a) and (b) visualize the learned representations with extremely high noise ratio, demonstrating that our method can produce distinct structures of learned representations with meaningful semantic information. Especially, Fig.\u00a0<ref>(b) presents the samples with noisy labels in the embedding space, in which the label noise can be accurately detected by our proposed method. In addition, by visualizing the histogram of p(y=z|x) for the training set in Fig.\u00a0<ref>(c), we confirm that the proposed method can effectively distinguish the samples noisy and clean labels.\nWe visualize the validation accuracy across training in Fig.\u00a0<ref>(d). As expected, performs stable even with the extreme 90% label noise.\n\n\n\n \u00a7.\u00a7 Ablation study\n\n\nWe conduct ablation studies to validate our motivation and design with the following baselines, and the results are shown in Table\u00a0<ref>.\n\n\n\n  *  Baseline.  We start the baseline method by removing the proposed noisy label detection and bootstrap cross-supervision, where the model is directly guided by noisy labels.  As expected, the performance significantly degrades for the extremely high noise ratio\u00a0(, 90%).\n\n\n\n  *  Label Noise Detection.  We assess the effectiveness of different detection methods including the cross-entropy loss\u00a0<cit.>, k-NN search\u00a0<cit.>, and our out-of-distribution\u00a0(OOD) detection. For fair comparisons, the predictions from the images before mixup are employed as the true labels in Eq.\u00a0(<ref>).\nObviously, the label noise detection has alleviated the degeneration to some degree\u00a0(Exp.\u00a0<ref>), where our method consistently outperforms other baselines.\nFig.\u00a0<ref> visualizes their AUCs across training.\nThe proposed OOD detection is better at distinguishing clean and wrong labels. Thanks to the representations learned by contrastive learning, k-NN search performs better than cross-entropy loss. However, it is limited due to the use of the original labels to detect noisy ones, while our method constructs a GMM using the model predictions.\n\n\n\n\n  *  Target Estimation.  Another key component is the cross-supervision that bootstraps the true targets from the predictions of another data augmentation. We replace it with the temporal ensembling\u00a0<cit.>, where the hyperparameters are set as suggested by\u00a0<cit.>. Furthermore, Exp.\u00a0<ref> estimates true targets from the images before mixup\u00a0<cit.>. The results suggest that our bootstrap cross-supervision has shown strong robustness on 90% label noise.\n\n\n\n\n  *  Without \u2112_reg.  We remove \u2112_reg and the results indicate that it plays an important role, especially on extremely high label noise. Removing each term in \u2112_reg obtains similar results. We argue that \u2112_reg works in two aspects: 1) it can avoid the model collapse which outputs single classes, and 2) it can encourage the model to have high confidence for the predictions, which has shown its effectiveness for unlabeled data in semi-supervised learning.\n\n  *  Without \u2112_align.  We remove \u2112_align and the performance has decreased, as expected, but is still more promising than other baselines. \u2112_align has leveraged mixup augmentation to regularize both classification and representation learning. Appendix\u00a0<ref> shows the evaluation of k-NN classification, demonstrating that \u2112_align can also greatly improve the learned representations.\n\n  *  Contrastive Framework.  We implement into another contrastive framework for representation learning, , MoCo\u00a0<cit.>. Based on the MoCo framework, our method has achieved more improvements in various experiments, which benefits from a large number of negative examples in the memory queue and a moving-averaged encoder\u00a0(we set the queue size and the factor of moving-average to 4,096 and 0.99, respectively).\n\n\n\n\n\n\n\n\n  \nHyperparameters.\nWe evaluate the most essential hyperparameters  to our design, including the temperature \u03c4 for contrastive loss and update frequency for on CIFAR-10 with 90% symmetric noise. Here, the update frequency denotes how may epochs that we update the parameters of , {(\u03bc_k, \u03c3_k)}_k=1^K and {w_i}_i=1^N.\nFig.\u00a0<ref> shows that our method is robust to different choices of hyperparameters.\nEven though updates for every 32 epochs, our method has still performed well, which indicates that the computational cost can be significantly reduced.\n\n\n\n \u00a7.\u00a7 Results on real-world datasets\n\n\n\n\n  \nDatasets and training details. \nWe validate our method on two real-word noisy datasets: WebVision\u00a0<cit.> and Clothing1M\u00a0<cit.>. Webvision contains millions of noisily-labeled images collected from the web using the keywords of ImageNet ILSVRC12\u00a0<cit.>. Following the convension\u00a0<cit.>, we conducted experiments on the first 50 classes of the Google image subset, termed WebVision\u00a0(mini) and evaluated on both WebVision and ImageNet validation set. Clothing1M consists of 14 kinds of images\ncollected from online shopping websites. Only the noisy training set is used in our experiments.\nWe used a batch size of 256 on 4 GPUs, and trained a ResNet-50 for 40 epochs\u00a0(without warm-up) on Clothing1M and a ResNet-18 for 130 epochs\u00a0(warm-up 10 epochs) on WebVision, respectively. Following\u00a0<cit.>, for Clothing1M, the encoder is initialized with ImageNet pre-trained weights, the initial learning rate is 0.01, and 256 mini-batches are sampled as one epoch. Other hyper-parameters are kept to be the same without further tuning.\n\n\n\n  \nQuantitative results. \nTables\u00a0<ref> and\u00a0<ref> present the results on WebVision and Clothing1M datasets. Our method outperforms state-of-the-art methods on both datasets, demonstrating its superior performance in handling real-world noisy datasets. We note that after checking the Clothing1M dataset, there are still lots of mislabeled images in the testing set. Therefore, the results on Clothing1M may not be such reliable as other datasets to evaluate the true performance of different methods.\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\nIn this paper, we introduced , a novel twin contrastive learning model for learning from noisy labels. By connecting the label-free latent variables and label-noisy annotations, can effectively detect the label noise and accurately estimate the true labels. \nExtensive experiments on both simulated and real-world datasets have demonstrated the superior performance of than existing state-of-the-art methods. \nIn particular, achieves 7.5% performance improvement under extremely 90% noise ratio.\nIn the future, we will improve with semantic information for low noise ratios and explore dynamically updating the GMM.\n\n\n\nieee_fullname\n\n10=-1pt\n\n    arazo2019unsupervised\n    Eric Arazo, Diego Ortego, Paul Albert, Noel O\u2019Connor, and Kevin McGuinness.\n    Unsupervised label noise modeling and loss correction.\n    In International Conference on Machine Learning, pages\n      312\u2013321. PMLR, 2019.\n    \n    chen2019understanding\n    Pengfei Chen, Ben\u00a0Ben Liao, Guangyong Chen, and Shengyu Zhang.\n    Understanding and utilizing deep neural networks trained with noisy\n      labels.\n    In International Conference on Machine Learning, pages\n      1062\u20131070. PMLR, 2019.\n    \n    chen2020simple\n    Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.\n    A simple framework for contrastive learning of visual\n      representations.\n    In International Conference on Machine Learning, pages\n      1597\u20131607. PMLR, 2020.\n    \n    chen2021exploring\n    Xinlei Chen and Kaiming He.\n    Exploring simple Siamese representation learning.\n    In Proceedings of the IEEE/CVF Conference on Computer Vision and\n      Pattern Recognition, pages 15750\u201315758, 2021.\n    \n    dempster1977maximum\n    Arthur\u00a0P Dempster, Nan\u00a0M Laird, and Donald\u00a0B Rubin.\n    Maximum likelihood from incomplete data via the EM algorithm.\n    Journal of the Royal Statistical Society: Series B\n      (Methodological), 39(1):1\u201322, 1977.\n    \n    deng2009imagenet\n    Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.\n    ImageNet: A large-scale hierarchical image database.\n    In 2009 IEEE Conference on Computer Vision and Pattern\n      Recognition, pages 248\u2013255. Ieee, 2009.\n    \n    ghosh2017robust\n    Aritra Ghosh, Himanshu Kumar, and PS Sastry.\n    Robust loss functions under label noise for deep neural networks.\n    In Proceedings of the AAAI Conference on Artificial\n      Intelligence, 2017.\n    \n    goldberger2016training\n    Jacob Goldberger and Ehud Ben-Reuven.\n    Training deep neural-networks using a noise adaptation layer.\n    In International Conference on Learning Representations, 2017.\n    \n    grandvalet2004semi\n    Yves Grandvalet and Yoshua Bengio.\n    Semi-supervised learning by entropy minimization.\n    In Advances in Neural Information Processing Systems, 2004.\n    \n    grill2020bootstrap\n    Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec,\n      Pierre\u00a0H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo\u00a0Avila Pires,\n      Zhaohan\u00a0Daniel Guo, Mohammad\u00a0Gheshlaghi Azar, et\u00a0al.\n    Bootstrap your own latent: A new approach to self-supervised\n      learning.\n    In Advances in Neural Information Processing Systems, 2020.\n    \n    han2018co\n    Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and\n      Masashi Sugiyama.\n    Co-teaching: Robust training of deep neural networks with extremely\n      noisy labels.\n    In Advances in Neural Information Processing Systems,\n      volume\u00a031, 2018.\n    \n    he2020momentum\n    Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\n    Momentum contrast for unsupervised visual representation learning.\n    In Proceedings of the IEEE/CVF Conference on Computer Vision and\n      Pattern Recognition, pages 9729\u20139738, 2020.\n    \n    he2016deep\n    Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n    Deep residual learning for image recognition.\n    In Proceedings of the IEEE Conference on Computer Vision and\n      Pattern Recognition, pages 770\u2013778, 2016.\n    \n    he2016identity\n    Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n    Identity mappings in deep residual networks.\n    In European Conference on Computer Vision, pages 630\u2013645.\n      Springer, 2016.\n    \n    jiang2020beyond\n    Lu Jiang, Di Huang, Mason Liu, and Weilong Yang.\n    Beyond synthetic noise: Deep learning on controlled noisy labels.\n    In International Conference on Machine Learning, 2020.\n    \n    jiang2018mentornet\n    Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei.\n    MentorNet: Learning data-driven curriculum for very deep neural\n      networks on corrupted labels.\n    In International Conference on Machine Learning, 2018.\n    \n    khosla2020supervised\n    Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip\n      Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan.\n    Supervised contrastive learning.\n    In Advances in Neural Information Processing Systems, 2020.\n    \n    kim2021fine\n    Taehyeon Kim, Jongwoo Ko, JinHwan Choi, Se-Young Yun, et\u00a0al.\n    Fine samples for learning with noisy labels.\n    In Advances in Neural Information Processing Systems, 2021.\n    \n    krizhevsky2009learning\n    Alex Krizhevsky, Geoffrey Hinton, et\u00a0al.\n    Learning multiple layers of features from tiny images.\n    2009.\n    \n    li2020dividemix\n    Junnan Li, Richard Socher, and Steven\u00a0CH Hoi.\n    Dividemix: Learning with noisy labels as semi-supervised learning.\n    In International Conference on Learning Representations, 2020.\n    \n    li2021learning\n    Junnan Li, Caiming Xiong, and Steven\u00a0CH Hoi.\n    Learning from noisy data with robust representation learning.\n    In Proceedings of the IEEE/CVF International Conference on\n      Computer Vision, pages 9485\u20139494, 2021.\n    \n    li2020mopro\n    Junnan Li, Caiming Xiong, and Steven\u00a0CH Hoi.\n    Mopro: Webly supervised learning with momentum prototypes.\n    In International Conference on Learning Representations, 2021.\n    \n    li2022selective\n    Shikun Li, Xiaobo Xia, Shiming Ge, and Tongliang Liu.\n    Selective-supervised contrastive learning with noisy labels.\n    In Proceedings of the IEEE/CVF Conference on Computer Vision and\n      Pattern Recognition, pages 316\u2013325, 2022.\n    \n    li2017webvision\n    Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van\u00a0Gool.\n    Webvision database: Visual learning and understanding from web data.\n    arXiv preprint arXiv:1708.02862, 2017.\n    \n    liu2020early\n    Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda.\n    Early-learning regularization prevents memorization of noisy labels.\n    In Advances in Neural Information Processing Systems, 2020.\n    \n    ma2018dimensionality\n    Xingjun Ma, Yisen Wang, Michael\u00a0E Houle, Shuo Zhou, Sarah Erfani, Shutao Xia,\n      Sudanthi Wijewickrema, and James Bailey.\n    Dimensionality-driven learning with noisy labels.\n    In International Conference on Machine Learning, pages\n      3355\u20133364. PMLR, 2018.\n    \n    malach2017decoupling\n    Eran Malach and Shai Shalev-Shwartz.\n    Decoupling \u201cwhen to update\" from \u201chow to update\".\n    In Advances in Neural Information Processing Systems, 2017.\n    \n    oord2018representation\n    Aaron van\u00a0den Oord, Yazhe Li, and Oriol Vinyals.\n    Representation learning with contrastive predictive coding.\n    arXiv preprint arXiv:1807.03748, 2018.\n    \n    ortego2021multi\n    Diego Ortego, Eric Arazo, Paul Albert, Noel\u00a0E O'Connor, and Kevin McGuinness.\n    Multi-objective interpolation training for robustness to label noise.\n    In Proceedings of the IEEE/CVF Conference on Computer Vision and\n      Pattern Recognition, pages 6606\u20136615, 2021.\n    \n    patrini2017making\n    Giorgio Patrini, Alessandro Rozza, Aditya Krishna\u00a0Menon, Richard Nock, and\n      Lizhen Qu.\n    Making deep neural networks robust to label noise: A loss correction\n      approach.\n    In Proceedings of the IEEE Conference on Computer Vision and\n      Pattern Recognition, 2017.\n    \n    reed2014training\n    Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan,\n      and Andrew Rabinovich.\n    Training deep neural networks on noisy labels with bootstrapping.\n    arXiv preprint arXiv:1412.6596, 2014.\n    \n    Rothe-IJCV-2018\n    Rasmus Rothe, Radu Timofte, and Luc\u00a0Van Gool.\n    Deep expectation of real and apparent age from a single image without\n      facial landmarks.\n    International Journal of Computer Vision, 126(2-4):144\u2013157,\n      2018.\n    \n    shannon2001mathematical\n    Claude\u00a0Elwood Shannon.\n    A mathematical theory of communication.\n    The Bell system technical journal, 27(3):379\u2013423, 1948.\n    \n    tanaka2018joint\n    Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa.\n    Joint optimization framework for learning with noisy labels.\n    In Proceedings of the IEEE Conference on Computer Vision and\n      Pattern Recognition, pages 5552\u20135560, 2018.\n    \n    tanno2019learning\n    Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel\u00a0C Alexander, and\n      Nathan Silberman.\n    Learning from noisy labels by regularized estimation of annotator\n      confusion.\n    In Proceedings of the IEEE/CVF Conference on Computer Vision and\n      Pattern Recognition, pages 11244\u201311253, 2019.\n    \n    wang2019imae\n    Xinshao Wang, Yang Hua, Elyor Kodirov, and Neil\u00a0M Robertson.\n    IMAE for noise-robust learning: Mean absolute error does not treat\n      examples equally and gradient magnitude's variance matters.\n    arXiv preprint arXiv:1903.12141, 2019.\n    \n    wang2019symmetric\n    Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey.\n    Symmetric cross entropy for robust learning with noisy labels.\n    In Proceedings of the IEEE/CVF International Conference on\n      Computer Vision, pages 322\u2013330, 2019.\n    \n    wu2018unsupervised\n    Zhirong Wu, Yuanjun Xiong, Stella\u00a0X Yu, and Dahua Lin.\n    Unsupervised feature learning via non-parametric instance\n      discrimination.\n    In Proceedings of the IEEE Conference on Computer Vision and\n      Pattern Recognition, pages 3733\u20133742, 2018.\n    \n    xia2019anchor\n    Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and\n      Masashi Sugiyama.\n    Are anchor points really indispensable in label-noise learning?\n    In Advances in Neural Information Processing Systems, 2019.\n    \n    xiao2015learning\n    Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang.\n    Learning from massive noisy labeled data for image classification.\n    In Proceedings of the IEEE Conference on Computer Vision and\n      Pattern Recognition, pages 2691\u20132699, 2015.\n    \n    xu2019l_dmi\n    Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang.\n    L_DMI: A novel information-theoretic loss function for training\n      deep nets robust to label noise.\n    In Advances in Neural Information Processing Systems, 2019.\n    \n    yan2014learning\n    Yan Yan, R\u00f3mer Rosales, Glenn Fung, Ramanathan Subramanian, and Jennifer\n      Dy.\n    Learning from multiple annotators with varying expertise.\n    Machine Learning, 95(3):291\u2013327, 2014.\n    \n    yi2019probabilistic\n    Kun Yi and Jianxin Wu.\n    Probabilistic end-to-end noise correction for learning with noisy\n      labels.\n    In Proceedings of the IEEE/CVF Conference on Computer Vision and\n      Pattern Recognition, pages 7017\u20137025, 2019.\n    \n    yu2019does\n    Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama.\n    How does disagreement help generalization against label corruption?\n    In International Conference on Machine Learning, pages\n      7164\u20137173. PMLR, 2019.\n    \n    yu2018learning\n    Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao.\n    Learning with biased complementary labels.\n    In Proceedings of the European Conference on Computer Vision,\n      pages 68\u201383, 2018.\n    \n    zhang2017mixup\n    Hongyi Zhang, Moustapha Cisse, Yann\u00a0N Dauphin, and David Lopez-Paz.\n    mixup: Beyond empirical risk minimization.\n    In International Conference on Learning Representations, 2018.\n    \n    zhang2018generalized\n    Zhilu Zhang and Mert Sabuncu.\n    Generalized cross entropy loss for training deep neural networks with\n      noisy labels.\n    In Advances in Neural Information Processing Systems, 2018.\n    \n    zheltonozhskii2022contrast\n    Evgenii Zheltonozhskii, Chaim Baskin, Avi Mendelson, Alex\u00a0M Bronstein, and Or\n      Litany.\n    Contrast to divide: Self-supervised pre-training for learning with\n      noisy labels.\n    In Proceedings of the IEEE/CVF Winter Conference on Applications\n      of Computer Vision, pages 1657\u20131667, 2022.\n    \n    \n\n\n\n\n\n\n\ntocsectionAppendix\nequationsection\n\n\n \n\n\n\n\u00a7 K-NN EVALUATION\n\n\n\nWe perform the k-NN classification over the learned representations with k=200.\nFor comparisons, we removed all proposed components and reported the performance on the representations learned in a pure unsupervised manner. The clean labels are involved in testing but excluded in the training phase.\nThe results are shown in Tables\u00a0<ref> and\u00a0<ref>.\n\nThe representations learned by our method have consistently outperformed the unsupervised learning, regardless of label noise with different ratios. These results indicate that our method has maintained meaningful representations better than the pure unsupervised learning model.\n\n\n\n\n\n\n\n\u00a7 ASYMMETRIC LABEL NOISE\n\n\n\nTable\u00a0<ref> shows the results of TCL and TCL+ for CIFAR-10/100 under different asymmetric ratios following\u00a0<cit.>, where our method has consistently outperformed the competitors.\n\nWe note that, unlike symmetric label noise, the classes with above 50% asymmetric label noise cannot be distinguished, which makes 40% becomes the most extreme scenario. In addition, we found that the asymmetric label noise would make the dataset imbalance, where the assumption of uniform distribution in Sec.\u00a0<ref> does not hold.\n\nHere, we employ the class imbalance ratio r=max({N_z}_z=1^K)/min({N_z}_z=1^K) used in long-tailed learning to measure whether the label distribution is uniform, where K and N_z are the numbers of classes and samples in z-th class, respectively. \nThe lower r is, the more uniform the distribution becomes. \nFor CIFAR-10 under the extreme high asymmetric label noise (40%), r=2.40; that is, the asymmetric label noise makes the dataset non-uniform.\nHowever, TCL can still achieve pleasing performance on non-uniform datasets, which suggests that TCL can effectively detect those mislabeled samples to form a uniform distribution.\nSpecifically, for those clean samples\u00a0(clean probability w_i>0.5), r=1.37, which is much more balanced over noisy labels.\n\n\n"}