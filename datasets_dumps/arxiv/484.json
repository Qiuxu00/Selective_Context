{"entry_id": "http://arxiv.org/abs/2303.06644v1", "published": "20230312122652", "title": "Mitigating the Effect of Class Imbalance in Fault Localization Using Context-aware Generative Adversarial Network", "authors": ["Yan Lei", "Tiantian Wen", "Huan Xie", "Lingfeng Fu", "Chunyan Liu", "Lei Xu", "Hongxia Sun"], "primary_category": "cs.SE", "categories": ["cs.SE"], "text": "\n\n\n\n\n\n\nMitigating the Effect of Class Imbalance in Fault Localization Using Context-aware Generative Adversarial Network\n    \nYan Lei^1,2\u2217 *Corresponding author.,\nTiantian Wen^1, Huan Xie^1, \nLingfeng Fu^1, Chunyan Liu^1, Lei Xu^3, Hongxia Sun^4 \n^1School of Big Data & Software Engineering Chongqing University, Chongqing, China\n\n^2Peng Cheng Laboratory, ShenZhen, China\n^3Haier Smart Home Co., Ltd., Qingdao, China\n\n^4Qingdao Haidacheng Purchasing Service Co., Ltd., Qingdao, China\n\n {yanlei, tiantianwen, huanxie, lingfengfu, chunyanliu}@cqu.edu.cn, {xulei1, sunhongxia}@haier.com\n\n    \n===============================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\nFault localization (FL) analyzes the execution information of a test suite to pinpoint the root cause of a failure. \nThe class imbalance of a test suite,\nthe imbalanced class proportion between passing test cases (majority class) and failing ones (minority class), \nadversely affects FL effectiveness.\n\nTo mitigate the effect of class imbalance in FL,\nwe propose : a data augmentation approach using Context-aware Generative Adversarial Network for Fault Localization.\nSpecifically,\nuses program dependencies to construct a failure-inducing context showing how a failure is caused.\nThen, leverages a generative adversarial network to analyze the failure-inducing context and synthesize the minority class of test cases (failing test cases).\nFinally, augments the synthesized data into original test cases to acquire a class-balanced dataset for FL.\nOur experiments show that significantly improves FL effectiveness,\npromoting MLP-FL by 200.00%, 25.49%, and 17.81% under the Top-1, Top-5, and Top-10 respectively.\n\n\n\n\n\nfault localization, class imbalance, program dependencies, generative adversarial network\n\n\n\n\n\u00a7 INTRODUCTION\n\nTo reduce debugging cost\u00a0<cit.>, it is essential to develop effective approaches in the software debugging process.\nIn the literature, \nvarious fault\nlocalization (FL) approaches (\u00a0<cit.>) have been proposed to pinpoint potential locations of faulty code over the past several decades.\n\n\n <ref> shows the typical workflow of FL, spectrum-based fault localization (SFL)\u00a0<cit.> and deep learning-based fault localization (DLFL)\u00a0<cit.>. \nFL executes the test cases of a test suite and collects the coverage information (denoted as coverage matrix) and test results (represented as errors) of each test case.  \nThe coverage matrix and errors are raw data for FL. \nIn the raw data, \neach row of the coverage matrix represents the coverage information of a test case, \nand each column corresponds to the coverage information of a statement in all test cases of a test suite.\nSpecifically, for an element x_ij in the coverage matrix,\nx_ij=1 means that the i-th test case executes the j-th statement and otherwise x_ij=0;\nfor an element e_i in the errors,\ne_i=1 denotes that the i-th test case is a failed test case and otherwise e_i=0.\nAfter the raw data have been acquired, \nmany FL approaches use them directly as input, and develop different suspiciousness evaluation algorithms (SFL using correlation coefficients and DLFL using neural networks) to evaluate the suspiciousness value for each statement.\nFinally, \nFL outputs a ranked list of program statements in descending order of suspiciousness values for manual or automated debugging\u00a0<cit.>.\n\nThus the raw data is indispensable for conducting effective FL. \nThere are two classes of test cases: passing test cases and failing ones.\nIn practice, \nfailing test cases are much less than passing test cases.\nIt leads to a class imbalance problem in the raw data,\nthe data with failing labels are much less than the data with passing labels.\nThe existing studies\u00a0<cit.> have shown that the class imbalance problem inevitably introduces a bias\u00a0<cit.> into the suspiciousness evaluation of FL and adversely affects FL effectiveness.\n\nSince failing test cases are usually irregularly distributed and occupy a very small portion in the input domain, \nit is difficult to directly generate valid failing test cases in practice\nto address the imbalance problem.\nInspired by the recent wide use of data augmentation approaches\u00a0<cit.>,\nwe can augment the raw data of FL by synthesizing new raw data with failing labels to acquire balanced raw data, the class ratio between the raw data with failing labels and passing labels is balanced. \nIn this way, \na suspiciousness evaluation algorithm of FL can afterward utilize the class-balanced raw data to improve its accuracy.\n\nBased on the above analysis,\nwe seek a data augmentation solution to the raw data of FL\nfor addressing the class imbalance problem in FL. \nGenerative adversarial network (GAN)\u00a0<cit.> is amongst the most popular data augmentation approach.\nHowever, a failure-inducing context is useful for FL to acquire a reduced searching scope, \nand the original GAN does not consider a failure-inducing context into its data augmentation process, causing the augmentation to be potentially inaccurate.\nIt means that we should further incorporate a failure-inducing context into GAN to guide its data augmentation for FL.\n\nTherefore, we propose : a data augmentation approach using Context-aware Generative Adversarial Network for Fault Localization, to mitigate the effect of class imbalance in FL.\nanalyzes program dependencies via program slicing\u00a0<cit.> to construct a failure-inducing context, showing how a subset of statements propagates among each other to cause a program failure.\nThen, combines the failure-inducing context into a generative adversarial network to devise a context-aware generative adversarial network, \nwhich can synthesize the raw data of FL with failing labels.\nFinally, we add the new synthesized failing raw data into the original raw data\nto acquire class-balanced raw data,\nwhere the data with failing labels have the same number \nas the data with passing labels.\n\nTo evaluate our approach ,\nwe design and conduct large-scale experiments on six representative subject programs. \nWe apply for six state-of-the-art FL approaches,\nand further compare with two representative data optimization approaches.\nThe experimental results show that our approach improves the effectiveness of the six state-of-the-art FL approaches, and outperforms the two representative data optimization approaches.\nSpecifically, the experimental results indicate that our approach compared with the six state-of-the-art FL approaches improves the effectiveness of fault localization by 125.34%, 56.32%, and 59.71% respectively on Top-1, Top-5, and Top-10 metrics on average.\n\n\n\nThe main contributions of this paper can be summarized as follows:\n\t\n\t\n  * We propose a data augmentation approach , which synthesizes failing raw data for mitigating the effect of class imbalance problem in FL.\n\t\t\n\t\n  * We present a context-aware generative adversarial network which integrates a failure-inducing context into the data augmentation process of a generative adversarial network to guide data synthesization for FL.\n\t\t\n\t\n  * We conduct large-scale experiments and compare our approach with six state-of-the-art FL approaches and two representative data augmentation approaches, \n\tshowing that significantly improves fault localization effectiveness.\n    \n  * We open source the replication package online including all relevant code[https://anonymous.4open.science/r/CGAN4FL-B448].\n\t\n\t\n\tThe remainder of this paper\n\tis organized as follows.\n\tSection\u00a0<ref> introduces background information.\n\tSection\u00a0<ref> presents our approach .\n\tSection\u00a0<ref> and Section\u00a0<ref> show the experimental\n\tresults and discussion.\n\tSection\u00a0<ref> discusses related work\n\tand Section\u00a0<ref> draws the conclusion.\n\n\n\u00a7 BACKGROUND\n\n\n\n \u00a7.\u00a7 Generative Adversarial Network\n\n\nGenerative Adversarial Network (GAN) <cit.> is a deep learning framework that learns to generate adversarial data. \n\u00a0<ref> shows the basic framework of a GAN. \nGAN contains two components: the generator G and the discriminator D. \nThe generator G is responsible for generating fake data that look like real data from the latent variable z while the discriminator D distinguishes whether the data belongs to raw data or generated data as accurately as possible. \nG and D are aggressive since they compete in order to accomplish their own objectives.\nThe purpose of the model training is to minimize the loss of G and maximize the loss of D. \nSpecifically when a generator has a lower loss, it means that the generated data is almost identical to real data; and when a discriminator obtains a higher loss, it means that it is hard to discriminate between real and generated data. \n\nThis adversarial learning situation can be formulated as Eq.\u00a0(<ref>) with parametrized networks G and D.\n\n    min _Gmax _D V(G, D)= \ud835\udd3c_x \u223c p_data [log D(x)]+\n    \ud835\udd3c_z \u223c p_z[log (1-D(G(z))]\n\n\nIn Eq.\u00a0(<ref>), p_data (x   ) and p_z ( z  ) represent the real data probability distribution defined in data space \ud835\udcb3 and the probability distribution of z defined in latent space  \ud835\udcb5.\nV ( G,  D  ) is a binary cross entropy function that is commonly used in binary classification problems<cit.>. \nIt should be noted that G maps z from \ud835\udcb5 into the element of \ud835\udcb3, while D takes an input x and determines whether x is real data or fake data generated by G.\n\nSince the goal of D is to identify real or fake samples, V ( G, D  ) is a natural choice for this goal with its ability to solve binary classification problems. From the perspective of D, if the input data is real, the output of D should be close to maximum; \nif the input data comes from G, D will minimize its output. Thus, the log_ (   1-D ( G ( z  )   ) ) term is added to Eq.\u00a0(<ref>). \nAt the same time, G plans to cheat D and thus it tries to maximize D\u2019s output when the input data is generated by it. Consequently, D tries to maximize V ( G,  D  )  while G tries to minimize V ( G,  D  ), forming a type of adversarial relationship.\n\nThe existing studies\u00a0<cit.> have shown that the class imbalance problem of raw data adversely affects FL effectiveness, and it is crucial to address the class imbalance problem in FL.\nOne of the best merits of GAN is that they generate data that is similar to real data. Due to this merit, \nthey have many different applications in the real world,\ngenerating images, text, audio, and video that are indistinguishable from real data<cit.>.\nInspired by the merit of GAN,\nour study utilizes the ability of GAN to mitigate the effect of class imbalance in FL\nvia synthesizing minority class data for acquiring a class-balanced dataset.\n\n\n\n\n \u00a7.\u00a7 Fault Localization\n\nFault localization (FL) typically collects and abstracts the runtime information of a test suite as the raw data (the coverage matrix and errors in \u00a0<ref>);\nthen takes the raw data as input to evaluate the suspiciousness value for each statement; finally outputs a ranked list of program elements in descending order of suspiciousness values. \nThere are many granularity types of program elements, \nstatements, methods, and files.\nOur study adopts the most widely-used granularity type\nof program elements, statements.\nThis section will introduce two popular FL techniques (spectrum-based fault localization and deep learning-based fault localization), and they all use the raw data in  <ref> as input for suspiciousness evaluation. \nOur experiments will also apply our approach for these FL techniques to evaluate its effectiveness.\n\nSpectrum-based Fault Localization (SFL). SFL\u00a0<cit.> has been intensively studied in the literature. \n\nA program spectrum is a measurement of \nruntime behavior<cit.>, which records the runtime information of a test suite. An early study by Collofello and \nCousins<cit.> suggests that such spectra can be used for software fault localization.\nBy comparing program spectra on passed and failed test cases, program elements can be ranked. \nAs shown in Figure <ref>, the program is executed on the test suite to obtain the execution information. Specially, the test suite T = {   t1, t2, ..., tM}\nwith M test cases contains at least one failing test case and the program P = {s1, s2, ..., s_N} has N\nstatements. The M\u00d7 N matrix records the coverage information which include execution information of each statement \nfor all tests in the test suite T. X_ij represents the execution of the j-th statement s_j for the i-th test t_i ,\nwhere i \u2208{  1, 2, . . . , M}, j \u2208{  1, 2, . . . , N}. For instance, X_ij  = 1\nrepresents the statement s_j is executed by the test t_i , and X_ij  = 0 otherwise. The element e_i \ncorresponds to the result of the test t_i that e_i=1 represents that the test t_i fails, and e_i=0 otherwise.\n\nThe basic idea of SFL is that the  suspiciousness of a statement should increase when it is executed more frequently by failing test cases; its suspiciousness should decrease when it is executed more frequently by passing test cases. \n\nTo implement the above idea, \nSFL uses the raw data (coverage matrix and errors) to define the four variables for each statement.\nLet s_j be a statement in the program.\nEq.\u00a0(<ref>) defines the four variables for s_j as follows:\n\n\n    a_np(s_j)    = | {i| x_ij = 0 \u2227 e_i = 0}|  \n    \n       a_nf(s_j)    = | {i| x_ij = 0 \u2227 e_i = 1}|   \n    \n       a_ep(s_j)    = | {i| x_ij = 1 \u2227 e_i = 0}|  \n    \n       a_ef(s_j)    = | {i| x_ij = 1 \u2227 e_i = 1}|\n\n\nWhere, a_np(s_j) and a_nf(s_j)\nrepresent the numbers of test cases\nthat do not execute the statement s_j\nand return the passing and failing test results,\nrespectively;\na_ep(s_j) and a_ef(s_j) stand for the numbers of test cases\nthat execute s_j,\nand return the passing and failing testing results, respectively.\n\nBased on the four variables,\nSFL devises many suspiciousness evaluation formulas to evaluate the suspiciousness of a statement<cit.>.\nThe existing work\u00a0<cit.> has empirically identified the three most effective SFL formulas (Ochiai<cit.>, DStar[The \u2018*\u2019 in Dstar formula is usually assigned to 2.]<cit.>, and Barinel<cit.>) in locating real faults.\nSince our study focuses on locating real faults, our experiments use the three SFL formulas.\nBased on the four variables defined in Eq.\u00a0(<ref>), Eq.\u00a0(<ref>), Eq.\u00a0(<ref>) and Eq.\u00a0(<ref>) show the definitions of the three SFL formulas to compute the suspiciousness of a statement s_j.\n\n\n    Ochiai(s_j) =a_ef(s_j)/\u221a( ( a_ef(s_j)+a_nf(s_j)   ) \u00d7 ( a_ef(s_j)+a_ep(s_j)  )  )\n\n\n    Dstar (s_j)= a_ef(s_j)^*/a_ep(s_j)+ a_nf(s_j)\n\n\n    Barinel (s_j)= 1-a_ep(s_j)/a_ep(s_j)+a_ef(s_j)\n\n\n\nDeep Learning-based Fault Localization (DLFL). \nDLFL\u00a0<cit.> has recently attracted much attention and acquired promising results. \nThe basic idea of DLFL is that it utilizes the learning ability\u00a0<cit.> of neural networks to learn a FL model\nwhich reflects the relationship between a statement and a failure. \n\u00a0<ref> shows the typical architecture of DLFL. DLFL usually has three parts: \ninput layer, deep learning component with several\nhidden layers, and output layer. Next, we will introduce three representative DLFL approaches, MLP-FL<cit.>, CNN-FL<cit.> and RNN-FL<cit.>, and our experiments apply our approach to the three DLFL approaches to evaluate its effectiveness.\n\nIn the input layer, DLFL takes the raw data (coverage matrix and errors) in  <ref> as input. \nSpecifically, k rows of the coverage matrix and its corresponding errors vector, the coverage information of k test cases and their corresponding test results, are used as input. As shown in  <ref>, these k test cases are the rows starting from the i-th row, where i\u2208{ 1,1+  k,1+2k,\u2026 ,1+ ( \u2308 M/k  \u2309 -1  )\u00d7 k  }. In the part of deep learning components, different fault localization methods use different neural networks.\nFor example, MLP-FL<cit.> uses multi-layer perceptron, CNN-FL<cit.> adopts convolutional neural network, and RNN-FL<cit.> utilizes recurrent neural network. In the output layer, the model uses sigmoid function to make sure that the output results are between 0 and 1.\nEach element in the result of sigmoid function could differ from its corresponding element in the target vector. \nThe parameters of the model are updated using the backpropagation algorithm with the intention of minimizing the difference between training result y and errors vector e (the errors in \u00a0<ref>). The network is trained iteratively. Finally, DLFL learns a trained\nmodel, which can reflect the relationship between a statement and a failure. \nWith the trained model, DLFL can evaluate the suspiciousness value for statements.\n\n\n\n\n\n\n\n\n\u00a7 APPROACH\n\n\nThis section will introduce our approach : \na data augmentation approach using Context-aware Generative Adversarial Network for Fault Localization.\nAs shown in \u00a0<ref>,\nfirst uses program dependencies to construct a failure-inducing context;\nthen combines the failure-inducing context into the GAN training to learn a context-aware GAN model;\nfinally uses the trained context-aware GAN model to generate new failing raw data until a class-balanced dataset is acquired,\nwhere the raw data with failing labels have the same number as the raw data with passing labels.\n\n\n\n \u00a7.\u00a7 Failure-inducing Context Construction\n\nA failure-inducing context shows \nhow a subset of program elements (statements) act on each other to cause a failure,\nand it is useful for FL to acquire a reduced searching scope.\nTherefore,\n we intend to integrate a failure-inducing context into the GAN training to guide its data augmentation.\nTo implement the above idea,\nadopts the widely-used program dependencies via program slicing\u00a0<cit.> \nto construct a failure-inducing context,\nshowing the dependencies between a subset of statements that cause a failure.\nThe program slicing technique\u00a0<cit.> extracts the program dependencies among statements \nto pick out a subset of statements\nwhose execution leads to the incorrect output (a failure). A few approaches have evaluated the effectiveness of dynamic slices in fault localization<cit.>.\nThe subset of statements is a program slice, a failure-inducing context in our approach . Thus, we define a failure-inducing context as follows:\n\n\n\t\n\t\t0.46\n\t\t\tA failure-inducing context:\n\t\t\tstatements that directly or indirectly affect the computation of the faulty output value of a failure through chains of dynamic data and/or control dependencies.\n\t\n\n\n\nTo compute a failure-inducing context using program slicing,\nwe use the following slicing criterion contextSC.\n\n    contextSC =  ( outStm,outVar,failTest  )\n\n\nIn Eq.\u00a0(<ref>), outStm \nis an output statement whose value of a variable (outVar) is incorrect in the execution of a failing test case (failTest).\nDynamic slicing collects runtime information along the execution path of a test case, the set of executed statements of a test case. \nIt means that a test case with a smaller set of executed statements is\nusually easier for a dynamic slicing tool to perform efficient instrumentation and produce compressed traces for space\noptimization. Thus, for multiple failing test cases, the one with the least executed statements usually is beneficial for\nthe efficiency of constructing a failure-inducing context. From the efficiency aspect, will choose the failing test case having the least executed statements to construct a slicing criterion in Eq.\u00a0(<ref>).\n\nSuppose that a failure-inducing context has K statements. \nIt means that these K statements interact with each other to cause an incorrect output (a program failure). \nSince the statements not in the failure-inducing context do not affect the incorrect output, \nwe combine the failure-inducing context \nvia keeping the coverage information of these statements in the failure-inducing context and removing others.\nThus, finally acquires a new M \u00d7 K matrix called context matrix\nwhich records the execution information of the failure-inducing context in\nthe test suite. \n\n\n\n \u00a7.\u00a7 Context-aware GAN Model Training\n\nAfter constructing a failure-inducing context,\nwe acquire a M \u00d7 K context matrix\nfrom the original M \u00d7 N matrix (original raw data).\nThe context matrix shows the runtime information of these statements whose\nexecution leads to the incorrect output of a program. \nuses the context matrix as the input of the GAN model,\ntrains the GAN\nto generate a new synthesized vector\nby the discriminating network D and the generating network G with all failing test cases selected \nfrom the context matrix as samples. It means that \nwill learn the features of all failing test cases,\nthe newly synthesized test cases will cover the common feature of all failing test cases. \nThus, \nwill mark the newly synthesized test cases (the newly synthesized vectors with the same structure of raw data) as failing labels.\nWe add the new failing synthesized test cases to the context matrix and \nform a new matrix (new raw data) whose failing data and passing data are balanced. \nFinally, the new raw data are used as the new input for the FL approach (SFL and DLFL) to improve its effectiveness.\n\nThe model training part of \u00a0<ref> shows the specific training procedure. trains D (discriminator) first to initiate the training procedure of the GAN model. \nselects the K-dimensional vector [ y_1, y_2, \u2026, y_K ], and inputs it into G (generator) after noise processing to obtain the generated data [z_1, z_2, \u2026, z_K ] (the synthesized data). \nThen, \nit further selects the original failing test cases in the original raw data ([x_i1,  x_i2, \u2026 ,  x_iK ], i\u2208{ 1,  2, \u2026 ,  M } and e_i=1) as the real data, and uses the generated data [ z_1,  z_2, \u2026 ,  z_K ] as the generated sample to be spliced together and input them into D (discriminator). \nD gives label 1 to real data [ x_i1, x_i2, \u2026 , x_iK ], and 0 to generated data [ z_1, z_2, \u2026 , z_K ]. The difference between D's output score and the label is trained using loss backpropagation.\nAfter D training is complete, \nstarts G training \nwith the fixed parameters of D. \nIn the G training process, D and G are regarded as a whole. [ y_1, y_2, \u2026 , y_K ]  processed by the noise is used as the input, and then G outputs generated data [ z_1, z_2, \u2026 , z_K ]. The discriminator D with fixed parameters is used for scoring. The difference between the output score and label 1 is used as the loss backpropagation to train G.\n\nThroughout the GAN model training process, \nG is weak at the beginning, and D can easily distinguish between real data and generated data. With the gradual increase of training G, D cannot distinguish between real data and generated data. Eq.\u00a0(<ref>) is the training process of the minimax two-player game between generator G and discriminator D. D maximizes the objective function to identify whether the generated data  [ z_1,  z_2, \u2026,  z_K ] are fake. In contrast, G constantly minimizes the distribution difference between the real data and the generated data, minimizing D's discrimination of generated data. Finally, a Nash equilibrium<cit.> is reached.\n\n\n \u00a7.\u00a7 Class-balanced Raw Data Generation\n\nAfter context-aware GAN training,\nlearns a context-aware GAN model which generates synthesized failing data (the new synthesized failing vectors with the same structure as the original raw data) for FL.\nThe trained model will generate synthesized failing data and add them to the original raw data until we acquire a class-balanced dataset,\nwhere the number of failing vectors and \nthe number of passing vectors are the same \nin the new raw data.\ninputs the new class-balanced raw data into the suspiciousness evaluation algorithm of the FL approach to mitigate the effect of the class imbalance problem in FL.\nFinally, \nwith the new class-balanced raw data,\nFL outputs a ranked list of all statements in descending order of suspiciousness values.\n\n\n \u00a7.\u00a7 An Illustrative Example\n\n\nTo illustrate how the methodology of works, \n <ref> shows an example of applying .\nAs shown in  <ref>, \nthere is a faulty program P with 16 statements including a fault at line 3, in which the number 0 should be 6 instead. We use one SFL approach (GP02<cit.>) to locate the faulty statement for our illustrative example.  \nThe cells below each statement indicate whether the statement \nis executed by the test case or not (0 for not executed and 1 for executed). The cells below the `Result' which is the\nerrors vector for the coverage matrix as shown in  <ref> represent whether the test result of a test case is failing or passing (1 for \nfalling and 0 for passing).\nThe original test suite is class-imbalanced since it has four passing test cases (t_2, t_3, t_4, and t_5) and two failing test cases (t_1 and t_6). \n\nFor acquiring a class-balanced dataset,\nwe need to generate two pieces of new raw data with failing labels. \nfirst uses the failing test case t_1 to compute the failure-inducing context using program slicing.\nAccording to Eq.\u00a0(<ref>), \nwe set (S_14, d1, t_1) as the slicing criterion since the output value of the variable d1 in the output statement S_14 is incorrect when executing the failing test case t_1.  \nAs shown in  <ref>, the failure-inducing\ncontext regarding t_1 is { S_1, S_3, S_7 ,S_14}. Then, based on the failure-inducing context, uses GAN to generate two pieces of synthesized failing raw data (t_7 and t_8) marked with yellow in  <ref>.\nFinally, adds the two failing synthesized failing test cases into the context matrix to form a new raw data, \nand GP02 uses the new raw data to conduct the suspiciousness evaluation for each statement. \n\nThe bottom rows are the FL results of original GP02 and GP02 with ,\nthe two ranked lists of statements in descending order of suspiciousness in  <ref> marked with different colors.\nWithout using ,\nthe ranked list of the statements using GP02 marked with blue is { S_7,S_8,S_9, S_12,S_10,S_11,S_14,S_15,S_16,S_1,S_2,S_3,S_13,S_4, S_5,S_6}.\nAfter applying our approach , \nthe ranked list of the statements using GP02 is { S_7, S_3,S_1,S_14}. \nWe can observe that the faulty statement S_3 is ranked 12th place with the original raw data while ranks the faulty statement S_3 2nd place. \nIt means that yields better FL results than the original GP02, \nmitigating the effect of the class imbalance in FL.\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n \u00a7.\u00a7 Datasets\n\nTo evaluate the effectiveness of our approach , we adopt the Defects4J\u00a0<cit.>\nthat has been widely used in the software testing and debugging community\u00a0<cit.>. We use all the six representative subject programs of Defects4J[https://github.com/rjust/defects4j] (\nChart, Closure, Math, Mockito, Lang, and Time) and all faults from these programs are real faults. \n\n\u00a0<ref> summarizes the information of the six subject programs.\nFor each program, it lists a brief functional description (column `Description\u2019), the number\nof faulty versions used (column `Versions\u2019), the number of\nthousand lines of statements (column `LoC(K)\u2019), the number\nof test cases (column `Test\u2019).\nSince it is time-consuming to \ncollect the inputs (raw data) of the six large programs of Defects4J, \nwe reuse the coverage matrix and the errors collected by Pearson\u00a0\u00a0<cit.>.\n\n\n\n \u00a7.\u00a7 Experiment Settings\n\nFor every faulty version of the six subject programs shown in \u00a0<ref>, we set the training period of context-aware GAN model to 1,000, and the dimension of hidden variable z defined in latent space \ud835\udcb5 to 100. \nOur experiments were conducted on a 64-bit Linux server with 40 cores of\n2.4GHz CPU and 252GB RAM. The operating system is Ubuntu 20.04.\n\n\n \u00a7.\u00a7 Evaluation Metrics\n\nWe adopt the four widely-used FL evaluation metrics to evaluate the effectiveness of our approach. \nTheir definitions are as follows:\n\n    \n  * Number of Top-K<cit.>: It is the number of faulty versions with at\n     least one faulty statement that is within the first K position of the rank list produced by the FL technique. In the previous study, \n     many respondents view fault localization as successful only if it can localize bugs in the top 10 positions from\n      a practical perspective <cit.>. Following the prior work\u00a0<cit.>, we assign K with the value of 1, 5, and 10 for our evaluation. A higher value of Top-K means better FL effectiveness.\n\n    \n  * Mean Average Rank (MAR)<cit.>: For a faulty version, the average rank is the mean rank of all faulty statements in the rank list. \n    MAR is the mean average value for the project that includes several faulty versions. A lower value of MAR indicates better FL effectiveness.\n\n    \n  * Mean First Rank (MFR)<cit.>: It first computes the rank that any of the statements are located first for a faulty version.\n     Then compute the mean value of the ranks for the project. \n     A lower value of MFR shows better FL effectiveness.\n     \n     \n  * Relative Improvement (RImp)<cit.>: This metric can see the improvement of one fault localization approach relative to another fault localization approach. It is to compare the total number of statements that need to be examined to find the first faulty statement using versus the number that needs to be examined by using baselines. \n    \n     \n      \n       \n    \n    \n    A lower value of RImp indicates better FL effectiveness.\n\n\n\n\n\n \u00a7.\u00a7 Research Questions and Results\n\nTo evaluate the effectiveness of our approach, we design and conduct the experiments to investigate the following research questions:\n\n    \n  * RQ1: How does perform in localizing real faults compared with original state-of-the-art FL approaches? This RQ aims at investigating whether improves FL effectiveness after applying our approach. If the effectiveness of the FL approach increases after applying , it means that can mitigate the effect of class imbalance in FL.\n    \n  * RQ2: How effective is as compared with the representative data optimization approaches? This RQ is to further verify the ability of to mitigate the effect of class imbalance in FL via comparing other representative data optimization approaches. If outperforms other representative data optimization approaches, it means that is more effective than other representative approaches in addressing the class imbalance problem of FL.\n    \n  * RQ3:  Does each component contributes to the effectiveness of ? This RQ is to check whether each component of (a GAN or a failure-inducing context) contributes to the effectiveness of . We use three cases: original FL (denoted as baseline), only using GAN (denoted as CGAN4FL(GAN)), using GAN and failure-inducing context (denoted as CGAN4FL(GAN+context)). If we acquire a FL effectiveness relationship: CGAN4FL(GAN+context) > CGAN4FL(GAN) > baseline,\n    it means that the GAN (due to CGAN4FL(GAN) > baseline) and the failure-inducing context (due to CGAN4FL(GAN+context) > CGAN4FL(GAN)) both contribute to .\n\n \nRQ1. How does perform in localizing real\nfaults compared with original state-of-the-art FL approaches?\n\nThere are two main types of FL: spectrum-based fault\nlocalization (SFL) and deep learning-based fault localization (DLFL). \nRecent studies\u00a0<cit.> have shown the most effective SFL approaches (Dstar\u00a0<cit.>, Ochiai\u00a0<cit.>, and Barinel\u00a0<cit.>) and DLFL approaches (MLP-FL\u00a0<cit.>, CNN-FL\u00a0<cit.>, and RNN-FL\u00a0<cit.>) in locating real faults.\nThus, we use the six state-of-the-art FL approaches as the baselines and apply to them to compare their effectiveness. \nFor details of these FL approaches, please refer to Section\u00a0<ref>.\n\n\u00a0<ref> shows the Top-K, MAR, and MFR results of the comparisons of the FL baselines and our approach .\nIt illustrates two scenarios: a baseline without using (referred to as baseline) and using (referred to as ). \nFor the convenience of reading, \nwe bold the experimental results in the tables, indicating which approach performs better.\n\nAs shown in \u00a0<ref>,\nsignificantly outperforms all the baselines.  \nFor SFL approaches,\ntake Ochiai as an example.\nThe number of faults that can locate is 50, 123, and 177 for the Top-1, Top-5, and Top-10 metrics, respectively.\nThe results denote the Top-1, Top-5, and Top-10 metrics have increased by 31.58%, 8.85%, and 13.46% as compared with Ochiai. \nFor DLFL approaches,\ntake MLP-FL as an example. \nThe number of faults that can locate is 27, 64, and 86 for the Top-1, Top-5, and Top-10 metrics, respectively, \nthe Top-1, Top-5, and Top-10 metrics have increased by 200.00%, 25.49%, 17.81% as compared with the MLP-FL.\nFurthermore,\nthe MFR and MAR metrics show that the rank of is lower than that of baselines for all six FL techniques. The results show that can always locate one buggy line first and find all buggy lines with the least effort.\n\n\u00a0<ref> visually shows the MFR distribution of the FL baselines and . The results show that significantly improves FL effectiveness.\n \n\n\n\n\n\u00a0<ref> shows the RImp distribution of our approach vs the six FL baselines. As shown in \u00a0<ref>,\nall RImp values are less than 100%,\nshowing that our approach improves all the baselines after applying .\nTake the MLP-FL as an example. \nMLP-FL will examine 987.94 lines on average to locate the first bug in all faulty versions (MFR), while only checks 259.24 lines of code. \nThus, the value of RImp is 26.24%, indicating that for locating the first faulty statement, \nthe number of statements to be checked by is 26.24% of the original MLP-FL.\n[boxrule=0pt, frame empty]\n Summary for RQ1: In RQ1, we discuss the effectiveness of the six state-of-the-art FL approaches using and without using . \n The experimental results show that \nis effective to improve FL effectiveness by mitigating the effect of class imbalance in FL.\n\n \nRQ2. How effective is as compared with the representative data optimization approaches?\n\nTo further evaluate the ability of to mitigate the effect of class imbalance in FL, we compare our approach with two representative data optimization approaches, resampling\u00a0<cit.> and undersampling\u00a0<cit.>.\nResampling and undersampling acquire a class-balanced dataset by replicating minority samples and removing the majority samples, respectively. \nFor more details, resampling  and undersampling can refer to Gao\u00a0\u00a0 <cit.> and Wang\u00a0\u00a0 <cit.>, respectively.\n\n\u00a0<ref> shows the Top-K, MAR, and MFR results of the two representative data optimization approaches and our approach .\nAs shown in \u00a0<ref>, outperforms resampling\nand undersampling in all cases of SFL and most cases of DLFL. \nTaking Ochiai as an example, the number of faults can locate is 50, 123, and 177 for Top-1, Top-5, and Top-10 metrics, respectively. \nThe results indicate that the Top-1, Top-5, and Top-10 metrics have increased by 284.62%, 136.54%, and 126.92% respectively as compared with undersampling, and increased by 47.06%,33.70%, and 40.48% respectively as compared with resampling. \nFurthermore,\nthe MFR and the MAR of are lower than the two representative data optimization approaches in almost all cases, \nshowing that performs better than the two representative approaches. \n\nFurthermore, \u00a0<ref> visually shows the MFR distribution of resampling, undersampling, and ,\nindicating that is the best of these three scenarios except for one case of resampling is better in CNN-FL. \n\u00a0<ref> shows the RImp distribution under two scenarios: vs resampling and vs undersampling. \nAs shown in \u00a0<ref>,  the RImp values of all the cases are less than 100% except for one case of vs resampling in CNN-FL. \nThus, \nwe can conclude that performs better than resampling\nand undersampling in addressing the class imbalance problem in FL.\n[boxrule=0pt, frame empty]\n Summary for RQ2: In RQ2, we compare with two representative data optimization approaches, resampling and undersampling. The experimental results show that is more effective than the two representative data optimization approaches in almost all cases for mitigating the effect of class imbalance in FL.\n\n\n\n\n\nRQ3. Does each component contributes to the effectiveness of ?\n\nTo check whether each component contributes to the effectiveness of , we use Wilcoxon-Signed-Rank Test (WSR)\u00a0<cit.> to verify whether the effectiveness relationship (CGAN4FL(GAN+context) > CGAN4FL(GAN) > baseline) is satisfied or not,\nwhere CGAN4FL(GAN+context), CGAN4FL(GAN), and baseline denote using GAN and failure-inducing context, only using GAN, and original FL respectively.\nFor each FL approach, \nwe perform two paired Wilcoxon-Signed-Rank tests (CGAN4FL(GAN+context) vs CGAN4FL(GAN) and CGAN4FL(GAN) vs baseline) by using the ranks of the faulty statements\nas the pairs of measurements.\n\n\u00a0<ref> shows the statistical results of all the tests at the  \u03c3 level of 0.05.\n\n    \u00a0<ref> shows the statistical results of all the tests at the  \u03c3 level of 0.05.\n\nThe `conclusion\u2019 column gives the conclusion according to p-value. \nTake Ochiai as an example.\nFor CGAN4FL(GAN) vs baseline, \nthe the p-value of greater, less, and two-sided are 1, 2.68e-08, and 5.36e-08 respectively. According to the definition of WSR, it means that the MFR value of CGAN4FL(GAN) (Ochiai using CGAN4FL(GAN)) is less than that of the baseline (original Ochiai), leading to a BETTER result.\nFor CGAN4FL(GAN+context) vs CGAN4FL(GAN), the p-value of greater, less, and two-sided are 1, 1.43e-03, and 2.86e-03  respectively.  It means that the MFR value of CGAN4FL(GAN+context) (Ochiai using our approach ) is less than that of CGAN4FL(GAN) (Ochiai using CGAN4FL(GAN)), also leading to a BETTER result. \nFrom the table, we can observe that both CGAN4FL(GAN) vs baseline and CGAN4FL(GAN+context) vs (GAN) obtain BETTER results in all cases, \nmeaning that the effectiveness relationship CGAN4FL(GAN+context) > CGAN4FL(GAN) > baseline is satisfied.\nThus, each component of contributes to its effectiveness.\n\n\n[boxrule=0pt, frame empty]\n Summary for RQ3: In RQ3, we make statistical comparisons in two scenarios: CGAN4FL(GAN+context) vs CGAN4FL(GAN) and CGAN4FL(GAN) vs baseline. \n The experimental results show that the effectiveness relationship CGAN4FL(GAN+context) > CGAN4FL(GAN) > baseline holds. Thus, we can conclude that each component of (failure-inducing context and GAN) contributes to its effectiveness.\n\n\n\n\n\n\n\n\u00a7 DISCUSSION\n\n\n\n \u00a7.\u00a7 Threats to Validity\n\n\n\nThe implementation of baselines and our approach. Our implementation of baselines and may potentially contain bugs. \nFor the three SFL approaches (Dstar, Ochiai, and Barinel), \nwe implement them according to their formulas and then manually test their correctness. \nFor the three DLFL approaches (MLP-FL, RNN-FL, and CNN-FL),\nwe acquire the source code of CNN-FL from the authors and implement the other two DLFL approaches via replacing the deep learning component with MLP and RNN from the source code of CNN-FL. \nSince a neural network has many parameters for its construction (learning rate, batch size), \nsome parameters of MLP-FL, RNN-FL, and CNN-FL may differ from the original paper. \nBesides the implementation of baselines, \nwe implement our pipeline of failure-inducing context construction, context-aware GAN model training, and class-balanced raw data generation, which may also potentially include bugs. \nTo mitigate those threats, we check our code implementation rigorously and make all relevant code publicly available (see the footnote in Section\u00a0<ref>).\n\nThe generalizability. We conduct our experiments on the real faults dataset benchmark, Defects4J, which is widely used in fault localization and program repair community.  \nAlthough the subject programs selected in our experiments are all from the real world and our approach performs well on these programs, \nit may be not effective for other programs since no dataset can cover all possible cases of faults in practice. \nThus,\nit is worthwhile to conduct more experiments on more large-sized programs with real faults to further verify the effectiveness of our approach in mitigating the effect of class imbalance in FL.  \n\n\n\n\n \u00a7.\u00a7 Reasons for Is Effective\n\nOur experiments demonstrate that is more effective than the compared baselines.\nThe main reasons are threefold:\n(1) takes full advantage of the program dependency to capture the statements closely related to the faulty statements.\nIn other words, the utilization of program slicing removes the fault-irrelevant statements precisely.\n\n(2) The GAN is a powerful model that could generate synthesized samples that are like real samples.\nMore importantly, we provide the GAN model with the expert knowledge in software debugging (the fault-relevant statements obtained by program slicing), which could potentially improve the accuracy and efficiency of the model.\n(3) The advantages of both the program analysis technique (the program slicing) and the advanced generative network (the GAN model) are well combined by to gain the high-quality generated input data.\nFurther, the high-quality and class-balanced input data could be beneficial to the state-of-the-art SFL and DLFL approaches.\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n \u00a7.\u00a7 Fault Localization\n\nSpectrum-based fault localization (SFL)\u00a0<cit.> and Deep learning-based fault localization (DLFL)\u00a0<cit.> are the two most popular FL approaches.\nResearchers have proposed many SFL techniques (Tarantula<cit.>, Ochiai<cit.>, Jaccard<cit.>, and DStar<cit.>),\nand enhanced program spectrum (<cit.>) and a test suite (<cit.>) for more improvement.\nWith a large number of SFL techniques, \nmany studies\u00a0<cit.> have explored the best SFL techniques. \nYoo \u00a0<cit.> have found that \nthere is no SFL technique claiming that it can outperform all others under every scenario.\nEven if the best SFL technique does not exist.\nThe existing studies\u00a0<cit.> have found a group of optimal SFL techniques, the group of optimal SFL techniques cannot outperform each other whereas they can outperform all the other SFL techniques outside the group.\nDLFL\u00a0<cit.> uses deep learning to locate a fault and recently attracts much attention. Wang\u00a0propose the FL approach BPNN-FL<cit.> using BP neural network model as a pipeline for learning input and output relationships. \nThen, Wong\u00a0\u00a0<cit.> improve their BPNN-FL approach by removing irrelevant statements. \nSimilar to the idea of Wong\u00a0, \nmany researchers directly use the raw data as training data, \nand propose different DLFL approaches using different neural networks (MLP-FL<cit.>, CNN-FL<cit.>, and RNN-FL<cit.>).\nIn contrast to devising an effective FL approach,\nour work focuses on addressing the class imbalance problem in FL and can be used in tandem with these FL approaches. \n\n\n\n \u00a7.\u00a7 Class Imbalance\n\nIn recent decades, \nresearchers have proposed many methods to address the class imbalance problem. \nThe typical methods are data-level, algorithm-level, hybrid, and ensemble learning methods. \nThe data-level methods add a preprocessing step to mitigate the effect of class imbalance in the learning process<cit.>. \nThe algorithm-level methods create or modify deep learning algorithms for addressing the class imbalance problem<cit.>. \nThe hybrid methods combine algorithm-level and data-level methods\u00a0<cit.>.\nThe ensemble learning methods\u00a0<cit.> use ensembles to increase the accuracy of classification by training several different classifiers and combining their decisions to output a single class label.\nSpecifically, with regard to ensemble learning methods,\nthere are lots of different approaches, SMOTEBoost<cit.>, RUSBoost<cit.>, IIVotes<cit.>, EasyEnsemble<cit.>, and SMOTEBagging<cit.>.\nThese works focus on addressing the class imbalance problem in the artificial intelligence field.\nIn contrast,\nour work focuses on mitigating the effect of class imbalance in a different research field (fault localization).\n\n\n\n\n\u00a7 CONCLUSION\n\nIn this paper, we propose : a data augmentation approach that uses context-aware GAN to mitigate the effect of the class imbalance in FL. \nembraces two main ideas: \n(1) data augmentation is a potential and effective solution to the class imbalance problem in FL;\n(2) a failure-inducing context is useful for guiding and acquiring a more precise data augmentation process.\nTo implement the above ideas,\nwe use program dependencies to construct a failure-inducing context showing how a failure is caused, and integrate the context into a generative adversarial network to learn the features of minority class and synthesize minority class data for generating a class-balanced dataset for FL.\nThe experiments show that our approach is effective to mitigate the effect of class imbalance in FL.\n\nIn the future, we intend to use more large-sized program to further verify our approach, and explore other generative networks for more improvement.\n\n\n\u00a7 ACKNOWLEDGMENT\n\nThis work is partially supported by the National Natural Science Foundation of China (No. 62272072), the Fundamental\nResearch Funds for the Central Universities (No. 2022CDJDX-005), and the Major Key Project of PCL (No. PCL2021A06).\n\n\nIEEEtran\n\n"}