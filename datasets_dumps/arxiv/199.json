{"entry_id": "http://arxiv.org/abs/2303.07079v1", "published": "20230313130355", "title": "Automatically Identifying Relations Between Self-Admitted Technical Debt Across Different Sources", "authors": ["Yikun Li", "Mohamed Soliman", "Paris Avgeriou"], "primary_category": "cs.SE", "categories": ["cs.SE", "cs.AI"], "text": "\n\n\n\n\n\nAutomatically Identifying Relations Between Self-Admitted Technical Debt Across Different Sources\n\n    Yikun Li, Mohamed Soliman, Paris Avgeriou \n\nBernoulli Institute for Mathematics, Computer Science and Artificial Intelligence \n\nUniversity of Groningen \n\nGroningen, The Netherlands \n\n{yikun.li, m.a.m.soliman, p.avgeriou}@rug.nl\n\n    March 30, 2023\n========================================================================================================================================================================================================================================\n\n\n\n\nSelf-Admitted Technical Debt or SATD can be found in various sources, such as source code comments, commit messages, issue tracking systems, and pull requests.\nPrevious research has established the existence of relations between SATD items in different sources; such relations can be useful for investigating and improving SATD management.\nHowever, there is currently a lack of approaches for automatically detecting these SATD relations.\nTo address this, we proposed and evaluated approaches for automatically identifying SATD relations across different sources.\nOur findings show that our approach outperforms baseline approaches by a large margin, achieving an average F1-score of 0.829 in identifying relations between SATD items. \nMoreover, we explored the characteristics of SATD relations in 103 open-source projects and describe nine major cases in which related SATD is documented in a second source, and give a quantitative overview of 26 kinds of relations.\n\n\n\ndeep learning, self-admitted technical debt, SATD duplication, SATD repayment, code comments, commit messages, pull requests, issue tracking systems\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nThe implicit documentation of technical debt by developers with statements like \u201cTODO\u201d or \u201cFIXME\u201d has become known as Self-Admitted Technical Debt or SATD <cit.>.\nThis can take place in various sources such as source code comments, commit messages, issue tracking systems, and pull requests <cit.>.\nThe analysis of SATD has shown to be an important and valuable complement to using static source code analysis for technical debt management <cit.>.\nConsequently, significant research work has focused on identifying SATD from various sources; most of this work uses source code comments <cit.>, but recently some work also used issue tracking systems <cit.>, commit messages <cit.>, and pull requests <cit.>.\n\nFurthermore, recent research has found that there are relations between SATD items in different sources<cit.>.\nFor example, developers may document and explain the same SATD item in both code comments and issues, and then record its repayment in a commit message. The automated identification of such SATD relations could support more efficient and effective SATD management. First, it would allow developers to trace multiple SATD items at once.\nFor instance, tracing the SATD from code comments to issues could assist developers in identifying other relevant SATD items under the same issue.\nAs a result, developers could address these related SATD items together.\nFurthermore, it could assist developers in having a better understanding of how debt is introduced, accumulated, and eventually repaid.\nThis would give a better overview of how SATD evolves in the system. Last, but not least, it  would help to tackle individual SATD items by combining information from multiple sources and forming a more complete picture on how to prioritize and fix them. \n\nDespite this significance of SATD relations in different sources,  there is currently a lack of approaches for automatically identifying such relations.\nTo address this problem, we propose and evaluate approaches for automatically identifying relations between instances of SATD across four different sources: source code comments, issue trackers, commit messages, and pull requests.\nSpecifically, as related SATD items are often explicitly linked (e.g. a commit message references a pull request through its unique ID - see <ref>), we use such linked data between different sources and randomly collect a dataset of 1,000 pairs of SATD items from 103 open-source projects. \nWe then manually analyze these pairs to identify the relations between SATD items.\nUsing this dataset for training, we compare the predictive performance of two deep learning approaches (i.e., BERT-based and CNN-based) and one baseline method (i.e., random) for automatically identifying SATD relations. \nFinally, we summarize the characteristics of SATD relations with examples and present the number of various types of SATD relations in 103 open-source projects.\nThe main contributions of this paper are as follows:\n\n\n    \n  * Contributing a self-admitted technical debt relation dataset.\n    We gather a dataset of 1,000 pairs of SATD items from 103 open-source projects across four sources (code comments, commit messages, pull requests, and issues).\n    Our dataset includes manual annotations on the relationship types of each SATD pair.\n    In order to promote further research in this area, we make our dataset publicly accessible[<https://github.com/yikun-li/satd-relations>].\n\n    \n  * Proposing an approach to automatically identify relations between SATD items.\n    We propose a BERT-based approach for identifying SATD relations and compare it to a CNN-based approach and a baseline method in terms of F1-score.\n    The results showed that our BERT-based approach attains an average F1-score of 0.829 for identifying two types of SATD relations, significantly outperforming the CNN-based approach and the baseline.\n\n    \n  * Exploring the amount of data necessary for training the model.\n    We find that only a small amount of training data is needed to achieve a high level of accuracy in identifying relations between SATD items.\n\n    \n  * Characterizing SATD relations in 103 open-source projects.\n    We report on the sequences and amount of SATD relations.\n    Specifically, with the aim of gaining a qualitative understanding of SATD relations, we summarize the most prevalent SATD relations and provide examples based on the manually annotated dataset.\n    To provide a better overview of the quantity of SATD relations, we then apply the proposed machine learning model to automatically identify all relations between SATD items from 103 open-source projects.\n    We consequently present the number of various types of SATD relations.\n\n\nThe structure of the remainder of this paper is as follows.\nIn <ref>, we discuss related work. \nThe case study design is then described in detail in <ref>, and the results are presented and discussed in <ref> and <ref>, respectively.\nIn <ref>, we evaluate threats to validity, and we draw conclusions in <ref>.\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\nIn this work, we focus on automatically identifying relations between SATD items across different sources.\nThus, we review related work from two areas: 1) research on SATD in general and 2) research on SATD relations.\n\n\n\n\n \u00a7.\u00a7 Self-Admitted Technical Debt.\n\n\nPotdar and Shihab <cit.> were pioneers in the field of studying SATD in source code comments.\nThey conducted an analysis of four open-source projects, identifying instances of SATD within the codebase.\nTheir findings revealed that between 2.4% and 31% of source files contained SATD comments, and that only between 26.3% and 63.5% of identified SATD was removed after its introduction.\nThen Maldonado and Shihab <cit.> expanded on this work by focusing on the different types of SATD present in open-source projects.\nThey analyzed 33,000 code comments from five projects, categorizing SATD into five distinct types: design, requirement, defect, documentation, and test debt.\nThe study results indicated that design debt was the most prevalent type of SATD, accounting for between 42% and 84% of the classified instances.\n\nFollowing the initial work on exploring SATD in source code comments, there has been a significant focus on developing automatic methods for SATD identification. \nMaldonado et al. <cit.> manually classified source code comments from ten open-source projects into different types of SATD and used a maximum entropy classifier to automatically identify design and requirement debt.\nMoreover, Ren et al. <cit.> proposed a convolutional neural network-based approach to accurately identify SATD from source code comments.\nSome researchers have also focused on identifying SATD from other sources.\nLi et al. <cit.> examined 23,000 issue sections and proposed a convolutional neural network-based approach for identifying SATD from issue tracking systems.\nFurthermore, they conducted another study to identify four types of SATD from multiple sources, including source code comments, commit messages, pull requests, and issues <cit.>.\n\nIn addition to identifying SATD, there has been research about the measurement, prioritization, and repayment of SATD.\nKamei et al. <cit.> examined ways to measure the interest accrual of SATD and proposed to use lines of code (LOC) and Fan-In measures.\nMensah et al. <cit.> developed an SATD prioritization scheme that consists of identification, examination, and estimation of rework effort.\n\n\n\n\n \u00a7.\u00a7 Relation Between SATD Items.\n\n\nThere are limited studies that have explored or utilized the relations between SATD items across different sources.\nZampetti et al. <cit.> investigated the degree to which the removal of SATD in code comments is documented in commit messages.\nThey found that the removal of SATD was documented in commit messages in 131 out of 997 cases.\nLi et al. <cit.> further examined the relations between SATD items across different sources.\nTheir results indicated that there are four types of relations between SATD in different sources and that the number of related SATD items in different sources is comparable.\n\nCompared to the previously mentioned studies, our study is the first to propose an automated method for identifying relations between SATD items across different sources.\nThis can help to improve the effective management of technical debt.\nAdditionally, we applied our trained machine learning model to 103 open-source projects and summarized the characteristics of SATD relations, allowing for a comprehensive understanding of SATD relations and providing valuable insights for developers and managers.\n\n\n\n\n\n\u00a7 STUDY DESIGN\n\n\n\n\n\nThe goal of this study, formulated according to the Goal-Question-Metric <cit.> template is to \u201canalyze self-admitted technical debt from source code comments, commit messages, pull requests, and issue trackers for the purpose of automatically identifying and characterizing relations between self-admitted technical debt with respect to the identification accuracy, the sequences of relations, and the quantity of relations from the point of view of software engineers in the context of open-source software systems.\u201d\nThis goal is refined into two research questions (RQs):\n\n\n    \n  * RQ1: How to automatically and accurately identify relations between SATD from different sources?\n\n    This RQ is further refined into three sub-RQs:\n\n    \n        \n  * RQ1.1: Which deep learning approaches can effectively identify relations between SATD items?\n\n        Rationale:\n        As there is no approach available for SATD relation identification, we investigate the efficacy of two deep learning methods: a CNN-based approach and a BERT-based approach.\n        We choose these two methods because: a) previous studies <cit.> have demonstrated that CNN-based approaches are capable of extracting features from SATD items; and b) the self-attention mechanism of BERT, is particularly powerful in extracting relationships between items <cit.>.\n        \n    \n        \n  * RQ1.2: How to further improve the predictive performance of the machine learning model?\n\n        Rationale:\n        In order to make machine learning models as accurate and reliable as possible for real-world applications, we need to explore further optimizing the predictive performance of machine learning models.\n        This can be achieved, for example, by investigating the effectiveness of pre-training models on domain-specific data (e.g., pre-training on SATD comments).\n        \n        \n  * RQ1.3: How much data is sufficient for training the machine learning models to identify related SATD?\n\n        Rationale:\n        The lack of training data could lead to poor machine-learning model performance.\n        Therefore, we need to investigate whether the current training dataset is sufficient for training machine learning models to identify SATD relations or whether it needs to be further extended.\n    \n    \n    \n  * RQ2: What are the sequences and quantities of SATD relations?\n    This RQ is further refined into two sub-RQs:\n    \n    \n        \n  * RQ2.1: What are the sequences of documenting related SATD in different sources?\n\n        Rationale:\n        SATD items can be related between two sources in a bi-directional way: developers can consider SATD that already exists in source A and subsequently document it in source B, and vice versa.\n        For instance, existing SATD from issues is sometimes also documented in code comments, so that developers are aware of its location; but also, code comments that contain SATD are sometimes also documented as issues, so that they become visible at the issue tracker level.\n        Thus, it is important to know the sequences of documenting related SATD.\n        This could aid in further understanding how related SATD occurs and is utilized.\n        \n        \n  * RQ2.2: How much SATD is related between different sources?\n\n        Rationale:\n        Quantifying how much SATD is related across different sources can help us to understand how developers make use of different sources to manage SATD.\n        For example, when developers create an issue to solve a technical debt item, they could document the repayment of that item in commit messages or in code comments.\n        Knowing the amount of SATD repayment documented in commit messages or in code comments helps us understand how developers are managing SATD.\n        This RQ also gives us insights on how to use SATD relation information.\n        For example, if we find that a large amount of SATD in issues, is additionally documented in code comments, we could combine the information in both sources to obtain a more complete understanding of such SATD items.\n     \n\n\nTo answer these Research Questions, we follow the approach that is presented in <ref>.\nIn the following subsections, we will go into more detail about each step.\n\n\n\n\n \u00a7.\u00a7 Data Collection\n\n\nTo analyze relations between SATD in different sources, we chose to use the dataset provided by the work of Li et al. <cit.>.\nThe reasons for using this dataset are listed below:\n\n\n    \n  * This dataset collects data from 103 projects from the Apache ecosystem.\n    These projects possess a high level of quality and are maintained by mature communities.\n    \n    \n  * This dataset contains 23.6M code comments, 1.2M commit messages, 3.1M issue sections, and 2.5M pull request sections; these provide us with sufficient data to mine SATD items and study SATD relations from different sources.\n    We note that because issues and pull requests are both composed of three sections (i.e. summaries, descriptions, and comments), we consider each of these sections individually. This is because developers commonly share a single item of SATD in an issue or pull request section <cit.>.\n    In other words, a pull request or issue may have one, two, or more SATD items in the respective sections.\n\n\nThe details of this dataset are presented in <ref>.\n\n\n\n\n\n\n \u00a7.\u00a7 Linking Data in Different Sources\n\n\nTo study relations between SATD in different sources, we linked different sources using issue ID, pull request ID, and commit hash. This approach is similar to the study of Li et al. <cit.>.\nAn example of link building is shown in <ref>.\nIn this example, we can link the pull request and issue with the issue ID (#12769) (see Link A).\nMoreover, the pull request and the commit are linked with the pull request ID (#13005) (see Link B).\nFurthermore, we can also use the commit hash (0ffd5fa) to link the commit and pull request (see Link C).\n\n\n\n\n\n\n \u00a7.\u00a7 Data Cleansing\n\n\nSome comments are generated by bot users.\nBecause we focused on technical debt that was explicitly documented and admitted by developers (i.e., SATD), we removed data generated by bot users when manually analyzing the data.\nSpecifically, during data analysis, when we found that a comment is generated by a bot, we note down the username of the bot.\nThen we remove all the comments created by identified bot users.\n\n\n\n\n \u00a7.\u00a7 Data Sampling\n\n\nLi et al. observed in their study <cit.>, that whether two SATD items are related, is associated with the cosine similarity between the two items.\nSpecifically, a higher cosine similarity indicates a higher likelihood of SATD items being related, with most SATD items considered related when the cosine similarity is greater than 0.5.\nMoreover, they also discovered that the majority of the SATD item pairs have low cosine similarity, with an average similarity score of 0.135 and a standard deviation of 0.142.\nIf we randomly select pairs of SATD items from the dataset, it is likely that most of the pairs will have low similarity, thus resulting in a very small percentage of SATD items being related.\nTo solve this issue, we used the stratified sampling method, similar to the previous study <cit.>, to obtain 10 groups of samples with similarity scores between 0 and 1: 0.1 and 0.2, ..., 0.9 and 1.\nThis way of sampling ensures the diversity of the sample for analysis.\nIn total, we obtained a sample of 1,000 pairs of SATD items to be analyzed.\nIt is worth noting that in RQ1.3, we investigate whether this amount of data is indeed sufficient to train the machine learning models.\n\n\n\n\n \u00a7.\u00a7 Data Classification\n\n\n\nIn order to construct a dataset of SATD relations for training machine learning models, we first developed a classification framework through the independent analysis of 50 pairs of SATD items by the first and second authors.\nThe results of this analysis were then compared and discussed between the two authors to ensure consistency and agreement on the classification framework.\nTo further refine the framework, the first and second authors independently classified an additional 50 pairs of SATD items, resulting in a more consistent, accurate, and condensed classification framework.\nFinally, we reached an agreement on the framework, comprising two types of relationships: duplication of SATD referring to documenting identical SATD items in different sources; and repayment of SATD, referring to documenting in source A the repayment of an SATD item, originally reported in source B.\nFor example, after documenting an SATD item in code comments, developers may occasionally report it in issues.\nWe categorize this case as duplication of SATD (see more examples in <ref>).\nThe first author then independently classified 1,000 pairs of SATD items.\nOf these pairs, 197 were classified as SATD duplication and 390 pairs were classified as SATD repayment.\nTo further assess the level of agreement on the annotations, a random sample of 15% of the dataset (150 pairs) was selected and independently classified by the second author.\nCohen's kappa coefficient <cit.>, which measures the level of agreement between the classifications of the first and second authors, was calculated.\nThe results indicate that we achieved a substantial level of agreement <cit.> with a coefficient of +0.785.\nExamples of SATD relations are provided in <ref>.\n\n\n\n\n\n\n \u00a7.\u00a7 Machine Learning and Model Training\n\n\nWe propose two deep-learning approaches for automatically identifying relations between pairs of SATD items: BERT-based and CNN-based.\nIn order to provide an objective evaluation of the effectiveness of our proposed approaches for SATD relation identification, we compare their performance with one random baseline approach.\nThe details of these three approaches are presented below:\n\n\n    \n  * CNN-based Approach:\n    Text-CNN is a state-of-the-art text classification algorithm proposed by Kim <cit.> that has been widely used in several studies on SATD identification <cit.>.\n    Given its demonstrated effectiveness in identifying SATD, we leveraged Text-CNN to extract features for SATD relation identification in this study. \n    Inspired by the Siamese network architecture, we created a Siamese Text-CNN architecture to identify relations between SATD items.\n    The architecture of Text-CNN is demonstrated in <ref> (a).\n    As can be seen, our CNN-based approach utilizes Text-CNN to extract features from two input SATD items, concatenates the extracted features, and then predicts the type of relationship between the two SATD items.\n    \n    \n  * BERT-based Approach:\n    BERT is a pre-trained transformer-based neural network that has been widely utilized for natural language processing tasks such as text classification, question answering, and language translation <cit.>.\n    BERT has achieved state-of-the-art performance on various natural language understanding tasks by learning deep bidirectional representations of text.\n    Given that BERT leverages the self-attention mechanism, which helps the model to understand the relationships between different words in a sentence and how they depend on each other, it is particularly adept at capturing underlying relations in natural language.\n    Therefore, we employed the BERT base model to extract features for identifying relations between SATD items.\n    As we can see in <ref> (b), a fully-connected layer is added after feature extraction with the BERT model to predict the SATD relation types.\n    \n    \n  * Random Approach:\n    This baseline approach randomly predicts the SATD relations based on the probability of the proportion of SATD relation types present in our SATD relation dataset.\n\n\nIn this study, we implemented machine learning approaches utilizing the Pytorch library.\nFor the CNN-based approach, we set the word-embedding dimension at 300, employed filters of five different window sizes (1, 2, 3, 4, 5), and utilized 200 filters for each window size.\nThis configuration was previously found to achieve the best F1-score in identifying SATD in previous research <cit.>.\nFurthermore, we utilized the pre-trained BERT base model (uncased) for feature extraction.\nAll machine learning models were trained on NVIDIA Tesla V100 GPUs.\n\n\n\n\n\n\u00a7 RESULTS\n\n\n\n\n\n \u00a7.\u00a7 RQ1: How to automatically and accurately identify relations between SATD from different sources?\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 RQ1.1: Which deep learning approaches can effectively identify relations between SATD items?\n\n\n\nTo accurately evaluate the predictive performance of machine learning approaches, we choose to use stratified 10-fold cross-validation.\nSpecifically, we shuffle and split the manually-analyzed dataset into ten equally-sized partitions, while keeping the number of duplicated SATD pairs and repayment SATD pairs (see Section <ref>) in each division approximately equal.\nThen, we choose one of the ten subgroups for testing and the remaining nine for training, repeating the process for each subset and calculating the average precision, recall, and F1-score across ten experiments.\n\n\n\n<ref> shows the precision, recall, and F1-score of the two deep learning approaches (BERT-based and CNN-based) and the baseline approach (random).\nAs can be seen, the BERT-based approach achieved the highest F1-score of 0.759 and 0.853, for identifying SATD duplication and SATD repayment respectively, with an average F1-score of 0.806.\nMoreover, we notice that the CNN-based method achieved a lower average F1-score of 0.607, compared to the BERT-based approach.\nBoth of these deep-learning approaches significantly outperform the average F1-score of the random baseline (0.234).\n\n\n\n\nThe BERT-based approach achieves the highest F1-score of 0.759 and 0.853 for identifying SATD duplication and SATD repayment respectively, with an average F1-score of 0.806.\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 RQ1.2: How to further improve the predictive performance of the machine learning model?\n\n\n\nWhile the BERT-based model achieved an average F1-score of 0.806, there are still opportunities for improvement in identifying duplicated SATD, as evidenced by the lower F1-score of 0.759.\nOne way is to enhance the model pre-training.\n\nThe BERT model is pre-trained on a general-domain corpus, specifically a large corpus of text data comprised of the BookCorpus and English Wikipedia.\nHowever, in-domain pre-training can significantly enhance the predictive performance of the BERT model for text classification <cit.>.\nThus, we explore the effectiveness of pre-training the BERT model on the SATD data.\nBecause this task focuses on identifying SATD relations and we need domain-specific data to pre-train the BERT model, we first need to collect a sufficient amount of SATD comments for pre-training the BERT model.\nSpecifically, we utilize the pre-trained machine learning model from the previous work of Li et al. <cit.> to collect SATD items from code comments, pull requests, issues, and code commits; these are the raw data without labels.\nWe then further pre-train BERT with masked language models on the collected SATD comments.\nThe collected SATD comments for pre-training are made publicly available[1].\n\n\n\nA common issue in transfer learning is catastrophic forgetting <cit.>: during the process of acquiring new knowledge, previously obtained knowledge is erased.\nTo avoid this, we follow the suggestion of previous work <cit.> to use lower learning rates (i.e., 2e-5, 5e-5, and 1e-4).\nThe results of different pre-trained models are shown in <ref>; note that reported results are averaged over three random initializations.\nWe find that two out of three pre-trained models outperform the original BERT-base model (see row w/o pre-train in <ref>) in both identifying SATD duplication and SATD repayment.\nGenerally, using a lower learning rate for pre-training models leads to higher predictive performance.\nSpecifically, we observe that the pre-trained model with a learning rate of 2e-5 obtains the highest average F1-score of 0.829, while the model with a learning rate of 1e-4 achieves the lowest average F1-score of 0.767.\n\n\nAfter pre-training the BERT-based model, the average F1-score for identifying SATD duplication and SATD repayment is improved from 0.806 to 0.829.\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 RQ1.3: How much data is sufficient for training the machine learning models to identify related SATD?\n\n\n\nTo answer this research question, we train the BERT-based model on datasets of various sizes using the settings of the BERT-based model from <ref>.\nSpecifically, we first shuffle and split the manually-analyzed dataset into two partitions in a ratio of 1:9.\nThen, we use the large partition for training and the small partition for testing.\nBecause we want to train the model on datasets of various sizes, we create an empty training dataset, add five items to it each time, and train the machine learning model using the training data.\n\nThe results of the F1-score for identifying SATD repayment and SATD duplication, achieved while increasing the size of the training dataset, are presented in <ref>; the left subfigure (A) concerns SATD repayment while the right subfigure (B) concerns duplication.\nIn subfigure (A), we can observe that the BERT-based model can obtain a relatively good F1-score when trained on a very small training dataset.\nWhen more data is added to the training dataset, the model keeps improving until there are around 200 instances.\nAfter that, the model improves rather slowly when further increasing the training dataset, until the curve flattens completely.\nIn subfigure (B), we find that, the BERT-based model performs much worse when trained on a very small training dataset, compared to identifying SATD repayment relations.\nWhen training on the dataset from 0 to 300 instances, the F1-score goes up dramatically.\nAfter 300 instances, the F1-score improves slowly until there are around 800 instances.\nThere is a very slight upward trend for the full partition of the training dataset (900 instances).\n\n\nTo reach a high F1-score, it is sufficient to train the model with approximately 300 instances for identifying SATD duplication and 200 instances for identifying SATD repayment.\nThe current dataset seems to achieve the highest F1-score for SATD repayment, while for SATD duplication, further increasing the dataset could potentially improve the F1-score, but only slightly. \n\n\n\n\n\n\n\n \u00a7.\u00a7 RQ2: What are the sequences and quantities of SATD relations?\n\n\n\n\n  \u00a7.\u00a7.\u00a7 RQ2.1: What are the sequences of documenting related SATD in different sources?\n\n\n\nTo investigate the sequences in which related SATD items are documented, we grouped our SATD relation dataset by source and eliminated cases where there were fewer than five related SATD items.\nThrough this analysis, we observed nine major cases where the related SATD is documented in other sources. Note that `R' and `D' denote `Repayment' and `Duplication' respectively.\n\n    \n  * Issue:summary/Pull:summary \u2192\u00a0Commit (R)\n    \n    Developers create issues or pull requests to report SATD items.\n    When this kind of SATD is solved, they always document the repayment of SATD in the commit messages (in contrast this does not always happen when SATD is in the issue comments - see following case).\n    For example, a developer wanted to improve the performance of an iterator, so they created a new issue:\n    \n    \n    \u201cImprove MergeIterator performance\u201d - [Cassandra-issue-summary-8915]\n    \n\n    When the improvement is accomplished, it is then documented in the commit message:\n    \n    \n    \u201cIntroduce a more efficient MergeIterator.\u201d - [Cassandra-commit]\n    \n\n    Similarly, developers also document SATD in pull requests and then log the repayment in commit messages.\n    For example, a developer reported a problem with unnecessary calls in a new pull request:\n    \n    \n    \u201cAvoid unnecessary leaderFor calls when ProducerBatch queue empty.\u201d - [Kafka-pull-summary-7196]\n    \n\n    After solving this SATD, they mentioned the repayment in the relevant commit message:\n    \n    \n    \u201cAvoid unnecessary leaderFor calls when ProducerBatch queue empty (#7196).\u201d - [Kafka-commit]\n    \n\n    \n  * Issue:comment/Pull:comment \u2192\u00a0Commit (R)\n\n    When SATD is documented in issue comments or pull request comments during the discussion, developers sometimes record its repayment in commits.\n    For instance, a developer pointed out a piece of code that is not being used:\n    \n    \n    \u201cThe `unused-argument' is quite popular in tests...\u201d - [Airflow-pull-comment-5586]\n    \n\n    When the unused code is cleaned up, it is then recorded in the commit message:\n    \n    \n    \u201cRemove unused arguments in tests\u201d - [Airflow-commit]\n    \n    \n    \n  * Issue:summary/Pull:summary \u2192\u00a0Comment (D/R)\n\n    Developers occasionally introduce technical debt in new issues or pull requests during software development.\n    Subsequently, the same SATD item may be duplicated in other sources, such as code comments.\n    For example, a temporary solution was adopted when solving the following Helix issue:\n    \n    \n    \u201cEnable periodic rebalance as a temporary work-around for the Helix issue.\u201d - [Pinot-pull-summary-6989]\n    \n\n    After introducing this technical debt, developers documented this debt as a TODO comment in the source code:\n    \n    \n    \u201cTODO: Enable periodic rebalance per 10 seconds as a temporary work-around for the Helix issue.\u201d - [Pinot-code-comment]\n    \n\n    Documenting the repayment of issue SATD in code comments is also common.\n    For instance, a developer pointed out a consistency problem in a new issue: \n    \n    \n    \u201cMaintained consistency between local, remote, and caching table.\u201d - [Samza-pull-summary-555]\n    \n\n    When the debt was solved, they documented the repayment of the debt in a code comment:\n    \n    \n    \u201cMaintains naming consistency\u201d - [Samza-code-comment]\n    \n    \n    \n  * Issue:comment/Pull:comment \u2192\u00a0Comment (D/R)\n\n    When SATD is discussed in issue comments and developers postpone revolving them in a future iteration, they may document the same SATD in code comments.\n    For example, a developer decided to keep using the old variable names but noted that this could be improved in the future, so he created an issue comment:\n    \n    \n    \u201cI added this TODO. Kept the old config names from BoundedBBPool for BC.\u201d - [HBase-issue-comment-15525]\n    \n\n    Then, a relevant TODO comment was added to the codebase.\n    \n    \n    \u201c// TODO better config names?\u201d - [HBase-code-comment]\n    \n    \n\n    In other cases, when SATD is pointed out by developers, they sometimes solve it directly and document the repayment of it in code comments.\n    For example, a developer indicated that Java's built-in steam is much slower than loops in an issue comment:\n    \n    \n    \u201cNit: streams are significantly slower than for loops even with JDK 12.\u201d - [Beam-pull-comment-9374]\n    \n\n    Following that, the debt was solved and documented in a code comment.\n    \n    \n    \u201cUse a loop here due to the horrible performance of Java Streams\u201d - [Beam-code-comment]\n    \n    \n    \n  * Comment \u2192\u00a0Issue:summary/Pull:summary (D)\n\n    When developers decide to fix SATD which is documented in code comments, they could create an issue or a pull request for it.\n    For example, a code comment indicated that this code should be removed after the server is updated: \n    \n    \n    \u201c// TODO: remove the code for backward compatible after server updated to the latest code.\u201d - [Pinot-code-comment]\n    \n\n    Subsequently, a developer duplicated this SATD item by creating a new issue:\n    \n    \n    \u201c...after server update to the latest version, remove the code for backward compatible to get the highest performance.\u201d - [Pinot-pull-summary-910]\n    \n    \n    \n  * Comment \u2192\u00a0Issue:comment/Pull:comment (D)\n    \n    After documenting an SATD item in code comments, developers may sometimes duplicate it in issue or pull comments.\n    For example, a developer admitted an SATD item in code comments by stating:\n    \n    \n    \u201c// TODO: Include the generated file name in the response to the server\u201d - [Pinot-code-comment]\n    \n    \n    Then they mentioned the documentation of this SATD in a pull request comment:\n\n    \n    \u201c// Add a TODO to include the generated file name in the response to server\u201d - [Pinot-pull-comment]\n    \n    \n    \n  * Comment \u2192\u00a0Commit (D/R)\n\n    When developers document the existence or repayment of SATD in code comments and push those commits to repositories, they sometimes also mention the SATD in relevant commit messages.\n    For instance, developers added a TODO comment about consistency checking:\n    \n    \n    \u201c// TODO: need GraphComputer.Exceptions consistency checks\u201d - [TinkerPop-code-comment]\n    \n\n    When pushing the code to the repository, they also mentioned the introduction of TODO in the commit message.\n    \n    \n    \u201cAdd todo for consistency checks for GraphComputer exceptions.\u201d - [TinkerPop-commit]\n    \n\n    In addition, the repayment of SATD documented in code comments is also noted in commit messages.\n    For example, a developer created a code comment to indicate unnecessary caching that was optimized:\n    \n    \n    \u201c// avoid unnecessary caching of input...\u201d - [SystemDS-code-comment]\n    \n\n    The repayment of this SATD was also recorded in a commit message:\n    \n    \n    \u201cImproved rdd checkpoint injection (avoid unnecessary input caching)\u201d - [SystemDS-commit]\n    \n    \n    \n  * Comment[Deleted] \u2192\u00a0Commit (R)\n\n    When SATD is repaid and its corresponding code comments are subsequently removed, developers occasionally document the repayment of SATD in commit messages.\n    For example, one code comment mentioned the temporary workaround used to initialize static variables:\n\n    \n    \u201c[Deleted] // Servlet injection does not always work for servlet container. We use a hacking here to initialize static variables at Spring wiring time.\u201d - [CloudStack-code-comment]\n    \n\n    After this SATD was repaid, the developer documented the repayment of SATD in a commit message:\n    \n    \n    \u201cRemove temporary hacking and use Official way to wire-up servlet with injection under Spring.\u201d - [CloudStack-commit]\n    \n    \n    \n  * Issue:summary \u2192\u00a0Pull:summary (D)\n\n    After an issue is created to solve SATD items, it is common to create a pull request with a similar description to discuss the detailed solution for fixing the SATD.\n    For instance, a developer created an issue to report the problem about expensive tests:\n    \n    \n    \u201cSpeed Up Unit Tests\u201d - [Drill-issue-summary-5752]\n    \n\n    Then, they created another pull request to submit solutions and review code to solve this SATD:\n    \n    \n    \u201cSpeed Up Unit Tests add Test Categories\u201d - [Drill-pull-summary-940]\n    \n\n\n\nThere are nine major cases in which related SATD is documented in a second source, either to duplicate it or report its repayment.\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 RQ2.2: How much SATD is related between different sources?\n\n\n\nTo answer this research question, we first train our BERT-based machine learning model using the best settings described in <ref>.\nWe then use the 103 Apache projects described in <ref> to identify the related SATD item pairs using the trained machine learning model.\n\n\n\n<ref> presents the number and percentage of related SATD items across the four data sources.\nWe note that pairs of sources are highlighted in bold when they have more than 1000 related SATD items.\nWe also note that this automated analysis covers all cases of related SATD items, compared to the nine major cases of the manual analysis discussed in <ref>; to distinguish these nine major cases, we label them in the first column, according to their numbers in <ref>.\nAs we can see, most related SATD items are duplicated/repaid in code comments from various original sources.\nSpecifically, we observe that SATD items are commonly duplicated/repaid in code comments, starting from issue comments, pull comments, issue summaries, and pull summaries (41755, 16173, 8682, and 2509 respectively).\nAdditionally, we note that when starting from issue and pull comments, there is a higher likelihood of duplicating SATD in code comments, whereas from issue and pull summaries, there is more SATD repayment in code comments.\n\nAfter code comments, the commit message is the second most popular source for documenting duplicated/repaid SATD items.\nSpecifically, a large number of related SATD items are documented in commit messages based on information from added code comments, deleted code comments, issue summaries, and pull summaries (5691, 4341, 2359, and 1865 respectively).\nIn contrast to documenting related SATD in code comments, the majority of related SATD in commit messages pertains to SATD repayment rather than SATD duplication.\n\nFurthermore, we can see that a certain number of related SATD items are documented from added code comments to issue comments. \nIn addition, a small number of related SATD items are documented in other sources, such as comments and summaries of issues and pull requests.\n\n\nDevelopers predominantly document related SATD items in code comments, commit messages, and issue comments.\nAdditionally, a small number of related SATD items are documented in comments and summaries of issues and pull requests.\n\n\n\n\n\n\n\n\u00a7 DISCUSSION\n\n\n\nBased on the findings of our study, we propose the following implications for researchers:\n\n\n    \n  * This study proposes a BERT-based deep learning approach for the automated identification of relations between SATD items from four different sources.\n    This subsequently allows investigation of the impact of relations between instances of SATD.\n    One possible direction is to explore whether documenting duplicated SATD leads to a higher likelihood of SATD repayment in the future.\n    Additionally, the identified SATD relations can be utilized by researchers to conduct further studies to better understand SATD repayment.\n    For instance, one could investigate the extent to which SATD repayment is documented, why developers choose to do so, and what the best practices are for documenting related SATD items.\n    Such studies can provide a deeper understanding of the nature of SATD, helping to manage it more efficiently.\n    \n    \n  * Our approach focuses on identifying relations between pairs of SATD items; the approach cannot be used to identify relations among multiple SATD items.\n    Therefore, we suggest that researchers investigate the relations among multiple SATD items by enhancing our approach, or by developing new approaches.\n    \n    \n  * In order to facilitate further research in this field, we make our SATD relation dataset and trained machine learning models publicly available[1].\n    The former includes 1,000 pairs of SATD items, of which 197 and 390 pairs are classified as SATD duplication and SATD repayment, respectively.\n    Sharing the dataset with the research community enables other researchers to build on and improve the work that we have done, and to develop new methods and tools for identifying SATD relations.\n    \n    \n  * Our study found that relatively small datasets can achieve decent accuracy in identifying relations between SATD items across different sources (see <ref>).\n    However, we also observed that it is relatively more challenging for machine learning models to capture SATD duplication as compared to SATD repayment (see <ref>).\n    Therefore, we recommend that researchers investigate approaches to further improve the accuracy of identifying SATD duplication by augmenting the dataset specifically for SATD duplication or by adopting other machine learning techniques.\n    \n    \n  * While our study has explored the generalizability of our approach across 103 open-source projects, it is important to note that the scope of our study is still limited.\n    Thus, we recommend that researchers investigate the applicability of our approach to other projects, particularly industrial projects. Additionally, if possible, we advise researchers to make their datasets publicly available[1] for use in training new SATD relation identifiers.\n\n\nIn addition to the research implications of our study, we also propose implications for software practitioners:\n\n\n    \n  * We recommend that tool developers incorporate our SATD relation identifier into their toolkits and dashboards and experiment with it in practice. \n    Specifically, they can use our relation identifier to explore how SATD is propagated and accumulated through different sources, and look for insights for developers and managers.\n    \n    \n  * Our study presents the sequences of documenting related SATD in different sources, which can help practitioners to gain a better understanding of how SATD is created, how it spreads across different sources, and how it is being paid back.\n    This information can help practitioners develop more effective strategies for managing SATD and make more informed decisions about how to prioritize their efforts in reducing technical debt.\n\n\n\n\n\n\n\u00a7 THREATS TO VALIDITY\n\n\n\n\n\n \u00a7.\u00a7 Threats to Construct Validity\n\n\nThreats to Construct Validity concern the correctness of operational measures for the studied subjects.\nWe observed that our dataset of SATD relations is imbalanced.\nTo address this, we chose to use precision, recall, and F1-score as evaluation metrics, rather than accuracy alone. \nThis is to ensure that the performance of our classifier is evaluated in a comprehensive manner and to avoid bias.\n\n\n\n\n \u00a7.\u00a7 Threats to Reliability\n\n\nThreats to Reliability concern potential bias from the researchers in data collection or data analysis.\nRelations between SATD pairs were identified manually for training machine learning models.\nTo mitigate this threat, the first and second authors independently annotated a sample of 100 SATD pairs and reached an agreement on the classification through discussion.\nAdditionally, the level of agreement as measured by Cohen's Kappa coefficient was +0.785, indicating substantial inter-rater agreement.\nFurthermore, all data used in this study are publicly available in the replication package[1], allowing for replication and verification of the results.\n\n\n\n\n \u00a7.\u00a7 Threats to External Validity\n\n\nThreats to External Validity concern the generalization of findings.\nIn this study, we identified and predicted SATD relations from 103 open-source projects.\nThus, while our findings may be generalizable to other open-source projects of similar size and complexity to the studied Apache projects, we cannot claim broader generalizability.\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\n\nIn this research, we presented and evaluated methods for automatically identifying relations between instances of SATD across four distinct sources: code comments, issue trackers, commit messages and pull requests.\nTo accomplish this, we first gathered a dataset of 1,000 pairs of SATD items from 103 open-source projects.\nWe then conducted a manual analysis of these pairs of SATD items to identify the relations between them.\nUsing this dataset for training, we compared the predictive performance of two deep learning approaches, BERT-based and CNN-based, and one baseline method, for automatically identifying SATD relations.\nFinally, we summarized the characteristics of SATD relations with examples and presented the number of various types of SATD relations in 103 open-source projects.\n\n\nIEEEtran\n\n"}