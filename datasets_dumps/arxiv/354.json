{"entry_id": "http://arxiv.org/abs/2303.06836v2", "published": "20230313034637", "title": "Label Information Bottleneck for Label Enhancement", "authors": ["Qinghai Zheng", "Jihua Zhu", "Haoyu Tang"], "primary_category": "cs.LG", "categories": ["cs.LG"], "text": "\n\n\n\n\n\nLabel Information Bottleneck for Label Enhancement\n    Qinghai Zheng^1,\u00a0 Jihua Zhu^2[1]Corresponding author, E-mail: zhujh@xjtu.edu.cn,\u00a0 Haoyu Tang^3 \n\n\t^1College of Computer and Data Science, Fuzhou University, China \n\n\t^2School of Software Engineering, Xi'an Jiaotong University, Xi'an, China \n\n\t^3School of Software, Shandong University, Jinan, China \n\n\t\n\n\n\n\n\n\n    Received: date / Accepted: date\n========================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\n   In this work, we focus on the challenging problem of Label Enhancement (LE), which aims to exactly recover label distributions from logical labels, and present a novel Label Information Bottleneck (LIB) method for LE. For the recovery process of label distributions, the label irrelevant information contained in the dataset may lead to unsatisfactory recovery performance. To address this limitation, we make efforts to excavate the essential label relevant information to improve the recovery performance. Our method formulates the LE problem as the following two joint processes: 1) learning the representation with the essential label relevant information, 2) recovering label distributions based on the learned representation. The label relevant information can be excavated based on the \u201cbottleneck\u201d formed by the learned representation. Significantly, both the label relevant information about the label assignments and the label relevant information about the label gaps can be explored in our method. Evaluation experiments conducted on several benchmark label distribution learning datasets verify the effectiveness and competitiveness of LIB. Our source codes are available at <https://github.com/qinghai-zheng/LIBLE>\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nLearning with label ambiguity is important in computer vision and machine learning. Different from the traditional Multi-Label Learning (MLL), which employs multiple logical labels to annotate one instance to address the label ambiguity issue <cit.>, Label Distribution Learning (LDL) considers the relative importance of different labels and draws much attention in recent years <cit.>. By distinguishing the description degrees of all labels, LDL annotates one instance with a label distribution. Therefore, LDL is a more general learning paradigm, MLL can be regarded as a special case of LDL <cit.>. \n\nRecently, many LDL methods are proposed and achieve great success in practice <cit.>. Instances with exact label distributions are vital for the training process of LDL methods. Nevertheless, annotating instances with label distributions is time-consuming<cit.>. We take the label distribution annotation process of SJAFFE dataset for example here. SJAFFE dataset is the facial expression dataset, which contains 213 grayscale images collected from 10 Japanese female models, each facial expression image is rated by 60 persons on 6 basic emotions, including happiness, surprise, sadness, fear, anger, and disgust, with a five-level scale from 1 - 5, the higher value indicates the higher emotion intensity. Consequently, the average score of each emotion is served as the emotion label distribution <cit.>. Clearly, the above annotation process is costly and it is unpractical to annotate data with label distributions manually, especially when the number of data is large. Fortunately, most existing datasets in the field of computer vision and machine learning are annotated by single-label or multi-labels <cit.>, therefore, a highly recommended promising solution is Label Enhancement (LE), which attempts to recover the desired label distributions exactly from existing logical labels <cit.>.\n\nDriven by the urgent requirement of obtaining label distributions and the convenience of LE, some LE methods are proposed in recent years\u00a0<cit.>. Given a dataset X = {x_1,x_2, \u22ef ,x_n}\u2208\u211d^q \u00d7 n, in which q and n denote the number of dimensions and the number of instances, the potential label set is {y_1,y_2, \u22ef ,y_c}. The available logical labels and the desired distribution labels of X are separately indicated by L = {l_1,l_2, \u22ef ,l_n} and D = {d_1,d_2, \u22ef ,d_n}, where l_i and d_i are:\n\n    l_i = ( l_i^y_1,l_i^y_2, \u22ef ,l_i^y_c)^T,\n    \td_i = ( d_i^y_1,d_i^y_2, \u22ef ,d_i^y_c)^T.\n\nTo be specific, LE aims to recover D based on the information provided by X and L. For most existing LE methods, their objectives can be concisely summarized as follows:\n\n    min_\u03b8\u00a0f_\u03b8(X) - L_F^2 + \u03b3 reg(f_\u03b8(X)),\n\nin which D = f_\u03b8(X), f_\u03b8(\u00b7) indicates the mapping from X to D, reg(\u00b7) denotes the regularization function, and \u03b3 is the trade-off parameter. Most existing LE methods vary in reg(\u00b7). For example, GLLE <cit.> calculates the distance-based similarity matrix of data and employs the smoothness assumption <cit.> to construct reg(\u00b7); LESC <cit.> considers the global sample correlations and introduces the low-rank constraint as the regularization; PNLR <cit.> leverages reg(\u00b7) to maintain positive and negative label relations during the recovery process. Although a remarkable progress can be made by aforementioned methods, they ignore the label irrelevant information contained in X, which prevents the further improvement of recovery results. For example, in the LE task of recovering facial age label distributions, the label irrelevant information, such as specularities information, cast shadows information, and occlusions information, may result in the incorrect mapping process of f_\u03b8(\u00b7) and the unsuitable regularization of reg(\u00b7), eventually leads to the unsatisfactory recovery performance. \n\n\n\n\nTo overcome the aforementioned limitation, we present a Label Information Bottleneck (LIB) method for LE. Concretely, the core idea of LIB is to learn the latent representation H, which preserves the maximum label relevant information, from X, and jointly recovers the label distributions based on the latent representation. For the LE problem, the label relevant information is the information that describes the description degrees of labels. It is tough to explore the label relevant information directly. As shown in Fig.\u00a0<ref>, we decompose the label relevant information into two components, namely the assignments of labels to the instance and the label gaps between label distributions and logical labels. Inspired by Information Bottleneck (IB) <cit.>, LIB utilizes the existing logical labels to explore the information about the assignments of labels to the instance. Unlike simply employing the original IB on the LE task, our method further considers the information about the label gaps between label distributions and logical labels. It is noteworthy that the above two components of the label relevant information are jointly explored in our method, and that is why we term the proposed method Label Information Bottleneck (LIB). The main contributions can be summarized as follows:\n\n\t\n  \u2219We decompose the label relevant information into the information about the assignments of labels to instance and the information about the label gaps between logical labels, both of which can be jointly explored during the learning process of our method.\n\t\n  \u2219We introduce a novel LE method, termed LIB, which excavates the label relevant information to exactly recover the label distributions. Based on the original IB, which explores the label assignments information for LE, LIB further explores the label gaps information.\n\t\n  \u2219We verify the effectiveness of LIB by performing extensive experiments on several datasets. Experimental results show that the proposed method can achieve the competitive performance, compared to state-of-the-art LE methods.\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n\n\n\n \u00a7.\u00a7 Label Enhancement\n\n\nTo recover the label distributions from the existing logical labels, many efforts are made recently <cit.>. In general, most existing LE methods can be roughly divided into two categories, namely, algorithm adaptation and specialized algorithm <cit.>.\n\n\n\nAlgorithm adaptation extends some existing methods to achieve the goal of LE <cit.>. For example, FCM <cit.> recovers the label distributions by utilizing the fuzzy clustering and fuzzy relabeling. To be specific, FCM utilizes the fuzzy C-means clustering to get different clusters and cluster prototypes, then obtains membership degrees of each instance with respect to different cluster prototypes, finally annotates all instances with label distributions by employing the fuzzy composition and softmax normalization. KM <cit.> leverages the fuzzy SVM to achieve the membership function. During the recovery process, KM separates instances into two clusters and employs the nonlinear function to get the radius and distances between centers and kernelized instances, and then gets the label distributions with the help of the softmax normalization. \n\nSpecialized algorithm is specially designed to deal with the LE problem. Most existing LE methods belong to the category of specialized algorithm and have the basic objective Eq.\u00a0(<ref>). By using different constraints, different methods adopt different reg(\u00b7) in Eq.\u00a0(<ref>). For example, based on the assumption that instances closed in the feature space are more likely to share the same label, GLLE <cit.> employs the following local graph information in the feature space to boost the recovery performance:\n\n    q_i,j={   exp( -x_i-x_j^2/2\u03b5^2),if\u00a0x_j\u2208 k( i ), \n        0,\u00a0otherwise, \n    .\n\nwhere k( i ) denotes the k-nearest neighbours of x_i. reg(\u00b7) in GLLE is constructed as follows: \n\n    reg(f_\u03b8(X)) = \u2211_i,jq_i,jf_\u03b8(x_i) - f_\u03b8(x_j)_2^2.\n\nUnlike GLLE, LESC <cit.> considers the global graph information and uses the low-rank representation learning <cit.>:\n\n    G,Emin \u00a0G_*+\u03bb_2E_2,1, \u00a0s.t., X=XG+E,\n\nwhere  G indicates the low-rank representation of instances in the feature space. The regularization function in LESC is written as follows:\n\n    reg(f_\u03b8(X)) = f_\u03b8(X) - f_\u03b8(X)G_F^2.\n\n\nFor these aforementioned LE methods, they all neglect the label irrelevant information contained in X, the negative effect of which can result in the unsatisfactory recovery results. Taking the recovery of facial emotion label distributions for example, the inaccurate graph information would be obtained in GLLE and LESC with the presence of label irrelevant information, such as the identity information, hindering the further improvement of recovery results.\n\n\n\n \u00a7.\u00a7 Information Bottleneck\n\n\nInformation bottleneck (IB) <cit.> is an information theoretic principle, which describes the relevant information in data formally. To be concrete, IB has the following objective:\n\n    min_B\u00a0-I(B,C), \u00a0s.t., I(A,B) \u2a7d I_c,\n\nwhere I(\u00b7,\u00b7) measures the mutual information and I_c is the information constraint. Clearly, IB aims to learn the representation B, which preserves the relevant information about C, from A. Considering the scenario of LE, it is natural to get the following formula:\n\n    min_H\u00a0-I(H,L), \u00a0s.t., I(X,H) \u2a7d I_c.\n\nAs discussed in Section <ref>, the information merely about the assignments of labels to the instance can be explored based on Eq.\u00a0(<ref>), which neglects the vital information about the label gaps between logical labels and label distributions.\n\nRecently, IB has been successfully utilized in many real-world applications <cit.>. To the best of our knowledge, the method introduced in this paper is the first work that leverages IB to deal with the LE problem. More notably, rather than using IB simply (as shown in Eq.\u00a0(<ref>)), our method conducts more in-depth exploration to exactly recover label distributions based on IB.  \n \n\n\n\u00a7 THE PROPOSED METHOD\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 The Objective Construction\n\nGenerally, the basic idea can be written as follows:\n\n    min_H\u00a0\u2112_as + \u03b1\u2112_gap,\u00a0s.t.,I(X,H) \u2a7dI_c,\n\nwhere \u2112_as excavates the information about the assignments of labels to the instance, \u2112_gap investigates the information about the label gaps between the logical labels and distribution labels, \u03b1 is the trade-off parameter, the constraint aims to remove the label irrelevant information. The framework of our method is depicted in Fig.\u00a0<ref>. It's worth noting that employing the original IB for LE merely explores the information about the label assignments. While our LIB makes attempts to capture the information about both the label assignments and the description degrees of labels.\n\n\n\n  \u00a7.\u00a7.\u00a7 Label assignmens information modeling\n\n\nFor \u2112_as, inspired by IB, we have the following formula:\n\n    \u2112_as = -I(H,L).\n\nAccording to the concept of mutual information, \u2112_as can be rewritten out in full as follows: \n\n    \u2112_as =  - \u2211_h\u2211_lp(h,l)logp(l|h)/p(l).\n\nFor the convenience of optimization, we introduce the variational approximation q(l|h) to p(l|h). Since both the Kullback Leibler divergence and the entropy are positive:\n\n    KL(p(l|h)||q(l|h)) = \u2211_lp(l|h)logp(l|h)/q(l|h)\u2a7e 0 \n    \u21d2\u2211_lp(l|h)logp(l|h)\u2a7e\u2211_lp(l|h)logq(l|h),\n\n\n    \ud835\udd3c_p(l)[-log p(l)] = - \u2211_lp(l) log p(l)\u2a7e 0,\n\nbased on Markov chain that L\u2190X\u2192H, we can get:\n\n    L_as\u2a7d  - \u2211_x\u2211_l\u2211_hp(x,l)p(h|x)log q(l|h).\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Label gaps information modeling\n\n\nTo investigate the label-relevant information about the description degrees of labels, we introduce the label gaps between logical labels and label distributions \u0394, and consider the conditional self-information, i.e., I(\u0394|H). Therefore, we construct \u2112_gap[It can be also interpreted and derived from the view of the probability distribution: max_\u0394\u00a0log p(H,\u0394) \u21d2max_\u0394\u00a0log p(\u0394 |H) + log p(H) \u21d2max_\u0394\u00a0log p(\u0394 |H). We appreciate reviewers for their helpful comments.] as follows:\n\n    \u2112_gap = I(\u0394|H) = -log p(\u0394 |H)\n    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 = - \u2211_\u03b4\u2211_hlog p(\u03b4 |h)\n    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 = - \u2211_l\u2211_hlog p(l - d\u0302|h).\n\nwhere \u03b4 = l - d\u0302, d\u0302 is the label distribution recoveried in our method. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Label irrelevant information modeling\n\nRegarding the label irrelevant information, LIB employs the constraint in Eq.\u00a0(<ref>) to discard it during the learning process. I(X,H) can be formulated as follows:\n\n    I(X,H) =   \u2211_x\u2211_hp(x,h)logp(h|x)/p(h).\n\nSince it is difficult to calculate p(h) directly, we also introduce the variational approximation q(h) to p(h). Similar to Eq.\u00a0(<ref>), based on KL(p(h)||q(h)) \u2a7e 0, we have:\n\n    \u2211_hp(h)logp(h)\u2a7e\u2211_hp(h)logq(h).\n\nSubsequently, the following formula can be written:\n\n    I(X,H) \u2a7d\u2211_x\u2211_hp(x,h)logp(h|x)/q(h)\n    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0= \u2211_x\u2211_lp(x,l)KL(p(h|x)||q(h)).\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Objective of LIB\n\n\nBy employing the Lagrange multiplier method and combining Eq.\u00a0(<ref>), (<ref>), (<ref>), and (<ref>), we have:\n\n    \u2112 = \u2112_as + \u03b1\u2112_gap + \u03b2 I(X,H) \n    \u00a0\u00a0\u00a0\u2a7d - \u2211_x\u2211_l\u2211_hp(x,l)p(h|x,l)log q(l|h)\n    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- \u03b1\u2211_l\u2211_hlog p(l - d\u0302|h)\n    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0+ \u03b2\u2211_x\u2211_lp(x,l)KL(p(h|x)||q(h)).\n\nwhere \u03b2 is the Lagrange multiplier. Considering the bound of \u2112 and using the empirical  Monte Carlo approximation of sampling <cit.>, we have the following objective of LIB:\n\n    .887!\u2112_LIB = 1/n\u2211_i = 1^n [ - \u2211_hp(h|x_i)log q(l_i|h)\n    \n    \t\t+ \u03b2KL(p(h|x_i)||q(h))] - \u03b1\u2211_l\u2211_hlog p(l - d\u0302|h).\n\n\n\n\n \u00a7.\u00a7 The Optimization of LIB\n\n\nTo minimize the objective of \u2112_LIB, we use the reparameterization trick <cit.>. For p(h|x), we assume that:\n\n    p(h|x) \u223c\ud835\udca9(\u03bc _h|x,\u03c3 _h|x^2I),\n\t \nwhere \u03bc _h|x and \u03c3 _h|x are obtained by using the encoder network f_\u03b8_en(\u00b7), i.e., \u03bc _h|x = f_\u03b8_en^\u03bc(x) and \u03c3 _h|x = f_\u03b8_en^\u03c3(x). Subsequently, we have that:\n\n    h = \u03bc _h|x + \u03c3 _h|x\u2299\u03f5,\n\nwhere \u03f5\u223c\ud835\udca9(0,I) and \u2299 is the element-wise product. For q(l|h), we assume:\n\n    q(l|h) \u223c\ud835\udca9(\u03bc _l|h,I),\n\nwhere \u03bc _l|h is learned by using the decoder network f_\u03b8_de(\u00b7), namely, \u03bc _l|h = f_\u03b8_de(h). For q(h), we assume that:\n\n    q(h) \u223c\ud835\udca9(0,I).\n\nFor p(l - d\u0302|h), the following assumption is adopted:\n\n    p(l - d\u0302|h) \u223c\ud835\udca9(0,\u03c3 _\u03b4|h^2I),\n\nwhere \u03c3 _\u03b4|h can be achieved by introducing the gap deviation network f_\u03b8_gd(\u00b7), i.e., \u03c3 _\u03b4|h = f_\u03b8_gd(h). For the recovered label distribution d\u0302, we introduce the label distribution network f_\u03b8_ld(\u00b7) and has the following formula:\n\n    d\u0302 = f_\u03b8_ld(h).\n\n\nConsequently, based on Eq.\u00a0(<ref>)-(<ref>), we have:\n\n    .887!1pt\u00a0\u00a0min_\u03b8 _en,\u03b8 _de,\u03b8 _gd,\u03b8 _ld\u2112_LIB\n    \u21d2min_\u03b8 _en,\u03b8 _de,\u03b8 _gd,\u03b8 _ld1/n\u2211_l [1/2\u03bc _l|h - l_2^2\n    \n    \t\t+ \u03b1 (1/2(l - d\u0302)^T(\u03c3 _\u03b4|h^-2I)(l - d\u0302) + log (\u03c3 _\u03b4|h^2I))] \n     \n    \t\t+ \u03b2/2\u2211_x[\u03bc _h|x^T\u03bc _h|x + tr(\u03c3 _h|x^2I) - log (\u03c3 _h|x^2I)].\n\n\nWhen the problem of Eq.\u00a0(<ref>) is optimized, we can effectively recover the desired label distributions. To be specific, given {X,L}, we can obtain H according to Eq.\u00a0(<ref>) and achieve the recovery results based on Eq.\u00a0(<ref>), namely, D\u0302 = f_\u03b8_ld(H).\n\n\n\n \u00a7.\u00a7 Comparison with Existing LE Methods\n\n\nThe main difference between LIB and existing methods is that our method deals with the problem of LE from the perspective of information bottleneck. Considering the first term in Eq.\u00a0(<ref>), it aims to minimize d - l_2^2 under the assumption that information in the\nlabel distributions is inherited from the initial logical labels <cit.>. For LIB, the more reasonable term:\n\n    1/2(l - d\u0302)^T(\u03c3 _\u03b4|h^-2I)(l - d\u0302) + log (\u03c3 _\u03b4|h^2I)\n\nwhich can be deduced by excavating the label relevant information about the label gaps between logical labels and label distributions.\n\nBesides, we compare our method with the recently proposed LEVI <cit.> further. Although the objectives of LEVI and LIB are somewhat similar in form, they are essentially different as follows: 1) LIB makes attempts from the perspective of information bottleneck, while LEVI from the view of variational inference; 2) The formulas of LEVI and LIB are just partially similar in form, since the variational inference is employed as the optimization tool in LIB. The details of these two formulas are totally different; 3) LEVI requires an extra regularizer, i.e., d - l_2^2, to constrain the recovery process, while LIB achieves d based on the more reasonable term, i.e., Eq.\u00a0(<ref>).\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\nTo verify the effectiveness and competitiveness of LIB, extensive experiments are conducted in this section.\n\n \n\n\n\n \u00a7.\u00a7 Experimental Setup\n\n\nAs shown in Table <ref>, we use both one toy dataset and 13 real-world datasets for evaluation[http://palm.seu.edu.cn/xgeng/LDL/index.htm]. For the toy dataset, i.e., Artificial dataset, it is utilized to vividly show the recovery performance <cit.>. Movie dataset is collected from movies, SBU-3DFE and SJAFFE datasets are two facial expression datasets. Yeast datasets (alpha to spoem) are collected from 10 biological experiments on the budding yeast genes <cit.>. It is important to note that only the ground-truth label distributions are provided by these datasets. Therefore, we adopt the binarization strategy, which is also used in existing LE works <cit.>, to ensure the consistency of evaluation.\n\nWe compare our method LIB to 7 LE methods, including FCM <cit.>, KM <cit.>, LP <cit.>, ML <cit.>, GLLE <cit.>, LESC <cit.>, and LEVI <cit.>. The first two methods belong to the algorithm adaption, and the rest methods are specialized algorithms. For the sake of fairness, we utilize the parameter settings recommended in their original works. Specifically, for FCM, we set the parameter \u03b2 = 2. For KM, we leverage the Gaussian kernel. For LP, we set  the parameter \u03b1=0.5. For ML, we set the number of neighbors k=c+1. For GLLE, we select \u03bb from {0.01,0.1,...,100} and set the number of neighbors k to c+1. For LESC, \u03bb_1 and \u03bb_2 are selected from {0.0001,0.1,...,10}. For LEVI, MLPs with two hidden layers and softplus activation functions are utilized, and the results are reported after 150 training epochs. For LIB, we select \u03b1 and \u03b2 from {0.001,0.01,...,10}, and the fully connected networks with 3 layers and sigmoid activation function are leveraged in the proposed method. \n\nTo evaluate the recovery performance, we adopt 6 metrics, namely Chebyshev, Canberra, Clark, Kullback-Leibler, Cosine, and Intersection <cit.>. Given the ground-truth label distribution d and the recovered label distribution d\u0302, the first four metrics and the rest two metrics respectively measure the distance and similarity between d and d\u0302:\n\n    D_Chebyshev(d,d\u0302) =max_i| d^y_i-d\u0302^y_i|,\n    \n    \tD_Canberra(d,d\u0302) = \u2211_i=1^c| d^y_i-d\u0302^y_i|/d^y_i+d\u0302^y_i, \n    \n    \tD_Clark(d,d\u0302) = \u221a(\u2211_i=1^c( d^y_i-d\u0302^y_i)^2/( d^y_i+d\u0302^y_i)^2), \n    \n    \tD_Kullback-Leibler(d,d\u0302) = \u2211_i = 1^c d^y_ilnd^y_i/d\u0302^y_i, \n     \n    \tS_Cosine(d,d\u0302) = \u2211_i=1^cd^y_id\u0302^y_i/\u221a(\u2211_i=1^c( d^y_i)^2)\u221a(\u2211_i=1^c( d\u0302^y_i)^2), \n     \n    \tS_Intersection(d,d\u0302) = \u2211_i=1^cmin( d^y_i,d\u0302^y_i).\n\nThe smaller values of distance metric and similarity metric indicate the better and the worse results, respectively.\t\n\n\n\n\n\n \u00a7.\u00a7 Visualization Results on Toy Dataset\n\n\n\n\nThe recovery results on the Artificial dataset are vividly presented in Fig.\u00a0<ref>, which shows the three-dimensional label distributions by the RGB color channels separately. The more similar the color patterns of the recovered results and the ground-truth are, the better the recovery results are.\n\nIt can be seen that FCM, GLLE, LESC, LEVI, and LIB can obtain the similar color pattern, while KM, LP, and ML are incapable to obtain the promising recovery performance on Artificial dataset. Regarding the visualization results of FCM, GLLE, LESC, LEVI, and LIB, the color pattern that is most close to the ground-truth is achieved by our LIB. \n\n\n\n \u00a7.\u00a7 Comparison Results on Real-world Datasets\n\n\n\n\nWe provide the detailed comparison results on 13 real-world datasets in Table <ref>. Overall, LIB has the competitive recovery performance. We have the following observations: 1) Compared with FCM and KM, which belong to the category of algorithm adaption, remarkable improvements can be achieved by our method; 2) Compared with the methods belonging to the category of specialized algorithm, LIB can also obtain better recovery results in most cases. For example, LIB obtains the best recovery results on Movie datasets in all metrics. Moreover, although LESC can obtain slightly favorable results in some cases, the corresponding recovery results of LIB are also promising and competitive. The underlying reason may be that LESC further considers the sample correlations during the recovery process; 3) The recovery performance of all methods can be roughly ranked as LIB>LESC\u2248LEVI>GLLE>LP\u2248FCM>ML>KM. We can conclude that LIB is suitable for the LE problem. The underlying reason for the significant improvement is that the label relevant information, including the information about label assignments and the information about label gaps, can be effectively investigated by LIB.\n\n\n\n \u00a7.\u00a7 Analysis and Discussion of LIB\n\n\nWe analyze the parameter sensitivity of LIB firstly, and then we conduct the ablation studies as well. \n\n\n\n  \u00a7.\u00a7.\u00a7 Sensitivity of LIB\n\n\nIn the proposed method, we choose the values of \u03b1 and \u03b2 from {0.001,0.01,...,10}. To show the parameter sensitivity of LIB, we conduct experiments on SBU-3DFE datasets with different values of \u03b1 and \u03b2. Regarding the dimension of the learned latent representation, we set it to 256 for all datasets. The experimental results in metrics of Chebyshev distance, Cosine coefficient, and Intersection similarity are provided in Fig.\u00a0<ref>. It can be observed that LIB method can get promising recovery results and is robust with respect to different values of \u03b1 and \u03b2 in a large range. \n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Ablation studies of LIB\n\n\nThe ablation studies are conducted to further verify the effectiveness of introducing the label information bottleneck framework for LE. In the proposed objective Eq.\u00a0(<ref>), \u2112_as and \u2112_gap explore the label assignments information and label gaps information during the recovery process. As can be observed from Eq.\u00a0(<ref>) and Eq.\u00a0(<ref>), only the latent representation H can be learned if we employ \u2112_as merely during the recovery process. Consequently, considering the goal of LE, we compare the proposed LIB with the method termed LIB_gap, which only employs \u2112_gap to achieve the recovery results. In other words, LIB_gap investigates the label gaps information in the case of not considering the label assignments information during the recovery process.\n\nTo be specific, LIB_gap has with the following objective:\n\n    min_\u03b8 _gd,\u03b8 _ld1/2\u2211_x [(l - d\u0302)^T(\u03c3 _\u03b4|x^-2I)(l - d\u0302)  + log (\u03c3 _\u03b4|x^2I)].\n\nNotably, \u03c3 _\u03b4|x = f_\u03b8_gd(x) and d\u0302 = f_\u03b8_ld(x), which are different from the objective Eq.\u00a0(<ref>) utilized in LIB. Only the partial label relevant information, i.e., the information about the label gaps, is explored in\nLIB_gap. \n\nTable <ref> provides the recovery results of LIB_gap and LIB. It can be observed that LIB outperforms LIB_gap in all cases. Compared with LIB, LIB_gap merely makes the effort to explore the label gap information to boost the recovery performance, while LIB excavates both the information about the label assignments and the information about the label gaps jointly. Therefore, the promising recovery performance can be achieved by LIB. Furthermore, according to the results provided in Table <ref> and <ref>, the recovery results of LIB_gap seem to be competitive, which also indicates that the exploration of information about label gaps is beneficial for LE.\n\n\n\n\u00a7 CONCLUSION\n\nIn this paper, we present a new perspective to deal with the Label Enhancement (LE) problem and introduce the novel Label Information Bottleneck (LIB) method. The label relevant information is decomposed into the information about label assignments and the information about label gaps. Consequently, our method transform the LE problem into simultaneously learning the latent representation and modeling the label gaps. Extensive experiments carried on both the toy dataset and real-world datasets verify the competitiveness of LIB.\n\n\n\n\u00a7 ACKNOWLEDGMENTS\n\nThis work was supported by the National Key R&D Program of China under Grant 2020AAA0109602; the Education and Research Foundation for Middle-aged and Young Teacher of Fujian Province under Grant JAT220005; the National Natural Science Foundation of China (NSFC) under Grant 62206156; Alibaba Group through Alibaba Innovative Research Program, No.21169774.   \n\n\nieeefullname\n\n\n\n"}