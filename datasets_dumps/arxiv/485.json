{"entry_id": "http://arxiv.org/abs/2303.06643v1", "published": "20230312120858", "title": "General Boolean Formula Minimization with QBF Solvers", "authors": ["Eduardo Cal\u00f2", "Jordi Levy"], "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CC", "cs.LO"], "text": "\n\n\n\n\n\n\n\n\n\nE. Cal\u00f2 and J. Levy\n\n\n\nUtrecht University, the Netherlands\n\ne.calo@uu.nl\n\n<https://www.uu.nl/staff/ECalo> \nIIIA, CSIC, Spain\n\nlevy@iiia.csic.es\n\n<https://www.iiia.csic.es/\u00a0levy>\n\nGeneral Boolean Formula Minimization\n with QBF SolversThis project has received funding from the European Union\u2019s Horizon 2020 research and innovation program under the Marie Sk\u0142odowska-Curie grant agreement No.\u00a0860621 and the MICINN project PROOFS\n(PID2019-109137GB-C21).\n    Eduardo Cal\u00f210000-0003-3881-8994 \nJordi Levy20000-0001-5883-5746\n\n    \n================================================================================================================================================================================================================================================================================\n\n\n\n\nThe minimization of propositional formulae is a classical problem in logic, whose first algorithms date back at least to the 1950s with the works of Quine and Karnaugh. Most previous work in the area has focused on obtaining minimal, or quasi-minimal, formulae in conjunctive normal form (CNF) or disjunctive normal form (DNF), with applications in hardware design. In this paper, we are interested in the problem of obtaining an equivalent formula in any format, also allowing connectives that are not present in the original formula. We are primarily motivated in applying minimization algorithms to generate natural language translations of the original formula, where using shorter equivalents as input may result in better translations. Recently, Buchfuhrer and Umans have proved that the (decisional version of the) problem is \u03a3_2^p-complete. \n\nWe analyze three possible (practical) approaches to solving the problem. First, using brute force, generating all possible formulae in increasing size and checking if they are equivalent to the original formula by testing all possible variable assignments. Second, generating the Tseitin coding of all the formulae and checking equivalence with the original using a SAT solver. Third, encoding the problem as a Quantified Boolean Formula (QBF), and using a QBF solver. Our results show that the QBF approach largely outperforms the other two.\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nThe minimization of complex Boolean expressions is a longstanding problem in logic. The first algorithms developed in the 1950s, e.g., the works of Quine, McCluskey <cit.>, and Karnaugh <cit.> paved the way for extensions and optimizations in the following years (e.g., the Petrick's method <cit.>, and the Espresso heuristic logic minimizer <cit.>, i.a.). These works have focused on obtaining minimal equivalent representations in specific canonical forms (e.g., conjunctive normal form (CNF) or disjunctive normal form (DNF)), and confined the studies to a limited set of connectives. Here, we are interested in the general Boolean formula minimization, where no assumptions are made in the form of the input formula or the output. In fact, our minimization methods allow us to use distinct sets of connectives for the input and the output.\n\nWe frame Boolean minimization (i.e., finding the logically equivalent shortest formula(e) to a given one) as a Quantified Boolean Formulae (QBF) satisfiability problem and design an algorithm that consistently finds the shortest equivalents of a given formula. We compare this algorithm with a brute force baseline, and an approach based on SAT.\n\n\n\n  \u00a7.\u00a7.\u00a7 Motivation.\n\nWe have two distinct motivations behind our work, which lay very far from each other. Our first motivation is shared in\u00a0<cit.>, where the authors present qbf2epr, a tool that translates QBF to formulas in effective propositional logic (EPR). Their aim is to generate benchmarks for EPR and compare solvers for QBF and EPR. Similarly, our formula minimization problem, encoded as QBF, generates benchmarks for QBF solvers and allows us to compare SAT and QBF techniques. The automated deduction community is divided into sub-communities (e.g., SAT, QBF, SMT, MaxSAT, EPR), which try to solve distinct classes of problems, from SAT which is NP-complete, to EPR which is NEXPTIME-complete, passing by QBF which is PSPACE-complete, and each one has its own competition and set of benchmarks. However, many ideas that proved effective in one area (like learning in SAT) have been exported to others. In this sense, problems that could be solved with two distinct technologies, like ours, contribute to comparing the level of maturity reached in each area.\n\nOur second motivation relates to a use case of minimization algorithms in natural language processing. Grasping the meaning of logical formalisms is a crucial task for many scholars, yet sometimes even experienced logicians might have trouble deciphering a complex formula. Techniques from natural language generation <cit.>, and in particular logic-to-text generation methodologies <cit.>, can be used for simplifying and translating logical formulae into optimally intelligible text in natural languages (NLs) (such as English, Mandarin, or Korean), which can effectively explain formulae to systems' users. For example, given the following first-order formula:\n\n    \u2203 x ( Problem ( x ) \u2227\u2200 y ( Researcher ( y ) \u2192 Interested ( y , x ) ) )\n\n\nwe want a system that can automatically generate a faithful and comprehensible explanation, via the following (or another semantically equivalent) text:\n\n\n\nThere is a problem that every researcher finds interesting.\n\n\n\nWhat are the characteristics that a formula should have to become a suitable input for a logic-to-text translation system? One aspect that one might want to look at is length. Brevity has surrounded linguistic debate at least since <cit.>. Arguably, shorter utterances should be preferred over longer ones and unnecessary prolixity should be avoided. This principle might also apply to logical formulae. Intuitively, a short formula, rather than a longer logical equivalent, should be better suited to be translated into NL. In this paper, we tackle exclusively the logical aspect of the problem. We focus on propositional logic, a formalism in which equivalence is decidable, and limit our examination to formulae's length,[We define length as the number of symbols (i.e., predicates and connectives, parentheses excluded) contained in a formula.] aiming only for the shortest equivalents to a given formula.\n\n\n\n  \u00a7.\u00a7.\u00a7 Related Work: Formula Minimization.\n \nBoolean formula minimization is a natural optimization problem in the second level of the Polynomial-Time Hierarchy \u03a3^p_2. Indeed, the problem is used by <cit.> to motivate the definition of the Polynomial Hierarchy. Its decisional version can be formulated as the following problem: Given a Boolean formula, prove the existence of a (smaller) formula (in the same set of variables) that gets the same evaluation of the given formula, for all possible assignments of the variables. The fact that both sets of quantified variables, as well as the time to evaluate the formulae, are bounded in the input proves its inclusion in \u03a3^p_2. As we will see in Section\u00a0<ref>, this corresponds to our brute-force algorithm. It is assumed that both the given formula and its minimization are circuits or formulae of the same form. However, in our implementation, we leave open the possibility to use distinct sets of connectives. Apart from some completeness proofs for some particular forms of the input and output, the proof for the general form had eluded researchers until <cit.> proved \u03a3^p_2-completeness of the problem.\n\nThe optimization of complex Boolean expressions has been studied extensively in electronic circuits, where practical matters (i.e., complex circuits take up physical space and costs more resources in their implementation) make it crucial to find optimal circuit representations. Well-known minimization methods include the Quine-McCluskey algorithm <cit.> and the Karnaugh map <cit.>. In the Karnaugh map, Boolean results are transferred from a truth table onto a two-dimensional grid, where each cell position represents one combination of input conditions, while each cell value is the corresponding output value. Optimal groups of 0s and 1s are identified, which represent the terms of a canonical form that can be used to write a minimal expression. The Quine\u2013McCluskey algorithm finds all the prime implicants of a function and uses them in a chart to find (i) the essential prime implicants of the function, and (ii) other prime implicants that are necessary to cover the function. The method is functionally identical to the Karnaugh map, but its tabular form makes it more efficient to employ in computer systems.\n\nHowever, despite this long history of research and attempts to extend well-established methods (e.g., <cit.> tries to implement the XOr operator in the Quine-McCluskey algorithm), most work has focused on a limited set of connectives and canonical forms (e.g., CNF or DNF). For our scope, we need a more general approach where all connectives could be tackled, on demand.\n\n\n\n  \u00a7.\u00a7.\u00a7 Quantified Boolean Formulae.\n \n\nQuantified Boolean Formulae (QBFs) are an extension of propositional logic, where universal and existential quantifications are allowed <cit.>. The use of quantifiers results in a greater expressive power than classic propositional logic. If all variables occurring in a QBF \u03d5 are bound, then \u03d5 is called closed. QBFs often assume a canonical prenex conjunctive normal form (PCNF) \u03d5 = \u2203x\u20d7\u2200y\u20d7\u2203z\u20d7\u22ef\u03c8, where the portion containing only quantifiers and bound variables is called the prefix, followed by \u03c8 that is a quantifier-free Boolean formula with conjunctions over clauses, called the matrix.\n\nThe QBF satisfiability problem <cit.> consists of determining, for a given QBF \u03d5, the existence of an assignment for the free variables, such that \u03d5 evaluates to true under this assignment. Hence, \u03d5 is true iff, there exists a truth assignment to x\u20d7, such that, for all truth assignments to y\u20d7, there exists a truth assignment to z\u20d7,\u2026such that \u03c8 is true. Several QBF solvers have been developed over time,[<http://www.qbflib.org>] and applications of QBFs technologies range from AI to planning <cit.>. QBF solvers only use to provide the instantiation of most externally existentially-quantified variables x\u20d7, since for the other ones, instantiation depends on previous universal variables z\u20d7= f(y\u20d7). In this work, we exploit QBFs to encode and solve the Boolean minimization problem.\n\n\n\n  \u00a7.\u00a7.\u00a7 Structure of the Paper.\n\nThe rest of the paper is structured as follows. \nSection\u00a0<ref> introduces the algorithms that we employ in our experiments and the QBF encoding we develop. Section\u00a0<ref> illustrates the experiments we carry out, comparing the three aforementioned approaches, and shows the results. We present some reflections on possible future directions in Section\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 ALGORITHMS\n\n\nIn our experimentation, we analyze three algorithms that we will call brute-force, SAT-based, and QBF-based.\n\n\n\nThe brute-force algorithm (see Alg.\u00a0<ref>) is the algorithm that we mention in Section\u00a0<ref> as proof that formula minimization is in \u03a3_2^p. Two formulae \u03d5 and \u03c8 are equivalent iff \u03d5\u2194\u03c8 is a tautology. Like TAUT, the formula equivalence problem is CoNP-complete. However, considering that we test the equivalence for all formulae \u03c8 smaller than \u03d5, the average time for the calls to equivalent(\u03d5,\u03c8) is the same as considering \u03c8 a random formula smaller than \u03d5. Then, the average time required by the function call is only \ud835\udcaa(|\u03d5|). Notice that in this situation, half of the calls finish after checking one assignment, 1/4 after checking two assignments, etc. Hence, on average we check \u2211_i=1^2^|\u03d5| i1/2^i< 2 assignments in every call.\n\nWe can also estimate the number of calls to this function as follows. The number of distinct complete trees of size n that we can construct with k binary symbols is k^n. If the trees can have any form, then the computation is more complicated. Let \ud835\udc9e be the set of possible binary symbols (hence, we are not considering Not) and \ud835\udcb1 be the set of possible leaves. The number of forms of trees with m binary nodes and m+1 leaves is given by the recurrence f(m) = \u2211_i=0^m-1 f(i) f(m-i-1) that define the Catalan numbers C_m. The number of distinct trees will be C_m |\ud835\udc9e|^m |\ud835\udcb1|^m+1. Using this Stirling approximation, this can be approximated as 4^m/\u221a(\u03c0)m^3/2 |\ud835\udc9e|^m |\ud835\udcb1|^m+1. As a function of the tree size n=2m+1, this is \ud835\udcaa((4|\ud835\udc9e||\ud835\udcb1|)^n/2 / n^3/2) calls to the equivalent function.\n\n\n\nThe second algorithm (see Alg.\u00a0<ref>) is based on the use of a SAT solver and the Tseitin encoding of the two formulae that we want to prove equivalent. Given two formulae \u03d5, \u03c8, we can find, in linear time |\u03d5|+|\u03c8|, a CNF formula \u0393 such that the two formulae are equivalent iff \u0393 is not satisfiable. At first sight, it may not look reasonable to use a SAT solver to check a property that on average only requires linear time. However, experiments show that, in practice, we still can get some gain with respect to the brute-force algorithm (see Section\u00a0<ref>).\n\n\n\nThe third algorithm (see Alg.\u00a0<ref>) is based on the use of a QBF solver. Here, instead of testing every possible minimal formula \u03c8, we test every possible depth \u03b4. This supposes a significant improvement since there is a linear number of depths to try, instead of an exponential number of formula candidates. Second, instead of a Tseitin encoding of the candidate, we compute a scheme of the candidate. The equivalence between the original formula and this scheme can be encoded as a QBF formula with three quantifier alternations: \u2203x\u20d7.\u2200y\u20d7.\u2203z\u20d7.\u0393. In Alg.\u00a0<ref>, these three sets of variables are represented as \u2203^(1), \u2200^(1), and \u2203^(2) and individual variables are named x, y, and z, respectively. If the QBF formula is true, the values we got for variables x\u2208\u2203^(1) will encode the minimal formula. Notice that QBF solvers only provide the instantiations of the most external existentially-quantified variables, since the values of the other existentially-quantified variables depend on more externally universally-quantified variables.[In the case of using a QBF solver unable to provide these instantiations, we cannot compute the minimal equivalent formula.] Basically, for every node i of the scheme and every truth constant, variable y of the original formula, or connective c, we have a variable x_c^i that gets the value true when at position i we have the connective c (resp. variable y or constant). The constraints CNF({x_false + \u2211_y\u2208\u2200^(1) x_y + \u2211_c\u2208\ud835\udc9e x_c = 1}) ensure that one, and only one, of them get the value true. Variables in \u2200^(1) are just the set of variables in the original formula. The original formula and the scheme are equivalent if for all assignments to these variables, both the original formula and scheme get the same evaluation. Variables in z^i\u2208\u2203^(2) encode the truth values for every possible subformula at position i of the scheme or of the Tseitin encoding of the original formula. The clauses in the QBF formula encode restrictions of the form \n\n    [                         x^i_\u2192 z^i;                   x^i_y \u2192(z^i\u2194 y); x_c^i \u2192 (z^i \u2194 z^i\u00b7 1  c  z^i\u00b7 2) ]\n\nThe intended meanings of these constraints are: if at position i of the scheme we have the constant false, the sub-scheme is evaluated to false, if there is an original variable y\u2208\u2200^(1), it is evaluated to y, and if there is a connective c\u2208\ud835\udc9e, then the sub-scheme gets the same value as the connective c operated on the evaluations z^i\u00b7 1 of the left-child of i and the evaluation z^i\u00b7 2 of the right-child.\n\nNotice that the size of the QBF formula we get is \ud835\udcaa(2^depth(\u03d5)\u00b7 (|\ud835\udc9e|+|Vars(\u03d5)|)). Assuming that the original formula is balanced, in practice 2^depth(\u03d5)\u2248 |\u03d5|. Therefore, we could consider it a polynomial encoding. Notice also that we do not make a profit from the commutativity and associativity of most connectives.\n\nWhen in a node of the scheme we put a Not, we only use one of the children, and in the case of putting a variable, we do not use any of the children. To avoid useless search in the QBF solver we can force all these useless nodes to be fixed to a dummy value by adding the constraints CNF({x^i_\u2192 x^i\u00b7 2_dummy}) and CNF({(x^i_y \u2228 x^i_dummy\u2228 x^i_false) \u2192 (x^i\u00b7 1_dummy\u2227 x^i\u00b7 2_dummy)}).\n\nIn this approach, we only bound the depth of the scheme. If we also want to limit its size, we can add the encoding of some cardinality constraint that bound the number of nodes in the schema that are distinct from the dummy: \n\u2211_i  x^i_dummy\u2264 size_bound\n\n\n\n\n\u00a7 EXPERIMENTS AND RESULTS\n\n\nWe conduct some experimentation with our algorithms. The three algorithms are implemented in Python 3 and are publicly available at <https://gitlab.nl4xai.eu/eduardo.calo/QBF-boolean-minimization>. In the case of the SAT-based algorithm, we use the Python module python-sat[<https://github.com/pysathq/pysat>] as SAT solver. In the case of the QBF-based algorithm, we use the CAQE\u00a0<cit.> QBF solver, although any other QBF solver that accepts QDIMACS standard[<http://www.qbflib.org/qdimacs.html>] input and output may be used. \n\nFor every size in s=1,\u2026,20, we generate 100 random formulae of size s and minimize them using the three algorithms. We make sure that all syntactically distinct formulae are generated with the same probability. However, we do not take into account the commutativity and associativity of connectives or other formula equivalences. Formulae of size s are generated over a set of \u221a(s) variables[Using s/c or s^c variables does not seem to affect substantially the results.] and connectives \ud835\udc9e={Not, And, Or} and minimization are searched among formulae with connectives \ud835\udc9e'={Not, And, Or, Implies}.\n\nWe use a cluster with 11 calculating nodes with 2 Intel Xeon CPUs at 2.2 GHz with 10 cores/CPU and 92 GB of RAM. We set a time-out of 20,000s. The brute-force and the SAT-based algorithms reach the time-out in some instances for s=15,18,19,20. These values are not considered in the computation of the mean and median times. Therefore, these mean and median values are abnormally low. \n\n\n\nIn Figure\u00a0<ref> (left), we show the average and median (logarithm of) CPU time required by each one of the algorithms as a function of the size of the input formula. We clearly observe that the QBF-based algorithm outperforms the other two algorithms, which seem to require exponential time on the size of the input. We also observe that the SAT-based is consistently better than the brute-force algorithm (a constant distance between the functions, in logarithmic axes, means an improvement of constant factor). This is quite surprising since, as we mention in Section\u00a0<ref>, the computation of the formula equivalence can be done in linear average time. It is also remarkable that, in the case of brute-force and SAT-based, there is a significant difference between the average and median time. The reason, as we comment in detail below, is the significant variability in the times required by each instance. The same effect produces a fluctuation in the values of the average time. We can conclude that, although in most of the instances (attending to the median), the three algorithms minimize the formula in less than one second, for sizes smaller than 20, just a few instances make brute-force and SAT-based require around 1h on average when the size is around\u00a020.\n\nIn Figure\u00a0<ref> (right), we show the average and median (logarithm of) CPU time as a function of the size of the obtained minimal formula. Here the differences between the mean and median times are smaller. Hence, we can conclude that the size of the output determines the time required by the algorithms. Again, it is clear that the QBF-based algorithm outperforms the other two. We still observe that the median time is smaller than the average time, which indicates that significant variability still exists. Curiously, the times depend on the parity of the formula sizes: even-size formulae are easier than odd-size formulae. The reason could be that, except in the case of negation, the rest of the connectives are binary. \n\n\n\nIn Figure\u00a0<ref> (left), we show how the average size of the minimized formula grows with respect to the size of the original formula. We observe that the growth is close to the square root of the original size. Recall that we generate random formulae of size s and with \u221a(s) variables. Curiously, we observe that odd-size formulae are simplified more than even-size formulae, although the reason for this is not clear. In Figure\u00a0<ref> (right), we show the distribution of sizes of the minimized formulae (for original formulae of sizes 20 and 8).\n\n\n\nAs mentioned above, we observe significant variability in CPU times, for the brute-force and SAT-based algorithms. In Figure\u00a0<ref>, we sort the instances in decreasing order of CPU time and represent, in double logarithmic axes, these times for the 100 instances. We observe that this representation is close to a line (truncated on the top due to time-outs) with an increasing (negative) slope when the size increases. This implies that the CPU time in these algorithms follows a power-law probability distribution, where the time required by a few instances is responsible for most of the average time. The standard solution in these situations is to use some kind of restart policy or some randomization of the algorithm. In our case, we could randomize the order of the candidates to minimal formulae. However, since we want to obtain the minimal equivalent formula, we cannot randomize the order of the sizes of formulae that we try. \n\n\n\n\u00a7 CONCLUSION AND FUTURE WORK\n\n\nIn this paper, we have analyzed the practical use of three algorithms for general Boolean formula minimization. A simple algorithm that proves that the problem is in \u03a3^p_2, one based on the use of a SAT solver to check formula equivalences, and one that uses a Tseitin encoding of a formula's scheme and a QBF solver. We show that the third one clearly outperforms the other two. Therefore, the use of QBF solvers represents the state-of-the-art for the Boolean minimization problem.\n\nOur experiments have been limited to Boolean formulae. The first natural extension of this work would be to see if this or similar methods could scale up to other (more expressive) formalisms, e.g., first-order logic (FOL). This would open up a range of interesting research questions, as in FOL, equivalence is undecidable. Adapting the QBF approach would probably not be feasible, yet, a semi-brute force approach, e.g., using a first-order theorem prover, could prove successful.\n\nsplncs04\n\n\n"}