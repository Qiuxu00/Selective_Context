{"entry_id": "http://arxiv.org/abs/2303.06812v1", "published": "20230313020155", "title": "Weighted Euclidean balancing for a matrix exposure in estimating causal effect", "authors": ["Juan Chen", "Yingchun Zhou"], "primary_category": "stat.ME", "categories": ["stat.ME"], "text": "\n\t\n\t\n\t1]Juan Chen\n\t1]Yingchun Zhou Corresponding author:  yczhou@stat.ecnu.edu.cn\n\t[1]Key Laboratory of Advanced Theory and Application in Statistics\n\t\tand Data Science-MOE, School of Statistics, East China Normal University.\n\t\n\t\n\tWeighted Euclidean balancing for a matrix exposure in estimating causal effect\n    [\n    \n==============================================================================\n\n\n\t\tIn many scientific fields such as biology, psychology and sociology, there is an increasing interest in estimating the causal effect of a matrix exposure on an outcome.  Covariate balancing is crucial in causal inference and both exact balancing and approximate balancing methods have been proposed in the past decades. However, due to the large number of constraints, it is difficult to achieve exact balance or to select the threshold parameters for approximate balancing methods when the treatment is a matrix. To meet these challenges, we propose the weighted Euclidean balancing method, which approximately balance covariates from an overall perspective. This method is also applicable to high-dimensional covariates scenario. Both parametric and nonparametric methods are proposed to estimate the causal effect of matrix treatment and theoretical properties of the two estimations are provided. Furthermore, the simulation results show that the proposed method outperforms other methods in various cases. Finally, the method is applied to investigating the causal relationship between children's participation in various training courses and their IQ. The results show that the duration of attending hands-on practice courses for children at 6-9 years old has a siginificantly positive impact on children's IQ.\n\t\n\tKeywords: causal inference, matrix treatment, weighting methods, overall imbalance, observational study. \n\t\n\t\n\n\u00a7 INTRODUCTION\n\n\tFor decades, causal inference has been widely used in many fields, such as biology, psychology and economics, etc. Most of the current research is based on univariate treatment (binary, multivalued or continuous treatment)  (<cit.>; <cit.>; <cit.>; <cit.>; <cit.>; <cit.>; <cit.>; <cit.>; <cit.>). However, one may be interested in the causal effect of a matrix treatment. For example, in studying the impact of children's participation in training courses on children's intelligence (measured by IQ), the exposure is a matrix, whose rows represent different age groups, columns represent the types of trainging courses and each element represents the number of hours of class per week. The existing methods are not suitable for matrix exposure and there have been few research on this. Therefore, the goal of this paper is to develop a new method to estimate the causal effect function for matrix exposure.\n\tTo estimate causal effects in observational studies, it is common to use propensity scores (<cit.>; <cit.>; <cit.>). There are several classes of propensity score-based methods, such as matching, weighting and subclassification, that have become part of applied researchers' standard toolkit across many scientific displines (<cit.>; <cit.>). In this article we focus on the weighting method. \n\tIn the past decade, various weighting methods have been proposed to balance covariates in the estimation procedure (<cit.>; <cit.>; <cit.>). The key idea of these methods is to estimate propensity score ( <cit.>; <cit.>; <cit.>; <cit.>).\n\t\n\tWhen using the parametric method to model the propensity score, the estimation bias of the causal effect will be large if the model is mis-specified. Therefore, some nonparametric methods for estimating the propensity score have been proposed, such as the kernel density estimation (<cit.>). In addition, in recent years, some studies have used optimized weighting methods to directly optimize the balance of covariates (<cit.>; <cit.>; <cit.>).\n\t\n\tThese methods avoid the direct construction of the propensity scores, therefore the obtained estimate achieves higher robustness. One of the methods, the entropy balancing method, has been established as being doubly robust, in that a consistent estimate can still be obtained when one of the two models, either the treatment assignment model or the outcome model, is correctly specified (<cit.>).\n\t\n\tFurthermore, this method can be easily implemented by solving a convex optimization problem. Here we extend the entropy balancing method to the matrix treatment scenario to balance the covariates.\n\tThe methods mentioned above assume that all balancing conditions hold exactly, that is, they are exact balancing methods. However, the balancing conditions cannot hold exactly when the dimension of covariate or treatment is high as there will be too many equations to hold simultaneously. For matrix treatment, it is even more difficult for the balancing conditions to hold exactly. To meet this challenge, literatures have shown that approximate balance can trade bias for variance whereas exact balance cannot and the former works well in practice in both low- and high-dimensional settings (<cit.>; <cit.>). The potential limitations of the existing approximate balancing methods are that they directly control univariate imbalance, which cannot guarantee the overall balance especially in the high-dimensional scenario. Besides, there is no principled way to select the threshold parameters simultaneously in practice. Another potential issue of the univariate approximate balancing methods is that it is difficult to handle high-dimensional constraints since the theoretical results require that the number of the balancing constraints should be much smaller than the sample size(<cit.>).\n\tTo alleviate the limitations of univariate balancing methods, we propose an overall balancing approach, which is called Weighted Euclidean balancing method. The weight is obtained by optimizing the entropy function subject to a single inequality constraint, hence the issue of tuning multiple threshold parameters in univariate balancing methods is solved. The Weighted Euclidean distance is defined to measure the overall imbalance and a sufficient small value of the distance suggests that the covariates are approximately balanced from the overall perspective. Moreover, we propose an algorithm to deal with high-dimensional constraints, so that the proposed method is not restrictive to the low-dimensional setting.\n\tThe main contributions of the paper are summarized as follows. First, an overall balancing method (Weighted Euclidean balancing method) is proposed, which extends the binary univariate entropy approximate balancing method to the matrix treatment scenario. Unlike univariate approximate balancing method, the Weighted Euclidean balancing method controls the imbalance  from the overall perspective. Moreover, to the best of our knowledge, it is the first time that matrix treatment is studied by weighting method in causal inference literature. Second, both parametric and nonparametric causal effect estimation methods for matrix treatment are proposed. Under the parametric framework, a weighted optimization estimation is defined and its theoretical properties are provided. Under the nonparametric framework, B-splines are used to approximate the causal effect function and the convergence rate of the estimation is provided. Third, the proposed method is applied to explore the impact of children's participation in training courses on their IQ and meaningful results are obtained.\n\tThe remainder of this article is organized as follows: In Section 2, the preliminaries are introduced. In Section 3, the Weighted Euclidean balancing method (WEBM) is proposed. In Section 4, the theoretical properties of the WEBM method are shown. In section 5, a numerical simulation is performed to evaluate the performance of the WEBM method under finite samples. In Section 6, the WEBM method is applied to analyze a real problem. The conclusions and discussions are summarized in Section 7.\n\t\n\t\n\t\n\t\n\t\n\t\n\n\u00a7 PRELIMINARY\n\n\t\n\n \u00a7.\u00a7 Notation and assumptions\n\n\tSuppose an independent and identically distributed sample (\ud835\udc19_1,\u2026,\ud835\udc19_n) is observed, where the support of \ud835\udc19 = (\ud835\udc13,\ud835\udc17,Y) is \ud835\udcb5=(\ud835\udcaf\u00d7\ud835\udcb3\u00d7\ud835\udcb4). Here \ud835\udc13\u2208 R^p\u00d7 q denotes a matrix exposure, \ud835\udc17\u2208 R^J denotes a vector of covariates, and Y \u2208 R denotes the observed outcome. Since the causal effect is characterized by potential outcome notion, let Y(t) for all t\u2208\ud835\udcaf denotes the potential outcome that would be observed under treatment level \ud835\udc2d, i.e. Y = Y(t) if T= t.  \n\t\n\tIn this paper, our goal is to estimate the causal effect function \ud835\udd3c(Y(\ud835\udc2d)), which is defined in terms of potential outcomes that are not directly observed. Therefore, three assumptions that commonly employed for indentification are made (<cit.>; <cit.>).\n\n\t\n\t\n\tAssumption 1 (Ignorability):\n\n\t\n\n\tT_i \u22a5 Y_i(t) |X_i, which implies that the set of observed pre-treatment covariates \ud835\udc17_i, is sufficiently rich such that it includes all confounders , i.e. there is no unmeasured confounding.\n\t\n\n\t\n\tAssumption 2 (Positivity):\n\n\t\n\n\tf_T|X(T_i = t|X_i ) > 0 for all t\u2208\ud835\udcaf, which means that treatment is not assigned deterministically.\n\n\t\n\t\n\tAssumption 3 (SUTVA):\n\n\t\n\n\tAssume that there is no interference among the units, which means that each individual's outcome depends only on their own level of treatment intensity.\n\t\n\n\t\n\tUnder the above assumptions, we first define the stabilized weight as\n\n\t\n    w_i = f(T_i)/f(T_i |X_i),\n\n\tthen one can estimate the causal effect function based on the stabilized weight with observational data.\n\t\n\t\n\n \u00a7.\u00a7 Exact entropy balancing and approximate entropy balancing for matrix exposure\n\n\tThe entropy balancing method (<cit.>)\n\t\n\tis used to determine the optimal weight for inferring causal effects. It has been used for univariate treatment and here this method is extended to matrix exposure and to balance covariates approximately.\t\n\tNote that the stabilized weight \n\n\t\n    w_i = f(T_i)/f(T_i |X_i)\n\n\tsatisfies the following conditions for any suitable functions u(\ud835\udc13) and v(\ud835\udc17):\n\t\n    \ud835\udd3c(w_iu(\ud835\udc13_i)v(\ud835\udc17_i)) =    \u222cf(T_i)/f(T_i |X_i)u(\ud835\udc13_i)v(\ud835\udc17_i)f(\ud835\udc13_i, X_i)dT_idX_i \n       =\u222b{f(T_i)/f(T_i |X_i)u(T_i)f(T_i |X_i)dT_i } v(X_i)f(X_i)dX_i \n       =\ud835\udd3c(u(\ud835\udc13_i))\ud835\udd3c(v(\ud835\udc17_i))\n\n\tBesides, it also satisfies that \n\t\n    \ud835\udd3c(w_i) = \u222cf(T_i)/f(T_i |X_i)f(T_i, X_i)dT_idX_i = 1.\n\n\tHowever, Equation (2) implies an infinite number of moment conditions, which is impossible to solve with a finite sample of observations. Hence, the finite dimensional sieve space is considered to approximate the infinite dimensional function space. Specifically, let \n\t\n    u_K1(\ud835\udc13) = (u_K1,1(\ud835\udc13), u_K1,2(\ud835\udc13),\u2026, u_K1,K1(\ud835\udc13))^', \n    \n    \t\tv_K2(\ud835\udc17) = (v_K2,1(\ud835\udc17), v_K2,2(\ud835\udc17), \u2026, v_K2,K2(\ud835\udc17))^'\n\n\tdenote the known basis functions, then \n\t\n    \ud835\udd3c(w_i u_K1(\ud835\udc13_i) v_K2(\ud835\udc17_i)^') = \ud835\udd3c(u_K1(\ud835\udc13_i))\ud835\udd3c(v_K2(\ud835\udc17_i)^').\n\n\t\n\tIn practice, the covariate balancing conditions given in Equation (4) cannot hold exactly with high dimensional covariates or treatments. It is even more difficult to hold exactly for matrix exposure. To overcome this difficulty, approximate balance is considered rather than exact balance, which has been demonstrated to work well in practice in both low- and high-dimensional settings (<cit.>; <cit.>; <cit.>). Specifically, the balancing weights that approximately satisfy the conditions in Equation (4) are the global minimum of the following optimization problem:\n\t\n    min_\ud835\udc30\u2211_i=1^nw_ilog(w_i)\n\n\ts.t.\n\t\n    |1/n\u2211_i=1^nw_i u_K1,l(\ud835\udc13_i)v_K2,l\u0303(\ud835\udc17_i) - (1/n\u2211_i=1^n u_K1,l(\ud835\udc13_i)) (1/n\u2211_i=1^nv_K2,l\u0303(\ud835\udc17_i)) |\u2264\u03b4_l,l\u0303,\n\n\twhere u_K1,l(\ud835\udc13_i) and v_K2,l\u0303(\ud835\udc17_i) denote the lth and l\u0303th components of u_K1(\ud835\udc13_i) and v_K2(\ud835\udc17_i), respectively.\n\tLet m_K(\ud835\udc13_i, \ud835\udc17_i) = vec(1/n u_K1(\ud835\udc13_i)v_K2(\ud835\udc17_i)^') and m\u0305_K = vec (1/nu\u0305_K1v\u0305_K2^')  denote two column vectors with dimension K, where K= K1 K2, the lth and l\u0303th components of u\u0305_K1 and v\u0305_K2 are defined as\n\t\n    u\u0305_K1,l = 1/n\u2211_i=1^n u_K1,l(\ud835\udc13_i)  and v\u0305_K2,l\u0303 = 1/n\u2211_i=1^n v_K2,l\u0303(\ud835\udc17_i),\n\n\tthen condition (5) is equivalent to \n\t\n    min_w \u2211_i=1^nw_ilog(w_i)\n\n\ts.t.\n\t\n    |\u2211_i=1^nw_i m_K,k(\ud835\udc13_i, \ud835\udc17_i)  -  nm\u0305_K,k|\u2264\u03b4_k,  k= 1,\u2026,K.\n\n\tHowever, there is a large number of tuning parameters (\u03b4_1,\u2026,\u03b4_K) which is very time-consuming to determine and there is lack of guideline on tuning these parameters simultaneously in practice. \n\t\n\t\n\t\n\n\u00a7 METHODOLOGY\n\n\tDue to the potential issues of univariate approximate balancing methods, the weighted Euclidean balancing method is proposed in this section, whose key idea is to control the overall imbalance in the optimization problem (7). \n\t\n\n \u00a7.\u00a7 Weighted Euclidean balancing method\n\n\t\n\tDefine the following weighted Euclidean imbalance measure (WEIM) as a weighted version of the squared Euclidean distance:\n\t\n    WEIM = \u2211_k=1^K{\u03bb_k^2 [\u2211_i=1^n w_i (m_K,k(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K,k)]^2 }.\n\n\tThe weighted Euclidean balancing obtains the balancing weights that approximately satisfy the condition (4) by solving the following convex optimization problem:\n\t\n    min_w \u2211_i=1^nw_ilog(w_i)\n\n\ts.t.\n\t\n    \u2211_k=1^K{\u03bb_k^2 [\u2211_i=1^n w_i (m_K,k(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K,k)]^2 }\u2264\u03b4,\n\n\twhere (\u03bb_1,\u2026, \u03bb_K) is a pre-sepecified weight vector and \u03b4\u2265 0 is a threshold parameter. Assume that condition (3) holds exactly, whose sample condition is 1/n\u2211_i=1^n w_i = 1.\n\t\n\t\n\tNote that univariate exact balance is equivalent to the overall exact balance, in the sense that \u2211_i=1^n w_i (m_K,k(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K,k)=0, k= 1,\u2026,K  is equivalent to WEIM=0. However, the univariate approximate balance does not imply the overall approximate balance since it is possible that \u2211_i=1^n w_i (m_K,k(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K,k), k= 1,\u2026,K is small while the WEIM is large.\n\tThe pre-specified vector  (\u03bb_1,\u2026, \u03bb_K) reflects the importance of each univariate constraint. In this paper, we set \u03bb_k= \u03c3_k^-1, where \u03c3_k^2 is the variance of m_K,k(\ud835\udc13,\ud835\udc17). Since problem (9) is difficult to solve numerically, its dual problem is considered here, which can be solved by numerically efficient algorithms. Theorem 1 provides the dual formulation of problem (9) as an unconstrained problem.\n\t\n\n\t\n\tTheorem 1.  Assume that max_i (max_k |\u03bb_km_K,k(\ud835\udc13_i,\ud835\udc17_i) |) < \u221e, the dual of problem (9) is equivalent to the following unconstrained problem \n\t\n    min_\u03b8\u2208 R^K\u2211_i=1^nexp( \u2211_k=1^K\u03b8_j \u03bb_j (m_K,k(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K,k))  + \u221a(\u03b4)||\u03b8||_2,\n\n\t\n\tand the primal solution \u0175_i is given by \n\t\n    \u0175_i = exp{\u2211_k=1^K\u03b8\u0302_k \u03bb_k (m_K,k(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K,k) -1} ,  i=1,\u2026,n,\n\n\twhere \u03b8\u0302 is the solution to the dual optimization problem (10). \n\tThe proof of Theorem 1 is in Appendix A.1. \n\t\n\t\n\n  \u00a7.\u00a7.\u00a7 Selection of tuning parameter\n\n\tAnother practical issue that arises with weighted Euclidean weights is how to choose the degree of approximate balance \u03b4. A tuning algorithm is proposed as follows. First, determine a range of positive values \ud835\udc9f for \u03b4, then the optimal value of \u03b4 is selected by the following algorithm.\n\t\n\n\t\n\t Algorithm 1. Selection of \u03b4.\n\n\tFor each \u03b4\u2208\ud835\udc9f,\n\t\n\t\t\n  1.  Compute the dual parameters \u03b8\u0302 by solving the dual problem (10);\n\t\t\n  2.  Compute the estimated weights \u0175_i using equation (11);\n\t\t\n  3.  Calculate WEIM in (8) using \u0175_i;\n\t\n\tOutput \u03b4^* that minimizes WEIM.\n\t\n\t\n\n  \u00a7.\u00a7.\u00a7 Weighted Euclidean balancing with high-dimensional covariates\n\n\tIn the high-dimensional or ultra high-dimensional covariate setting with K relatively large compared to n or K>>n, it becomes difficult to control the overall imbalance using the Weighted Euclidean balancing method. To meet this challenge, we propose an algorithm to select a small subset of the covariates in the sparse setting. Specifically, consider v_K2(\ud835\udc17) = (1, \ud835\udc17) in the high-dimensional setting. Let Bcor_j be the ball correlation (<cit.>) between X_j and \ud835\udc13, j=1,\u2026,L. Rank X_1, \u2026,X_L as X_(1),\u2026,X_(L) such that X_(1) has the largest Bcor value, X_(2) has the second largest Bcor value, and so forth. The covariates are added successively according to the rank of ball correlation until there is a break point of WEIM's, and the set added before the break point appears is the target set. The key idea hings on WEIM, which represents the contribution of the j most imbalanced covariates to the overall imbalance after WEBM weighting.  If WEIM remains stable as j inceases, it indicates that the overall imbalance can be controlled. However, if there is a break point at Step j, it implies that adding the  jth covariates greatly inceases WEIM, which is harmful to the control of the overall imbalance. Therefore, the algorithm should be stopped and print the outputs at Step j-1. Specifically, procedures to select the subset of covariates are given by the following algorithm.\n\n\t\n\t\n\t Algorithm 2. Subset selection of covariates in the high dimensional case.\n\n\tFor each j \u2208{ 1,\u2026, L},\n\t\n\t\t\n    compute the estimated weights \u0175_i^(j) using v_K2(\ud835\udc17) = (1,X_(1),\u2026, X_(j));\n\t\t\n     calculate WEIM^(j) in (8) using \u0175_i^(j);\n\t\t\n    add (j,WEIM^(j)) to the x-y plot and observe whether there is a break point at (j,WEIM^(j)):\n\t\t\n\t\t\t\n    If no, let j=j+1;\n\t\t\t\n    If yes, stop and output L_0 = j-1.\n\t\t\n\t\n\tThe selected subset of the covariates is (X_(1),\u2026, X_(L_0)).\n\t\n\t\n\t\n\t\n\n \u00a7.\u00a7 Causal effect estimation\n\n\t\n\tIn this subsection, both parametric and nonparametric approaches are developed to estimate the causal effect function. A weighted optimization estimation is defined under the parametric framework and broadcasted nonparametric tensor regression method (<cit.>) is used to estimate the causal effect function under the nonparametric framework.\n\t\n\n  \u00a7.\u00a7.\u00a7 Parametric approach\n\n\tThe causal effect function is parametrized as s(\ud835\udc2d;\u03b2), assume that it has a unique solution  \u03b2^* defined as\n\t\n    \u03b2^* = agrmin_\u03b2\u222b_\ud835\udcaf\ud835\udd3c[Y(t)- s(\ud835\udc2d;\u03b2) ]^2f_T(t)dt.\n\n\t\n\tThe difficulty of solving Equation (12) is that the potential outcome Y(t) is not observed for all t. Hence, Proposition 1 is proposed to connect the potential outcome with the observed outcome. \n\n\t\n\tProposition 1  Under Assumption 1, it can be shown that\n\t\n    \ud835\udd3c[w(Y- s(\ud835\udc2d;\u03b2) )^2] = \u222b_\ud835\udcaf\ud835\udd3c[Y(t)- s(\ud835\udc2d;\u03b2) ]^2f_T(t)dt.\n\n\tThe proof of Proposition 1 can be found in Appendix A.2. Note that Y(t) on the right hand side of Equation (13) represents the potential outcome and Y on the left hand side represents the observed outcome. Proposition 1 indicates that by having w on the left hand side of Equation (13), one can represent the objective function with the potential outcome (right side) by that with the observed outcome (left side). Therefore, the true value \u03b2^* is also a solution of the weighted optimization problem:\n\t\n    \u03b2^* = argmin_\u03b2\ud835\udd3c[w(Y- s(\ud835\udc2d;\u03b2))^2].\n\n\tThis result implies that the true value \u03b2^* can be identified from the observational data.\n\tOne can obtain the estimator based on the sample, which is\n\t\n    \u03b2\u0302 = argmin_\u03b2\u2211_i=1^n\u0175_\u0302\u00ee(Y_i- s(\ud835\udc13_i;\u03b2) )^2.\n\n\t\n\t\n\n  \u00a7.\u00a7.\u00a7 Nonparametric approach\n\n\tSuppose \ud835\udd3c(Y(\ud835\udc2d)) = s(\ud835\udc2d). In a similar manner to the proof of Proposition 1, it can be shown that \n\t\n    \ud835\udd3c(wY |\ud835\udc13=t) = \ud835\udd3c(Y(\ud835\udc2d)).\n\n\tExisting work of nonparametric tensor regression suffers from a slow rate of convergence due to the curse of dimensionality. Even if one flattens the tensor covariate into a vector and applies common nonparametric regression models such as additive models or single-index models to it, this issue still exists. Besides, when dealing with a vectorized tensor covariate, one would ignore the latent tensor structure and this might result in large bias. To meet these challenges, we adopt the broadcasted nonparametric tensor regression method (<cit.>) to estimate the causal effect function s(\ud835\udc2d).\n\tThe main idea of the broadcasted nonparametric tensor regression is to use the (low-rank) tensor structure to discover important regions of the tensor so as to broadcast a nonparametric modeling on such regions. Specifically, assume that \n\t\n    s(\ud835\udc13) = c+1/pq\u2211_r=1^R<\u03b2_1^(r)\u2218\u03b2_2^(r), F_r(\ud835\udc13)>,\n\n\twhere c\u2208 R,  \ud835\udc13\u2208 R^p \u00d7 q,  F_r(\ud835\udc13) = \u212c(f_r, \ud835\udc17), \u212c is a broadcasting operator, which is defined as\n\t\n    (\u212c(f,\ud835\udc13))_i_1,i_2 = f(T_i_1,i_2),  for all i_1,i_2.\n\n\tThe broadcasted functions f_r, r=1,\u2026,R, will be approximated by B-spline functions, i.e.,\n\t\n    f_r(x) \u2248\u2211_d=1^D\u03b1_r,db_d(x),\n\n\twhere \ud835\udc1b(x) = (b_1(x),\u2026,b_D(x))^' is a vector of B-spline basis functions and \u03b1_r,d's are the corresponding spline coefficients. Define \u03b1_r = (\u03b1_r,1,\u2026,\u03b1_r,D)^' and (\u03a6(\ud835\udc13))_i_1,i_2,d = b_d(T_i_1,i_2), the regression function (16) can be approximated by \n\t\n    s(\ud835\udc13) \u2248 c+1/pq\u2211_i=1^R<\u03b2_1^(r)\u2218\u03b2_2^(r)\u2218\u03b1_r, \u03a6(\ud835\udc13)>.\n\n\tTo separate out the constant effect from f_r's, the condition \u222b_0^1 f_r(x) dx=0 is imposed, which leads to \n\t\n    \u222b_0^1\u2211_d=1^D\u03b1_r,db_d(x)dx=0,  r=1,\u2026, R.\n\n\tThen the following optimization problem is considered:\n\t\n    argmin_c,\ud835\udc06 \u2211_i=1^n (\u0175_iY_i- c-1/pq<\ud835\udc06,\u03a6(\ud835\udc13_i)>)^2\n\n\ts.t.\n\t\n    \ud835\udc06 = \u2211_r=1^R\u03b2_1^(r)\u2218\u03b2_2^(r)\u2218\u03b1_r  \n    \u2211_d=1^D\u03b1_r,d\u222b_0^1 b_d(x)dx =0,  r=1,\u2026,R,\n\n\tand the estimated regression function is \n\t\n    \u015d(\ud835\udc13) = \u0109+1/pq <\ud835\udc06\u0302,\u03a6(\ud835\udc13)>,\n\n\twhere (\u0109,\ud835\udc06\u0302) is a solution of (20). \n\tSince optimization problem (20) contains too many constraints, it is not computationally efficient to solve it directly. To further simplify the optimization problem, an equivalent truncated power basis (<cit.>) is used to reduce the constraints. Specifically, let b\u0303_d(x), d=1,\u2026,D denote the truncated basis:\n\t\n    b\u0303_1(x)=1,b\u0303_2(x)=x,\u2026, b\u0303_\u03c2(x) = x^\u03c2-1,\n    b\u0303_\u03c2+1(x) = (x-\u03be_2)_+^\u03c2-1,\u2026, b\u0303_D(x)=(x-\u03be_D-\u03c2+1)_+^\u03c2-1,\n\n\twhere \u03c2 and (\u03be_2,\u2026,\u03be_D-\u03c2+1) are the order and the interior knots of the aforementioned B-spline, respectively. Based on these basis functions, consider the following optimization\n\t\n    argmin_c\u0303,\ud835\udc06\u0303 \u2211_i=1^n (\u0175_iY_i- c\u0303-1/pq<\ud835\udc06\u0303,\u03a6\u0303(\ud835\udc13_i)>)^2\n\n\ts.t.\n\t\n    \ud835\udc06\u0303 = \u2211_r=1^R\u03b2_1^(r)\u2218\u03b2_2^(r)\u2218\u03b1\u0303_r,\n\n\twhere \u03a6\u0303(\ud835\udc13)_i_1,i_2,d = b\u0303_d+1(\ud835\udc13_i_1,i_2), d=1,\u2026,D and \u03b1\u0303_r \u2208 R^D-1 is the vector of coefficients. Compared with (20), the mean zero constraints are removed by reducing one degree of freedom in the basis functions. According to <cit.>, Lemma B.1, one can show that\n\t\n    \u015d(\ud835\udc13) = \u0109\u0303\u0302+1/pq <\ud835\udc06\u0302\u0303\u0302,\u03a6\u0303(\ud835\udc13)>,\n\n\twhere (\u0109\u0303\u0302,\ud835\udc06\u0302\u0303\u0302) is the solution of (22). \n\tThe optimization problem (22) can be solved by the scaled-adjusted block-wise descent algorithm (<cit.>).\n\t\n\t\n\n\u00a7 THEORETICAL PROPERTIES\n\n\tIn this section, the large sample properties of the proposed estimators in section 3 are established. First the consistency of the estimated weight in section 3.1 is shown, then the consistency of the parametric estimator in section 3.2.1 and the convergence rate of the nonparametric estimator in section 3.2.2 are shown. The following assumptions are made. \n\t\n\n\tAssumption 4\n\t\n\t\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n  * There exists a constant c_0 such that 0 < c_0 < 1, and c_0 \u2264exp (z-1) \u2264 1-c_0 for any z= M\u0303_K(\ud835\udc2d,\ud835\udc31)^'\u03b8 with \u03b8\u2208int(\u0398). Besides, exp (z-1) = O(1) in some neighborhood of z^* = M\u0303_K(\ud835\udc2d,\ud835\udc31)^'\u03b8^*, where M\u0303_K(\ud835\udc2d,\ud835\udc31)= \u039b (m_K(\ud835\udc2d,\ud835\udc31)-m\u0305_K) and \u039b= diag(\u03bb_1,\u2026,\u03bb_K).\n\t\t\n  * There exists a constant C such that \n\t\tE {M\u0303_K(\ud835\udc13_i, \ud835\udc17_i)M\u0303_K(\ud835\udc13_i, \ud835\udc17_i)^'}\u2264 C. \n\t\t\n  * \u03b4 = o(n).\n\t\t\n  * sup_(\ud835\udc13,\ud835\udc17) exp{\u2211_j=1^K\u03b8_j^* \u03bb_j [m_K,j(\ud835\udc13,\ud835\udc17)-Em_K,j(\ud835\udc13,\ud835\udc17)] } = O(1).\n\t \n\t\n\t\n\tAssumption 5\n\t\n\t\t\n  * The parameter space \u0398_1 is a compact set and the true parameter \u03b2_0 is in the interior of \u0398_1.\n\t\t\n  * (Y-s(T;\u03b2))^2 is continuous in \u03b2, \ud835\udd3c[sup_\u03b2(Y-s(T;\u03b2))^2] < \u221e and sup_\u03b2\ud835\udd3c[(Y-s(T;\u03b2))^4]  < \u221e.\n\t \n\t\n\t\n\tAssumption  6\n\t\n\t\t\n  * s(\ud835\udc2d;\u03b2) is twice continuously differentiable in \u03b2\u2208\u0398_1 and let h(\ud835\udc2d;\u03b2) \u2261\u25bd_\u03b2 s(\ud835\udc2d;\u03b2).\n\t\t\n  * \ud835\udd3c{ w(Y-s(\ud835\udc13;\u03b2))h(\ud835\udc13;\u03b2) } is differentiable with respect to \u03b2 and \n\n\t\tU \u2261 - \u25bd_\u03b2\ud835\udd3c{ w(Y-s(\ud835\udc13;\u03b2))h(\ud835\udc13;\u03b2) }|_\u03b2=\u03b2^* is nonsingular.\n\t\t\n  * \ud835\udd3c[sup_\u03b2| Y-s(T;\u03b2) |^2+\u03b4] < \u221e for some \u03b4 >0 and there extists some finite positive constants a and b such that \ud835\udd3c[sup_\u03b2_1:  ||\u03b2_1-\u03b2||  < \u03b4_1| s(T;\u03b2_1) - s(T;\u03b2) |^2]^1/2 < a\u00b7\u03b4_1^b for any \u03b2\u2208\u0398_1 and any small \u03b4_1 >0.\n\t\n\t\n\t\n\t\n\tAssumption 7\n\t\n\t\t\n  * The treatment \ud835\udc13\u2208 [0,1]^p\u00d7 q has a continuous probability density function f, which is bounded away from zero and infinity.\n\t\t\n  * The vector of random errors, \u03f5 = (\u03f5_1,\u2026,\u03f5_n)^', has independent and identically distributed entries. Each \u03f5_i is sub-Gaussian with mean 0 and sub-Gaussian norm \u03c3 < \u221e.\n\t\t\n  * The true broadcasted functions f_0r\u2208\u210b, r= 1,\u2026,R_0. Here \u210b is the space of functions from [0,1] to R satisfying the H\u00f6lder condition of order \u03c9, i.e.,\n\t\t\n    \u210b = { g: | g^(l) (x_1)-g^(l) (x_2) |\u2264 S_1| x_1-x_2 |^\u03c9, \u2200 x_1,x_2 \u2208 [0,1] },\n\n\t\tfor some constant S_1>0, where g^(l) is the l-th derivative of g, such that \u03c9\u2208 (0,1] and \u03c4 = l+\u03c9 >1/2.\n\t\t\n  * The order of the B-spline used in (16) satisfies \u03c2\u2265\u03c4+1/2. Let 0= \u03be_1 < \u03be_2 <\u2026 < \u03be_D-\u03c2+2=1 denote the knots of B-spline basis and assume that\n\t\t\n    h_n = max_d=1,\u2026, D-\u03c2+1|\u03be_d+1\n    \t\t\t-\u03be_d |\u224d D^-1 and h_n/min_d=1,\u2026, D-\u03c2+1|\u03be_d+1\n    \t\t\t\t-\u03be_d |\u2264 S_2\n\n\t\tfor some constant S_2>0.\n\t \n\t\n\tAssumption 4(1) enables consistency of \u03b8\u0302 to translate into consistency of the weights.  Assumption 4(2) is a standard technical condition that restricts the magnitude of the basis functions; see also Assumption 4.1.6 of <cit.> and Assumption 2(2) of <cit.>. Assumption 4(3) requires that the threshold parameter \u03b4 should be much smaller than the sample size. Assumption 4 (4) is needed for consistency of the estimated weight. Assumption 5(1) is a commonly used assumption in nonparametric regression. Assumption 5(2) is an envelope condition applicable to the uniform law of large numbers. Assumption 6(1) and (2)  impose sufficient regularity conditions on the causal effect function and its derivative function. Assumption 6(3) is a stochastic equicontinuity condition, which is needed for establishing weak convergence (<cit.>). Assumption 7(1), (3) and (4) are common in nonparametric regression models. In particular, Assumption 7(3) and (4) regularize the space where the true broadcasted functions lie in and guarantee that they can be approximated welll by B-spline functions. Similar assumptions can be found in <cit.> and <cit.>. Assumption 7(2) is a standard tail condition of the error. Based on these assumptions, the following theorems are established. \n\n\t\n\n\t\n\tTheorem 2. \tLet \u03b8\u0302 denote the solution to Problem (10) and \n\t\n    \u0175_i = exp{\u2211_k=1^K\u03b8\u0302_k \u03bb_k (m_K,k(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K,k) -1} ,  i=1,\u2026,n,\n\n\tthen under Assumptions 1-4,\n\n\t\n\t\t\n\t\t\n  * \u222b|\ud835\udc30\u0302-\ud835\udc30^* |^2 dF(\ud835\udc2d,\ud835\udc31) = O_p(n^-1).\n\t\t\n  * 1/n\u2211_i=1^n|\u0175_i- w_i^* |^2 = O_p(n^-1).\n\t\n\tBased on Theorem 2, we can establish the consistency of the parametric estimator and the convergence rate of the nonparametric estimator.\n\n\t\n\tTheorem 3  \n\t\n\t\t\n  * Under Assumptions 1-5, ||\u03b2\u0302-\u03b2^* || \u2192_p 0. \n\t\t\n  * Under Assumptions 1-6, \u221a(n)(\u03b2\u0302-\u03b2^*) \u2192_d N(0,V), where \n\t\t\n    V = 4U^-1\u00b7\ud835\udd3c{ w^2(Y-s(\ud835\udc13;\u03b2^*))^2h(\ud835\udc13;\u03b2^*)h(\ud835\udc13;\u03b2^*)^'}\u00b7 U^-1\n\n\t\n\t\n\tTheorem 4  If Assumptions 1-4 and 7 hold, R\u2265 R_0, and \n\t\n    n > S_1 h_n^-2-2/log(h_n)(log^-2(h_n))(R^3+R(p+q)+RD)\n\n\tfor some large enough constant S_1>0, then \n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n    ||\u015d(\ud835\udc13)-s_0(\ud835\udc13) ||^2 = O_p  ( R^3+R(p+q)+RD/n )+O_p  ( {\u2211_r=1^R_0||vec(\ud835\udc01_0r)||_1/pq}^2 1/D^2\u03c4 ),\n\n\twhere s_0(\ud835\udc13) = c_0+1/pq\u2211_r=1^R_0<\u03b2_1^(0r)\u2218\u03b2_2^(0r), F_0r(\ud835\udc13)> represents the true regression function.\n\tThe proofs of Theorem 2, 3 and 4 can be found in Appendix A.3, A.4 and A.5, respectively. \n\t\n\t\n\t\n\n\u00a7 SIMULATION\n\n\tTo evaluate the finite sample performance of the proposed method, simulation studies are carried out under different data settings. The main motivation of the simulation is to compare the proposed method with three other methods when the outcome model are linear and nonlinear in various ways.\n\t\n\n \u00a7.\u00a7 The low-dimensional covariate setting\n\n\tIn this subsection, we compare the performance of the proposed method (WEBM) with the unweighted method (Unweighted), entropy balancing method (EB) and univariate approximate balancing method (MDABW) in the low-dimensional covariate setting, where EB refers to the method proposed by <cit.> that balances covariates exactly and MDABW refers to the method proposed by <cit.> that balances covariates approximately.\n\tSince the covariates are shared across all scenarios, their data generating process is first described. Specifically, we independently draw 5 covariates from a multivariate normal distribution with mean 0, variance 1 and covariance 0.2, that is,\n\t\n\t\n    \ud835\udc17 = (X_1,....,X_5)^'\u223c  N_5(\u03bc,\u03a3)  with \u03bc= [ 0; \u22ee; 0 ]and \u03a3=[     1   0.2     \u2026   0.2;   0.2     1 0.2 \u2026   0.2;     4;   0.2   0.2     \u2026     1 ]_5\u00d75.\n\n\t\n\n\t\n\tConsider a linear treatment assignment model, which is defined as\n\t\n    \ud835\udc13_i= X_i1\ud835\udc01_1+X_i2\ud835\udc01_2+X_i3\ud835\udc01_3+\ud835\udc04_i,\n\n\twhere \ud835\udc01_j = [ 1 0; 0 1; 1 1 ]_3\u00d72, j=1,2,3 denotes the jth coefficient matrix, and \ud835\udc04_i \u2208 R^3\u00d7 2 denotes the error matrix, whose element follows a standard normal distribution. For the outcome model, we consider four scenarios and conduct 100 Monte Carlo simulations for each scenario. The first two scenarios assume an outcome model that is linear in treatment and the others assume a nonlinear relationship.\n\tIn scenario 1, the linear outcome model is defined as\n\t\n    Y_i = 1+ <\ud835\udc01, \ud835\udc13_i>+X_i1+(X_i2+1)^2+X_i4^2+\u03f5_i,\n\n\twhere \ud835\udc01 = [ 1 0; 0 1; 1 1 ]_3\u00d72 and \u03f5_i \u223c N(0,2^2). In this scenario, v_k2(\ud835\udc17) = (1, \ud835\udc17, \ud835\udc17*\ud835\udc17)^', where * represents the Hadamard product of two matrices, and the corresponding element of \ud835\udc17*\ud835\udc17 is (X*X)_ij = (x_ijx_ij).\n\t\n\tIn scenario 2, the linear outcome model is defined as\n\t\n    Y_i = 1+ <\ud835\udc01, \ud835\udc13_i>+X_i2+X_i3+X_i1X_i2+\u2026+X_i4X_i5+\u03f5_i,\n\n\twhere \ud835\udc01 and \u03f5_i are the same as in Equation (21). In this scenario, the interaction terms are strong confounders, hence set v_k2(\ud835\udc17) = (1, \ud835\udc17, X_jX_k)^', 1\u2264 j < k \u2264 5.  \n\t\n\tIn scenarios 3 and 4, the nonlinear outcome models are considered and are defined as\n\t\n    Y_i = 1+ <\ud835\udc01, F_1(\ud835\udc13_i)>+X_i1+(X_i2+1)^2+X_i4^2+\u03f5_i,\n\n\tand\n\t\n    Y_i = 1+ <\ud835\udc01, F_1(\ud835\udc13_i)>+X_i2+X_i3+X_i1X_i2+\u2026+X_i4X_i5+\u03f5_i,\n\n\trespectively.\n\tHere \ud835\udc01 and \u03f5_i are the same as in Equation (24), (F_1(\ud835\udc13))_k_1,k_2= f_1(T_k_1,k_2)= T_k_1,k_2+0.6sin{2\u03c0(T_k_1,k_2-0.5)^2 }. For all four scenarios, consider u_K1(\ud835\udc13) =(1, vec(\ud835\udc13)^')^' for simplicity.\n\tFor each method, the mean RMSE and its standard deviation of the coefficient estimates for the linear outcome model, and those of the fitted values for the nonlinear outcome model are reported based on 100 data replications. \n\t\n\tTable 1 shows the mean RMSE and its standard deviation of the coefficient matrix for the linear outcome model. Observe that the mean RMSE of WEBM is the smallest among the four methods, and the standard deviation of WEBM is the second smallest while that of Unweighted is the smallest in both scenario 1 and scenario 2. Besides, the results of scenario 2 indicate that MDABW and EB  have poor performance when the basis function of covariates includes interaction entries, and their mean RMSEs are even larger than those of Unweighted. The mean RMSE and standard deviation of all methods decreases as the sample size increases.\n\tTable 2 shows the mean RMSE and its standard deviation of the fitted values of the nonlinear outcome model. As can be seen, the results for both scenario 3 and 4 are similar to those in Table 1, that is, WEBM performs the best with both the smallest mean RMSE in all cases. Similarly, MDABW and EB methods have poor performance when the basis function of covariates includes interaction entries and the mean RMSE and standard deviation of all methods decreases as the sample size increases.\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n \u00a7.\u00a7 The high-dimensional covariate setting\n\n\tIn this subsection, since the MDABW and EB methods can only deal with the low-dimensional covariate case, we compare the performance of the proposed method (WEBM) with the Unweighted method and the method (Mapping) proposed by <cit.>, which selects a small important subset of covariates by a joint screening procedure, \n\tin the high-dimensional covariate setting. Since the Mapping method can only deal with linear outcome model, these two methods are compared in the linear outcome model setting.\n\tFor both methods, set the sample size n=500 and the dimension of \ud835\udc13 to be 3 \u00d7 2. Consider two scenarios (scenario 5-6) with the dimension of covariates  L=49 and L=99, respectively. The motivation for such settings is to consider the number of constraints K < n (scenario 5) and K>n (scenario 6), respectively. For the basis functions, u_K1(\ud835\udc13)= (1,vec(\ud835\udc13)^')^' and v_K2(\ud835\udc17) = (1,\ud835\udc17^')^' are considered.\n\tAdditionally, covariates are drawn from a multivariate normal distribution with mean 0, variance 1 and covariance 0.2. For each scenario, consider a linear treatment assignment model, which is defined as\n\t\n    \ud835\udc13_i= X_i1\ud835\udc01_1+X_i2\ud835\udc01_2+\u2026+X_i5\ud835\udc01_5+\ud835\udc04_i,\n\n\twhere \ud835\udc01_j = [ 1 0; 0 1; 1 1 ]_3\u00d72, j=1,2,3,4,5 denotes the jth coefficient matrix, and \ud835\udc04_i \u2208 R^3\u00d7 2 denotes the error matrix, whose element follows a standard normal distribution. \n\tMoreover, the linear outcome model is defined as\n\t\n    Y_i = 1+ <\ud835\udc01, \ud835\udc13_i>+X_i1+X_i2+\u2026+X_i5+\u03f5_i,\n\n\t\n\t\n\t\n\t\n\t\n\twhere \ud835\udc01=[ 1 0; 0 1; 1 1 ]_3\u00d72, \u03f5_i \u223c N(0,2^2).\n\tFor each method, the mean RMSE and its standard deviation of the tensor regression estimation for the linear outcome model are reported based on 100 data replications.  \n\tTable 3 shows the mean RMSE and its standard deviation of coefficent matrix for the linear outcome model in the high-dimensional covariate setting. The results indicate that WEBM performs the best with the smallest mean RMSE in all cases.\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n\u00a7 APPLICATION\n\n\tIntelligence Quotient (IQ) is based on biological attributes, most of which are inherited from parents. It mainly refers to a person's cognitive abilities, such as logical reasoning, pattern recognition, and short-term memory. Of course, due to genetic mutations, parents with average IQ may have offspring with superior intelligence, and vice versa. But IQ also has social attributes. Studies have found that IQ has obvious plasticity, and environmental factors affect IQ levels. From 1947 to 2002, the IQ level of developed countries rose steadily at a rate of 3% every 10 years, which is called the \"Flynn effect.\" This effect has been repeatedly observed in various countries, various age groups, and a large number of different environments, and has become a reliable evidence that \"environment affects IQ\" (<cit.>). In particular, literatures have shown that taking training lessons, such as music, sports, chess and so on, can significantly enhance children's IQ (<cit.>; <cit.>; <cit.>; <cit.>). \n\tUnfortunately, there are some limitations of existing studies. First, existing studies have only analyzed the relevant effects of attending single training course at a fixed age on children's IQ. In practice, however, a child may attend different training courses at the same age and participation may also vary by age. Second, existing studies mainly investigated the impact of whether or not to attend training classes, ignoring the effect of class duration. Third, existing studies only analyzed the correlation between children's participation in training courses on children's IQ, while their causal relationship is much more of a concern. To overcome the aforementioned limitations, the proposed method is applied to a matrix treatment, which contains the structural information of children's participation in training courses, to investigate their causal impact on children's IQ.\n\tThe data are obtained from the Brain Science Innovation Institute of East China Normal University's Child Brain Intelligence Enhancement Project, whose goal is to explore brain development and help improve brain power. The treatment we are interested in is a 2 \u00d7 5 matrix about children's participation in training courses, whose rows represent age groups, including 3-6 years old and 6-9 years old, columns represent the types of training courses, including knowledge education (Chinese, mathematics and English, etc.), art (music, art, calligraphy, etc.), sports (swimming, ball games, etc.), hands-on practice (STEM, Lego, etc.) and thinking training (logical thinking, EQ education, attention, etc.), and each element of the matrix represents the number of hours of class per week. The outcome is children's IQ and the pre-treatment covariates include children's gender as well as parental education, which have been shown to be associated with both the treatment and outcome variables (<cit.>; <cit.>; <cit.>; <cit.>). A complete-case analysis is conducted with a sample of 103 participants.  \n\tBefore estimating the causal effect, we first examine the covariate balancing of WEBM, MDABW and EB methods based on the WEIM statistics. The statistic WEIM defined in equation (8) is 0.1050 for the WEBM method, 0.3761 for the MDABW method and 0.9881 for the EB method, which implies that WEBM balances covariates well while EB does not. Assume a linear tensor outcome model and the bootstrap method with 200 replicates is used to obtain confidence intervals for the parameter estimates. The results are shown in Table 4.\n\t\n\t\t\n\t\t\n\t\t\n\t\n\tTable 4 shows the estimated causal effects of the duration of attending different classes at different ages on children's IQ. It can be seen that most methods (except EB) suggest that the duration of attending hands-on practice courses at 6-9 years old has a siginificantly positive impact on children's IQ. This finding is not only consistent with previous findings that participation in hands-on practice classes can improve children's IQ (<cit.>; <cit.>; <cit.>),  but further suggests that longer participation in hands-on practice classes is more beneficial to children's IQ.\n\t\n\tThe results imply that future work of an intervention study about attending hands-on practice training courses, which in turn may improve children's IQ, is suggested. Besides, the width of confidence interval of the estimated causal effect based on WEBM is the smallest, which implies that the estimation accuracy of WEBM is the highest. \n\t\n\t\n\t\n\t\n\t\n\n\u00a7 CONCLUSION AND DISCUSSION\n\t\n\tIn this study, the weighted Euclidean balancing method is proposed, which obtains stabilized weights by adopting a single measure that represents the overall imbalance. An algorithm for the high-dimensional covariate setting is also proposed. Furthermore, parametric and nonparametric methods are developed to estimate the causal effect and their theoretical properties are provided. The simulation results show that the proposed method balances covariates well and produces a smaller mean RMSE compared to other methods under variaous scenarios. In the real data analysis, the WEBM method is applied to investigate causal effect of children's participation in training courses on their IQ. The results show that the duration of attending hands-on practice at 6-9 years old has a siginificantly positive impact on children's IQ.\n\tSince the causal effect function \ud835\udd3c(Y(t)) is more general, we mainly consider it as the estimand for matrix treatment in this paper. Actually, one can also consider the average treatment effect  (\ud835\udd3c(Y(t+ t)-Y(t))) or average partial effect (\ud835\udd3cY(t+ t)-\ud835\udd3cY(t)/ t)  , which can be easily estimated based on the estimates of causal effect function  (<cit.>). Indeed, the causal effect function provides a complete description of the causal effect, rather than a summary measure. Moreover, parametric and nonparametric methods are developed to estimate the causal effect function. Parametric method is recommended when reasonable assumptions can be made about the true model since it is easier to implement and requires less sample size. Despite nonparametric method has higher requirements of the sample size, one can choose to use it according to the real situations due to its higher flexibility. Besides, this paper mainly focuses on the small-scale matrix treatment. Large-scale matrix treatment with low-rank structure can also be considered. In such case, one may control the overall imbalance by only balancing their non-zero elements based on some decomposition technology, and this will be investigated in future work.\n\t\n\t\n\t\n\t*\n\tapalike\n\t\n\t\n\t\n\n\u00a7 APPENDIX\n\n\t\n\n \u00a7.\u00a7 A.1.Proof of Theorem 1\n \n\tThe primal problem is\n\t\n    min_\ud835\udc30\u2211_i=1^nw_ilog(w_i)\n\n\ts.t.\n\t\n    \u2211_j=1^K{\u03bb_j^2 [\u2211_i=1^n w_i (m_K,j(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K,j)]^2 }\u2264\u03b4.\n\n\t\n\tLet ||\u03b8||_2 = \u221a(\u03b8_1^2+\u2026+\u03b8_K^2) be the l_2 norm for an arbitrary K-dimensional vector \u03b8 =(\u03b8_1,\u2026,\u03b8_K)^' and \u039b = diag(\u03bb_1,\u2026,\u03bb_K), then the inequality constraint in the primal problem can be rewritten as ||\u2211_i=1^n w_i \u039b (m_K(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K) ||_2 \u2264\u221a(\u03b4). Let \ud835\udc9c\u2286 R^K be a convex set such that \ud835\udc9c = { a \u2208 R^K: || a||_2 \u2264\u221a(\u03b4)}. Define I_\ud835\udc9c(a) = 0 if a \u2208\ud835\udc9c and I_\ud835\udc9c(a) = \u221e otherwise. Then, the primal problem (15) is equivalent to the following optimaization problem:\n\t\n    min_\ud835\udc30 \u2211_i=1^nw_ilog(w_i)+I_\ud835\udc9c( \u2211_i=1^n w_i \u039b (m_K(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K)).\n \n\tLet h(w) = \u2211_i=1^n w_i log(w_i), the conjugate function of h is \n\t\n    h^*(w)    = sup_t(\u2211_i=1^n w_i t_i-\u2211_i=1^n w_i log(w_i)) \n       = sup_t \u2211_i=1^n (w_i t_i-w_i log(w_i)) \n       = \u2211_i=1^n sup_t_i(w_i t_i-w_i log(w_i)) \n       =  \u2211_i=1^n f^*(w_i),\n\n\twhere f^*(w_i) = sup_t_i(w_i t_i-w_i log(w_i)) is the conjugate function of f(w_i) = w_i log(w_i). Let g(\u03b8) = I_\ud835\udc9c(\u03b8) for any \u03b8\u2208 R^K, then the conjugate function of g is \n\t\n    g^*(\u03b8)    = sup_a (\u2211_k=1^K\u03b8_ka_k -T_\ud835\udc9c(a) ) \n       = sup_|| a ||_2 \u2264\u221a(\u03b4) (\u2211_k=1^K\u03b8_ka_k) \n       = sup_|| a ||_2 \u2264\u221a(\u03b4)  (||\u03b8||_2 || a ||_2) \n       = \u221a(\u03b4)||\u03b8||_2.\n\n\tDefine the mapping H: R^n \u2192 R^K such that Hw = \u2211_i=1^n w_i \u039b (m_K(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K), then H is a bounded linear map. Let H^* be the adjoint operator of H, then for all \u03b8 = (\u03b8_1,\u2026,\u03b8_K)^'\u2208 R^K,\n\t\n    H^*\u03b8 = (\u2211_k=1^K\u03b8_k \u03bb_k (m_K,k(\ud835\udc13_1,\ud835\udc17_1)-m\u0305_K,k), \u2026, \u2211_k=1^K\u03b8_k \u03bb_k (m_K,k(\ud835\udc13_n,\ud835\udc17_n)-m\u0305_K,k) )^'.\n\n\tDefine \u03b8\u0303 = Hw\u0303 = 1/n^r\u2211_i=1^n\u039b (m_K(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K), where w\u0303 = (1/n^r,\u2026,1/n^r)^'\u2208 dom(F). Here, we choose b to be sufficiently large such that ||\u03b8\u0303||_2 \u2264\u221a(\u03b4), then we obtain that g(\u03b8\u0303) = 0 and g is continuous at \u03b8\u0303. Therefore, \u03b8\u0303\u2208 H(dom(F) \u2229 cont(g)), which implies that H(dom(F) \u2229 cont(g)) \u2260\u2205. Here, dom(F) and cont(g) denotes the domain of F and the continuous set of g, respectively. Therefore, the strong duality condition of the Fenchel duality theorem is verified. Moreover,\n\t\n    F(H^*\u03b8)+g^*(-\u03b8) = \u2211_i=1^n f^*(\u2211_k=1^K\u03b8_k \u03bb_k (m_K,k(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K,k))+\u221a(\u03b4)||\u03b8||_2.\n\n\tAccording to the Fenchel duality theorem (Mohri et al. (2018), Theorem B.39), we have \n\t\n    min_w  \u2211_i=1^nw_ilog(w_i)+I_\ud835\udc9c( \u2211_i=1^n w_i \u039b (m_K(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K)) \n       = min_w  \u2211_i=1^n f^*(\u2211_k=1^K\u03b8_k \u03bb_k (m_K,k(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K,k))+\u221a(\u03b4)||\u03b8||_2.\n \n\tFurthermore, since the strong duality condition holds, we can conclude that H^*\u03b8\u0302 is a subgradient of F at \u0175. That is, \n\t\n    \u2211_k=1^K\u03b8\u0302_k \u03bb_k (m_K,k(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K,k) = log(\u0175_i)+1.\n\n\tTherefore, \u0175_i = exp(\u2211_k=1^K\u03b8\u0302_k \u03bb_k (m_K,k(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K,k)-1). The proof of theorem 1 is completed.\n\t\n\t\n\t\n\n \u00a7.\u00a7 A.2.Proof of Proposition 1\n\n\tUsing the law of total expectation and Assumption 1, we can deduce that\n\t\n    \ud835\udd3c[w(Y- s(\ud835\udc13;\u03b2) )^2]\n       =E[f(T)/f(T|X)(Y-s(\ud835\udc13;\u03b2)  )^2]\n       = \ud835\udd3c(\ud835\udd3c[f(T)/f(T|X)(Y- s(\ud835\udc13;\u03b2)  )^2] |T=t,X=x )\n       = \ud835\udd3c(f(t)/f(t|x)\ud835\udd3c([(Y- s(\ud835\udc13;\u03b2)  )^2] |T=t,X=x) )\n       = \u222b_\ud835\udcaf\u00d7\ud835\udcb3f(t)/f(t|x)\ud835\udd3c[(Y(T)- s(\ud835\udc13;\u03b2)  )^2 |T = t, X= x]f(t|x)dtdx\n       =\u222b_\ud835\udcaf\u00d7\ud835\udcb3\ud835\udd3c[(Y(T)- s(\ud835\udc13;\u03b2)  )^2 |T = t, X= x]f(t)f(x)dtdx\n       = \u222b_\ud835\udcaf\u00d7\ud835\udcb3\ud835\udd3c[(Y(t)- s(\ud835\udc13;\u03b2) )^2 |X= x]f(t)f(x)dtdx   (using Assumption 1)\n       = \u222b_\ud835\udcaf\ud835\udd3c[(Y(t)- s(\ud835\udc13;\u03b2)  )^2] f(t)dt  .\n\n\tHence, we complete the proof of Proposition 1.\n\t\n\t\n\n \u00a7.\u00a7 A.3.Proof of Theorem 2\n\n\tThe first order optimality condition for the dual probblem (10) is \n\t\n    \u2211_i=1^nexp{\u2211_j=1^K\u03b8\u0302_j\u03bb_jM_K,j(\ud835\udc13_i,\ud835\udc17_i) }\u00b7\u03bb_jM_K,j(\ud835\udc13_i,\ud835\udc17_i) +\u221a(\u03b4)\u03b8\u0302_j/||\u03b8\u0302||_2 =0,   j=1,\u2026,K,\n\n\twhere M_K,j(\ud835\udc13_i,\ud835\udc17_i) = m_K,j(\ud835\udc13_i,\ud835\udc17_i)- m\u0305_K,j, M_K(\ud835\udc13_i,\ud835\udc17_i)= (M_K,1(\ud835\udc13_i,\ud835\udc17_i), \u2026, M_K,K(\ud835\udc13_i,\ud835\udc17_i))^'.\n\tLet \u039b = diag(\u03bb_1,\u2026,\u03bb_K) and \n\t\n    1/n\u2211_i=1^n\u03a6(\ud835\udc13_i,\ud835\udc17_i;\u03b8) = 1/n\u2211_i=1^nexp{\u2211_j=1^K\u03b8_j\u03bb_j[m_K,j(\ud835\udc13_i,\ud835\udc17_i)-\ud835\udd3c(m_K,j)] }\u039b [m_K(\ud835\udc13_i,\ud835\udc17_i) -\ud835\udd3c(m_K)],\n\n\twhich is a set of K estimating functions. Note that\n\t\n    |\ud835\udd3c( \u03a6(\ud835\udc13_i,\ud835\udc17_i;\u03b8^*)) |\n       = |\ud835\udd3c{exp{\u2211_j=1^K\u03b8_j^*\u03bb_j[m_K,j(\ud835\udc13_i,\ud835\udc17_i)-Em_K,j] }\u039b [m_K(\ud835\udc13_i,\ud835\udc17_i) -Em_K] }|\n       \u2264sup_(\ud835\udc13_i,\ud835\udc17_i) exp{\u2211_j=1^K\u03b8_j^*\u03bb_j[m_K,j(\ud835\udc13_i,\ud835\udc17_i)-Em_K,j] }\u00b7|\ud835\udd3c\u039b [m_K(\ud835\udc13_i,\ud835\udc17_i) -Em_K] |\n       \u2264 O(1) \u00b70 = 0,\n\n\thence we have \ud835\udd3c( \u03a6(\ud835\udc13_i,\ud835\udc17_i;\u03b8^*))=0, which implies that \u03b8^* is the unique solution of \ud835\udd3c( \u03a6(\ud835\udc13_i,\ud835\udc17_i;\u03b8))=0. Therefore, by the estimating equation theory (Van der Vaart (2000)), the solution of the estimating equations \n\t\n    1/n\u2211_i=1^n\u03a6(\ud835\udc13_i,\ud835\udc17_i;\u03b8) = 0,\n \n\tdenoted by \u03b8\u0303, is asymptotically consistent for \u03b8^*. Furthermore, by the Taylor expansion, we have\n\t\n    \u221a(n)(\u03b8\u0303 - \u03b8^*) \u2192_d N(0, \u03a3),\n\n\twhere \u03a3 = { E(\u2202\u03a6/\u2202\u03b8^') }^-1 E(\u03a6\u03a6^'){ E(\u2202\u03a6/\u2202\u03b8) }^-1.\n\tMoreover, by the assumption that \u03b4 = o(n), we have 1/n\u221a(\u03b4)\u03b8_j/||\u03b8||_2 = o_p(n^-1/2) for any \u03b8\u2208 int(\u0398). Therefore, by the Slutsky's theorem, we obtain that \n\t\n    \u221a(n)(\u03b8\u0302 - \u03b8^*) \u2192_d N(0, \u03a3).\n\n\tLet M\u0303_K,j(\ud835\udc13_i,\ud835\udc17_i) = \u03bb_j(m_K,j(\ud835\udc13_i,\ud835\udc17_i)- m\u0305_K,j),\n\tthen \n\t\n    \u0175_i    = exp{\u2211_j=1^K\u03b8\u0302_j \u03bb_j (m_K,j(\ud835\udc13_i,\ud835\udc17_i)-m\u0305_K,j) -1}\n       = exp{M\u0303_K(\ud835\udc13_i,\ud835\udc17_i)^'\u03b8\u0302 -1}\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\tBy Mean Value Theorem, we can deduce that\n\t\n    \u222b|\u0175-w^*|^2dF(\ud835\udc2d,\ud835\udc31) \n       \u2264sup_(\ud835\udc2d,\ud835\udc31) |exp{M\u0303_K(\ud835\udc2d,\ud835\udc31)^'\u03b8_1 -1}|^2 \u00d7\u222b|M\u0303_K(\ud835\udc2d,\ud835\udc31)^'(\u03b8\u0302-\u03b8^*) |^2 dF(\ud835\udc2d,\ud835\udc31) \n       \u2264 O_p(1) \u00b7\u222b|M\u0303_K(\ud835\udc2d,\ud835\udc31)^'(\u03b8\u0302-\u03b8^*) |^2 dF(\ud835\udc2d,\ud835\udc31),\n\n\twhere \u03b8_1 lies between \u03b8\u0302 and \u03b8^*.\n\tSince \n\t\n    \u222b|M\u0303_K(\ud835\udc2d,\ud835\udc31)^'(\u03b8\u0302-\u03b8^*) |^2 dF(\ud835\udc2d,\ud835\udc31) \n        \t= \u222bM\u0303_K(\ud835\udc2d,\ud835\udc31)^'(\u03b8\u0302-\u03b8^*)(\u03b8\u0302-\u03b8^*)^'M\u0303_K(\ud835\udc2d,\ud835\udc31) dF(\ud835\udc2d,\ud835\udc31)\n       = tr{ (\u03b8\u0302-\u03b8^*)(\u03b8\u0302-\u03b8^*)^'\u222bM\u0303_K(\ud835\udc2d,\ud835\udc31)M\u0303_K(\ud835\udc2d,\ud835\udc31)^'dF(\ud835\udc2d,\ud835\udc31) }\n       \u2264 C tr{ (\u03b8\u0302-\u03b8^*)(\u03b8\u0302-\u03b8^*)^'}\n       = C||\u03b8\u0302-\u03b8^*||^2 \n       = O_p(n^-1).\n\n\tThen we have \t\n\t\n    \u222b|\u0175-w^*|^2dF(\ud835\udc2d,\ud835\udc31) = O_p(n^-1).\n\n\tFurthermore, one can show that \n\t\n    1/n\u2211_i=1^n|M\u0303_K(\ud835\udc2d,\ud835\udc31)^'(\u03b4\u0302-\u03b4^*) |^2 - \u222b|M\u0303_K(\ud835\udc2d,\ud835\udc31)^'(\u03b4\u0302-\u03b4^*) |^2 dF(\ud835\udc2d,\ud835\udc31)=o_p(1).\n\n\tHence,\n\t\n    1/n\u2211_i=1^n|\u0175_i- w_i^*|^2 \n       \u2264sup_(\ud835\udc2d,\ud835\udc31) |exp{M\u0303_K(\ud835\udc2d,\ud835\udc31)^'\u03b8_1 -1}|^2 \u00b71/n\u2211_i=1^n|M\u0303_K(\ud835\udc2d,\ud835\udc31)^'(\u03b8\u0302-\u03b8^*) |^2 \n       \u2264 O_p(1) \u222b|M\u0303_K(\ud835\udc2d,\ud835\udc31)^'(\u03b8\u0302-\u03b8^*) |^2 dF(\ud835\udc2d,\ud835\udc31) +o_p(1) \n       = O_p(n^-1)\n\n\tTherefore, the proof of Theorem 2 is completed.\n\t\n\t\n\n  \u00a7.\u00a7.\u00a7 A.4.Proof of Theorem 3\n\n\t\n\t\n\tWe first show that the conclusion of Theorem 3(1). \n\n\t\n\tSince \u03b2\u0302 (as a estimator of \u03b2^*) is a unique minimizer of 1/n\u2211_i=1^n\u0175_i(Y_i-s(T_i;\u03b2))^2(regarding \ud835\udd3c[w(Y-s(T;\u03b2))^2], according to the theory of M-estimation (van der Vaart, 2000, Theorem 5.7), if\n\t\n    sup_\u03b2\u2208\u0398_1|1/n\u2211_i=1^n\u0175_\u0302\u00ee(Y_i-s(T_i;\u03b2))^2-\ud835\udd3c[w(Y-s(T;\u03b2))^2]) |\u2192_p 0,\n\n\tthen \u03b2\u0302\u2192_p \u03b2^*.\n\tNote that\n\t\n    sup_\u03b2\u2208\u0398_1|1/n\u2211_i=1^n\u0175_\u0302\u00ee(Y_i-s(T_i;\u03b2))^2-\ud835\udd3c[w(Y-s(T;\u03b2))^2]) |\n    \u2264sup_\u03b2\u2208\u0398_1|1/n\u2211_i=1^n(\u0175_\u0302\u00ee-w_i)(Y_i-s(T_i;\u03b2))^2 |\n    \n    \t\t+sup_\u03b2\u2208\u0398_1|1/n\u2211_i=1^nw_i(Y_i-s(T_i;\u03b2))^2-\ud835\udd3c[w(Y-s(T;\u03b2))^2]) |.\n\n\tWe first show that sup_\u03b2\u2208\u0398_1|1/n\u2211_i=1^n(\u0175_\u0302\u00ee-w_i)(Y_i-s(T_i;\u03b2))^2 | is o_p(1). Using the Causchy-Schwarz inequality and the fact that \u0175\u2192^L^2 w, we have\n\t\n    sup_\u03b2\u2208\u0398_1|1/n\u2211_i=1^n(\u0175_\u0302\u00ee-w_i)(Y_i-s(T_i;\u03b2))^2 |   \u2264{1/n\u2211_i=1^n(\u0175_\u0302\u00ee-w_i)^2 }^1/2sup_\u03b2\u2208\u0398_1{1/n\u2211_i=1^n(Y_i-s(T_i;\u03b2))^2 }^1/2\n       \u2264 o_p(1){sup_\u03b2\u2208\u0398_1\ud835\udd3c[w(Y-s(T;\u03b2))^2]+o_p(1) }^1/2\n       =o_p(1).\n\n\tThereafter, under Assumption 5, we can conclude that sup_\u03b2\u2208\u0398_1|1/n\u2211_i=1^nw_i(Y_i-s(T_i;\u03b2))^2-\ud835\udd3c[w(Y-s(T;\u03b2))^2]) | is also o_p(1) (Newey and McFadden (1994), Lemma 2.4). Hence, we complete the proof for Theorem 3(1). Next, we give the proof of Theorem 3(2). Define\n\t\n    \u03b2\u0302^* = argmin_\u03b2\u2211_i=1^n w_i(Y_i-s(\ud835\udc13_i;\u03b2))^2.\n\n\tAssume that 1/n\u2211_i=1^n w_i(Y_i - s(\ud835\udc13_i;\u03b2\u0302^*))h(\ud835\udc13_i;\u03b2\u0302^*)) = o_p(n^-1/2) holds with probablility to one as n \u2192\u221e\n\t\n\tBy Assumption 5 and the uniform law of large number, one can get that\n\t\n    1/n\u2211_i=1^n w_i(Y_i-s(T_i;\u03b2))^2 \u2192\ud835\udd3c{ w(Y-s(T;\u03b2))^2 } in probability uniformly over  \u03b2,\n\n\twhich implies ||\u03b2\u0302^* -\u03b2^* ||\u2192_p 0. Let\n\t\n    r(\u03b2) = 2\ud835\udd3c{ w(Y-s(T;\u03b2))h(T;\u03b2) },\n\n\twhich is a differentiable function in \u03b2 and r(\u03b2^*) = 0. By mean value theorem, we have\n\t\n    \u221a(n)r(\u03b2\u0302^*)- \u25bd_\u03b2 r(\u03b6) \u00b7\u221a(()n)(\u03b2\u0302^* - \u03b2^*) =\u221a(n)r(\u03b2^*) =0\n\n\twhere \u03b6 lies on the line joining \u03b2\u0302^* and \u03b2^*. Since \u25bd_\u03b2 r(\u03b2) is continuous at \u03b2^* and ||\u03b2\u0302^* -\u03b2^* ||\u2192_p 0, then\n\t\n    \u221a(n)(\u03b2\u0302^* - \u03b2^*)  = \u25bd_\u03b2 r(\u03b2^*)^-1\u00b7\u221a(n) r(\u03b2\u0302^*) +o_p(1)\n\n\tDefine the empirical process\n\t\n    G_n(\u03b2)= 2/\u221a(n)\u2211_i=1^n{ w_i(Y_i-s(T_i;\u03b2))h(T_i;\u03b2) - \ud835\udd3c{ w(Y-s(T;\u03b2))h(T;\u03b2)  }}.\n\n\tThen we have\n\t\n    \u221a(n)(\u03b2\u0302^* - \u03b2^*)\n       =  \u25bd_\u03b2 r(\u03b2^*)^-1\u00b7{\u221a(n) r(\u03b2\u0302^*) -  2/\u221a(n)\u2211_i=1^n{ w_i(Y_i-s(T_i;\u03b2\u0302^*))h(T_i;\u03b2\u0302^*)  + 2/\u221a(n)\u2211_i=1^n{ w_i(Y_i-s(T_i;\u03b2\u0302^*))h(T_i;\u03b2\u0302^*)  }\n       =  -\u25bd_\u03b2 r(\u03b2^*)^-1\u00b7 G_n(\u03b2\u0302^*)+o_p(1)\n       = U^-1\u00b7{ G_n(\u03b2\u0302^*)-G_n(\u03b2^*) +G_n(\u03b2^*) } +o_p(1).\n\n\tBy Assumption 5, 6, Theorem 4 and 5 of Andrews(1994), we have G_n(\u03b2\u0302^*)-G_n(\u03b2^*) \u2192_p 0. Thus,\n\t\n    \u221a(n)(\u03b2\u0302^* - \u03b2^*) = U^-12/\u221a(n)\u2211_i=1^n{ w_i(Y_i-s(T_i;\u03b2^*))h(T_i;\u03b2^*) } +o_p(1),\n\n\tthen we can get that the asymptotic variance of \u221a(n)(\u03b2\u0302^* - \u03b2^*) is V.\n\tTherefore, \u221a(n)(\u03b2\u0302^* - \u03b2^*) \u2192_d N(0,V). Next, we will prove \u03b2\u0302\u2192_p \u03b2\u0302^*.\n\tSince\n\t\n\t\n    sup_\u03b2\u2208\u0398_1|1/n\u2211_i=1^n\u0175_\u0302\u00ee(Y_i-s(T_i;\u03b2))^2-1/n\u2211_i=1^nw_i(Y_i-s(T_i;\u03b2))^2) |\n    \u2264sup_\u03b2\u2208\u0398_1|1/n\u2211_i=1^n(\u0175_\u0302\u00ee-w_i)(Y_i-s(T_i;\u03b2))^2 |\n    \u2264{1/n\u2211_i=1^n(\u0175_\u0302\u00ee-w_i)^2 }^1/2sup_\u03b2\u2208\u0398_1{1/n\u2211_i=1^n(Y_i-s(T_i;\u03b2))^2 }^1/2\n    \u2264 o_p(1){sup_\u03b2\u2208\u0398_1\ud835\udd3c[w(Y-s(T;\u03b2))^2]+o_p(1) }^1/2\n    \n    \t\t=o_p(1),\n\n\twhich implies \u03b2\u0302^* \u2192_p \u03b2\u0302. Then by Slutskey's Theorem, we can draw the conclusion that \u221a(n)(\u03b2\u0302 - \u03b2^*) \u2192_d N(0,V). Therefore, we have completed the proof of Theorem 3. \n\t\n\t\n\n  \u00a7.\u00a7.\u00a7 A.5.Proof of Theorem 4\n\n\tFor convenience, we use a mapping \u03a9: R^p\u00d7 q \u00d7 D\u00d7 R \u2192 R^p\u00d7 q \u00d7 D to represent the operator of absorbing the constant into the coefficients of B-spline basis for the first predictor. More precisely, \u03a9 is defined by \n\t\n    \ud835\udc06^b= \u03a9(\ud835\udc06,c),\n\n\twhere \ud835\udc06_i_1,i_2,d= \ud835\udc06_i_1,i_2,d for (i_1,i_2) \u2260 (1,1) and \ud835\udc06_1,1,d=\ud835\udc06_1,1,d+pqc, d=1,\u2026,D. It then follows from the property of B-spline functions that\n\t\n    c+1/pq<\ud835\udc06,\u03a6(\ud835\udc13)> = 1/pq <\ud835\udc06^b,\u03a6(\ud835\udc13)>.\n\n\tWe also write \ud835\udc06_0= \u2211_r=1^R_0\ud835\udc01_0r\u2218\u03b1_0r, r= 1,\u2026, R_0. Suppose \ud835\udc06\u0302,\u0109) is a solution to (19) and \n\t\n    \ud835\udc06\u0302 = \u2211_r=1^R\u03b2\u0302_1^(r)\u2218\u03b2\u0302_2^(r)\u2218\u03b1\u0302_r,\n\n\tthen by <cit.>, Lemma B.1, there exists \u010d\u2208 R and \n\t\n    \ud835\udc06\u030c = \u2211_r=1^R\u03b2\u0302_1^(r)\u2218\u03b2\u0302_2^(r)\u2218\u03b1\u030c_r,\n\n\tsuch that\n\t\n    \u010d+1/pq<\ud835\udc06\u030c ,\u03a6(\ud835\udc13)> = \u0109+1/pq<\ud835\udc06\u0302 ,\u03a6\u0303(\ud835\udc13)>,\n\n\twhere \u03b1\u030c_r= (\u03b1\u030c_r,1,\u2026,\u03b1\u030c_r,D )^' satisfying\n\t\n    \u2211_d=1^D\u03b1\u030c_r,du_d=0\n\n\twith u_d= \u222b_0^1 b_d(x)dx.\n\tUsing (27), \n\twe have\n\t\n    \u2211_i=1^n (\u0175_iy_i-\u010d-1/pq<\ud835\udc06\u030c,\u03a6(\ud835\udc13_i)>)^2 \u2264\u2211_i=1^n (\u0175_iy_i-c_0-1/pq<\ud835\udc06_0,\u03a6(\ud835\udc13_i)>)^2.\n \n\tLet \ud835\udc06\u030c^b= \u03a9(\u03b1\u030c,\u010d) and \ud835\udc06_0^b = \u03a9(\ud835\udc06_0,c_0), then \n\t\n    \u2211_i=1^n (\u0175_iy_i-1/pq<\ud835\udc06\u030c^b,\u03a6(\ud835\udc13_i)>)^2 \u2264\u2211_i=1^n (\u0175_iy_i-1/pq<\ud835\udc06_0^b,\u03a6(\ud835\udc13_i)>)^2.\n\n\tTherefore, we have\n\t\n    \u2211_i=1^n ((\u0175_i-w_i+w_i)y_i-1/pq<\ud835\udc06\u030c^b,\u03a6(\ud835\udc13_i)>)^2 \u2264\u2211_i=1^n ((\u0175_i-w_i+w_i)y_i-1/pq<\ud835\udc06_0^b,\u03a6(\ud835\udc13_i)>)^2,\n\n\twhich leads to \n\t\n    \u2211_i=1^n (w_iy_i-1/pq<\ud835\udc06\u030c^b,\u03a6(\ud835\udc13_i)>)^2    \u2264\u2211_i=1^n (w_iy_i-1/pq<\ud835\udc06_0^b,\u03a6(\ud835\udc13_i)>)^2 \n       +2 \u2211_i=1^n (\u0175_i-w_i)y_i(1/pq<\ud835\udc06\u030c^b-\ud835\udc06_0,\u03a6(\ud835\udc13_i)).\n\n\tLet \ud835\udc06^# =\ud835\udc06\u030c^b-\ud835\udc06_0^b,  \ud835\udc1a^# =vec(\ud835\udc06^# ),  \ud835\udc1a_0^b =vec(\ud835\udc06_0^b), \ud835\udc1a\u030c^b= vec(\ud835\udc06\u030c^b) and \ud835\udc19\n\t= (\ud835\udc33_1,\u2026,\ud835\udc33_n)^'\u2208 R^n\u00d7 pqD, where \ud835\udc33_i = vec(\u03a6(\ud835\udc13_i)),i=1,\u2026,n. \n\tLet y_\u0175= (\u0175_1y_1,\u2026, \u0175_ny_n)^' and y_w = (w_1y_1,\u2026, w_ny_n)^', then using (31) and working out the squares, we obtain\n\t\n    1/p^2q^2||\ud835\udc19\ud835\udc1a^#||^2   \u2264  2<1/pq\ud835\udc19\ud835\udc1a\u030c^b,y_w>-2<\t1/pq\ud835\udc19\ud835\udc1a_0^b,y_w>\n       -21/p^2q^2<\ud835\udc19\ud835\udc1a^#,\ud835\udc19\ud835\udc1a_0^b>+2<1/pq\ud835\udc19\ud835\udc1a^#, y_\u0175-y_w>  \n       = 2<1/pq\ud835\udc19\ud835\udc1a^#,y_\u0175-y_w>+2<1/pq\ud835\udc19\ud835\udc1a^#,\u03f5>+2<\t1/pq\ud835\udc19\ud835\udc1a^#,y_w-\u03f5-1/pq\ud835\udc19\ud835\udc1a_0^b>\n\n\tFirst, we show the upper bound of <1/pq\ud835\udc19\ud835\udc1a^#,y_\u0175-y_w>. Using the Cauchy-Schwarz inequality, we have\n\t\n    <1/pq\ud835\udc19\ud835\udc1a^#,y_\u0175-y_w> \n       \u2264||\u0175-w ||_2 \u00b7||1/pq\ud835\udc19\ud835\udc1a^#||_2 \n       \u2264C_1\u221a(nh_n)/pq\u00b7||\u0175-w ||_2 \u00b7||\ud835\udc1a^#||_2\n\n\t\n\tBy the conclusion of Theorem 2(2), we have \n\t\n    ||\u0175-w ||_2 = O_p(1).\n\n\tApplying (37) to (36), we can obtain that\n\t\n    <1/pq\ud835\udc19\ud835\udc1a^#,y_\u0175-y_w> \u2264C_2\u221a(nh_n)/pq||\ud835\udc1a^#||_2.\n\n\tSecond, by the conclusion of <cit.>, (A.18) and (A.20), we can obtain the upper bound of\n\t<1/pq\ud835\udc19\ud835\udc1a^#,\u03f5> and <\t1/pq\ud835\udc19\ud835\udc1a^#,y_w-\u03f5-1/pq\ud835\udc19\ud835\udc1a_0^b>, which are\n\t\n    <1/pq\ud835\udc19\ud835\udc1a^#,\u03f5> \u2264C_3/pq||\ud835\udc1a^#||_2 { nh_n(R^3+R(p+q)+RD)}^1/2.\n\n\tand\n\t\n    <1/pq\ud835\udc19\ud835\udc1a^#,y_w-\u03f5-1/pq\ud835\udc19\ud835\udc1a_0^b> \u2264C_4/pq||\ud835\udc1a^#||_2 {\u2211_r=1^R_0||vec(\ud835\udc01_0r)||_1/pq}n\u221a(h_n)/D^\u03c4\n\n\tTherefore, applying (38), (39) and (40) to (35), we have\n\t\n    C_5/pq||\ud835\udc1a^#||_2^2 \u2264 R_1 ||\ud835\udc1a^#||_2,\n\n\twhere  R_1 = C_6\u221a(D/n)+C_7  {D(R^3+R(p+q)+RD)/n}^1/2+C_8{\u2211_r=1^R_0||vec(\ud835\udc01_0r)||_1/pq}1/D^\u03c4-1/2.\n\t\n\tBy solving the second order inequality (41), we have\n\t\n    C_5/pq||\ud835\udc1a^#||_2 \u2264 R_1\n\n\tFurther, by Assumption 6 and <cit.>, (A.38) of Lemma A.2, we have\n\t\n    ||\u015d(\ud835\udc13) -s(\ud835\udc13) ||^2   \u2264 C_9 h_n 1/p^2q^2||\ud835\udc1a^#||^2 \n       = C_10R_1^2/D\n       = O_P(1/n)+O_p(R^3+R(p+q)+RD/n)+O_p({\u2211_r=1^R_0||vec(\ud835\udc01_0r)||_1/pq}^2 1/D^2\u03c4) \n       =O_p(R^3+R(p+q)+RD/n)+O_p({\u2211_r=1^R_0||vec(\ud835\udc01_0r)||_1/pq}^2 1/D^2\u03c4).\n\n\tHence, the proof of Theorem 4 is completed. \n\t\n\t\n\n\u00a7 APPENDIX REFERENCE\n\n\tMohri, M., Rostamizadeh, A., and Talwalkar, A. (2018). Foundations of ma- chine learning. MIT press.\n\t0.2cm\n\tNewey, W. K. and McFadden, D. (1994). Large sample estimation and hypoth- esis testing. Handbook of econometrics, 4:2111\u20132245.\n\t0.2cm\n\tVan der Vaart, A. W. (2000). Asymptotic statistics, volume 3. Cambridge university press.\n\t0.2cm\n\tZhou, Y., Wong, R., and He, K. (2020). Broadcasted nonparametric tensor regression.\n"}