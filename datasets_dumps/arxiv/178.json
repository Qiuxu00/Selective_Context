{"entry_id": "http://arxiv.org/abs/2303.07105v1", "published": "20230313134255", "title": "Measuring Multi-Source Redundancy in Factor Graphs", "authors": ["Jesse Milzman", "Andre Harrison", "Carlos Nieto-Granda", "John Rogers"], "primary_category": "cs.IT", "categories": ["cs.IT", "cs.RO", "math.IT", "94A17 (Primary), 68T40 (Secondary)"], "text": "\n\n\n\n\n\n\n\n\n\n\n\nB-.05emi-.025em b-.08em\n    T-.1667em.7exE-.125emX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthmTheorem\ncorollaryCorollary\nlemmaLemma\nformulaFormula\npropositionProposition\ndefinitionDefinition\ncomputationComputation\n*remarkRemark\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshapes.geometric, arrows,decorations.pathreplacing\npose = [circle, text centered, draw=black, fill=orange!30, minimum size = 1.25cm]\nspur = [circle, text centered, draw=black, fill=violet!30, minimum size = 1.25cm]\nfactor = [rectangle, text centered, draw=black, fill=blue!30]\nrendezFactor = [rectangle, text centered, draw=black, fill=violet!30]\nrendezFactorEliminated = [rectangle, text centered, draw=black!40, text = black!40, fill=violet!10]\nfactorNew = [rectangle, text centered, draw=black, fill=green!30]\nfactorNewEliminated = [rectangle, text centered, draw=black!40, text=black!40, fill=green!10]\n\n\n\nposeEliminated = [circle, text centered, draw=black!40, text=black!40, fill=orange!10]\nspurEliminated = [circle, text centered, draw=black!40, text=black!40, fill=violet!10, minimum size = 1.25cm]\nfactorEliminated = [rectangle, text centered, draw=black!40, text=black!40, fill=blue!10]\n\nintra = = [thick,->,>=stealth]\ninter = = [dashed,-, very thick,draw=blue]\nfactorLine = = [thick,-]\nfactorLineEliminated = = [thick,-,draw=black!40]\nfactorLineNew == [thick, green, -]\nBNarrow = = [dashed, blue, ->]\ninterrobotComms = [dashed,->,draw=violet,thick]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nwarning\n[colback=yellow!10!white,colframe=orange!70!yellow,title =Warning]\n    \n     \n    \n\n\n\n\n\n\n\nassumption[section]\n\nassumption[2][]\nassumption\n#1\n\nframetitle=\n[baseline=(current bounding box.east),outer sep=0pt]\n[anchor=east,rectangle,fill=blue!20]\nAssumption\u00a0;\n\n\nframetitle=\n[baseline=(current bounding box.east),outer sep=0pt]\n[anchor=east,rectangle,fill=blue!20]\nAssumption\u00a0:\u00a0#1;\n\ninnertopmargin=10pt,linecolor=blue!20,\nlinewidth=2pt,topline=true,\nframetitleaboveskip=-\n[]\n\n\n\nmetric[section]\n\nmetric[2][]\nmetric\n#1\n\nframetitle=\n[baseline=(current bounding box.east),outer sep=0pt]\n[anchor=east,rectangle,fill=violet!20]\nMetric\u00a0;\n\n\nframetitle=\n[baseline=(current bounding box.east),outer sep=0pt]\n[anchor=east,rectangle,fill=blue!20]\nMetric\u00a0:\u00a0#1;\n\ninnertopmargin=10pt,linecolor=blue!20,\nlinewidth=2pt,topline=true,\nframetitleaboveskip=-\n[]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring Multi-Source Redundancy in Factor Graphs \n\n\n    Jesse Milzman, Andre Harrison, Carlos Nieto-Granda, John Rogers\n\nDEVCOM Army Research Laboratory\n\nAdelpi, MD USA \n\n{jesse.m.milzman.civ, andre.v.harrison2.civ, carlos.p.nieto2.civ, john.g.rogers59.civ}@army.mil\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    March 30, 2023\n===========================================================================================================================================================================================================================================================================\n\n\n\n\n\nFactor graphs are a ubiquitous tool for multi-source inference in robotics and multi-sensor networks.\nThey allow for heterogeneous measurements from many sources to be concurrently represented as factors in the state posterior distribution, so that inference can be conducted via sparse graphical methods.\nAdding measurements from many sources can supply robustness to state estimation, as seen in distributed pose graph optimization.\nHowever, adding excessive measurements to a factor graph can also quickly degrade their performance as more cycles are added to the graph.\nIn both situations, the relevant quality is the redundancy of information.\nDrawing on recent work in information theory on partial information decomposition (PID),\nwe articulate two potential definitions of redundancy in factor graphs, both within a common axiomatic framework for redundancy in factor graphs.\nThis is the first application of PID to factor graphs, and only one of a few presenting quantitative measures of redundancy for them.\n\n\n\nfactor graphs, redundancy, SLAM, partial information decomposition, information theory\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n \n\nAutonomous agents operating within an unknown environment must often simultaneously solve several related yet distinct optimization problems.\nConcurrently, they model their surrounding environment, estimate their own pose trajectory, the poses of other agents with respect to themselves and the environment, and classify, localize, and track objects within that environment.\nThese optimization tasks are often modeled in robotics research as a factor graph consisting of variables which represent the unknown quantities and factors that define functions that act on a subset of the variables. \nTypically factors operate only on a limited set of variables, corresponding to measurements on those variables.   \n\nMoreover, when several of these agents operate together information is shared amongst them to generate faster and more accurate estimates of the given target variable(s) that they need to estimate. \nIn some of the most austere environments these autonomous systems will form a distributed mobile ad-hoc network (MANET) as there may be no supporting communication infrastructure or equipment.\nA recurrent research question in these scenarios is how to maintain or even improve estimation accuracy (and speed) using the least amount of information, in order to both minimize communication between agents as well as reduce the number of variables incorporated into the optimization problem.\nThis can be done by estimating the individual or sets of information sources that provide the most unique or synergistic information for estimating the target variable while simultaneously minimizing information from redundant sources of information <cit.>.  \nWhile minimizing the communication of redundant information can also reduce computational complexity, it can make the network brittle as the corruption or invalidation of any information source will reduce the estimation accuracy of the target variable(s).\nIncorporating a certain amount of redundancy for an information source or set of sources may lead to a more robust distributed estimation network.\n\n\nRedundancy also plays an important role in the implementation of resilient estimation.\nAlternative communication and information-gathering pathways can facilitate adaptation to unexpected failure modes <cit.>.\n\nCummingham et al.  proposed one the first distributed implementations of Smoothing and Mapping  (DDF-SAM) resulting in systems with increased resilience to node failures and network topology changes <cit.>.\nKhedekar et al. <cit.> proposed a multi-modal approach to enable resilient SLAM through the use of redundant sets of heterogeneous sensors.\n\n\nTo summarize, in many autonomy scenarios, adding heterogeneous sensors or additional agents can be used to increase the overall robustness and resilience of factor graph-based SLAM estimates.\nHowever, precise measures of the amount of redundancy from difference sources of information present within a factor graph hasn't received as much attention.\nWang et al. proposed the use of maximal information content (MIC) to estimate the similarity between different features within a factor graph as part of an unsupervised feature selection approach (FGUFS) <cit.>.\n\n\nThe partitioning of information sources at the level of granularity \u2014 to fully capture the uniqueness, synergy, and redundancy among information sources  \u2014 has not historically been possible with standard information theoretic concepts of mutual information or conditional mutual information \u00a0<cit.>.\nRecent developments in information theory have seen the advancement of the partial information decomposition (PID) of multivariate information to address such questions.\nAt its core, PID untangles these notions by hierarchically decomposing the information content among many variables with a redundancy function.\nHowever, this approach, while it has been applied to problems in feature selection and information fusion \u00a0<cit.> has not been applied to factor graph analysis.  \n\nIn this paper, we present the initial development of a framework for estimating the redundancy within a factor, building upon PID.\nWe present two measures of redundancy: the direction application of PID as an information-theoretic measure, and a statistical distance-based approach with an analogous form.\n\nThe contributions made in this paper are the following:\n\n\n    \n  * Building upon the work in PID, we outline a formal axiomatic framework for defining multi-source redundancy in factor graphs.\n    \n  * We define two potential redundancies within this framework: an information-theoretic redundancy and a statistical distance-based redundancy.\n    \n  * We investigate these definitions analytically for supplemental factors in linear factor graphs, rewriting them as closed-form integrals on the base graph.\n    \n  * We apply these to simulations of a 2D landmark SLAM task, in order to compute the redundancy of the information contributed by two landmarks.\n\n\n\n\n\n\n\n\u00a7 PRELIMINARIES\n\n\nIn this section, we briefly review the key definitions from the PID framework, as well as those for linear factor graphs.\nFor factor graphs in particular, we will largely use our own notation in order to simplify the later presentation of our PID-based redundancy framework in Sec.\u00a0<ref>.\nThis work is primarily focused on linear factor graphs.\n\n\n\n \u00a7.\u00a7 Partial Information Decomposition\n\n\n\nThe partial information decomposition was originally put forward in <cit.> as a one-sided multivariate extension of mutual information, and as a resolution to the signed nature of interaction information.\nIn its most common form, PID decomposes the information I(T; X_1, ..., X_n) into D(n)-2 components, where D(n) is the n-th Dedekind number.\nFor two sources X_1 and X_2, this resolves into the so-called bivariate PID, i.e. the sum I(T;X_1,X_2) = S + R + U_1 + U_2, with each summand (atom) respectively referred to as the synergistic, redundant, and unique information(s).\nCrucially, every PID is uniquely defined by a redundancy function , such as the  function defined in <cit.>.\nIn the bivariate case, the redundancy function determines R.\nMore generally, it determines the redundant information shared between multiple sources, formalized as collections of random variables.\n\nThe PID has primarily been applied to theoretical and experimental neuroscience <cit.>, although there have been a few works applying its tools to other areas <cit.>.\n\nAlthough the framework was received with great interest from interdisciplinary information theorists and complex systems scientists, PID remains ill-posed due to the multiplicity of proposed redundancy functions.\nThe original  redundancy function remains controversial, since it does not distinguish between sources providing the same information regarding a target outcome from the same amount of information <cit.>.\nAt present, we do not find this critique relevant to the application at hand, as we are more concerned with choosing a posterior that sufficiently constrains the target variables for accurate state inference.\n\nThe  redundancy function takes the PID lattice of source antichains as its domain, so we must first define that set.\nLet Z_1, ..., Z_n \u2208\u211d^N be a collection of random \nvariables in our N-dimensional configuration space, referred to as predictor variables, and Z = (Z_i)_i=1^N be the full n \u00d7 N vector.\nIn keeping with many previous PID works including <cit.>, when it will not cause confusion, we will identify sets of indexed variables with vectors ordered along increasing index, i.e. Z = (Z_i)_i = { Z_i }_i.\nLet \ud835\udcab(A) to denote the powerset of any set A, and [z] = { 1, ..., z} for any integer z.\nThen \ud835\udcab(Z) denotes the collection of sources, i.e. subsets of predictor variables, which we denote Z_J for some index vector J \u2208\ud835\udcab([n]).\nThe PID lattice, denoted \ud835\udc9c (Z), is the collection of antichains of the poset (\ud835\udcab(Z), \u2286).\nPut differently, the PID lattice is comprised of all possible collections of sources in \ud835\udcab(Z) such that no two sources in the same collection are comparable by inclusion. Put formally, for any nonempty \u03b1\u2208\ud835\udcab(\ud835\udcab(Z)),\n\n    \u03b1\u2208\ud835\udc9c(Z) \u21d4\u2200Z_J,Z_J'\u2208\u03b1,  Z_J \u2284Z_J'.\n\nThe antichain lattice \ud835\udc9c(Z) comes equipped with the following partial ordering:\n\n    \u03b1\u227c\u03b2\u21d4\u2200Z_J\u2208\u03b2, \u2203Z_J'\u2208\u03b1,   J' \u2286 J\n\nThus, as one ascends the lattice, individual sources within the antichain will encompass more variables, non-monotonically decreasing the number of distinct sources in the antichain by the top. In particularly, if \u03b1_min = {{Z_i }}_i (i.e. the antichain of singletons) and \u03b1_max = {Z} (i.e. the source containing all predictors), we have \u03b1_min\u227c\u03b2\u227c\u03b1_max for all \u03b2\u2208\ud835\udc9c(Z).\nWhen convenient, we may use an abuse of notation whereby the antichain \u03b1 contains the index sets for its sources, e.g. J \u2208\u03b1.\nIn this work, every source variable Z_i has a smooth density p_Z_i supported everywhere on the configuration space. Moreover, every joint density p_Z_J, including p_Z, is smooth and supported everywhere on the product space.\n\nLet the target variable T \u2208\u211d^M be given, and assume that it has a smooth joint density p_Z,T with the predictors, supported everywhere.[We assume full support to simplify our presentation.]\nThe specific information of a predictor source Z_J toward the target variable is the integral function defined pointwise on target realizations T=t.\n\n\nLet the predictors and target variable and (Z, T) have a smooth joint density p_Z,T.\nThen for any t such that p_T(t) > 0, we define the specific information of Z_J about T=t to be\n\n    I_Z_j(t)\n        \n            = D (   p_Z_J|T(Z_J|t)   ||   p_Z_J(Z_J) ).\n\nMoreover, we let I_Z(t) = 0 whenever p_T(t) = 0.\n\nWe emphasize that we use the subscript Z_J to label our specific informations, but they are not functions of the variable Z_J, only T.\nThe  redundancy function is defined on \ud835\udc9c(Z) as follows:\n\n    \n    Let \u03b1\u2208\ud835\udc9c(Z) be the antichain composed of sources \u03b1 = (Z_J_k)_k.\n    Then\n    \n    (\u03b1;T)    = \ud835\udd3cmin_k I_Z_J_k(T)\n\n\n\n    The  redundancy function satisfies several desirable properties:\n    \n        \n  * Nonnegativity (P)\n        \n    (\u03b1;T) \u2265 0   \u2200\u03b1\u2208\ud835\udc9c(Z)\n\n        \n  * Monotonicity (M)\n        \n    (\u03b1;T) \u2264(\u03b2;T)    if \u03b2\u2286\u03b1\n\n        \n        \n  * Self-redundancy (SR)\n        \n    ({Z_J}, T) = I(Z_J, T)\n\n    \n    The latter two of these are typically enumerated among the Williams-Beer (WB) axioms, along with a symmetry property that follows from  being well-defined on \ud835\udc9c(Z).\n\n\nThis structure allows the  function to be used to decompose the full information I(T; Z) into nonnegative atoms in the discrete case, using the M\u00f6bius inversion of , typically denoted \u03a0:\n\n    \u03a0(\u03b1) = (\u03b1) - \u2211_\u03b2\u227a\u03b1(\u03b2)\n\nIn the bivariate case, the redundancy, synergy, and unique information are given by R= \u03a0({Z_1}{Z_2}), S = \u03a0({Z}), and U_i = \u03a0({Z_i}).\nAlthough these other atoms may be of future interest for the analysis of factor graphs, in this work we restrict our attention to redundancies.\n\n\n\n\n\n \u00a7.\u00a7 Linear Factor Graphs\n\n\nFactor graphs\u00a0<cit.> are graphical models that are well suited to modeling complex perception estimation problems in robotics, such as Simultaneous Localization and Mapping (SLAM), or Structure from Motion (SFM)\u00a0<cit.>\u00a0<cit.>. \n\n\nSimilar to other probabilistic graphical models (PGMs) such as Bayesian networks and Markov random fields, factor graphs allow high-dimensional state variables to be decomposed into components with sparse dependencies.\nA common application of factor graphs in robotics is in pose graph optimization (PGO), in which there have been many advancements in recent years.\nFor centralized PGO, there has been a development of certifiably correct estimation methods that are capable of efficiently recovering globally optimal solutions of generally intractable estimation problems\u00a0<cit.>\u00a0<cit.>. \nIn distributed PGO, Tian et al.\u00a0<cit.> presented the first Distributed Certifiably Correct Pose- Graph Optimization (DC2-PGO).\n\nGiven all measurements and priors on the state z, factor graphs formally factor the posterior density p(x|z) into the product of functions specific to each measurement, typically unnormalized densities of the relationship between a state observable h_i(x) and the corresponding measurement z_i.\nFor instance, in SLAM and PGO, measurements of one or two keyframe(s) from the trajectory are pulled from odometry, IMU, LiDAR scans, and/or visual perception.\nTaken together, all these measurements allow for the inference of the full trajectory from its posterior.\nHowever, by representing this distribution in a factor graph, the maximal a posteriori (MAP) trajectory estimate can be computed piecemeal by exploiting the sparsity of the graph\u00a0<cit.>.\nFor the theoretical elements of this paper, we will use the closed-form expression for the MAP of a linear factor graph.\n  \n\nConsider a system comprised of n variables in N-dimen-\nsional configuration space, i.e. X_1, ..., X_n \u2208\u211d^N. \nA factor graph is comprised of these variables and an associated collection of factors \u2131 = {\u03d5_j}_j=1^m, which is a collection of real-valued functions, with each \u03d5_j taking a subset of variables X_i_j as its argument.\nMost often, factors take one or two variables as their argument.\nFormally, a factor graph is the bipartite graph \ud835\udca2 = (\ud835\udcb1,\u2130), where \ud835\udcb1 = \u2131\u222aX and { i,j }\u2208\u2130 iff i \u2208i_j, i.e. iff X_i is an argument for \u03d5_j.\nIn robotic state estimation, univariate factors can encode priors (including starting position and exogenous map information) and GPS measurements, while bivariate factors might encode wheel odometry, visual perception, or LiDAR point-cloud comparisons <cit.>.\nIn this work, we model every factor as a multivariate Gaussian distribution, produced by a perfectly calibrated Gaussian measurement.\nWe use \ud835\udca9_z(m, \u03a3) to represent Gaussian densities at Z=z for mean m and covariance matrix \u03a3.\nFor any vector z, we denote the Mahalanobis norm with its precision matrix, i.e. z ^2_M = z^\u22a4 M z, since we will more often work with precision matrices instead of noise matrices.\nEvery factor \u03d5_j is given by:\n\n    \u03d5_j (x_i_j) = exp -1/2A_j x - z_j  _\u0393_j^2\n\nfor some N \u00d7 n N measurement matrix A_j.\nThis is analogous to the Jacobian matrix of the measurement operator in nonlinear factor graphs <cit.>.\n\n\nWe will often merely refer to the collection of factors \u2131 as the factor graph, and subsets of it as subgraphs.\nSuch subsets are denoted \u2131_J\u2286\u2131 for some vector of factor indices J \u2282 [m].\nUsing powerset notation, the collection of all subgraphs forms a partially ordered set (poset) under inclusion: (\ud835\udcab(\u2131), \u2286).\nNote that \u2131_J'\u2282\u2131_J iff J' \u2282 J, and we will often prefer the latter notation for convenience.\nAny factor subgraph \u2131_J is associated to the product factor \u03a6_\u2131_J = \u220f_j \u2208 J\u03d5_j(x_i_j).\nBy concatenating the measurements and measurement matrices, we may rewrite these\n\n    \u03a6_\u2131_J (x) = exp -1/2A_Jx - z_J_\u0393_J^2\n\nwhere\n\n    A_J = [   A_j_1;       \u22ee; A_j_|J| ],\n             z_J = [   z_j_1;       \u22ee; z_j_|J| ],   \u0393_J =\n        [   \u0393_j_1       0        ;               \u22f1        ;               0 \u0393_j_|J| ]\n\nFor the full graph \u2131, we omit subscripts on A, z, and \u2131.\nIn the event that (A_J)=n, the product factor \u03a6_\u2131_J presents the unnormalized density for a joint distribution on X.\nIn this situation, we say that \u2131_J is full-rank.\nBy solving the quadratic form in the exponential, i.e. refactoring it into the standard Gaussian form, we have\n\n    \u03a6_\u2131_J (  )    \u221d\ud835\udca9_ (\u03bc_J, \u039b_J^-1) \n    \u039b_J   =  A_J^\u22a4\u0393_J A_J\n    \u03bc_J   = \u039b_J^-1 A_J^\u22a4\u0393_J\n\nwhere, as before, we omit subscripts when referencing the full graph \u2131.\nIn fact, for any full-rank \u2131_J, this distribution \u03a6_\u2131_J corresponds to posterior state distribution after accounting for the included measurements, i.e.\n\n    p(x | z_J) \u221d\u03a6_\u2131_J(x)\n\nViewed from this perspective, we see that \u03bc_J is the maximum a posteriori (MAP) state estimate <cit.>.\n\nIn practice, the dimensionality of \u039b will be quite large, and it is often computationally difficult if not intractable to naively solve for \u03bc.\nFactor graphs are employed in contexts in which we expect them to be sparse, i.e. we expect A to be sparse, or (as the Jacobian) sparse at every point for nonlinear factor graphs.\nThis allows one to exploit this sparsity to solve for \u03bc iteratively <cit.>. \n\nWe assume that every measurement z_j is independently sampled, each from the conditional distribution p(z_j | x) \u223c\ud835\udca9(A_J x, \u0393_z_j^-1).\nNote that the density of this distribution is merely a normalization of \u03d5_j.\nThis assumption will allow us to estimate the information content of individual factors, given some prior on X.\nHowever, in practice, all of our information regarding X is encoded in \u2131.\nRather than assuming a prior distribution separate from \u2131, we instead partition our factors into two sets: base factors (forming the base graph) and supplementary factors.\n\n    \u2131 = \u2131_B\u222a\u2131_B\u0305\n\nwhere B\u0305 = [m] \u2216 B.\nTaking the density induced by the base graph as our prior distribution, we denote our prior and posteriors as:\n\n    p_X(x)    := p(x | z_B) \u221d\u03a6_\u2131_B\n    \n    p_X|Z_J(x | z_J)    := p(x | z_B \u222a J) \u221d\u03a6_\u2131_B \u222a J\n\nwhere we omit subscripts p(x),   p(x|z_J) where they are redundant.\nTo simplify notation, we denote J\u0303 = B \u222a J for all J \u2282B\u0305, so that we have p(x|z_J) \u223c\ud835\udca9_x(\u03bc_J\u0303, \u039b_J\u0303) as per (<ref>).\nMoreover, from here on, we will employ \u0394_J in place of \u039b_J for the matrix expression in (<ref>) when it is ambiguous as to whether \u2131_J is full-rank, i.e. whether \u0394_J is nonsingular.\nIf \u0394_J is singular, then \u03bc_J is not defined.\nEven if \u0394_J is singular, it will still be symmetric and positive semi-definite, and thus \u2131_J\u0303 will be full-rank, as we will see below (<ref>).\nHence, (<ref>) and \u03bc_J\u0303 are always well-defined.\n\nWe summarize this exposition by introducing the shorthand of a supplemented factor graph, i.e. factor graphs with such a partitioning of their factors as described above.\n\n\n    A supplemented factor graph is represented by the triplet (X, \u2131, B), where X are the target variables, \u2131 is a collection of m linear Gaussian factors of the form (<ref>), and B \u2282 [m] is a subset of indices such that \u2131_B is full-rank for X. The elements of \u2131_B are referred to as base factors, and determine the prior distribution p_X as per (<ref>). We refer to \u2131_B\u0305 as the supplemental factors, Z_B\u0305 as the supplemental measurements, and any \u2131_J\u0303 as a supplemented subgraph of \u2131. A supplemented subgraph \u2131_J\u0303 determines the Z_J\u0303-posterior distribution p_X|Z_J as per (<ref>).\n\n\nNow that we have meaningful prior and posterior distributions, we will be able to quantify and decompose the information provided by the supplementary factors in this model.\nTo conclude this section, we provide some useful properties that we will need in the next section.\n\n\nLet J,K \u2282 [m] be mutually exclusive sets of factors.\nThen we have that:\n\n    \u0394_J \u222a K   = \u0394_J + \u0394_K \n    If \u2131_J \u222a K is full-rank,\u03bc_J \u222a K   = \u039b_J \u222a K^-1( A_J^\u22a4\u0393_Jz_J + A_K^\u22a4\u0393_Kz_K)\n\nIn particular, for a given base graph B and supplementary set J \u2282B\u0305, we have that\n\n    \u039b_J\u0303   = \u039b_B + \u0394_J\n    \u03bc_J\u0303   = \u039b_J\u0303^-1( \u039b_B \u03bc_B + A_J^\u22a4\u0393_Jz_J )\n\n\n\nBy expanding A_J\u222a K, z_J \u222a K, and \u0393_J \u222a K into their J and K block components, we arrive at (<ref>) by direct computation of (<ref>).\nSimilarly, (<ref>) follows from (<ref>).\n\nMoreover, the poset of subgraphs is compatible with the Loewner ordering on their information matrices.\n\n\nFor any J,J' \u2286 [m],\n\n    J' \u2286 J \u21d2\u0394_J'\u2264\u0394_J\n\n\n\nFollows readily from (<ref>).\n\n\nUsing the prior/posterior from (<ref>-<ref>), we may compute the mutual information for any supplemented factor graph \u2131_J\u0303\u2282\u2131.\n\n    \n    Let \u2131_J\u0303\u2282\u2131 be a supplemented subgraph for some J \u2282B\u0305.\n    Then the mutual information between factor measurements Z_J and the state X are given by\n    \n    I(Z_J ; X)\n            = 1/2log|\u039b_J\u0303|/|\u039b_B|\n            = 1/2log |I + \u0394_J \u039b_B^-1|\n\n\n\nWe recall that for a Gaussian vector U \u223c\ud835\udca9(\u03bc, \u03a3), the differential entropy is given by\nh(U) = 1/2ln |2 \u03c0 e \u03a3| <cit.>.\nThus, using (<ref>-<ref>) and (<ref>-<ref>), we compute\n\n    I(Z_J ; X)    = h(X) - h(X | Z_J)\n       = 1/2log|2 \u03c0 e \u039b_B^-1|/|2 \u03c0 e \u039b_J\u0303^-1|\n        = 1/2log|\u039b_J\u0303|/|\u039b_B|\n\n\nThis computation gives us an information-theoretic monotonicity for the poset of subgraphs.\n\n\n    For any pair of full-rank subgraphs \u2131_J, \u2131_J'\u2286\u2131,\n    \n    J' \u2286 J \u21d2 I(Z_J' ; X) \u2264 I(Z_J ; X).\n\n\n\n\nThere are a few conditional moments of \u03bc_J\u0303 that we will need in the next section.\nWe enumerate those here:\n\n\n    Let supplemented subgraph \u2131_J\u0303\u2282\u2131 be given, with prior and posterior distributions as in (<ref>-<ref>).\n    Then we have the following conditional moments for all matrices T and vectors m:\n    \n    \ud835\udd3c( \u03bc_J\u0303 |  x)\n           = \u039b_J\u0303^-1( \u039b_B \u03bc_B + \u0394_J x) \n    \ud835\udd3c( \u03bc_J\u0303 + m_T^2  |  x)\n           = ( T\u039b_J\u0303^-1\u0394_J \u039b_J\u0303^-1)\n        + \u039b_J\u0303^-1( \u039b_B \u03bc_B + \u0394_J x) + m_T^2\n\n\n\n\n    The first moment (<ref>) follows readily from (<ref>) and \ud835\udd3c( Z_J |x)= A_J x.\n    Then let m' = \ud835\udd3c ( \u03bc_J\u0303|x ) be this conditional mean.\n    We rewrite the quadratic form:\n    \n    \u03bc_J\u0303 - m' _T^2\n           = \u039b_J\u0303^-1A_J^\u22a4\u0393_J( z_J - A_J x) \n         _T^2 \n       = z_J - A_J x_H^\u22a4TH^2\n\n    letting H = \u039b_J\u0303^-1A_J^\u22a4\u0393_J.\n    Since p(z_J | x) \u223c\ud835\udca9(A_J x, \u0393_J^-1), in expectation we have that:\n    \n    \ud835\udd3c_p(z_J | x) \u03bc_J\u0303 - m' _T^2\n           = (\u0393_J^-1H^\u22a4TH) \n       = ( T\u039b_J\u0303^-1\u0394_J \u039b_J\u0303^-1)\n\n    taking advantage of the cyclic property to rearrange the trace.\n    The extension to Eq.\u00a0(<ref>) is an application of a well-known recentering formula <cit.>, whereby for U\u223c\ud835\udca9(v, \u03a3):\n    \n    \ud835\udd3cU - v' ^2_M   = (M\u03a3) + v - v' _M^2.\n\n\n\n\n\n\u00a7 MEASURING REDUNDANCY IN LINEAR FACTOR GRAPHS\n\n\n\nIn this section, we introduce two metrics for measuring the redundancy of information provided by supplemental factors in a factor graph.\nOne of these metrics is a straight-forward application of the  redundancy function from PID, while the latter will consider redundancy pointwise on the target variable(s) in a manner analogous to  but from the perspective of statistical distance.\nIn order to maintain notational similarity between our metrics without causing confusion with PID functions from the literature (which, ultimately, are all extensions of mutual information and denoted with an annotated I), we denote the -based metric as .\nWe then define another redundancy metric based upon pointwise statistical distance, denoted  in reference to the Wasserstein-2 distance.\nThese two functions  and  are by no means the only possibilities, but demonstrate distinct ways to quantify redundancy \u201cpointwise\u201d with respect to target variables.\n\nBefore we define each metric in turn, we will first introduce a minimal formalism for redundancy that both will follow, generalizing the PID lattice from \u00a0<cit.> to more arbitrary quality metrics of posterior distributions.\nThe motivation here is to provide a unified language for redundancy, independent of the quality function that is being redundantly guaranteed.\n\n\n\n\n\n    \n    Let the supplemented factor graph (X, \u2131, B) be given as in Def.\u00a0<ref>.\n    Further, consider the measurements Z as the collection of predictor variables for target X as in Sec.\u00a0<ref>, with the collection of antichains \ud835\udc9c(Z) defined as in Eqs.\u00a0(<ref>-<ref>).\n    \n    A PID-like redundancy measure is the pair (\ud835\ude81, \ud835\ude80), where \ud835\ude80: \ud835\udcab(Z_B\u0305) \u2192 [Q_min, Q_max] is the quality function and \ud835\ude81: \ud835\udc9c(Z_B\u0303) \u2192 [Q_min, Q_max] is the redundancy function, when they fulfill the following axioms:\n    \n        \n        \n        \n  (MR) Monotonicity of \ud835\ude81 For all \u03b1, \u03b2\u2208\ud835\udc9c(Z), we have that \ud835\ude81(\u03b1) \u2264\ud835\ude81(\u03b2) whenever \u03b2\u2282\u03b1.\n        \n  (MQ) Monotonicity of \ud835\ude80 For all Z_J, Z_J'\u2208\ud835\udcab(Z), we have that \ud835\ude80(Z_J) \u2264\ud835\ude80(Z_J') whenever Z_J\u2286Z_J'\n        \n  (SR) Self-Redundancy \ud835\ude81({Z_J }) = \ud835\ude80(Z_J) for all J \u2282B\u0305.\n        \n        \n    \n\n\n\n\n\n\n\nA notable feature of the  PID is that it quantifies redundancy by looking at the minimal specific information provided by any source, in expectation, about the target variable pointwise.\nThat is, for any realization X=x of the target,  accounts for the least specific information that any source might provide under conditional expectation.\n\n\nAlthough this definition of redundancy has generated controversy in the PID literature, we consider it intuitively aligned with what we would want from a redundancy metric in factor graphs.\nNamely, redundancy ought to (a) provide robustness guarantees on the minimal information known if one or more sources fail, and (b) identify superfluous information sources in resource-constrained scenarios.\nWe want this same target-pointwise specificity for any redundancy function.\nThus, we offer the following definition as a further specification of Def.\u00a0<ref>:\n\n\nA PID-like redundancy measure (\ud835\ude81,\ud835\ude80) is said to be a target integral if there exists a mapping \ud835\ude82: \ud835\udcab(Z_B\u0305) \u2192 L^1(\u211d^nN, p(x) dx), denoted Z_J \u21a6\ud835\ude82_Z_J, such that\n\n    \ud835\ude81(\u03b1)    = \u222b p( x) min_J \u2208\u03b1\ud835\ude82_Z_J ( x)   d  x \n    \ud835\ude80(Z_J)    = \u222b p( x)  \ud835\ude82_Z_J ( x)   d  x\n\nWe refer to this mapping as the specific quality function, with \ud835\ude82_Z_J(x) as the specific quality about x provided by Z_J.\n\nThis notion of specific quality is motivated by specific information from <cit.>, which was used to define PID in <cit.>.\nBoth  and  are target integrals, and with Defs.\u00a0<ref>-<ref> can now be entirely defined by their specific quality functions.\nMoreover, it is evident that any target integral redundancy measure will satisfy axioms (MR) and (SR).\nNote that computing (<ref>) explicitly requires one to partition the target state space into regions where a given specific quality is minimal.\nIn practice, it will be easier to numerically approximate this integral with Monte Carlo sampling from p(x). \n\n\n\n \u00a7.\u00a7 An Information-Theoretic Redundancy Measure\n\n\nThe first redundancy metric we consider is the application of the  PID from <cit.>, (Sec.\u00a0<ref>) to the supplementary measurements in factor graphs (Sec.\u00a0<ref>).\nHere, the posterior quality metric will be standard mutual information, i.e. \u2261 I(\u00b7; X), decomposed by the \u2261( \u00b7 ; X) redundancy function from Def.\u00a0<ref>.\nIt is already well-known that , as the  PID, satisfies (MR) and (SR) axioms in Def.\u00a0<ref>, while (MQ) follows from Cor.\u00a0<ref>.\n\n\nLet the supplemented factor graph (X, \u2131, B) be given.\nFor every supplemental source Z_J \u2282Z_B\u0305, the specific information provided about X=x is given by, \n\n    _Z_J(x)\n       = \ud835\udd3c( logp(x | Z_J)/p(x) |  x)\n\n\n\nFor any (X,\u2131, B), we may compute the closed-form of this function.\n\n\n    \n    _Z_J(x)\n               =\n             I(Z_J ; X) - 1/2[ (  M_J') - x - \u03bc_B_ M_J^2 ]\n\n    where\n    \n    M_J   =  \u039b_B - \u039b_B\u039b_J\u0303^-1\u039b_B\n    M'_J   = \u0394_J\u039b_J\u0303^-1\n    \u039b_J\u0303   = \u039b_B + \u0394_J\n\n\nSince \u039b_J\u0303 > \u039b_B, we have that M_J > 0, i.e. is positive definite.\nThus, the term x - \u03bc_B_ M_J^2 \u2265 0 for all x, and \ud835\udd3cx - \u03bc_B_ M_J^2 = (M'_J).\nTherefore _Z_J(x) () reduces to mutual information when there is only one source in \u03b1, i.e. the self-redundancy axiom (SR).\n\nWhat distinguishes  and PID from previous information-theoretic approaches to redundancy (e.g. interaction information\u00a0<cit.>) is the pointwise decomposition of information.\nAs we mentioned above, for the  PID, this pointwise decomposition is only with respect to the target variable, not the predictors.\n\nWe see this pointwise nature explicitly in the quadratic form in (<ref>).\nFor any realized error x - \u03bc_B between the true state and the prior mean, the pointwise minimal source Z_J^\u22c6(x), where J^\u22c6=_J \u2208\u03b1_Z_J(x), will be that source which least corrects this error \u2014 assuming all predictor sources J \u2208\u03b1 contribute information matrices \u0394_J with similar profiles.\n\nProposition\u00a0<ref> will follow readily from the following computation.\n\n\n\n    \ud835\udd3c( x - \u03bc_J\u0303_\u039b_J\u0303^2 - x - \u03bc_B_\u039b_B^2  |  x) \n    \n    = (M_J') - x - \u03bc_B_M_J^2\n\n\n\n\n    From (<ref>) from Prop.\u00a0<ref>, we compute the moment\n    \n    \ud835\udd3c( x - \u03bc_J\u0303_\u039b_J\u0303^2   |  x)\n               = (\u039b_J\u0303\u039b_J\u0303^-1\u0394_J \u039b_J\u0303^-1)  \n       + \u039b_J\u0303^-1(\u039b_B \u03bc_B + \u0394_J x) - x^2_\u039b_J\u0303\n       = (M'_J) \n            + x - \u03bc_B ^2_\u039b_B \u039b_J\u0303^-1\u039b_B\n\n    By recombining with the conditional constant x - \u03bc_B_\u039b_B^2, we arrive at (<ref>).\n     \n     \n\nWe may now prove Proposition\u00a0<ref>.\n\nUsing the expressions from Sec.\u00a0<ref>, we may write the pointwise mutual information term in (<ref>):\n\n    logp(x | z_J)/p(x)\n    = log\u03a6_\u2131_B \u222a J(x)/\u03a6_\u2131_B(x)\n    \n    = 1/2log|\u039b_J\u0303|/|\u039b_B| - 1/2[  - \u03bc_J\u0303_\u039b_J\u0303^2 -  - \u03bc_B _\u039b_B^2 ]\n\nWe take this in expectation with respect to p(z_J | x). The first term is constant, and we use Lemma\u00a0<ref> to integrate the second, arriving at (<ref>).\n\n\n\n\n\n \u00a7.\u00a7 A Statistical Distance Redundancy Measure\n\n\nFor our second redundancy metric, we consider a different approach.\nWe instead define redundancy using the statistical distances between the source-defined posterior distributions p(X'| Z_J) and ground-truth target \u03b4_x.\nOur redundancy metric will look at the pointwise reduction in this statistical distance relative to that of the prior distribution p(X') from the base graph.\n\n\n\nLet the supplemented factor graph (X, \u2131, B) be given.\nWe define the specific Wasserstein error reduction (WER) function to be\n\n    _Z_J( x) = \ud835\udd3c( d_\ud835\udcb2^2^2(p(X'), \u03b4_x) - d_\ud835\udcb2^2^2(p(X' | Z_J), \u03b4_x)  |  x)\n\nwhere d^2_\ud835\udcb2^2 is the square Wasserstein-2 distance. For arbitrary Gaussian density f(u) \u223c\ud835\udca9(m,\u03a3) and point mass \u03b4_u', this takes the form:\n\n    d_\ud835\udcb2^2^2( f(u), \u03b4_u' ) \n        =  (\u03a3) + m - u' ^2\n\n\n\nEquivalently, one could also define _Z_J( x) as a reduction in the target-centered L^2-norm of the posterior pose-graph distribution, relative to the prior: \n \n    _Z_J( x) = \ud835\udd3c( X' - x^2 - X\u201d - x^2  |  x)\n\nwhere X' \u223c p_X and X\u201d\u223c p_X | Z_J (<ref>-<ref>).\n\nBefore we proceed to computing a closed-form for , we will first compute the closed form of the quality function  induced by Def.\u00a0<ref> through Def.\u00a0<ref>.\nThis quality function will serve a role analogous to single-source mutual information in PID.\n\n    The quality function for the redundancy measure induced by Def.\u00a0<ref> has the closed form:\n    \n    (Z_J) = 2 (\u039b_B^-1 - \u039b_J\u0303^-1)\n\n    Moreover, this function satisfies monotonicity (MQ).\n\n\n    Combining Defs.\u00a0<ref>&<ref>, we have that \n    \n    (Z_J)    = \ud835\udd3c d_\ud835\udcb2^2^2(p(X'), \u03b4_X)\n       - \ud835\udd3c d_\ud835\udcb2^2^2(p(X' | Z_J), \u03b4_X)\n\n    Since \ud835\udd3c| \u03bc_J' - x ^2 = (\u039b_J'^-1) for all Z_J'\u2282Z, we use (<ref>) to find that find that \ud835\udd3c  d_\ud835\udcb2^2^2(p_X, \u03b4_X) = 2 (\u039b_B^-1) and \ud835\udd3c  d_\ud835\udcb2^2^2(p_X|Z_J, \u03b4_X) = 2 (\u039b_J\u0303^-1).\n    The monotonicity of \ud835\ude80 follows from Prop.\u00a0<ref> and the monotonicity of the trace.\n\n\nWe now compute the closed-form solution of our specific quality function.\n\n\n    \n    _Z_J( x)\n               = ( N'_J ) \n            + \u03bc_B - x^2_N_J\n    whereN'_J\n            = \u039b_B^-1 - \u039b_J\u0303^-1 \n         - \u039b_J\u0303^-1\u0394_J\u0303\u039b_J\u0303^-1\n    N_J\n            = I - \u039b_B \u039b_J\u0303^-2\u039b_B\n\n\n\n    From (<ref>) and (<ref>), we have that\n    \n    _Z_J( x)\n               = (\u039b_B^-1 - \u039b_J\u0303^-1)\n       + \ud835\udd3c(\n            \u03bc_B - x^2 - \u03bc_J\u0303 - x^2   |  x)\n\n    As a special case of the moment (<ref>) from Prop.\u00a0<ref>, we have\n    \n    \ud835\udd3c( \u03bc_J\u0303 - x^2  |  x)\n           = (\u039b_J\u0303^-1\u0394_J \u039b_J\u0303^-1) \n       + \u03bc_B - x^2_\u039b_B \u039b^-2_J\u0303\u039b_B\n\n    Substituting this into (<ref>) and recombining terms, we arrive at (<ref>).\n\n\n\n\n\u00a7 AN APPLICATION TO 2D LANDMARK SLAM\n\n\nIn order to investigate the plausibility and behavior of our proposed  and  redundancy metrics, we now apply them to simulations of 2D landmark-based SLAM task.\n\n\nIn our simulation, a single robot randomly traverses a 2D environment populated by two landmarks.\nBy taking measurements of both landmarks, it will supplement its pose graph with additional, potentially redundant information.\nWe will use our redundancy metrics to estimate the redundancy that comes from using both landmarks, rather than one.\nIn these simulations, we dispense with the linearity restriction we have assumed up this point.\nOur robot's configuration space will be the 2D rigid body transformations (2), representing its pose (translation and rotation) relative to the origin in \u211d^2.\nFor all factor graphs operations, we use the GTSAM\u00a0<cit.>  toolbox for nonlinear factor graphs.\n\nAllow y^t\u2208\u211d^2 to represent the translation of any y \u2208(2), and y^t = (y_i^t)_i=1^n for any vector y\u2208( (2) )^n.\n\n\nWe denote the robot's pose trajectory as\nX = (X_0, ..., X_n), taking values in SE(2) over n timesteps, and let L_0 and L_1 denote the positions of the landmarks in \u211d^2. Collectively,\nthese n+2 variables are represented in the robot's factor graph, yet we will compute the redundancy metrics only with respect to trajectory variables X.\nBoth the robot's initial position and those of the landmarks are independently sampled from uniform distributions at initialization,\ni.e L_0, L_1 \u223c\ud835\udcb0([-C,C]^2) and X_0^t\u223c\ud835\udcb0([-C/2,C/2]^2) for some C>0.\nWe generate the robot's trajectory with a stationary random walk, i.e. \u03b7_i = X_i-1^-1 X_i is sampled with the same mean and covariance for each i.\nAt each step i=1,...,n, the robot takes an unbiased odometry measurement Z_i of \u03b7_i with fixed noise matrix \u03a3_odom in (x,y,\u03b8)-coordinates.\nThese odometry measurements compose the base graph.\nMoreover, at every timestep i, the robot takes a range-bearing measurement of each landmark, indexed Z_(s+1)n+i for landmark s \u2208{0,1}, and adds it to its factor graph.\nThe bearing measurement has fixed variance, while the variance of the range measurement scales quadratically with distance.\n\nWe will be estimating the redundancy of the 2-antichain \u03b1 = {Z_J_0, Z_J_1} where J_s = { i | (s+1)n < i \u2264 (s+2) n} \u2014 that is, the redundancy between the cumulative measurements of L_0 and those of L_1.\nIn order to compute  and , we compute the integral in (<ref>) by sampling from the base graph X' \u223c p_X.\nWe linearize each subgraph \u2131_J\u0303_s about its solution \u03bc_J\u0303.\nWe then estimate each of the specific quality functions \ud835\ude82_Z_J\u0303_s at x using the linear formulae from Props.\u00a0<ref> and <ref> as a first approximation.\n\n\n\n\n\nGiven ground-truth, one can evaluate the localization component of a SLAM pipeline using absolute or relative trajectory error (ATE and RTE, respectively) <cit.>.\nIt is less immediately obvious what ground-truth redundancy estimates should be compared against.\nWe will define a variant of ATE that aims at ground-truth error from a redundancy perspective.\n\n\n\n\n\n    Let the nonlinear supplemented factor graph (X, \u2131, B) be given for ground-truth trajectory x\u22082^n.\n    Moreover, let an antichain of supplemental factors \u03b1 = (Z_J_1, ... Z_J_k) be given as well, with \u03bc_J\u0303_k as the MAP estimate of X from each \u2131_J\u0303_k.\n    The worst-case absolute trajectory error (WC-ATE) is given by:\n    \n    WC-ATE(x, z) = \u2211max_J_kx^t_i - R_J_k\u03bc_J\u0303, i^t - t_J_k^2\n\n    where R_J_k, t_J_k = (R,t)(x^t,\u03bc_J\u0303_k^t) are the transformation and translation from the Umeyama method for aligning point-sets <cit.>.\n\nIn this case, we compute the WC-ATE for the antichain \u03b1 = {Z_J_0, Z_J_1} to compare to the redundancy scores.\n\nWe present exploratory results from our experiments in Figs.\u00a0<ref>-<ref>.\nIn Fig.\u00a0<ref>, we plot the WC-ATE of 500 simulated trajectories of n=10 poses against the  and  redundancies estimated from the robot's factor graph.\nFor the , we see that higher redundancies suggest, with high probability, a lower WC-ATE \u2014 i.e, the error in any pose is likely to be small regardless of which landmark is used for SLAM.\nHowever, the converse is not true: lower estimated  does not necessarily imply that both landmarks are needed in every realization.\nThis is expected: WC-ATE is based upon the fully realized trajectory x and measurements z, whereas the redundancies are computed in expectation.\nPut differently, it will often be the case that \u03bc_J\u0303_s for both s=0,1 will be closer to x in realization than expectation.\n\nIn Fig.\u00a0<ref>, we explore a possible source of heterogeneity between simulations that we would expect to affect redundancy: the distance of the landmarks to the robot.\nSince the simulated range-bearing measurements degrade with distance, we expect the information quality contributed by each landmark to scale down with distance.\nMoreover, we expect maximal redundancy in the contributed quality when both landmarks are close to the robot.\nWe see that this is generally the case: the highest redundancy scores for both  and  were computed from trajectories that remained close to both landmarks.\n\n\n\n\n\u00a7 CONCLUSION AND FUTURE WORK\n\n\n\nIn this paper, we have begun the exploration in the formal and theoretical direction for developing a theory of multi-source redundancy for factor graphs, drawing upon ideas from the partial information literature.\nWe adapt the PID axiomatic framework to the factor graph setting, generalizing the language of redundancy for quality metrics beyond mutual information (as in PID).\nWithin this framework, we contribute two redundancy metrics on linear factor graphs.\nThese metrics were applied to simulations of a 2D SLAM scenario.\n\n\nWe are currently working toward creating a rigorous toolbox for estimating and employing redundancy in linear factor graphs.\nIn future work, we will generalize the redundancy metrics proposed to nonlinear factor graphs.\n\n\n\nieeetr\n\n\n\n\n\n\n"}