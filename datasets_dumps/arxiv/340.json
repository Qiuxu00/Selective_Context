{"entry_id": "http://arxiv.org/abs/2303.06854v1", "published": "20230313044946", "title": "Robust Contrastive Language-Image Pretraining against Adversarial Attacks", "authors": ["Wenhan Yang", "Baharan Mirzasoleiman"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.CL", "cs.CR", "cs.LG"], "text": "\n\n[\n\nRobust \nContrastive Language-Image\nPretraining\nagainst Adversarial Attacks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nequal*\n\n\nWenhan Yangequal,yyy\nBaharan Mirzasoleimanequal,yyy\n\n\n\n\nyyyDepartment of Computer Science, University of California, Los Angeles\n\nWenhan Yanghangeryang18@g.ucla.edu\nBaharan Mirzasoleimanbaharan@cs.ucla.edu\n\n\n\n\nMachine Learning, ICML\n\n0.3in\n]\n\n\n\nContrastive vision-language representation learning \nhas achieved state-of-the-art performance for zero-shot classification,\n\nby learning from millions of image-caption pairs crawled from the internet.\n\n\n\n\nHowever, the massive data that powers large multimodal models such as CLIP, makes them extremely vulnerable to various types of adversarial attacks, including targeted and backdoor data poisoning attacks. \nDespite this vulnerability, robust contrastive vision-language pretraining against adversarial attacks has remained unaddressed. \nIn this work, we propose , the first effective method for robust pretraining and fine-tuning multimodal vision-language\nmodels. \n\n\neffectively breaks the association between poisoned image-caption pairs by considering a pool of random examples, and (1) matching every image with the text that is most similar to its caption in the pool, and (2) matching every caption with the image that is most similar to its image in the pool. \n\nOur extensive experiments show that our method renders state-of-the-art targeted data poisoning and backdoor attacks ineffective during pre-training or fine-tuning of CLIP. In particular, decreases the poison and backdoor attack success rates down to 0% during pre-training and 1%-4% during fine-tuning, and effectively improves the model's performance.\n\n\n\n\n\u00a7 INTRODUCTION\n\nRecent large-scale vision-language models pre-trained on \n\nmillions of image-captions pairs crawled from the internet has gained an unprecedented success. This is evident by their impressive zero-shot transferability of the model to downstream tasks, where natural language is used to describe visual concepts <cit.>. \n\n\n\nContrastive pre-trained vision-language models such as CLIP <cit.> and ALIGN <cit.> are trained using a multimodal contrastive loss which pulls the representations of every image-caption pair together while pushing those of different pairs apart. \n\n\nThis alleviates the need for expensive labeling of training examples, and enables scaling up the training data to millions of examples. \nHowever, the massive data that powers such large models also makes them extremely vulnerable to various types of adversarial attacks \n<cit.>. =-1\n\n\n\nIn particular, targeted data poisoning attacks on multimodal models add mismatched image-captions pairs to the pre-training data, to change the prediction of particular \nimages at the test time. Similarly, backdoor attacks overlay a small patch on a subset of training data to cause the model to misclassify test images with the same patch. \n\nNotably, poisoning just 0.0001% of the pre-training examples \ncan lead to success of a targeted poisoning attack.\nSimilarly, poisoning 0.01% of\n\n\npre-training examples can makes a backdoor attack successful <cit.>.\n\nCompared to clean-label data poisoning and backdoor attacks in the supervised settings which require poisoning on average 1% of training data <cit.>, attacking multimodal contrastive models requires orders of magnitude fewer poisoned examples.\n\nInterestingly, \nthe larger the model, the more vulnerable it is against adversarial attacks <cit.>.\n\n\n\nDespite this vulnerability, robust pre-training of multimodal vision-language models has remained unaddressed. \n\n\n\nRecent work of <cit.> studied poison identification during fine-tuning CLIP, by using another trusted pre-trained CLIP to remove \ndissimilar image-caption pairs. \n\nThis approach, however, assumes the knowledge of the similarity distribution of the clean and poisoned image-caption pairs, to be able to distinguish them based on a threshold.\nHowever, the similarity distribution of poisoned pairs is not available in practice, especially when the exact form of the attack is unknown. The similarity distribution of a backdoor attack is different from a poisoning attack, and specific settings like the patch size of the backdoor can also influence the similarity of poisoned pairs, making it impossible to determine the similarity distribution before training. Importantly, this approach is not applicable to pre-training the model on poisoned data, which is much more vulnerable to adversarial attacks than fine-tuning on a smaller dataset.\n\n\nIn this work, we propose the first effective method, namely , for robust training of multimodal vision-language models such as CLIP, against adversarial attacks. \n\nOur approach is based on the following key observation: while the similarity between the image-caption pairs of clean examples increases rapidly during the training, similarity between poisoned image-caption pairs grows slowly.\n\n\n\n\n\nAs a result, poisoned images and captions are not close to the groups of similar images and captions in the representation space, during the training. \nTo break the association between poisoned image-caption pairs, our main idea is to keep a relatively large and varying pool of randomly selected image-caption pairs. Then, we (1) match every image with the text that is most similar to its caption in the pool, \nand (2) match every caption with the image that is most similar to its image in the pool. This effectively prevents the attack by breaking the association between poisoned image-caption pairs. Leveraging image and text augmentations, we \ncan effectively defend the model and even improves its \nperformance significantly. \n\nOur extensive experiments show that our method renders state-of-the-art targeted data poisoning and backdoor attacks ineffective during pre-training or fine-tuning. In addition, our method leads to an increase of linear probe accuracy by up to 14% and zero-shot accuracy by up to 10 %. We note that is the only effective defense method against state-of-the-art attacks that can efficiently scale to pre-training large-scale vision-language models such as CLIP.\n\n\n\n\u00a7 RELATED WORK\n \n\n\nContrastive Representation Learning. Contrastive learning was originally proposed for self supervised representation learning from unimodal data. Self-supervised contrastive learning \nworks\nby maximizing agreement between differently augmented views of the same example and minimizing agreement between differently augmented views of\ndifferent examples <cit.>. Several works improved the performance of contrastive-learning on downstream tasks by imposing additional constraints to remove redundancy between components of the representation vectors and prevent collapse of the representations <cit.>, or using nearest-neighbor as positive pairs in the contrastive loss <cit.>. =-1\n\nContrastive Language-Image Pretraining.\n\n\n\n\n\nMultimodal vision-language models like\nCLIP <cit.> and ALIGN <cit.> \n\nare pre-trained on 400M/1B image-text pairs, by maximizing the agreement between representations of matched image-caption pairs and minimizing those of non-matched pairs.\nA recent line of work aims at improving the data efficiency and quality of CLIP representations, by leveraging image and text augmentations.\n\n\nDeCLIP <cit.> \n\nimproves data-efficiency of CLIP by\n\nmaximizing the similarity between two augmented image features using SimSiam <cit.>, two augmented text features using \nMasked Language Modeling (MLM) <cit.>, \n\nand matching augmented image features with their augmented text pairs and other similar text features.\n\n\nSLIP <cit.> improves the performance by \n\nmaximizing the agreement between two augmented image features\nusing SimCLR <cit.>, and matching the augmented image features with their text pair. \n\n\nCyCLIP <cit.> improves the representations by\n\n\nsymmetrization of the similarity between the two mismatched image-text pairs, as well as \n\nthe similarity between the image-image pair and the text-text pair.\nFinally, FILIP <cit.> uses transformer-based encoders for both modalities to learn more fine-grained features. \n\n\nAdversarial Attacks on CLIP.\n\n\n\nContrastive pretrained language-image models are extremely vulnerable to various types of data poisoning attacks \n<cit.>. \nIn particular, \ntargeted data poisoning attacks fool the model to misclassify a particular test example as an adversarial label. Backdoor attacks overlay a small patch on a subset of training data, and cause the model to misclassify test images with the same patch.\n\n \n\nCLIP has been also shown to be vulnerable to data poisoning attacks during fine-tuning\n<cit.>. \n\n\nDespite this vulnerability, designing effective defenses has remained unaddressed.\n\n<cit.> proposed a pre-processing and a post-processing defense for fine-tuning. \n\n\nThe pre-processing requires a clean pre-trained CLIP to remove examples with low cosine similarity between image and their corresponding text representation. \nThis requires knowledge of the similarity distribution between clean and poisoned examples, which is not available in particular when the exact form of the attack is not known.\n\nThe post-processing fine-tunes the poisoned model on another clean dataset of the same scale as the fine-tuning data. This is clearly not applicable to pre-training, due to the very high data and computational requirements.\n\n\n\n\n\nIn contrast, we propose a highly effective defense method that can be applied during pre-training or fine-tuning CLIP, without the need for pre- or post-processing. =-1\n\nDefense Strategies in Supervised Setting.\nDefense against data poisoning attacks has been extensively studied in the supervised settings.\nSupervised defenses can be divided into data sanitization and robust training. Data sanitization eliminates anomalies that fall outside a spherical radius in feature space <cit.>, activation space <cit.>, spectrum of feature covariance matrix <cit.>, gradients <cit.> or based on nearest neighbors <cit.>. Such methods do not scale to multimodal pre-training on millions of examples.\n\nRobust training relies on strong data augmentation <cit.>, randomized smoothing <cit.>, model ensembling <cit.>,\nbounding gradients <cit.>, adding noise <cit.>, or adversarial training <cit.>.\nSuch methods are not applicable to multimodal models like CLIP, and are often prohibitively slow, or drastically degrade the performance, even in the supervised setting. \n\n\n\n\n\n\n\u00a7 PRELIMINARY\n\n\n\n\n \u00a7.\u00a7 Contrastive Language-Image Pre-training (CLIP)\n\nCLIP is trained on millions of images caption pairs scraped from the web. Formally, we consider\na dataset \u2286\u00d7 consisting of pairs (_j^I, _j^T) where _j^I\u2208 is a raw image and _j^T\u2208 is a text caption. \n\nThe CLIP architecture consists of  an image encoder f_I:\u2192\u211d^d that encodes the raw image x_i^I into an embedding vector _i^I, and a text encoder f_T:\u2192\u211d^d that encodes the raw text _i^T into an embedding vector _i^T of the same dimension. Then projected image and text embeddings _i^I, _i^T are obtained by passing the encoded image and text _i^I, _j^T through their corresponding projection heads. The projected representations are normalized to have unit \u2113_2-norm.\nFinally, the InfoNCE loss <cit.>\n\nis applied to pull the projected embeddings of every image and its corresponding caption together while pushing apart the projected embeddings of the image from other captions in the same mini-batch.\nFormally, for a mini-batch of N image-captions pairs {(_j^I,_j^T)}_j=1^N, and their projected embeddings {(_j^I,_j^T)}_j=1^N, the CLIP loss is defined as:\n\n    \u2112_CLIP\n        =   -1/2N\u2211_j=1^N log [ exp(<_j^I,_j^T>/\u03c4)/\u2211_k=1^N exp(<_j^I,_k^T>/\u03c4) ] \n       -1/2N\u2211_k=1^N log [ exp(<_k^I,_k^T>/\u03c4)/\u2211_j=1^N exp(<_j^I,_k^T>/\u03c4) ],\n\nwhere <.,.> represents the inner product, and \u03c4 is a trainable temperature parameter.\nWe evaluate CLIP pre-trained with our method using both zero-shot and linear probe methods, as discussed below.\n\n\nZero-shot classification. Pre-trained Language-Image models such as CLIP enable zero-shot transfer of the model to downstream tasks, i.e., classifying test images by labels not seen at training time. To do so, the downstream labels can be transformed into suitable captions using the provided engineered prompts templates, e.g. \u201c\u201d. Then, the cosine similarity of the test image to each caption is computed, and the model predicts the label with the highest image-caption similarity. \n\nLinear probe. For a labeled image dataset, CLIP image representations can also be evaluated by training a linear classifier on the image representations obtained from the pre-trained CLIP image encoder and the corresponding labels. \n\n\n\nPre-trained Language-Image models such as CLIP enable zero-shot transfer of the model to downstream tasks, i.e., classifying test images by labels not seen at training time. To do so, the downstream labels can be transformed into suitable captions using the provided engineered prompts templates, e.g. \u201c\u201d. Then, the cosine similarity of the test image to each caption is computed, and the model predicts the label with the highest image-caption similarity. \n\n\n\n\n\n \u00a7.\u00a7 Poisoning and Backdoor Attacks\n\nLet \ud835\udc9f = {(_i^I, _i^T)}_i=1^n be the set of all training examples. Poisoning attacks <cit.> inject a small subset of poisoned examples \ud835\udc9f_p \nto the original training dataset \ud835\udc9f, such that when the model is trained on the poisoned training data {\ud835\udc9f\u222a\ud835\udc9f_p}, its prediction on particular test examples are changed to the adversarial label y_adv. At the same time, the poisoned model performs normally on other test examples. In this work, we consider both targeted poisoning and backdoor attacks as we discuss next.\n\n\n\n\n\n  \nTargeted Image attacks. In a targeted attack, the adversary aims to change the prediction of one \nparticular test examples \n_t^I to the adversarial label y_adv. \n\n\nTargeted poisoning attacks can be crafted following <cit.>, by constructing a caption set _adv of potential text descriptions related to the label y_adv, and making poisons by assigning captions in _adv to every target _t^I, i.e., \ud835\udc9f_p={(_t^I, _c^T): _c^T \u2208_adv}. \n\nFor constructing the caption set _adv, one can either search the training dataset for all sequences that contain this label string, and use these sequences as\nthe caption set. Alternatively, one can use the set of 80 different \u201cprompt-engineered\u201d text descriptions provided by CLIP for classification, and either use a subset or repeat them as necessary. \nThe number of captions in _adv determines the number of generated poisons per target.\nTo evade automated cleaning algorithms (e.g., removing duplicated images), tiny Gaussian noise can be added to the image, or the captions can be modified by substituting or adding words, without degrading the attack\nsuccess rate.\nA diverse caption set ensures that the image encoder is poisoned instead of the projection layers.\n\n\n\n\n\n\n\n  \nTargeted class attacks (fine-tuning). \nThis attack is similar to the targeted poisoning attack, with the difference that instead of a particular _t^I, the adversary aims to change the prediction of an entire target class \n_t \nto the adversarial label y_adv during fine-tuning <cit.>. Note that labels of the training examples are available and can be replaced in engineered prompts for fine-tuning CLIP.\nHaving an adversarial caption set _adv as explained above, the poisons are made by assigning captions in _adv to multiple images in the target class _i^I\u2208_t, \ni.e., \ud835\udc9f_p={(_t^I, _c^T): _c^T \u2208_adv, _i^I \u2208_t}. \n\n\n\n\n\n  \nBackdoor attacks. \nIn a targeted attack, the adversary attaches a small trigger patch to the poisoned images that are paired with adversarial captions _adv related to y_adv. In doing so, all the test images with the trigger patch will be misclassified as y_adv. \n\nSimilar to the targeted label attack,\ninstead of using a particular _t^I, we use different images _i^I\u2208, and add the trigger patch to them. Specifically, we define \ud835\udc9f_p = {(_i^I \u2295patch, _c^T): _c^T \u2208_adv, _i^I \u2208}. The caption set can be constructed in a similar manner to targeted attacks using captions found in the training data.\n\n\nIn general, while injecting poisoned examples in curated datasets used for supervised learning might be difficult, such poisons can be easily injected in uncurated datasets used by large multimodal models. This makes such models highly vulnerable to adversarial attacks.\n\n\n\n\n\u00a7 ROBUST TRAINING OF MULTIMODAL MODELS\n\n\nIn this section, we first study the effect of data poisoning attacks on multimodal models. Then, we present our method for robust training of such models against data poisoning attacks.\n\n\n\n \u00a7.\u00a7 Effect of Data Poisoning Attacks on CLIP\n\n\n\nBy minimizing the CLIP contrastive loss in Eq. (<ref>), the model changes such that every image representation _j^I moves towards its caption representation _j^T and gets far away from other (dissimilar) caption representations _k^T. \nThis makes corresponding categories of image and text (e.g. category of \u201cCat\" or \u201cDog\" in the image and text modality) to get closer to each other and get distant from other categories, during the training. \nCrucially, as image-caption pairs belonging to a particular category are relatively similar to each other, their gradients have a large alignment with other examples in the same category. \n\nTherefore, categories of similar image-caption pairs insert a large cumulative gradient on the model and change the model relatively quickly to get close to their pairs in the other modality.\n\nThe blue line in Fig. <ref> confirms that the average cosine similarity between image-caption representations of the clean examples increases rapidly during the initial phase of training.\n\nOn the other hand, image-caption pairs of poisoned examples are not similar to the other clean examples in the data. Hence, their gradient does not align well with the gradient of the clean examples. As the number of poisoned examples is small, such examples introduce a much smaller cumulative gradient on the model. Hence, image-caption pairs of poisoned examples move toward each other at a considerably lower speed. \nThe orange line in Fig. <ref> shows the average cosine similarity between 5 targeted poisoned image-caption pairs  of a truck image _t^I poisoned as deer {(_t^I, _c^T): _c^T\u2208_deer}.\nWe see that the average cosine similarity between poisoned image-caption pairs is smaller than that of clean examples, in particular after a few training epochs.\nCrucially, this implies that the poisoned images are not very close to the group of similar images, and poisoned captions are not very close to the group of similar captions in the representation space. \n\n\n\n\n\n\n \u00a7.\u00a7 Robust Training with \n\n\nTo prevent an attack from being successful, \n\nwe aim to break the association between the poisoned image-caption pairs. \nIf this can be done, the poisoned image and caption representations do not get close enough to each other during training and the attack does not succeed.\nTo achieve this, we rely on our key observation in Sec. <ref>: poisoned images and captions are not close to groups of similar images and captions in the representation space. \nTo break the association between poisoned image-caption pairs, our main idea is to keep a relatively large and varying pool of randomly selected image-caption pairs. Then, we (1) match every image with the text that is most similar to its paired caption in the pool, \nand (2) match every caption with the image that is most similar to its paired image in the pool. \nIn doing so, we match the poisoned images with a different text than the adversarial caption and match the poisoned caption with a different image than the poisoned image. Fig. <ref> illustrates our method.\n\nNote that the randomly selected set of examples in the pool change over time, and the poisoned examples need to be trained on for multiple iterations for the attacks to succeed.\nSince the number of poisoned examples is small, the probability of more than one poisoned image being in the pool during several iterations is negligible. Therefore, a poisoned caption is not matched to a poisoned image multiple times to be learned.\nAt the same time, as the pool is relatively large, matching the images with a similar caption and matching the captions with similar images do not considerably hurt the performance. Therefore, our method is able to  disassociate the poisoned image-caption pairs and effectively break the poisoning attacks, while preserving the superior accuracy of the model. As shown in Fig. <ref>, throughout the training, majority of the clean images and captions are paired with captions and images in the same category. On the other hand, poisoned images and captions are paired with captions and images in other categories. Hence, they cannot poison the model to associate the adversarial text category to the target images. Fig. <ref> shows nearest neighbors of image and caption of a clean and a poisoned  example during the training.\n\nIn addition, to further prevent the poisoned samples from being paired with other poisoned samples that potentially exist in the pool, we use various image and text augmentations.\n\nFinding nearest neighbors based on augmented image and text representations further prevent poisoned images and text to match with other poisoned text or images.\n\n\n\nIn particular, we use random image cropping, horizontal flipping, color jittering <cit.>, grayscale conversion <cit.>, and blurring <cit.> in our image augmentation policies. For the text augmentation, we use the EDA proposed by <cit.>, which includes synonym replacement, random swap, and random deletion as its augmentation policies. Each text will randomly select one policy for augmentation. The combination of NN and data augmentation not only defends the model against data poisoning and backdoor attack but also significantly improves the performance of the model, as shown in Table <ref>. We formally present our method below.  \n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, we sample a pool of P image-caption pair representations \ud835\udcab={(_i^I,_i^T)}_i=1^P as the representative of the distribution of data representations. During training, for every example (_j^I,_j^T) in the mini-batch, we first augment its image and text with our augmentation policies, and then match its augmented image representation _j^I with the augmented caption representation in the pool that is most similar to _j^T, i.e., \n\n_nn(j)^T=min__p^T\u2208\ud835\udcab_j^T-_p^T_2. \nEffectively, we form the positive image-caption representation pair (_j^I,_nn(j)^T) and use it instead of (_j^I,_j^T). Similarly, for the caption _j^T, we find the most similar image  representation to _j^I in the pool, i.e., \n\n_nn(j)^I=min__p^I\u2208\ud835\udcab_j^I-_p^I_2\nand form the positive image-caption representation pair (_nn(j)^I,_j^T) to use instead of (_j^I,_j^T).\nSimilar to the CLIP loss, we obtain the negative pairs from the mini-batch.\n\nThat is, for a mini-batch of N image-captions pairs {(_j^I,_j^T)}_j=1^N, and their projected embeddings {(_j^I,_j^T)}_j=1^N, the loss is defined as: =-1\n\n\n\n\n\n\n\n    \u2112_=   -1/2N\u2211_j=1^N log [ exp(<_j^I,_nn(j)^T>/\u03c4)/\u2211_k=1^N exp(<_j^I,_nn(k)^T>/\u03c4) ] \n       -1/2N\u2211_k=1^N log [ exp(<_nn(k)^I,_k^T>/\u03c4)/\u2211_j=1^N exp(<_nn(j)^I,_k^T>/\u03c4) ],\n\nFor the pool \ud835\udcab, we consider a first-in-first-out queue, which is initialized with P randomly selected image-caption pairs. We chose P large enough so that it represents the full image-caption distribution in the representation space. After training on every mini-batch, we update \ud835\udcab by taking the image and caption representations of the N examples in the mini-batch and concatenating them at the end of the queue. We discard the oldest N elements from the queue. \n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\nIn this section, we evaluate the effectiveness of our method in breaking targeted data poison and backdoor attacks while maintaining the model's performance. We evaluate our method for defending against various data poisoning and backdoor attacks, described in Sec. <ref>, during pre-training and fine-tuning CLIP.\n\n\n\n \u00a7.\u00a7 Datasets\n\n\nWe use Conceptual Captions 3M (CC3M) <cit.> as our pre-training dataset and MSCOCO <cit.> as our fine-tuning dataset. Due to limited computational resources, \nfor pre-training we randomly sampled 1M image-caption pairs from  CC3M as our training dataset. For MSCOCO, we used the 25% split proposed by <cit.>. We assess our method on 10 more downstream datasets introduced by <cit.>, the detail of which can be found in Table <ref>.\n\n\n\n\n\n \u00a7.\u00a7 Training\n\nWe evaluate during pre-training and fine-tuning CLIP. \nFor pre-training, we use an open-source implementation of CLIP as our model, with default ResNet-50 as the image encoder and Transformer as the text encoder. Each experiment is run with a batch size of 512 for 30 epochs. \nFor fine-tuning, \nwe use the Open AI's released ViT-B/32 parameters to initialize our pre-trained model. Each experiment is run with a batch size of 128 for 30 epochs. The hyperparameter settings follow the one used by the original CLIP paper <cit.>.\n\n\n\n\n \u00a7.\u00a7 Attack Methods\n\nWe consider targeted image attacks, targeted class attacks, and backdoor attacks, discussed in Sec. <ref>.\n\n\nTargeted Image Attacks\nThe goal of the targeted image attacks is to poison the model to classify a particular image _t^I into the adversarial label y_adv without harming the performance of the model on other images. As shown empirically in <cit.>, 3 poisoned samples out of 3 million examples are enough to fool the model into misclassifying the target image by the adversarial label. We evaluate against targeted image attacks during pre-training and fine-tuning CLIP.\n\nIn our pre-training experiment, \n\nwe choose a random target image _t from the conceptual captions validation set, and then choose a random target class from the ImageNet test set to generate a set of |_adv|=15 adversarial captions.\n\n\nNote that,  <cit.> pre-trained 32 CLIP models and measured the attack success rate as the fraction of positioned models. This requires 3200 GPU hours. \nTo reduce the computation, we poisoned 32 different random images by generating 15 adversarial captions related to a label selected at random from ImageNet. Then, we report the attack success rate as the number of images that are classified as the adversarial label, in a single pre-training run. In doing so, the attack success will be at least as high as <cit.>. In addition, note that attacking our smaller dataset of 1M examples also results in a higher attack success rate compared to that of 3M used by <cit.>.\n\n\n\nIn our fine-tuning experiment, we follow the same procedure as <cit.>. We select two random classes from MSCOCO, \n\nand poison 100 images from each class by replicating each image 3 times, and using an adversarial label for them. We use two adversarial labels selected at random from MSCOCO. \n\n\nIn total, 600 poisoned images are injected to the training set.\n\n \n\n \n\nTargeted Class Attacks (fine-tuning)\nThe goal of the targeted class attacks is to make the model  miclassify an entire class of images _t by the adversarial label  y_adv, without harming its performance on other classes. Note that this attack is specific to fine-tuning on a labeled dataset, since image-caption datasets like CC3M do not have class information. We select \ntwo random classes from MSCOCO \nas the adversarial labels and chose 100 images from \neach class as our target images. Since misclassifying the whole class demands stronger poison, each poisoned image is replicated 5 times as opposed to 3 in the targeted image attack, so there are in total 1000 poisoned images injected to the training set. We measure the attack success rate on the validation set of the poisoned classes.\n\n\n\n\nBackdoor Attacks\nThe goal of the backdoor attacks is to make the model misclassify any image with the backdoor patch \nto the desired class label. We use the public Hidden Trigger Backdoor Attacks (HTBA) patches <cit.>, that are square triggers generated by drawing a random 4x4 matrix of colors and resizing it to the desired patch size using bilinear interpolation. We use a resized 16x16 patch and put it consistently on the left top corner of the image. \n\nWe evaluate the effectiveness of backdoor attacks during pre-training and fine-tuning.\nAs shown empirically in <cit.>, a 0.01% poison ratio is enough to create a backdoor in the CLIP model trained on 3M data. Thus, we used 0.01% backdoor ratio for both our pre-training and fine-tuning experiments. In our pre-training experiments, we randomly select 100 images from the CC3M dataset and pair them with adversarial captions related to a random target class from ImageNet. To evaluate the effectiveness of the backdoor attacks, we select 100 random images from the ImageNet validation set and patch them in the left top corner.\n\nFor fine-tuning, we randomly select 15 images from the MSCOCO dataset and label them as a another random target class from MSCOCO. \n\nFor evaluation, we select 100 random images from ImageNet validation set and patch them in the left top corner to measure the attack success rate.\n\n\n\n \u00a7.\u00a7 robustly  Pre-trains CLIP\n\n\nFirst, we evaluate the effectiveness of our method, , against poisoning and backdoor attacks during the pretraining phase. We present our result in Table <ref>. We define the attack success rate as the fraction of poisoned or backdoored images successfully classified as the desired label. We see that without any defense, 45% of the total poisoned images are classified to the desired target class. Moreover, the 52% of the total backdoored images unseen in the training set are classified to the desired target class. On the other hand, is able to fully defend the attack and reduce the attack success rate to  0% for both the backdoor and the poisoning attacks. \nThis clearly confirms the effectiveness of in breaking various types of data poisoning and backdoor attacks on CLIP during pre-training.\n\n\n\n \u00a7.\u00a7 Robustly Fine-tunes CLIP\n\nNext, we evaluate how effective is against data poisoning and backdoor attacks during the fine-tuning phase. We present our result in Table <ref>. Since every example in MSCOCO has a caption \nand a class label, we use text retrieval as our downstream evaluation task. \nFollowing <cit.>, we construct our test set by selecting 50 image-caption pairs from each of the 78 classes of the MSCOCO dataset, excluding the two classes that have fewer than 50 instances. To do text retrieval, we calculate the similarity between the \nrepresentation of test image and the the representations of all the captions from the test set. Then, we select the topk captions with the highest similarity as our retrieved captions. \n\n\nWe use hit@k ratio as our retrieval metrics, defined as the fraction of poisoned images whose topk retrieved captions \n\ninclude the captions from the target label. A higher hit@k ratio indicates a higher fraction of images being successfully poisoned. We consider 3 commonly used values for k\u2208{1,3,5}. As shown in Table <ref>, all three attacks are highly effective, which is indicated by their high hit@k ratios on CLIP. The Hit@5 ratio on targeted image attack is 80% (compared to 12.5% on clean model). \n\nThe targeted class attack is 16% (compared to 7.5% on clean model), and the backdoor attack Hit@1 is 85% (compare to 1% on clean model). On the other hand, is able to effectively defend the attacks and provide a similar or even lower hit@k ratio than that of the clean model on all k\u2208{1,3,5} and all attacks. This clearly confirms the effectiveness of in breaking various types of data poisoning and backdoor attacks on CLIP during fine-tuning. Note that the Hit@k ratio is not 0 even for the CLIP model trained on clean dataset. This is also confirmed by  <cit.>. \n\n\n\n \u00a7.\u00a7 Does not Hurt the Performance\n\nLast but not least, we evaluate if negatively impacts the model performance. We assess the performance of on a variety of datasets introduced by <cit.>, the detail of which can be found in Table <ref>. We evaluate with both zero-shot and linear-probe methods and compare its performance with the standard pretrained CLIP. Both and CLIP are trained on the clean 1M CC from scratch. Each experiment is run with a batch size of 512 for 30 epochs. We present our result in Table <ref>. As shown, our method does not harm the overall model performance. In contrast, effectively improves the classification performance across all ten datasets on both zero-shot and linear probing, up to 10% on Caltech101 on zero-shot and 14% on DTD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\nWe proposed RoCLIP, an effective method for robust training multimodal vision-language models such as CLIP against data poisoning and backdoor attacks. Our method utilizes the nearest neighbor as well as random data augmentation to break the associations between the poisoned image and caption pairs, thus effectively defending the models. Through extensive experiments, we demonstrated that our proposed method drops the attack success rate down to 0% on both image target attack and backdoor attack and 2.5% on label target attack. At the same time, it improves the model's performance by up to 12% compared to the baseline CLIP model.  \n\n\n\n\n\n\n\nicml2022\n\n\n\n"}