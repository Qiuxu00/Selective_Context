{"entry_id": "http://arxiv.org/abs/2303.06945v2", "published": "20230313092734", "title": "CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for Protein-Protein Interaction Site Prediction", "authors": ["Jiaxing Guo", "Xuening Zhu", "Zixin Hu", "Xiaoxi Hu"], "primary_category": "q-bio.QM", "categories": ["q-bio.QM", "cs.LG"], "text": "\n\nContext-Aware Selective Label Smoothing for Calibrating Sequence Recognition Model\n    Shuangping Huang^1,2,  Yu Luo^1,  Zhenzhou Zhuang^1,  Jin-Gang Yu^1,2*,  Mengchao He^3,  Yongpan Wang^4\n    \n===========================================================================================================\n\n\n\n\n\nProtein-protein interactions are of great importance in biochemical processes. Accurate prediction of the protein-protein interaction sites (PPIs) from protein sequences deepens our understanding of biological mechanism and is crucial for new drug design. However, conventional experimental methods for PPIs prediction are costly and time-consuming so that many computational approaches, especially ML-based approaches, have been developed recently. Although these approaches have achieved gratifying results, there are still two limitations: (1) Most existing models have excavated a number of useful input features, but failed to take coevolutionary features into account, which could provide clues for inter-residue relationships and could be helpful for PPIs prediction; (2) The attention-based models only allocate attention weights for neighboring residues, instead of doing it globally, which may limit the model's prediction performance since some residues being far away from the target residues in the protein sequence might also matter.\n\nWe propose a coevolution-enhanced global attention neural network, a sequence-based deep learning model for PPIs prediction, called CoGANPPIS. Specifically, CoGANPPIS utilizes three layers in parallel for feature extraction: (1) Local-level representation aggregation layer, which aggregates the neighboring residues' features as the local feature representation similar to previous studies; (2) Global-level representation learning layer, which employs a novel coevolution-enhanced global attention mechanism to allocate attention weights to all the residues on the same protein sequences; (3) Coevolutionary information learning layer, which applies CNN & pooling to coevolutionary information to obtain the coevolutionary profile representation. Then, the three outputs are concatenated and passed into several fully connected layers for the final prediction. Extensive experiments on two benchmark datasets have been conducted, demonstrating that our proposed model achieves the state-of-the-art performance. The source code is publicly available at https://github.com/Slam1423/CoGANPPIS_source_codehttps://github.com/Slam1423/CoGANPPIS_source_code.\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nProteins participate in a variety of biological processes in organisms. They rarely act alone, instead, they usually carry out various functions by interacting with different kinds of molecules, such as DNA, lipids, carbohydrates, and other proteins <cit.>. Such a process of establishing physical contacts of high specificity between two or more protein molecules is known as protein-protein interaction, which plays an important role in many biochemical processes including immune response, muscle contraction, and signal transduction. Considering the high practical and research values of PPIs prediction, many approaches have been proposed so far. There are some conventional experimental methods, such as two-hybrid screening, relationship purification, and intragenic complementation, being commonly applied to identify PPIs <cit.>. However, these experimental methods suffer from being costly and time-consuming so that more accurate and efficient computational predictors for PPIs are of great value for biologists.\n\nWith the rapid development of computer science, a lot of computational approaches, especially ML-based approaches, have been developed, which take protein sequences or structures as input and be known as sequence-based and structure-based respectively <cit.>. Although Structure-based methods have achieved some promising progress in recent years <cit.>, they may cause problems for biological researchers since the number of proteins with available structures is limited. For example, AlphaFold2 has shown promising performance in protein structure prediction, but its effectiveness on some newly-discovered proteins and some exotic proteins still remains to be tested. At the same time, its requirement for computational resources could be too high for most researchers <cit.>. In contrast, sequence-based methods are more practical since protein sequences are easier to be obtained with the noticeable development of high-throughput techniques. \n\nSequence-based methods could be classi\ufb01ed as partner-speci\ufb01c and nonpartner speci\ufb01c sequence-based PPIs prediction <cit.>, and in this paper we focus on the later one. The partner-speci\ufb01c sequence-based PPIs prediction aims to identify the interaction residue pairs of two given proteins, which has not been covered in our present work.\n\n\n\nSequence-based methods can be further classified into 2 categories: traditional machine learning approaches and deep learning approaches. The commonly-used traditional machine learning approaches include SVM <cit.>, Na\u00efve Bayes <cit.>, shallow neural network <cit.>, random forest <cit.>, and logistic regression <cit.>. However, these methods cannot capture the relationships among different residues located on the same protein sequences since they treat every residue as an independent sample.\n\nIn recent years, due to the great success of deep learning in many fields such as computer vision, speech recognition and natural language processing, the models based on deep learning have also been used in PPIs prediction. Among these, DELPHI proposed by Li et al., a fine-tuned ensemble model combining recurrent neural networks and convolutional neural networks, shows significant improvement in PPIs prediction compared with traditional machine learning models <cit.>. Zeng et al. devised DeepPPISP, which is a convolutional neural network-based model and achieves good outcomes by combining both local and global sequence contexts as input and processing them respectively <cit.>. On the basis of DeepPPISP, Lu et al. constructed an attention-based convolutional neural network, which gives different attention weights to the neighboring residues so that it could make a better understanding of the local environment of the target residue <cit.>. Tang et al. used a double-layer attention mechanism prediction model based on graph convolution, which further improves the performance and interpretability of the prediction <cit.>. Besides, Stringer et al. proposed an ensembled model combining several neural net architectures and achieved consistently peak prediction accuracy <cit.>.\n\nConventional sequence-based input features can be roughly classified into 4 categories: raw protein sequences, evolutionary information, residue physiochemical properties, and predicted structural features <cit.>. Raw protein sequence features are obtained by encoding each amino acid on the protein sequences into a 20D one-hot vector since there are 20 different kinds of amino acids. Evolutionary information usually refers to the position-specific scoring matrices (PSSM) as well as other conservative scores of the proteins, which are calculated from multiple sequence alignments (MSA) and very informative for protein-related prediction tasks. Residue physiochemical properties (such as residue's charge and polarity) have been applied in many models in recent years, which can be obtained from databases or some specific predictors. Besides, in the absence of protein structure, the predicted structural features (such as hydrophobicity, secondary structure, disorder) can also be helpful.\n\nRecently, another sequence-based feature, coevolutionary information based feature, has been applied to another important protein-related problem, protein contact-map prediction, and brings about significant performance improvement <cit.>. These features are mainly obtained by direct coupling analysis (DCA) and could quantify the inter-residue coevolutionary relationships. Intuitively, for a residue, its properties and behaviours should be more similar to the residues closely related to it, which inspires us to develop a coevolution-enhanced global attention mechanism for a better attention weight allocation among different residues on the protein sequences.\n\nTo evaluate the performance of our model, we compare it with other seven computational methods (PSIVER, ISIS, SPRINGS, DELPHI, DeepPPISP, ACNN and ensnet_p) on two benchmark datasets Dset422 and Dset448. The experimental results show that our model achieves state-of-the-art performance for PPIs prediction. The main contributions of this paper are as follows:\n\n(1) To the best of our knowledge, this is the first time to introduce coevolutionary information as input features into deep learning based PPIs prediction model and we verify its usefulness in PPIs prediction by ablation analysis.\n\n(2) We propose a novel coevolution-enhanced global attention mechanism for global-level representation learning, which allocates attention weights based on a better understanding of the whole protein sequences and the coevolutionary relationships among the residues.\n\n(3) We conduct extensive experiments on two benchmark datasets, the results of which demonstrate that CoGANPPIS achieves state-of-the-art performance.\n\nThe rest of this paper is organized as follows: Section 2 introduces the datasets and the input features. Section 3 presents the architecture of our model, the experimental results and experiment analysis. In the end, section 4 summarizes the discussion of this paper.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 MATERIALS AND METHODS\n\n\n\n\n \u00a7.\u00a7 Datasets\n\n\nIn this study, two benchmark datasets, Dset422 and Dset448, are used in experiments. Dset422 consists of Dset72 <cit.>, Dset186, and Dset164 <cit.>, whose protein sequences were collected from Protein Data Bank <cit.>. The protein sequence homology is less than 25% and if an amino acid has absolute solvent proximity less than 1 \u00c5^2 before and after binding with other proteins, it will be defined as an interaction site <cit.>. Dset448 is sourced from the BioLip database, where residues are defined as interaction sites if the distance between an atom of this residue and an atom of a given protein-partner less than 0.5 \u00c5 plus the sum of the Van der Waal's radii of the two atoms <cit.>. First, the protein sequences were mapped into Uniprot databases to collect binding residues across different complexes. Then, they were clustered by Blastclust at 25% similarity, after which one protein was selected from each cluster to ensure that proteins in Dset448 share similarities less than 25%. Besides, a dataset of 4392 protein sequences was constructed in the paper of PIPENN <cit.>. In this paper, we name it Dset4392. The data of Dset4392 is from the BioLip Database, where binding sites are defined similar to Dset448. We utilize it for pretraining.\n\nFor Dset422 and Dset448, we randomly divided them into training set (about 83% of randomly selected proteins), validation set (about 5% of randomly selected proteins), and test set (the remaining proteins) respectively. Consequently, for Dset422, there are 352 proteins in the training set, 21 proteins in the validation set, and 49 proteins in the test set. And for Dset448, there are 373 proteins in the training set, 22 proteins in the validation set, and 53 proteins in the test set.\n\n\n\nWe count the distribution of sequence lengths of the three datasets. As shown in Table <ref>, only a small proportion of protein sequences in the two datasets are longer than 500. Hence, for the convenience of model training, we preprocess the protein sequences by unifying their lengths to 500, that is, if a protein sequence is longer than 500, we truncate it to 500; if a protein sequence is shorter than 500, we pad their length to 500 with zeros. \n\n\n\n \u00a7.\u00a7 Input Features\n\n\nThe features commonly used in previous research, including the position-specific scoring matrix, Netsurf-based features, and raw sequence features, are applied in our model. Besides, we also introduce coevolutionary information into our model, which, to the best of our knowledge, is the first time to be used as input features in deep learning based PPIs prediction. These features are described in detail as follows.\n\n\n\n  \u00a7.\u00a7.\u00a7 Position-specific scoring matrix\n\n\nPosition-specific scoring matrix (PSSM) contains evolutionary information, which has been shown effective for PPIs prediction <cit.>. We perform the PSI-BLAST algorithm on each input sequence against NCBI's non-redundant sequence database with three iterations and an E-value threshold of 0.001 to obtain its PSSM, where every amino acid residue on the underlying sequence is encoded as a vector with 20 elements.\n\n\n\n  \u00a7.\u00a7.\u00a7 Netsurf-based features\n\n\nNetSurfP-3.0 is a tool for predicting solvent accessibility, secondary structure, structural disorder and backbone dihedral angles for each residue of an amino acid sequence <cit.>. Here we utilize it to predict RSA, ASA, 3-state secondary structure as well as 8-state secondary structure from the sequence. Each amino acid residue is encoded as a vector with 13 elements, which represents the predicted solvent accessibilities and the probabilities of being the corresponding secondary structure state at this position.\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Raw sequence features\n\n\nIn this study, we include two raw features, amino acid type and sequence length into our model. Most proteins consist of 20 different amino acids. Hence, we encode each amino acid residue as a 20D one-hot vector representing the amino acid type at this position. Besides, we utilize an integer to represent the length of the sequence for each residue.\n\n\n\n  \u00a7.\u00a7.\u00a7 Coevolutionary information\n\n\nCoevolutionary relationship between amino acid residues refers to the interdependent changes that occur in pairs of residues on the same protein sequences, which help maintain proteins' stability, function, and folding <cit.>. As mentioned earlier, coevolutionary information based input features brought about great performance improvement in protein-contact map prediction, inspiring us to apply it in this study.\n\nDirect-coupling analysis (DCA) is one of the main computational approaches to capture proteins' coevolutionary information. The key idea of DCA is to disentangle direct pairwise couplings of each two amino acid residues on the same protein sequences. For each protein, DCA takes its multiple sequence alignments (MSA) as the input, which is obtained by BLASTP, and returns a N\u00d7 N matrix, where N refers to the length of this protein sequence. The (i,j) element of this matrix refers to the direct coupling degree of the ith residue and the jth residue on this protein sequence. The larger this value is, the higher the coevolutionary relationship exists between these two residues. In our study of predicting whether an amino acid residue is an interaction site or not, the corresponding column of the target amino acid residue in the DCA matrix has been extracted as its coevolutionary information feature.\n\nThere are three usual DCA algorithms, mpDCA <cit.>, mfDCA <cit.>, and plmDCA <cit.>. The mpDCA uses a semi-heuristic message-passing approach, and its slow computation speed makes it difficult to be applied to a large-scale dataset. The mfDCA uses a mean-field approach based on the maximum-entropy model, which greatly improves the computational speed. On the basis of mfDCA, the plmDCA applies pseudo-likelihood to the Potts model and achieves higher accuracy than mfDCA. Based on the above comparison, we utilize plmDCA to generate DCA matrices in this study. \n\n\n\n\n \u00a7.\u00a7 Model Architecture\n\n\n\n\nFigure <ref> is an overview of the proposed framework. First, we extract the local features, global features, and coevolutionary features of the primary protein sequence with different components respectively, which, in this paper, are referred to as local-level representation aggregation layer, global-level representation learning layer, and coevolutionary information learning layer. Each layer outputs a feature representation vector. Then we concatenate three feature representation vectors as the output of the feature extraction and pass it into the prediction layer consisting of four fully connected layers for the final prediction about whether the target amino acid residue is an interaction site or not. Now we introduce the three layers in feature extraction in detail.\n\n\n\n  \u00a7.\u00a7.\u00a7 Local-level representation aggregation layer\n\n\nFor each target residue, a sliding window of length (2n+1) is used to aggregate the features of itself and its neighboring 2n residues. For the ith residue on the protein sequence, we denote its local feature representation as h_i^local.\n\n\n\n  \u00a7.\u00a7.\u00a7 Global-level representation learning layer\n\n\nIt has been shown that the global features of protein sequences are critical for PPIs prediction <cit.>. Also, the usage of the coevolutionary information has gained success in some other related protein problems <cit.>. Hence, we consider to utilize a coevolution-enhanced global attention mechanism to distinguish the importance of residues. Assuming that the ith residue is the predicting target, all residues on the same sequence are linearly added according to the attention scores,\n\n\n    h_i^p=\u2211_j=1^N\u03b1_ijh_j,\n\n\nwhere h_j refers to the PSSM, predicted secondary structure, and raw protein sequence features of residue j and \u03b1_ij refers to the attention score, which estimates the importance weight of residue j on target residue i. Intuitively, different residues on the same protein sequence should have different importance for the target residue, and those which match the charateristics of the whole protein and share a close relationship with the target residue should be paid more attention to. Therefore, \u03b1_ij is calculated as follows:\n\n\n    \u03b1_ij= softmax(\u03c0(i,j))=exp(\u03c0(i,j))/\u2211_k=1^Nexp(\u03c0(i,k)),\n\n\n\n    \u03c0(i,j)=q_1^ T LeakyRelu([W_1(p\u2299 h_j)\u2016 W_2(h_i\u2299 h_j)]\u2016 w_ij).\n\n\nHere, we use LeakyRelu as the activation function. \u2299 indicates element-wise product, \u2016 indicates concatenation operation, and w_ij\u2208\u211d^1 refers to the (i,j) element of DCA matrix of the current protein, which provides us some clues for the relationship between the residue i and residue j. W_1\u2208\u211d^d\u00d7 d, W_2\u2208\u211d^d\u00d7 d and q_1\u2208\u211d^2d+1 are trainable parameters. p can be seen as the feature representation of the whole protein sequence, which is obtained by mean-pooling on all the residues' features on this protein,\n\n\n    p=1/N\u2211_j=1^Nh_j.\n\n\nOur approach makes the attention weights between the target residue and other residues dependent on not only the whole protein sequence feature representation but also the coevolutionary relationships between them, suggesting that those residues which match the charateristics of the whole protein and are closely related to the target residue will be attached more importance.\n\nThen we concatenate the global representation of the target residue h_i^p and its original feature h_i,\n\n\n    h_i^global = h_i^p\u2016 h_i,\n\n\nwhere h_i^global is the result of the global-level representation learning layer.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Coevolutionary information learning layer\n\n\nWe have introduced coevolutionary information into the attention mechanism to exploit the relationship among residues as above. Now we further utilize the coevolutionary information on a larger scale, i.e, on the whole protein sequence level. Suppose we are predicting the ith residue on the protein sequence. First, we take its corresponding column in the DCA matrix as its coevolutionary information. Then we pass it into the CNN & pooling layer as shown in Figure <ref>:\n\n\n    h_i,k^dca= Relu( conv1d^(k)( BN( DCA[:,i]))), k\u2208[1,K],\n\n\n\n    h_i^dca=\u2016_k=1^Kh_i,k^dca,\n\n\nwhere DCA[:,i] is the ith column of the DCA matrix of the underlying protein. BN refers to the BatchNormalization operation and the 1D convolution operation conv1d extracts the normalized coevolutionary features. We use Relu as the activation. Here we use K different convolution kernels for a better extraction of coevolutionary features. Finally, we obtain the coevolutionary representation h_i^dca by concatenating all K results linearly.\n\n\n\n  \u00a7.\u00a7.\u00a7 Prediction layer\n\n\nTo predict whether an amino acid residue is an interaction site or not, first we concatenate three former feature extraction results to obtain the final representation:\n\n\n    h_i^pred=h_i^local\u2016 h_i^global\u2016 h_i^dca,\n\n\nwhere h_i^local, h_i^global and h_i^dca are the results of local-level representation aggregation layer, global-level representation learning layer, and coevolutionary information learning layer. h_i^pred is the final representation of the residue, which will be passed into fully connected layers:\n\n\n    x^(t)= Relu(W^(t)x^(t-1)+b^(t)), t\u2208 [1,T],\n\n\nwhere x^(t) and x^(t-1) refer to the input vector and output vector of the tth fully connected layer, respectively. Here, x^(0)=h_i^pred. W^(t) denotes the weight matrix and b^(t) denotes the bias. Besides, ReLU and dropout are utilized in each layer except the last one. After the last layer, a Sigmoid function is used to generate the final prediction:\n\n\n    \u0177=1/1+e^-x^(T),\n\n\nwhere \u0177 denotes the predicted probability of the residue being the interaction site. And 1-\u0177 is the predicted probability of the residue being the non-interaction site.\n\n\n\n \u00a7.\u00a7 Evaluation Metrics\n\n\nPPIs prediction can be seen as a binary classification problem for identifying whether an amino acid residue is an interaction site or not. Consequently, there could be four types of results based on the residue's true category and predicted category, i.e., true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). Here, TP and TN refer to the correctly predicted interaction sites and non-interaction sites respectively; FP and FN refer to the incorrectly predicted interaction sites and non-interaction sites respectively.\n\nWe select six evaluation metrics to comprehensively evaluate the predictive performance, including area under the precision-recall curve (AUPRC), accuracy (ACC), recall, precision, F-measure (F_1), and Matthews correlation coefficient (MCC). Considering that our dataset is imbalanced with more non-interaction sites than interaction sites, F_1 and MCC indices deserve more attention. The formulas for calculating these metrics are as follows:\n\n\n    ACC=TP+TN/TP+TN+FP+FN,\n\n\n    Recall=TP/TP+FN,\n\n\n    Precision=TP/TP+FP,\n\n\n    F_1=2\u00d7 Precision\u00d7 Recall/Precision+Recall,\n\n\n    MCC=TP\u00d7 TN - FP\u00d7 FN/\u221a((TP+FP)(TP+FN)(TN+FP)(TN+FN)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 RESULTS AND DISCUSSION\n\n\n\n\n \u00a7.\u00a7 Implementation\n\n\nIn the feature extraction part, the sliding window length in the local-level representation aggregation layer is set as 7. We use three convolution kernels in the CNN & pooling layer and the sizes are set as 3, 5 and 7, respectively. In the classification part, we conduct four fully connected layers with 1024, 256, 8, 1 nodes respectively with a dropout ratio of 0.1. We use weighted cross-entropy loss as the loss function:\n\n\n    L=-1/m\u2211_i=1^m(wy_ilog(\u0177_i)+(1-y_i)log(1-\u0177_i)),\n\n\nwhere m is the number of training samples. w refers to the weight and is set to 4. Interaction site is labeled as 1 (y_i=1) and non-interaction site is labeled as 0 (y_i=0). \u0177_i is the predicted probability of being interaction site of the sample i. Besides, we utilize Adaptive Momentum (Adam) as the optimizer with a learning rate of 0.0001. The batch size is set to 256. The model is implemented by PyTorch and trained on NVIDIA GTX 1080 Ti.\n\nFine tuning is used in this model. Before training on Dset422 and Dset448, we first trained our model on Dset4392. After achieving the best performance on Dset4392, the parameters of the model are saved to files. When training on Dset422 and Dset448, the feature extraction part loaded the saved weights from the file and freeze the weights so that during the process of training, the parameters in feature extraction stayed unchanged. Training and validation data are used again only to train the fully connected layers in the prediction layer.\n\n\n\n \u00a7.\u00a7 Comparison with competing methods\n\n\nTo evaluate the predictive performance of our model (CoGANPPIS), we compare it with seven popular sequence-based competing methods (PSIVER, ISIS, SPRINGS, DELPHI, DeepPPISP, ACNN and ensnet_p). Specifically, PSIVER <cit.> utilizes Na\u00efve Bayes Classifier and kernel density estimation method to predict PPIs based on sequence features. ISIS <cit.> combines predicted structural features with evolutionary information to predict PPIs based on shallow neural networks. SPRINGS utilizes an artificial neural network to generate PPIs predictions <cit.>. DELPHI <cit.> employs a fine-tuned ensemble model by combining several recurrent neural networks and convolutional neural networks. DeepPPISP <cit.> considers both local contextual and global information and applies a convolutional neural network to predict PPIs. ACNN <cit.> employs a local attention mechanism to make PPIs prediction. And ensnet_p is an ensembled model combining different neural net models <cit.>. In words, all these traditional approaches do not apply coevolutionary information or global attention mechanism.\n\nTable <ref> presents the experimental results of seven sequence-based competitive PPIs prediction models and our proposed model. It can be observed that CoGANPPIS achieves the best performance across both two datasets in terms of all six metrics consistently, which ascertains its effectiveness. The ROC and PR curves of\nCoGANPPIS and other competing methods on Dset422 and Dset448 are shown in Figure <ref>. It demonstrates that AUPRC and AUC of CoGANPPIS are higher than that of other competing methods.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Ablation analysis\n\n\nIn this part, we test the usefulness of introducing coevolutionary information into PPIs prediction. First, we evaluate the performance of the model with coevolutionary information included. Then we remove coevolutionary information from the model and train the model again to measure the comparing performance. The model without coevolutionary information is named as CoGANPPIS^\u2296. Table <ref> demonstrates the performance of CoGANPPIS and \nCoGANPPIS^\u2296. We can find that CoGANPPIS outperforms CoGANPPIS^\u2296 on both two datasets, which indicates that introducing coevolutionary information could help improve predictive accuracy and thus validates its effectiveness in PPIs prediction.\n\n\n\n\n\n \u00a7.\u00a7 Model performance on proteins of different lengths\n\n\nConsidering that protein sequence length varies greatly from each other, it could be necessary to study the predictive performance on proteins of different length. To answer this question, we plot the experimental results of CoGANPPIS, CoGANPPIS^\u2296 as well as ensnet_p in terms of F_1 and MCC under proteins of different lengths on the two datasets in Figure <ref>.\n\n\n\nThrough the results, we have the following observations: First, we can observe that with the protein sequence length increasing, the performance of three models on two datasets all show an overall downward trend. This can be explained as the protein structure and function become more complex with the increase of length, making PPIs more difficult to be predicted. Second, it is interesting that the performance improvement of CoGANPPIS and CoGANPPIS^\u2296 compared with ensnet_p increases as the increase of the protein sequence length. Take the F_1 on Dset422 in Figure <ref> as an example, when the length is less than 100, the F_1 of CoGANPPIS and CoGANPPIS^\u2296 are 0.450 and 0.443 respectively, which are only 0.023 and 0.016 higher than that of ensnet_p (0.427). When the protein length is between 200 and 300, the improvements increase to 0.029 and 0.019 (0.414 vs 0.385 and 0.404 vs 0.385). When the protein length is greater than 500, the gaps further increase to 0.31 and 0.20 (0.402 vs 0.372 and 0.392 vs 0.372). This clearly shows that the longer the protein sequence is, the more PPIs prediction relies on global information extraction, which can be better captured by our global attention mechanism (even without coevolutionary information). Third, we pay attention to the comparison between CoGANPPIS and CoGANPPIS^\u2296. The two metrics F_1 and MCC of CoGANPPIS on both two datasets are better than the ones of CoGANPPIS^\u2296, which also verifies the effectiveness of coevolutionary information in PPIs prediction and confirms the conclusion that we obtained in ablation analysis.\n\n\n\n \u00a7.\u00a7 Impact of coevolution-enhanced global attention mechanism\n\n\nWe have verified the effectiveness of coevolutionary information and global attention mechanism. Now let's further study how the coevolution-enhanced global attention mechanism works. First, for each pair of residues, we examine the relationships between its direct coupling degree and labels. As shown in Figure <ref>, the larger the direct coupling degree, the higher the probability that the pairs of residues have the same label. Further, let's take protein Q2T3W4 in Dset448 as an example. We first extract the attention weights of the first residue on Q2T3W4 in the training process. Then we plot a scatter diagram and fit a trend line of the points with attention weights larger than 0.001 as shown in Figure <ref>. We find that the slope of the trend line is positive, which implies that in general, the larger the direct coupling degrees, the higher the attention weights. Hence, the target residue could be paid more attention to the residues with high correlation during training process, which is an noticeable advantage of this attention mechanism.\n\n\n\n\n\n \u00a7.\u00a7 Visualization\n\n\nAs mentioned above, the attention weights between two residues show a positive correlation with their direct coupling degrees. It prompts us to explore to which extent our attention mechanism enhances compared with allocating weights purely according to DCA. For a protein sequence with N amino acid residues, we sort residue pairs according to their attention weights and direct coupling degrees respectively, and keep the 5N, 10N as well as 20N highest-ranking pairs. Figure <ref> shows an example of the protein 7cei_A from Dset422. The three rows refer to the 5N, 10N and 20N situations in turn (Only the corresponding number of selected residue pairs are colored). The pairs with same labels are indicated in red and the pairs with different labels are shown in green. It is evident that our attention mechanism works consistently better than pure DCA because of the obviously higher proportion of red points in all three situations. To become more quantitative, we have binned the predicted pairs according to their separation along the protein sequence as shown in the third column in Figure <ref>. We observe that our attention mechanism captures the residues with same labels more accurately than pure DCA. Also, our attention mechanism could attach more attention weights to distant residues, whereas pure DCA tends to pay more attention to neighboring aggregated residues, which could be attributed to the consideration of the whole protein feature representation of our attention mechanism.\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nThe aim of this paper is to improve the PPIs prediction performance solely based on protein sequences, which is important for understanding the biological mechanism of proteins both experimentally and theoretically. A dozen of sequence-based PPIs predictors have been developed in recent years. However, most of these works just utilize some commonly used features without considering coevolutionary information which provides rich clues for inter-residue relationships. Also, they are not good at predicting PPIs of long-length proteins.\n\nHere, we propose a coevolution-enhanced global attention neural network (CoGANPPIS). Specifically, we employ a coevolution-enhanced global attention mechanism both for better inter-residue relationship capture and for better prediction of long-length proteins. We further aggregate the local residue features and apply a CNN & pooling layer to the coevolutionary information features as a supplement. Then we utilize several fully connected layers to generate the final prediction. Extensive experiments of CoGANPPIS and other seven popular methods on two standardized datasets show that our proposed model CoGANPPIS achieves the state-of-the-art performance.\n\nFurther experimental analysis shows that: (1) Coevolutionary information can improve the performance of PPIs prediction. (2) CoGANPPIS can bring more performance improvement compared with previous methods as the protein sequence becomes longer, implying that CoGANPPIS has a better understanding of the whole protein sequences. (3) Compared with allocating attention weights according to pure DCA, the proposed coevolution-enhanced global attention mechanism pays more attention to the residues with same labels and shows a more evenly distributed attention weights instead of local aggregated attention weights.\n\nAlthough CoGANPPIS shows advantages over previous methods, it has some limitations: First, CoGANPPIS takes a lot of computation time due to its usage of multiple sequence alignments and direct coupling analysis to generate coevolutionary information. In addition, its predictive accuracy depends on the number of homologs since the direct coupling analysis becomes inaccuract if the number is smaller than 1,000. In the future, we would be commited to find useful, practical and time-saving features to make prediction faster and more accurate.\n\n\n\n\u00a7 FUNDING\n\n\nThis study was supported by funding from the National Natural Science Foundation of China (72222009, 71991472 to X.Z), the National Natural Science Foundation of China (3210040426 to Z.H.), the Shanghai Rising-Star Program (21QB1400900 to Z.H.), and was also partly supported by a grant from the major project of Study on Pathogenesis and Epidemic Prevention Technology System (2021YFC2302500) by the Ministry of Science and Technology of China.\n\n\n\n\n"}