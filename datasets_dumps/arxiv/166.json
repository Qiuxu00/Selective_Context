{"entry_id": "http://arxiv.org/abs/2303.07127v2", "published": "20230313135803", "title": "Improving physics-informed neural networks with meta-learned optimization", "authors": ["Alex Bihlo"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.NA", "math.NA"], "text": "\n\nImproving physics-informed neural networks \n\nwith meta-learned optimization\n\n\n\nAlex Bihlo^\u2020\n\n\n\n^\u2020Department of Mathematics and Statistics, Memorial University of Newfoundland,\n\n^\u2020\u00a0St. John's (NL) A1C 5S7, Canada\n\n\n\n abihlo@mun.ca\n\n\n\n140mmWe show that the error achievable using physics-informed neural networks for solving differential equations can be substantially reduced when these networks are trained using meta-learned optimization methods rather than using fixed, hand-crafted optimizers as traditionally done. We choose a learnable optimization method based on a shallow multi-layer perceptron that is meta-trained for specific classes of differential equations. We illustrate meta-trained optimizers for several equations of practical relevance in mathematical physics, including the linear advection equation, Poisson's equation, the Korteweg\u2013de Vries equation and Burgers' equation. We also illustrate that meta-learned optimizers exhibit transfer learning abilities, in that a meta-trained optimizer on one differential equation can also be successfully deployed on another differential equation.\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nPhysics-informed neural networks are a class of methods for solving systems of differential equations. Originally proposed in the 1990s\u00a0<cit.> and popularized through the work\u00a0<cit.>, physics-informed neural networks have seen an immense raise in popularity in the past several years. This is in part due to the overall rise in interest in all things related to deep neural networks\u00a0<cit.>, but also due to some practical advantages of this method compared to traditional numerical approaches such as finite difference, finite elements or finite volume methods. These advantages include the evaluation of derivatives using automatic differentiation\u00a0<cit.>, their mesh-free nature and an overall ease of implementation through modern deep-learning frameworks such as  or . Given the expressive power of deep neural networks\u00a0<cit.>, neural networks are also a well-suited class of function approximation for the solution of systems of differential equations. \n\nA main downside of physics-informed neural networks is that a complicated optimization problem involving a rather involved composite loss function has to be solved\u00a0<cit.>. The difficulty in solving such so-called multi-task problems is well-documented in the deep learning literature, see e.g.\u00a0<cit.>. Moreover, since essentially all methods of optimization for deep neural network today are at most of first-order, such as stochastic gradient descent, and its momentum-based flavours such as Adam\u00a0<cit.>, the level of error that can typically be achieved with vanilla physics-informed neural networks as proposed in\u00a0<cit.> is often subpar compared to their traditional counterparts used in numerical analysis. While lower numerical error can be achieved using more involved strategies, such as domain decomposition approaches\u00a0<cit.>, modified loss functions\u00a0<cit.> or operator-based approaches\u00a0<cit.>, all of these approaches either sacrifice some of the simplicity of vanilla physics-informed neural networks or substantially increase their training times.\n\nSince a main culprit in of the overall unsatisfactory error levels achievable with vanilla physics-informed neural networks is the optimization method used, it is natural to aim to find better optimizers. More broadly, optimization is a topic extensively studied in the field of machine learning, with many new optimizers being proposed that aim to overcome some of the (performance or memory) shortcomings of the de-facto standard Adam, see e.g.\u00a0<cit.>. There has also been growing interest in the field of learnable optimization, referred to as learning to learn\u00a0<cit.>, which aims to develop optimization methods parameterized by neural networks, that are then meta-learned on a suitably narrow class of tasks, on which they typically outperform generic (non-learnable) optimization methods.\n\nThe aim of this paper is to explore the use of learnable optimization for training physics-informed neural networks. We show that meta-trained learnable optimizers with very few parameters can substantially outperform standard optimizer in this field. Moreover, once meta-trained, these optimizers can be used to train physics-informed neural networks with minimal computational overhead compared to traditional optimizers. \n\nThe further organization of this paper is as follows. In Section\u00a0<ref> we present a more formalized review on how neural networks can be used to solve differential equations. Section\u00a0<ref> presents a short overview of the relevant previous work on both physics-informed neural networks and learnable optimization. The main Section\u00a0<ref> introduces the class of learnable optimizers used in this work. Section\u00a0<ref> contains the numerical results obtained by using these meta-trained optimizers for solving a variety of differential equations using physics-informed neural networks. A summary with a discussion on further possible research directions can be found in the final Section\u00a0<ref>.\n\n\n\n\n\u00a7 SOLVING DIFFERENTIAL EQUATIONS WITH NEURAL NETWORKS\n\n\nThe numerical solution of differential equations with neural networks was first proposed in\u00a0<cit.>. In this algorithm, the trial solution is brought into a form that accounts for initial and/or boundary conditions (as hard constraints), with the actual solution being found upon minimizing the mean-squared error that is defined as the residual of the given differential equations evaluated over a finite number of collocation points which are distributed over the domain of the problem. This method was recently popularized by\u00a0<cit.>, coining the term physics-informed neural networks, and extended to also allow for the identification of differential equations from data. A recent review on this subject can be found in\u00a0<cit.>.\n\nMore formally, consider the following initial\u2013boundary value problem for a general system of L partial differential equations of order n,\n\n    \u0394^l(t,\ud835\udc31,\ud835\udc2e_(n)) = 0,      l=1,\u2026, L,   t\u2208[0,t_ f], \ud835\udc31\u2208\u03a9,\n       \ud835\udda8^l_ i(\ud835\udc31,\ud835\udc2e_(n_ i)|_t=0) = 0,     l_ i = 1,\u2026,L_ i,  \ud835\udc31\u2208\u03a9,\n       \ud835\udda1^l_ b(t,\ud835\udc31,\ud835\udc2e_(n_ b))=0,   l_ b=1,\u2026, L_ b,     t\u2208[0,t_ f], \ud835\udc31\u2208\u2202\u03a9,\n\nwhere t\u2208[0,t_ f]  is the time variable, \ud835\udc31=(x_1,\u2026,x_d)\u2208\u03a9 is the tuple of spatial independent variables, \ud835\udc2e=(u^1,\u2026, u^q) is the tuple of dependent variables, and \ud835\udc2e_(n) is the tuple of all derivatives of the dependent variables with respect to the independent variables of order not greater than n. The initial value operator is denoted by \ud835\udda8=(\ud835\udda8^1,\u2026\ud835\udda8^L_ i) and \ud835\udda1=(\ud835\udda1^1,\u2026,\ud835\udda1^L_ b) denotes the boundary value operator. The spatial domain is \u03a9 and the final time is t_ f.\n\nIn the following, we consider evolution equations for which the initial value operator reduces to\n\n    \ud835\udda8 = \ud835\udc2e(0,\ud835\udc31) - \ud835\udc1f(\ud835\udc31),\n\nwhere \ud835\udc1f(\ud835\udc31)=(f^1(\ud835\udc31),\u2026, f^q(\ud835\udc31)) is a fixed vector-valued function. We also consider Dirichlet boundary conditions of the form\n\n    \ud835\udda1 = \ud835\udc2e(t,\ud835\udc31) - \ud835\udc20(t,\ud835\udc31),\n\nwhere \ud835\udc20(t,\ud835\udc31)=(g^1(\ud835\udc2d,\ud835\udc31),\u2026, g^q(t,\ud835\udc31)) is another fixed vector-valued function.\n\nSolving system\u00a0(<ref>) with a neural network \ud835\udca9^\u03b8 requires the parameterization of the solution of this system in the form \ud835\udc2e^\u03b8 = \ud835\udca9^\u03b8(t,\ud835\udc31), where the weights \u03b8 of the neural network are found upon minimizing the loss function\n\n\n    \u2112(\u03b8) = \u2112_\u0394(\u03b8) + \u03b3_ i\u2112_ i(\u03b8) + \u03b3_ b\u2112_ b(\u03b8).\n\nHere\n\n    \u2112_\u0394(\u03b8)    = 1/N_\u0394\u2211_i=1^N_\u0394\u2211_l=1^L|\u0394^l(t^i_\u0394,\ud835\udc31^i_\u0394,\ud835\udc2e^\u03b8_(n)(t^i_\u0394,\ud835\udc31^i_\u0394))|^2,\n    \u2112_ i(\u03b8)    = 1/N_ i\u2211_i=1^N_ i\u2211_l_ i=1^L_ i|\ud835\udda8^l_ i(\ud835\udc31^i_ i,\ud835\udc2e_(n_ i)^\u03b8(0,\ud835\udc31^i_ i))|^2,\n    \u2112_ b(\u03b8)    = 1/N_ b\u2211_i=1^N_ b\u2211_l_ b=1^L_ b|\ud835\udda1^l_ i(t^i_ b,\ud835\udc31^i_ b,\ud835\udc2e_(n_ b)^\u03b8(t^i_ b,\ud835\udc31^i_ b))|^2,\n\n\nare the mean squared error losses corresponding to the differential equation, the initial condition  and the boundary value residuals, respectively, and \u03b3_ i and \u03b3_ b are positive scaling constants. These losses are evaluated over the collection of collocation points  {(t_\u0394^i,\ud835\udc31_\u0394^i)}_i=1^N_\u0394 for the system \u0394, {(0,\ud835\udc31_ i^i)}_i=1^N_ i for the initial data, and {(t_ b^i,\ud835\udc31_ b^i)}_i=1^N_ b for the boundary data, respectively. Upon successful minimization, the neural network \ud835\udca9^\u03b8 provides a numerical parameterization of the solution of the given initial\u2013boundary value problem.\n\n\n\n\u00a7 RELATED WORK\n\n\nPhysics-informed neural networks were proposed in\u00a0<cit.>, and popularized through the work\u00a0<cit.>, and have since been used extensively for solving differential equations in science and engineering. While the general algorithm for training neural networks to solve differential equations is straightforward, several complications arise in practice. Firstly, balancing the individual loss contributions in\u00a0(<ref>) so that all the initial values, the boundary values, and the differential equations are adequately enforced simultaneously constitutes a multi-task learning problem which may not be properly solved by minimizing the composite loss function\u00a0(<ref>), see\u00a0<cit.> for some work on multi-task learning problems. Secondly, it is well-known that training neural networks using gradient descent methods leads to a spectral bias in the form of low frequencies being learned first and high-frequencies requiring longer training times\u00a0<cit.>. Correspondingly, oscillatory solutions or stiff problems may not be accurately learned using standard physics-informed neural networks. Lastly, the general setup\u00a0(<ref>) requires proportionally more collocation points the larger the spatio-temporal domain of the differential equation being solved is. Training neural networks for solving differential equations over large spatio-temporal domains can destabilize training, which is frequently encountered in practice. In most cases, the solution for such problems is a trivial constant solution of the given differential equation\u00a0<cit.>. One straightforward solution for this problem is to break the entire domain into multiple sub-domains, and solve a sequence of smaller problems with multiple neural networks instead. This multi-model approach has recently been used for solving the shallow-water equations on a rotating sphere\u00a0<cit.>.\n\nLearnable optimization has been the topic of research since the works\u00a0<cit.>, with\u00a0<cit.> popularizing the use of neural network based learning to learn optimization. The latter paper specifically introduced an LSTM-type neural network optimizer that is being trained using gradient descent. Subsequent work focussed on improving the performance of learnable neural network based optimizers by improving their training strategies, see e.g.\u00a0<cit.>, improving the LSTM architecture of the optimizer\u00a0<cit.>, or replacing the LSTM-based architecture in favour of a simpler MLP-based one\u00a0<cit.>. Below, we will use the optimizer proposed in\u00a0<cit.>. For a more comprehensive review on learnable optimization consult the recent review paper\u00a0<cit.>.\n\nTo the best of our knowledge, the use of learnable optimization for physics-informed neural networks has not been pursued so far. The related field of using meta-learning to accelerating the training of physics-informed neural networks has been investigated in\u00a0<cit.> and\u00a0<cit.> recently. Specifically, in these works the authors used meta-learning to discover suitable initialization methods and physics-informed neural network loss functions that generalize across relevant task distributions, respectively, thereby speeding up training of individual physics-informed neural networks from these task distributions.\n\n\n\n\u00a7 META-LEARNABLE OPTIMIZATION FOR PHYSICS-INFORMED NEURAL NETWORKS\n\n\nA main goal of meta-learned optimization it to improve hand-designed optimization rules such as the Adam optimizer\u00a0<cit.> for updating the weight vector \u03b8 of a neural network with loss function\u00a0L(\u03b8). Recall that the Adam update rule is given by\n\n    \ud835\udc26_t = \u03b2_1 \ud835\udc26_t-1 + (1-\u03b2_1)\u2207_\u03b8L(\u03b8_t-1),  \ud835\udc2f_t = \u03b2_2 \ud835\udc2f_t-1 + (1-\u03b2_2)(\u2207_\u03b8L(\u03b8_t-1))^2,\n       \ud835\udc26\u0302_t = \ud835\udc26_t/(1-\u03b2_1^t),  \ud835\udc2f\u0302_t = \ud835\udc2f_t/(1-\u03b2_2^t),\n       \u03b8_t = \u03b8_t-1 - \u03b7\ud835\udc30_ adam = \u03b8_t - \u03b7\ud835\udc26\u0302_t/(\u221a(\ud835\udc2f\u0302_t) + \u03b5),\n\nwhere t=1,\u2026, is the optimization time step, \ud835\udc26 and \ud835\udc2f are the first and second moment vectors, with \u03b2_1,\u03b2_2\u2208[0,1) being the exponential decay rates for the moment estimates, \u03b5 being a regularization constant, and \u03b7 being the learning rate.\n\nSimilarly, the parameter updates of a meta-learned optimizer is structured as\n\n    \u03b8_t = \u03b8_t-1 - \ud835\udc1f(\ud835\udc33_t;\u03d1),\n\nwhere \ud835\udc1f is the parametric update function with \ud835\udc33_t referring to the input features of the learnable optimizer, and \u03d1 are the trainable meta-parameters of the optimizer, usually the weights of a neural network. To allow for the learnable optimizer to be transferable to neural networks of different sizes it is customary to have the parameter update rule\u00a0(<ref>) act component-wise, with each weight \u03b8_i of the weight vector \u03b8 being updated in the same way. Thus, in the following we describe the parameteric update formula in terms of scalar variables, rather than vector variables.\n\nWhile there are several optimizer architectures that have been proposed in the literature\u00a0<cit.>, here we use a relatively simple multi-layer perceptron for the optimizer architecture. Notably, we follow the work\u00a0<cit.> and structure the parametric update formula for each weight \u03b8_i as\n\n    f = \u03bb_1exp(\u03bb_2 s^ adam_\u03d1)) w_ adam + \u03bb_3/\u221a(v_t)+\u03b5d^ bb_\u03d1exp(\u03bb_4 s^ bb_\u03d1),\n\nwhere \u03bb_i, i=1,\u2026,4 are positive constants, w_ adam corresponds to the Adam update step and s^ adam_\u03d1, s^ bb_\u03d1 and d^ bb_\u03d1 are to the output heads of the meta-learned optimizer with neural network weights \u03d1. \n\nOn a high level, the first term in the learnable update formula\u00a0(<ref>) can be seen as a nominal term derived from the Adam update formula with scalable learning rate \u03bb_1exp(\u03bb_2 s^ adam_\u03d1)), which guarantees an update step in a descent direction, and the second term corresponds to a blackbox update term structured as the product of a directional and magnitudinal term, d^ bb_\u03d1 and exp(\u03bb_4 s^ bb_\u03d1), respectively, with the denominator \u221a(v_t)+\u03b5 acting as a preconditioner that should guarantee that the overall update formula leads corresponds to a descending on the loss surface. For more details on the rationale behind the update rule\u00a0(<ref>), consult\u00a0<cit.>.\n \nThe inputs \ud835\udc33_t at optimization step t to the multi-layer perceptron optimizer with output heads s^ adam_\u03d1, s^ bb_\u03d1 and d^ bb_\u03d1 are chosen as follows:\n=0ex\n\n  * The weights \u03b8_t;\n\n  * The gradients \u2207_\u03b8L(\u03b8_t);\n\n  * The second momentum accumulators \ud835\udc2f_t with decay rates \u03b2_2\u2208{0.5, 0.9, 0.99, 0.999};\n\n  * One over the square root of the above four second momentum accumulators;\n\n  * The time step t.\n\nHere, we build upon the extensive study carried out in\u00a0<cit.>, with the above input parameters heuristically being found to perform well for the physics-informed neural networks that were trained in this work.\n\nAll input features (except the time step) were normalized to have a second moment of one. The time step is converted into a total of 11 features by computing tanh(t/x) where x\u2208{1,3,10,30,100,300,1000,3000,10k,30k,100k}. All features were then concatenated and passed through a standard multi-layer perceptron to yield the above three output heads.\n\n\n\n\u00a7 NUMERICAL RESULTS\n\n\nIn this section we showcase the use of meta-learned optimization for solving some well-known differential equations from mathematical physics, that have been extensively studied using physics-informed neural networks. In all of the following examples we use the vanilla version of physics-informed neural networks as laid out in\u00a0<cit.>. As discussed in Section\u00a0<ref>, it is well-understood by now that this formulation can suffer from several drawbacks which to remedy is currently an active research field. As such, the goal of this section is not to obtain the best possible numerical solution for each given model, but to show how meta-learned optimization can improve the results obtainable using vanilla physics-informed neural network when compared to using standard optimization. Our base optimizer we compare against is the Adam optimizer, the de-facto standard being used in the field of physics-informed neural networks today. \n\nIn all examples below, the output heads of the meta-learned optimizer were initialized using a normal distribution with zero mean and variance of 10^-3, to guarantee that the neural network output is close to zero at the beginning of meta-training of the optimizer. Due to the form of the meta-learned optimizer\u00a0(<ref>), this means that before meta-training starts, the meta-learned optimizer is very close to the standard Adam optimizer. \n\nFor all examples, the multi-layer perceptron being used for the meta-learned optimizer has two hidden layers with 32 units each, using the swish activation function. This architecture was found using hyperparameter tuning to give a good balance between computational overhead of meta-training the optimizer and error level of the resulting optimizer. We should like to note here that in contrast to the application of meta-learned optimization in areas of modern deep learning, such as computer vision or natural language processing, which work with neural networks with up to hundreds of hidden layers and billions of weights, the neural networks arising in physics-informed neural networks are typically relatively small. In fact, all of the architectures considered in this paper have less than 10,000 trainable parameters. This allows for larger neural networks being used for the meta-learned optimizer, without incurring computationally infeasible costs. Still, the underlying multi-layer perceptron of the meta-learned optimizer is relatively small, having only 2,115 trainable parameters.\n\nWe train this optimizer using the persistent evolutionary strategy, a zeroth-order stochastic optimization method described in\u00a0<cit.>. This algorithm has several hyperparameters, including the total number of particles N used for gradient computation, the partial unroll length K of the inner optimization problem before a meta-gradient update is computed, the standard deviation of perturbations \u03c3 and the learning rate \u03b1 for the meta-learned weight update. Using hyperparamter tuning, we determined N=2 (antithetic) particles, K=1 epochs and a learning rate of \u03b1=10^-4 to be the best hyperparameters for our problem. For more details, see Algorithm\u00a02 in\u00a0<cit.>. \n\nFor each problem, unless otherwise specified, we then sample a total of 20 different tasks and train the meta-learned optimizer for a total of 50 epochs on the associated tasks. Each task corresponds to a new instantiation of the particular neural network architecture for the same equation parameters, meaning the only difference in each task are the initial (random) weights of the neural network model. We found empirically that training the meta-learned optimizer for relatively few epochs (50 epochs compared to using the learned optimizer for more than 1000 epochs at testing stage) provided a good balance between performance and meta-training cost. To guarantee a fair comparison, at testing time the initial weights of the two neural networks being trained with the respective optimizers are exactly the same. \n\nIn Table\u00a0<ref> we summarize the parameters of the physics-informed neural networks trained in this section. We use hyperbolic tangents as activation function for all hidden layers. We use mini-batch gradient computation with a total of 10 batches per epoch.\n\n\n\nWe report both the time series of the loss for the standard Adam optimizer and the meta-learned optimizer, and the error e=u_ nn - u_ ref, where u_ ref is either the analytical solution (if available), or a high-resolution numerical reference solution obtained from using a pseudo-spectral method for the spatial discretization and an adaptive Runge\u2013Kutta method for time stepping using the method of lines approach\u00a0<cit.>.\n\nThe algorithm described here has been implemented using  2.11 and the codes will be made available on GitHub[<https://github.com/abihlo/LearnableOptimizationPinns>].\n\n\n\n\n \u00a7.\u00a7 One-dimensional linear advection equation\n\n\nAs a first example, consider the one-dimensional linear advection equation\n\n    u_t + cu_x = 0,\n\nwhere we consider t\u2208[0,3] and x\u2208[-1,1] with c=1 being the advection velocity. We set u(0,x)=u_0(x)=cos\u03c0 x and use periodic boundary conditions. We enforce the periodic boundary conditions as hard constraint in the physics-informed neural networks, using the strategy introduced in\u00a0<cit.>. We set \u03b3_ i=1 in the loss function\u00a0(<ref>). The learning rate for Adam was \u03b7=10^-3. The constants of the learnable optimizer were all chosen as \u03bb_i=10^-3, i=1,\u2026,4.\n\n\n\n\n\nThe numerical results for this example are depicted in Figures\u00a0<ref> and\u00a0<ref>. For this particular example, the meta-learned optimizer considerably outperforms the standard Adam optimizer, resulting in a training loss and point-wise error that is more than 10 times smaller. The loss for the physics-informed neural network using the meta-learned optimizer after 500 epochs is lower than the final loss after 3000 epochs for the respective network using Adam.\n\n\n\n \u00a7.\u00a7 Poisson equation\n\n\nAs an example for a boundary-value problem, consider the two-dimensional Poisson equation\n\n    u_xx + u_yy = f(x,y),\n\nover the domain \u03a9=[-1,1]\u00d7[-1,1] for the exact solution\n\n    u_ exact(x,y) = (0.1sin 2\u03c0 x + tanh 10 x)sin2\u03c0 y,\n\nwith the associated right-hand side using Dirichlet boundary conditions. This problem was considered in\u00a0<cit.>. Since this is a boundary value problem, there is no initial loss in the loss function\u00a0(<ref>) and we use \u03b3_ b= 1000. This value was chosen heuristically to balance the differential equation and boundary value losses. The learning rate for Adam for this example was set to \u03b7=10^-3 and so were the constants of the meta-learned optimizer, \u03bb_i=10^-3, i=1,\u2026,4.\n\n\n\n\n\nThe training loss for this example is shown in Fig.\u00a0<ref>, the numerical results as compared to the exact solution with the associated point-wise error are depicted in Fig.\u00a0<ref>. As with the linear advection equation from the previous example, also for the Poisson equation the meta-learned optimization method leads to better results both in terms of a lower training loss and smaller point-wise errors compared to the standard Adam optimizer.\n\n\n\n\n \u00a7.\u00a7 Korteweg\u2013de Vries equation\n\n\nWe next consider the Korteweg\u2013de Vries equation\n\n    u_t + uu_x - \u03bd u_xxx = 0,\n\nwith initial condition u(0,x) = -sin\u03c0 x using periodic boundary conditions over the domain x\u2208[-1,1] and t\u2208[0,1], setting \u03bd=0.0025. This equation has been extensively studied using physics-informed neural networks, see e.g.\u00a0<cit.>. Again, we enforce the periodic boundary conditions as hard constraint and set \u03b3_ i=1 in the loss function\u00a0(<ref>). The learning rate of the Adam optimizer was chosen as \u03b7=5\u00b7 10^-4, and the constants of the meta-learned optimizer were set to \u03bb_1 = 5\u00b7 10^-4 and \u03bb_i=10^-3, i=2,\u2026,4.\n\n\n\n\n\nFigure\u00a0<ref> contains the respective training losses of the Adam and meta-learned optimizers. The numerical solutions for the associated trained physics-informed neural networks as compared against the numerical solution obtained from a pseudo-spectral numerical integration method are featured in Figure\u00a0<ref>. These plots again illustrate that the meta-learned optimizer reduces the training loss considerably faster than the standard Adam optimizer, which also improves upon the point-wise error of the numerical solution compared to the reference solution. In fact, the training loss after 200 epochs is lower for the meta-learned optimizer than what the Adam optimizer achieves at the end of training.\n\nWe next consider a problem of transfer learning for meta-learned optimizers for the Korteweg\u2013de Vries equation. A common task in the numerical solution of differential equation is to change the initial condition of the problem. For physics-informed neural networks this requires re-training of the network, which is computationally costly. To investigate this problem, we sample our task distribution for meta-training the optimizer from an ensemble of initial conditions here. For the sake of simplicity we consider initial conditions of the form\n\n    u(0,x) = cos (kx+\u03d5),\n\nwhere k is sampled from integers between 1 and 3 and \u03d5 is sampled uniformly from [-\u03c0/2,\u03c0/2]. We choose a relatively narrow task distribution to speed up meta-learning. Once trained, we evaluate the optimizer on the unseen test problem with k=2 and \u03d5=-\u03c0/4. Since this is a harder problem than using the meta-learned optimizer on the same problem (i.e. same initial condition and same differential equation), we meta-train the optimizer on a total of 75 tasks here instead of the 20 tasks used so far.\n\n\n\n\n\nThe results of this experiment are depicted in Figures\u00a0<ref> and\u00a0<ref>. These figures again show improvement of the meta-learned optimizer when compared to the results obtained using Adam. This demonstrates that transfer learning across the same equation class, i.e. choosing different initial values but keeping the equation the same, is indeed feasible. Moreover, the loss level achieved after 200 epochs using the meta-learned optimizer is comparable to the loss level obtained using the Adam optimizer after 1000 epochs, again pointing to the possibility of significant speed-up in training physics-informed neural networks. In the next example we show that transfer learning across different equation classes is possible as well.\n\n\n\n \u00a7.\u00a7 Burgers' equation\n\n\nAs a last example we consider Burgers' equation\n\n    u_t + uu_x -\u03bd u_xx = 0,\n\nover the temporal-spatial domain [0,1]\u00d7[-1,1] with initial condition u(0,x) = -sin\u03c0 x and periodic boundary conditions in x-direction. The diffusion parameter was set as \u03bd=0.01/\u03c0. Burgers equation is also one of the most prominent examples considered using physics-informed neural networks, see\u00a0<cit.> for some results.\n\nAs for the Korteweg\u2013de Vries equation, we enforce the periodic boundary conditions as hard constraints, use \u03b3_ i=1 in the loss function\u00a0(<ref>), and set the learning rate of the Adam optimizer to \u03b7=5\u00b7 10^-4, and the constants of the meta-learned optimizer to \u03bb_1 = 5\u00b7 10^-4 and \u03bb_i=10^-3, i=2,\u2026,4.\n\nHere we consider two meta-learned optimizers. The first is being trained as for the previous example, i.e. using Burgers' equation on 20 tasks, which each task being a newly instantiated neural network with different random initial weights. The second one is being meta-trained using the linear advection equation. This second optimizer should assess the transfer learning abilities of meta-trained optimizers across different differential equations. For this optimizer, we choose our tasks for varying advection velocities sampled uniformly from c\u2208[-1,1]. At testing time, this optimizer meta-trained on the linear advection equation is also used to train a physics-informed neural network for Burgers' equation.\n\n\n\n\n\nFigures\u00a0<ref> and\u00a0<ref> contain the associated numerical results for this example, showing the training loss of the three respective optimizers and the actual numerical results for solving Burgers' equation using the trained neural networks. Figures\u00a0<ref> illustrates that the meta-learned optimizer trained using the linear advection equation also outperforms the Adam optimizer. Interestingly, this optimizer also outperforms the meta-learned optimizer trained on Burgers' equation over the first 400 epochs, although exhibiting substantially higher oscillations than the latter. At the end of the training, the loss for the linear advection trained meta-learned optimizer is still a bit lower than Adam, although the loss seems to have stagnated after about 600 epochs of training. \n\nThe loss levels are also consistent with the numerical results shown in Fig.\u00a0<ref>, illustrating that the meta-learned optimizer using Burgers' equation is the best with the other two optimizers yielding comparable errors. Still, these results show the transfer learning abilities of meta-learned optimizers across different differential equations, which could be leveraged in a multitude of ways. For the particular example of Burgers' equation, the meta-learned optimizer trained on the linear advection equation could be used for the first few hundred epochs, before being chained with another optimizer more suitable for longer training. It is also conceivable that more extensive meta-training, either using more tasks sampled from a wider task distribution, or from wider classes of differential equations altogether, could give optimizers that are applicable to more than a single class of differential equations.\n\n\n\n\u00a7 CONCLUSION\n\n\nWe have investigated the use of meta-learned optimization for improving the training of physics-informed neural networks in this work. Meta-learned optimization, or learning to learn, has become an increasingly popular topic in deep learning and thus it is natural to investigate its applicability in scientific machine learning as well. We have done so here by illustrating that meta-learned optimization can be used to improve the numerical results obtainable using physics-informed neural networks, which is a popular machine learning-based method for solving differential equations. We have also provided proof-of-concept that these meta-learned optimizers have transfer learning capabilities, i.e. that they can be used for problems that are different from those they were trained on.\n\nThe goal of this paper was to illustrate that meta-learned optimization alone can substantially improve the vanilla form of physics-informed neural networks, which was laid out in the seminal works\u00a0<cit.>. This form has been extensively studied, and we have shown here that meta-learned optimization can give (sometimes substantially) better numerical results compared to standard hand-crafted optimization rules. This means that meta-learned optimizers are able to reach a particular error level quicker than standard optimizers, resulting in either shorter training times (for a given target computational error) or better numerical accuracy (for the same number of training epochs). \n\nThere are several avenues for future research that would provide natural extensions to the present work. Firstly, one could investigate the use of meta-learned optimization for other formulations of physics-informed neural networks. We have refrained from doing so here, as there is not one canonical formulation of improved training strategies for physics-informed neural networks but rather a zoo of methods that are applicable to different classes of differential equations. This list of methods includes, to name a few, variational formulations\u00a0<cit.>, formulations based on domain decompositions\u00a0<cit.>, formulations based on improved loss functions\u00a0<cit.>, re-sampling strategies\u00a0<cit.>, and operator-based formulations\u00a0<cit.>. It should also be stressed that while these formulations can considerably outperform vanilla physics-informed networks, the latter are still extensively being used in the literature today, see\u00a0<cit.> for a recent review. \n\nSecondly, there is a multitude of other meta-learned optimization algorithms based on neural networks that have been proposed in the literature, see the review paper\u00a0<cit.> for an extensive list of such optimizers. There are also several training strategies available for meta-learned optimization, including gradient descent, evolutionary strategies and reinforcement learning based ones\u00a0<cit.>. \n\nTogether, this provides a rich set of training strategies, meta-learnable optimizer architectures and physics-informed model formulations that could be explored together to possibly find more accurate solutions of differential equations using physics-informed neural networks. We plan to explore some of these possibilities in the near future. \n\n\n\n\u00a7 ACKNOWLEDGEMENTS\n\n\nThis research was undertaken thanks to funding from the Canada Research Chairs program and the NSERC Discovery Grant program.\n\n\n\n10\n\n\nurlstyle\n\nandr16a\nAndrychowicz M., Denil M., Gomez S., Hoffman M.W., Pfau D., Schaul T.,\n  Shillingford B. and De\u00a0Freitas N., Learning to learn by gradient descent by\n  gradient descent, Advances in neural information processing systems\n  29 (2016).\n\nbayd18a\nBaydin A.G., Pearlmutter B.A., Radul A.A. and Siskind J.M., Automatic\n  differentiation in machine learning: a survey, J.\u00a0Mach. Learn. Res.\n  18 (2018), Paper No.\u00a0153.\n\nbeng95a\nBengio S., Bengio Y. and Cloutier J., On the search for new learning rules for\n  ANNs, Neural Process. Lett. 2 (1995), 26\u201330.\n\nbeng90a\nBengio Y., Bengio S. and Cloutier J., Learning a synaptic learning rule,\n  Universit\u00e9 de Montr\u00e9al, 1990.\n\nbihl22a\nBihlo A. and Popovych R.O., Physics-informed neural networks for the\n  shallow-water equations on the sphere, J. of Comput. Phys.\n  456 (2022), 111024.\n\nchen22a\nChen T., Chen X., Chen W., Heaton H., Liu J., Wang Z. and Yin W., Learning to\n  optimize: A primer and a benchmark, J. Mach. Learn. Res. 23\n  (2022), 1\u201359.\n\ncuom22a\nCuomo S., Di\u00a0Cola V.S., Giampaolo F., Rozza G., Raissi M. and Piccialli F.,\n  Scientific machine learning through physics\u2013informed neural networks: where\n  we are and what's next, J. Sci. Comput. 92 (2022), 88.\n\ncybe89a\nCybenko G., Approximation by superpositions of a sigmoidal function,\n  Math. Control Signals Syst. 2 (1989), 303\u2013314.\n\ndurr10a\nDurran D.R., Numerical methods for fluid dynamics: With applications to\n  geophysics, vol.\u00a032, Springer Science & Business Media, 2010.\n\nharr22a\nHarrison J., Metz L. and Sohl-Dickstein J., A closer look at learned\n  optimization: Stability, robustness, and inductive biases, arXiv\n  preprint arXiv:2209.11208  (2022).\n\njagt20a\nJagtap A.D., Kharazmi E. and Karniadakis G.E., Conservative physics-informed\n  neural networks on discrete domains for conservation laws: applications to\n  forward and inverse problems, Comput. Methods Appl. Mech. Eng.\n  365 (2020), 113028.\n\njin21a\nJin X., Cai S., Li H. and Karniadakis G.E., NSFnets (Navier\u2013Stokes flow\n  nets): Physics-informed neural networks for the incompressible\n  Navier\u2013Stokes equations, J.\u00a0Comput. Phys. 426 (2021),\n  109951.\n\nkhab09Ay\nKhabirov S.V., A property of the determining equations for an algebra in the\n  group classification problem for wave equations, Sibirsk. Mat. Zh.\n  50 (2009), 647\u2013668, in Russian; translation in Sib. Math. J.,\n  50:515\u2013532, 2009.\n\nkhar21a\nKharazmi E., Zhang Z. and Karniadakis G.E., hp-VPINNs: Variational\n  physics-informed neural networks with domain decomposition, Comput.\n  Methods Appl. Mech. Eng. 374 (2021), 113547.\n\nking14a\nKingma D.P. and Ba J., Adam: A method for stochastic optimization,\n  arXiv:1412.6980, 2014.\n\nlaga98a\nLagaris I.E., Likas A. and Fotiadis D.I., Artificial neural networks for\n  solving ordinary and partial differential equations, IEEE Trans. Neural\n  Netw. 9 (1998), 987\u20131000.\n\nlecu15a\nLeCun Y., Bengio Y. and Hinton G., Deep learning, Nature 521\n  (2015), 436\u2013444.\n\nliu22a\nLiu X., Zhang X., Peng W., Zhou W. and Yao W., A novel meta-learning\n  initialization method for physics-informed neural networks, Neural.\n  Comput. Appl. 34 (2022), 14511\u201314534.\n\nluca18a\nLucas J., Sun S., Zemel R. and Grosse R., Aggregated momentum: Stability\n  through passive damping, in International Conference on Learning\n  Representations, 2019.\n<https://openreview.net/forum?id=Syxt5oC5YQ>\n\nlv17a\nLv K., Jiang S. and Li J., Learning gradient descent: Better generalization\n  and longer horizons, in International Conference on Machine Learning,\n  PMLR, 2017, pp. 2247\u20132255.\n\nmetz22a\nMetz L., Freeman C.D., Harrison J., Maheswaranathan N. and Sohl-Dickstein J.,\n  Practical tradeoffs between memory, compute, and performance in learned\n  optimizers, in Conference on Lifelong Learning Agents, PMLR, 2022, pp.\n  142\u2013164.\n\npenw23a\nPenwarden M., Jagtap A.D., Zhe S., Karniadakis G.E. and Kirby R.M., A unified\n  scalable framework for causal sweeping strategies for physics-informed neural\n  networks (PINNs) and their temporal decompositions, arXiv preprint\n  arXiv:2302.14227  (2023).\n\npsar22a\nPsaros A.F., Kawaguchi K. and Karniadakis G.E., Meta-learning PINN loss\n  functions, J. Comput. Phys. 458 (2022), 111121.\n\nraha19a\nRahaman N., Baratin A., Arpit D., Draxler F., Lin M., Hamprecht F., Bengio Y.\n  and Courville A., On the spectral bias of neural networks, in\n  Proceedings of the 36th International Conference on Machine Learning,\n  PMLR, 2019, pp. 5301\u20135310.\n\nrais19a\nRaissi M., Perdikaris P. and Karniadakis G.E., Physics-informed neural\n  networks: A deep learning framework for solving forward and inverse problems\n  involving nonlinear partial differential equations, J. Comput. Phys.\n  378 (2019), 686\u2013707.\n\nsene18a\nSener O. and Koltun V., Multi-task learning as multi-objective optimization, in\n  Proceedings of the 32nd International Conference on Neural Information\n  Processing Systems, 2018, pp. 525\u2013536, arXiv:1810.04650.\n\nshaz18a\nShazeer N. and Stern M., Adafactor: Adaptive learning rates with sublinear\n  memory cost, in International Conference on Machine Learning, PMLR,\n  2018, pp. 4596\u20134604.\n\nvico21a\nVicol P., Metz L. and Sohl-Dickstein J., Unbiased gradient estimation in\n  unrolled computation graphs with persistent evolution strategies, in\n  International Conference on Machine Learning, PMLR, 2021, pp.\n  10553\u201310563.\n\nwang23a\nWang S. and Perdikaris P., Long-time integration of parametric evolution\n  equations with physics-informed deeponets, J.Comput. Phys.\n  475 (2023), 111855.\n\nwang22a\nWang S., Sankaran S. and Perdikaris P., Respecting causality is all you need\n  for training physics-informed neural networks, arXiv preprint\n  arXiv:2203.07404  (2022).\n\nwich17a\nWichrowska O., Maheswaranathan N., Hoffman M.W., Colmenarejo S.G., Denil M.,\n  Freitas N. and Sohl-Dickstein J., Learned optimizers that scale and\n  generalize, in International conference on machine learning, PMLR,\n  2017, pp. 3751\u20133760.\n\nwu2023a\nWu C., Zhu M., Tan Q., Kartha Y. and Lu L., A comprehensive study of\n  non-adaptive and residual-based adaptive sampling for physics-informed neural\n  networks, Comput. Methods Appl. Mech. Eng. 403 (2023),\n  115671.\n\nyu20a\nYu T., Kumar S., Gupta A., Levine S., Hausman K. and Finn C., Gradient surgery\n  for multi-task learning, in Advances in Neural Information Processing\n  Systems, vol.\u00a033, Curran Associates, 2020, pp. 5824\u20135836, arXiv:2001.06782.\n\n\n\n\n"}