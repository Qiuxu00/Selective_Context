{"entry_id": "http://arxiv.org/abs/2303.07874v1", "published": "20230313130702", "title": "Bayes Complexity of Learners vs Overfitting", "authors": ["Grzegorz G\u0142uch", "Rudiger Urbanke"], "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n    \n       \n            \n       \n       \n       \n      \n         \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nobservationObservation\n\n\nclaimClaim\n\nfactFact\n\nassumptionAssumption\nnoteNote\nlieLie theory derivation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n \n \n \n \n \n \n \n \n \n \n\n\n\n\n \n \n \n\n \n \n \n \n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmycomment\n\n\n\n\n\n\n\n\n\n\n\n\nM>X<L>r<Bayes Complexity of Learners vs Overfitting]Bayes Complexity of Learners vs OverfittingGrzegorz G\u0142uchgrzegorz.gluch@epfl.ch\nRuediger Urbankeruediger.urbanke@epfl.ch\n EPFL, Lausanne, Switzerland[\n    Hiroshi C. Watanabe\n    March 30, 2023\n=======================\n\n\n\n\nWe introduce a new notion of complexity\nof functions and we show that it has the following properties: (i) it governs a PAC Bayes-like generalization bound, (ii) for neural networks it relates to natural notions of complexity of functions (such as the variation), and (iii) it explains the generalization gap between neural networks\nand linear schemes. While there is a large set of papers which describes bounds that \nhave each such property in isolation, and even some that have two, as far as we know, this is a first notion that satisfies all three of them. Moreover, in contrast to previous works, our notion naturally generalizes to neural networks with several layers. \n\nEven though the computation of our complexity is nontrivial in general, an upper-bound is often easy to derive, even for higher number of layers and functions with structure, such as period functions. \nAn upper-bound we derive allows to show a separation in the number of samples needed for good generalization between 2 and 4-layer neural networks for periodic functions. \n\n\n\n\n\n\u00a7 INTRODUCTION\n\nThere is a large body of literature devoted to the question of generalization, both from a practical point of view as well as concerning our theoretical understanding, e.g., <cit.>, to mention just a few. We add to this discussion. In particular, we ask what role is played by the hypothesis class assuming a Bayesian point of view. Our main observation is that there is a striking theoretical difference between linear schemes and neural networks. In a nutshell, neural networks, when trained with appropriate gradient methods using a modest amount of training data, strongly prefer hypothesis that are \u201ceasy\u201d to represent in the sense that there is a large parameter space that approximately represents this hypothesis. For linear schemes no such preference exists. This leads us to a notion of a complexity of a function with respect to a given hypothesis class and prior. We then show that (i) this complexity is the main component in a standard PAC-Bayes bound, and (ii) that the ordering implied by this complexity corresponds well to \u201cnatural\u201d notions of complexity of functions that have previously been discussed in the literature. In words, neural networks learn \u201csimple\u201d functions and hence do not tend to overfit.\n\nFor n \u2208 we define [n] = {1,\u2026,n}. Let  be the input space,  be the output space and := \u00d7 be the sample space. Let \u210b_\u03b8 be the hypothesis class, parameterized by \u03b8\u2208\u211d^m. We define the loss as a function \u2113: \u210b\u00d7\u2192_+. We focus on the clipped to C version of the quadratic loss but our results can be generalized to other loss functions. We denote by _x a distribution on the input space , and by  a distribution on the sample space . Finally, we let ={z_1, \u22ef z_N} be the given sample set, where we assume that the individual samples are chosen iid according to the distribution \ud835\udc9f.\n\n\n\n \u00a7.\u00a7 The PAC Bayes Bound\n\nOur starting point is a version of the well-known PAC-Bayes bound, see <cit.>.\n\nLet the loss function \u2113 be bounded, i.e., \u2113: \u210b\u00d7\ud835\udcb5\u2192 [0, C]. \nLet P be a prior on \u210b and Q be any other distribution on \u210b (possibly dependent on ). Then \n\n    _[L_\ud835\udc9f(Q)]  \u2264_[ L_(Q) + C\u221a(D(Q  P)/2N)],\n\nwhere\n\n    L_\ud835\udc9f(Q)     = _z \u223c\ud835\udc9f; h \u223c Q[\u2113(h, z)],\n\n    L_(Q)     = _ h \u223c Q[1/N\u2211_n=1^N\u2113(h, z_i)],\n\nand the divergence D(Q  P) is defined as\n\n    D(Q P) = \u222b Q logQ/P.\n\nThere is a large body of literature that discusses use cases, interpretations, and extensions of this bound. Let us just mention a few closely related works.\n\nA related prior notion is that of flat minima. These are minimizers in the parameter space that are surrounded by many functions with similarly small empirical error. The reason for this connection is straightforward. In order for Q to give a good bound two properties have to be fullfilled: (i) Q must be fairly broad so that D(Q  P) is not too large (afterall, P must be broad since we do not know the function a priori), and (ii) Q must give rise to a low expected empirical error. These properties are exactly the characteristics one expects from a flat minimum. The importance of such minima was recognized early on, see e.g., <cit.> and <cit.>. More recently <cit.> and <cit.> derive from this insight an algorithm for training discrete neural networks that explicitly drives the local search towards non-isolated solution. Using a Bayesian approach they argue that these minima have good generalization. Building on these ideas <cit.> give an algorithm with the aim to directly optimize (<ref>). They demonstrate empirically that the distributions Q's they find give non-vacuous generalization bounds. \n\nTo summarize, the bound (<ref>) can be used in various ways. In the simplest case, given a prior P and an algorithm that produces a \u201cposterior\u201dQ, (<ref>) gives a probabilistic upper bound on the average true risk if we sample the hypothesis according to Q. But (<ref>) can also be taken as the starting point of an optimization problem. Given a prior distribution P one can in principle look for the posterior Q that gives the best such bound. Further, one can split the available data and use one part to find a suitable prior P and the remaining part to define a that posterior Q distribution that minimizes this bound.  \n\nWe take the PAC-Bayes bound as our starting point. We impose a Gaussian distribution on the weights of the model. This defines our prior P. In principle other priors can be used for our approach but a Gaussian is the most natural choice and it illustrates the main point of the paper in the cleanest fashion. Further, we postulate that the samples z_n=(x_n, y_n) are iid and, assuming that the true parameter is \u03b8, come from the stochastic model\n\n\n    x_n \u21a6 y_n = f_\u03b8(x_n) + \u03b7_n, \u03b7_n \u223c\ud835\udca9(0,\u03c3_e^2).\n\nIn words, we assume that the actual underlying function is  realizable, that we receive noisy samples, and that the noise is Gaussian and independent from sample to sample.  \n\nThis gives rise to the posterior distribution,\n\n    Q(\u03b8) = P(\u03b8) e^- 1/2 \u03c3_y^2\u2211_n=1^N (y_n - f_\u03b8(x_n))^2/\u222b P(\u03b8') e^- 1/2 \u03c3_y^2\u2211_n=1^N (y_n - f_\u03b8'(x_n))^2 d \u03b8'.\n\nOne valid criticism of this approach is that it is model dependent. But there is a significant payoff. First recall that this posterior can at least in principle be sampled by running the SG algorithm with Langevin dynamics. For the convenience of the reader we include in Section\u00a0<ref> a short review. Most importantly, taking this point of view a fairly clear picture arises why neural networks tend not to overfit. In a nutshell, if we sample from this posterior distribution then we are more likely to sample \u201csimple\u201d functions. The same framework also shows that this is not the case for linear schemes.\n \n\n\n \u00a7.\u00a7 Stochastic Gradient Langevin Dynamics\n\nWe follow <cit.>. Assume that we are given the data set\n\n    = {z_1, \u22ef, z_N} = {(x_1, y_1), \u22ef, (x_N, y_N)},\n\nwhere the samples z_n=(x_n, y_n), n=1, \u22ef, N, are chosen iid according to an unknown distribution . We model the relationship between x and y probabilistically in the parametrized form\n\n    y \u223c p(y | x, \u03b8).\n\nWe use the log-loss \n\n    _\u03b8(x, y) = - ln p(y | x, \u03b8).\n\nAssume further that we use the  stochastic gradient Langevin descent (SGLD) algorithm:\n\n    \u03b8^(t)   = \u03b8^(t-1) - \u03b7_Z \u223c[\u2207_\u03b8_\u03b8(X, Y) - 1/Nln P(\u03b8) ] \n       + \u221a(2 \u03b7/N)\ud835\udca9(0, I),\n\nwhere t = 1, 2, \u22ef;  \u03b7>0 is the learning rate, P(\u03b8) is the density of the prior, and \ud835\udca9(0, I) denotes a zero-mean Gaussian vector of dimension dim(\u03b8) with iid components and variance 1 in each component.\n\n\n\n\n\nNote that due to the injected noise, the distribution of \u03b8 at time \u03c4, call it \u03c0_\u03c4(\u03b8), converges to the posterior distribution of \u03b8 given the data, i.e., it converges to \n\n    p(\u03b8|{z_1, \u22ef, z_N}) \n    = p(\u03b8, {z_1, \u22ef, z_N})/p({z_1, \u22ef, z_N})\n       =\n    P(\u03b8) p({z_1, \u22ef, z_N}|\u03b8)/p({z_1, \u22ef, z_N})\n        =P(\u03b8) \u220f_n=1^N p(y_n | x_n, \u03b8)/\u220f_n=1^N p(y_n | x_n)\u221d P(\u03b8) \u220f_n=1^N p(y_n | x_n, \u03b8).\n\nThis is shown in <cit.>. In the sequel we use the more common notation p_\u03b8(y_n | x_n) instead of p(y_n | x_n, \u03b8). This makes a clear distinction between the parameters of the model and the samples we received.\n\nA few remarks are in order. An obvious choice from a theoretical point of view is to use an iid Gaussian prior. In practice it is best not to use iid Gaussian prior in order to speed up the convergence. Indeed, the main point of <cit.> is to discuss suitable schemes. But for our current conceptual purpose we will ignore this (important) practical consideration. \n\n\n\n\u00a7 THE PAC BAYES BOUND AND BAYES COMPLEXITY\n\nLet us now get back to the main point of this paper. We start by defining two notions of complexity. Both of them are \u201cBayes\u201d complexities in the sense that both relate to the size of the parameter space (as measured by a prior) that approximately represents a given function. We will then see how this complexity enters the PAC-Bayes bound.\n\n\n\n  \nContribution. Our main contribution is an introduction of a new notion of complexity of functions and we show that it has the following properties: (i) it governs a PAC Bayes-like generalization bound, (ii) for neural networks it relates to natural notions of complexity of functions, and (iii) it explains the generalization gap between neural networks and linear schemes in some regime. While there is a large set of papers which describes each such criterion, and even some that fulfill both (e.g., <cit.>), as far as we know, this is a first notion that satisfies all three of them. \n\n\nFor every > 0 we define the sharp complexity of a function g with respect to the hypothesis class \u210b_\u03b8 as\n\n    \u03c7^#(\u210b_\u03b8, g, _x, ^2)    := -log[ _\u03b8{\u03b8 : _x \u223c_x [(g(x) - f_\u03b8(x))^2] \u2264^2 }],\n\nwhere the probability _\u03b8 is taken wrt to the prior P.\n\n\nIn words, we compute the probability, under prior P, of all these functions f_\u03b8 that are close to g under the quadratic loss and distribution _x.\n\nIn general, it is difficult to compute \u03c7^# for a given\n\u03f5.  However, for realizable functions it is often possible to compute\nthe limiting value of the sharp complexity, properly normalized, when \u03f5\ntends to 0.\n\nWe define the sharp complexity of a function g with respect to the hypothesis class\n\n    \u03c7^#(\u210b_\u03b8, g, _x)     := lim_\u03f5\u2192 0log[_\u03b8{\u03b8: _x \u223c_x[ (g(x) - f_\u03b8(x))^2 ] \u2264^2 }]/log().\n\n\nThe above definitions of complexity implicitly depend on the hypothesis class \u210b_\u03b8. If the hypothesis class (and/or _x) is clear from context we will omit it from notation, e.g. \u03c7^#(g, ^2) = \u03c7^#(g, _x, ^2) = \u03c7^#(\u210b_\u03b8, g, _x, ^2). \n\nWe now state the main theorem. It is a generalization bound, which crucially depends on the sharp complexity from Definition\u00a0<ref>. The proof is deferred to Appendix\u00a0<ref>.\n\n\nIf L_(P) \u2265 2\u03c3_e^2 and g \u2208supp(P) then for every \u03b2\u2208 (0,1] there exists \u03c3_alg^2 such that if we set \u03c3_y^2 = \u03c3_alg^2 then _\u223c^N[L_S(Q(\u03c3_y^2))] = (1+\u03b2)\u03c3_e^2 and\n\n    _\u223c^N[L_(Q(\u03c3_y^2))]\n       \u2264\u03c3_e^2 + [ \u03b2\u03c3_e^2 + C/\u221a(2)\u221a(\u03c7^#(g, _x, \u03b2\u03c3_e^2)/N)].\n\n\n  \nDiscussion of Assumptions.\nRequiring that g \u2208supp(P) is only natural as it indicates that g is realizable with prior P. It is also natural to assume that L_(P) \u2265 2\u03c3_e^2 as the lowest possible error is attained by g and is equal \u03c3_e^2. Thus we require that the expected loss over the prior is twice as big as the minimal one. As P should cover a general class of functions it is only natural that L_(P) \u2265 2\u03c3_e^2.\n\nFor a fixed \u03b2, \u03c3_alg^2 from Theorem\u00a0<ref> is, in general, not known. However, as proven in Appendix\u00a0<ref>, we have\nlim_\u03c3_y^2 \u2192 0_\u223c^N[L_(Q(\u03c3_y^2))]= \u03c3_e^2,   lim_\u03c3_y^2 \u2192\u221e_\u223c^N[L_(Q(\u03c3_y^2))]= 2\u03c3_e^2.\n\nMoreover, _\u223c^N[L_(Q(\u03c3_y^2))] is continuous in \u03c3_y^2, which implies that \u03c3_alg^2 can be found by a binary search-like procedure by holding out some part of  for estimating _\u223c^N[L_(Q(\u03c3_y^2))] for different \u03c3_y^2 values.\n\n\n\n  \nBound (<ref>) in terms of limiting complexity.  Notice that (<ref>) is governed by \n\u03c7^#(g,_x,\u03b2\u03c3_e^2). Aaccording to (<ref>), for small enough \u03b2\u03c3_e^2, we have\n\n    \u03c7^#(g,_x,\u03b2\u03c3_e^2) \u2248 -\u03c7^#(g,_x)log(\u03b2\u03c3_e^2).\n\nThis means that for small enough noise level, where the exact regime for which the approximation holds depends on a specific problem, we have\n\n    _\u223c^N[L_(Q(\u03c3_y^2))]\n    \u2a85\n    (1 + \u03b2) \u03c3_e^2 + C/\u221a(2)\u221a(-\u03c7^#(g, _x)log(\u03b2\u03c3_e^2)/N).\n\nWe see that the generalization bound depends crucially on the limiting complexity. \n\n\n\n\n  \nMain message. \nNote that the smallest we can hope to get on the right hand side is \u03c3_e^2 since this is the variance of the noise and this is achievable if we pick Q that puts all its mass on g.\nThis means that \u03b2\u03c3_e^2 plus the square root term from (<ref>) represents the expected excess generalization error. \n\nThis brings us to the punch line of this paper. In the subsequent sections we will see that\n(i) natural notions of complexity that have previously been discussed in the literature align with our new notion when we consider neural networks, whereas \n(ii) for linear schemes our notion of complexity is essentially independent of the function (as long as it is realizable) and as a consequence is as high as for the most complex (in the natural sense) function in our hypothesis class.\n\nTo the degree that we assume that reality prefers simple functions this explains why neural nets generalize significantly better than linear schemes.\n\nIn Section\u00a0<ref> we show that for neural networks and a piece-wise linear function g the limiting complexity is equal to the number of slope changes g. In light of (<ref>), this means that neural networks require the fewer samples (for a good generalization bound) the fewer slope changes g has.\n\nThere is a further connection to a natural notion of complexity. In Section\u00a0<ref> we show that sharp complexity is related to the variation of g, i.e. the integral of the second derivative of g. Thus, in the light of (<ref>), fewer samples are needed (for a good generalization) for g's with smaller variation.\n\nAs we discussed above, sharp and limiting complexity are related via (<ref>) when \u03b2\u03c3_e^2 is small. We can thus think of sharp complexity as a refinement of limiting complexity. This is reflected in the two cases discussed above \u2013 the number of slope changes can be seen as an approximation of the variation of a function.\n\nIn Section\u00a0<ref>, on the other hand, we show that for linear schemes the limiting complexity is virtually independent of the function and equal to the number of basis functions. This means that in this case the number of samples needed for a good generalization bound is the same for simple and complicated functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 MODELS\n\n\nAlthough the basic idea applies to any parametric family, we will consider restricted types of families and demonstrate our concepts with two concrete examples, namely linear schemes and NNs. We will be interested in parametric families of functions from  to . More precisely families of the form \u210b_\u03b8 := {_\u03b8 : , \u03b8\u2208^m },\nwhere \u03b8 is the vector of parameters. for a function g : \u2192 and a distribution _x we define the set of exact representations as A_g,\u210b,_x := {\u03b8\u2208^m : f_\u03b8\u2261_(_x) g }. If \u210b and _x are clear from context we will often write A_g. The 0 function will play an important role, thus we also define A_0 := {\u03b8\u2208^m : f_\u03b8\u2261_(_x) 0 }\n\n \u00a7.\u00a7 Linear Schemes\n\nConsider the linear family _\u03b8^(L, o)={f_\u03b8(x): f_\u03b8(x) = \u2211_i=0^d-1_i b_i(x), x \u2208 = [-1, 1]},\ni.e., the vector of parameters \u03b8 is equal to the vector of weights . We assume that the functions {b_i(x)} form an orthonormal basis.\nAlthough the exact basis that is used is not of importance one might think of b_i(x) as a polynomial of degree i or the first few Legendre polynomials. In this way the basis functions are naturally ordered by complexity. \n\n\n\n\n \u00a7.\u00a7 Neural Networks\n\nConsider the family ^NN represented by NNs with layers numbered from 0 (input) to K (output), containing d = d_0, d_1, \u2026, and d_K = d_y neurons respectively. We will limit our attention to d_y = 1. The activation functions for the layers 1 to K are presumed to be \u03c3_1, \u2026, \u03c3_K :. The weight matrices will be denoted by W^(1), W^(2), \u2026, W^(K), respectively, where matrix  W^(k) connects layer k-1 to layer k. We define \n\n    _\u03b8(x) := \u03c3_K (^(K) + W^(K)\u03c3_K-1( \u2026\u03c3_1(^(1) + W^(1) x ))) .\n\n\n\u00a7 WHY NEURAL NETS GENERALIZE WELL\n\nWe now get to the main point of this paper, namely why neural nets generalize much better than other schemes, in particular linear schemes.\n\nThe basic idea is simple. We have seen in the previous sections that (i) a suitable version of SGD gives us a posterior of the form (<ref>), and (ii) this posterior gives rise to a an upper bound on the generalization error that depends mainly on the \u201ccomplexity\u201d of the underlying true hypothesis.\n\nThis notion of complexity of a function depends on the underlying hypothesis class.\nTo close the circle we will now discuss how this complexity behaves for interesting hypothesis classes. In particular, as we will see that there is a striking difference between linear schemes and neural networks. For linear schemes, every realizable function has essentially the same complexity. This in particular means that we do not expect to learn a \u201csimple\u201d function (e.g., think of a constant function) with fewer samples than a \u201ccomplex\u201d one (think of a highly variable one). For neural nets the complexity behaves entirely differently and there is a large dynamic range. As we will see, in a suitable limit the complexity is to first order determined by the number of degrees of freedom that have to be fixed in order to realize a function. Therefore, for neural nets, simple functions have a much lower complexity than complicated ones. \n\n\n\n \u00a7.\u00a7 Neural Networks with a Single Hidden Layer\n\n\nWe start with analyzing our notion of complexity for the case of NN with a single hidden layer and 1-dimensional input. More precisely let x \u2208 denote the input and y \u2208 denote the output.  There are k nodes in the hidden layer. More precisely, the network represents the function\n \n    f_\u03b8(x) \n        = \u2211_i=1^k _i^(2)\u03c3( _i^(1) x + _i^(1))  + b^(2)\n        = \u2211_i=1^k _i^(2)[ _i^(1) x + _i^(1)]_+ + b^(2),\n\ni.e., we use ReLU activation functions. \n\nThe _i^(1) denotes the bias of the i-th node, the _i^(2) represents the weight of the i-th output signal, and b^(2) is the global bias term of the output. We let \u03b8 = (\u03b8_w, \u03b8_b) = ((^(1), ^(2)), (^(1), b^(2))) denote the set of all parameters, where \u03b8_w denotes the set of weights and \u03b8_b denotes the set of bias terms.\n\n\n\n  \nParametrization and prior. We will use the following non-standard parametrization of the network\n\n    f_\u03b8(x) \n       = \u2211_i=1^k _i^(2)[_i^(1)(x - _i^(1)) ]_+ + b^(2)\n       = \u2211_i=1^k _i^(2)\u00b7|_i^(1)| \u00b7[sgn(_i^(1))( x - _i^(1))]_+ + b^(2),\n\nwhere in the last equality we used the fact that the ReLU activation function is 1-homogenous. Note that there are two kinds of ReLU functions (depending on the sign of w_i^(1)) they are either of the form [x -b]_+ or 0 at [-(x-b)]_+. If we restrict our attention to how f_\u03b8 behaves on a compact interval then considering just one of the kinds gives us the same expressive power as having both. This is why for the rest of this section we restrict our attention only to the case of [x-b]_+ as it simplifies the proofs considerably. Thus the final parametrization we consider is\n\nf_\u03b8(x) = \u2211_i=1^k _i^(2)\u00b7_i^(1)[x - _i^(1)]_+ + b^(2).\n\n\nWe define the prior on \u03b8 as follows: each component of \u03b8_w comes i.i.d. from \ud835\udca9(0,\u03c3_y^2), each component of ^(1) comes i.i.d. from U([0,M]), where M will be fixed later and b^(2) comes i.i.d. from \ud835\udca9(0,\u03c3_b^2)[The different parametrization and the uniform prior on the bias terms are non-standard choices that we make to simplify the proofs. These choices would not affect the spirit of our results but as always the details need to be verified.].\n\nWe will argue that our notion of complexity (\u03c7^#(_x,g,^2) \nand \u03c7^#(_x,g)) corresponds, in a case of NN, to natural notions of complexity of functions. \n\n\n\n\n\n\n  \nTarget function. We will be interested in target functions g that are representable with a single hidden layer networks. Let g : [0, 1] be continuous and piece-wise linear. I.e., there is a sequence of points 0=t_1 < t_2 < \u22ef < t_l+1=1 so that for x \u2208 [t_i, t_i+1], 1 \u2264 i < l+1,\n\n    g(x) = c_i + \u03b1_i (x-t_i),\n\nfor some constants c_i and \u03b1_i, where c_i+1 = c_i + \u03b1_i (x_i+1-x_i). Then f can be written as a sum of ReLU functions, \n\n    g(x) = b + \u2211_i=1^l v_i [x-t_i]_+,\n\nwhere v_1=\u03b1_1 and v_i=\u03b1_i-\u03b1_i-1, i=2, \u22ef, l. The terms in (<ref>) for which v_i = 0 can be dropped without changing the function. We call the number of nonzero v_i's in (<ref>) to be the number of changes of slope of g. \n\n\n  *  Start with the fundamental questions of generalization and review various papers.\n\n  *  Say what the overall idea is, namely NNs with regularizer minimizes the sum of the  complexity of the function plus loss.\n    \n  *  Start with Srebo paper that says that for a one-hidden NN minimizing the loss plus the square of the norm is equal to finding a hypothesis so that the sum of the loss plus complexity are minimized.\n    \n    \n  *  We show that, suitably generalized, a similar picture emerges for the \"general\" case.\n    \n  *  We consider a general network.\n        \n  *  We consider the SGLD.\n        \n  *  We impose a Gaussian prior on the weights.\n    \n  *  We then show that the general measure of complexity is given by the \"Bayesian\" complexity of a function (need a better word). I.e., in general, the samples we get from the SGLD are such that they minimize the sum of two exponents, one coming from the approximation error and one from the complexity measure.\n    \n  *  The multiplicity complexity measure is naturally connected to several other perhaps more intuitive complexity measures. E.g., the initial scheme is one example but it would be nice to find at least one other example (perhaps the square functions)\n    \n  *  We show that if we apply the same framework to linear schemes the complexity measure does not behave in the same way, giving a strong indication why overfitting does not happen to the same degree for NNs.\n    \n  *  We show what happens if we add layers to a network.\n    \n  *  We explore the role of dropout (not so sure if we can do this; what does this mean for the dynamics?)\n\n\n  \u00a7.\u00a7.\u00a7 Complexity in the Asymptotic Case\n\nIn this section we explore what is the limiting value of the sharp complexity for the case of NN.\n\n\nAssume that we are given a Gaussian vector  of length ,\nwith mean , and with covariance matrix  that has full rank.  Let \u2208^.  Let _1, c and _1, c denote the\nrestrictions of  and  to the first c components and\nlet _c+1, denote the restriction of  to the\nlast -c components. Finally, let R \u2286^-c\nbe a set of strictly positive Lebesgue measure.  Then\n\n    lim_\u03f5\u2192 0log[{{: _c+1, \u2208 R \u2227_1, c-_1, c_2 \u2264\u03f5}]/log(\u03f5) = c.\n\nBefore we proceed to the proof let us quickly discuss how we will apply this observation. Assume that we can represent a given function g(x) exactly within a model _\u03b8 by fixing c of the components to a definite value and that the remaining -c components\ncan be chosen within a range that does not depend on \u03f5. This is e.g. the case for neural networks. Due to the non-linearity some parameters can range freely without changing the function. Assume further, that the model has a finite derivative with respect to each of the c fixed values.  For Gaussian prior we have by Lemma\u00a0<ref> that the complexity of this function is c. In the above discussion we implicitly assumed that the function has a unique representation. But, as we will discuss in Section\u00a0<ref> and in in the appendix, in general, realizable functions do have many representations. Besides the discrete symmetries inherent in many models there are also continuous symmetries that often arise. E.g., the output of a single ReLU can be exactly replicated by the sum of several ReLU functions. Nevertheless, even though the actual probability for a fixed \u03f5 can be significantly larger due to this multiplicity, the asymptotic limit remains the same Is it clear?.\n\nLet us start by assuming that the Gaussian distribution has iid components. In this case the probability factors into the probability that the last k-c components are contained in the region R, which by assumption is a strictly positive number, independent of \u03f5 and the probability that the first c components are contained in a ball of radius \u03f5 around a fixed point. Note that this second probability behaves like \u03ba\u03f5^c, where \u03ba is strictly positive and does not depend on \u03f5. The result follows by taking the log, dividing by log(\u03f5) and letting \u03f5 tend to 0. \n\nThe general case is similar. Write \n\n    {: _c+1, \u2208 R \u2227_1, c-_1, c_2 \u2264\u03f5}\n       = \u222b__c+1, k\u2208 R\u222b__1, c-_1, c^*\u2264\u03f5 f(_1, c, _c+1, k) \n        = \u222b__c+1, k\u2208 R f(_c+1, k) \u222b__1, c-_1, c^*\u2264\u03f5  f(_1, c|_c+1, k).\n\nNow note that each value of _c+1, k the inner integral scales like \u03f5^c, and hence this is also true once we integrate over all values of _c+1, k.\n[Function with c changes of slope]\nImagine that d=1, that is g : \u2192 and assume that g is a piece-wise linear function with c changes of slope. We can represent this function by fixing c degrees freedom to definite values. For instance we can choose c nodes in the hidden layer and represent one change of slope with each of these neurons. If (_x) contains all x's for which the changes of slope occur then Lemma\u00a0<ref> guarantees that \u03c7^#(\ud835\udc9f_x,g) = cIt's not super clear to me. What about for instance the fact that we can distribute the change of slope as \u221a(a), \u221a(a) and a^1/3, a^2/3?. Plugging this result in (<ref>) we get that for small  the true versus empirical loss gap behaves as\n\n    \u2248\u221a(c log(1/)/N + \u03c3_y^2/N + /2\u03c3^2_y + ln(N)/\u03b4 N).\n\nWe see that in this case the generalization bound strongly depends on the complexity of g, which in this case is the number of changes of slope.\n\n\nIt will turn out that the key object useful for computing \u03c7^#(g) is a particular notion of dimension of A_g. \n\nFor A, S \u2286^m we define the Minkowski-Bouligand co-dimension of A w.r.t. S as\n_S(A) := lim_R \u2192\u221elim_\u2192 0log(( (A + B_) \u2229 B_R \u2229 S))/log() , \nwhere  is the Lebesgue measure and + denotes the Minkowski sum. \n\nOur definition is a variation of the standard Minkowski-Bouligand dimension. The first difference is that we measure the co-dimension instead of the dimension. Secondly,  we compute lim_R \u2192\u221e. We do this because the sets we will be interested in are unbounded. We also define the co-dimension wrt to an auxilary set S, i.e., all volumes are computed only inside of S. One can view it as restricting the attention to a particular region. In our use cases this region will be equal to the support of the prior. We will sometimes use _P(A) to denote _(P)(A), when P is a distribution.\n\nTechnically the notion is not well defined for all sets. Formally, one defines a lower and an upper co-dimension, corresponding to taking lim inf and lim sup. Sets A and S need also be measurable wrt to the Lebesgue measure. We will however assume that for all of our applications the limits are equal, sets are measurable and thus the co-dimension is well defined. This is the case because all sets we will be interested in are defined by polynomial equations.\n\n\nThe first lemma relates sharp complexity and co-dimension.\n\n\nLet g(x) = b + \u2211_i=1^c v_i [x - t_i]_+,\nwhere 0 < t_1 < \u2026 < t_c < 1, v_1,\u2026,v_c \u2260 0 and c \u2264 k. Then\n1/5_P(A_g) \u2264\u03c7^#(g, U([0,1])) \u2264_P(A_g).\n\nRecall that A_g = {\u03b8 : f_\u03b8\u2261_[0,1] g}.\n\n\nThe next lemma computes the co-dimension of a function with c changes of slope.\n\n\nLet g(x) = b + \u2211_i=1^c v_i [x - t_i]_+,\nwhere 0 < t_1 < \u2026 < t_c < 1, v_1,\u2026,v_c \u2260 0 and c \u2264 k. Then\n_P(A_g) = 2c+1.\n\nThere exists a universal constant C such that for all In the general case there's also b^(2) but I guess it'll work.f_\u03b8_0(x) = \u2211_i=1^k _i[x - _i^(1)]_+, such that f_\u03b8_0_2^2 = ^2, there exists \u03b8_1 such that f_\u03b8_1\u2261_[0,1] 0 and \u03b8_0 - \u03b8_1_2^2 \u2264 O(^C).\n\nLet L(\u03b8) := f_\u03b8^2. Consider the following differential equation\n\n    \u03b8\u0307 = - \u2207 L(\u03b8)/\u2207 L(\u03b8)||_2,\n\nwhich can be understood as a normalized gradient flow. By definition\n\n    L\u0307 = (\u2207 L(\u03b8))^T \u03b8\u0307(<ref>)= - \u2207 L(\u03b8)_2.\n\nWe will later show that \n\n    \u2207 L(\u03b8)_2 \u2265 L^0.8.\n\nNote that the solution to L\u0307 = - L^0.8 is of the form L(t) = c (C - t)^0.8. More precisely, with the initial condition L(0) = ^2 we get that C = (^2/c)^1/4/5. What follows is that L ((^2/c)^5/4) = 0. Using (<ref>) we get that there exists t^* < (^2/c)^5/4 such that L(t^*) = 0. Because the change of \u03b8 is normalized (see (<ref>)) we get that \u03b8(0) - \u03b8(t^*)_2^2 \u2264(^2/c)^4/5 = O (^4/5/4). What is left is to show (<ref>).\n\nWe start by computing derivatives of L wrt to \u03b8. For every i \u2208 [1,k]\n    \u2202 L/\u2202_i^(1) = _i \u222b__i^(1)^1 f_\u03b8(x)  dx.\n\n    \u2202 L/\u2202_i = \u222b__i^(1)^1 f_\u03b8(x)(x - _i^(1))  dx.\n\nWe will show that there exists i \u2208 [1,k] such that max{|\u2202 L/\u2202_i^(1)|,|\u2202 L/\u2202_i|} is large. \n\nFor a function f : [0,1] \u2192, f(0) = 0, f'(0) = 0 (that one should understand as an abstraction of f_\u03b8) consider the following expression (related to (<ref>))\n\n    f\u201d(y) \u222b_a^1 f(y)  dx.\n\nThe following computation will be helpful\n\n    \u03b1(a,b)    := \u222b_a^b f\u201d(y) \u222b_y^1 f(x)  dx  dy \n       = [f'(y) \u222b_y^1 f(x)  dx ]_a^b - \u222b_a^b f'(y) \u00b7 (- f(y))  dy       by parts\n       = f'(b)\u222b_b^1 f(x)  dx - f'(a) \u222b_a^1 f(x)  dx + [1/2 f^2(x) ]_a^b \n       = f^2(b)/2 + f'(b)\u222b_b^1 f(x)  dx - f^2(a)/2 - f'(a) \u222b_a^1 f(x).\n\nNow note that\n\n    \u03b1(0,b) \n       = f^2(b)/2 + f'(b)\u222b_b^1 f(x)  dx - f^2(0)/2 - f'(0) \u222b_0^1 f(x) \n       = f^2(b)/2 + f'(b)\u222b_b^1 f(x)  dx       As  f'(0) = f(0) = 0.\n\nLet M := max_x \u2208 [0,1] |f(x)| and x^* \u2208 f^-1(M). We claim that \n\n    \u03b1(0,x^*) = M^2/2.\n\nTo see that use (<ref>) and note that either x^* \u2208 [0,1] and then f'(x^*) = 0 because it is an extremal point, or x^* = 0 and then f'(0)= by definition, or x^* = 1 and then \u222b_1^1 f(x)  dx = 0. Using (<ref>) and the definition of \u03b1 we get that there exists x_0 \u2208 [0,x^*] such that\n\n    |f\u201d(x_0) \u222b_x_0^1 f(x)  dx | \u2265M^2/2 x^*\u2265M^2/2.\n\nNow note that f_\u03b8 satisfies f_\u03b8(0) = 0. It might not be true that f'_\u03b8(0) = 0 but if we increase all the bias terms by a negligible amount then f'_\u03b8(0) = 0 and the quantity of interest (<ref>) changes only negligibly I guess it's true. Moreover observe that for every i \u2208 [1,k]f\u201d_\u03b8(_i^(1)) = \u2211_j : _j^(1) = _i^(1)_j and for all x \u2208 [0,1] \u2216{_1^(1), \u2026, _k^(1)} we have f\u201d_\u03b8(x) = 0. As the number of nodes is k we get from (<ref>) that there exists i \u2208 [1,k] such that\n|_i \u222b__i^(1)^1 f(x)  dx | \u2265M^2/2 k^2. \n\nIf M \u2265^0.9 then \n|\u2202 L/\u2202_i^(1)| \u2265^1.8/2 k^2\u22651/2k^2(^2 )^0.9\u22651/2k^2 L(\u03b8)^0.9,\nwhich implies (<ref>) and ends the proof in this case. Thus we can assume for the rest of the proof that M < ^0.9.\n\nBy Holder's inequality we have \n\n    f_\u03b8_1 \u2265f_\u03b8_2^2 / f_\u03b8_\u221e\u2265^2 / ^0.9 = ^1.1.\n\nLet 0 = a_1 \u2264 a_2 \u2264\u2026\u2264 a_k+2 = 1 be the ordering of {b_1^(1), \u2026, b_k^(1)}\u222a{0,1}. Consider a generalization of (<ref>)\u222b_a^1 f(x) (x - a)  dx.\n\nLet I(a) := \u222b_a^1 f_\u03b8(x)  dx. Note that\n\n    d/d a\u222b_a^1 f(x) (x - a)  dx = \u222b_a^1 f(x) = I(a).\n\nLet i \u2208 [1,k] be such that it satisfies\n\n  * \u222b_a_i^a_i+2 |f_\u03b8(x)| _{sgn(f_\u03b8(x)) = sgn(f_\u03b8(a_i+1)) } dx \u2265^1.1/k, \n  * \u222b_a_i^a_i+2 |f_\u03b8(x)| _{sgn(f_\u03b8(x)) = sgn(f_\u03b8(a_i+1)) } dx \u2265\u222b_a_i^a_i+2 |f_\u03b8(x)| _{sgn(f_\u03b8(x)) \u2260sgn(f_\u03b8(a_i+1)) } dx.  \nSuch an i exists because of (<ref>) and the fact that f_\u03b8 crosses 0 at most k times Is it enough of a proof?. Assume without loss of generality that f_\u03b8(a_i+1) > 0. By definition f_\u03b8 is two-piece linear on [a_i, a_i+2], because of that and the assumption that f_\u03b8(a_i+1) > 0 we know that \u222b_a^1 f_\u03b8(x) first increases, then decreases and finally increases (the first and the third phase might not happen). By (<ref>) we know that I(a_i) \u2265 I(a_i+2). Let a_max := _a I(a), a_min := _a I(a). By (<ref>) we know that I(a_max) - I(a_min) > ^1.1/k. Consider two cases:\n\n\n  \nCase 1: I(a_max) \u2265I(a_max) - I(a_min)/2.\n\n  \nCase 2: I(a_max) < I(a_max) - I(a_min)/2.We need a bound on weights!!! Or do wee\n\nThis brings us to the main result of this subsection\n\n[Function with c changes of slope - Bayes Complexity]\nLet g : [0,1] \u2192 and assume that g is a piece-wise linear function with c \u2264 k changes of slope. Then\n2c+1/5\u2264\u03c7^#(g, U([0,1])) \u2264 2c+1.\n\n\nWe see that the limiting complexity is \u2248 c, for c \u2264 k. This means that the complexity depends strongly on the function and simpler - in a sense of fewer changes of slopes - functions have smaller complexity. In Section\u00a0<ref> we will compute the limiting complexity for linear models. It will turn out, see Example\u00a0<ref>, that in this case the complexity doesn't depend on the function and is equal to the number of basis functions used in the linear model.\n\n\n\n  \u00a7.\u00a7.\u00a7 The -Complexity Case\n\n\nWe saw in the previous section that for the case of neural networks our notion of complexity corresponds (in the limit and up to constant factors) to the number of degrees of freedom that need to be fixed to represent a given function.\nWhen we evaluate the complexity at more refined scales it can be shown that it is closely related to another natural complexity measure. \n\n[Function with \u222b |g\u201d(x)| dx = a] Let\n\n    C(g) = max(\u222b |g\u201d(x)| dx, |g'(-\u221e) + g'(+\u221e)| ).\n\nIn <cit.> it was shown that, for the case of a single hidden layer NN with 1-D input, for every g : \u2192 if we let the width of the network go to infinity Is \u03b8_w defined? then\n\n    min_\u03b8 : f_\u03b8 = g\u03b8_w^2 = C(g).\n\nThis means that if we use an \u2113_2 regularizer for training a neural network\n\n    \u03b8^* = _\u03b8( L_S(f_\u03b8) + \u03bb\u03b8_w^2 ),\n\nthen C(f_\u03b8^*) = \u03b8^*_w^2. In words, the function that is found via this scheme balances the empirical error and C(g).\n\nIn the appendix we show that in some regimes C(g) \u2248\u03c7^#(_x, g, ). Plugging it in (<ref>) we get that the expected true versus empirical loss gap is\n\n    \u2248\u221a(O_\u03c3_w^2,(C(g)/N) + \u03c3_y^2/N + /2\u03c3^2_y + ln(N)/\u03b4 N),\n\nwhere O_\u03c3_w^2, drops terms dependent on \u03c3_w^2,. See the appendix for details. We see that the gap crucially relies on C(g). This result can be seen as a quantitative version of Example\u00a0<ref> as \u222b |g\u201d(x)| dx can be seen as a more refined version of the number of changes of slope.\n\n\n\n  \nVariational Complexity\nLet us now introduce a complexity measure for a piece-wise linear function g. \nWe start by introducing a complexity measure for a particular choice of the network parameters. The complexity of the function will then be the minimum complexity of the network that represents this function. \nWe choose\n\n    C_k(\u03b8) = 1/2\u03b8_w^2 = 1/2( ^(1)^2 + ^(2)^2 ),\n\ni.e., it is half the squared Euclidean norm of the  weight parameters. \n\nIf we use the representation (<ref>) in its natural form, i.e.,  w^(2)_i =a_i and W^(1)_i = 1, then we have C_k(\u03b8) = 1/2\u2211_i=1^k (a_i^2+1). But we can do better. Write\n\n    f(x) = c + \u2211_i=1^k w^(2)_i [W^(1)_i(x-x_i)]_+,\n\nwhere w^(2)_i =a_i/\u221a(|a_i|) and W^(1)_i = |w^(2)_i |. This gives us a complexity measure C_k(\u03b8) = \u2211_i=1^k |a_i| = \u2211_i=1^k |\u03b1_i-\u03b1_i-1|, where \u03b1_0=0. Indeed, it is not very hard to see, and it is proved in <cit.>, that this is the best one can do even if we keep f(x) fixed and are allowed to let the number k of hidden nodes tend to infinity. In other words, for the function f described in (<ref>) we have\n\n    C(f) = inf_k \u2208, \u03b8: f_\u03b8 = f C_k(\u03b8) = (f'),\n\nwhere (f') denotes the total variation of f', the derivative of f. Why total variation?\nNote that \u03b1_i denotes the derivative of the function so that |\u03b1_i-\u03b1_i-1| is the change in the derivative at the point x_i. Therefore, \u2211_i=1^k |\u03b1_i-\u03b1_i-1| is the total variation associated to this derivative. \n\n\n\n\n\n\n\n\n\n\n\n\nIf we consider a general function f: [0, 1] then for every \u03f5>0, f can be uniformly approximated by a piecewise linear function, see <cit.>. As \u03f5 tends to 0 for the best approximation the variation of the piece-wise linear function converges to the total variation of f'. This can equivalently be written as the integral of \n|f\u201d|.\nIt is therefore not surprising that if we look at general functions f: and let the network width tend to infinity then the lowest cost representation has a complexity of\n\n    C(f) = max(\u222b |f\u201d(x)| dx, |f'(-\u221e) + f'(+\u221e)| ).\n\nAs we previously mentioned, this concept of the complexity of a function was introduced in <cit.> and this paper also contains a rigorous proof of (<ref>). (Note: The second term in (<ref>) is needed\nwhen we go away from a function that is supported on a finite domain to . To see this consider the complexity of f(x) = \u03b1 x. It is equal to 2\u03b1 (f(x) = \u221a(\u03b1) [\u221a(\u03b1) x]_+ - \u221a(\u03b1) [-\u221a(\u03b1) x]_+) but \u222b |f\u201d(x)| dx = 0.)\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \nSharp versus Variational Complexity. Now we explain how the notion of sharp complexity is, in some regimes, equivalent to the variational complexity. This gives a concrete example of our promise that sharp complexity aligns well with natural complexity measures.\n\n\n\n\n\n\nAssume at first that the target function is of the form g(x) = b + \u2211_i=1^c v_i[x - t_i]_+\nand requires only a single change of the derivative. I.e., the piece-wise linear function consists of two pieces and we require only one term in the sum, g(x) = a[x - t]_+ + b Call this function g_1, where the 1 indicates that there is only a single change of the derivative and the change is of magnitude a. \n\n\n\n\nWe now ask what is the value of \u03c7^#(g_1, _x, ^2), for _x = U([0,1]) - as this is what appears in (<ref>). We claim that for small , specific choices of M and \u03c3_w^2, \u03c3_b^2 and particular regimes of parameters we have\n\n    \u03c7^#(g_1, U([0,1]), ^2) = \u0398(a / \u03c3_w^2) = \u0398(C(g_1) / \u03c3_w^2).\n\nThis means that the sharp complexity is closely related to the variational complexity of g_1. The more formal version of (<ref>) of which a proof is in Appendix\u00a0<ref> reads\n \nLet t,\u2208 (0,1), a,b\u2208. Define g_1(x) := b + a[x - t]_+. If k \u2264 M \u22641/\u03c3_w^2, \u03c3_b^2 \u22641/\u03c3_w^2 and \u03a9(^1/4),\u03a9(log(k/\u03c3_w) \u03c3_w^2) \u2264 |a| < 2, \u03a9(^1/4) \u2264  |b|, \u03a9(^1/2) \u2264min(t,1-t) then\n|a|/3 \u03c3_w^2\u2264\u03c7^#(g_1, U([0,1]), ^2) \u2264 2(|a|/\u03c3_w^2 + |b|/\u03c3_b^2) + 11 - 3log().\n\n\nThe above lemma is stated with the most general setting of parameters. To get more insight into the meaning of the lemma we give the following corollary.\n\n\nFor every sufficiently small \u03c3_e^2 and M = k, \u03c3_w^2 = 1/k, \u03c3_b^2 = 1, |b| = \u0398(\u03c3_e^1/2), \u03a9( \u03c3_e^1/4), \u03a9(log(k)/k) \u2264 |a| < 2 if we define g_1(x) := b + a[x-1/2]_+ then\n\u03c7^#(g_1,U[0,1],\u03c3_e^2) \u2264 3|a|k + 3 log(1/\u03c3_e).\n\nOne can easily verify that the assumptions of Lemma\u00a0<ref> are satisfied. Applying the lemma we get\n\n    \u03c7^#(g_1,U[0,1],\u03c3_e^2) \n       \u2264 2(|a|/\u03c3_w^2 + |b|/\u03c3_b^2) + 11 + 3log(1/\u03c3_e)  \n       \u2264 2|a| k + \u0398(\u03c3_e^1/2) + 11 + 3log(1/\u03c3_e) \n       \u2264 3|a|k + 3 log(1/\u03c3_e)       As \u03a9( log(k)/k) \u2264 |a|.\n\n\n  \nGeneralization bound. Now we want to understand what Example\u00a0<ref> gives us for the generalization bound from Theorem\u00a0<ref>. Setting \u03b2 = 1 in Theorem\u00a0<ref> and applying Example\u00a0<ref>,  we can bound\n\n    _\u223c^N[L_(Q)] \n       \u2264\n    2\u03c3_e^2 + C/\u221a(2)\u221a(\u03c7^#(g_1, _x, \u03c3_e^2)/N)\n       \u2264 2\u03c3_e^2 + C/\u221a(2)\u221a(3|a|k + 3 log(1/\u03c3_e)/N).\n\n\nNow we interpret (<ref>). First note that the setting of parameters in Example\u00a0<ref> is natural. The choice of \u03c3_w^2 = 1/k and \u03c3_b^2 = 1 are among standard choices for initialization schemes. We pick |b| = \u0398(\u03c3_e^1/2) and t = 1/2 in order to analyze functions g_1(x)\u2248 a[x - 1/2]_+, where the bias term b is nonzero because of the assumptions of Lemma\u00a0<ref>. Note that depending on the relation between k and \u03c3_e^2 one of the terms dominates (<ref>): either 3|a|k or 3 log(1/\u03c3_e). \n\nIf \u03c3_e^2 \u226a k then 3 log(1/\u03c3_e) dominates and the generalization bound depends mostly on the noise level \u03c3_e^2[As a side note, notice that the 3 in 3 log(1/\u03c3_e) corresponds to the 2c+1 bound on the limiting complexity in Example\u00a0<ref>, as we consider a function with one change of slope and a very small ^2 for computing \u03c7^#. This illustrates the relationship between sharp and limiting complexity.].\n\nIf \u03c3_e^2 \u226b k then 3|a|k dominates. In this case we get the promised dependence of the generalization bound on |a|, which we recall is equal to C(g_1). Note that there is a wide range of |a| for which the bound holds, i.e. \u03a9(log(k)/k) \u2264 |a| \u2264 2. We see that the simpler g_1, measured in terms of C, the better a generalization bound we get. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Neural Networks with Several Hidden Layers\n\nConsider now exactly the same set-up as before, except that now we have K=4, i.e., we have three hidden layers and still d = 1. We can still represent piece-wise linear functions (e.g., by using the first layer to represent the function and just a single node in the second layer to sum the output of the previous layers). But the asymptotic complexity of some functions is now different! \n\n[Periodic function]\nImagine that we want to represent a function g : [0,l] \u2192 that is periodic with period 1. That is g(x - 1) = g(x) for all x \u2208 [1,l]. What we can do is to (i) represent g|_[0,1] in the output of a single neuron v in layer 2 (ii) represent shifted versions of g|_[0,1] (which are equal to g|_[1,2], g|_[2,3], \u2026 due to periodicity) in the outputs of neurons in layer 3 (iii) sum outputs of neurons from layer 3 in the single output neuron in layer 4. Assume moreover that g|_[0,1] has m changes of slope. Then observe that we implemented g fixing O(l+m) degrees of freedom. But g itself has m \u00b7 l changes of slope over the whole domain. \n\nThis representation gives an upper-bound for limiting complexity as there might be other ways to represent the function. \n\nBut because of Example\u00a0<ref> it is enough to arrive at a separation. Indeed if l \u2248 m then the asymptotic complexity of g for NN with 4 layers is smaller than for 2 layers, which is in \u03a9(m l). In words, we obtain a quadratic gain in terms of the number of samples needed to get the same generalization bound. \n\n\n\nWe leave it for future work to explore this direction in more depth (no pun intended).\n\n\n\u00a7 WHY LINEAR SCHEMES GENERALIZE POORLY\n\n\nIn Section\u00a0<ref> we've seen that for NNs our notion of complexity aligns well with natural notions of complexity. This, in the light of the connections to the PAC-Bayes bound, partly explains their good generalization. In this section we will show that for the case of linear schemes the complexity is basically independent of a function.  \n\nWe investigate  \nthe linear model _\u03b8^(L, o)={f_\u03b8(x): f_\u03b8(x) = \u2211_i=0^d-1_i b_i(x), x \u2208 = [-1, 1]}. Further let _x be the uniform distribution on [-1, 1]. We assume a prior on _i's to be iid Gaussians of mean 0 and variance \u03c3_w^2.\n\nWe will see that in this setting all realizable functions have the same complexity. This in the light of (<ref>) tells us that even if reality prefers simple functions the number of samples needed to get a non vacuous bound is as big as the one needed for the highest complexity function in the class.  In short: it is equally \u201ceasy\u201d to represent a \u201ccomplicated\u201d function as it is to represent a \u201csimple\u201d one. Therefore, given some samples, there is no reason to expect that linear models will fit a simple function to the data. Indeed, to the contrary. If the data is noisy, then linear models will tend to overfit this noise.\n\n\n\n \u00a7.\u00a7 Orthonormal Basis\n\n\nFor simplicity assume that the basis functions are the Legendre polynomials. I.e., we start with the polynomials {1, x, x^2, ...} and then create from this an orthonormal set on [-1, 1] via the Gram-Schmidt process.\n\n[Constant Function]\nLet g(x)=1/\u221a(2). This function is realizable. Indeed,\nit is equal to the basis function b_0(x).  Let us compute \u03c7^#(^(L, o),g,\n\u03f5^2).  If we pick all weights in f_(x) = \u2211_i=0^d-1_i b_i(x) equal to 0 except _0 equal to 1 then we get\ng(x).  Hence, taking advantage of the fact that the basis functions\nare orthonormal, we have\n\n    _x \u223c_x[(f_ - g(x))^2]  =\n    1/2\u222b_-1^1 (f_(x)-g(x))^2 dx  \n      \n    =    1/2\u2211_i=0^d-1 (_i-_{i=0})^2 \u222b_-1^1 b_i(x)^2 dx  \n    = 1/2\u2211_i=0^d-1 (_i-_{i=0})^2.\n\nSo we need to compute the probability \n\n    [:  1/2\u2211_i=0^d-1 (_i-_{i=0})^2 \u2264^2].\n\nRecall that our weights are iid Gaussians of mean 0 and variance \u03c3_w^2. Hence\n\n    \u2211_i=1^d-1_i^2  \u223c\u0393(d-1/2, 2 \u03c3_w^2 ),\n\nwhere \u0393(k, \u03b8) denotes the Gamma distribution with shape\nk and scale \u03b8.  It follows that the probability we are\ninterested in can be expressed as [:  1/2\u2211_i=0^d-1 (_i-_{i=0})^2 \u2264^2] = q(\u03ba=1, \u03c3_w, \u03f5), where\nq(\u03ba, \u03c3_w, \u03f5) =\n1/\u221a(2 \u03c0\u03c3_w^2)\u222b_0^\u03f5 F(\u03f5^2-x^2; d-1/2, 2 \u03c3_w^2) [e^-(\u03ba+x)^2/2 \u03c3_w^2+e^-(\u03ba-x)^2/2 \u03c3_w^2]dx.\n\nHere, F(x; k, \u03b8) denotes the cdf of the Gamma distribution\nwith shape k and scale \u03b8. From the above expression we can compute \u03c7^#(^(L, o), g(x) =\n1/\u221a(2),\u03f5^2), although there does not seem to be an elementary expression.\n\n\nFor non-negative \u03ba, \u03c3_w, and \u03f5\u2208 (0, 1] the function\nq(\u03b1, \u03c3_w, \u03f5) has the following properties:\n\n  *  Scaling: q(\u03ba, \u03c3_w, \u03f5) = \u03ba q(1, \u03c3_w/\u03ba, \u03f5/\u03ba)\n  *  Monotonicity in \u03ba: q(\u03ba, \u03c3_w, \u03f5) is non-increasing in \u03ba\n  *  Monotonicity in \u03c3_w: q(\u03ba, \u03c3_w, \u03f5) is non-increasing in \u03c3_w\n  *  Monotonicity in \u03f5: q(\u03ba, \u03c3_w, \u03f5) is non-decreasing in \u03f5\n  *  Limit: lim_\u03f5\u2192 0log(q(\u03ba, \u03c3_w, \u03f5))/log(\u03f5)=d\n\nIf we are just interested in \u03c7^#(^(L, o), g(x) = 1/\u221a(2)),\nwe can start from \u03c7^#(^(L, o), g(x) = 1/\u221a(2), \u03f5^2)\nor we can make use of (v) of Lemma\u00a0<ref> to get \n\n    \u03c7^#(^(L, o), g(x) = 1/\u221a(2))=d.\n\nTo see this result intuitively note that all weights\nhave to be fixed to a definite value in order to realize g(x).\n[Basis Function]\nAlthough we assumed in the above derivation that g(x)=b_0(x) the\ncalculation is identical for any g(x)=b_i(x), i=0, \u22ef, d-1.\nWe conclude that \u03c7^#(^(L, o), b_i(x)) does not depend\non i.  [Realizable Function of Norm 1]\nAssume that g(x)= \u2211_i=0^d-1_i b_i(x) with\n\u2211_i=0^d-1_i^2=1. In other words, the function\nis realizable and has squared norm equal to 1.\n\nIf we \u201crotate\u201d (orthonormal transform) our basis {b_i(x)}_i=0^d\ninto the new basis {b\u0303_i(x)}_i=0^d so that\ng(x)=b\u0303_0(x) then due to the rotational symmetry of our\nprior we are back to our first example.\n\nWe conclude that for any realizable function g(x) of norm 1,\n\u03c7^#(^(L, o), g(x), \u03f5^2) = \u03c7^#(^(L, o), b_0(x), \u03f5^2).[Realizable Function]\nAssume that g(x)= \u2211_i=0^d-1_i b_i(x) with\n\u2211_i=0^d-1_i^2=\u03ba^2. In other words, the\nfunction is realizable and has norm equal to \u03ba.\n\nUsing the scaling property of Lemma\u00a0<ref> we can write \n\n    \u03c7^#(_(\u03c3_w)^(L, o), g(x), \u03f5^2 ) \n        = -log(q(\u03ba, \u03c3_w, \u03f5)) \n        = -log(\u03ba q(1, \u03c3_w/\u03ba, \u03f5/\u03ba)) \n        = -log(\u03ba) + \u03c7^#(_(\u03c3_w/\u03ba)^(L, o), b_0(x), \u03f5^2/\u03ba^2),\n\nwhere we wrote _(\u03c3_w)^(L, o) to indicate that in the model\neach parameter's prior is a Gaussian with variance \u03c3_w^2.\n\nThis means that the complexity of a function changes depending on the norm of the vector of weights  that represent it. However if we are interested in the asymptotic complexity all functions (apart from the 0 function) have the same complexities as lim_\u2192 0log(\u03ba) /log() = 0, which leads to the next example.\n[Limiting Sharp Complexity]\nAssume that g(x)= \u2211_i=0^d-1_i b_i(x). Then\n\u03c7^#(^(L, o), g(x))=d.\n\n\nRecall that we showed (Example\u00a0<ref>) that for the case of 2-layer neural networks the limiting complexity depends strongly on the function and simpler functions - in a sense of number of changes of slope - have lower complexity. Here we see that for linear models basically all functions have the same complexity, which is equal to the number of basis functions in the model.\n\n[Unrealizable Function]\nGiven any function g(x), we can represent it as\ng(x)=g_\u22a5(x)+g_(x), where the two components are orthogonal\nand where g_(x) represents the realizable part. We then have\nthat \u03c7^#(_(\u03c3_w)^(L, o), g(x), \u03f5^2) is equal to\n\n    \u221e,    g_\u22a5(x)_2^2 > \u03f5^2, \n    \n    -log(q (1, \u03c3_w, \u221a(\u03f5^2-g_\u22a5(x)_2^2)) ),    g_\u22a5(x)_2^2 < \u03f5^2.\n\n\n \u00a7.\u00a7 Non-Orthonormal Basis\n[Non-Orthogonal Basis]\nIf the functions do not form an orthonormal basis but are independent, then we\ncan transform them into such base. After the transform the probability distribution is\nstill a Gaussian but no longer with independent components. Now the\n\"equal simplicity\" lines are ellipsoids.\n\nAnd if we have dependent components then we also still have Gaussians\nbut we are in a lower dimensional space.\n\n\n \u00a7.\u00a7 Summary\n\nWe have seen that for the linear model the complexity of a function\ng(x) only depends on the norm of the signal. This complexity measure is therefore only\nweakly correlated with other natural complexity measures. E.g., if\nthe basis consists of polynomials of increasing degrees and the reality is modeled by a function of low degree then the bound from (<ref>) is the same as when the reality is modeled by a high degree polynomial. It means that the number of samples needed for a good generalization bound is independent of g.\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 GENERALIZATION BOUND\n\n\nTo derive the bound from Theorem\u00a0<ref> in terms of \u201csharp complexity\u201d we first define a series of related notions that are helpful during the derivation.\n\n\n\n\nWe define the  empirical complexity of a function g as \n\n\n\n\n\n    \u03c7^E(g, _x, _\u03f5, \u03c3_y^2) \n       := -log[ ( \u222b_\u03b8 P(\u03b8) e^-1/2\u03c3_y^2N\u2211_n=1^N (g(x_n) + \u03b7_n - f_\u03b8(x_n))^2 d \u03b8) ],\n\nwhere we denoted by _x the x's part of  and by _ the particular realization of noise used for generating , i.e. \u03b7's.\n\nIn order to compute it, we integrate over the parameter space and weigh the prior P(\u03b8) by an exponential factor which is the smaller the further the function f_\u03b8 is from g on the given sample _x plus noise _. Recall that noise samples _\u03f5 come from an iid Gaussian zero-mean sequence of variance \u03c3_e^2. We then take the negative logarithm of this integral.\n\nThe  true complexity with noise is defined as\n\n    \u03c7^N(g, _x, \u03c3_y^2, \u03c3_^2) := \n       -log[ ( \u222b_\u03b8 P(\u03b8) e^-1/2\u03c3_y^2_x \u223c_x, \u223c\ud835\udca9(0,\u03c3_e^2) [(g(x) +  - f_\u03b8(x))^2] d \u03b8) ],\n\nwhere the sum has been replaced by an expectation using the underlying distribution of the input.\n\nThe  exponential complexity is\n\n    \u03c7(g, _x, \u03c3_y^2) := \n       -log[ ( \u222b_\u03b8 P(\u03b8) e^-1/2\u03c3_y^2_x \u223c_x [(g(x) - f_\u03b8(x))^2] d \u03b8) ].\n\nNote that\n\n    \u03c7(g, _x, \u03c3_y^2) + \u03c3_e^2/2 \u03c3_y^2 = \u03c7^N(g, _x, \u03c3_y^2, \u03c3_e^2).\n\n\nFinally, the  sharp complexity with noise is defined as\n\n    \u03c7^#N(g, _x, \u03c3_e^2, ^2) \n       := -log[ _\u03b8[ _x \u223c_x, \u223c\ud835\udca9(0,\u03c3_e^2) [(g(x) +  - f_\u03b8(x))^2] \u2264^2] ].\n\n\nThe following two lemmas establish some relationships between these notions of complexity.\n\nFor every _x, every g : \u2192, and ^2 > 0 we have:\n\n    \u03c7^#N(g, _x, \u03c3_e^2, ^2) = \u03c7^#(g, _x, ^2 - \u03c3_e^2).\n\n    \u03c7^#N(g, _x, \u03c3_e^2, ^2) \n        = -log[ _\u03b8[ _x \u223c_x, \u223c\ud835\udca9(0,\u03c3_e^2) [(g(x) +  - f_\u03b8(x))^2] \u2264^2 ] ] \n       = -log[ _\u03b8[ _x \u223c_x, \u223c\ud835\udca9(0,\u03c3_e^2) [(g(x) - f_\u03b8(x))^2] \u2264^2 - \u03c3_e^2 ] ] \n       = \u03c7^#(g, _x, ^2 - \u03c3_e^2),\n\nwhere in the second equality we write (g(x) + - f_\u03b8(x))^2 as the sum of (g(x)-f_\u03b8(x))^2, 2(g(x) - f_\u03b8(x)) and ^2 and use the fact that [] = 0 and [^2] = \u03c3_e^2.\n\nFor every _x, every g : \u2192, and \u03c3_y^2, \u03c3_e^2, ^2 > 0 we have:\n\n    \u03c7^N(g, _x, \u03c3_y^2, \u03c3_e^2) \u2264\u03c7^#N(g, _x, \u03c3_e^2, ^2) + ^2/2\u03c3_y^2.\n\n    \u03c7^#N(g, \u03c3_e^2,^2) \n       =\n    -log( \u222b_\u03b8 P(\u03b8) 1{_x \u223c_x, \u223c\ud835\udca9(0,\u03c3_e^2) [(g(x) +  - f_\u03b8(x))^2] \u2264^2 } d \u03b8) \n       \u03b1>0= -log( \u222b_\u03b8 P(\u03b8) 1{\u03b1/2\u03c3_y^2_x \u223c_x, \u223c\ud835\udca9(0,\u03c3_e^2) [(g(x) +  - f_\u03b8(x))^2] \u2264\u03b1^2/2\u03c3_y^2} d \u03b8) \n       e^ x\u22651{x \u2265 0 }\u2265-log( \u222b_\u03b8 P(\u03b8) e^\u03b1^2/2\u03c3_y^2 -\u03b1/2\u03c3_y^2_x \u223c_x, \u223c\ud835\udca9(0,\u03c3_e^2) [(g(x) +  - f_\u03b8(x))^2] d \u03b8) \n       = \u03c7^N(g, \u03c3_y^2/\u03b1, \u03c3_e^2) - \u03b1^2/2\u03c3_y^2.\n\n\nThe sharp complexity is very convenient to work with. Hence we will formulate our final bound in terms of the sharp complexity. The reason we call it  sharp complexity is that the region of \u03b8 we integrate over is defined by an indicator function whereas for the true complexity the \u201cboundary\u201d of integration is defined by a smooth function.\n\nLet us now look more closely at the divergence where we assume the data model (<ref>) and that the true hypothesis is g. We have\n\n    D(Q  P) \n       = \u222bP(\u03b8) e^- 1/2 \u03c3_y^2\u2211_n=1^N (y_n - f_\u03b8(x_n))^2/\u222b P(\u03b8') e^- 1/2 \u03c3_y^2\u2211_n=1^N (y_n - f_\u03b8'(x_n))^2 d \u03b8'\u00b7\n       \u00b7log(\n     e^- 1/2 \u03c3_y^2\u2211_n=1^N (y_n - f_\u03b8(x_n))^2/\u222b P(\u03b8') e^- 1/2 \u03c3_y^2\u2211_n=1^N (y_n - f_\u03b8'(x_n))^2 d \u03b8') d \u03b8\n       \u2264\u03c7^E(g,_x, _\u03f5,\u03c3_y^2/N)- N/2\u03c3_y^2 L_(Q),\n\nwhere in the last inequality we used the fact that we use a clipped version of a square loss.\nTherefore the expectation over S \u223c^N of the square root term of the right-hand side of the PAC-Bayes bound (<ref>) can be upper-bounded as\n\n    _\u223c^N[C\u221a(D(Q  P)/2 N)] \n       By (<ref>)\u2264_\u223c^N[C \u221a(\u03c7^E(g, _x,_, \u03c3^2_y/N) - N/2\u03c3_y^2 L_(Q) /2 N)] \n       \u221a(\u00b7) concave\u2264C/\u221a(2)\u221a(_\u223c^N[\u03c7^E(g, _x,_, \u03c3^2_y/N) ]/N -L/2\u03c3_y^2),\n\nwhere we denoted _\u223c^N[L_(Q)] by L\u0302. Before we proceed we state a helpful lemma.\n\n\nLet X and Y be independent random variables and f(X, Y) be a non-negative function. Then\n\n    _X [ ln( _Y [ e^-f(X, Y)] )  ]\n    \u2265ln( _Y[e^-_X[f(X, Y)]]).\n\nWe limit our proof to the simple case where the distributions are discrete and have a finite support, lets say from {1, \u22ef, I}. We claim that for 1 \u2264 j <I,\n\n    (\u2211_i=1^j p(X=x_i))\n    ln( _Y[e^-\u2211_i=1^jp(X=x_i) f(x_i, Y)/\u2211_i=1^j p(X=x_i) ]) + \n    \u2211_i=j+1^I p(X=x_i) [ ln( _Y [ e^-f(x_i, Y)] )  ] \n    \u2265   \n     (\u2211_i=1^j+1 p(X=x_i))\n    ln( _Y[e^-\u2211_i=1^j+1 p(X=x_i) f(x_i, Y)/\u2211_i=1^j+1 p(X=x_i) ]) +\n    \u2211_i=j+2^I p(X=x_i) [ ln( _Y [ e^-f(x_i, Y)] )  ].\n\nThis gives us a chain of inequalities. Note that the very first term in this chain is equal to the left-hand side of the desired inequality and the very last term is equal to the right-hand side of the inequality. \n\nConsider the j-th such inequality. Cancelling common terms, it requires us to prove\n\n    (\u2211_i=1^j p(X=x_i))\n    ln( _Y[e^-(\u2211_i=1^jp(X=x_i) f(x_i, Y)/\u2211_i=1^j p(X=x_i) )]) + \n     p(X=x_j+1) [ ln( _Y [ e^-f(x_j+1, Y)] )  ] \n    \u2265   \n     (\u2211_i=1^j+1 p(X=x_i))\n    ln( _Y[e^-(\u2211_i=1^j+1p(X=x_i) f(x_i, Y)/\u2211_i=1^j+1 p(X=x_i) )]).\n\nTaking the prefactors into the logs, combining the two log terms on the left-hand side, and finally cancelling the logs, the claimed inequality is true iff\n\n    _Y[e^-\u2211_i=1^j p(X=x_i) f(x_i, Y)/\u2211_i=1^j p(X=x_i) ]^\u2211_i=1^j p(X=x_i)/\u2211_i=1^j+1 p(X=x_i)_Y [ e^-f(x_j+1, Y)] ^p(X=x_j+1)/\u2211_i=1^j+1 p(X=x_i)\u2265   _Y [e^-\u2211_i=1^j+1 p(X=x_i) f(x_i, Y)/\u2211_i=1^j+1 p(X=x_i) ].\n\nBut this statement is just an instance of the Hoelder inequality with 1/p+1/q=1, where 1/p=\u2211_i=1^j p(X=x_i)/\u2211_i=1^j+1 p(X=x_i) and 1/q=p(X=x_j+1)/\u2211_i=1^j+1 p(X=x_i).\n\n\n\nNow we bound the complexity term from (<ref>) further. We have for every ^2 > 0\n    _S \u223c^N[\u03c7^E(g, _x,_, \u03c3_y^2/ N)] \n       = -_S \u223c^N[ log( \u222b_\u03b8 P(\u03b8) e^-1/2\u03c3^2_y\u2211_n=1^N (g(x_n) + _n  - f_\u03b8(x_n))^2 d \u03b8)   ] \n       Lem\u00a0<ref>\u2264 -log( \u222b_\u03b8 P(\u03b8) e^-N/2\u03c3^2_y_x \u223c_x\u223c\ud835\udca9(0,\u03c3_e^2) [(g(x) +  - f_\u03b8(x))^2] d \u03b8) \n       = \u03c7^N(g, _x, \u03c3^2_y/N, \u03c3_e^2)\n    Lem\u00a0<ref>\u2264\u03c7^#N(g, _x, \u03c3_e^2, ^2) + ^2 N /2 \u03c3^2_y\n       Lem\u00a0<ref>=\u03c7^#(g, _x, ^2 - \u03c3_e^2) + ^2 N /2 \u03c3^2_y.\n\nHence by combining (<ref>) and (<ref>) we get that for every ^2 > 0 the expectation over S \u223c^N of the PAC-Bayes bound can be bounded as\n\n    _\u223c^N[L_(Q) + C\u221a(D(Q  P) /2 N)] \n       \u2264L + C/\u221a(2)\u221a(\u03c7^#(g, _x, ^2 - \u03c3_e^2)/N  + 1/2\u03c3^2_y(^2 - L)).\n\n\nLet \u03b2\u2208 (0,1]. Recall that parameter \u03c3_y^2 is chosen freely by the learning algorithm. By the assumption of the theorem we have \n\n\n    L_(P) \u2265 2\u03c3_e^2.\n\nBecause g \u2208supp(P), which in words means that g is realizable with prior P, then\n\n    lim_\u03c3_y^2 \u2192 0L   = lim_\u03c3_y^2 \u2192 0_\u223c^N [ L_(Q) ]  \n       = _\u223c^N[ lim_\u03c3_y^2 \u2192 0 L_(Q) ]  \n       \u2264_\u223c^N L_(g) \n       = \u03c3_e^2 .\n\n\n\n\n\n\nwhere in the second equality we used Lebesgue dominated convergence theorem and in the inequality we used the fact that the smaller \u03c3_y^2 gets the bigger the penalty on \u2211_n (y_n - f_\u03b8(x_n))^2 in Q, which means that, in the limit, L_(Q) is smaller than L_(h) for every fixed h \u2208supp(P) and in particular for g. \n\nOn the other hand, by an analogous argument, we have\n\n    lim_\u03c3_y^2 \u2192\u221eL   = _\u223c^N[ L_(P) ] \n       = _\u223c^N[  _\u03b8\u223c P[1/N\u2211_i=1^N \u2113(f_\u03b8, y_n) ] ] \n       =  _\u03b8\u223c P[ _\u223c^N[1/N\u2211_i=1^N \u2113(f_\u03b8, y_n)  ] ] \n       = L_(P)  \n       \u2265 2 \u03c3_e^2,\n\nwhere we used the independence of P and  in the third equality and (<ref>) in the inequality.\n\nEquations (<ref>) and (<ref>) and the fact that L is a continuous function of \u03c3_y^2 give us that there exists \u03c3_alg^2 > 0 such that\n_\u223c^N[L_(Q(\u03c3_alg^2)) ] = (1 + \u03b2) \u03c3_e^2,\n\nwhere we wrote Q(\u03c3_alg^2) to explicitly express the dependence of Q on \u03c3_y^2. With this choice for \u03c3_y^2 and setting ^2 = (1+\u03b2)\u03c3_e^2 applied to (<ref>) we arrive at the statement of Theorem\u00a0<ref>. Note that with this choice of parameters term 1/2\u03c3_y^2(^2 - L) from (<ref>) is equal to 0.\n\n\n\n\n\u00a7 OMITTED PROOFS\n\nLet {x_i}_i=1^k be a set of real numbers. For i=1, \u22ef, k, define the partial sums X_i=\u2211_j=1^i x_j. Then\n\n    \u2211_i=1^k X_i^2 \u22651/8\u2211_i=1^k x_i^2.\n\nDefine X_0=0. Note that for i=1, \u22ef, k, X_i = X_i-1+x_i. Hence if |X_i-1| \u22641/2 |x_i| then |X_i|\u22651/2 |x_i| so that X_i^2\u22651/4 x_i^2. And if |X_i-1| \u22651/2 |x_i| then X_i-1^2\u22651/4 x_i^2. Therefore, X_i-1^2+X_i^2 \u22651/4 x_i^2. Summing the last inequality over i=1, \u22ef, k, and adding X_k^2 to the left hand side we get 2 \u2211_i=1^k X_i^2 \u22651/4\u2211_i=1^k x_i^2.\n\nLet f(x)= \u2211_i=1^k w_i [x-b_i]_+, where 0 \u2264 b_1 \u2264\u22ef\u2264 b_k \u2264 1 = b_k+1. For i=1, \u22ef, k, define the partial sums W_i=\u2211_j=1^i w_j. Then \n\n    f^2 \u22651/12\u2211_i=1^k W_i^2 (b_i+1 - b_i)^3.\n\nNote that there are k non-overlapping intervals, namely [b_1, b_2], \u22ef, [b_k, 1], where the function is potentially non-zero. On the i-th interval the function is linear (or more precisely, affine) with a slope of W_i and, by assumption, the interval has length b_i+1-b_i. On this interval the integral of f(x)^2 must have a value of at least 1/12 W_i^2 (b_i+1-b_i)^3. The last statement follows by minimizing the integral of the square of an affine function with slope W_i over the choice of the parameters.\n\nLet f_\u03b8(x)= \u2211_i=1^k w_i [x-b_i]_+, where 0 \u2264 b_1 \u2264\u22ef\u2264 b_k < +\u221e.\n\nIf f_\u03b8^2 < 1/12(k+1)^5 then there exists \u03b8^* such that f_\u03b8^*\u2261_[0,1] 0 and\n\n    \u03b8 - \u03b8^*^2 \u2264 O ( k^13/5f_\u03b8^4/5).\n\nStarting with the parameter \u03b8 that defines the function f_\u03b8(x), we define a process of changing it until the resulting function is equal to the zero function on [0, 1]. Most importantly, this process does not change \u03b8 too much compared to the norm of f_\u03b8(x). \n\nNote that there are two ways of setting the function to 0 on a particular interval. Either, we can make the length of the interval to be 0. This requires to change one of the bias terms by the length of the interval. Or we set the slope of this interval to be 0 (assuming that the function is already 0 at the start of the interval. Our approach uses both of those mechanisms. Let \u03b8^0 \u2190\u03b8. The process has two phases. In the first phase we change the bias terms and in the second phase we change the weights. For x \u2208 [0,1], define the partial sums W(x)=\u2211_j: b_j \u2264 x w_j. \n\n\n\n  \nFirst phase. Let \nS := {[b_1,b_2], \u2026, [b_k-1,b_k],[b_k,1]} and  S_b := {[l,r] \u2208 S : r - l < |W(l)| }. Let {[l_0,r_0],[l_1,r_1], \u2026, [l_i,r_i]}\u2286 S_b be a maximal continuous subset of intervals in S_b. That is, for all j \u2208 [i], r_j = l_j+1 and the intervals ending at l_0 and starting at r_i are not in S_b. Perform the following: for all b_j \u2208 [l_0,r_i] set b_j \u2190 r_i. We do this operation for all maximal, continuous subsets of S_b. This finishes the first phase. Call the resulting vector of parameters \u03b8^1. We bound\n\n    \u03b8^0 - \u03b8^1^2 \n       \u2264 k (\u2211_[l,r] \u2208 S_b (r-l) )^2 \n       \u2264 k^13/5(\u2211_[l,r] \u2208 S_b (r-l)^5 )^2/5      By the Power Mean Inequality\n       \u2264 k^13/5(\u2211_[l,r] \u2208 S_b (r-l)^3 W(l)^2 )^2/5      By definition of  S_b \n       \u2264 k^13/5 (12 f_\u03b8^2)^2/5      By Lemma\u00a0<ref>\n\n\n  \nSecond phase. Observe that f_\u03b8^1 has the following properties. For every x \u2208 [0,1] \u2216\u22c3_[l,r] \u2208 S_b [l,r) we have W^1(x) = W^0(x). It is enough to make W(l) = 0 for all [l,r] such that [l,r] \u2208 S \u2216 S_b.  Let i_1 < i_2 < \u2026 < i_p be all i_j's such that [b_i_j, b_i_j+1] \u2208 S \u2216 S_b. Applying Lemma\u00a0<ref> to {W_i_1, W_i_2 - W_i_1, \u2026, W_i_p - W_i_p-1} we get that\n\n    8\u2211_j=1^p W_i_j^2 \u2265 W_i_1^2 + (W_i_2 - W_i_1)^2 + \u2026 (W_i_p - W_i_p-1)^2\n\nThe RHS of (<ref>) gives an upper-bound on the \u00b7^2 norm distance needed to change w_i's in \u03b8^1 so that all W_i_j = 0. It is because we can change w_1, \u2026, w_i_1 by at most W_i_1^2 to make W_i_1 = 0 and so on for i_2, \u2026, i_p. Call the resulting vector of parameters \u03b8^2. We bound the change in the second phase\n\n    \u03b8^1 - \u03b8^2^2\n       \u2264 8 \u2211_j=1^p W_i_j^2       (<ref>)\n       \u2264 8k (1/k\u2211_j=1^p |W_i_j^5| )^2/5      Power Mean Inequality\n       = 8k^3/5( \u2211_i : [b_i, b_i+1] \u2208 S \u2216 S_b  |W_i^5| )^2/5      By definition\n       \u2264 8k^3/5( \u2211_i : [b_i, b_i+1] \u2208 S \u2216 S_b  (b_i+1 - b_i)^3 |W_i^2| )^2/5      By definition of  S_b \n       \u2264 8 k^3/5(12 f_\u03b8^2 )^2/5      By Lemma\u00a0<ref>.\n\nWe conclude by\n\n    \u03b8^0 - \u03b8^2^2 \n       \u2264 4 max(\u03b8^0 - \u03b8^1^2, \u03b8^1 - \u03b8^2^2 )       Triangle inequality\n       \u2264 96 k^13/5(f_\u03b8^2 )^2/5      (<ref>) and (<ref>)\n\nLet S^0 = {[b_1, b_2], \u2026, [b_k,1]} be the set of  active intervals at time t=0. I.e., initially all intervals are active. For t \u2265 0\n    if there exists an i such that  [b_i^t,b_i+1^t] \u2208 S^t  and  b_i+1^t - b_i^t < |W^t(b_i^t)|\n\nthen perform \n\n    \u03b8^t+1\u2190\u03b8^t, \n       \u03b1\u2190 b_i^t,  \u03b2\u2190 b_i+1^t,\n       for every only rightendpoint j  such that  b_j^t = \u03b2 set  b^t+1_j \u2190\u03b1, \n       S^t+1\u2190 S^t \u2216{[b^t_i, b^t_i+1] }.\n\nIn each step of the process one interval is removed from S, hence the process terminates in at most t_max\u2264 k steps.\nThe following properties hold for every t < t_max:\n\n  * \u03b8^t+1 - \u03b8^t^2\u2264 2k \u00b7 (b^t_i+1 - b^t_i)^2 < 2k (W^t(b_i^t))^2, as at most 2k bias terms were changed, \n  *  for every x \u2208 [0,1] \u2216 [b_i^t, b_i+1^t) we have W^t+1(x) = W^t(x), i.e. in the t-th step the slope changes only at [b_i^t, b_i+1^t), \n  *  for every x \u2208 [b_i^t, b_i+1^t) we have W^t+1(x) = W^t(b_i+1^t). \nNote that, by construction, for every [b_i^t_max, b_i+1^t_max] \u2208 S^t_max we have \n\n    b_i+1^t_max - b_i^t_max\u2265 |W^t_max(b_i^t_max)|.\n\nWe bound\n\n    12 \u222b_0^1 f(x)^2  dx  \n       \u2265\u2211_i=1^k W_i^2 (b_i+1 - b_i)^3        Lemma\u00a0<ref>\n       \u2265\u2211_i : [b_i^t_max, b_i+1^t_max] \u2208 S^t_max W_i^2 (b_i+1 - b_i)^3 + \u2211_i : [b_i^t_max, b_i+1^t_max] \u2209S^t_max W_i^2 (b_i+1 - b_i)^3 \n       = \u2211_i : [b_i^t_max, b_i+1^t_max] \u2208 S^t_max W^t_max(b_i^t_max)^2 (b_i+1^t_max - b_i^t_max)^3 \n          + \u2211_t=1^t_max-1\u2211_i : [b_i^t-1, b_i+1^t-1] \u2208 S^t-1\u2216 S^t  W^t-1(b^t-1_i)^2(b^t-1_i+1 - b^t-1_i)^3       By Property (<ref>)\n       \u2265\u2211_i : [b_i^t_max, b_i+1^t_max] \u2208 S^t_max |W^t_max(b_i^t_max)^5| + \u2211_t=1^t_max-1\u2211_i : [b_i^t-1, b_i+1^t-1] \u2208 S^t-1\u2216 S^t (b^t-1_i+1 - b^t-1_i)^5       By (<ref>) and (<ref>)\n       = \u2211_i : [b_i, b_i+1] \u2208 S^t_max |W_i^5| + \u2211_i : [b_i^t_max, b_i+1^t_max] \u2209S^t_max (b_i+1 - b_i)^5       By Property (<ref>)\n\nWe bound the change in the first phase\n\n    \u03b8 - \u03b8^t_max^2 \n       \u2264(\u2211_t=1^t_max\u03b8^t-1 - \u03b8^t)^2       Triangle inequality\n       \u2264 2( \u2211_i : [b_i^t_max, b_i+1^t_max] \u2209S^t_max k^1/2(b_i+1 - b_i) )^2       By Property (<ref>)\n       \u2264 2k^3 (1/k\u2211_i : [b_i^t_max, b_i+1^t_max] \u2209S^t_max (b_i+1 - b_i)^5 )^2/5      Power Mean Inequality\n       \u2264 6k^13/5(\u222b_0^1 f_\u03b8(x)^2  dx )^2/5      (<ref>)\nRUnow sure what we use above; what does 1 refer to? it seems that we have several 1s and 2s references around\n\nNow we show how to change the w_i's in \u03b8^t_max to make the function the 0 function - this is the second phase. By Properties (<ref>) and (<ref>) it is enough to make W_i = 0 for all i such that [b_i, b_i+1] \u2208 S^t_max. Let i_1 < i_2 < \u2026 < i_p be all i_j's such that [b_i_j, b_i_j+1] \u2208 S^t_max. Applying Lemma\u00a0<ref> to {W_i_1, W_i_2 - W_i_1, \u2026, W_i_p - W_i_p-1} we get that\n\n    8\u2211_j=1^p W_i_j^2 \u2265 W_i_1^2 + (W_i_2 - W_i_1)^2 + \u2026 (W_i_p - W_i_p-1)^2\n\nThe RHS of (<ref>) gives an upper-bound on the \u00b7^2 norm distance needed to change w_i's in \u03b8^t_max so that all W_i_j = 0. It is because we can change w_1, \u2026, w_i_1 by at most W_i_1^2 to make W_i_1 = 0 and so on for i_2, \u2026, i_p. Call the resulting vector of parameters \u03b8^*. We bound the change in the second phase\n\n    \u03b8^t_max - \u03b8^*^2\n       \u2264 8 \u2211_j=1^p W_i_j^2       (<ref>)\n       \u2264 8k (1/k\u2211_j=1^p |W_i_j^5| )^2/5      Power Mean Inequality\n       = 8k^3/5( \u2211_i : [b_i, b_i+1] \u2208 S^t_max |W_i^5| )^2/5      By definition\n       \u2264 24 k^3/5(\u222b_0^1 f_\u03b8(x)^2  dx )^2/5      (<ref>).\n\nWe conclude by\n\n    \u03b8 - \u03b8^*^2 \n       \u2264 4 max(\u03b8 - \u03b8^t_max^2, \u03b8^t_max - \u03b8^*^2 )       Triangle inequality\n       \u2264 96 k^13/5(\u222b_0^1 f_\u03b8(x)^2  dx )^2/5      (<ref>) and (<ref>)\n[Withb^(2)]\nLet R \u2208_+, \u03b8\u2208 B_R \u2229(P) be such that f_\u03b8(x)= b^(2) + \u2211_i=1^k w_i [x-b_i]_+, where 0 \u2264 b_1 \u2264\u22ef\u2264 b_k < +\u221e. If f_\u03b8^2 is small enough, where the bound depends only on R and k, then there exists \u03b8^* such that f_\u03b8^*\u2261_[0,1] 0 and\n\n    \u03b8 - \u03b8^*^2 \u2264 O ( k^5 R^4/5f_\u03b8^2/5) .\n\nLet ^2 = f_\u03b8^2. For x \u2208, define the partial sums W(x)=\u2211_j: b_j \u2264 x w_j. \n\n\n\n\nConsider the following cases:\n\n\n  \nCase |b^(2)| \u2264^1/2.  We perform \u03b8' \u2190\u03b8, b^(2)'\u2190 0. By triangle inequality we can bound\nf_\u03b8'||^2 \u2264(  + |b^(2)| )^2 \u2264 4 . We apply Lemma\u00a0<ref> to \u03b8' to obtain \u03b8^* such that f_\u03b8^*\u2261_[0,1] 0 and \u03b8' - \u03b8^*^2 \u2264 O(k^13/5f_\u03b8'||^4/5) \u2264 O(k^13/5^2/5). We conclude by noticing\n\n    \u03b8 - \u03b8^*^2 \n       \u2264(\u03b8 - \u03b8' + \u03b8' - \u03b8^*)^2       Triangle inequality\n       \u2264(^1/2 + O(k^13/10^1/5) )^2 \n       \u2264 O(k^13/5f_\u03b8^2/5)       As ^2 \u2264 1.\n\n\n  \nCase |b^(2)| > ^1/2.  Without loss of generality assume that b^(2)>0. There exists x_0 \u2208 (0,/4), such that f_\u03b8(x_0) = b^(2)/2, as otherwise \n^2 \u2265\u222b_0^/4 f_\u03b8(x)^2  dx \u2265\u222b_0^/4 (b^(2))^2 / 4  dx > ^2. By the mean value theorem there exists x_1 \u2208 (0,x_0) \u2216{b_1, \u2026, b_k} such that \n\n    f_\u03b8(x_1) \u2208 [b^(2)/2, b^(2)]  and  W(x_1) \u2264f_\u03b8(x_0) - f_\u03b8(0)/x_0 - 0\u2264 -4b^(2)/2\u2264 -2^-1/2.\n\nWe perform the following transformation\n\n    \u03b8' \u2190\u03b8, \n       for every  i  such that  b_i < x_1  do  b'_i \u2190 b_i - x_1 + f_\u03b8(x_1)/W(x_1), \n       i_0 \u2190_i b_i > x_1, \n       b'_i_0\u2190 0.\n\nObserve that we shifted all b_i's exactly so that f_\u03b8'(0) = 0. Note also that b_i_0\u2264 4 as otherwise by Lemma\u00a0<ref>^2 \u2265\u222b_x_1^b_i_0 f_\u03b8(x)^2  dx \u22651/12 W(x_1)^2 (b_i_0 - x_1)^3 > 1/12 4^-1 (3)^3 \u2265^2.\n\nBy (<ref>) we can bound \n\n    \u03b8 - \u03b8'^2 \n    \u2264 k (-x_1 + f_\u03b8(x_1)/W(x_1))^2 + 16^2 \u2264 O(k^2).\nf_\u03b8 is R-Lipshitz wrt to b_i's in B_R thus the triangle inequality and (<ref>) gives \n\n    f_\u03b8'^2\n    \u2264  (f_\u03b8 + O(R k^3/2))^2 \n    \u2264 O(R^2 k^5 ^2).\n\nWe apply Lemma\u00a0<ref> to f_\u03b8', after we removed all b'_i < 0 and set w'_i_0\u2190\u2211_j \u2264 i_0 w_j. Lemma\u00a0<ref> might require to change w'_i_0, which we can realize with the same cost by changing {w_j : j \u2264 i_0}. Thus Lemma\u00a0<ref> and (<ref>) gives us that there exists \u03b8^* such that f_\u03b8^*\u2261_[0,1] 0 and \u03b8' - \u03b8^*^2 \u2264 O(k^13/5 k^2 R^4/5^4/5). We conclude by using the triangle inequality and (<ref>) to get\n\u03b8 - \u03b8^*^2 \u2264 O(k^23/5 R^4/5f_\u03b8^4/5).\nLet R \u2208_+, \u03b8\u2208 B_R \u2229(P) be such that f_\u03b8(x)= b^(2) + \u2211_i=1^k w_i [x-b_i]_+ and g(x) = b + \u2211_i=1^c v_i [x - t_i]_+, where c \u2264 k, 0 \u2264 b_1 \u2264\u22ef\u2264 b_k < +\u221e, 0 < t_1 < \u2026 < t_c < 1 and v_1,\u2026,v_c \u2260 0. If g - f_\u03b8^2 is small enough,  where the bound depends only on g,R and k, then there exists \u03b8^* such that f_\u03b8^*\u2261_[0,1] g and\n\n    \u03b8 - \u03b8^*^2 \u2264 O ( k^7 R^4/5g - f_\u03b8^2/5) .\n\nConsider a model on c+k \u2264 2k neurons represented as\n\n    h_\u03b8 := (b^(2) - b) + \u2211_i=1^k w_i [x-b_i]_+ - \u2211_i=1^c v_i [x - t_i]_+,\n\nwhere, to distinguish it from \u03b8, we denoted by  the set of parameters of h. Observe that h^2 = g - f_\u03b8^2. By Lemma\u00a0<ref> there exists ^* such that h_^*\u2261_[0,1] 0 and - ^*^2 \u2264 O ( k^5 R^4/5g - f_\u03b8^2/5). If  is small enough then the parameters in ^* corresponding to v_i's are all still all non-zero and the bias terms corresponding to t_i's are still all different. As h_^*\u2261_[0,1] 0 it implies that for every i \u2208 [c] there is a set of bias terms corresponding to b_j's that are exactly at where t_i was moved. Let \u03c0 : [c] \u2192 2^[k] be the mapping from t_i's to subsets of b_i's certifying that. \n\nWe define \u03b8^* such that f_\u03b8^*\u2261_[0,1] g as the result of two steps. First, changing \u03b8 as its corresponding parameters were changed in the transition \u2192^*. Second, changing the parameters as v_i's and t_i's are changed in ^* \u2192 under the map \u03c0. Observe that \u03b8 - \u03b8^*^2 \u2264 k^2  - ^*^2. It is because in the second step we move at most k bias terms for every parameter corresponding to t_i.     \n\n\nProof of Lemma\u00a0<ref>\n\n\n\nLet R \u2208_+. Notice that f_\u03b8 is R^2-Lipschitz with respect to each of its parameters, when restricted to a ball B_R. This implies that for all > 0\n    (A_g + B_) \u2229 B_R \u2286{\u03b8 : g - f_\u03b8^2 \u2264 R^4 ^2 }.\n\nOn the other hand by Lemma\u00a0<ref> we have that for small enough \n    {\u03b8 : g - f_\u03b8^2 \u2264^2 }\u2229 B_R \u2229(P) \u2286 A_g + B_O ( k^7/2 R^2/5^1/5) \u2286 A_g + B_C(k,R)^1/5,\n\nfor some function C.\n\nNext we prove (<ref>). Let \u03b8\u2208 B_R be such that g - f_\u03b8^2 \u2264^2. Let \u03b7(\u0394, W) denotes the minimum \u2113_2 difference on [-\u0394, \u0394] between a linear function and a two-piece linear function that has a change of slope of W at 0, i.e.\n\u03b7(\u0394,W) = min_a,b\u222b_-\u0394^0 (ax + b)^2 dx + \u222b_0^\u0394 (ax+b - W x)^2  dx. Solving the minimization problem we get\n\n    \u03b7(\u0394, W) = \u0394^3 W^2/24.\n\n \nWe proceed by changing \u03b8 in phases to arrive at an exact representation of g while incurring only a small change to \u03b8 in the \u00b7^2 norm. In phase 1 we make sure that f\u201d roughly agrees with g\u201d at t_1, \u2026, t_c, then, in phase 2, we make sure that the agreement is exact and finally, in phase 3, we enforce agreement of f and g on whole [0,1].\n\n\n\n  \nPhase 1. \n\nWe perform the following transformation\n\n    \u03b8' \u2190\u03b8, \n       for every  i \u2208 [1,c]  such that  |v_i| \u2265^1/2 do \n              for every  j \u2208 [1,k]  such that  |_j^(1) - t_i| \u2264 4^1/3 do \n                     _j^(1)'\u2190 t_i,\n\nFirst note that every bias term is changed at most once because the intervals [t_i - 4^1/3,t_i + 4^1/3] don't intersect by assumption that = o(\u03ba^3). After this transformation the following holds. For every i \u2208 [1,c] we have |f\u201d_\u03b8' - v_i| \u2264^1/2\nObserve that there exists _j^(1) such that |_j^(1) - t_i| \u2264 4^1/3 as otherwise the cost incurred to g - f_\u03b8^2 on [t_i - 4^1/3, t_i + 4^1/3] is at least 64/24^2. Note that we implicitly assumed that < 1/4\u03ba^3.\n\nIf we perform \u03b8' \u2190\u03b8, _i^(1)'\u2190 t_i then \u03b8 - \u03b8'^2 \u2264 16^2/3 and \n\n    g - f_\u03b8'^2 \n       \u2264 (g - f_\u03b8 + f_\u03b8 - f_\u03b8')^2 \n       \u2264 ( + 4 R^2 ^1/3)^2       f is R^2-Lipschitz in B_R with respect to _i^(1)\n       \u2264^2 + 8^4/3 R^2 + 16 R^4 ^2/3\n       \u2264 32 R^4 ^2/3.\nI think we need to be careful here. All operations should be done at the same time\nWe can view the transformation \u03b8\u2192\u03b8' as an operation after which we have a new target function g' = g - _i^(2)_i^(1) [x - t_i]_+ and a new model for f, where we drop the i-th node. We apply the operation for as long as possible. This process terminates because in each step we remove one node. After the process is finished, if we denote the resulting set of parameters by \u03b8\u201d, we have that for every i \u2208 [1,c]\n|g\u201d(t_i) - f_\u03b8\u201d\u201d(t_i)| < ^1/2.\n\nMoreover by an analogous argument to (<ref>) we have that g - f_\u03b8\u201d^2 \u2264 O( k R^4 ^2/3 ). We also have \u03b8 - \u03b8\u201d^2 \u2264 O( k ^2/3).\n\n\n\n  \nPhase 2. In this phase we change \u03b8\u201d further to obtain \u03b8\u201d' so that for every j \u2208 [1,c]g\u201d(t_j) = f_\u03b8\u201d'\u201d(t_j). Let j \u2208 [1,c] and let S_j := {i \u2208 [1,c] : _i^(1)\u201d = t_j}. Let i \u2208 S_j. We can change each of w_i^(2)\u201d, w_i^(1)\u201d by at most ^1/2 in the \u00b7^2 norm so that \u2211_i \u2208 S_j w_i^(2)\u201d  w_i^(1)\u201d = f_\u03b8\u201d\u201d(t_j) = g\u201d(t_j). We apply such a transformation for every j \u2208 [1,c] and call the result \u03b8\u201d'.  The result satisfies \u03b8 - \u03b8\u201d'^2 \u2264 O(k ^2/3) + 2k ^1/2\u2264 O(k ^1/2), \n    g - f_\u03b8\u201d'^2 \n       \u2264 O( k R^4 ^2/3 ) + k(R + ^1/4)^4 ^1/2\n       \u2264 O( k R^4 ^1/2 )       As ^1/4 < R,\n\nwhere in the first inequality we used the fact that f_\u03b8 is R-Lipshitz with respect to w_i^(2) in B_R. \n\n\n\n  \nPhase 3. Let S := {i \u2208 [1,k] : _i^(1)\u201d'\u2208{t_1, \u2026, t_c }}. Let \u03b8^0 represent a model where the weights are equal to \u03b8\u201d' but all nodes in S are removed. We will change \u03b8^0 so that it represents the 0 function. By definition\n\n    f_\u03b8^0^2 \u2264 O( k R^4 ^1/2 ).\n\nWe would like to now use Lemma\u00a0<ref>. But note that in this lemma we assumed that the model is b^(2) +\u2211_i=1^k w_i [x-b_i]_+ not \u2211_i=1^k _i^(2)\u00b7_i^(1)[x - _i^(1)]_+ + b^(2). Let i \u2208 [1,k]. If w_i was changed by \u03b4^2 in the \u00b7^2 norm then we can realize the same effective change in _i^(2)\u00b7_i^(1) by changing the weight with the smaller absolute value by at most \u03b4 +\u03b4^2 in the \u00b7^2 norm. Thus Lemma\u00a0<ref> and (<ref>) give us that there exists \u03b8^* such that f_\u03b8^*\u2261_[0,1] 0 and\n\u03b8 - \u03b8^*^2 \u2264 O (k^5 R^4/5 k^1/5 R^4/5^1/10) \u2264 O (k^6 R^8/5^1/10).\n\nTo finish the proof we bound\n\n    \u03c7^#(g, U([0,1])) \n       = lim_\u03f5\u2192 0log[_\u03b8{\u03b8: g - f_\u03b8^2 \u2264^2 }]/log()\n       (1)=lim_R \u2192\u221elim_\u03f5\u2192 0log[_\u03b8{\u03b8: g - f_\u03b8^2 \u2264^2, \u03b8_2 \u2264 R }]/log()\n       (2)\u2265lim_R \u2192\u221elim_\u2192 0log(( (A + B_C(k,R) ^1/5) \u2229 B_R \u2229(P) ) max_\u03b8\u2208 B_R P(\u03b8) )/log()\n       (3)=lim_R \u2192\u221elim_\u2192 0log(( (A + B_C(k,R) ^1/5) \u2229 B_R \u2229(P)) )/log(C(k,R) ^1/5)\u00b7log(C(k,R) ^1/5)/log()\n       (4)=1/5lim_R \u2192\u221elim_\u2192 0log(( (A + B_C(k,R) ^1/5) \u2229 B_R \u2229(P)) )/log(C(k,R) ^1/5)\n       = 1/5_P(A_g),\n\nwhere in (1) we assumed that the two quantities are equal, in (2) we used (<ref>), in (3) we used  lim_\u2192 0max_\u03b8\u2208 B_R P(\u03b8)/log() = 0 and in (4) we used lim_\u2192 0log(C(k,R)^1/5)/log() = 1/5. The second bound reads\n\n    \u03c7^#(g, U([0,1])) \n       = lim_R \u2192\u221elim_\u03f5\u2192 0log[_\u03b8{\u03b8: g - f_\u03b8^2 \u2264^2, \u03b8_2 \u2264 R }]/log()\n       (1)\u2264lim_R \u2192\u221elim_\u2192 0log(( (A + B_R^2 ) \u2229 B_R \u2229(P)) \u00b7min_\u03b8\u2208 B_R \u2229(P) P(\u03b8) )/log()\n       (2)=lim_R \u2192\u221elim_\u2192 0log(( (A + B_R^2 ) \u2229 B_R \u2229(P)))/log( R^2 )\u00b7log(R^2 )/log()\n       = _P(A_g),\n\nwhere in (1) we used (<ref>) and in (2) we used min_\u03b8\u2208 B_R \u2229(P) P(\u03b8) > 0, which is true because B_R is compact.\n\n\n\n\n\n\n\n\n\nProof of Lemma\u00a0<ref>\nLet  and  denote the vectors of t_i's, and v_i's respectively. Note that if for i \u2208 [1,c] we define  b_i^(1) := t_i, w_i^(2) := v_i/w_i^(1) and b^(2) := b then for every x \u2208 [0,1]\ng(x) = \u2211_i=1^c w_i^(2)\u00b7 w_i^(1)[x - b_i^(1)]_+ + b^(2).\n\nMoreover if the neurons i \u2208 [c+1,k] are inactive on [0,1], that is if b_i^(1) > 1 for all i > c, then g \u2261_[0,1] f_\u03b8, i.e. functions g an f_\u03b8 agree on [0,1]. If we denote by _[p,q] the restrictions of  to coordinates p,\u2026,q, then for < max(t_1, t_2 -t_1, \u2026, t_c, 1 - t_c) we can write\n\n    (A_g + B_) \u2229 B_R \u2229(P) \n       \u2287{\u03b8 : _[1,c] - ^2 \u2264^2/3, _[c+1,k]\u2208 [1,M]^k-c, ^(2)^(1) - ^2 \u2264^2/3, (b^(2) - b)^2 \u2264^2/3}\u2229 B_R.\n\nNow we will estimate ({ : ^(2)^(1) - ^2 \u2264^2 }\u2229 B_R).\n\n\n\n\n\nIf k=1 and R^2 > 5|v_1|:\n\n    ({w^(1),w^(2)\u2208 : (w^(2)w^(2) - v)^2 \u2264^2 }\u2229 B_R ) \n       \u2265 2\u222b_|v|^1/2^2|v|^1/22/w^(1) dw^(1)\n       = 4(log(2|v|^1/2) - log(|v|^1/2)) = 4log(2) .\n\nBound from (<ref>) generalizes to higher dimensions. If R^2 > 5^2 then\n\n    ({ : ^(2)^(1) - ^2 \u2264^2 }\u2229 B_R) \u2265\u03ba^c,\n\nwhere \u03ba is independent of , \u03ba depends only on the volume of balls in ^c and the constants 4log(2) from (<ref>). Now we can lower-bound the co-dimension\n\n    _P(A_g)    =\n    lim_R \u2192\u221elim_\u2192 0log(( (A_g + B_) \u2229 B_R \u2229(P)))/log()\n       \u2264lim_\u2192 0log(\u03ba' (/\u221a(3))^c \u00b7 (M-1)^k-c\u00b7\u03ba (/\u221a(3))^c \u00b72/\u221a(3))/log()      By (<ref>) and (<ref>)\n       = 2c+1,\n\nwhere similarly as before \u03ba' is a constant independent of .\n\nNow we will show an inequality in the other direction. Assume towards contradiction that (A_g) < 2c+1. This means that there exists \u03b8\u2208int((P)), f_\u03b8 = g and u_1, \u2026, u_3k+1-2c\u2208^3k+1 linearly independent such that \u03b8 + ConvHull(u_1, \u2026, u_3k+1-2c) \u2286 A_g. Fix one such \u03b8.\n\nNext observe that \n\n    b^(2) = b.\n\nMoreover\n\n    {t_1, \u2026, t_c}\u2286{_1^(1),\u2026, _k^(1)},\n\nbecause if there was t_i \u2209{_1^(1),\u2026, _k^(1)} then f\u201d_\u03b8(t_i) = 0 but g\u201d(t_i) = v_i \u2260 0. For every i \u2208 [1,k] define S_i := {j \u2208 [1,k] : _j^(1) = _i^(1)}. Note that for every i \u2208 [1,k] such that _i^(1) = t_j for some j \u2208 [1,c] we have:\n\n    \u2211_p \u2208 S_i_p^(2)\u00b7_p^(1) =  \n                v_j    _i^(1) = t_j \n     \n                0    _i^(1)\u2208 [0,1] \u2216{t_1, \u2026, t_c}\n\nIf not then let i_0 be such that _i_0^(1) is the minimal one such that (<ref>) doesn't hold. Note that then g \u2262_[_i_0^(1), _i_0^(1) + \u03b4] f_\u03b8, where \u03b4 > 0 is small enough so that {_1^(1),\u2026, _k^(1)}\u2229 (_i_0^(1) , _i_0^(1) + \u03b4) = \u2205. \nNow observe that (<ref>), (<ref>) and (<ref>) give us locally at least 2c+1 linearly independent equations around \u03b8 which contradicts with \u03b8 + ConvHull(u_1, \u2026, u_3k+1-2c) \u2286 A_g. Thus (A_g) \u2265 2c+1. \n\n\nNext we give a helpful fact.\n\n\nLet X,Y be two independent random variables distributed according to \ud835\udca9(0,\u03c3_w^2). Then for every a_0 \u2208 we have that the density of XY at a_0 is equal to\n\n    f_XY(a_0) = 1/2\u03c0\u03c3_w^2\u222b_-\u221e^+\u221e e^-1/2\u03c3_w^2(w^2 + a_0^2/w^2) dw = 1/\u221a(2\u03c0\u03c3_w^2)e^-|a_0|/\u03c3_w^2.\n\n\nProof of Lemma\u00a0<ref>\nTo prove the lemma we estimate the probability of f_\u03b8's close to g_1. Without loss of generality assume that a > 0.\n\n\n\n  \nUpper bound. We can represent g_1 with a single node i by assigning \u221a(a) to the outgoing weight (^(2)_i), \u221a(a) to the incoming weight (^(1)_i) of this node, the bias term (^(1)_i) to t and b^(2) to b. The bias terms of all other nodes lie in (1,M], i.e. they are inactive in the interval [0,1].\n\nThese are exact representations of the function but to compute a lower bound on the probability we should also consider functions that are close to g_1. We can change _i^(1), _i^(2), _i^(1) by a little bit and still have a function that satisfies g_1 - f_\u03b8^2 \u2264^2. We claim that the target probability is lower bounded by\n\n    ( /21/\u221a(2\u03c0\u03c3_w^2) e^-10 a/9 \u03c3_w^2) \u00b7(9/20 M a) \u00b7( /401/\u221a(2\u03c0\u03c3_b^2) e^-(|b| + /40)^2/2\u03c3_b^2) \u00b7( M-1/M)^k-1.\n\nWe arrive at this expression by noting the following facts. By (<ref>) and the assumption that a \u2265 20 the probability that _i^(2)_i^(1) = a \u00b1/2 is lower bounded by /21/\u221a(2\u03c0\u03c3_w^2) e^-10 a/9 \u03c3_w^2. The probability that _i^(1) = t \u00b19/20a is equal 9/20M a. The probability that b^(2) = b \u00b1/40 is lower bounded by /401/\u221a(2\u03c0\u03c3_b^2) e^-(|b| + /40)^2/2\u03c3_b^2. The last term is the probability that all other nodes have bias terms in [1,M]. Their weights can realm over the whole space and these nodes don't affect the function on [0,1]. We claim that all functions of this form satisfy g_1 - f_\u03b8^2 \u2264^2. We bound the pointwise difference of g_1 and f_\u03b8 in [0,1], i.e. for every x \u2208 [0,1]\n    f_\u03b8(x) = b + (_i^(2)_i^(1)\u00b1/2)[x - (_i^(1)\u00b19/20a)]_+ \u00b1/40\n       = b + _i^(2)_i^(1)[x - (_i^(1)\u00b19/20a)]_+ \u00b1/2[x - (_i^(1)\u00b19/20a)]_+ \u00b1/40\n       = b + _i^(2)_i^(1)[x - _i^(1)]_+ \u00b1_i^(2)_i^(1)9/20a\u00b1/2(1 + 9/20a) \u00b1/40\n       = b + _i^(2)_i^(1)[x - _i^(1)]_+ \u00b19/20\u00b1/2(21/20 + 9/20a)       As _i^(2)_i^(1) = a \n       = b + _i^(2)_i^(1)[x - _i^(1)]_+ \u00b1      As  a \u2265 20,\n\nwhich implies that for such representations g_1 - f_\u03b8^2 \u2264^2. From (<ref>) we get an upper bound on the sharp complexity \n\n    \u03c7^#( g_1, ^2) \n       \u2264 -log[ ( /21/\u221a(2\u03c0\u03c3_w^2) e^-10 a/9 \u03c3_w^2) \u00b7(9/20 M a) \u00b7( /401/\u221a(2\u03c0\u03c3_b^2) e^-(|b| + /40)^2/2\u03c3_b^2) \u00b7( M-1/M)^k-1] \n       \u226410/9(a/\u03c3_w^2 + |b|/\u03c3_b^2) + log(M a ) - (k-1) log(1 - 1/M) + log(2\u03c0\u03c3_w \u03c3_b) + 7 - 3 log()\n       \u226410/9(a/\u03c3_w^2 + |b|/\u03c3_b^2) + log(M a ) - (k-1) log(1 - 1/M) + 10 - 3 log().       As \u03c3_b^2 \u22641/\u03c3_w^2\n       \u2264 2(a/\u03c3_w^2 + |b|/\u03c3_b^2) + 11 - 3log(),\n\nwhere in the last inequality we used that log(x) < x/2,  log(1+x) < x for x> 0 and the assumption k \u2264 M \u22641/\u03c3_w^2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObserve that according to Corollary\u00a0<ref> we have that \u03c7^#(g_1, _x) \u2264 3. Recall that \u03c7^#(g_1, _x) = lim_\u2192 0 -\u03c7^#(g_1, _x, ^2)/log(). This means that, at least approximately, if we took the bound from (<ref>), divided it by -log() we would get an upper bound on  \u03c7^#(g_1, _x). This would yield for us \u03c7^#(g_1, _x) \u2264 3, as all other terms go to 0 when \u2192 0. \n\n\n\n  \nLower bound. There are other \u03b8's that represent the function approximately. For example we could represent g_1 with more than 1 node, by spreading the change of slope a over many nodes. Another possibility is that a number of nodes with the same bias terms t \u2260 b \u2208 [0,1] effectively cancel out. These \u03b8's contribute to the probability and decrease the complexity. \n\n\n\n\n\n\nLet \u03b8 be such that g_1 - f_\u03b8^2 \u2264^2 and let S := {i \u2208{1, \u2026, k} : _i^(1)\u2208 [t - 9^1/2, t + 9^1/2] }. \n\n\n\n\nAssume towards contradiction that \u2211_i \u2208 S |_i^(1)_i^(2)| < a -^1/4. This implies that either\n\n    \u2211_i : _i^(1)\u2208 [t - 9^1/2,t] |_i^(1)_i^(2)| < f'_\u03b8(t) - ^1/4/2\n\nor\n\n    \u2211_i : _i^(1)\u2208 [t, t + 9^1/2] |_i^(1)_i^(2)| < a - f'_\u03b8(t) -  ^1/4/2.\n\nAssume that (<ref>) holds. A similar argument covers (<ref>). Now consider two cases.\n\n\n\n  \nCase 1. For all x \u2208 [t, t+ 3^1/2] we have f_\u03b8(x) > a(x-t) + ^3/4. Then g_1 - f_\u03b8^2 \u2265 3^1/2\u00b7^3/2 > ^2. \n\n\n  \nCase 2. There exists x_0 \u2208 [t,t+3^1/2] such that \n    f_\u03b8(x_0) < a(x_0-t) + ^3/4.\n\nBy (<ref>) we know that for all x \u2208 [t,t+9^1/2] we have f'_\u03b8(x) < a - ^1/4/2. This means that f_\u03b8(x) is below a linear function of slope a-^1/4/2 passing through (x_0,f_\u03b8(x_0)). Now we lower bound the error using the fact that f_\u03b8 is below this line. \n\n    g_1 - f_\u03b8^2 \n       \u2265\u222b_x_0^t+9^1/2[a(x-t) - (f(x_0) + (a - ^1/4/2)(x-x_0)) ]^2 1_{a(x-t) > f(x_0) + (a - ^1/4/2)(x-x_0)} dx\n\nNote that the function \u03b4(x) := a(x-t) - (f(x_0) + (a - ^1/4/2)(x-x_0)) is increasing in x and moreover \n\n    \u03b4(7^1/2 + t) \n       =  a(x_0-t) - f(x_0) + ^1/4/2(7^1/2 + t - x_0) \n       \u2265 -^3/4 + 2^3/4      By (<ref>) and  x_0 < t + 3^1/2\n       \u2265^3/4.\n\nCombining (<ref>) and (<ref>) we get that\ng_1 - f_\u03b8^2 \u2265 2^1/2\u00b7^6/4 > ^2,\n\nwhich is a contradiction .\n\nWe arrived at a contradiction in both cases thus \u2211_i \u2208 S |_i^(1)_i^(2)| \u2265 a -^1/4. We claim that for every such S the probability of \u2211_i \u2208 S |_i^(1)_i^(2)| \u2265 a -^1/4 is at most\n\n    ( 18^1/2/M)^|S|\u222b_a - ^1/4^\u221e x^(|S| - 1)2^|S|/|S|!\u00b72/\u221a(2\u03c0\u03c3_w^2)e^-x/\u03c3_w^2 dx.\n\n\n\n\n\nWe arrive at this expression by noting that x^(|S| - 1)2^|S|/|S|! is the area of an \u2113_1 sphere of radius x in |S| dimensions; the density for _i's satisfying \u2211_i \u2208 S |_i^(1)_i^(2)| = x is, by Fact\u00a0<ref>, 2/\u221a(2\u03c0\u03c3_w^2)e^-x/\u03c3_w^2; the probability that a single bias term is equal to t \u00b1 9^1/2 is 18^1/2/M.\n\n\nThere has to exist S \u2286{1, \u2026, k} such that \u2211_i \u2208 S_i^(1)_i^(2) = a  \u00b1 and for all i \u2208 S we have _i^(1) = t  \u00b1/\u221a(a), i.e. there exists a subset of nodes whose slopes add up to approximately a and their bias terms are around t. For every such S the probability What about +1/-1? that \u2211_i \u2208 S_i^(1)_i^(2) = a  \u00b1 and _i^(1) = t  \u00b1/\u221a(a) is approximately\n\n    a^(|S| - 1)/22 \u03c0^|S|/2/\u0393(|S|/2)\u00b7\u00b7( /\u221a(a)M)^|S|\u00b7\u221a(2\u03c0)e^-a/\u03c3_w^2.\n\nWe arrive at this expression by noting that a^(|S| - 1)/22 \u03c0^|S|/2/\u0393(|S|/2) is the area of a sphere of radius \u221a(a) in |S| dimensions, multiplying it by thickness ; the density for _i's satisfying \u2211_i \u2208 S_i^(1)_i^(2) = a is by (<ref>)\u221a(2\u03c0)e^-a/\u03c3_w^2; the probability that a single bias term is equal to t \u00b1/\u221a(a) is /\u221a(a)M.\n\n\nNow we upper bound the probability of all these functions by taking a union bound over sets S. We get \n\n    _\u03b8[g_1 - f_\u03b8^2 \u2264^2 ] \n       \u2264\u2211_S \u2286{1,\u2026,n}\u222b_a - ^1/4^\u221e x^(|S| - 1)2^|S|/|S|!\u00b7( 18^1/2/k)^|S|\u00b7\u221a(2/\u03c0\u03c3_w^2)e^-x/\u03c3_w^2 dx       By (<ref>) and k \u2264 M\n       \u2264\u221a(2/\u03c0\u03c3_w^2)\u2211_i=1^k \u222b_a - ^1/4^\u221eki2^i/i!( 1/2k)^i\u00b7 x^i - 1 e^-x/\u03c3_w^2 dx        As  18^1/2\u22641/2\n       \u2264\u221a(2/\u03c0\u03c3_w^2)\u2211_i=1^k \u222b_a/2^\u221ex^i-1/2^i-1 e^- x/\u03c3_w^2 dx       As ki\u2264 k^i, i! \u2265 2^i-1, a \u2265 2^1/4\n\nFor every i \u2208 [1,k] we can upper bound \n\n    \u222b_a/2^\u221ex^i-1/2^i-1 e^-x/\u03c3_w^2 dx \n       \u2264\u222b_a/2^2 e^-x/\u03c3_w^2 dx + [-x^i/2^i-1 e^-x/\u03c3_w^2]_2^\u221e      As (-x^i e^-x/\u03c3_w^2)' \u2265 x^i-1 e^-x/\u03c3_w^2 for  x \u2265 2 \n       \u2264[-\u03c3_w^2 e^-x/\u03c3_w^2]^2_a/2  + 2e^-2/\u03c3_w^2\n       \u2264\u03c3_w^2 e^-a/2\u03c3_w^2 + 2e^-2/\u03c3_w^2\n       \u2264 3e^-a/2\u03c3_w^2      As \u03c3_w^2 \u2264 1, a \u2264 2\n\nPlugging (<ref>) back to (<ref>) we get \n\n    _\u03b8[g_1 - f_\u03b8^2 \u2264^2 ] \u2264\u221a(18/\u03c0)k/\u03c3_w e^-a/2\u03c3_w^2.\n\nWith (<ref>) we can bound the sharp complexity\n\n    \u03c7^#(_x, g_1, ^2) \n       \u2265a/2\u03c3_w^2 - log(k/\u03c3_w) + log(\u221a(2\u03c0)) \n       \u2265a/3\u03c3_w^2      As \u03a9(\u03c3_w^2log(k/\u03c3_w)) \u2264 |a|.\n\n    _\u03b8[g_1 - f_\u03b8^2 \u2264^2 ] \n       \u2264\u221a(2\u03c0)\u2211_S \u2286{1,\u2026,n} e^-ak a^(|S| - 1)/22 \u03c0^|S|/2/\u0393(|S|/2)\u00b7( /\u221a(a)k)^|S|      By (<ref>), k = M = 1/\u03c3_w^2\n       \u2264\u221a(2\u03c0)\u2211_i=1^k e^-akki a^(|S| - 1)/22 \u03c0^|S|/2/\u0393(|S|/2)\u00b7( /\u221a(a)k)^|S|\n       \u2264\u221a(2\u03c0)\u2211_i=1^k e^-ak + ilog(k e/i) + i-1/2log(a) - i/2log(i/2\u03c0 e) + i log(/\u221a(a)k)       As ki\u2264(k e/i)^i  and \u0393(x+1) \u2248\u221a(2\u03c0 x)(x/e)^x \n       \u2264\u221a(2\u03c0)\u2211_i=1^k e^-ak + ilog(  e \u221a(2\u03c0 e)/\u221a(a) i^3/2) + i-1/2log(a)\n       \u2264\u221a(2\u03c0)\u2211_i=1^k e^-ak/2 + ilog(  e \u221a(2\u03c0 e)/\u221a(a) i^3/2)      Because log(a) \u2264 a  for  a > 0.\n\nUsing the assumption  e \u221a(2 \u03c0 e)/\u221a(a) < 1 we can upper bound it further\n\n    \u221a(2\u03c0)e^-ak/2\u00b7\u2211_i=1^k (  e \u221a(2\u03c0 e)/\u221a(a) i^3/2)^i \n       \u2264\u221a(2\u03c0)e^-ak/2\u00b7 2       As \u2211_i=1^k (1/i^3/2)^i \u2264\u2211_i=1^k 2^-i\u2264 2.\n\nFinally we get a lower bound for the complexity\n\n    \u03c7^#(_x, g_1, ) \n       \u2265ak/2 -1/2log(2\u03c0) - log() - log(2) \n       \u2265ak/2 - log() - 3."}