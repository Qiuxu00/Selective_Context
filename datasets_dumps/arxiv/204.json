{"entry_id": "http://arxiv.org/abs/2303.07073v2", "published": "20230313125202", "title": "Can spoofing countermeasure and speaker verification systems be jointly optimised?", "authors": ["Wanying Ge", "Hemlata Tak", "Massimiliano Todisco", "Nicholas Evans"], "primary_category": "eess.AS", "categories": ["eess.AS"], "text": "\n\n\u00a9 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\n\nSpatial Attention and Syntax Rule Enhanced Tree Decoder for Offline Handwritten Mathematical Expression Recognition\n    Zihao Lin1 Jinrong Li2 Fan Yang1Shuangping Huang13Xu Yang4\n\nJianmin Lin2Ming Yang2\n\n    March 30, 2023\n===================================================================================================================\n\n\n\n\n\n\n    Spoofing countermeasure (CM) and automatic speaker verification (ASV) sub-systems can be used in tandem with a backend classifier as a solution to the spoofing aware speaker verification (SASV) task. The two sub-systems are typically trained independently to solve different tasks. While our previous work demonstrated the potential of joint optimisation, it also showed a tendency to over-fit to speakers and a lack of sub-system complementarity. Using only a modest quantity of auxiliary data collected from new speakers, we show that joint optimisation degrades the performance of separate CM and ASV sub-systems, but that it nonetheless improves complementarity, thereby delivering superior SASV performance. Using standard SASV evaluation data and protocols, joint optimisation reduces the equal error rate by 27% relative to performance obtained using fixed, independently-optimised sub-systems under like-for-like training conditions.\n\n\n\nspoofing detection, spoofing countermeasures, speaker verification, joint optimisation\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n    Solutions to spoofing aware speaker verification (SASV)\u00a0<cit.> typically comprise a pair of essentially-independent sub-systems operating in tandem: a spoofing countermeasure (CM) to verify that the test utterance is bona fide rather than spoofed; an automatic speaker verification (ASV) system to verify whether enrolment and test utterances correspond to the same speaker. Even so, ASV also has potential to detect spoofing attacks. In support of this claim, one can imagine the presentation of poor-quality spoofing attacks, namely artificially generated or manipulated utterances which do not represent well the characteristics of the target speaker. Such poor-quality spoofs may not fool the ASV sub-system and hence be rejected even without an auxiliary CM. \n\n\n    ASV and CM sub-systems are therefore not independent and there is hence potential for joint optimisation to exploit their complementarity. The CM can be optimised to detect spoofing attacks which will fool the ASV system, thereby avoiding the unnecessary waste of CM classifier capacity. Despite the appeal of joint optimisation, to the best of our knowledge, no successful solutions have been reported thus far, with all leading solutions combining independently-optimised sub-systems with a backend classifier, e.g. using score\u00a0<cit.> or embedding\u00a0<cit.> fusion techniques.\n    \n    Our previous work\u00a0<cit.> showed that jointly-optimised SASV solutions can succeed in improving robustness to spoofing but that they have also a tendency to over-fit to the speakers of data used for training. This is hardly surprising; state-of-the-art ASV systems are nowadays trained using data collected from many hundreds of speakers\u00a0<cit.> whereas the training partition of the SASV database\u00a0<cit.> contains data collected from only 20 speakers. We showed the number of speakers whose data is used for training is a bottleneck and that progress is unlikely to be made without using additional, auxiliary data collected from a larger pool of speakers.\n\n    The work reported in this paper aims to establish the potential and determine whether spoofing countermeasures and speaker verification systems can be jointly optimised. Our hypothesis is that, with data collected from a sufficient speaker population, joint optimisation can deliver superior SASV performance to that obtained using fixed, independently-optimised systems trained under like-for-like training conditions. While joint optimisation might even degrade CM and/or ASV performance, our expectation is that improvements will emerge from more complementary CM and ASV sub-systems that work in synergy to provide more reliable SASV solutions.\n\n\n\n\u00a7 SASV FRAMEWORK\n\n    \n    All work reported in this paper was performed using the same SASV framework used in our previous work\u00a0<cit.>. While the framework is the same, there are differences in the training policy in terms of data and trial types. A summary of the framework is presented in this section. The new training policy is described in Section\u00a0<ref>.\n    \n    The architecture is illustrated in Fig.\u00a0<ref> and consists of an ASV sub-system, a CM sub-system and a backend classifier. The ASV sub-system is a ResNet34 model with squeeze-and-excitation (SE) blocks\u00a0<cit.>, namely the ResNetSE34 model reported in\u00a0<cit.>. The input waveform is first decomposed into log Mel-filterbank features. Four convolutional layers with SE blocks and attentive statistics pooling\u00a0<cit.> are used for deep feature extraction and to project the variable length input to a fixed-length embedding vector. A combination of softmax and angular prototypical loss\u00a0<cit.> is used for optimisation. The ASV sub-system extracts both an enrolment embedding e^ASV_enr and test embedding e^ASV_tst from corresponding utterances.\n        \n    The CM sub-system is the SASV 2022 AASIST\u00a0<cit.> baseline. Raw waveforms are decomposed using a RawNet2-based encoder\u00a0<cit.>. Extracted features are integrated using a graph attention network\u00a0<cit.> before a readout operation and a hidden linear layer are used to generate output scores. Optimisation is performed using a weighted cross-entropy loss function. A linear layer is used to transform representations extracted from the penultimate layer to CM embeddings e^CM_tst of the same dimension as ASV embeddings.\n            \n    The backend classifier is a convolutional neural network with an adaptive average pooling layer\u00a0<cit.>. It operates upon the pair of ASV embeddings and the single CM embedding. These are first stacked along a new dimension so that three 1D convolutional layers can be used to capture the variance between speaker representations of both enrolment and test utterances, and also the variance between the ASV and CM embeddings. Deep representations are aggregated using a 1D adaptive average pooling layer and are then mapped to a bona fide, target class score using a pair of linear layers and a one class (OC) softmax layer\u00a0<cit.>. A OC-softmax loss function is used to learn higher output scores for the target class.\n    \n       \n    \n\n\n\u00a7 EXPERIMENTAL SETUP\n\n\n\n    We describe the set of databases, protocols, assessment and metrics used in this work, together with specific implementation details.\n\n\n\n \u00a7.\u00a7 Databases\n\n\n    Experimental work was performed using three databases. The VoxCeleb2\u00a0<cit.> database is used for ASV pre-training. The CM is pre-trained using the training partition of the ASVspoof 2019 Logical Access (LA) database\u00a0<cit.>, referred to in the remainder of this paper simply as ASVspoof. The key characteristics of the ASVspoof training partition are detailed in the top half of Table\u00a0<ref>.  It contains data collected from 20 speakers, a number far less than databases typically used for ASV research\u00a0<cit.> and, as we found in previous work\u00a0<cit.>, a number that is insufficient to support joint optimisation. All development and evaluation experiments are performed using the corresponding partitions of the ASVspoof database.\n    \n    The Fake Audio Detection (FAD) database\u00a0<cit.> contains bona fide and spoofed Madarin-language utterances generated using TTS algorithms. It is used optionally for training the backend classifier or, in the case of joint optimisation, the backend classifier and both CM and ASV sub-systems. The FAD database is sourced from 6 different public databases\u00a0<cit.>. Its non-homogeneous nature makes it ideal as a source of speaker augmentation. To be consistent with the ASVspoof database, we used the data collected from speakers for whom there is both bona fide and spoofed data. The key characteristics of the resulting training partition used in this work are detailed in the lower half of Table\u00a0<ref>. It contains data collected from 40 speakers, a number twice that of the ASVspoof training partition.\n\n      \n      \n\n\n \u00a7.\u00a7 Protocols\n\n\n    We used SASV\u00a0<cit.> protocols for the ASVspoof database for all work reported in this paper, modifying only the training protocol to incorporate optional FAD data. For fixed optimisation, pre-trained sub-systems are used without additional optimisation. The backend is optimised according to one of three different training conditions using either: ASVspoof data alone; ASVspoof + FAD data; ASVspoof + FAD bona fide data only. The last training condition is included since we aim to reduce the over-fitting of the ASV sub-system which is usually trained with bona fide data only. In doing so we avoid, to the extent possible, domain mismatch in terms of spoofing attacks (which is not the goal of this work); domain mismatch in the bona fide data nonetheless remains. Only for the jointly-optimised system are all three components trained simultaneously through back propagation.\n\n    Training is performed using different combinations of trial pairs including a bona fide target enrolment utterance in addition to a test utterance. The latter can be: 1 a bona fide target speaker utterance; 2 a bona fide non-target speaker utterance; 3 a spoofed target speaker utterance. The proportion of each trial type for fixed and jointly-optimised systems is illustrated in Table\u00a0<ref>. For joint optimisation we found the use of a fourth trial type involving 4 a spoofed non-target speaker utterance to be beneficial, but found no improvement for our fixed system. The full SASV system should accept only type 1 trials. All others should be rejected, even though type 2 trials are within the positive class for the CM.[By convention, the CM produces higher scores for bona fide inputs and lower scores for spoofed inputs.] The conflict between SASV and CM classes then results in degraded CM performance through back propagation. The introduction of type 4 trials acts to compensate for this behaviour since they are among the negative class for SASV and both sub-systems. Finally, the development and evaluation protocols are the standard SASV protocols reported in\u00a0<cit.>.\n        \n\n\n \u00a7.\u00a7 Assessment and metrics\n\n\n    We use the output scores from the backend OC-softmax layer to assess the performance of both fixed and jointly-optimised systems. In order to analyse impacts of joint optimisation and complementarity, we also assess performance at the sub-system level. ASV sub-system scores are the cosine distance between enrolment and test utterance embeddings. CM sub-system scores are the AASIST outputs for the bona fide class and test utterance only. Three different SASV metrics, all equal error rate (EER) estimates, are used to assess speaker verification performance (SV-EER) involving a set of 1 and 2 trials, spoofing detection performance (SPF-EER) involving a set of 1 and 3 trials and spoofing-aware speaker verification (SASV-EER) performance involving the full set of 1, 2 and 3 trials. The remaining trial type 4 is not used for assessment, neither for development, nor for evaluation.\n        \n        \n        \n\n\n \u00a7.\u00a7 Implementation\n\n        \n    All trainable network parameters are updated for 20 epochs with an initial learning rate of 5e-5. The batch size is set to 20 for both systems. Model selection is made according to the lowest SASV-EER estimate for the development partition. Systems performance is estimated from the average of 5 independent runs each with a different random seed. All experiments were performed on a single NVIDIA GeForce RTX 3090 GPU. Results are reproducible with the same set of random seeds and GPU environment using the implementation available online.[<https://github.com/eurecom-asp/sasv-joint-optimisation>]\n        \n\n\n\u00a7 RESULTS\n\n\n    \n\n    Results are shown in Table\u00a0<ref>. It shows SASV-EER, SV-EER, and SPF-EER estimates for the full system (left), the ASV sub-system (middle) and CM sub-system (right) when training is performed using ASVspoof data alone (top), ASVspoof and FAD data (middle) and then ASVspoof and only FAD bona fide data (bottom). In the following we outline a number of principle observations and supporting results.\n    \n    CM performance \u2013 SPF-EER estimates for the CM sub-system in the last column of Table\u00a0<ref> show that the lowest SPF-EER (0.65%) is obtained using a fixed system trained using only ASVspoof data. The degradations in CM performance (1.02%, 1.81% and 1.35%) stem from the CM being optimised to detect only attacks that are successful in fooling the ASV system. While there is no need for the CM to detect less potent attacks, but since performance estimates are still made using the full set, the SPF-EER increases. The worst result is for the ASVspoof + FAD training condition, and is the result of differences between spoofing attacks in the ASVspoof and FAD databases. SPF-EER estimates for the ASV sub-system (25.75%) show that even the fixed system can detect spoofing attacks, but that joint optimisation acts to improve performance (17.43%, 10.82% and 10.60%). SPF-EER results for the full system show the complementarity of the ASV and CM sub-systems in improving overall robustness to spoofing for all three training conditions.\n    \n    ASV performance \u2013 SV-EER estimates for the CM sub-system shown in the penultimate column of Table\u00a0<ref> show evidence of speaker-awareness. Without over-fitting to speakers, the CM sub-system should achieve an SV-EER of 50%. Results for jointly-optimised systems (46.8%, 46.37% and 47.99%) deviate further than the result for the fixed system (49.01%). Corresponding SV-EER results for the ASV sub-system show that joint optimisation acts to degrade performance, with the best result being achieved using the fixed system (1.27%). This translates to worse ASV performance for the full system; SV-EER results for all jointly-optimised systems (2.34%, 2.66% and 1.77%) are all worse than those for fixed systems (1.27%, 1.85% and 1.47%). Again, the worst result is for the ASVspoof + FAD training condition.\n    \n    SASV performance \u2013 Joint optimisation makes little difference to the SASV-EER for the CM sub-system (24.5% vs. 23.65%, 22.83% and 24.66%). This is not surprising, since the CM sub-system operates only upon the test utterance. Nonetheless, joint optimisation improves ASV sub-system performance, with results for all jointly-optimised systems (13.47%, 8.57% and 8.58%) being better than the fixed system (19.7%). Even if there is little difference in CM results, but substantial improvements to ASV results, this does not translate to better performance for the full system in the case of the ASVspoof (1.15% vs. 1.49%) and ASVspoof + FAD (1.52% vs. 1.74%) training conditions. Only for the ASVspoof + FAD bona fide condition is joint optimisation beneficial (1.72% vs. 1.26%).  \n    \n\n\n\u00a7 DISCUSSION\n\n    \n    The lowest SASV-EER comes from the fixed system trained using ASVspoof data alone. This result may cast doubt upon the claimed merit since, even with more training data, the jointly-optimised system does not outperform the fixed counterpart. However, while the jointly-optimised system trained using ASVspoof + FAD bona fide only data does give performance that is behind that of the fixed system trained with ASVspoof data only, the use of different training data makes for an unfair comparison. The FAD training data is out-of-domain in the form of speech data in a different language and recording conditions, among other differences. In this sense, comparisons should be made only between results for the same training conditions, i.e. results for ASVspoof or ASVspoof + FAD bona fide only training conditions. In this sense, lower EERs observed across different training conditions are not an indication of joint optimisation not working, but are the result of domain mismatch which is not tackled in this work. Domain mismatch is a penalty which likely degrades results for training conditions that use FAD data. The use of domain-matched training data, or domain adaptation, may than show the full potential and even better results.\n    \n    Results for the ASVspoof and FAD bona fide only training condition show a 27% relative reduction in the SASV-EER under like-for-like training conditions. It comes as a result of using data collected from a greater number of speakers and jointly-optimised CM and ASV sub-systems with worse performance than their independently-optimised counterparts. Even so, the resulting sub-systems are more complementary leading to better overall reliability (SASV-EER).\n    \n\n\n\u00a7 CONCLUSIONS AND FURTHER WORK\n\n\n    Results presented in this paper add to the evidence that joint optimisation has potential to better exploit the synergy between spoofing countermeasures and speaker verification sub-systems so that they function cooperatively as a more reliable solution to spoofing aware speaker verification (SASV). Interestingly, results show that joint optimisation degrades the performance of each sub-system but that it improves their complementarity. Joint optimisation improves SASV performance by 27% relative to its fixed, independently-optimised counterpart under like-for-like training conditions. This result is obtained despite using a training database of the same size to jointly-optimise a considerably more complex system.\n\n    While other approaches to joint optimisation might make better use of data collected from fewer speakers, our solution is only successful when using a modest quantity of auxiliary data collected from new speakers. Given that state-of-the-art ASV solutions are trained with data collected from many hundreds of speakers, it seems wise for future ASVspoof and SASV challenges to collect and make available training data collected from far more speakers than in past editions. The domain robustness problem persists; even if we can learn speaker variation in a joint optimisation framework using bona fide data sourced from a different database, it does not translate to reliable detection for spoofing attacks in the same database. Research in domain robustness is hence a priority. Other directions include the investigation of joint optimisation strategies to reduce CM over-fitting; the CM shows signs of speaker awareness. Speaker-dependent spoofing detection might also be an interesting avenue for future research.\n    \nIEEEbib\n\n\n"}