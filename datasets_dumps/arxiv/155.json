{"entry_id": "http://arxiv.org/abs/2303.07139v1", "published": "20230313140519", "title": "Comparing statistical and machine learning methods for time series forecasting in data-driven logistics -- A simulation study", "authors": ["Lena Schmid", "Moritz Roidl", "Markus Pauly"], "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "text": "\n\n\n\n\n\n\n\n1]Lena Schmidcor1\nlena.schmid@tu-dortmund.de\n2]Moritz Roidl\n1,3]Markus Pauly\n[1]Department of Statistics, TU Dortmund University, 44227 Dortmund, Germany\n[2]Chair of Material Handling and Warehousing, TU Dortmund University, 44221\nDortmund, Germany\n[3]UA Ruhr, Research Center Trustworthy Data Science and Security, 44227 Dortmund, Germany\n[cor1]Corresponding Author\n\n\n\n Many planning and decision activities in logistics and supply chain management are based on forecasts of multiple time dependent factors. Therefore, the quality of planning depends on the quality of the forecasts. We compare various forecasting methods in terms of out of the box forecasting performance on a broad set of simulated time series. We simulate various linear and non-linear time series and look at the one step forecast performance of statistical learning methods.\n\n\n\n\n\nMachine Learning, Time Series, Forecasting, Simulation Study\n\n  \n\n\n\n\n\n\n\n\n\n\n \n\n\u00a7 INTRODUCTION\n\n \n\n\n\n\n\n\n\nForecasting methods are essential for efficient planning in logistics domains such as warehousing, transport, and supply chain management. They enable companies to anticipate and plan for future demand, capacity needs, and supply chain requirements. Thereby, different logistics applications require different forecasts due to their unique characteristics. \n\nIn the transport domain, e.g., accurate transportation forecasting enables logistics companies to optimize their transportation networks, reduce transportation costs and enhance delivery reliability <cit.>. \n While precise forecasting allows warehouse managers to optimize the use of space, reduce the risk of stockouts, and improve overall efficiency <cit.>. In supply chain management accurate forecasts are, e.g,  used to optimize the use of resources across the entire supply chain <cit.>.\n\n The above references show that the use of forecasting techniques such as time series models and machine learning methods has become increasingly popular in recent years. However, there is a lack of consensus on which method is more effective.\n \n\n\nTime series models have been used in forecasting for several decades, and they are widely used in logistics for demand forecasting. These models are based on historical data and use statistical techniques to identify patterns and trends in the data, which can then be used to make predictions about future demand. Some commonly used time series models in logistics include (seasonal) autoregressive integrated moving average (ARIMA) and exponential smoothing models. For example, <cit.> developed an ARIMA multistage supply chain model that is based on time series models. \nAnother example is Prophet <cit.>.  In <cit.>, the authors examined ARIMA and Prophet models for predicting supermarket sales. The Prophet models demonstrated superior predictive performance in terms of lower errors. In a another paper, <cit.> used a double exponential smoothing for inventory forecasting. \n\n\n\n\nMore recently, machine learning (ML) methods have become increasingly popular for demand forecasting in logistics due to their ability to handle large and complex data sets. There are many literature reviews <cit.>, that discuss the use of machine learning techniques in forecasting for supply chain management, including an overview of the various techniques used and their advantages and limitations. \n\n\n\nSeveral studies have shown that ML methods such as neural networks, support vector regression, and Random Forests outperform traditional time series models in demand forecasting. For example, a study by <cit.> compared the prediction power of more than ten different forecasting models including classical methods as ARIMA and advanced techniques such as long short-term memory (LSTM) and convolution neural networks, using a dataset containing the sales history of furniture in a retail store. The results showed that the LSTM outperformed the other models in term of accuracy. Another study by <cit.> also compared the forecasting performace of ARIMA and neural network using a commodity prices data set. Again the neural network performed better than ARIMA model. Similar results were obtained in other studies <cit.>.\nHowever, other studies have found mixed results, with some suggesting that time series models perform better than ML methods. For instance, <cit.> compared the forecasting accuracy of ARIMA and neural network models in predicting wind speed for short time-intervals. The results showed that the performance of both are very similar indicating that a simple forecasting model could be used in order to administrate energy sources. A comparisation of daily hotel demand forecasting performance of SARIMAX, GARCH Models and Neutral Network showed that both time series approaches outperformed the neural networks <cit.>.\n\n\n\n\nThe comparison of the forecasting performance of ML methods and time series models in logistics has significant implications for businesses seeking to improve their forecasting accuracy. By identifying the most effective forecasting methods, businesses can make better-informed decisions about production, inventory management, and resource allocation.\nThus, the purpose of this article is to provide a comprehensive comparison of the forecasting performance of time series models and ML methods. To be more precise, various forecasting methods in terms of out of the box forecasting performance are compared on a broad set of simulated time series. We simulate various linear and non-linear time series and look at the one step forecast performance of the statistical learning methods.\n\n\n\nThis work is structured as follows: Section\u00a0<ref> presents the different used forecasting methods. More precisely, the (seasonal) ARIMA and TBATS models are presented. In addition, the machine learning approaches (Random Forest and XGBoost) are described in more detail. The simulation design and framework are then presented in Section\u00a0<ref>, while Section\u00a0<ref> summarizes the main simulation results. The manuscript concludes with a discussion of our findings and an outlook for future research (Section\u00a0<ref>).\n\n\n\n\n\n\n \n\n\u00a7 METHODS\n\n \n\n\n\n\n\nIn this section, we explain the one-step forecasting methods under investigation.\nThere are several different approaches to modeling and predicting time series. Traditional time series models, including moving averages and exponential smoothing, are linear in the sense that predictions of future values are linear functions of past observations. Because of their relative simplicity in terms of understanding and implementation, linear models have been used in many forecasting problems <cit.>. To overcome the limitations of linear models and to account for certain nonlinear patterns observed in real-world problems, several classes of nonlinear models have been proposed in the literature. Examples cover the threshold autoregressive model (TAR) <cit.>  and the generalized autoregressive conditional heteroscedastic model (GARCH) <cit.>. Although some improvements have been noted, the utility of their application to general prediction problems is limited <cit.>: Since these models were developed for specific nonlinear patterns, they are not able to model other types of nonlinearity in time series. More recently, machine learning methods have been proposed as an alternative to time series forecasting <cit.>. \n\n\nSince it is impossible to include the full range of machine learning models and time series methods that exist into our simulation study, we focus only on some of the most popular algorithms. \nTo evaluate the performance we additionally compare the methods with a naive approach, where the last observation of the time series is used as prediction.\n\n\n\n \u00a7.\u00a7 Time Series Methods\n\nWe focus on ARIMA, SARIMA, and TBATS. The first two models are among the most popular models in traditional time series forecasting and are often used as benchmark models for comparison with machine learning algorithms <cit.>. In addition, TBATS models combine many different approaches that are commonly used in forecasting.\n\n\n\n  \nARIMA\nAutoregressive integrated moving average model (ARIMA) is a generalized model of autoregressive moving average (ARMA) that combines autoregressive (AR) process and moving average (MA) processes and builds a composite model of the time series <cit.>. As acronym indicates, ARIMA(p, d, q) captures the key elements of the model:\n\n         \n  * AR: Autoregression. A regression model that uses the dependencies between an observation and a number of lagged observations (p).\n\n    \n  * I: Integrated. To make the time series stationary by measuring the differences of observations at different time (d).\n\n    \n  * MA: Moving Average. An approach that takes into accounts the dependency between observations and the residual error terms when a moving average model is used to the lagged observations (q).\n    \nIn general a time series {x_t}_t generated from an ARIMA(p, d,q) model has the form\n\n    \u0394^d y_t = \u2211_i=1^p \u03d5_i \u0394^d y_t-i+ \u2211_i=0^q \u03b8_i \u03b5_i.\n\n Here, \u0394  denotes the difference operator, p is the lag order of the AR process, \u03d5 is the coefficient of each parameters p, q is the order of the MA process, \u03b8 is the coefficient of each parameter q, and \u03b5_t denotes the residuals of the errors at time t.\n\n\n\n\n  \nSARIMA\nWith seasonal time series data, it is likely that short run non-seasonal components contribute to the model. Therefore, we need to estimate seasonal ARIMA model, which incorporates both non-seasonal and seasonal factors in a multiplicative model <cit.>. The general form of a seasonal ARIMA model is denoted as SARIMA(p, d, q)(P, D, Q)_m, where p is the non-seasonal AR order, d is the non-seasonal differencing, q is the non-seasonal MA order, P, D and Q are the similar parameters for the seasonal part. The Parameter m denotes the number of time steps for a single period. \n\n\n\n\n  \nTBATS\n\nTBATS model is an exponential smoothing method, which includes Box-Cox Transformation, ARMA model for residuals, trigonometric terms for modeling seasonality <cit.>. The trigonometric seasonality expression can significantly reduce model parameters at high seasonality frequencies and at the same time offer the model plasticity to compromise with complex seasonality. This model can also be used for forecasting time series with multiple seasonality.\n\n\n\n\n \u00a7.\u00a7 Machine Learning Methods\n\nMachine learning methods are increasingly being used to address time series prediction problems.  In many applications and studies, neural networks or familiar approaches are more often used  than other general machine learning approaches. Thus, we decided to focus on the popular tree-based ensemble learners, which are briefly introduced below. \n\n \n\n  \nXGBoost\n Gradient boosting is a powerful ensemble machine learning technique, that is used in classification and regression problems, while is also famous in predictive scenarios <cit.>. As an ensemble technique, gradient boosting combines the results of several weak learners, referred to as base learners, to build a model that performs generally better than the conventional single machine learning models. Typically, gradient boosting utilizes decision trees as base learners. Like other boosting methods, the core idea of gradient boosting is that during the learning procedure new models are build and fitted consecutively and not independently, to provide better predictions of the output variable. New base learners are constructed aiming to minimize a loss function, associated with the whole ensemble. Instances that are not predicted correctly in previous steps and score higher errors are correlated with larger weight values, so the model can focus on them, and learn from its mistakes.\n\n\nXGBoost stands for Extreme Gradient Boosting and is a specific implementation of gradient boosting, that was initially developed at the University of Washington as a research project <cit.>. Specific details that differ in XGBoost than traditional gradient boosting techniques, make this implementation more powerful, resulting in better performance. Specifically, XGBoost uses advanced regularization which improves model generalization and reduces overfitting. Moreover, computes second-order gradients of the loss function, which provide more information about the gradient\u2019s direction, making it easiest to minimize the loss function. \n\nThe important hyperparameters for the XGBoost are:\n\n    \n  * The learning parameter, which controls how much information from a new tree will be used in the Boosting.\n    \n  * The maximum depth of the trees.\n\n\n\n\n\n  \nRandom Forest\nA Random Forest is a machine learning method based on\nbuilding ensembles of decision trees. It was developed to address predictive shortcomings\nof traditional Classification and Regression Trees (CARTs) <cit.>. Random Forests consist of a large number\nof weak decision tree learners, which are grown in parallel to reduce the bias and variance\nof the model at the same time <cit.>. For training a Random Forest, \nbootstrap samples are drawn from the training data set. Each bootstrap sample is\nthen used to grow an unpruned tree. Instead of using all available features in this step,\nonly a small and fixed number of randomly sampled m_try features are selected as split\ncandidates.  A split is chosen by the CART-split criterion for regression, i.e. by minimizing the sum of squared errors in both child nodes. Instead of the CART-split criterion, many\nother distances such as the least absolute deviations of the mean (L1-norm) can also be used. These steps are then repeated until B such trees are grown, and new data is predicted by taking the mean of all B tree predictions.\nThe important hyperparameters for the Random Forest are:\n\n    \n  * B as the number of grown trees. Note that this parameter is usually not tuned since it is known that more trees are better.\n    \n  * The cardinality of the sample of features at every node is m_try.\n    \n  * The minimum number of observations that each terminal node should contain (stopping criteria).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 SIMULATION SET-UP\n\n\n\n\n\n\n\nIn our simulation study, we compare the one step forecast prediction power of the methods described in Section 2. All simulations were conducted in the statistical computing software  <cit.>. For the time series approaches we use the  package <cit.>. For the machine learning methods, we used the  <cit.> and <cit.> packages, respectively.\n\nThe concrete simulation settings and data generating processes are described below. \n\n\n  \nData Generating Processes\nTwelve data generating processes - an autoregressive model (AR), two bilinear models, two nonlinear autoregressive models (NAR), a nonlinear moving average model (NMA), two sign autoregressive models, two smooth transition autoregressive models and two TAR models-  are considered as summarized in Table\u00a0<ref>, where the error terms \u03b5_t are independent and identically distributed with a standard normal distribution. These are similar to those used in <cit.>. In particular the data generating processes included in our study display different degrees of nonlinearity, with some having very little nonlinearity (such as AR), while others exhibit highly nonlinear relationships between past and present values (such as SAR). Moreover, these time series models represent a variety of problem which have different characteristics. \n\n\n\n\nTo add more complexity to the analysis, we have incorporated a jump process and a random walk into each data generating process. The jump process introduces sudden regime changes, while the random walk adds noise to the data. Our study considers four different scenarios: (1) the data generating process without additional complexity, (2) the data generating process superposed with the jump process, (3) the data generating process superposed with random noise, and (4) the data generating process superposed with both the jump process and random noise.\n\n\nThe jumps are modeled using a compund Poisson process {p_t}_t. The orginal data generating process  {x_t}_t is then superposed by p_t as follows\n\n    x_t^*=x_t+p_t,\n\nwhere x_t^* dentoes the resulting data generating process and the compund Poisson process is given by\n\n    p_t=\u2211_i=1^N_t Z_i,\n\nwith N_t follows a Poisson distribution with parameter \u03bb and Z_i \u223c\ud835\udca9(0,\u03c3_p^2). For the jump experiments we set \u03c3_p^2 to 1. A larger \u03c3_p^2 results in larger jumps in magnitude, while the mean over positive and negative jumps remains zero. The parameter \u03bb is set to n/10, where n denotes the\nlength of the generated time series. This means that, on average, a jumpis expected to occur after every \u03bb periods.\nSuperposing the data generating process with the compound Poisson process results in a mean shift by the actual jump size that occurred at each jump event.\n\n\nAs mentioned before the noise is modeled by a random walk {w_t}_t with\n\n    w_t= w_t-1+e_t,\n\nwhere e_t\u223c\ud835\udca9(0, \u03c3_rw^2). In our study we choose \u03c3_rw^2 in such a way that we obtain a setting with medium noise, i.e. a signal to noise ratio of 4.  By including the random walk, we achieve a resulting data generating process that is globally nonstationary due to the random walk overlay.\n\n\nAdditionally, we  have included the M/M/1 and M/M/2 queueing models  <cit.> to our study.  Queueing models are commonly used in operations research and industrial engineering to study the behavior of waiting lines or queues <cit.>. Both models have numerous real-world applications, such as in call centers, healthcare facilities, and transportation systems. The M/M/1 model is a classic queueing model that assumes a single queue and one server. It is a stochastic model where customer arrivals are assumed to follow a Poisson process and service times are exponentially distributed. The M/M/1 model can be used to analyze the expected waiting time, the expected number of customers in the queue, and the expected utilization of the server.\nThe M/M/2 model is a variation of the M/M/1 model that assumes two parallel servers. We set the arrival rate to 4 and the service rate to 2. \n\n\nFor each setting, we generated time series of length n from the respective data generating processes with n\u2208{100, 500, 1000}. In total, this results in (12 (time series data generating processes) \u00d7 3 (further complexity)+ 2 queueing models) \u00d7 3 (lengths) = 114 different simulation settings for each of the forecasting methods.\n\n\n\n  \nData preprocessing\nTo forecast time series using a machine learning algorithm, the sliding window approach is used. In this approach, a fixed-sized window is moved over the time series data, and at each step, the data within the window is used as input to a machine learning algorithm for prediction. One advantage of the sliding window approach is that it allows the machine learning algorithm to capture the temporal dependencies and patterns in the data. The window size is an important parameter in this approach. If the window size is too small, it may not capture the relevant information in the data, while if it is too large, it may introduce unnecessary noise and reduce the accuracy of the model. We considered sliding window sizes of 2, 4, 8 and 16. \nMoreover, in machine learning-based time series forecasting setting, we use the original time series and the differentiated time series as input.\n\n\n\n  \nChoice of Parameters\nIn order not to have to discuss the different possibilities for hyperparameter tuning of the machine learning algorithm, we use the default values recommended in the literature <cit.>. This has the additional advantage of a reduced runtime. Thus, each ensemble learner consists of 500 trees, the inner bootstrap sample is equal to m_try=\u230ap3\u230b, where p denotes the number of features, the number of sample point in the bagging step is equal to the number of sample size. Each terminal node should at least contain five observations. For XGBoost we used a learning rate of 0.3 and a maximal depth of 6.\n\n\nTo estimate the hyperparameters of the time series approaches we used the algorithms implemented in the R-package .\n\n \n\n  \nEvaluation Measure\nTo evaluate the predictive power the mean squared error (MSE) is used and the forecast step is repeated 1.000 times. More precisely, the  MSE is given by\n\n    1/1000\u2211_i=1^1000( x_i-x\u0302_i)^2,\n \nwhere  x_i are the observed values and x\u0302_i are the predicted values in the i-th forecasting step.\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\u00a7 RESULTS\n\n \n\n\n\n\n\nIn this section, we describe the results of the simulation study. In particular, we present the MSE\nof the different forecasting algorithms under various simulation configurations. \n\n\n \u00a7.\u00a7 Predictive Power in Queueing Models\n\n\n\nThe influence of the different sliding window sizes and of the differentiation is shown in Figures <ref> and <ref>.\nGenerally, we observe that in both setting differentiation improves the prediction power of both ML approaches. Especially for the Random Forest, the MSE decreases by one fifth after differentiation. The length of the time series just have a small influence on the MSE. For all lengths the Random Forest with differentiated data outperformed the other methods. Comparing the effects of sliding window size, we find small differences in performance. Random Forests have smaller MSE values with smaller sliding windows in both settings, while larger window sizes slightly improve performance in the other approaches. \n\n\nThe predictive power of the time series and naive approaches can be seen in Figure <ref>.\n\nNote that both ARIMA and SARIMA models have identical MSE values. In both cases, we obtain that the time series approaches perform better than the naive approach. However, the difference in performance is smaller for M/M/2. Again, the influence of the time series length is minimal. While all time series approaches perform similarly in the M/M/1 setting, the TBATS method has slightly smaller values in the M/M/2 setting.\n\n\nAcross both settings, the Random Forest approach with differentiated data have the smallest MSE. However, the differences between this method and the time series approaches. were not large.\n\n\n\n\n \u00a7.\u00a7 Predictive Power in Time Series Settings\n\nWhen comparing the influence of sliding window size and differentiation on the performance \nof Random Forest across all settings (Figure\u00a0<ref>),\n\nwe observed that, except for the AR setting, non-differentiation resulted in smaller MSE values. In the AR setting, differentiation slightly outperformed non-differentiation. However, it should be noted that as the length of the time series increases, the differences between the two approaches narrow. In all settings, the MSE values slightly decrease with an increase in time series length. The sliding window size has a small influence on the prediction power and shows similar behavior across different time series lengths.\n\n \n Similar observations can be made for XGBoost in Figure <ref>. The size of the sliding window and the length of the time series have a small effect on the quality of the performance results. For all data generating processes, the MSE values decrease slightly with increasing time series length, except for BL1. Here, the MSE values increase initially.  In general, the XGBoost approaches have slightly larger MSE values than the Random Forest approaches. \n\n\n\n \n In Figure <ref> the MSE values for the time series approaches are shown. \n \nThe performance of time series approaches is comparable to that of Random Forest results. All methods have very similar MSE values. The time series length has only a minor impact on the predictive power, except for the BL1 setting. As observed for the XGBoost approaches, MSE values in this setting first increase and then decrease with increasing time series length. \n\n\nFigure <ref> in the Appendix shows that the naive approach has the largest MSE values in comparisation to the other. However, for some settings, the difference in performance between the XGBoost approaches and the naive method is minimal.The performance of the naive approach is dependent on the data generating process and the length of the time series. For BL2, longer length of the time series generally leads to better performance but for NAR1, performance may slightly decrease. For AR, BL1, and NMA models, the MSE values typically decrease initially and then slightly increase as the time series length increases. Conversely, NAR2, SAR1, SAR2, STAR1, STAR2, TAR1 and TAR2 tend to show the opposite trend.\n\n\n\n\n\n \u00a7.\u00a7 Influence of the Additional Complexities on the Predictive Power\n\nBased on the findings of the previous sections, we focus on the simulation results obtained with a sliding window size of 8. Details on the other results can be found in the Appendix.\n\n\nThe influence of the jumping process can be seen in Figure\u00a0<ref>. All MSE values increase monotonically with increasing sample size, indicating that the jump process has a large impact on predictive performance. Note that as time series length increases, the Random Forest approach with differentiated data outperforms all other approaches. Using the differenced data significantly improves the MSE values for both ML approaches, with differences increasing with increasing time series length. The performance of the time series approaches is similar for all data generating processes and slightly better than that of the naive approach.\n\n\n\n\n\nFigure\u00a0<ref> summarizes the prediction results for all methods and all data generating processes superposed by a random walk.\n\nThe time series length has only a small impact on the prediction performance of the data polluted by a random walk. While for the AR setting and BL2 setting the MSE values slightly increase when increasing the time series lengths from 100 to 500, in all other data generating processes the MSE values are minimal decreasing, expect for the naive approach. In all settings the naive approach has the highest MSE values followed by XGBoost, expect for BL2. Here, the both approaches have similar values. The performance of the other methods are dependent on the specific setting.\n\n\nFor the AR, BL2, SAR1 and SAR2  settings, the Random Forests with differenced data have the smallest MSE values, while the time series approaches have slightly larger ones.  Note that in these settings the XGBoosts with differenced data perform better than Random Forests with with non-differentiated data. Only slightly differences in the performance of the Random Forests and time series approaches are observed in the BL1, NAR1, NAR2, NMA and STAR2 settings. When comparing the two XGBoost approaches in these settings, it can be seen that differentiation leads to a decrease in the MSE values. The ML approaches have larger MSE values than the time series approaches in the STAR1, TAR1 and TAR2 setting, where Random Forests perform better than the XGBoost method.\n\n\nThe influence of both the random walk and the Poisson process on the prediction performance is presented in Figure\u00a0<ref> in the Appendix. Similar to the case when the data is superposed by a compound Poisson process, we observe an increase in MSE values with increasing time series lengths for all settings. Notably, for time series lengths of 500, we obtain MSE values larger than 2000.\n\n\n\n\n\n\n \n\n\u00a7 DISCUSSION AND OUTLOOK\n\n \n\n\n\n\n\n\nThe main purpose of this simulation study was to compare the predictive accuracy of one-step forecasts using both, tree-based machine learning (ML) and time series approaches.\n\n\n\nIn most of the simulation settings either the tree-based ML or the time series approaches outperformed the other.  However, comparing the simulation settings, advantages for the ML methods in terms of performance can be observed in the queueing settings and in  simulation settings, where the data generating processes are superposted by a Poisson process. In all other simulation settings both ML approaches have shown at least similar or only slightly worse performances compared to the time series approaches.\n\nAnother interesting finding was that differantation of the data showed substantial impacts on the performance of the ML approaches in some simulation settings.\n \n\nThe additional complexities (adding jump process or random noise) have a high influence on the predictive power:\nIntroducing a jump process to the data results in high MSE values for all methods and settings, whereas superposing the data with additional noise can even lead to performance power dependending on the setting.\n\n\nAs only one step forecasts were considered, future simulation studies should investigate whether the same observations can also be found for more step forecasting. Also, additional or even hybrid methods have to be investigated <cit.>. Another line of future research needs to compare the methods with respect to uncertainty quantification, i.e. point-wise or simultaneous prediction intervals and regions. \nMoreover, as only simulated data sets were used, the behavior of these approaches in real dataset applications should also be further researched. \n\n\n\n\n\n  \n\n\n\n\u00a7 ADDITIONAL SIMULATION RESULTS\n\n\n\n\n \u00a7.\u00a7 Influence of Jump Process\n\nFigure <ref> and <ref> summarize the prediction results for all sliding window sizes and data generating processes using the ML methods. For both methods applied to differenced data, the performance is quite similar across the different windows sizes. However, a small difference in  MSE values can be observed for the Random Forests, where a smaller window size slightly improves the prediction power.\n\n\n\n\n\n\n \u00a7.\u00a7 Influence of additional Noise\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Influence of additional Noise and Jump Process\n\n\n\n\n\n"}