{"entry_id": "http://arxiv.org/abs/2303.07334v1", "published": "20230313175656", "title": "Assessing the performance of spatial cross-validation approaches for models of spatially structured data", "authors": ["Michael J Mahoney", "Lucas K Johnson", "Julia Silge", "Hannah Frick", "Max Kuhn", "Colin M Beier"], "primary_category": "stat.CO", "categories": ["stat.CO", "stat.ME"], "text": "\nRevisiting Class-Incremental Learning with Pre-Trained Models:\n\nGeneralizability and Adaptivity are All You Need\n    Da-Wei Zhou^1, Han-Jia Ye^1[2], De-Chuan Zhan^1, Ziwei Liu^2[2]\n\n\t^1 State Key Laboratory for Novel Software Technology, Nanjing University\n\t^2 \n\tS-Lab, Nanyang Technological University\n\n\t{zhoudw, yehj, zhandc}@lamda.nju.edu.cn,  ziwei.liu@ntu.edu.sg\n\n    March 30, 2023\n===============================================================================================================================================================================================================================================================\n\n\nEvaluating models fit to data with internal spatial structure requires\nspecific cross-validation (CV) approaches, because randomly selecting\nassessment data may produce assessment sets that are not truly\nindependent of data used to train the model. Many spatial CV\nmethodologies have been proposed to address this by forcing models to\nextrapolate spatially when predicting the assessment set. However, to\ndate there exists little guidance on which methods yield the most\naccurate estimates of model performance.\n\nWe conducted simulations to compare model performance estimates produced\nby five common CV methods fit to spatially structured data. We found\nspatial CV approaches generally improved upon resubstitution and V-fold\nCV estimates, particularly when approaches which combined assessment\nsets of spatially conjunct observations with spatial exclusion buffers.\nTo facilitate use of these techniques, we introduce the\n package which provides tooling for performing\nspatial CV as part of the broader tidymodels modeling framework.\n\nKeywords\n\u2022 \ncross-validation spatial data machine learning random\nforests simulation\n\nShaded[breakable, borderline west=3pt0ptshadecolor, sharp corners, enhanced, interior hidden, frame hidden, boxrule=0pt]\n\nsec-introduction\n\n\n\u00a7 INTRODUCTION\n\n\nEvaluating predictive models fit using data with internal spatial\ndependence structures, as is common for Earth science and environmental\ndata (Legendre and Fortin 1989), is a difficult task. Low-bias models,\nsuch as the machine learning techniques gaining traction across the\nliterature, may overfit on the data used to train them. As a result,\nmodel performance metrics may be nearly perfect when models are used to\npredict the data used to train the model (the \u201cresubstitution\nperformance\u201d of the model) (Kuhn and Johnson 2013), but deteriorate\nwhen presented with new data.\n\nIn order to detect a model's failure to generalize to new data, standard\ncross-validation (CV) evaluation approaches assign each observation to\none or more \u201cassessment\u201d sets, then average performance metrics\ncalculated against each assessment set using predictions from models fit\nusing \u201canalysis\u201d sets containing all non-assessment observations. We\nrefer to the assessment set, which is \u201cheld out\u201d from the model\nfitting process, as D_out, while we refer to the\nanalysis set as D_in. Splitting data between\nD_out and D_in is typically\ndone randomly, which is sufficient to determine model performance on\nnew, unrelated observations when working with data whose variables are\nindependent and identically distributed. However, when models are fit\nusing variables with internal spatial dependence (often referred to as\nspatial autocorrelation), random assignment will likely assign\nneighboring observations to both D_out and\nD_in. Given that neighboring observations are often\nmore closely related (Legendre and Fortin 1989), this random assignment\nyields similar results as the resubstitution performance, providing\nover-optimistic validation results that over state the ability of the\nmodel to generalize to new observations or to regions not\nwell-represented in the training data (Roberts et al. 2017; Bahn and\nMcGill 2012).\n\nOne potential solution for this problem is to not assign data to\nD_out purely at random, but to instead section the\ndata into \u201cblocks\u201d based upon its dependence structure and assign\nentire blocks to D_out as a unit (Roberts et al.\n2017). For data with spatial structure, this means assigning\nobservations to D_out and D_in\nbased upon their spatial location, in order to increase the average\ndistance between observations in D_out and those\nused to train the model. The amount of distance required to ensure\naccurate estimates of model performance is a matter of some debate, with\nsuggestions to use variously the variogram ranges for model predictors,\nmodel outcomes, or model residuals (Le Rest et al. 2014; Roberts et al.\n2017; Telford and Birks 2009; Valavi et al. 2018; Karasiak et al. 2021).\nHowever, it is broadly agreed that increasing the average distance\nbetween observations in D_out and those used to\ntrain the model may produce more accurate estimates of model\nperformance.\n\nThis objective \u2013 evaluating the performance of a predictive model \u2013 is\nsubtly but importantly distinct from evaluating the accuracy of a map of\npredictions. Map accuracy assessments assume a representative\nprobability sample in order to produce unbiased accuracy estimates\n(Stehman and Foody 2019), while assessments of models fit using spatial\ndata typically assume a need to estimate model performance without\nrepresentative and independent assessment data. Such situations emerge\nfrequently across model-based studies (Gruijter and Braak 1990; Brus\n2020), such as during hyperparameter tuning (Schratz et al. 2019); when\nextrapolating spatially to predict into \u201cunknown space\u201d without\nrepresentative assessment data (Meyer and Pebesma 2021, 2022); or when\nworking with data collected via non-representative convenience samples\n(any non-probability sample where observations are collected on the\neasiest to access members of a population) as commonly occurs in ecology\nand environmental science (Martin, Blossey, and Ellis 2012; Yates et al.\n2018). In these situations, understanding the predictive accuracy of the\nmodel on independent data is essential. Although recent research has\nargued against spatial CV for map accuracy assessments (Wadoux et al.\n2021), spatial CV remains an essential tool for evaluating the\nperformance of predictive models.\n\nFor many situations spatial CV has been shown to provide empirically\nbetter estimates of model performance than non-spatial methods (Bahn and\nMcGill 2012; Schratz et al. 2019; Meyer et al. 2018; Le Rest et al.\n2014; Ploton et al. 2020), and as such is popularly used to evaluate\napplied modeling projects across domains (Townsend, Pape\u015f, and Eaton\n2007; Meyer et al. 2019; Adams et al. 2020). As such, a variety of\nmethods for spatially assigning data to D_out have\nbeen proposed, many of which have been shown to improve performance\nestimates over randomized CV approaches. However, the lack of direct\ncomparisons between alternative spatial CV approaches makes it difficult\nto know which methods may be most effective for estimating model\nperformance. Understanding the different properties of various CV\nmethods is particularly important given that many spatial modeling\nprojects rely on CV for their primary accuracy assessments (Bastin et\nal. 2019; Fick and Hijmans 2017; Hoogen et al. 2019; Hengl et al. 2017).\n\nHere, we evaluated the leading spatial CV approaches found in the\nliterature, using random forest models fit on spatially structured data\nfollowing the simulation approach of Roberts et al. (2017). To\nfacilitate comparison among methods, we offer a useful taxonomy and\ndefinition of these CV approaches, attempting to unify disparate\nterminology used throughout the literature. We then provide\ncomprehensive overview of the performance of these spatial CV methods\nacross a wide array of parameterizations, providing the first\ncomparative performance evaluation for many of these techniques. We\nfound that spatial CV methods yielded overall better estimates of model\nperformance than random assignment. Approaches that incorporated both\nD_out of spatially conjunct observations and\nbuffers yielded the best results. Lastly, to facilitate further\nevaluation and use of spatial CV methods, we introduce the\n R package that implements each of the approaches\nevaluated here, and avoids many of the pitfalls encountered by prior\nimplementations.\n\nspatialsample-and-the-tidymodels-framework\n\n\n\u00a7 SPATIALSAMPLE AND THE TIDYMODELS FRAMEWORK\n\n\nThe tidymodels framework is a set of open-source packages for the R\nprogramming language that provide a consistent interface for common\nmodeling tasks across a variety of model families and objectives\nfollowing the same fundamental design principles as the tidyverse (R\nCore Team 2022; Kuhn and Silge 2022; Wickham et al. 2019). These\npackages help users follow best practices while fitting and evaluating\nmodels, with functions and outputs that integrate well with the other\npackages in the tidymodels and tidyverse ecosystems as well as with the\nrest of the R modeling ecosystem. Historically, data splitting and CV in\nthe tidymodels framework has been handled by the \npackage (Frick et al. 2022), with additional components of the\ntidymodels ecosystem providing functionality for hyperparameter tuning\nand model evaluation (Kuhn 2022; Kuhn and Frick 2022). However,\n primarily focuses on randomized CV approaches, and as\nsuch it has historically been difficult to implement spatial CV\napproaches within the tidymodels framework.\n\nA new tidymodels package, , addresses this gap by\nproviding a suite of functions to implement the most popular spatial CV\napproaches. The resamples created by  rely on the\nsame infrastructure as those created by , and as such\ncan make use of the same tidymodels packages for hyperparameter tuning\nand model evaluation. As an implementation of spatial CV methods,\n improves on alternate implementations in R by\nrelying on the  and  packages for calculating\ndistance in geographic coordinate reference systems, improving the\naccuracy of distance calculations and CV fold assignments when working\nwith geographic coordinates (Pebesma 2018; Dunnington, Pebesma, and\nRubak 2021). The  package is also able to\nappropriately handle data with coordinate reference systems that use\nlinear units other than meters and to process user-provided parameters\nusing any units understood by the  package (Pebesma,\nMailund, and Hiebert 2016). The  package also\nallows users to perform spatial CV procedures while modeling data with\npolygon geometries, such as those provided by the US Census Bureau.\nDistances between polygons are calculated between polygon edges, rather\nthan centroids or other internal points, to ensure that adjacent\npolygons are handled appropriately by CV functions. Finally,\n provides a standard interface to apply inclusion\nradii and exclusion buffers to all CV methodologies\n(Section\u00a0<ref>), providing a high degree of flexibility for\nspecifying spatial CV patterns.\n\nsec-overview\n\n\n\u00a7 RESAMPLING METHODS\n\n\nThis paper evaluates a number of the most popular spatial CV approaches,\nbased upon their prevalence across the literature and software\nimplementations. We focus on CV methods which automatically split data\neither randomly or spatially but did not address any assessment methods\nthat divide data based upon pre-specified, user-defined boundaries or\npredictor space. As such, we use \u201cdistance\u201d and related terms to refer\nto spatial distances between observations, unless we specifically refer\nto \u201cdistance in predictor space\u201d.\n\nresubstitution\n\n\n \u00a7.\u00a7 Resubstitution\n\n\nEvaluating a model's performance when predicting the same data used to\nfit the model yields what is commonly known as the \u201capparent\nperformance\u201d or \u201cresubstitution performance\u201d of the model (Kuhn and\nJohnson 2013). This procedure typically produces an overly-optimistic\nestimate of model performance, and as such is a poor method for\nestimating how well a model will generalize to new data (Efron 1986;\nGong 1986; Efron and Gong 1983). We include an assessment of\nresubstitution error in this study for comparison, but do not recommend\nit as an evaluation procedure in practice.\n\nrandomized-v-fold-cv\n\n\n \u00a7.\u00a7 Randomized V-fold CV\n\n\nPerhaps the most common approach to CV is V-fold CV, also known as\nk-fold CV. In this method, each observation is randomly assigned to one\nof v folds. Models are then fit to each unique combination of\nv - 1 folds and evaluated against the remaining fold, with\nperformance metrics estimated by averaging across the v iterations\n(Stone 1974). V-fold CV is generally believed to be the least biased of\nthe dominant randomized CV approaches (Kuhn and Johnson 2019), though\nresearch has suggested it overestimates model performance (Varma and\nSimon 2006; Bates, Hastie, and Tibshirani 2021). This optimistic bias is\neven more notable when training models using data with internal\ndependency structures, such as spatial structure (Roberts et al. 2017).\n\nFor this study, V-fold CV was performed using the \nfunction in .\n\nblocked-cv\n\n\n \u00a7.\u00a7 Blocked CV\n\n\nOne of the most straightforward forms of spatial CV is to divide the\nstudy area into a set of polygons using a regular grid, with all\nobservations inside a given polygon assigned to\nD_out as a group (Valavi et al. 2018; Brenning\n2012; Wenger and Olden 2012). This technique is known as \u201cspatial\nblocking\u201d (Roberts et al. 2017) or \u201cspatial tiling\u201d (Brenning 2012).\nFrequently, each block is used as an independent\nD_out (leave-one-block-out CV, Wenger and Olden\n2012), though many implementations allow users to use fewer\nD_out, combining multiple blocks into sets either\nat random or via a systematic assignment approach (Valavi et al. 2018).\nSpatial blocking attempts to address the limitations of randomized\nV-fold CV by introducing distance between D_in and\nD_out, though the distance from observations on the\nperimeter of a block to D_in will be much less than\nthat from observations near the center of the block (O'Sullivan and\nUnwin 2010). This disparity can be addressed through the use of an\nexclusion buffer around D_out, wherein points\nwithin a certain distance of D_out (depending on\nthe implementation, calculated alternatively as distance from the\npolygon defining each block, the convex hull of points in\nD_out, or from each point in\nD_out independently) are excluded from both\nD_out and D_in (Valavi et al.\n2018).\n\nA challenge with spatial blocking is that dividing the study area using\na standard grid often results in observations in unrelated areas being\ngrouped together in a single fold, as regular gridlines likely will not\nalign with relevant environmental features (for instance, blocks may\nspan significant altitudinal gradients, or reach across a river to\ncombine disjunct populations). Although this can be mitigated through\ncareful parameterization of the grid, it is difficult to create\nmeaningful D_out while still enforcing the required\nspatial separation between D_in and\nD_out.\n\nFor this study, spatial blocking was performed using the\n function in . Each\nblock was treated as a unique fold (leave-one-block-out CV).\n\nclustered-cv\n\n\n \u00a7.\u00a7 Clustered CV\n\n\nAnother form of spatial CV involves grouping observations into a\npre-specified number of clusters based on their spatial arrangement, and\nthen treating each cluster as a D_out in V-fold CV\n(Brenning 2012; Walvoort, Brus, and Gruijter 2010). This approach allows\nfor a great degree of flexibility, as alternative distance calculations\nand clustering algorithms may produce substantially different clusters.\nSimilarly to spatially-blocked CV this approach may produce unbalanced\nD_out and folds that combine unrelated areas,\nthough in practice most clustering algorithms typically produce more\nsensible fold boundaries than spatial blocking. Clustering is also\nsimilar to spatial blocking in that observations closer to the center of\na cluster will be much further spatially separated from\nD_in than those near the perimeter, although\nclustering algorithms typically produce more circular tiles than\nblocking methods, and as such generally have fewer points along their\nperimeter overall. As with spatial blocking, exclusion buffers can be\nused to ensure a minimum distance between D_in and\nD_out.\n\nA notable difference between spatial clustering and spatial blocking is\nthat, depending upon the algorithm used to assign data to clusters,\ncluster boundaries may be non-deterministic. This stochasticity means\nthat repeated CV may be more meaningful with clustered CV than with\nspatial blocking, but also makes it difficult to ensure that cluster\nboundaries align with meaningful boundaries.\n\nFor this study, spatial clustering was performed using the\n function in .\nEach cluster was treated as a unique fold (leave-one-cluster-out CV).\n\nbuffered-leave-one-observation-out-cv-blo3-cv\n\n\n \u00a7.\u00a7 Buffered Leave-One-Observation-Out CV (BLO3\nCV)\n\n\nAn alternative approach to spatial CV involves performing leave-one-out\nCV, a form of V-fold cross validation where v is set to the number\nof observations such that each observation forms a separate\nD_out, with all points within a buffer distance of\nD_out omitted from D_in\n(Telford and Birks 2009; Pohjankukka et al. 2017). As the other methods\ninvestigated here are all examples of leave-one-group-out CV, with\ngroups defined by spatial positions, we refer to this procedure as\nbuffered leave-one-observation-out CV (BLO3 CV). This approach may be\nmore robust to different parameterizations than spatial clustering or\nblocking, as the contents of a given D_out are not\nas dependent upon the precise locations of blocking polygons or cluster\nboundaries. Many studies have recommended buffered leave-one-out cross\nvalidation for models fit using spatial data, with the size of the\nexclusion buffer variously determined by variogram ranges for model\npredictors, model outcomes, or model residuals (Le Rest et al. 2014;\nRoberts et al. 2017; Telford and Birks 2009; Valavi et al. 2018;\nKarasiak et al. 2021).\n\nFor this study, BLO3 CV was performed using the\n function in\n.\n\nleave-one-disc-out-cv-lodo-cv\n\n\n \u00a7.\u00a7 Leave-One-Disc-Out CV (LODO\nCV)\n\n\nThe final spatial CV method investigated here is leave-one-disc-out CV,\nfollowing Brenning (2012). This method extends BLO3 by adding all points\nwithin a certain radius of each observation to\nD_out, increasing the size of the final\nD_out. Data points falling within the exclusion\nbuffer of any observation in D_out, including those\nadded by the inclusion radius, is then removed from\nD_out (Figure\u00a0<ref>). Similarly to blocked\nand clustered CV, LODO CV evaluates models against multiple\nD_out of spatially conjunct observations. Similar\nto BLO3 CV, LODO CV approach may be more robust to different parameter\nvalues than methods assigning D_out based upon\nblocking polygons or cluster boundaries. Unlike any of the other\napproaches investigated, observations may appear in multiple\nD_out, with observations in more intensively\nsampled regions being selected more often.\n\nIn this study, spatial leave-one-disc-out CV was performed using the\n function in\n. An important feature of this implementation of\nleave-disc-out CV is that the exclusion buffer is calculated separately\nfor each point in D_out. Where other\nimplementations remove all observations within the buffer distance of\nthe inclusion radius to create a uniform \u201cdoughnut\u201d shaped buffer,\n only removes observations that are within the\nbuffer distance of data in D_out, potentially\nretaining more data in D_in by creating an\nirregular buffer polygon.\n\nsec-methods\n\n\n\u00a7 METHODS\n\n\nsec-simulation\n\n\n \u00a7.\u00a7 Landscape Simulation\n\n\nTo compare the validation techniques described above, we extended the\nsimulation approach used by Roberts et al. (2017, Box 1). We simulated\n100 landscapes, representing independent realizations of the same\ndata-generating process, generating a set of 13 variables calculated\nusing the same stochastic formulations across a regularly spaced 50 x 50\ncell grid, for a total of 2,500 cells per landscape\n(Table\u00a0<ref>). Simulated predictors included eight\nrandom Gaussian fields, generated using the  R\npackage (Schlather et al. 2015), which uses stationary isotropic\ncovariance models to generate spatially structured variables. Five\nadditional variables were calculated as combinations of the randomly\ngenerated variables to imitate interactions between environmental\nvariables. This simulation approach was originally designed to resemble\nthe environmental data that might be used to model species abundance and\ndistribution; further interpretation of what each predictor represents\nis provided in Appendix 2 of Roberts et al. (2017).\n\ntbl-simulations\n\n\nThese predictors were then used to generate a target variable y\nusing Equation\u00a0<ref>.\n\n\n    eq-y\n    y = \n    {[                   min(y),                  if X4\u2260 0;                      X11,                 if  y\u2265X11; X1 + X5 + X6 + X12 + X13,                 otherwise ]}\n\n\nOne instance of the spatially clustered y values produced by this\nprocess is visualized in Figure\u00a0<ref>.\n\n\n\nModels were then fit using variables X2, X3, and X6 - X10. Of these\nseven variables, three were involved in calculating the target variable\ny (X2 and X3, as components of X4 and X5; and X6, used directly) and\ntherefore provide useful information for models, while the remaining\nfour (X7 - X10) were included to allow overfitting.\n\nsec-resampling\n\n\n \u00a7.\u00a7 Resampling Methodology\n\n\nWe divided each simulated landscape into folds using each of the data\nsplitting approaches (Section\u00a0<ref>) across a wide range of\nparameter sets (Table\u00a0<ref>; Table\u00a0<ref>)\nin order to evaluate the usefulness of spatial CV approaches. Spatial\nblocking, spatial clustering, and leave-one-disc-out used a\n\u201cleave-one-group-out\u201d approach, where each D_out\nwas made up of a single block or cluster of observations, with all other\ndata (excluding any within the exclusion buffer) used as\nD_in. BLO3 used a leave-one-observation-out\napproach. We additionally evaluated spatial blocking with fewer\nD_out than blocks, resulting in multiple blocks\nbeing used in each D_out. Each simulated landscape\nwas resampled independently, meaning that stochastic methods (such as\nV-fold CV and spatial clustering) produced different CV folds across\neach simulation. All resampling used functions implemented in the\n and  packages (Frick et al. 2022;\nMahoney and Silge 2022). Examples of spatial clustering CV, spatially\nblocked CV, and leave-one-disc-out CV are visualized in\nFigure\u00a0<ref>.\n\ntbl-whichparams\n\n\ntbl-paramdefs\n\n\nsec-models\n\n\n \u00a7.\u00a7 Model Fitting and Evaluation\n\n\nFor each iteration, we modeled the target variable y using random\nforests as implemented in the  R package (Breiman 2001;\nWright and Ziegler 2017), fit using variables X2, X3, and X6 - X10.\nRandom forests generally provide high predictive accuracy even without\nhyperparameter tuning (Probst, Bischl, and Boulesteix 2018), and as such\nall random forests were fit using the default hyperparameter settings of\nthe  package, namely 500 decision trees, a minimum of 5\nobservations per leaf node, and two variables to split on per node.\n\nModel accuracy was measured using root-mean-squared error (RMSE,\nEquation\u00a0<ref>). To find the \u201cideal\u201d error rate that we would\nexpect CV approaches to estimate, we fit 100 separate random forest\nmodels, each trained using all values within one of the 100 simulated\nlandscapes. We then calculated the RMSE for each of these models when\nused to predict each of the 99 other landscapes. As each landscape is an\nindependent realization of the same data-generation process, the\nrelationships between predictors and y is identical across\nlandscapes, although the spatial relationships between y and\nvariables not used to generate y are likely different across\niterations. As such, RMSE values from a model trained on one landscape\nand used to predict the others represent the ability of the model to\npredict y based upon the predictors and without relying upon spatial\nstructure. These RMSE estimates therefore represent the \u201ctrue\u201d range\nof RMSE values when using these models for spatial extrapolation to\nareas with the same relationship between predictors and the target\nfeature, but without any spatial correlation to the training data\nitself. We defined the success of model evaluation methods as the\nproportion of iterations which returned RMSE estimates between the 5th\nand 9th percentile RMSEs of this \u201cideal\u201d estimation procedure.\n\nTo find the error rate of the resubstitution approach, we fit 100 random\nforests, one to each landscape, and then calculated the RMSE for each\nmodel when used to predict its own training data. To find the error of\neach CV approach, we first used each CV approach to separate each\nlandscape into n folds (Section\u00a0<ref>). We then fit\nmodels to each combination of n - 1 of these folds, and calculated\nRMSE when using the model to predict the remaining\nD_out (Equation\u00a0<ref>).\n\n\n    eq-rmseRMSE = \u221a((1/n)\u2211_i=1^n(y_i - \u0177_\u0302\u00ee)^2)\n\n\nWe then calculated the variance of the RMSE estimates of each method\nacross the 100 simulated landscapes, as well as the proportion of runs\nfor each method which fell between the 5th and 95th percentiles of the\n\u201ctrue\u201d RMSE range.\n\nBased upon prior research, we expected the optimal spacing between\nD_in and D_out to be related\nto the range of spatial dependence either in the outcome variable or in\nmodel residuals (Le Rest et al. 2014; Roberts et al. 2017; Telford and\nBirks 2009). As such, we quantified the range of spatial autocorrelation\nin both the target variable y and in resubstitution residuals from\nrandom forest models using the automated variogram fitting approach\nimplemented in the  R package (Hiemstra et al. 2008).\n\nsec-results\n\n\n\u00a7 RESULTS AND DISCUSSION\n\n\nspatial-cv-improves-model-performance-estimates\n\n\n \u00a7.\u00a7 Spatial CV Improves Model Performance\nEstimates\n\n\nSpatial cross-validation methods consistently produced more accurate\nestimates of model performance than non-spatial methods, which were\noptimistically biased (producing too-low estimates of RMSE)\n(Table\u00a0<ref>; Figure\u00a0<ref>). CV produced the\nbest estimates when D_out of spatially conjunct\nobservations were combined with exclusion buffers\n(Table\u00a0<ref>). Spatially clustered CV and LODO, both of\nwhich enforce D_out of spatially conjunct\nobservations, were among the most consistently effective CV methods\n(Figure\u00a0<ref>). Removing too much data from\nD_in, such as by clustering with only two folds or\nblocking with only two blocks resulted in pessimistic over-estimates of\nRMSE (Figure\u00a0<ref>).\n\ntbl-overall\n\n\n\n\ntbl-winners\n\n\n\n\nThe best parameter sets for CV methods consistently separated the center\nof D_out from D_in by\n25%-41% of the grid length (Clustering 25%-29%; LODO 36%-39%;\nBlocked 32%-41%; BLO3 24%-30%). Given that the target variable had a\nmean autocorrelation range of 24.61% of the grid length\n(Figure\u00a0<ref>), this suggests that spatial cross-validation\napproaches produce the best estimates of model performance when\nD_out is sufficiently separated from\nD_in such that there is no spatial dependency in\nthe outcome variable between the two sets.\n\n\n\nClustering appeared to be the spatial CV method most robust to different\nparameterizations (Figure\u00a0<ref>;\nFigure\u00a0<ref>), with the highest proportion of all\niterations within the target RMSE range (Clustered 36.97% of all\niterations; LODO 31.70%; Blocked 27.90%; BLO3 )\n(Figure\u00a0<ref>). This may, however, simply reflect the\nrelatively narrow range of parameters evaluated with clustering, as both\nblocking and LODO had a wide range of parameters which returned\nestimates within the target range at least half the time\n(Figure\u00a0<ref>). While BLO3 exhibited increasing RMSE with\nincreasing buffer radii (Figure\u00a0<ref>), as frequently\nreported in the literature, we found it only rarely produced RMSE\nestimates within the target range (Figure\u00a0<ref>).\n\n\n\nRMSE estimates from spatial blocking were inversely related to the\nnumber of D_out used, likely due to the distance\nbetween D_out and D_in\nincreasing if adjacent blocks were assigned to the same\nD_out (Figure\u00a0<ref>). RMSE\nestimates generally increased gradually as the number of\nD_out decreased, though notable increases were\nobserved when blocks were assigned via the continuous systematic method\nand the number of cells in each grid row were evenly divisible by the\nnumber of D_out (e.g., when 1/16th cell sizes\nproduced by a 4-by-4 grid were divided into 4 folds). In these\nsituations, each column of the grid will be entirely assigned to the\nsame D_out, somewhat resembling the CV method of\nWenger and Olden (2012), producing D_out which have\nno neighboring D_in observations in the y\ndirection and therefore a greater average distance between\nD_out and D_in. Overall, these\nresults suggest that using fewer D_out than blocks\nmay be appropriate when the range of autocorrelation in the outcome\nvariable is relatively small and there is concern about large blocks\nrestricting predictor space (Roberts et al. 2017); however, with longer\nautocorrelation ranges it is likely best to use a leave-one-block-out\napproach with fewer, larger blocks.\n\n\n\nAs such, the recommendations from this study are clear: CV-based\nperformance assessments of models fit using spatial data benefit from\nspatial CV approaches. Those spatial CV approaches are most likely to\nreturn good estimates of true model accuracy if they combine\nD_out of spatially conjunct observations with\nexclusion buffers, such that the average observation is separated from\nD_in by enough distance that there is no spatial\ndependency in the outcome variable between D_in and\nD_out.\n\nlimitations\n\n\n \u00a7.\u00a7 Limitations\n\n\nThis simulation study assumed that spatial CV could take advantage of\nregularly distributed observations, such that all locations had a\nsimilar density of measurement points. This assumption is often\nviolated, as it is often impractical to obtain a uniform sample across\nlarge areas, and as such observations are often clustered in more\nconvenient locations and relatively sparse in less accessible areas\n(Meyer and Pebesma 2022; Martin, Blossey, and Ellis 2012). Alternative\napproaches not investigated in this study may be more effective in these\nsituations; for instance, when the expected distance between training\ndata and model predictions is known, Mil\u00e0 et al. (2022) proposes an\nalternative nearest neighbor distance matching CV approach which may\nequal or improve upon buffered leave-one-out CV. An alternative approach\nput forward by Meyer et al. (2018) uses meaningful, human-defined\nlocations as groups for CV, which may produce better results than the\nautomated partitioning methods investigated in this study. While we\nbelieve our results clearly demonstrate the benefits of spatial CV for\nsampling designs resembling our simulation, we do not pretend to present\nthe one CV approach to rule them all.\n\nWe additionally do not expect these results, focused upon using CV to\nevaluate the accuracy of predictive models, will necessarily transfer to\nmap accuracy assessments. Stehman and Foody (2019) explained that\ndesign-based sampling approaches provide unbiased assessments of map\naccuracy, while Wadoux et al. (2021) demonstrated that spatial CV\nmethods may be overly pessimistic when assessing map accuracy, and Bruin\net al. (2022) suggested sampling-intensity weighted CV approaches for\nmap accuracy assessments in situations where the study area has been\nunevenly sampled. However, we expect these results will be informative\nin the many situations requiring estimates of model accuracy,\nparticularly given that traditional held-out test sets are somewhat rare\nin the spatial modeling literature.\n\nLastly, we did not investigate any CV approaches which aim to preserve\noutcome or predictor distributions across D_out.\nWhen working with imbalanced outcomes, random sampling may produce\nD_out with notably different outcome distributions\nthan the overall training data, which may bias performance estimates.\nAssigning stratified samples of observations to\nD_out can address this, but it is not obvious how\nto use stratified sampling when assigning groups of observations (such\nas a spatial cluster or block), with one outcome value per observation,\nto D_out as a unit. The  package\nallows stratified CV when all observations within a given group have\nidentical outcome values (that is, when groups are strictly nested\nwithin the stratification variable), but this condition is rare and\ndifficult to enforce when using unsupervised group assignment based on\nspatial location, as with all the spatial CV methods we investigated.\n\nCreating D_out based on predictor space, rather\nthan outcome distributions, has also been proposed as a solution to\nspatial CV procedures restricting the predictor ranges present in\nD_in. This is a particular challenge if the\npredictors themselves are spatially structured, and may unintentionally\nforce models to extrapolate further in predictor space than would be\nexpected when predicting new data (Roberts et al. 2017). As increasing\ndistance in predictor space often correlates with increasing error (e.g.\nThuiller et al. 2004; Sheridan et al. 2004; Meyer and Pebesma 2021),\nRoberts et al. (2017) suggest blocking approaches to minimize distance\nin predictor space between folds, although to the best of our knowledge\nthese approaches are not yet in widespread use. A related field of\nresearch suggests methods for calculating the applicability domain of a\nmodel (Netzeva et al. 2005; Meyer and Pebesma 2021), which can help to\nidentify when predicting new observations will require extrapolation in\npredictor space, and will likely produce predictions with higher than\nexpected errors. Such methods are particularly well-equipped to\nsupplement spatial CV procedures, as it adjusts the permissible distance\nin predictor space based upon the distance between\nD_in and D_out.\n\nsec-conclusion\n\n\n\u00a7 CONCLUSION\n\n\nThese results reinforce that spatial CV is essential for evaluating the\nperformance of predictive models fit to data with internal spatial\nstructure, particularly in situations where design-based map accuracy\nassessments are not practical or germane. Techniques that apply\nexclusion buffers around assessment sets of spatially conjunct\nobservations, such as spatial clustering and LODO, are likely to produce\nthe best estimates of model performance. The most accurate estimates of\nmodel performance are produced when the assessment and analysis data are\nsufficiently separated so that there is no spatial dependence in the\noutcome variable between the two sets.\n\nacknowledgements\n\n\n\u00a7 ACKNOWLEDGEMENTS\n\n\nWe would like to thank Posit, PBC, for support in the development of the\n and  packages.\n\nsoftware-data-and-code-availability\n\n\n\u00a7 SOFTWARE, DATA, AND CODE\nAVAILABILITY\n\n\nThe  package is available online at\nhttps://github.com/tidymodels/spatialsample . All data and code used in\nthis paper are available online at\nhttps://github.com/cafri-labs/assessing-spatial-cv .\n\n\n\nreferences\n\n\n\u00a7 REFERENCES\n\ntocsectionReferences\n\nrefs\n10\npreref-adams2020\nAdams, Matthew D., Felix Massey, Karl Chastko, and Calvin Cupini. 2020.\n\u201cSpatial Modelling of Particulate Matter Air Pollution Sensor\nMeasurements Collected by Community Scientists While Cycling, Land Use\nRegression with Spatial Cross-Validation, and Applications of Machine\nLearning for Data Correction.\u201d Atmospheric Environment 230\n(June): 117479. <https://doi.org/10.1016/j.atmosenv.2020.117479>.\n\npreref-bahn2012\nBahn, Volker, and Brian J. McGill. 2012. \u201cTesting the Predictive\nPerformance of Distribution Models.\u201d Oikos 122 (3): 321\u201331.\n<https://doi.org/10.1111/j.1600-0706.2012.00299.x>.\n\npreref-bastin2019\nBastin, Jean-Francois, Yelena Finegold, Claude Garcia, Danilo Mollicone,\nMarcelo Rezende, Devin Routh, Constantin M. Zohner, and Thomas W.\nCrowther. 2019. \u201cThe Global Tree Restoration Potential.\u201d\nScience 365 (6448): 76\u201379.\n<https://doi.org/10.1126/science.aax0848>.\n\npreref-Bates2021\nBates, Stephen, Trevor Hastie, and Robert Tibshirani. 2021.\n\u201cCross-Validation: What Does It Estimate and How Well Does It Do It?\narXiv:2104.00673v2 [Stat.ME].\u201d\n<https://doi.org/10.48550/arXiv.2104.00673>.\n\npreref-Breiman2001\nBreiman, Leo. 2001. \u201cRandom Forests.\u201d Machine Learning 45:\n5\u201332. <https://doi.org/10.1023/A:1010933404324>.\n\npreref-brenning2012\nBrenning, Alexander. 2012. \u201cSpatial Cross-Validation and Bootstrap for\nthe Assessment of Prediction Rules in Remote Sensing: The R Package\nsperrorest.\u201d 2012 IEEE International Geoscience and Remote\nSensing Symposium, July.\n<https://doi.org/10.1109/igarss.2012.6352393>.\n\npreref-debruin2022\nBruin, Sytze de, Dick J. Brus, Gerard B. M. Heuvelink, Tom van\nEbbenhorst Tengbergen, and Alexandre M.J-C. Wadoux. 2022. \u201cDealing\nwith Clustered Samples for Assessing Map Accuracy by\nCross-Validation.\u201d Ecological Informatics 69 (July): 101665.\n<https://doi.org/10.1016/j.ecoinf.2022.101665>.\n\npreref-brus2020\nBrus, Dick J. 2020. \u201cStatistical Approaches for Spatial Sample Survey:\nPersistent Misconceptions and New Developments.\u201d European\nJournal of Soil Science 72 (2): 686\u2013703.\n<https://doi.org/10.1111/ejss.12988>.\n\npreref-s2\nDunnington, Dewey, Edzer Pebesma, and Ege Rubak. 2021. s2:\nSpherical Geometry Operators Using the S2 Geometry Library.\n<https://CRAN.R-project.org/package=s2>.\n\npreref-efron1986\nEfron, Bradley. 1986. \u201cHow Biased Is the Apparent Error Rate of a\nPrediction Rule?\u201d Journal of the American Statistical\nAssociation 81 (394): 461\u201370.\n<https://doi.org/10.1080/01621459.1986.10478291>.\n\npreref-efron1983\nEfron, Bradley, and Gail Gong. 1983. \u201cA Leisurely Look at the\nBootstrap, the Jackknife, and Cross-Validation.\u201d The American\nStatistician 37 (1): 36\u201348.\n<https://doi.org/10.1080/00031305.1983.10483087>.\n\npreref-fick2017\nFick, Stephen E., and Robert J. Hijmans. 2017. \u201cWorldClim 2: New\n1-Km Spatial Resolution Climate Surfaces for Global Land Areas.\u201d\nInternational Journal of Climatology 37 (12): 4302\u201315.\n<https://doi.org/10.1002/joc.5086>.\n\npreref-rsample\nFrick, Hannah, Fanny Chow, Max Kuhn, Michael Mahoney, Julia Silge, and\nHadley Wickham. 2022. rsample: General Resampling\nInfrastructure. <https://CRAN.R-project.org/package=rsample>.\n\npreref-gong1986\nGong, Gail. 1986. \u201cCross-Validation, the Jackknife, and the Bootstrap:\nExcess Error Estimation in Forward Logistic Regression.\u201d Journal\nof the American Statistical Association 81 (393): 108\u201313.\n<https://doi.org/10.1080/01621459.1986.10478245>.\n\npreref-degruijter1990\nGruijter, J. J. de, and C. J. F. ter Braak. 1990. \u201cModel-Free\nEstimation from Spatial Samples: A Reappraisal of Classical Sampling\nTheory.\u201d Mathematical Geology 22 (4): 407\u201315.\n<https://doi.org/10.1007/bf00890327>.\n\npreref-hengl2017\nHengl, Tomislav, Jorge Mendes de Jesus, Gerard B. M. Heuvelink, Maria\nRuiperez Gonzalez, Milan Kilibarda, Aleksandar Blagoti\u0107, Wei Shangguan,\net al. 2017. \u201cSoilGrids250m: Global Gridded Soil Information Based on\nMachine Learning.\u201d Edited by Ben Bond-Lamberty. PLOS ONE 12\n(2): e0169748. <https://doi.org/10.1371/journal.pone.0169748>.\n\npreref-automap\nHiemstra, P. H., E. J. Pebesma, C. J. W. Twenh\u00f6fel, and G. B. M.\nHeuvelink. 2008. \u201cReal-Time Automatic Interpolation of Ambient Gamma\nDose Rates from the Dutch Radioactivity Monitoring Network.\u201d\nComputers & Geosciences.\n<https://doi.org/10.1016/j.cageo.2008.10.011>.\n\npreref-vandenhoogen2019\nHoogen, Johan van den, Stefan Geisen, Devin Routh, Howard Ferris, Walter\nTraunspurger, David A. Wardle, Ron G. M. de Goede, et al. 2019. \u201cSoil\nNematode Abundance and Functional Group Composition at a Global\nScale.\u201d Nature 572 (7768): 194\u201398.\n<https://doi.org/10.1038/s41586-019-1418-6>.\n\npreref-karasiak2021\nKarasiak, N., J.-F. Dejoux, C. Monteil, and D. Sheeren. 2021. \u201cSpatial\nDependence Between Training and Test Sets: Another Pitfall of\nClassification Accuracy Assessment in Remote Sensing.\u201d Machine\nLearning 111 (7): 2715\u201340.\n<https://doi.org/10.1007/s10994-021-05972-1>.\n\npreref-tune\nKuhn, Max. 2022. tune: Tidy Tuning Tools.\n<https://CRAN.R-project.org/package=tune>.\n\npreref-dials\nKuhn, Max, and Hannah Frick. 2022. dials: Tools for Creating\nTuning Parameter Values.\n<https://CRAN.R-project.org/package=dials>.\n\npreref-Kuhn2013\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive Modeling.\nVol. 26. Springer. <https://doi.org/10.1007/978-1-4614-6849-3>.\n\npreref-kuhn2019\n\u2014\u2014\u2014. 2019. Feature Engineering and Selection. Chapman;\nHall/CRC. <https://doi.org/10.1201/9781315108230>.\n\npreref-tmwr\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with R.\nO'Reilly.\n\npreref-lerest2014\nLe Rest, K\u00e9vin, David Pinaud, Pascal Monestiez, Jo\u00ebl Chadoeuf, and\nVincent Bretagnolle. 2014. \u201cSpatial Leave-One-Out Cross-Validation for\nVariable Selection in the Presence of Spatial Autocorrelation.\u201d\nGlobal Ecology and Biogeography 23 (7): 811\u201320.\n<https://doi.org/10.1111/geb.12161>.\n\npreref-legendre1989\nLegendre, Pierre, and Marie Jos\u00e9e Fortin. 1989. \u201cSpatial Pattern and\nEcological Analysis.\u201d Vegetatio 80 (2): 107\u201338.\n<https://doi.org/10.1007/bf00048036>.\n\npreref-spatialsample\nMahoney, Michael, and Julia Silge. 2022. spatialsample: Spatial\nResampling Infrastructure.\n<https://CRAN.R-project.org/package=spatialsample>.\n\npreref-martin2012\nMartin, Laura J, Bernd Blossey, and Erle Ellis. 2012. \u201cMapping Where\nEcologists Work: Biases in the Global Distribution of Terrestrial\nEcological Observations.\u201d Frontiers in Ecology and the\nEnvironment 10 (4): 195\u2013201. <https://doi.org/10.1890/110154>.\n\npreref-meyer2021\nMeyer, Hanna, and Edzer Pebesma. 2021. \u201cPredicting into Unknown Space?\nEstimating the Area of Applicability of Spatial Prediction Models.\u201d\nMethods in Ecology and Evolution 12 (9): 1620\u201333.\n<https://doi.org/10.1111/2041-210x.13650>.\n\npreref-meyer2022\n\u2014\u2014\u2014. 2022. \u201cMachine Learning-Based Global Maps of Ecological\nVariables and the Challenge of Assessing Them.\u201d Nature\nCommunications 13 (1).\n<https://doi.org/10.1038/s41467-022-29838-9>.\n\npreref-meyer2018\nMeyer, Hanna, Christoph Reudenbach, Tomislav Hengl, Marwan Katurji, and\nThomas Nauss. 2018. \u201cImproving Performance of Spatio-Temporal Machine\nLearning Models Using Forward Feature Selection and Target-Oriented\nValidation.\u201d Environmental Modelling & Software 101 (March):\n1\u20139. <https://doi.org/10.1016/j.envsoft.2017.12.001>.\n\npreref-meyer2019\nMeyer, Hanna, Christoph Reudenbach, Stephan W\u00f6llauer, and Thomas Nauss.\n2019. \u201cImportance of Spatial Predictor Variable Selection in Machine\nLearning Applications \u2013 Moving from Data Reproduction to\nSpatial Prediction.\u201d Ecological Modelling 411 (November):\n108815. <https://doi.org/10.1016/j.ecolmodel.2019.108815>.\n\npreref-miluxe02022\nMil\u00e0, Carles, Jorge Mateu, Edzer Pebesma, and Hanna Meyer. 2022.\n\u201cNearest Neighbour Distance Matching Leave-One-Out\nCross-Validation for Map Validation.\u201d Methods in Ecology and\nEvolution 13 (6): 1304\u201316.\n<https://doi.org/10.1111/2041-210x.13851>.\n\npreref-netzeva2005\nNetzeva, Tatiana I., Andrew P. Worth, Tom Aldenberg, Romualdo Benigni,\nMark T. D. Cronin, Paola Gramatica, Joanna S. Jaworska, et al. 2005.\n\u201cCurrent Status of Methods for Defining the Applicability Domain of\n(Quantitative) Structure-Activity Relationships.\u201d Alternatives\nto Laboratory Animals 33 (2): 155\u201373.\n<https://doi.org/10.1177/026119290503300209>.\n\npreref-osullivan2010\nO'Sullivan, David, and David J. Unwin. 2010. Geographic\nInformation Analysis. John Wiley & Sons, Inc.\n<https://doi.org/10.1002/9780470549094>.\n\npreref-sf\nPebesma, Edzer. 2018. \u201cSimple Features for R: Standardized Support\nfor Spatial Vector Data.\u201d The R Journal 10 (1): 439\u201346.\n<https://doi.org/10.32614/RJ-2018-009>.\n\npreref-units\nPebesma, Edzer, Thomas Mailund, and James Hiebert. 2016. \u201cMeasurement\nUnits in R.\u201d R Journal 8 (2): 486\u201394.\n<https://doi.org/10.32614/RJ-2016-061>.\n\npreref-ploton2020\nPloton, Pierre, Fr\u00e9d\u00e9ric Mortier, Maxime R\u00e9jou-M\u00e9chain, Nicolas Barbier,\nNicolas Picard, Vivien Rossi, Carsten Dormann, et al. 2020. \u201cSpatial\nValidation Reveals Poor Predictive Performance of Large-Scale Ecological\nMapping Models.\u201d Nature Communications 11 (1).\n<https://doi.org/10.1038/s41467-020-18321-y>.\n\npreref-pohjankukka2017\nPohjankukka, Jonne, Tapio Pahikkala, Paavo Nevalainen, and Jukka\nHeikkonen. 2017. \u201cEstimating the Prediction Performance of Spatial\nModels via Spatial k-Fold Cross Validation.\u201d International\nJournal of Geographical Information Science 31 (10): 2001\u201319.\n<https://doi.org/10.1080/13658816.2017.1346255>.\n\npreref-Probst2018\nProbst, Philipp, Bernd Bischl, and Anne-Laure Boulesteix. 2018.\n\u201cTunability: Importance of Hyperparameters of Machine Learning\nAlgorithms.\u201d arXiv. <https://doi.org/10.48550/ARXIV.1802.09596>.\n\npreref-R\nR Core Team. 2022. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\n<https://www.R-project.org/>.\n\npreref-Roberts2017\nRoberts, David R., Volker Bahn, Simone Ciuti, Mark S. Boyce, Jane Elith,\nGurutzeta Guillera-Arroita, Severin Hauenstein, et al. 2017.\n\u201cCross-Validation Strategies for Data with Temporal, Spatial,\nHierarchical, or Phylogenetic Structure.\u201d Ecography 40 (8):\n913\u201329. https://doi.org/<https://doi.org/10.1111/ecog.02881>.\n\npreref-RandomFields\nSchlather, Martin, Alexander Malinowski, Peter J. Menck, Marco Oesting,\nand Kirstin Strokorb. 2015. \u201cAnalysis, Simulation and Prediction of\nMultivariate Random Fields with Package RandomFields.\u201d Journal\nof Statistical Software 63 (8): 1\u201325.\n<https://doi.org/10.18637/jss.v063.i08>.\n\npreref-schratz2019\nSchratz, Patrick, Jannes Muenchow, Eugenia Iturritxa, Jakob Richter, and\nAlexander Brenning. 2019. \u201cHyperparameter Tuning and Performance\nAssessment of Statistical and Machine-Learning Algorithms Using Spatial\nData.\u201d Ecological Modelling 406 (August): 109\u201320.\n<https://doi.org/10.1016/j.ecolmodel.2019.06.002>.\n\npreref-sheridan2004\nSheridan, Robert P., Bradley P. Feuston, Vladimir N. Maiorov, and Simon\nK. Kearsley. 2004. \u201cSimilarity to Molecules in the Training Set Is a\nGood Discriminator for Prediction Accuracy in QSAR.\u201d Journal\nof Chemical Information and Computer Sciences 44 (6): 1912\u201328.\n<https://doi.org/10.1021/ci049782w>.\n\npreref-stehman2019\nStehman, Stephen V., and Giles M. Foody. 2019. \u201cKey Issues in Rigorous\nAccuracy Assessment of Land Cover Products.\u201d Remote Sensing of\nEnvironment 231 (September): 111199.\n<https://doi.org/10.1016/j.rse.2019.05.018>.\n\npreref-Stone1974\nStone, M. 1974. \u201cCross-Validatory Choice and Assessment of Statistical\nPredictions.\u201d Journal of the Royal Statistical Society. Series B\n(Methodological) 36 (2): 111\u201347.\n<http://www.jstor.org/stable/2984809>.\n\npreref-telford2009\nTelford, R. J., and H. J. B. Birks. 2009. \u201cEvaluation of Transfer\nFunctions in Spatially Structured Environments.\u201d Quaternary\nScience Reviews 28 (13-14): 1309\u201316.\n<https://doi.org/10.1016/j.quascirev.2008.12.020>.\n\npreref-thuiller2004\nThuiller, Wilfried, Lluis Brotons, Miguel B. Ara\u00fajo, and Sandra Lavorel.\n2004. \u201cEffects of Restricting Environmental Range of Data to Project\nCurrent and Future Species Distributions.\u201d Ecography 27 (2):\n165\u201372. <https://doi.org/10.1111/j.0906-7590.2004.03673.x>.\n\npreref-townsend2007\nTownsend, Peterson A., Monica Pape\u015f, and Muir Eaton. 2007.\n\u201cTransferability and Model Evaluation in Ecological Niche Modeling: A\nComparison of GARP and Maxent.\u201d Ecography 30 (4): 550\u201360.\n<https://doi.org/10.1111/j.0906-7590.2007.05102.x>.\n\npreref-valavi2018\nValavi, Roozbeh, Jane Elith, Jos\u00e9 J. Lahoz-Monfort, and Gurutzeta\nGuillera-Arroita. 2018. \u201cblockCV: An R Package for Generating\nSpatially or Environmentally Separated Folds for k-Fold Cross-Validation\nof Species Distribution Models.\u201d Edited by David Warton. Methods\nin Ecology and Evolution 10 (2): 225\u201332.\n<https://doi.org/10.1111/2041-210x.13107>.\n\npreref-varma2006\nVarma, Sudhir, and Richard Simon. 2006. \u201cBias in Error Estimation When\nUsing Cross-Validation for Model Selection.\u201d BMC Bioinformatics\n7 (1). <https://doi.org/10.1186/1471-2105-7-91>.\n\npreref-wadoux2021\nWadoux, Alexandre M. J.-C., Gerard B. M. Heuvelink, Sytze de Bruin, and\nDick J. Brus. 2021. \u201cSpatial Cross-Validation Is Not the Right Way to\nEvaluate Map Accuracy.\u201d Ecological Modelling 457 (October):\n109692. <https://doi.org/10.1016/j.ecolmodel.2021.109692>.\n\npreref-walvoort2010\nWalvoort, D. J. J., D. J. Brus, and J. J. de Gruijter. 2010. \u201cAn R\nPackage for Spatial Coverage Sampling and Random Sampling from Compact\nGeographical Strata by k-Means.\u201d Computers & Geosciences 36\n(10): 1261\u201367. <https://doi.org/10.1016/j.cageo.2010.04.005>.\n\npreref-wenger2012\nWenger, Seth J., and Julian D. Olden. 2012. \u201cAssessing Transferability\nof Ecological Models: An Underappreciated Aspect of Statistical\nValidation.\u201d Methods in Ecology and Evolution 3 (2): 260\u201367.\n<https://doi.org/10.1111/j.2041-210x.2011.00170.x>.\n\npreref-wickham2019\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nMcGowan, Romain Fran\u00e7ois, Garrett Grolemund, et al. 2019. \u201cWelcome to\nthe Tidyverse.\u201d Journal of Open Source Software 4 (43): 1686.\n<https://doi.org/10.21105/joss.01686>.\n\npreref-ranger\nWright, Marvin N., and Andreas Ziegler. 2017. \u201cranger: A Fast\nImplementation of Random Forests for High Dimensional Data in C++ and\nR.\u201d Journal of Statistical Software 77 (1): 1\u201317.\n<https://doi.org/10.18637/jss.v077.i01>.\n\npreref-yates2018\nYates, Katherine L., Phil J. Bouchet, M. Julian Caley, Kerrie Mengersen,\nChristophe F. Randin, Stephen Parnell, Alan H. Fielding, et al. 2018.\n\u201cOutstanding Challenges in the Transferability of Ecological\nModels.\u201d Trends in Ecology & Evolution 33 (10): 790\u2013802.\n<https://doi.org/10.1016/j.tree.2018.08.001>.\n\n\n\n\n\n\n"}