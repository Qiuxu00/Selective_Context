{"entry_id": "http://arxiv.org/abs/2303.06884v1", "published": "20230313060323", "title": "SCPNet: Semantic Scene Completion on Point Cloud", "authors": ["Zhaoyang Xia", "Youquan Liu", "Xin Li", "Xinge Zhu", "Yuexin Ma", "Yikang Li", "Yuenan Hou", "Yu Qiao"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\n\n\nSCPNet: Semantic Scene Completion on Point Cloud\n    Zhaoyang Xia^1, Youquan Liu^1, Xin Li^1, Xinge Zhu^2, Yuexin Ma^3,\n \nYikang Li^1, Yuenan Hou^1\u2020, and Yu Qiao^1\n\n^1Shanghai AI Laboratory ^2The Chinese University of Hong Kong ^3ShanghaiTech University\n\n^1{houyuenan, liyikang, qiaoyu}@pjlab.org.cn\n\n    March 30, 2023\n===========================================================================================================================================================================================================================================================\n\n\n\nSCPNet\n\n\n\n\n\nTraining deep models for semantic scene completion (SSC) is challenging due to the sparse and incomplete input, a large quantity of objects of diverse scales as well as the inherent label noise for moving objects. To address the above-mentioned problems, we propose the following three solutions: 1) Redesigning the completion sub-network. We design a novel completion sub-network, which consists of several Multi-Path Blocks (MPBs) to aggregate multi-scale features and is free from the lossy downsampling operations. 2) Distilling rich knowledge from the multi-frame model. We design a novel knowledge distillation objective, dubbed Dense-to-Sparse Knowledge Distillation (DSKD). It transfers the dense, relation-based semantic knowledge from the multi-frame teacher to the single-frame student, significantly improving the representation learning of the single-frame model. 3) Completion label rectification. We propose a simple yet effective label rectification strategy, which uses off-the-shelf panoptic segmentation labels to remove the traces of dynamic objects in completion labels, greatly improving the performance of deep models especially for those moving objects. Extensive experiments are conducted in two public SSC benchmarks, , SemanticKITTI and SemanticPOSS. Our \u00a0ranks 1st on SemanticKITTI semantic scene completion challenge and surpasses the competitive S3CNet\u00a0<cit.> by 7.2 mIoU. \u00a0also outperforms previous completion algorithms on the SemanticPOSS dataset. Besides, our method also achieves competitive results on SemanticKITTI semantic segmentation tasks, showing that knowledge learned in the scene completion is beneficial to the segmentation task. \n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\n\u2020: Corresponding author.\n\n\n\n\n\nSemantic scene completion (SSC)\u00a0<cit.> aims at inferring both geometry and semantics of the scene from an incomplete and sparse observation, which is a crucial component in 3D scene understanding. Performing semantic scene completion in the outdoor scenarios is challenging due to the sparse and incomplete input, a large quantity of objects of diverse scales as well as the inherent label noise for those moving objects (See Fig.\u00a0<ref> (a)).\n\n\nRecent years have witnessed an explosion of methods in the outdoor scene completion field\u00a0<cit.>. For example, S3CNet\u00a0<cit.> performs 2D and 3D completion tasks jointly and achieves impressive performance on the SemanticKITTI leaderboard\u00a0<cit.>. JS3C-Net\u00a0<cit.> performs semantic segmentation first and then feeds the segmentation features to the completion sub-network. The coarse-to-fine refinement module is further put forward to improve the completion quality. Although significant progress has been achieved in this area, these methods heavily rely on the voxelwise completion labels and show unsatisfactory completion performance on the small, distant objects and crowded scenes. Moreover, the long traces of the dynamic objects in original completion labels will hamper the learning of the completion models, which is overlooked in the previous literature\u00a0<cit.>.\n\nTo address the preceding problems, we propose three solutions from the aspects of the completion sub-network redesign, distillation of multi-frame knowledge as well as completion label rectification. Specifically, we first make a comprehensive overhaul of the completion sub-network. We adopt the completion-first principle and make the completion module directly process the raw voxel features. Besides, we avoid the use of downsampling operations since they inevitably introduce information loss and cause severe misclassification for those small objects and crowded scenes. To improve the completion quality on objects of diverse scales, we design Multi-Path Blocks (MPBs) with varied kernel sizes, which aggregate multi-scale features and fully utilize the rich contextual information.\n\nSecond, to combat against the sparse and incomplete input signals, we make the single-scan student model distil knowledge from the multi-frame teacher model. However, mimicking the probabilistic knowledge of each point/voxel brings marginal gains. Instead, we propose to distill the pairwise similarity information. Considering the sparsity and unorderness of features, we align the features using their indices and then force the consistency between the pairwise similarity maps of student features and those of teacher features, make the student benefit from the relational knowledge of the teacher. The resulting Dense-to-Sparse Knowledge Distillation objective is termed DSKD, which is specifically designed for the scene completion task.\n\nFinally, to address the long traces of those dynamic objects in the completion labels, we propose a simple yet effective label rectification strategy. The core idea is to use off-the-shelf panoptic segmentation labels to remove the traces of dynamic objects in completion labels. The rectified completion labels are more accurate and reliable, greatly improving the completion qualities of deep models on those moving objects.\n\n\n\n\nWe conduct experiments on two large-scale outdoor scene completion benchmarks, , SemanticKITTI\u00a0<cit.> and SemanticPOSS\u00a0<cit.>. Our \u00a0ranks 1st on SemanticKITTI semantic scene completion challenge\u00a0[https://codalab.lisn.upsaclay.fr/competitions/7170#results till 2022-11-12 00:00 Pacific Time, and our method is termed .] and outperforms the S3CNet\u00a0<cit.> by 7.2 mIoU. \u00a0also achieves better performance than other completion algorithms on the SemanticPOSS dataset. The learned knowledge from the completion task also benefits the segmentation task, making our \u00a0achieve superior performance on the SemanticKITTI semantic segmentation task.\n\nOur contributions are summarized as follows.\n\n\n\n  * The comprehensive redesign of the completion sub-network. We unveil several key factors to building strong completion networks.\n\n  * To cope with the sparsity and incompleteness of the input, we propose to distill the dense relation-based knowledge from the multi-frame model. Note that we are the first to apply knowledge distillation to the semantic scene completion task.\n\n  * To address the long traces of moving objects in completion labels, we present the completion label rectification strategy.\n\n  * Our \u00a0ranks 1st on SemanticKITTI semantic scene completion challenge, outperforming the previous SOTA S3CNet\u00a0<cit.> by 7.2 mIoU. Competitive performance is also shown in SemanticPOSS completion task and SemanticKITTI semantic segmentation task.\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n\n\n\n\n\n\n\nSemantic scene completion. Early works on scene completion mainly concentrates on the indoor scenarios\u00a0<cit.>. The point cloud in indoor scenarios is dense, small-scale and has uniform density. By contrast, point cloud in outdoor scenes is sparse, large-scale and has varying density, which poses great challenges to the semantic scene completion algorithms\u00a0<cit.>. Various algorithms have been proposed, for instance, LMSCNet\u00a0<cit.> uses a mixture of 2D and 3D convolutions to build the efficient completion backbone. S3CNet\u00a0<cit.> performs 2D and 3D scene completion simultaneously, and fuses the results by the proposed dynamic voxel fusion module. JS3C-Net\u00a0<cit.> attaches the completion network to the segmentation backbone and refines the completion outputs via the point-voxel interaction module. Compared with previous completion networks, our \u00a0is free from the lossy downsampling operations. Besides, our network is built on several MPBs that aggregate multi-scale features and can achieve high completion quality in objects of various sizes. \n\n\n\n\n\nKnowledge distillation. Knowledge distillation (KD) originates from the pioneering work of G. Hinton\u00a0\u00a0<cit.>. Its primary objective is to transfer the dark knowledge from the large over-parameterized teacher model to the small compact student model. A large number of methods have been proposed and various forms of knowledge\u00a0<cit.> have been designed, , intermediate features\u00a0<cit.>, visual attention maps\u00a0<cit.>, region-level affinity scores\u00a0<cit.>, similarity scores of different samples\u00a0<cit.>, . It is noteworthy that the majority of the distillation methods concentrate on the 2D tasks. Only a few distillation methods have focused on 3D domains, , PVKD\u00a0<cit.> and SparseKD\u00a0<cit.>. To our knowledge, this is the first work that applies knowledge distillation to the semantic scene completion task. We propose to transfer the dense, relation-based semantic knowledge from the multi-frame model to the single-frame one. \n\n\n\n\n\u00a7 METHODOLOGY\n\n\n\n\n\nThe objective of the semantic scene completion task is to infer the complete geometric and semantic layout given the incomplete and sparse input. Formally, given the input point cloud \ud835\udc17\u2208\u211d^N \u00d7 3, the network needs to assign a label to each voxel of the L \u00d7 W \u00d7 H voxel space to indicate whether it is empty or belongs to a specific class c \u2208{0, 1, 2,..., C-1 }, where C is the number of classes, L, W and H are the length, width and height of the voxel space, respectively. We denote the voxelwise output as \ud835\udc0e\u2208\u211d^L \u00d7 W \u00d7 H \u00d7 C.\n\n\n\n \u00a7.\u00a7 Framework overview\n\n\nAs shown in Fig.\u00a0<ref>, our \u00a0is comprised of two sub-networks, , the completion sub-network and the segmentation sub-network. The completion sub-network is designed based on several key design principles. The segmentation sub-network is built upon Cylinder3D\u00a0<cit.>, with some minor modifications. In the following sections, we first detail the completion sub-network and introduce several design principles that are vital to building a strong completion sub-network. The knowledge distillation of multi-frame model and the completion label rectification will be explained thereafter.\n\n\n\n \u00a7.\u00a7 Redesigning the Completion Sub-network\n\n\n\n\n\nTake JS3C-Net\u00a0<cit.> as example. The original JS3C-Net first performs semantic segmentation and then conducts completion upon the segmentation features. Although this pipeline can benefit from the segmentation outputs, there are several drawbacks in this framework. First, the parameters in the completion sub-network are much fewer than the segmentation sub-network, thus yielding unsatisfactory completion performance. Second, there are downsampling and upsampling blocks in the completion sub-network. The downsampling operations will inevitably lose the information of the original point cloud and the upsampling operation will cause over dilation and shape distortion. \n\nTo address the aforementioned problems, we make a comprehensive overhaul of the completion sub-network. More concretely, there are three principles in the design of the completion sub-network, , maintaining sparsity, no downsampling and aggregating multi-scale features.\n\nMaintain sparsity. The completion sub-network needs the vanilla dense convolution for dilation while the segmentation sub-network uses sparse convolution for efficient processing. However, the bias of the convolution weight, running mean and beta of BN layers will break the sparsity of the original voxel features, thus substantially increasing the computation burden of the segmentation sub-network. Therefore, to reduce the overall computation cost and enjoy the high efficiency of sparse convolution, we remove all the convolution bias and the BN layers in the completion sub-network. In this condition, the voxel features produced by the completion sub-network can still keep the sparse property and the segmentation sub-network can use sparse convolution to process these sparse voxel features. \n\nNo downsampling. In popular completion networks such as S3CNet\u00a0<cit.> and JS3C-Net\u00a0<cit.>, there are several downsampling and upsampling blocks in the completion part. The downsampling operations will inevitably lose the information of the original point cloud, causing severe completion and classification errors for small objects and crowded scenes. Therefore, we discard all downsampling and upsampling operations to relieve the information loss, maximally retaining the information of the raw point cloud. Besides, as opposed to JS3C-Net which takes the segmentation-first baseline, we adopt the completion-first principle. Concretely, we make the completion sub-network directly process the raw voxel features produced by the voxelization process. And the completion sub-network can also \n\n\n\nbenefit from the large number of parameters of the segmentation sub-network.\n\nAggregate multi-scale features. To aggregate multi-scale features, we design the multi-path block which is comprised of 3\u00d73\u00d73, 5\u00d75\u00d75 and 7\u00d77\u00d77 convolution blocks. As shown in Fig.\u00a0<ref>, there are three branches in the completion sub-network. The upper branch contains one MPB and the bottom branch is a residual connection. The middle branch is constructed by a 3\u00d73\u00d73 convolution block, two MPBs and a 3\u00d73\u00d73 convolution block. After the completion sub-network, we obtain the dense completed voxel features. We extract the non-empty voxel features as well as their voxel indices from the completed voxel features. The generated sparse voxel features are sent to the segmentation sub-network to produce the voxelwise segmentation output.\n\nModifications on the segmentation sub-network. Recall that, for the segmentation part, we take the Cylinder3D\u00a0<cit.> as the backbone. Since the voxelwise completion labels are defined based on the cubic partition, we replace the cylindrical partition of Cylinder3D with conventional cubic partition. Besides, the original point refinement module consumes much GPU memory and brings limited gains, we discard this module to save memory usage.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Distilling Multi-frame Knowledge\n\n\n\n\nSince single frame point cloud is sparse and incomplete in the outdoor scenarios, directly performing semantic scene completion from the single-frame input is extremely difficult. It is natural to wonder if we can construct a multi-frame model and then distil the dense semantic knowledge from this multi-frame network. From Fig.\u00a0<ref>, it is evident that the multi-frame input significantly reduces the completion difficulty since multiple frames have much more points in the scene and objects are easier to be identified. The completion difficulty will gradually decrease as the number of input point cloud frames increases. Therefore, we construct the multi-frame teacher which takes denser point cloud as input and achieves better completion performance. \n\nInspired by\u00a0<cit.>, we make the single-frame model distil the relation-based structural knowledge from the multi-frame teacher network. Since the original voxel features are in the sparse form, we leverage the sparse features and their indices to perform knowledge distillation. We denote the voxel features and corresponding indices of the teacher and student as F_T\u2208\u211d^N_m\u00d7 C_f, F_S\u2208\u211d^N_s\u00d7 C_f, \u2110_T\u2208\u211d^N_m\u00d7 3 and \u2110_S\u2208\u211d^N_s\u00d7 3, respectively. N_m is the number of non-empty voxel features in the multi-frame, N_s is the number of non-empty voxel features in the single-frame, C_f is the number of channels of the voxel features. Note that the indices of teacher features and student features are sorted and \u2110_S(i, j) = \u2110_T(i, j), where i \u2208{1, ..., N_s} and j \u2208{1, 2, 3}. We first compute the pairwise relational knowledge of the student model:\n\n\n    \ud835\udc0f_S(i, j) = F_S(i) F_S(j)/ F_S(i) _2 F_S(j) _2, i, j \u2208{1, ..., N_s}\n\n\nThe relational knowledge of the teacher model \ud835\udc0f_T is calculated in a similar way. The relational knowledge captures the similarity of each pair of voxel features and serve as important clues of the surrounding environment, which can be taken as high-level knowledge to be learned by the single-frame student model. The proposed Dense-to-Sparse Knowledge Distillation (DSKD) loss is given as below:\n\n\n    \u2112_dskd(\ud835\udc0f_S, \ud835\udc0f_T) = 1/N_s^2\u2211_i=1^N_s\u2211_j=1^N_s\ud835\udc0f_S(i, j) - \ud835\udc0f_T(i, j) _2^2.\n\n\n\n\n\n \u00a7.\u00a7 Completion Label Rectification\n\n\n\n\n\n\n\n\n\n\n\n\nGeneration of completion labels. As pointed out by\u00a0<cit.>, for outdoor semantic scene completion, the ground-truth completion labels are obtained by concatenating the segmentation labels of multiple consecutive point cloud frames. Specifically, for the t-th frame, the corresponding completion labels L^c_t are constructed in the following way:\n\n\n    L^c_t = concat[L^s_t;T_t+1 \u2192 tL^s_t+1;...;T_t+T-1 \u2192 tL^s_t+T-1],\n\nwhere L^s_t are the segmentation labels for the t-th frame, T is the number of frames used for concatenation, T_t+1 \u2192 t is the transformation matrix that transforms the coordinate from (t+1)-th frame to the t-th frame, and concat[...;...] is the concatenating operation. The concatenation of multiple frames will lead to long traces for those moving objects, such as car and person. A vivid example is shown in Fig.\u00a0<ref>. The long traces of those dynamic objects are obviously irrational and will hamper the learning of deep models.\n\n\n\nCompletion label rectification. To remove the long traces of moving objects in the completion labels, we resort to the panoptic segmentation labels. Specifically, given the panoptic labels of class i, we first voxelize the labels and obtain the voxelwise panoptic labels for class i. For each instance of class i, we calculate the bound of each instance, forming a cube. We union the cubes of all instances and use them to process the original voxelwise completion labels, filtering those voxels that are outside the cubes. The process is repeated for all classes that contain dynamic objects. The detailed information of the label rectification algorithm is shown in Algorithm\u00a0<ref>. As shown in Fig.\u00a0<ref>, the proposed label rectification operation can effectively remove the long traces of moving objects, making the completion labels more accurate.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Overall objective\n\n\nThe overall loss function is comprised of three terms, , the cross entropy loss, the lovasz-softmax loss\u00a0<cit.> and the proposed distillation loss.\n\n\n    \u2112 =    \u2112_ce + \u03b1\u2112_lovasz + \u03b2\u2112_dskd,\n\n\nwhere \u03b1 and \u03b2 are the loss coefficients to balance the effect of each loss term.\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n\nDatasets. Following the practice of popular scene completion models\u00a0<cit.>, we conduct experiments on two popular LiDAR semantic scene completion benchmarks, , SemanticKITTI\u00a0<cit.> and SemanticPOSS\u00a0<cit.>. As to SemanticKITTI, it has 22 point cloud sequences. Sequences 00 to 10, 08 and 11 to 21 are used for training, validation and testing, respectively. 19 classes are chosen for training and evaluation after merging classes with distinct moving status and discarding classes with very few points. As for SemanticPOSS, it has 2, 988 frames and 11 classes are selected for evaluation. Although SemanticPOSS is smaller than SemanticKITTI in terms of dataset size, it is much more challenging since it contains a larger quantity of moving objects than SemanticKITTI, such as person and rider.\n\nEvaluation metrics. Following\u00a0<cit.>, we adopt the intersection-over-union (IoU) of each class and mIoU of all classes as the evaluation metric. The IoU of class i is calculated via: IoU_i = TP_i/TP_i+FP_i+FN_i, where TP_i, FP_i and FN_i denote the true positive, false positive and false negative of class i, respectively. For semantic scene completion, the dimension of the completion label is 256 \u00d7 256 \u00d7 32. We also report the completion mIoU which is the class-agnostic version of mIoU. Note that mIoU is the major evaluation metric for the semantic scene completion task.\n\nImplementation details. The output size of Cylinder3D is set as 256 \u00d7 256 \u00d7 32 to adapt to the completion task. The number of training epochs is set as 30 and the initial learning rate is set as 0.0015. We use Adam\u00a0<cit.> as the optimizer. Gradient norm clip is set 10 to stabilize the training process. \u03b1 and \u03b2 are set as 1 and 3, 000, respectively. We filter points that are outside the point cloud range of the voxelwise completion labels. For SemanticKITTI, we first train the model on the training set and then finetune it on both training and validation sets before submitting the predictions of test set to the online server. \n\n\n\nBaseline KD algorithms. We take the vanilla knowledge distillation\u00a0<cit.>, FitNets\u00a0<cit.>, NST\u00a0<cit.>, PKT\u00a0<cit.> and PVKD\u00a0<cit.> as baseline distillation algorithms. Vanilla knowledge distillation takes the softened logits as the distilled knowledge. FitNets directly mimics the teacher features. NST adopts the maximum mean discrepancy to minimize the distance between student features and teacher features. PKT models the teacher knowledge as a probability distribution and then forces the consistency of the probability distribution between the teacher and the student. PVKD distils the voxelwise output and inter-voxel affinity knowledge. And we discard the original pointwise output distillation and inter-point affinity distillation of PVKD since they consume much GPU memory and bring marginal gains.\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Results\n\n\nQuantitative results. We summarize the performance of \u00a0and state-of-the-art semantic scene completion methods in Table\u00a0<ref> and\u00a0<ref>. On SemanticKITTI, our \u00a0significantly outperforms other scene completion algorithms in terms of mIoU. For example, our \u00a0is 7.2 mIoU higher than S3CNet\u00a0<cit.>. On classes such as car, other-vehicle, road, parking, sidewalk, fense, terrain and other-ground, the performance gap between \u00a0and S3CNet is more than 10 IoU. Our \u00a0also achieves superior performance on SemanticPOSS val set. On classes such as car, trunk, pole, fence and bike, \u00a0is at least 5 IoU higher than JS3C-Net\u00a0<cit.>. The impressive performance on two large-scale benchmarks strongly demonstrate the superiority of our \u00a0.\n\nBesides, we use the trained weight of the segmentation sub-network as initialization to train Cylinder3D on the SemanticKITTI semantic segmentation task. From Table\u00a0<ref>, Cylinder3D initialized from trained weight of the completion task outperforms the original Cylinder3D model by 2.6 mIoU, and achieves impressive segmentation performance among various competitive LiDAR segmentation models such as 2DPASS\u00a0<cit.>, PVKD\u00a0<cit.> and RPVNet\u00a0<cit.>. The encouraging results show that knowledge learned in the completion task is also beneficial to the segmentation task.\n\n\n\n\n\n\n\nComparison with baseline KD algorithms. From Table\u00a0<ref>, it is evident that the proposed DSKD method can bring more gains than conventional knowledge distillation algorithms. For instance, compared with FitNets\u00a0<cit.> which directly mimics the teacher features, our DSKD can bring more than 2 mIoU, showing the effectiveness of the proposed relation-based distillation algorithm. The vanilla KD objective and FitNets hamper the performance of the base model, indicating that directly mimicking the logits or features can not boost the completion performance.\n\nQualitative results. We also provide visual comparison between JS3C-Net\u00a0<cit.>, \u00a0(single-frame) and \u00a0(multi-frame). As can be seen from Fig.\u00a0<ref>, our \u00a0(single-frame) make more accurate completion predictions than JS3C-Net on road and vegetation. On long, thin objects such as poles, our single-frame model also yields high-quality completion results compared with JS3C-Net. The predictions of our single-frame model also resemble those of the multi-frame network, demonstrating the efficacy of the proposed DSKD algorithm.\n\n\n\n\n \u00a7.\u00a7 Ablation studies\n\n\n\nExperiments are conducted in SemanticKITTI val set.\n\n\n\nEffect of completion label rectification. We report the performance of our \u00a0on completion labels with and without rectification. From Table\u00a0<ref>, it is evident that the proposed rectification strategy greatly enhances the performance of \u00a0on those dynamic objects, , car and person. For example, the completion label rectification can bring 8.1, 24.5,  26.1 and 17.6 IoU improvement on car, person, bicyclist and motorcyclist, respectively. The impressive performance gains strongly demonstrate the effectiveness of the label rectification algorithm.\n\n\n\n\nEffect of the completion sub-network. To examine the effect of our completion sub-network, we add it to JS3C-Net. The detailed performance is shown in Table\u00a0<ref> (a). Our completion sub-network can bring 6.8 mIoU improvement to JS3C-Net, which strongly demonstrates the effectiveness and generalization of the proposed completion sub-network.\n\nEffect of DSKD. We compare the performance of our \u00a0with and without the proposed DSKD in Table\u00a0<ref> (b). The proposed distillation method can bring 2.8 mIoU improvement to our \u00a0, showing the benefit of distilling relation-based knowledge from the multi-frame model.\n\nEffect of the downsampling operation. We add the downsampling operation to our completion sub-network and examine its effect. As reported in Table\u00a0<ref> (c), the downsampling operation hampers the completion performance of our \u00a0. Specifically, the completion performance of \u00a0decreases from 37.2 mIoU to 33.1 mIoU. The negative results show that the no-downsampling principle is vital to the success of the completion sub-network redesign.\n\n\n\n\n\u00a7 CONCLUSION\n\n\nTo address the challenges of the semantic scene completion task, we propose three solutions from the aspects of the completion network redesign, dense-to-sparse knowledge distillation as well as completion label rectification. The resulting completion network, termed \u00a0, achieves superior completion performance in two large-scale semantic scene completion benchmarks, , SemanticKITTI and SemanticPOSS. The learned knowledge on the completion task is also beneficial to the semantic segmentation task.\n\nAcknowledgements. This work is partially supported by the National Key R&D Program of China (No.\n2022ZD0160100),and in part by  Shanghai Committee of Science and Technology (Grant No. 21DZ1100100).\n\n\n\nieee_fullname\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 ABLATION STUDIES\n\n\nLoss coefficient \u03b2. We investigate the effect of the loss coefficient of the DSKD loss on the final performance. As shown in Table\u00a0<ref>, when we change the loss coefficient from 1, 000 to 4, 000, the completion performance of SCPNet first improves and then declines. Therefore, we set the loss coefficient of the DSKD loss as 3, 000 to obtain the best performance.\n\nDetailed performance comparison on DSKD loss. The detailed performance comparison of SCPNet with and without DSKD is shown in Table \u00a0<ref>. On motorcycle, truck, person and bicyclist, the proposed DSKD loss can bring more than 3 IoU improvement.\n\nDetailed performance comparison on the downsampling operation. We examine the effect of adding the downsampling operation to the completion sub-network of SCPNet. The detailed performance comparison of SCPNet with and without the downsampling operation is shown in Table \u00a0<ref>. It is apparent that the completion performance drops significantly, especially for truck, other-vehicle, other-ground and traffic-sign. The severe performance degradation strongly shows the necessity of removing the lossy downsampling operation for the completion sub-network.\n\n\n\n\u00a7 ELABORATED IMPLEMENTATION DETAILS\n\n\nRange mismatch. On SemanticKITTI, the point cloud range used by our segmentation sub-network, , Cylinder3D, is [-36.2, 36.2] m, [-36.2, 36.2] m and [-4, 2] m for x, y, z, respectively. For semantic scene completion, the range of the completion labels is [0, 51.2] m, [-25.6, 25.6] m and [-2, 4.4] m for x, y, z, respectively. The range mismatch problem will cause the existence of many empty voxels, which will significantly hamper the completion performance. To address this problem, we directly use the point cloud range of the completion labels. \n\nWhy conv bias and BN layers breaks the sparsity of voxel features. The voxel features, which are treated as the input of the completion sub-network, are sparse, , only a part of the whole voxel space is occupied. The completion sub-network uses the vanilla dense convolution for dilation. However, the bias of 3D convolution weight, the mean and variance of the Batch Normalization (BN) layers will result in non-zero values of all empty voxel positions. This will cause all empty voxel features to become occupied, which breaks the sparsity of the original voxel features and significantly increases the computation burden of the segmentation sub-network.\n\nHow does changing the random seed influence the mIoU values. We conduct experiments on SemanticKITTI using three different random seeds, , 100, 240 and 666. Experiments on SemanticKITTI show that the performance variance of \u00a0is within 0.3 mIoU.\n\nApply the proposed distillation loss to other architectures. We apply the DSKD loss to JS3CNet. It improves the performance of JS3CNet from 24.0 mIoU to 26.2 mIoU on SemanticKITTI val set.\n\nApply label rectification to other models. We apply label rectification to JS3CNet. Experimental results show that on SemanticKITTI val set, the proposed label rectification can bring considerable gains to JS3CNet, improving the performance from 24.0 mIoU to 26.8 mIoU.\n\nComputational impact of the proposed adjustments. We calculate the computational overhead of the completion sub-network and finds that it only introduces around 24.4 ms overhead.\n\nError bands for the results. We run the experiments on SemanticKITTI and SemanticPOSS datasets for three times. The performance variance of SCPNet on these benchmarks is within 0.3 mIoU.\n\nWhy panoptic labels are useful in label rectification. The panoptic labels provide instance-level annotations for those thing classes (, cars and persons) and these instance-level annotations are helpful to remove the long traces of moving objects in completion labels which only provide semantic segmentation annotations and do not differentiate each single instance.\n\n\n\nTraining and inference time and a comparison with SOTA. We summarized the training and inference time between JS3CNet and our \u00a0in Table\u00a0<ref>. Our SCPNet has comparable training and inference time but exhibits much better completion performance than JS3CNet.\n\n\n\n\u00a7 QUALITATIVE RESULTS\n\n\nWe provide visual comparison of \u00a0with and without DSKD in Fig.\u00a0<ref>. Compared with SCPNet without DSKD, the single-frame SCPNet with DSKD achieves better completion and segmentation performance by distilling dense and relation-based information from the multi-frame teacher model. SCPNet without DSKD performs badly on the parking areas and small objects while SCPNet with DSKD exhibits much better completion performance owing to the proposed distillation objective.\n\nAnd we also provide visual comparison between original \u00a0and \u00a0with downsampling and upsampling operations. As can be seen from Fig.\u00a0<ref>, the downsampling and upsampling operations will cause over dilation and shape distortion for these objects marked by the red ellipses. \n\n\n\n\n\n\n\n\n"}