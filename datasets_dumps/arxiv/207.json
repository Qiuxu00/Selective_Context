{"entry_id": "http://arxiv.org/abs/2303.07069v1", "published": "20230313124501", "title": "Generating multiple-choice questions for medical question answering with distractors and cue-masking", "authors": ["Damien Sileo", "Kanimozhi Uma", "Marie-Francine Moens"], "primary_category": "cs.CL", "categories": ["cs.CL", "H.4; H.5; I.2"], "text": "\n[\n    Sven Apel\n    March 30, 2023\n==================\n\n\nMedical multiple-choice question answering (MCQA) is particularly difficult. Questions may describe patient symptoms and ask for the correct diagnosis, which  requires domain knowledge and complex reasoning. Standard language modeling pretraining alone is not sufficient to achieve the best results.  <cit.> showed that focusing masked language modeling on disease name prediction when using medical encyclopedic paragraphs as input leads to considerable MCQA accuracy improvement. In this work, we show that (1) fine-tuning on generated MCQA dataset outperforms the masked language modeling based objective and (2) correctly masking the cues to the answers is critical for good performance. We release new pretraining datasets and achieve state-of-the-art results on 4 MCQA datasets, notably  +5.7% with base-size model on MedQA-USMLE.\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nThe multiple-choice question answering <cit.> task can be formulated with {Q, {O_1 ...O_N}} examples  where Q represents a question and O the candidate options. The goal is to select the correct answer from the options. Medical multiple-choice question answering (MCQA) has valuable applications for patient or physician assistance, but is limited by the accuracy of current systems. \n\nMedical knowledge is key to this task which can ask questions about patient diagnosis or the appropriate treatment. Medical knowledge graphs, such as UMLS <cit.> and SnomedCT <cit.> mainly encode terminological knowledge <cit.>. They are sparse when it comes to the practical medical knowledge which is instead available as text in encyclopedias. Various training methodologies allow text encoders to absorb external knowledge.  Text encoders acquire some factual knowledge via masked language modeling (MLM) <cit.>, but <cit.> showed that MLM objective solely focused on disease names significantly enhances downstream task accuracy.\n\nWe compare targeted MLM with auxiliary pretraining on a generated MCQA dataset constructed with medical concepts as answers, associated paragraphs as questions Q, and generated distractors as other options. In particular, we show that we can leverage differential diagnoses to obtain distractors. Strictly speaking,  differential diagnosis is the process of differentiating several conditions by examining the associated clinical features with additional tests. The term differential diagnosis is also used to denote commonly associated conditions that often need to be distinguished \u2013 bronchitis is a differential diagnosis of common cold.\nWe assemble a dataset of differential diagnoses and show that they provide helpful distractors. We also show that we can find differential diagnoses with a \nmodel trained to retrieve incorrect options based on the correct option from an existing MCQA dataset.\n\nWe then analyze the importance of properly masking the cues[A cue is the presence of a set of tokens that can help the prediction of the correct answer.] to the correct option O^*, i.e., parts of the answer that are present in Q. The DiseaseBERT pretraining masks all the tokens from the disease name to incentivize the model to look at the symptoms. We show that token-level masking is sub-optimal, as the masking of some tokens can also give away the answer. We propose a new masking scheme tailored to MCQA pretraining, called probability-matching cue masking, to prevent both present and masked token from giving away the answer. We also collect new sources of encyclopedic text for medical pretraining. Our contributions are: (i) We compare MLM, targeted MLM and auxiliary fine-tuning on generated MCQA data;  (ii) We identify issues with previous cue-masking techniques and propose a new masking strategy; (iii) We propose and distribute[https://huggingface.co/datasets/sileod/wikimedqa] new pretraining datasets; and (iv) We perform controlled comparison experiments for our contributions and achieve state-of-the-art on 4 datasets. \n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n  \nMedical text encoders pretraining \nNumerous models\n<cit.> adapt BERT pretraining to the biomedical domain to derive domain-specific text encoders.\nOur work is close to DiseaseBERT <cit.> which builds upon these encoders as an additional pretraining stage <cit.> to improve their knowledge. \nOther work focus on external knowledge extraction <cit.> and integration <cit.>, but knowledge augmented models still rely on pretrained text encoders.\n\n\n\n  \nMedical question answering Multiple datasets were proposed for medical MCQA for English <cit.>, Spanish (with English translations) <cit.> and \nChinese <cit.>. PubMedQA <cit.> and emrQA <cit.> are other large-scale biomedical QA datasets, but they address extractive question answering, i.e., cases where the answer to a question is explicitly in the text. \nOur work is the first to generate MCQA data for medical domain pretraining.\n\n\n\n\n  \nDistractor prediction Our work is related to the problem of generating distractors for multiple-choice questions.\nThese models use the existing answers to derive other answers that are plausible yet wrong. We distinguish two strands of approaches. Retrieval-based models use the correct answer as a query to retrieve related yet wrong alternatives among the answers to other questions <cit.>.\nGeneration-based models <cit.> learn to generate distractors with language models and focus on the  diversity and adequacy of the generated distractors. Here, we tailor distractor prediction to medical MCQA and also draw a new parallel between distractor generation and differential diagnosis.\n\n\n\n\n\n\u00a7 IMPROVING KNOWLEDGE INFUSION \n\n\nDiseaseBERT infuses knowledge by predicting title tokens based on paragraphs where title tokens are masked. We replace that stage with a fine-tuning stage with synthetic MCQA data and provide new analyses on token masking. Figure <ref> illustrates the MCQA generation process and the problem of extraneous masking.\n\n\n\n \u00a7.\u00a7 MCQA data generation with differential diagnoses and distractor retrieval \n\nWe generate data by using paragraphs content as questions Q, the title as a correct option O^*, and to generate interesting data, we look for related but distinct options, and we mask the direct cues to the correct options in Q.\nWe noticed that some Wikipedia pages were associated with differential diagnoses cross-linked on DBPedia. We collect associations between pages and differential diagnoses and use them as negative examples. In section <ref>, we will show that they constitute high-quality negative examples. Since these differential diagnoses are not available for all pages, - especially pages that are not related to diseases, but procedures -, we  derive negative examples by using a retrieval model. We will train a retrieval model on previous,    smaller MCQA datasets, and evaluate the retriever's ability to find differential diagnoses. We then use the retrieval models to find the most related titles on the same encyclopedia. \n\nWe also experimented with distractor generation by using generative models, but obtained unconvincing results. One advantage of using retrieval, is that because of editorial choices, each page covers different information. This prevents retrieved negative examples from being too similar to the correct answer, and this feature is not exploited by distractor LM-based generations models.\n\n\n\n\n \u00a7.\u00a7 Cue masking\n\n\n\n\n  \nNaive masking <cit.> masks tokens that are in the answer. However, masking some particular tokens is not necessarily concealing the information about the targeted disease. Some specific tokens are masked everywhere (e.g., a dash -). If these tokens can be predicted based on surrounding terms, the correct disease can be guessed without actually using useful medical knowledge. We call the masking that helps easy guesses extraneous masking, an example of which is illustrated in Figure <ref>, where the masked dash can give away the answer if the model has learned that a dash is plausible between RT and PCR. To address this problem, we will evaluate word-level masking, which necessarily leads to less extraneous masking.  \n\n\n  \nProbability-matching masking (ours) Another problem with naive masking is that if tokens from the correct answer are necessarily masked, a model can detect an incorrect option when it contains a word w that is in the question as w would be masked if the option was correct. We propose a new strategy that takes advantage of the negative examples to alleviate this phenomena. Instead of always masking a word when it is in the correct answer, we mask a word w with the following probability: \n\n\n\n    p_w=  1/|{O_i, w\u2208 O_i, i\u22081..N}|\n\n\nwhere O denotes the other options. This masking scheme ensures that no cue-based classifier can predict the correct answer based on neither presence nor absence of specific tokens. It also prevents common tokens from being unnecessarily masked.\n\n\n\n\n\n\u00a7 DATASETS\n\n\n\n\n \u00a7.\u00a7 Pretraining data \n\n\nWe collect new pretraining data from three open-source websites:\n\n\n  \nWikipedia Medicine Portal We crawl pages from the Wikipedia Medicine projects which indexes medical pages.https://en.wikipedia.org/wiki/Category:All_WikiProject_Medicine_articlesen.wikipedia.org/...medicine_articles We remove pages that match persons or organizations according to WikiData, and pages referring to years. We obtain a total of 75k paragraphs.\n\n\n\n  \nWikidoc We crawl overview pages from the WikiDoc specialized encyclopediahttps://www.wikidoc.org/index.php?title=Special:AllPageswww.wikidoc.org/...:AllPages which leads to 28k paragraphs.\n\n\n\n  \nWikEM We crawl content pages from the the WikEM encyclopediahttps://wikem.org/w/index.php?title=Special:AllPages   hideredirects=1wikem.org/...?title=Special:AllPages\nwhich is an open source medical encyclopedia targeted for emergency medicine, and we obtain 15k paragraphs.\n\n\n\n\n\n \u00a7.\u00a7 Downstream tasks \n\n\n\n\n\n\n\nWe use 4 medical MCQA datasets to perform evaluation. These datasets contain a question and four options, one of them being correct.\n\n\n\n  \nMedQA-USMLE <cit.> gathers\n10k/1.2k/1.2k train/validation/test medical MCQA examples collected from training questions for medical entrance exams found on the Web.\n\n\n  \nMedMCQA <cit.> contains 182k/6.2k/4.2k  train/validation/test medical entrance exam training questions.\n\n\n  \nHEAD-QA <cit.> we focus on the medical questions translated to English with 0.2k 0.4k validation/test examples, and use the MedMCQA train set as our train set.\n\n\n\n  \nMMLU <cit.> is the professional medicine subset of the MMLU language understanding benchmark, which contains 272 test examples. Following <cit.>, we used MedQA-USMLE as a training set.\n\n\n\n\n\u00a7 EXPERIMENTS \n\n\n\nWe generate MCQA examples with the aggregated paragraphs of the pretraining data from Section <ref>, with the distractor generation of section and masking strategies of section <ref>. We first compare cue masking schemes and pretraining objectives, with ablations on the MedQA-USMLE dataset, then show overall results with the other datasets. We fine-tune BioLinkBERT[\nWikiMedQA fine-tuning also improves the accuracy of PubMedBERT <cit.> and BioElectra <cit.>\n but both still underperform BioLinkBERT on downstream tasks]\n on WikiMedQA then evaluate BioLinkBERT+WikiMedQA on each downstream task with fine-tuning. We always use standard hyperparameters (5 epochs, sequence length of 256, learning rate of 2.10^-5 batch size of 16). We use a multiple-choice-question answering setup (we predict logit scores for each option by concatenating the question and the option, then use a softmax and optimize the likelihood of the correct option).\n\n\n\n\n \u00a7.\u00a7 Knowledge augmentation \n\nWe also evaluate the pretrained models in a setting where retrieved external knowledge is concatenated to the question. We index previously mentioned Wikipedia medical articles with a BM25[We also experimented with Dense Passage Retrieval <cit.> but obtained inferior results.] search engine, using  ElasticSearch 8.0 default hyperparameters <cit.>, and we concatenate the 10 most relevant passages[The concatenated knowledge is truncated if it leads to overflow of text encoder input maximum sequence length.].\n\n\n\n \u00a7.\u00a7 Distractor prediction\n\n\nTo perform distractor prediction, we optimize the  ranking loss <cit.>\n on the SentenceBERT framework <cit.> using a BioLinkBERT-base <cit.> text encoder and default parameters. We train the ranking model on the MedMCQA training examples,  using the correct answer as a query, the associated non-correct options as relevant and answers to other questions as irrelevant distractors. We evaluate distractor prediction on 3446 differential diagnoses collected on DBPedia, and found out that using a disease as a query returns a correct differential diagnosis with  precision@3/recall@3 of 11%/15%[Random chance scores less than 0.2%/0.2%. BM25 scores 0.6%/0.7%].\n\n We generate 7 incorrect options for each title associated with the paragraphs of texts from the section <ref>, and we use probability-matching cue masking to build the WikiMedQA dataset. We use differential diagnoses and retrieved distractors as additional options.\n \n\n\n\n \u00a7.\u00a7 Cue masking and distractors retrieval\n\n\n\nTable <ref> compares fine-tuning on the WikiMedQA Wikipedia part to the DiseaseBERT infusion, and shows the impact of masking strategies. Word-level masking outperforms token-level masking, which shows that less masking leads \nto less extraneous masking and better knowledge infusion. Probability-matching masking also outperforms naive masking at the word level which further validates the importance of addressing extraneous masking. Finally, removing the differential diagnoses from the options substantially decreases accuracy, which showcases the value of differential diagnoses as natural distractors.\nFrom now on, we refer to the generated data with differential diagnoses and probability-matching masking as WikiMedQA.\n\n\n\n \u00a7.\u00a7 Overall results\n\n\nTable <ref> shows the test accuracy of BioLinkBERT fine-tuned on WikiMedQA then on various datasets, compared to the task-specific state-of-the-art. Fine-tuning on WikiMedQA leads to considerable accuracy improvements on all tasks, whether external knowledge is available or not, which shows that this pretraining leads to generalizable text representations for medical question answering.\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\n\nWe proposed a new dataset for Medical MCQA in English pretraining by leveraging distractors retrieval and cue masking. We identified the problem of extraneous masking, proposed the probability-matching masking and demonstrated its advantage, and showed that differential diagnoses were helpful distractors. Fine-tuning on WikiMedQA leads to considerable improvement on several datasets, and this method can be ported to other languages.\n\n\n\n\u00a7 ACKNOWLEDGEMENTS\n\nThis work was supported by the CHISTERA grant of the Call XAI 2019 of the ANR with the grant number Project-ANR-21-CHR4-000.\n\n\n\nacl_natbib\n\n\n\n\n\n"}