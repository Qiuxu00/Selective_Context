{"entry_id": "http://arxiv.org/abs/2303.07221v1", "published": "20230313155611", "title": "Generation-based Code Review Automation: How Far Are We?", "authors": ["Xin Zhou", "Kisub Kim", "Bowen Xu", "DongGyun Han", "Junda He", "David Lo"], "primary_category": "cs.SE", "categories": ["cs.SE"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneration-based Code Review Automation: \n How Far Are We?\n    Xin Zhou\nSingapore Management University \n\nSingapore \n\nxinzhou.2020@phdcs.smu.edu.sg\nKisub Kim**Corresponding author. Email: kisubkim@smu.edu.sg\nSingapore Management University \n\nSingapore \n\nkisubkim@smu.edu.sg\nBowen Xu\nSingapore Management University \n\nSingapore \n\nbowenxu@smu.edu.sg\nDongGyun Han\nRoyal Holloway, University of London \n\nUK \n\ndonggyun.han@rhul.ac.uk\nJunda He\nSingapore Management University \n\nSingapore \n\njundahe@smu.edu.sg\nDavid Lo\nSingapore Management University \n\nSingapore \n\ndavidlo@smu.edu.sg\n\n    March 30, 2023\n======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\nCode review is an effective software quality assurance activity; however, it is labor-intensive and time-consuming. \nThus, a number of generation-based automatic code review (ACR) approaches have been proposed recently, which leverage deep learning techniques to automate various activities in the code review process (e.g., code revision generation and review comment generation).\n\nWe find the previous works carry three main limitations. First, the ACR approaches have been shown to be beneficial in each work, but those methods are not comprehensively compared with each other to show their superiority over their peer ACR approaches. \nSecond, general-purpose pre-trained models such as CodeT5 are proven to be effective in a wide range of Software Engineering (SE) tasks. \nHowever, no prior work has investigated the effectiveness of these models in ACR tasks yet. \nThird, prior works heavily rely on the Exact Match (EM) metric which only focuses on the perfect predictions and ignores the positive progress made by incomplete answers.\nTo fill such a research gap, we conduct a comprehensive study by comparing the effectiveness of recent ACR tools as well as the general-purpose pre-trained models. \nThe results show that a general-purpose pre-trained model CodeT5 can outperform other models in most cases. \nSpecifically, CodeT5 outperforms the prior state-of-the-art by 13.4%\u201338.9% in two code revision generation tasks. \nIn addition, we introduce a new metric namely Edit Progress (EP) to quantify the partial progress made by ACR tools.\nThe results show that the rankings of models for each task could be changed according to whether EM or EP is being\nutilized.\nLastly, we derive several insightful lessons from the experimental results and reveal future research directions for generation-based code review automation. \n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\n\nModern software development involves numerous software quality assurance activities, such as defect management\u00a0<cit.>, testing\u00a0<cit.>, and code review\u00a0<cit.>, to ensure the quality of software. Code review specifically requires reviewers to assess whether the source code written by authors satisfies both functional (e.g., compilation and testing) and non-functional (e.g., code readability) requirements. \nMany studies\u00a0<cit.> have shown the outstanding effectiveness of the code review process in removing defects and improving maintainability and readability. \nIn addition, code review also helps to share programming knowledge among developers\u00a0<cit.>.\n\n\n\nThe benefits of code reviews have been broadly recognized and it is widely adopted in both proprietary and open-source software projects. \nHowever, the benefits come with extensive manual costs in reviewing. \nIn the modern software development process, a large number of code changes are required to be reviewed per month. \nFor example, Microsoft Bing and Linux projects require 3,000 and 500 reviews, respectively, per month\u00a0<cit.>.\nIn addition, studies\u00a0<cit.> show that program comprehension (one of the major work for code reviewers) takes up as much as half of a developer\u2019s time, especially when the code quality is low\u00a0<cit.>. \nTo reduce the developers' burden, researchers have created techniques\u00a0<cit.> to automate code review activities by leveraging deep learning algorithms.\nParticularly, they focus on revising the submitted code to address the possible flaws in the code (i.e., Code Revision Before Review)\u00a0<cit.>, writing review comments based on the submitted code (i.e., Review Comment Generation)\u00a0<cit.>, and revising the submitted code based on the comments written by reviewers (i.e., Code Revision After Review)\u00a0<cit.>.\n\n\n\nRecent approaches\u00a0<cit.> targeting the above code review tasks push code review automation to new heights.\nTufano et al.\u00a0<cit.> proposed an approach namely Trans-Review based on Transformer\u00a0<cit.> to complete the code revision generation tasks. \nThey first abstracted the source code by adopting the src2abs tool\u00a0<cit.> to reduce the vocabulary size. Then they trained a sequence-to-sequence Transformer on the abstracted code. \nThongtanunam et al.\u00a0<cit.> proposed AutoTransform, which leverages the Byte-Pair Encoding (BPE)\u00a0<cit.> tokenizer and a sequence-to-sequence Transformer to complete the code revision generation task. They highlighted the problem that the revised version of the code might have new identifiers/literals (i.e., new tokens), and it is challenging to generate those new tokens. The BPE approach could help to mitigate such a problem.\nTufano et al.\u00a0<cit.> leveraged the Text-To-Text Transfer Transformer (T5) architecture\u00a0<cit.> to develop a pre-trained model for code review (i.e., T5-Review) based on 1.4 million code snippets and texts from Stack Overflow[https://stackoverflow.com/] and CodeSearchNet\u00a0<cit.>. \nThe results showed that the pre-trained T5-Review led to a significant improvement over the non-pre-trained Transformer models in three generation-based code review tasks.\n\n\n\nAlthough ACR methods have been proven to be beneficial, \nthese methods are not compared against each other which may confuse the practitioners about which one is suitable for a specific task.\nIn addition, general-purpose pre-trained models for code like CodeBERT\u00a0<cit.>, GraphCodeBERT\u00a0<cit.>, and CodeT5\u00a0<cit.> have been proven to be effective in a wide range of downstream tasks\u00a0<cit.> in the field of software engineering such as code search and code summarization. \nHowever, the performance of these models on ACR remains unknown.\n\n\n\n\nExisting ACR works heavily rely on a strict evaluation metric called Exact Match (EM): a revised code is considered as correct only if it is identical to the ground truth code.\nAlthough Exact Match is useful, it is very strict and only focuses on the number of perfect predictions.\nPrior studies\u00a0<cit.> indicated that the developers did not blindly trust automated tools and their recommendations. For instance, Winter et al.\u00a0<cit.> surveyed 386 software developers about their attitudes toward automatic program repairs (APR) tools which are similar to ACR tools. 47.0% of responses highlighted that human decisions are needed when applying recommendations from automated tools and 23.0% of responses do not trust automatic tools at all.\nIn addition, the perfect prediction ratios of ACR tools are still low (i.e., 1.2%\u201323.2%). Thus, \nwe believe that the current ACR tools are targeting to provide high-quality drafts for code reviewers rather than fully replacing them. \nIn this sense, instead of only focusing on the number of perfect predictions (EM), we need to care more about the improvements made in the predictions compared to the initial submitted code on the average level.\nFigure\u00a0<ref> shows an example that CodeT5 generates a better draft code based on the submitted code by removing \u201c\u2019\u2019. Though the generated code cannot meet all requirements of reviewers, it still makes positive progress, which should be taken into account when evaluating the effectiveness of a model.\nTo quantify the progress made in predictions, we introduce a new evaluation metric namely Edit Progress (EP), which measures the reduction ratio of edits needed from the submitted code to the ground truth.\n\n\n\nTo fill the gaps, we conduct a large-scale comprehensive experiment by comparing the effectiveness of recent ACR tools as well as the general-purpose pre-trained models on a unified benchmark. \nOur goal is to figure out how far we have reached in generation-based code review tasks and highlight the challenges and potential research directions for better code review automation.\n\n\n\n\n\nIn summary, our study covers three state-of-the-art ACR tools (i.e., Trans-Review\u00a0<cit.>, AutoTransform \u00a0<cit.>, and T5-Review\u00a0<cit.>) and three general-purpose pre-trained models (i.e., CodeBERT\u00a0<cit.>, GraphCodeBERT\u00a0<cit.>, and CodeT5\u00a0<cit.>) on three code review downstream tasks: \nCode Revision Before Review (i.e., revising the submitted code to address the possible flaws), Code Revision After Review (i.e., revising the submitted code based on the comments written by reviewers), and Review Comment Generation (i.e., writing review comments pointing out problems in the submitted code).\n\n\nOur experimental results reveal the following key findings:\n\n\n\n\n\n  * A pre-trained encoder-decoder model, CodeT5, achieves state-of-the-art performance and outperforms all existing ACR techniques in most cases.  \nWe carry out a comprehensive evaluation and discover that 1) T5-Review is the best model among existing ACR approaches in terms of the Exact Match metric; 2) CodeT5 can outperform all existing ACR approaches by a large margin for most cases.\n\n\n\n\n\n  * Stack Overflow data is helpful to the Review Comment Generation task.\nAs more pre-training corpus written in natural language are provided in Stack Overflow data, T5-Review which is pre-trained on Stack Overflow data has significantly outperformed all other methods by a large margin.\n\n \n\n\n\n  * Partial progress also matters.\nWe observe that a model that can generate the most perfect predictions may not able to generate the most close-to-perfect predictions, indicating that there may exist much noise on the failure cases of the approaches that can generate many perfect predictions.\nIn addition, the rankings of approaches in EM and EP can be different. This indicates that we may miss tools that can contribute the most positive progress if it ranks not very high in EM. Thus, measuring partial progress is also important for a comprehensive understanding of the ACR tools' effectiveness. \n\n  \n\n\n\nOverall, our empirical investigation sheds light on the strength and weaknesses of each ACR approach as well as the future research directions for generation-based code review automation.\nWe summarize our contributions as follows: \n\n\n  * We conduct a comprehensive investigation on the effectiveness of existing ACR models as well as pre-trained models for code in three downstream code review tasks.\n\n  * We introduce a novel evaluation metric to quantify the progress made by ACR tools in generating revised code.\n\n  * We derive several insightful lessons from the experimental results and reveal the future direction of the technical design for code review tasks.\n\n\n\n\n\n\n\n\n\n\n\u00a7 BACKGROUND\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we introduce the code review processes and formulate the three automatic code review tasks. \nThen we briefly summarize the automatic code review tools and general-purpose pre-trained code models under investigation. \n\n\n\n\n \u00a7.\u00a7 Code Review Processes\n\n\nFigure\u00a0<ref> briefly shows how code reviews are performed in software development. Code authors first check the correctness of the code by themselves and then submit the code for review. Code reviewers check the submitted code and decide whether to accept/reject to merge the code into the main repository. If the submitted code is rejected, reviewers write comments explaining what and how to improve the code.\nThen the code authors address those comments and re-submit for review again.\n\n\n\n\n \u00a7.\u00a7 Generation-based Code Review Tasks\n\n\nIn this subsection, we describe the generation-based code review downstream tasks. The considered tasks in our study are all proposed to automate the subset of activities in code review processes. We follow the prior studies\u00a0<cit.> to formulate each of those tasks.\n\n\n\n\nCode Revision Before Review (CRB) mainly aims to help the code authors who make the code changes to improve and commit them to the repositories.\nIt could help the code authors to address some simple flaws and thus improve the quality of the code before submitting it for review.\nIt automates the code review activity 1 in Figure\u00a0<ref>.\nAt the same time, it also reduces the burden of reviewers on pointing out simple and repeated flaws. \nThis task is formulated as a sequence-to-sequence task: f(Code) Revised   Code where f is the DL model and \u201cRevised   Code\u201d is the ground truth (i.e., the revised code that is accepted by code reviewers).\n\n\n\n\n\n\nCode Revision After Review (CRA) \naims at supporting code authors by addressing review comments\nwhich automate the code review activity 2 in Figure\u00a0<ref>).\nThe techniques belonging to this task can boost the speed of the revision process as they generate the initial version.\nThis task is formulated as a sequence-to-sequence task: f(Code, Comment) Revised   Code where f is the DL model and \u201cRevised   Code\u201d is the ground truth code accepted by reviewers.\nFor this task, there have two inputs (i.e., the submitted code and the comments) and a single output (i.e., the revised code). \n\n\n\n\n\n\n\nReview Comment Generation (RCG) is designed for the reviewers.\nThe models in this task draft the initial review comments such that reviewers can save time by revising the written draft, which automates the code review activity 3 in Figure\u00a0<ref>.\nReview Comment Generation is also formulated as a sequence-to-sequence task: f(Code) Comment where f is the DL model.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Automatic Code Review  Tools\n\n\n\nAll considered tools are built based on a popular Deep Learning model architecture called Transformer\u00a0<cit.>, which mitigates the long-distance dependency issue of CNN-based models\u00a0<cit.> and the long-time computation issue of RNN-based models\u00a0<cit.>.\nA Transformer model typically has two parts: an encoder to understand the semantics of the input data and a decoder to generate the final results (e.g., description, translation, revision, etc.) \nAll studied models adopt the Transformer model (both the encoder and decoder). We highlight the novel designs of each studied model in the following. \n\n\n\n \nTrans-Review\u00a0<cit.> are proposed to complete the Code Revision Before Review task (CRB) and the Code Revision After Review (CRA).\nA large vocabulary size will hinder the learning process and lead to inaccurate generation\u00a0<cit.>. To reduce the vocabulary size, Trans-Review abstracted the source code by adopting the src2abs\u00a0<cit.>, which replaces actual identifiers/literals in code with reusable IDs.\nSpecifically, for both the submitted and revised versions of code, it replaces identifiers (e.g., types and names of source code elements) and literals (e.g., string values) with an ID that is allowed to be reused across different code snippets. \nFor instance, as shown in Figure\u00a0<ref>, the first variable appearing in each code will always be replaced with an ID of \u201c1\u201d and a string \u201cVAR\u201d to indicate it is a variable (i.e., VAR1). \n\n\n\n\nAutoTransform\u00a0<cit.> is only designed to complete the Code Revision Before Review task (CRB).\nUnlike Trans-Review, it applies a Byte-Pair Encoding (BPE)\u00a0<cit.> approach to reduce the size of the vocabulary.\nBPE is a tokenization approach that splits a code token into a sequence of frequently occurring subtokens. For instance, a code token \u201cbuyOrders\" in Figure\u00a0<ref> will be split into several subtokens \u201cbuy\u201d, \u201cOrd\u201d, and \u201cers\u201d. \nThis strategy brings benefits to efficiency as subtokens are not only occurred more frequently than the regular code tokens, but they also allow to reduce the size of the vocabulary.\nEmpirical evidence showed that BPE could effectively address the large vocabulary size  problem\u00a0<cit.>.\n\n\n\n\n\n\n\n\nT5-Review\u00a0<cit.> leveraged the Text-To-Text Transfer Transformer (T5) architecture\u00a0<cit.>. \nT5 is a pre-training paradigm that corrupts the input data (e.g., randomly masking/dropping/inserting tokens) before feeding into the Transformer encoder and reconstructing the original input data at the Transformer decoder. Those pre-training tasks are called denoising objectives.\nTufano et al. pre-trained a code review version of T5 (denoted T5-Review) on 1.4 million code snippets and texts from Stack Overflow and the CodeSearchNet dataset\u00a0<cit.>. T5-Review consists of a 6-layer Transformer encoder and a 6-layer Transformer decoder. To reduce the vocabulary size, T5-Review leverages BPE tokenization\u00a0<cit.>.\n\n\n\n\n\n\n\n \u00a7.\u00a7 General-purpose Pre-trained Models for Code\n\nThe following techniques are the popular pre-trained models that are trained on source code and texts.\n\n\n\n\nCodeBERT\u00a0<cit.> is a SE knowledge-enriched bi-modal pre-trained model, which is capable of modeling both natural languages (NL) and programming languages (PL). \nCodeBERT leverages a 12-layer Transformer encoder as the model and pre-trained the encoder with the NL-PL data pairs from the CodeSearchNet dataset\u00a0<cit.>.\nIt adopts two pre-training objectives jointly: Masked Language Modeling (MLM)\u00a0<cit.> and Replaced Token Detection (RTD)\u00a0<cit.>. \nThe eventual loss function for CodeBERT is formulated below:\n\n    \u03b8min( \n        \u2112_MLM(\u03b8)+\u2112_RTD(\u03b8))\n \nwhere \u03b8 denotes the model parameters of the 12-layer Transformer encoder.\nCodeBERT has shown great effectiveness in a range of SE downstream tasks such as code search and code summarization\u00a0<cit.>. \n\n\n\n\n\nGraphCodeBERT\u00a0<cit.> brings the inherent structure of code into consideration during source code modeling. \nTo learn source code representation, GraphCodeBERT leverages three input components, which are Programming Language, Natural Language, and Data Flow Graph.\nGraphCodeBERT introduces two structure-aware pre-training tasks (i.e., Edge Prediction and Node Alignment) aside from the MLM prediction task. \n\n\n\n\nCodeT5\u00a0<cit.>\nis another pre-trained model whose Transformer encoder and decoder are both pre-trained. CodeT5 is capable of establishing complex mappings from sequence to sequence, which is suitable for generation-based tasks. CodeT5 is developed based on the general T5 architecture and is pre-trained not only with the original pre-training tasks (i.e., the denoising objectives) of T5 but also a set of newly proposed pre-training tasks (e.g., Masked Identifier Prediction and Identifier Tagging). Specifically, CodeT5 is enhanced by making it aware of the type information of identifiers.\n\n\n\n\n \n\n\n\n\u00a7 EXPERIMENTAL DESIGN\n\n\nIn this section, we introduce our research questions and describe the corresponding experimental settings.\n\n\n\n \u00a7.\u00a7 Research Questions\n\nRQ1. Which is the best-performing ACR tool for each targeted code review activity?\nRecent ACR tools have been demonstrated to be effective in code review tasks.  \nHowever, a comparative study is missing, which makes it confusing to rank the existing techniques. \nAddressing this research question may support practitioners in selecting the most suitable technique for each downstream task, and the revealed state-of-the-art that can be considered for the baseline of future research.\nIn this RQ, we conduct a revisiting evaluation of existing ACR tools on the same set of available datasets.\n\n\n\nRQ2. Can popular pre-trained models outperform the existing ACR models?\nGeneral-purpose pre-trained code models have shown significant generalizability and demonstrated promising performance in a broad range of SE tasks\u00a0<cit.> with datasets of varying sizes and properties.\nHowever, prior ACR tools have not considered these models either as baselines or building blocks of the ACR tools. \nWe believe that discovering the effectiveness of such models on code review tasks would contribute to the research community, and they can also be the baseline for future works.\nThe subject models are listed as follows: (1) CodeBERT\u00a0<cit.>, GraphCodeBERT\u00a0<cit.>, and CodeT5\u00a0<cit.>.\nTo answer this research question, we take popular pre-trained models into consideration and compare them with existing ACR models.\n\nRQ3. Comparing the generated code with the human-written code, how far apart are they?\nThe prior ACR works only use a strict evaluation metric (i.e., Exact Match) to analyze the effectiveness of their models: a revised code is considered as a correct generation only if it is identical to the ground truth. Otherwise, no matter if the difference is just a single token or they are different, the generated code is equally considered wrong. \nThus, we may miss suggestions that are very close to ground truths that are still partially useful (as shown in the example in Figure\u00a0<ref>). \nIt may be interesting to have an overall idea of how close those generated codes are compared to the ground truths because the current automation of the code review process does not only aim to replace the practitioners but also provide suggestions/drafts for developers to work on\u00a0<cit.>.\nTo answer this research question, we introduce a new evaluation metric, Edit Progress, which quantifies the progress achieved by the generated code from the problematic code toward the ground truth.\nEdit Progress is firstly used in the text editing task to measure the progress\u00a0<cit.>. \nNote that this metric can only be applied in code revision generation tasks because Edit Progress needs an initial input and the revised version to compute whether the revised one improves or degrades the initial one. However, in the Review Comment Generation task, there is no initial comment.\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Experimental Settings\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Datasets\n\nWe include three different datasets used in evaluating the latest ACR tools, namely Trans-Review_data, AutoTransform_data, and T5-Review_data.\nTufano et al.\u00a0<cit.> collected the Trans-Review_data from projects in Gerrit and GitHub.\nThey filtered out noisy comments and comments that are longer than 100 tokens. Besides, they removed the review data where the revised code has new tokens not shown in the initial submitted code.\nThongtanunam et al.\u00a0<cit.> used the AutoTransform_data that is collected from three Gerrit code review repositories: Android[<https://android-review.googlesource.com/.>], Google[<https://gerrit-review.\ngooglesource.com/>], and Ovirt[<https://gerrit.ovirt.org/>]. Please note that AutoTransform_data only has the submitted codes and the revised codes but does not have the corresponding review comments.\nTufano et al\u00a0<cit.> collected T5-Review_data from Java open-source projects from GitHub which have at least 50 pull requests, 10 contributors, 10 stars, and are not forks. They filtered out noisy comments, non-English comments, and duplicate comments. \n\nIn this paper, for the Code Revision Before Review (CRB), we employ Trans-Review_data, AutoTransform_data, and T5-Review_data.\nFor the Code Revision After Review (CRA) and Review Comment Generation (RCG), we leverage Trans-Review_data and T5-Review_data. We do not use AutoTransform_data because it does not contain the reviewers' comments given to submitted codes.\nThe statistics of the used datasets are summarized in Table\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Evaluation Metrics\n\n\nFollowing previous literature\u00a0<cit.>, we report the performance of each investigated model using the Exact Match metric (RQ1 and RQ2). We additionally report Edit Progress to measure how close generated codes are compared to the ground truths (RQ3).\n\n\n\n\n\nExact Match (EM) is a strict metric. \nSpecifically, \nfor each input-output pair, if the tokens of the model's prediction (generated by the model given the input) exactly match the tokens of the ground truth (output), EM = 1; otherwise, EM = 0. \nWith just a single token difference between the generated code and the ground truth, the EM score of the generated code is 0. \n\n\n\n\n\nEdit Progress (EP) is a novel metric to measure the progress made by the generated code on the path from the erroneous code to the clean code.\nRecently, Dibia et al.\u00a0<cit.> highlighted the significance of measuring the effort to correct the generated code. They suggested that an edit distance-based metric\u00a0<cit.> could serve as an important proxy for estimating the effort required to edit and fix generated code.\nAs an edit distance-based metric, the EP metric may offer a better estimation of the user effort needed to edit code generations in ACR tasks than EM.\nIn automatic code review tasks,  each data instance in the test set contains an initial submitted code  C and a ground truth code C\u0305 that is revised by developers and accepted by code reviewers. Given the initial submitted code C, each ACR model generates a revised code \u0108. The Exact Match (EM) is a strict metric as EM = 1 only when \u0108 is identical to C\u0305.\nWhile useful, Exact Match also has limitations. It only expects models to be able to fully correct/revise an erroneous code into the correct code.  While in some cases, models are still able to make progress by reducing the number of errors in the submitted code, even though the generated code revisions \u0108 are not identical to the ground truth code revisions C\u0305.\nTo measure the progress made on the path from the erroneous code to the clean code, we adopt the Edit Progress\u00a0<cit.> metric in this work. \nSpecifically, for a data instance \u27e8C, C\u0305\u27e9, the Edit Progress (EP) scores of ACR models are computed as the relative edit reduction between the initial code C and the ground truth code C\u0305 by predicting a revision \u0108. The metric is formulated in the following equation:\n\n\n    Progress = |D_CC\u0305|  -  |D_\u0108C\u0305|/|D_CC\u0305|\n\nwhere D is the edit distance\u00a0<cit.> (i.e., the minimum number of operations required to transform one sequence into the other).\n|D_A  B| refers to the edit distance between a string A to a string B.\nAn example of how to calculate the EP score is shown in Figure\u00a0<ref>. In the example, to turn the submitted code C into the corresponding ground truth code C\u0305, there are four token-level edit actions: \n1) replace \u201chypot\u201d with \u201ccosh\u201d;\n2) delete \u201c,\u201d;\n3) delete \u201cdouble\u201d;\n4) delete \u201cy\u201d.\nIf a model generates a prediction like the first prediction in Figure\u00a0<ref>, there are three deletions left. Thus, the prediction makes 25% progress on the way from the submitted code to the ground truth revised code.\nSimilarly, the second prediction in Figure\u00a0<ref> makes 75% progress (reduce 75% edits needed from the submitted code to the ground truth code).\nA perfect prediction \u0108 that can fully correct all errors in the submitted code C would achieve a 100% Edit Progress score.\nA model prediction \u0108 can have negative progress if it generates more errors than correct edits. We calculate and report the average EP on all test data instances for each model.\n\n\n\n\nPrevious studies\u00a0<cit.> also consider the edit distance-based similarity metric namely Normalized Edit Distance to measure the similarity between the generated code and ground truth. This metric is the character-level edit distance and can be represented as:\nEdit Distance (x, y) = Levenshtein Distance (x, y)/max(len(x), len(y) )\nwhere x and y are strings of the generated code and ground truth, respectively; len(.) is the number of characters in the string; and Levenshtein Distance (x, y) is the number of single-character edits needed to transform one string (x) to another (y).\nThe proposed EP is a novel metric compared to the existing \u201cNormalized Edit Distance\u201d metric:\n\n\n  * EP uses token-level edit distance while Normalized Edit Distance only takes into account single-character edits and may not consider the semantics of code tokens. Furthermore, single-character edits may not be suitable for pre-trained models such as CodeT5, which build their vocabularies on the token or sub-token levels and cannot directly perform single-character edits\u00a0<cit.>. EP uses token-level edits, which preserve the meaning of tokens and are feasible for pre-trained models.\n\n  * EP compares the generated code with the input code to measure the generation quality of an ACR tool. In ACR tasks, the tools generate revised code, given the input. While the Normalized Edit Distance measures the similarity between the generated code and the ground truth, EP considers not only the similarity but also the improvement of the generated code compared to the input code. This makes EP more suitable than Normalized Edit Distance in situations where the input code is already very similar to the ground truth and needs to be improved further through the generated code.\n\n\n\n\n\n\n\n\n\nBLEU score is a widely used metric to measure the similarity of two natural language texts.\nFor review comment generation, the model generates comments written in natural language. We follow the prior work\u00a0<cit.> to adopt the commonly-used metric (i.e., the BLEU score\u00a0<cit.>) to measure the quality of generated texts. BLEU scores measure how well a generated text matches the ground truth text by counting the percentage of overlaps.\n\nPlease note that BLEU scores are different from Edit Progress scores: BLEU scores measure the similarity between predictions and ground truths, while Edit Progress scores measure the progress the predictions made toward generating the ground truths.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Implementation Details\n\nFor existing ACR tools, we adopt the original replication packages of Trans-Review[<https://github.com/RosaliaTufano/code_review>], AutoTransform[<https://github.com/awsm-research/AutoTransform-Replication>], and T5-Review[ <https://github.com/RosaliaTufano/code_review_automation>]. \n\n\n\nBy following the standard setting of fine-tuning pre-trained models\u00a0<cit.>, we set the maximum accepted input sequence as 512, learning rate as 5e-5, and the optimizer as AdamW\u00a0<cit.>. For validation, we select the best-performing model checkpoint on the validation set. \nThe best-performing checkpoint is used for testing.\nWe feed the test sets into each approach and get the predictions. Then, we calculate the evaluation metrics based on the predictions and the ground truths.\n\nBoth CodeBERT and GraphCodeBERT only have pre-trained encoders. Thus, when applying to a generation-based task where decoders are needed, we follow the original implementations provided by CodeBERT[ <https://github.com/microsoft/CodeBERT/tree/master/CodeBERT/code2nl>] and GraphCodeBERT[ <https://github.com/microsoft/CodeBERT/tree/master/GraphCodeBERT/translation>] in generation-based tasks to initialize a non-pre-trained decoder. \nTo make the size of the decoder the same as the size of the encoder, we initialize a 12-layer Transformer decoder for CodeBERT and GraphCodeBERT to complete three generation-based code review tasks. \nWe directly adopt the original CodeT5 since it has the pre-trained encode and decoder. \n\nNote that when generating the predictions, we allow each ACR model/pre-trained model to generate a single prediction (the Top 1 prediction) given the input data, which is widely used in prior studies\u00a0<cit.>. \n\n\n\n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n\n\nThis section describes the experimental results and answers the research questions. The experimental results are summarized in Tables\u00a0<ref>, <ref>, and <ref>, respectively. \nNote that tools in the light grey are the existing ACR tools and the others are general-purpose pre-trained models.\nRegarding the performance metrics, we adopt the EM by following prior studies to investigate the effectiveness of the existing ACR tools as well as pre-trained code models in RQ1 and RQ2. \nWe analyze our novel metric, EP, in RQ3 with its comprehensive rationale.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 RQ1: Which is the best-performing ACR tool for each targeted code review activity? \n\n\n\nFor Code Revision Before Review (Table\u00a0<ref>),\nT5-Review achieves a performance of 7.4%, 13.9%, and 3.3%  in terms of Exact Match (EM) on the three datasets, respectively. \nAmong the existing ACR tools, T5-Review shows the best performance. \n\n\nFor Code Revision After Review\n(Table <ref>), \nthe results show that T5-Review performs the best with 24.4% and 9.0% EM scores in Trans-Review_data and T5-Review_data respectively while Trans-Review achieves 13.5% and 0.3% for the same datasets.\n\nFor Review Comment Generation (Table <ref>), among ACR tools, only T5-Review is designed for this task. Thus, we only investigate its performance which achieves 2.0% and 2.3% for EM and BLEU scores on average, respectively. \n\n\nOverall, we observe that T5-Review shows the best performance among state-of-the-art ACR tools on the studied downstream tasks. \nThe major difference is that T5-Review is pre-trained on external large-scale data while Trans-Review and AutoTransform are not pre-trained. \nThe results indicate that such pre-training is an effective solution to improve performance in most ACR tasks. \nWe present further in-depth investigation on sub-findings (e.g., why Trans-Review struggles) in Section\u00a0<ref>.\n\n    Answer to RQ1: Among the existing ACR methods, T5-Review shows the best performance in the targeted downstream tasks, outperforming the second-best ACR tool by a large margin.\n    It implies that practitioners should consider using T5-Review for ACR tasks. \n\n\n\n\n\n \n\n \u00a7.\u00a7 RQ2: Can popular pre-trained models outperform the existing ACR models?\n\n\n\nAs we observed that pre-training may bring significant improvement in the case of T5-Review, we investigate further pre-trained code models. Although pre-training a model is usually computationally expensive\u00a0<cit.>,\nseveral general-purpose pre-trained code models (i.e., CodeBERT, GraphCodeBERT, and CodeT5) are publicly released. \nIn this RQ, we study the effectiveness of these pre-trained models under ACR tasks by comparing them against state-of-the-art ACR tools.\n\n\n\nWe also present the results of the general-purpose pre-trained code models in Table\u00a0<ref>, \u00a0<ref>, and\u00a0<ref>. \nAs the observation illustrates that the pre-trained models generally outperform non-pre-trained ACR tools, we focus on the comparison between pre-trained code models and T5-Review (i.e., the best existing ACR tool).\n\n\nFor the Code Revision Before Review (Table\u00a0<ref>), \non the one hand, CodeBERT and GraphCodeBERT fail to outperform the best ACR approach T5-Review identified in RQ1 for most cases.\nSpecifically, CodeBERT only outperforms T5-Review in Trans-Review_data by 12.2% while achieving 61.6% and 200% lower EM scores than T5-Review in the other two datasets.\nGraphCodeBERT consistently achieves lower EM scores than T5-Review. \nOn the other hand, \nCodeT5 is able to achieve higher/comparable EM scores of T5-Review. CodeT5 has significantly outperformed T5-Review in Trans-Review_data and T5-Review_data datasets by relative improvements of 18.9% (8.8-7.4/7.4) and 63.6% (5.4-3.3/3.3). However, for the AutoTransform_data dataset, CodeT5 achieves a comparable (slightly lower) EM score than T5-Review.\n\nFor the Code Revision After Review\n(Table\u00a0<ref>), \nCodeT5 still outperforms the best ACR tool, T5-Review, in most cases. \nParticularly, CodeT5 pushes forward the performances to new heights on EM scores of 30.2% and 16.1%, which are 23.8% (30.2-24.4/24.4) and 78.9% (16.1-9.0/9.0) over T5-Review improvements.\nMoreover, CodeBERT and GraphCodeBERT get EM scores of 20.2% and 19.2% for the Trans-Review_data, 11.3% and 11.8% for the T5-Review_data, which are slightly worse than T5-Review on average.\n\nFor the Review Comment Generation (Table\u00a0<ref>), CodeT5, however, is no longer the best-performing model, as T5-Review shows the best performance. \nIn particular, CodeT5 only achieves 1.1% and 1.6% in EM and BLEU on average, which are crucially lower than T5-Review (i.e., 2.0% and 2.3% in EM and BLEU) on average.\nBesides, both CodeBERT and GraphCodeBERT struggle with this task obtaining lower scores.\n\n\nOverall, the results imply that CodeT5 is the most suitable model for Code Revision Before Review (CRB) and Code Revision After Review (CRA), while practitioners can leverage T5-Review for the Review Comment Generation. Further in-depth analysis is provided in Section\u00a0<ref>.\n\n\n\n\n    Answer to RQ2: CodeT5 has impressive performance for two of our target code review tasks (i.e., Code Revision Before Review and Code Revision After Review) by surpassing the best state-of-the-art ACR tool, T5-Review. On the other hand, T5-Review is still the best on a task (i.e., Review Comment Generation). \n    We find that pre-trained models are generally effective for code review tasks.\n    \n\n\n\n\n\n \u00a7.\u00a7 RQ3: Comparing the generated code with the human-written code, how far apart are they?\n\n\nIn this research question, we adopt the Edit Progress (EP) metric to measure the progress made by each approach on the path from the erroneous code to the golden code.\nThe EP score indicates the percentage of progress made. If EP equals 100%, it means that the generated code has mitigated all the differences between the erroneous code and the ground truth. \n\nFor the Code Revision Before Review task, as shown in Table\u00a0<ref>, a model that can generate the most perfect predictions (i.e., EM) may not able to generate the close-to-perfect predictions (i.e., EP).\nFor the Trans-Review_data dataset, the best-performing model in EP is GraphCodeBERT, achieving an EP score of 50.6% while CodeT5 only achieves 41.8%.\nFor the AutoTransform_data dataset, AutoTransform shows the best performance with 29.9% in EP. Although CodeT5 performs much better than AutoTransform in EM, CodeT5 achieves much worse performance (i.e., -67.8%) than AutoTransform. This indicates that CodeT5 generates the most perfect predictions but may generate many noisy failure cases at the same time. While AutoTransform is more conservative: it does not generate many perfect predictions but ensures that most of its revisions bring positive contributions. \nFor T5-Review_data dataset, the best model in EP is CodeT5 with a 25.6% EP.\nThis indicates that we need to carefully select an approach for the CRB task when having different needs: either the one that can generate the most perfect predictions (i.e. in terms of EM) or the one that can make the most average positive contributions (i.e., in terms of EP).\n\n\n\nCompellingly, Table <ref> reports that CodeT5 consistently outperforms all the other approaches for the Code Revision After Review task.\nIt obtains EP scores of 66.8% and 41.9% in both datasets while T5-Review shows comparable results (i.e., 65.6% and 38.4%).\n\n\nOverall, the results suggest that CodeT5 is still the best model in EP for Code Revision After Review, while AutoTransform generates better code for Code Revision Before Review. Appealingly, AutoTransform achieves a 29.8% EP score on average for Code Revision Before Review, while other approaches show negative progress, which indicates that after their revisions, the generated codes are even worse than the originally submitted code by authors on average. \nWe further verify the research value of EP in Section\u00a0<ref>.\n\n\n\n\n    Answer to RQ3: \n    In summary, the EP metric can change the rank of the approaches for different code review tasks. For instance, in the Code Revision Before Review task, \n    the best model in EP (i.e., AutoTransform) is not even in the top 3 in EM.  \n    This also implies there may exist much noise on the failure cases of the approaches that can generate many perfect predictions.\n    CodeT5 can be the best candidate for Code Revision After Review according to the EP metric, while AutoTransform interestingly shows good performance in Code Revision Before Review for the partial progressive aspect.\n\n\n\n\n\n\n\u00a7 DISCUSSION\n\n\n\nThis section presents the lessons learned from our experimental results and the threats to validity.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Lessons Learned\n\n\n\nBPE can significantly improve model performances on CRB and CRA tasks if there are new tokens introduced in ground truth codes.\nPrior ACR tools (as well as pre-trained code models) adopt either BPE or abstraction to reduce the size of vocabulary for the ACR datasets. \nTrans-Review, the only ACR tool using abstraction, performs poorly (close to zero) on AutoTransform_data and T5-Review_data that contain new tokens (not shown in the submitted code) in ground truth code. \nThe abstraction technique replaces actual identifiers/literals in the submitted code with reusable IDs to form the vocabulary. However, if there are any new identifiers/literals (have not appeared in the submitted code) in the ground truth revised code, the built vocabulary does not contain the IDs for new identifiers/literals.\nThus, it is challenging for a DL model to generate new tokens beyond the vocabulary. \nWhile by using Byte-Pair Encoding (BPE), all other ACR tools (e.g., CodeT5, T5-Review, and AutoTransform) show good performance on datasets with new tokens, outperforming Trans-Review by at least 700% in EM.\n\n\n\n\n\n\n\nThe pre-trained encoder-decoder framework significantly outperforms the pre-trained encoder framework by 17.0%\u2013102.2%.\nFirstly, we find that pre-trained models perform better than not pre-trained models in most cases. For instance, pre-trained T5-Review and CodeT5 consistently show better performance than Trans-Review and AutoTransform in terms of Exact Match. \nSecondly, we find that within pre-trained models, pre-trained encoder-decoder models (e.g., CodeT5) perform significantly better (17.0%\u2013102.2%) than pre-trained encoder-only models (e.g., CodeBERT and GraphCodeBERT) in generation-based code review tasks. \nThe possible reason may be: when applying to generation-based tasks, pre-trained encoder-only models have to initialize a non-pre-trained decoder. This non-pre-trained decoder will lead to a worse performance\u00a0<cit.>. \nLastly, within pre-trained encoder-decoder models, general-purpose pre-trained model CodeT5 shows better performance than task-specific pre-trained model T5-Review in most cases. In particular, CodeT5 could outperform T5-Review by 13.4% to 38.9% in terms of EM for two code revision tasks. \n\n\nStack Overflow data is helpful to the Review Comment Generation task, especially for comments written in interrogative sentences (250.0% improvements).\nCodeT5 fails to outperform T5-Review for the Review Comment Generation task.\nTo explain why CodeT5 is inferior to T5-Review in this task, we further investigate the pre-training and downstream task datasets: \nFirstly, we find that many comments in the Review Comment Generation datasets are interrogative sentences (e.g., \u201cwhy is there a newArrayList?\u201d). \nThen we investigate the ratios of interrogative sentences in pre-training data by a simple rule-based method: if a comment contains a question word such as  \u201cwhich\", \u201cwhere\", \u201cwhat\", \u201cwhich\", \u201cdoes\", we assume this comment as an interrogative sentence.\nAs shown in Table\u00a0<ref>, CodeT5 is mainly pre-trained on the CodeSearchNet dataset, which contains 33.8% interrogative sentences while T5-Review is also pre-trained on the Stack Overflow data[Because both CodeT5 and T5-Review are pre-trained on the CodeSearchNet, to study the differences between them, we focus on analyzing Stack Overflow data for T5-Review.] with a higher ratio of interrogative sentences (i.e. 69.7% ).\nTable\u00a0<ref> also presents the performances on subsets with/without interrogative sentences, using the T5-Review_data as an example. \nWe observe that with a higher ratio of interrogative sentences in the Stack Overflow data, T5-Review can outperform CodeT5 by 250.0% on the subset with interrogative sentences.\nWhile T5-Review only outperforms CodeT5 by 88.3% on the subset without interrogative sentences.\nThe fewer interrogative sentences (comments with sentiments) in the CodeSearchNet dataset may hinder CodeT5 in generating interrogative sentences (or comments with sentiment words) given the submitted code. \n\nOverall, we find that Stack Overflow data is helpful to the review comment generation task (improving both with and without interrogative sentence subsets) possibly because it provides more pre-training corpus written in natural language. In particular, it can significantly improve the model's ability to generate interrogative sentences that frequently appear in human-written review comments.\nThis motivates future studies to consider building a large and more suitable pre-training dataset based on Stack Overflow and perform further pre-training if they aim to improve the review comment generation task.\n\n\n\n\n\n\nWe advocate future work to adopt Edit Progress to measure the partial progress achieved by ACR tools.\nThe performances of code revision approaches have mostly been measured with a specific metric named Exact Match (EM).\nEM is very strict as it completely ignores progressive contributions (i.e., non-perfect results) of generated code by an approach. \nTo measure the partial progressive aspect, we introduced Edit Progress (EP) and evaluated the approaches with such a metric.\nThe overall results demonstrated that the rankings of models for each task could be changed according to which metric is being utilized.\nWe study and verify the research value of such a metric here. \n\n\n\n\nFigure\u00a0<ref> shows the EP scores of each test instance of CodeT5 and AutoTransform in the CRB task using Trans-Review_data. We choose these two approaches as they are the best-performing approaches for each metric on average in the CRB task. The vertical lines show the average EP scores of each approach.\nNote that we omit the instances whose EP scores are less than -50% for better visualization. \nAs shown in Figure\u00a0<ref>, CodeT5 has a better EM score because it has more generated perfect code predictions with 100% Edit Progress scores (identical to ground truths). \nFrom these vertical lines (i.e., the average EP scores), we confirm that AutoTransform, which is generally inferior to CodeT5 in EM, retrieves better EP scores. \nOn the one hand, AutoTransform's positive effects on the improvement of partial progress are demonstrated by the more orange-colored bars in the high EP range (e.g., 80-99%).\nOn the other hand, it generates much less noise than CodeT5, as we can verify with the figure (i.e., there are much fewer orange-colored bars under 0% EP).\nThis phenomenon indicates that EP can evaluate the model performances in a more comprehensive way: not only focusing on those correct predictions but also taking the failure cases into account. EP can also evaluate the practical usability of ACR tools to some extent as users do not know which prediction is correct in advance and may expect all predictions have reasonably good qualities (e.g., at least making positive contributions).\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Threats to Validity\n\n\n\n\n\n\n\n\nGeneral purpose pre-trained models diverge depending on different aspects, such as the characteristics of pre-training tasks, characteristics of downstream tasks, and the size of datasets.\nOur study may have a selection bias by considering several pre-trained models. We believe that employing three of the most popular pre-trained code models (i.e., CodeBERT, GraphCodeBERT, and CodeT5) can mitigate the threat.\nAnother threat to validity regards the dataset selection, as it may deliver bias in the experimental results. In our work, we have minimized this threat by using multiple datasets, i.e., Trans-Review_data\u00a0<cit.>, AutoTransform_data\u00a0<cit.>, and T5-Review_data\u00a0<cit.>.\nFurthermore, we publicly share the replication package[Replication Package. <https://github.com/soarsmu/Generated_Review_How_Far>] including datasets for future comparisons by the research community.\nThe evaluation metrics used may also bring a selection bias.\nTo reduce it, we reuse the same evaluation metric (i.e., Exact Match) with existing ACR tools\u00a0<cit.> and the widely used metric for texts (i.e., BLEU\u00a0<cit.>).\nBesides, to study the partial progress achieved by ACR tools, we follow prior work in text editing\u00a0<cit.> to adopt the Edit Progress metric. \n\n\n\n\u00a7 RELATED WORK\n\n\n\n\nIn this section, we review the research work that relates to our work: approaches to automating code review activities and pre-trained models for SE.\n\n\nAutomating Code Review Activities. \nWe focus on three generation-based code review automation tasks: Code Revision Before Review\u00a0<cit.>, Code Revision After Review\u00a0<cit.>, and Review Comment Generation\u00a0<cit.>.\nOther studies focus on similar code review activities in different manners.  \nHellendoorn et al.\u00a0<cit.> aimed to predict the locations of code change hunks that possibly need review and revision. \nThey showed that locating a suitable hunk is a challenging task.\nSiow et al.\u00a0<cit.> proposed CORE, a multi-level embedding approach to represent the semantics of code changes and reviews, which recommends code reviews in a retrieval-based way.\nHong et al.\u00a0<cit.> proposed a retrieval-based approach to recommend code review comments. \nLi et al.\u00a0<cit.> developed CodeReviewer, a pre-trained model for code review. They evaluated it on three tasks using new datasets, aiming to improve code review. In contrast, we aimed to compare existing ACR tools and identify future directions.\n\n\n\nIn addition, there are other tasks to automate other code review activities to support software developers, such as reviewer recommendation\u00a0<cit.>, review prioritization\u00a0<cit.> and defect-proneness prediction in submitted code\u00a0<cit.>.\nFor instance, Thongtanunam et al.\u00a0<cit.> found 4% to 30% of reviews in open-source projects could not find suitable reviewers (i.e., reviewer recommendation). \nRahman et al.\u00a0<cit.> proposed CORRECT which utilizes external library similarity and technology expertise similarity of reviewers to recommend reviewers. \nAl-Zubaidi et al.\u00a0<cit.> did not only focus on reviewer experience but also take into account the review workload when recommending reviewers. \n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION AND FUTURE WORK\n\n\n\n\n\nIn this paper, we empirically study the existing generation-based automatic code review (ACR) techniques and the general-purpose pre-trained code models. \nWe evaluate the effectiveness of these tools on three representative generation-based code review tasks.\nOur experimental results show that CodeT5 is the best performer in most cases, outperforming the best existing ACR tool T5-Review by 13.4%\u201338.9% in Code Revision Before/After Review tasks.\nHowever, in Review Comment Generation, T5-Review outperforms all other methods by a large margin.\nOur work also has revealed several interesting findings that can motivate future work.   \n\n\nIn the future, we plan to design a new ACR tool that can perform well both in Exact Match and Edit Progress metrics. \n\n\n\n\nAcknowledgement. This research / project is supported by the National Research Foundation, Singapore, under its Industry Alignment Fund \u2013 Pre-positioning (IAF-PP) Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.\n\n\nIEEEtran\n\n\n\n"}