{"entry_id": "http://arxiv.org/abs/2303.06847v1", "published": "20230313043135", "title": "Label Distribution Learning from Logical Label", "authors": ["Yuheng Jia", "Jiawei Tang", "Jiahao Jiang"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "text": "\n\nOn the Regret of Online Edge Service Hosting t1\n    Sharayu Moharir\n    Received: date / Accepted: date\n===============================================\n\n\n\n\nLabel distribution learning (LDL) is an effective method to predict the label description degree (a.k.a. label distribution) of a sample. However, annotating label distribution (LD) for training samples is extremely costly. So recent studies often first use label enhancement (LE) to generate the estimated label distribution from the logical label and then apply external LDL algorithms on the recovered label distribution to predict the label distribution for unseen samples. But this step-wise manner overlooks the possible connections between LE and LDL. Moreover, the existing LE approaches may assign some description degrees to invalid labels. To solve the above problems, we propose a novel method to learn an LDL model directly from the logical label, which unifies LE and LDL into a joint model, and avoids the drawbacks of the previous LE methods. Extensive experiments on various datasets prove that the proposed approach can construct a reliable LDL model directly from the logical label, and produce more accurate label distribution than the state-of-the-art LE methods.\n\n\n\n\n\u00a7 INTRODUCTION\n\n Multi-label learning (MLL) <cit.> is a well-studied machine learning paradigm where each sample is associated with a set of labels. In MLL, each label is denoted as a logical value (0 or 1), indicating whether a label can describe a sample. However, the logical label cannot precisely describe the relative importance of each label to a sample. To this end, label distribution learning (LDL) <cit.> was proposed, in which real numbers are used to demonstrate the relative importance of a label to a certain sample. For example, Fig. 1(a) is a natural scene image, which is annotated with three positive labels (\u201cSky\", \u201cMountain\" and \u201cSea\") and one negative label (\u201cHuman\") as shown in Fig. 1(b). Since the relative importance of each label to this image is different, for example, the label \u201cMountain\" is more important than the label \u201cSea\", the real numbers (also known as the description degree) in Fig. 1(c) can better describe this image. The description degrees of all labels constitute a label distribution (LD). Specifically, if d^l_x represents the description degree of the label l to the instance x, it's subject to the non-negative constraint d^l_x\u2208[0,1] and the sum-to-one constraint \u2211_l d^l_x=1. LDL aims to predict the LD for unseen samples, which is a more general paradigm than the traditional MLL.\n\n\n\n\n\n\nIn LDL, a training set with samples annotated by LDs is required to train an LDL model. Unfortunately, the acquisition of label distributions of instances is a very costly and time-consuming process. Moreover, in reality, most datasets are only annotated by logical labels, which cannot be used directly by the existing LDL methods. Then a question naturally arises: can we directly train an LDL model from the logical labels? \n\nRecently, label enhancement (LE) <cit.> was proposed to partially answer this question. Specifically, LE first recovers the label distributions of training samples from the logical labels and then performs an existing LDL algorithm on the recovered LDs. Following <cit.>, many variants of LE were proposed, such as <cit.>, <cit.> and <cit.>. We refer the readers to the related work for more details. To achieve LE, those methods generally construct a linear mapping from the features to the logical labels directly, and then normalize the output of the linear mapping as the recovered LDs. However, those LE methods may assign positive description degrees to some negative logical labels. Moreover, as all the positive logical labels are annotated as 1, simply building a mapping from features to the logical labels is unreasonable, which will not differentiate the description degrees to different labels. Last but not least, the LE-based strategy is a step-wise manner to predict LD for unseen samples, which may lose the connection between LE and LDL model training. \n\nIn this paper, we come up with a novel model named DLDL, i.e., Directly Label Distribution Learning from the logical label, which gives a favorable answer to the question \u201ccan we directly train an LDL model from the logical labels\". The major contributions of our method are summarized as follows:\n\n\n\n  * Our model is the first attempt to combine LE and LDL into a single model, which can learn an LDL model directly from the instances annotated by the logical labels. By the joint manner, the LE and LDL processes will better match each other. \n\n\n  * We strictly constrain the description degree d^l_x to be 0 when the logical value corresponding to the label l is 0. The constraint will avoid assigning positive description degrees to the negative logical labels.\n\n\n  * The existing LE methods usually minimize a least squares loss between the recovered LD and the logical label, which can not differentiate the description degrees of different labels. The proposed model avoids this fidelity term, and uses KL-divergence to minimize the difference between the recovered LD and the predictive LD, which is a better difference measure for two distributions.\n\nExtensive experiments on six benchmark datasets clearly show that the LD recovered by our method is better than that recovered by state-of-the-art LE methods on the training set, and the prediction performance of our model on the testing set also achieves state-of-the-art results compared to the traditional step-wise strategy.\n\nThe structure of the paper is as follows. Firstly, we briefly review the related works on LDL and LE in Section 2. Secondly, we present the technical details of our model in Section 3. Then, the experimental results and analyses are  presented to  prove the effectiveness of our model in Section 4. Finally, conclusions and future working directions are drawn in Section 5.\n\n\n\n\n\u00a7 RELATED WORKS\n\nNotations: Let n, m and c represent the number of samples, the dimension of features, and the number of labels. Let x\u2208\u211d^m denote a feature vector and y\u2208{0,1}^c denote its corresponding logical label vector. The feature matrix and the corresponding logical label matrix can be denoted by \ud835\udc17=[x_1;x_2;\u2026;x_n]\u2208\u211d^n\u00d7 m and \ud835\udc18=[y_1;y_2;\u2026;y_n]\u2208{0,1}^n\u00d7 c, respectively. Let \ud835\udcb4={l_1, l_2, \u2026, l_c} be the complete set of labels. The description degree of the label l to the instance x is denoted by d^l_x, which satisfies d^l_x\u2208[0,1] and \u2211_l d^l_x=1, and the label distribution of x_i is denoted by d_i=(d^l_1_x_i,d^l_2_x_i,\u2026,d^l_c_x_i).\n\n\n\n \u00a7.\u00a7 Label Distribution Learning\n\nLDL is a new machine learning paradigm that constructs a model to predict the label distribution of samples. At first, LDL were achieved through problem transformation that transforms LDL into a set of single label learning problems such as PT-SVM, PT-Bayes <cit.>, or through  algorithm adaptation that adopts the existing machine learning algorithms to LDL, such as AA-kNN and AA-BP <cit.>. SA-IIS <cit.> is the first model that specially designed for LDL, whose objective function is a mixture of maximum entropy loss <cit.> and KL-divergence. Based on SA-IIS, SA-BFGS <cit.> adopts BFGS to optimize the loss function, which is faster than SA-IIS. Sparsity conditional energy label distribution learning (SCE-LDL) <cit.> is a three-layer energy-based model for LDL. In addition, SCE-LDL is improved by incorporating sparsity constraints into the objective function. To reduce feature noise, latent semantics encoding for LDL (LSE-LDL) <cit.> converts the original data features into latent semantic features, and removes some irrelevant features by feature selection. LDL forests (LDLFs) <cit.> is based on differentiable decision trees and may be combined with representation learning to provide an end-to-end learning framework. LDL by optimum transport (LDLOT) <cit.> builds an objective function using the optimal transport distance measurement and label correlations.\n\n\n\n \u00a7.\u00a7 Label Enhancement\n\nThe above mentioned LDL methods all assume that in the training set, each sample is annotated with label distribution. However, precisely annotating the label distributions for the training samples is extremely costly and time-consuming. On the contrary, many datasets annotated by logical labels are readily available. To this end, LE was proposed, which aims to convert the logical labels of samples in training set to label distributions. GLLE <cit.> is the first LE algorithm, which assumes that the LDs of two instances are similar to each other if they are similar in the feature space. LEMLL <cit.> adopts the local linear embedding technique to evaluate the relationship of samples in the feature spaces. Generalized label enhancement with sample correlations (gLESC) <cit.> tries to obtain the sample correlations from both of the feature and the label spaces. Bidirectional label enhancement (BD-LE) <cit.> takes the reconstruction errors from the label distribution space to the feature space into consideration.\n\nAll of these LE methods can be generally formulated as\n\n    min_\ud835\udc16||\ud835\udc17\ud835\udc16-\ud835\udc18||_F^2+\u03d5 (\ud835\udc17\ud835\udc16,\ud835\udc17),\n\nwhere \ud835\udc17 and \ud835\udc18 are the feature matrix and logical label matrix, \u00b7_F denotes the Frobenius of a matrix, \ud835\udc16\u2208\u211d^m\u00d7 c builds a linear mapping from features to logical labels, and \u03d5 (\ud835\udc17\ud835\udc16,\ud835\udc17) models the geometric structure of samples, which is used to assist LD recovery. After minimizing Eq. (<ref>), those methods usually normalize \ud835\udc17\ud835\udc16 as the recovered LD.\n\nAlthough those LE methods have achieved great success, they still suffer from the following limitations. Firstly, the fidelity term that minimizes the distance between the recovered LD \ud835\udc17\ud835\udc16 and the logical label \ud835\udc18 is inappropriate, because the logical labels are annotated as the same value and the linear mapping will not differentiate the description degrees for different labels. Besides, the Frobenius norm is also not the best choice to measure the difference between two distributions. Secondly, Eq. (<ref>) doesn't consider the restriction of the label distribution, i.e. \u2200 i, 0\u2264d_i \u2264y_i, \u2211_l d^l_x_i=1. Although those LE methods performs a post-normalization to satisfy those constraints, they may assign positive description degrees to some negative logical labels. Furthermore, to predict the LD for unseen samples, those methods need to first perform LE and then carry out an existing LDL algorithm. The step-wise manner does not consider potential connections between LE and LDL.\n\n\n\n\u00a7 THE PROPOSED APPROACH\n\nTo solve the above mentioned issues, we propose a novel model named DLDL. Different from the previous two-step strategy, our method can directly generate a label distribution matrix \ud835\udc03 = [d_1;d_2;\u2026;d_n] \u2208\u211d^n\u00d7 c for samples annotated by logical labels and at the same time construct a weight matrix \ud835\udc16\u2208\u211d^m\u00d7 c to predict LD for unseen samples.\n\nWe use the following priors to recover the LD matrix \ud835\udc03 from the logical \ud835\udc18. First, as each row of \ud835\udc03 (e.g., d_i) denotes the recovered LD for a sample, it should obey the non-negative constraint and the sum-to-one constraint to make it a well-defined distribution, i.e., 0_c\u2264d_i\u22641_c and d_i1_c=1, where 0_c, 1_c\u2208\u211d^c denote an all-zeros vector and an all-ones vector, and 0\u2264d_i\u22641 means each element of d_i is non-negative and no more than 1. Moreover, to avoid assigning a positive description to a negative logical label, we require that 0\u2264d_i\u2264y_i, \u2200 i. Reformulating the above constraints in the matrix form, we have 0_m\u00d7 c\u2264\ud835\udc03\u2264\ud835\udc18, \ud835\udc031_c = 1_n, where 0_m\u00d7 c is an all-zeros matrix with size m\u00d7 c, and the above inequalities are held element-wisely.\n\nSecond, we utilize the geometric structure of the samples to help recover the LDs from the logical labels, i.e., if two samples are similar to each other in the feature space, they are likely to own the similar LDs. To capture the similarity among samples, we define the local similarity matrix \ud835\udc00=[A_i j]_n \u00d7 n as\n\n    A_i j=\n        exp(-x_i-x_j_2^2 / 2\u03c3^2),   if x_j \u2208\ud835\udca9(x_i)\n    \n        0,     otherwise.\n\n\ud835\udca9(x_i) denotes the k-nearest neighbors of x_i, and \u03c3^2>0 is a hyper-parameter. Based on the constructed local similarity matrix \ud835\udc00, we can infer that the value of ||d_i-d_j|| is small when A_ij is large, i.e.,\n\n    min_\ud835\udc03 i,j\u2211A_ij||d_i-d_j||_2^2 =tr(\ud835\udc03^T\ud835\udc06\ud835\udc03),\n\nin which tr( \u00b7) denotes the trace of a matrix, and \ud835\udc06=\ud835\udc00\u0302-\ud835\udc00 is the graph Laplacian matrix and \ud835\udc00\u0302 is a diagonal matrix with the elements \u00c2_i i=\u2211_j=1^n A_i j.\n\nBy taking the above priors into consideration, the LD matrix \ud835\udc03 can be inferred from the logical label matrix \ud835\udc18, i.e.,\n\n    \ud835\udc03min tr(\ud835\udc03\ud835\udc06\ud835\udc03^T)+||\ud835\udc03||_F^2\n       s.t. 0_m\u00d7 c\u2264\ud835\udc03\u2264\ud835\udc18, \ud835\udc031_c=1_n,\n\nwhere an additional term \ud835\udc03_F^2 is imposed as regularization. \n\nBased on the recovered LD, we adopt a non-linear model that maps the features to the recovered LD, which can be used to predict the LD for unseen samples. Specifically, the non-linear model is formulated as\n\n    P_ij=1Zexp(\u2211^m_k=1 X_ikW_kj),\n\nwhere \ud835\udc16 is the weight matrix, \ud835\udc0f=[P_ij]_n\u00d7 n is the prediction matrix, and Z=\u2211^c_j=1exp(\u2211^m_k=1 X_ikW_kj) is a normalization term.  To infer the weight matrix \ud835\udc16, we minimize the KL-divergence between the recovered label distribution matrix \ud835\udc03 and the prediction matrix \ud835\udc0f, because KL-divergence is a good metric to measure the difference between the two distributions. Accordingly, the loss function for inferring \ud835\udc16 becomes\n\n    \ud835\udc16minKL(\ud835\udc03,\ud835\udc0f)+||\ud835\udc16||_F^2,\n\nwhere the widely-used squared Frobenius norm is imposed to control the model complexity.\n\nTo directly induce an LD prediction model from the logical labels, we combine Eqs. (<ref>) and (<ref>) together and the final optimization problem of our method becomes:\n\n    \ud835\udc03,\ud835\udc16minKL(\ud835\udc03,\ud835\udc0f)+\u03b1 tr(\ud835\udc03\ud835\udc06\ud835\udc03^T)+\u03b2||\ud835\udc03||_F^2+\u03b3||\ud835\udc16||_F^2\n       s.t. 0_m\u00d7 c\u2264\ud835\udc03\u2264\ud835\udc18, \ud835\udc031_c=1_n,\n\nin which \u03b1, \u03b2 and \u03b3 are hyper-parameters to adjust the contributions of different terms. By minimizing Eq. (<ref>), the proposed model can recover the LD for samples with logical labels, and simultaneously, can predict LD for unseen samples through Eq. (<ref>). \n\n\n \nThen, some restrictions to the label distribution matrix \ud835\udc03 should be attached to \u039b(\ud835\udc03). Specifically, considering that d^l_x\u2208[0,1] and if Y_i j=0, the corresponding description degree d^l_j_x_i, i.e., D_i j, should be 0, which means that D must satisfy 0\u2264\ud835\udc03\u2264\ud835\udc18, where 0\u2208\u211d^n\u00d7 c is a matrix of all zeros and \u201c\ud835\udc00\u2264\ud835\udc01\u201d means that every element in \ud835\udc00 is less than or equal to the corresponding element in \ud835\udc01. So the final loss function of the LE process can be written as:\n\n\n    \u039b(\ud835\udc03)   =i,j\u2211A_ij||d_i-d_j||_2^2 + ||\ud835\udc03||_F^2\n       =tr(\ud835\udc03^T\ud835\udc06\ud835\udc03) + ||\ud835\udc03||_F^2,\n       s.t. 0\u2264\ud835\udc03\u2264\ud835\udc18, \ud835\udc031_c=1_n,\n\nin which 1_c is a vector of size c with all of its elements equals to 1.\n\n\nTo make LDL predictions, we need a prediction matrix \ud835\udc0f\u2208\u211d^n\u00d7 c and make it close to the label distribution matrix \ud835\udc03. We adopt a non-linear predictive model and train the weight matrix \ud835\udc16 to achieve this. In detail, we specify P_ij=1Zexp(\u2211^m_k=1 X_ikW_kj), in which Z=\u2211^c_j=1exp(\u2211^m_k=1 X_ikW_kj). Moreover, instead of the least squares loss function, we take the KL-divergence to measure the division from the label distribution matrix \ud835\udc03 to the prediction matrix \ud835\udc0f, since it can better represent the difference between the two probability distributions. By adding the widely-used squared Frobenius norm to control the scale of \ud835\udc16, we can obtain the loss function \u03a9(\ud835\udc03,\ud835\udc16) of the LDL process:\n\n    \u03a9(\ud835\udc03,\ud835\udc16)=KL(\ud835\udc03,\ud835\udc0f)+||\ud835\udc16||_F^2.\n\n\nSince our model is a combination of LE and LDL, we optimize \u039b(\ud835\udc03) together with \u03a9(\ud835\udc03,\ud835\udc16). By introducing three penalty coefficient \u03b1, \u03b2 and \u03b3, the overall optimization objective can be written as\n\n    \ud835\udc03,\ud835\udc16minKL(\ud835\udc03,\ud835\udc0f)+\u03b1 tr(\ud835\udc03\ud835\udc06\ud835\udc03^T)+\u03b2||\ud835\udc03||_F^2+\u03b3||\ud835\udc16||_F^2\n       s.t. 0\u2264\ud835\udc03\u2264\ud835\udc18, \ud835\udc031_c=1_n.\n\n\n\n\n\n\n \u00a7.\u00a7 Optimization\n\nEq. (<ref>) has two variables, we solve it by an alternative iterative process, i.e., update \ud835\udc16 with fixed \ud835\udc03, and then update \ud835\udc03 with fixed \ud835\udc16. \n\n\n  \u00a7.\u00a7.\u00a7 Update W\n When \ud835\udc03 is fixed, the optimization problem (<ref>) with respect to \ud835\udc16 can be formulated as follows:\n\n    \ud835\udc16min    KL(\ud835\udc03,\ud835\udc0f)+\u03b3||\ud835\udc16||_F^2.\n\nExpanding the KL-divergence and substituting P_ij in Eq. (<ref>) into Eq. (<ref>), we obtain the objective function of \ud835\udc16 as\n\n    T(\ud835\udc16)=\n           \u2211_i ln\u2211_jexp(\u2211_k X_i k W_k j)\n         +\u03b3||\ud835\udc16||_F^2\n       -\u2211_i,j D_i j\u2211_k X_i k W_k j.\n\nEq. (<ref>) is a convex problem, we use gradient descent algorithm to update \ud835\udc16, where the gradient is expressed as\n\n    \u2202 T( \ud835\udc16)\u2202 W_k j=   \u2211_iexp(\u2211_k X_i k W_k j) X_i k\u2211_jexp(\u2211_kX_i k W_k j)\n       -\u2211_i D_i j X_i k+2\u03b3 W_k j.\n\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Update D\n With fixed \ud835\udc16, the optimization problem (<ref>) regarding \ud835\udc03 is formulated as\n\n    \ud835\udc03min KL(\ud835\udc03,\ud835\udc0f)+\u03b1tr(\ud835\udc03\ud835\udc06\ud835\udc03^T)+\u03b2||\ud835\udc03||_F^2\n       s.t. 0\u2264\ud835\udc03\u2264\ud835\udc18, \ud835\udc031_c=1_n.\n\nEq. (<ref>) involves multiple constraints and diverse losses, we introduce an auxiliary variable \ud835\udc01 = \ud835\udc03\u2208\u211d^n\u00d7 c to simplify it, and the corresponding augmented Lagrange equation becomes\n\n    \ud835\udc03,\ud835\udc01min KL(\ud835\udc03,\ud835\udc0f) +\u03b1tr(\ud835\udc03^T\ud835\udc06\ud835\udc03) +\u03b2||\ud835\udc03||_F^2 \n                +<\u03a6,\ud835\udc01-\ud835\udc03>\n    +\u03c4/2||\ud835\udc01-\ud835\udc03||^2_F\n       s.t. 0\u2264\ud835\udc01\u2264\ud835\udc18, \ud835\udc011_c=1_n,\n\nwhere \u03a6\u2208\u211d^n\u00d7 c is the Lagrange multiplier, and \u03c4 is parameter to introduce the augmented equality constraint. Eq. (<ref>) can be minimized by solving the following sub-problems iteratively. \n\n\n1) \ud835\udc03-subproblem: Removing the irrelated terms regarding \ud835\udc03, the \ud835\udc03-subproblem of Eq. (<ref>) becomes\n\n    \ud835\udc03min KL(\ud835\udc03,\ud835\udc0f) +\u03b1tr(\ud835\udc03^T\ud835\udc06\ud835\udc03) +\u03b2||\ud835\udc03||_F^2\n                +<\u03a6,\ud835\udc01-\ud835\udc03>\n    +\u03c4/2||\ud835\udc01-\ud835\udc03||^2_F.\n\nEq. (<ref>) is solved when its gradient regarding \ud835\udc03 vanishes, leading to \n\n    \ud835\udc03=(\ud835\udc17\ud835\udc16+\u03a6+\u03c4\ud835\udc01)(2\u03b2+\u03b1(\ud835\udc06+\ud835\udc06^T)+\u03c4)^-1.\n\n\n2) \ud835\udc01-subproblem: When \ud835\udc03 is fixed, the problem (<ref>) can be written as\n\n    \ud835\udc01min <\u03a6,\ud835\udc01-\ud835\udc03>\n    +\u03c4/2||\ud835\udc01-\ud835\udc03||^2_F\n       s.t. 0\u2264\ud835\udc01\u2264\ud835\udc18, \ud835\udc011_c=1_n.\n\nLet b\u0302=[b_1^T;b_2^T;\u22ef;b_n^T]\u2208\u211d^nc=vec(\ud835\udc01), where b_i is the i-th row of \ud835\udc01 and vec(\u00b7) is the vectorization operator. Then, the problem (<ref>) is equivalent to:\n\n    b\u0302min b\u0302^T \u03a3b\u0302 - (2d\u0302^T+2/\u03c4\u03d5\u0302^T) b\u0302\n       s.t. 0_nc\u2264b\u0302\u2264\u0177, \u2211_j=c(i-1)+1^cib\u0302_j=1   (\u2200 0\u2264 i\u2264 n ),\n\nin which d\u0302, \u03d5\u0302 and \u0177 represent vec(\ud835\udc03), vec(\u03a6) and vec(\ud835\udc18), respectively, and b\u0302_j is the j-th element of b\u0302. Eq. (<ref>) is a quadratic programming (QP) problem that can be solved by off-the-shelf QP tools. \n\nThe detailed solution to update \ud835\udc03 in Eq. (<ref>) is summarized in Algorithm 1.\n\n\n\n  \u00a7.\u00a7.\u00a7 Initialization of W and D\n\nThe above alternative updating of \ud835\udc16 and \ud835\udc03 needs an initialization of them. As is suggested by <cit.>, we initialize \ud835\udc16 as an identity matrix and we initialize \ud835\udc03 by the pseudo label distribution matrix in <cit.>.\n\nThe pseudo code of DLDL is finally presented in Algorithm 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n \u00a7.\u00a7 Real-World Datasets\n\nWe select six real-world datasets from various fields for experiment, and samples in each dataset are annotated by LDs. The statistics of the six datasets are shown in Table <ref>. Natural Scene <cit.> is generated from the preference distribution of each scene image, SCUT-FBP <cit.> is a benchmark dataset for facial beauty perception, RAF-ML <cit.> is a multi-label facial expression dataset, SCUT-FBP5500 <cit.> is a big dataset for facial beauty prediction, Ren_CECps <cit.> is a Chinese emotion corpus of weblog articles, and Twitter_LDL <cit.> is a visual sentiment dataset.\n\nTo verify whether our method can directly learn an LDL model from the logical labels, we generate the logical labels from the ground-truth LDs. Specifically, when the description degree is higher than a predefined threshold \u03b4, we set the corresponding logical label to 1; otherwise, the corresponding logical label is 0. In this paper, \u03b4 is fixed to 0.01.\n\n\n\n\n\n \u00a7.\u00a7 Baselines and Settings\n\nIn this paper, we split each dataset into three subsets: training set (60%), validation set (20%) and testing set (20%). The training set is used for recovery experiments, that is, we perform LE methods to recover the LDs of training instances; the validation set is used to select the optimal hyper-parameters for each LE algorithms; the testing set is used for predictive experiments, that is, we use an LDL model learned from the training set with the recovered LDs to predict the LDs of testing instances.\n\nIn the recovery experiment, for our method DLDL, \u03b1 and \u03b3 are chosen among {10^-3,10^-2,\u22ef,10,10^2}, \u03b2 is selected from {10^-3,10^-2,\u22ef,1,10}, and the maximum of iterations t is fixed to 10. We compare DLDL with five state-of-the-art LE methods, each configured with suggested configurations in respective literature:\n\n\n  * FLE <cit.>: \u03b1, \u03b2, \u03bb, \u03b3 are chosen among {10^-3,10^-2,\u22ef,10,10^2}.\n\n  * GLLE <cit.>: \u03bb is chosen among {10^-3,10^-2,\u22ef,1}.\n\n  * LEMLL <cit.>: the number of neighbors K is set to 10 and \u03f5 is set to 0.2.\n\n  * LESC <cit.>: \u03b2 is set to 0.001 and \u03b3 is set to 1.\n\n  * FCM <cit.>: the number of clustering centers is set to 10 times of the number of labels.\n\n\nIn the predictive experiment, we apply BFGS-LLD <cit.> algorithm to predict the LDs of testing instances based on the recovered LDs by performing the five baseline LE methods. Note that our model can directly predict the LDs without an external LDL model.\n\nAs suggested in <cit.>, we adopt three distance metrics (i.e., Chebyshev, Clark and Canberra) and one similarity metric (i.e., Intersection) to evaluate the recovery performance and the predictive performance.\n\n\n\n\n\n\n \u00a7.\u00a7 Evaluation Metrics\n\nAs suggested in <cit.>, we adopt three distance metrics (i.e., Chebyshev, Clark and Canberra) and one similarity metric (i.e., Intersection) to evaluate the recovery performance and the predictive performance. The formulas of the four evaluation metrics are summarized in Table 2, where d denotes the ground-truth LD vector and d\u0302 denotes the recovered or predicted LD vector. In addition, Dis and Sim represent the distance metric and the similarity metric, respectively. The \u201c\u2193\" after the distance measures means \u201cthe smaller the greater\", while the \u201c\u2191\" means \u201cthe larger the greater\".\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Results\n\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Recovery Results\n\nIn this subsection, we calculate the distances and similarities between the ground-truth LD and the LD recovered by each LE method via four evaluation metrics on each dataset. Table <ref> shows the detailed recovery results, where all methods are ranked according to their performance and the best average rank on each metric is shown in boldface. In addition, the \u201c\u2193\" after the metrics means \u201cthe smaller the greater\", while the \u201c\u2191\" means \u201cthe larger the greater\". Based on the recovery results, we have the following observations:\n\n\n\n  * DLDL achieves the lowest average rank in terms of all the four evaluation metrics. To be specific, out of the 24 statistical comparisons (6 datasets \u00d7 4 evaluation metrics), DLDL ranks 1st in 79.2% cases and ranks 2nd in 16.7% cases, which clearly shows the superiority of our approach in LE.\n\n\n  * On some datasets, our method achieves superior performance. For example, on SCUT, REN and FBP, DLDL significantly improves the results compared with other algorithms in terms of the Chebyshev and Intersection.\n\n\n  * Although the average ranks of FLE and GLLE are low, they do not perform well on all datasets as DLDL does. Specifically, FLE performs poorly on SCUT and RAF, and GLLE performs poorly on NS, REN and Twitter. From this point of view, DLDL is more robust to different datasets than comparison algorithms.\n\nWe can also see that in the REN dataset, our algorithm has a significant improvement in the results against Chebyshev, Cosine and Intersection compared with other algorithms. This is probably because the ground-truth labels in this dataset is mostly zero, which fits well with the restriction to the LD in our algorithm.\n\n\nMoreover, Fig. <ref> shows the recovery results of a typical sample on the Twitter dataset. For this sample, the logical values corresponding to the label 1, 4, 5, 6, 8 are all 0, indicating that these labels cannot describe this sample. However, the five comparison methods still assign positive description degrees for these invalid labels. Differently, DLDL avoids this problem, and produces differentiated description degrees fairly close to the ground-truth values.\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Predictive Results\n\nIn this subsection, BFGS-LLD is used as the predictive model to generate the LDs of testing instances for the five comparison methods, while our model can directly predict the LDs of testing instances. Then, we rank all methods and highlight the best average rank on each metric in boldface. The detailed predictive results are presented in Table <ref>. In addition, BFGS-LLD is trained on the ground-truth LDs of the training instances and its results are recorded as the upper bound of the prediction results. From the reported results, we observe that:\n\n\n\n  * DLDL achieves the lowest average rank in terms of the three evaluation metrics (i.e., Clark, Canberra and Intersection). Specifically, out of the 24 statistical comparisons, DLDL ranks 1st in 62.5% cases and ranks 2nd in 8.33% cases. In general, DLDL performs better than most comparison algorithms.\n\n\n  * In some cases, the performance of our approach even exceeds the upper bound. For example, on SCUT dataset, DLDL performs better than the BFGS-LLD trained on the ground-truth LDs in terms of all the four evaluation metrics, which shows that our model has considerable potential in learning from logical labels.\n\nThis may be interpreted as that the alternative optimization of the weight matrix \ud835\udc16 and the label distribution matrix \ud835\udc03 can excavate some hidden information in the deep layers of the training instance space.\n\n\n\n\nIn Figure 3, we visualize one of the recovery results in the Twitter dataset. Because of the high quality of the recovered label distributions on the training datasets, the LDL results of DLDL is also excellent. So in Figure 6, it's obvious that the description degrees generated by DLDL is also similar to the ground-truth ones.\n\n\n\n\n \u00a7.\u00a7 Further Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Ablation Study\n\nIn order to verify the necessity of the involved terms of our approach, we conduct ablation experiments and present the results on Chebyshev in Tables <ref> and <ref>, where \u03b1, \u03b2 and \u03b3 are hyper-parameters to adjust the contributions of different terms. When one of hyper-parameters is fixed to 0, the remaining ones are determined by the performance on the validation set.\n\nFrom the results, we observe that the performance of DLDL becomes poor when the term controlled by \u03b2 is missing, indicating that it is critical to control the smoothness of the label distribution matrix \ud835\udc03. In general, DLDL outperforms its variants without some terms, which verify the effectiveness and rationality of our model.\n\n\n\n  \u00a7.\u00a7.\u00a7 Significance Test\n\n\n\nIn this subsection, we use the Bonferroni\u2013Dunn test at the 0.05 significance level to test whether DLDL achieves competitive performance compared to other algorithms. Specifically, we combine the recovery results with the predictive results to conduct the Bonferroni-Dunn test, that is, the number of algorithms is 6 and the number of datasets is considered as 12 (2 groups of experiments \u00d7 6 datasets). Then, we use DLDL as the control algorithm with a critical difference (CD) to correct for the mean level difference with the comparison algorithms.\n\nThe results are shown in Fig. <ref>, where the algorithms not connected with DLDL are considered to have significantly different performance from the control algorithms. It is impressive that DLDL achieves the lowest rank in terms of all evaluation metrics and is comparable to FLE, GLLE and LEMLL based on all evaluation metrics.\n\n\n\n\u00a7 CONCLUSION AND FUTURE WORK\n\nThis paper gives a preliminary positive answer to the question \u201ccan we directly train an LDL model from the logical labels\". Specifically, we propose a new algorithm called DLDL, which unifies the conventional label enhancement and label distribution learning into a joint model. Moreover, the proposed method avoids some common issues faced by the previous LE methods. Extensive experiments validate the advantage of DLDL against other LE algorithms in label enhancement, and also confirm the effectiveness of our method in directly training an LDL model from the logical labels. Nevertheless, our method is still inferior to the traditional LDL model when the ground-truth LD of the training set is available. In the future, we will explore possible ways to improve the predictive performance of our algorithm. \n\n\n\nnamed\n\n\n"}