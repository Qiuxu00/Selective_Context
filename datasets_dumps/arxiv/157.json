{"entry_id": "http://arxiv.org/abs/2303.07137v1", "published": "20230313140514", "title": "Convergence proof for the GenCol algorithm in the case of two-marginal optimal transport", "authors": ["Gero Friesecke", "Maximilian Penka"], "primary_category": "math.NA", "categories": ["math.NA", "cs.NA", "math.OC"], "text": "\nTransferable Deep Learning Power System Short-Term Voltage Stability Assessment with Physics-Informed Topological Feature Engineering\n    Zijian\u00a0Feng, Xin\u00a0Chen*, Zijian\u00a0Lv, Peiyuan\u00a0Sun, Kai\u00a0Wu,\n\tThis work was supported in part by the National Natural Science Foundation of China (Grant No.21773182 (B030103)) and the HPC Platform, Xi'an Jiaotong University.\n\tZijian\u00a0Feng (e-mail: KenFeng@stu.xjtu.edu.cn),  Xin Chen (Corresponding author, e-mail: xin.chen.nj@xjtu.edu.cn), and Zijian\u00a0Lv (e-mail: jtlzj271828@stu.xjtu.edu.cn) are with the School of Electrical Engineering and the Center of Nanomaterials for Renewable Energy, State Key Laboratory of Electrical Insulation and Power Equipment, School of Electrical Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China.\n\tPeiyuan\u00a0Sun (e-mail:spy2018@stu.xjtu.edu.cn) is with the School of Electrical Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China.\n\tKai\u00a0Wu (e-mail: wukai@mail.xjtu.edu.cn.) is with State Key Laboratory of Electrical Insulation and Power Equipment, School of Electrical Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China.\n\n    March 30, 2023\n=========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n \nThe recently introduced Genetic Column Generation (GenCol) algorithm has been numerically observed to efficiently and accurately compute high-dimensional optimal transport plans for general multi-marginal problems, but theoretical results on the algorithm have hitherto been lacking. The algorithm solves the OT linear program on a dynamically updated low-dimensional submanifold consisting of sparse plans. The submanifold dimension exceeds the sparse support of optimal plans only by a fixed factor \u03b2. Here we prove that for \u03b2\u2265 2 and in the two-marginal case, GenCol always converges to an exact solution, for arbitrary costs and marginals. The proof relies on the concept of c-cyclical monotonicity. As an offshoot, GenCol rigorously reduces the data complexity of numerically solving two-marginal OT problems from O(\u2113^2) to O(\u2113) without any loss in accuracy, where \u2113 is the number of discretization points for a single marginal.\n\n\n\n\n\u00a7 INTRODUCTION\n\nLarge-scale discrete optimal transport problems are difficult to solve numerically because the full problem has a huge number of possible configurations. At the same time it is guaranteed that a rather sparse solution exists, a particularly well known manifestation in continuous OT being Brenier's theorem. This effect is especially important in the multi-marginal case, but occurs already in the classical two-marginal case when the support size of both marginals is large.\n\nIn recent years, computational strategies for optimal transport were driven by the idea of approximating the problem by adding an entropy-like penalty term. This transforms the problem into a strictly convex and more robust optimization problem, which can be solved in short time using the Sinkhorn algorithm. In theory this approach, called entropic optimal transport (EOT), is also valid for two-marginal problems in high dimension or general multi-marginal problem.\n\nUnfortunately, this approach corresponds to smearing out the transport plan, yielding a huge amount of configurations in its support: The true optimizer of the EOT problem has the same support as the full product measure of its marginals. Thus the support size scales polynomially in the support size of the marginals, and exponentially in the number of marginals. Recent approaches alleviate this problem by truncation or multi-scale methods <cit.> or \u2013 most recently \u2013 low-rank approximation <cit.>.\n\nAn alternative approach was proposed in <cit.>. Rather than looking for further refinements of interior point methods, which struggle to solve huge programs, one goes back to the original linear program, and exploits that the OT program possess extremely sparse solutions. If the k-th marginal is supported on \u2113_k points, there exist optimal plans with support size less than the sum of the \u2113_k, rather than their product <cit.>. \n\nThe standard approach to solve linear programs is the simplex algorithm and its descendants, most promisingly Column Generation. The latter tackles large LPs by iteratively solving smaller (\u201creduced\u201d) problems on a dynamically evolving subset of all variables. In optimal transport, every variable of the LP corresponds to a possible configuration in the product of the marginal domains, so Column Generation corresponds to solving the OT problem on a subset of the product of the marginal domains. Thus Column Generation can in principle exploit sparsity and  find the exact optimal plan of the full problem, provided a sparse superset of its support is known. But in practice such a superset has to be found.  \n\nThere are two obstructions for doing so. First, in Column Generation the generation of new variables is originally done by solving a second optimization problem, the so called pricing problem. Unfortunately the pricing problem for large problems is still expensive; in the multi-marginal case it has been proven to be NP hard <cit.>. For LPs of moderate size this problem can be \nalleviated by generating new configurations at random, but in this randomized version one needs to try many configurations, again leading to an unacceptable slowdown for large problems. \nSecond, the algorithm increases the size of the LP in each iteration step, making the iterations slower and slower and lacking any convergence guarantee until the size has reached the original LP size that one wanted to avoid! \n\nFor these reasons, <cit.> invented \n\n\n    \n  * a genetic search rule, restricting the number of possible proposals from all configurations to an update of one entry in one active configuration\n    \n  * a tail-clearing rule which discards those configurations which have not been used the longest, to keep the overall support size at a fixed small multiple of the size of sparse optimizers.\n\n\nThe resulting algorithm, which was termed Genetic Column Generation (GenCol), exhibited in several application examples of interest \na spectacularly accelerated convergence to global optimizers. A theoretical explanation has hitherto been lacking.\n\nIn this paper we present a rigorous proof of convergence to a global optimizer in the case of two marginals.\nThe fact that GenCol cannot get stuck in a local optimizer is far from obvious since the upper bound on the support size makes the reduced problem nonconvex.\nThe proof relies on the concept of c-cyclical  monotonicity which is well known in the theory of optimal transport. It finds here a beautiful application and yields an intuitive understanding of the algorithm. \n\nWe close with a counterexample which shows that for more than two marginals and general costs, the algorithm \u2013 at least with its original one-entry-at-a-time updating rule \u2013 can get stuck in a local minimizer. The investigation of sufficient criteria on costs and updating rules for convergence in the multi-marginal case is left to future work.  \n\n\n\n\n\n\n\u00a7 C-CYCLICAL MONOTONICITY\n\nGiven two probability measures \u03bc_1,\u03bc_2 on Polish spaces X respectively Y, the optimal transport problem is the following:\n\n    minimize     \u2131[\u03b3] := \u222b_X\u00d7 Y c(x,y) d\u03b3(x,y)  over \u03b3\u2208(X\u00d7 Y)\n    subject to     \u03b3(A\u00d7 Y) = \u03bc_1(A)    for all measurable  A \u2282 X\n    \u03b3(X\u00d7 B) = \u03bc_2(B)    for all measurable  B \u2282 Y.\n\nwhere  denotes the set of probability measures. Solutions to the constraints are called transport plans. Optimality of a transport plan \u03b3 can be characterized by a condition on its support, called c-cyclical monotonicity.\n\n[see e.g. <cit.>, Def. 1.36]\nGiven a function c X\u00d7 Y \u2192\u222a{+\u221e}, we say that a set \u0393\u2282 X \u00d7 Y  is c-cyclically monotone (c-CM) if for every k\u2208, every permutation \u03c3: {1,\u2026,k}\u2192{1,\u2026,k}, and every finite set of points {(x_1,y_1),...,(x_k,y_k)}\u2282\u0393 we have\n\n    \u2211_i=1^k c(x_i,y_i) \u2264\u2211_i=1^k c(x_i,y_\u03c3(i)).\n\n\n\nWhile it is easy to see (at least in the discrete case) that this is a necessary condition on the support of an optimal plan, it turns out to also be sufficient.\n\n \nSuppose X and Y are Polish spaces and c X\u00d7 Y \u2192 is uniformly continuous and bounded. Given \u03b3\u2208\ud835\udcab(X\u00d7 Y), if supp(\u03b3) is c-CM then \u03b3 is an optimal transport plan between its marginals \u03bc_1 = (\u03c0_1)_\u266f\u03b3 and \u03bc_2 = (\u03c0_2)_\u266f\u03b3 for the cost c.\n\n\n\n\n\n\n\n\u00a7 SPARSITY OF OPTIMAL PLANS\n\nThe support of optimal transport plans is typically a much smaller set than the product of the supports of the marginals. Rather than going into classical variants for convex costs like Brenier's theorem and their interesting relation to c-cyclical monotonicity, we focus directly on a discrete version for general costs which informed the design of the GenCol algorithm and is useful for its analysis. \n\nFor X and Y discrete, |X| = \u2113_1, |Y| = \u2113_2, the objective function \u2131 becomes a finite sum and the OT problem a linear program in standard form:\n\n    OTminimize     \u27e8 c,\u03b3\u27e9 := \u2211_(x,y) \u2208 X\u00d7 Y c(x,y)\u03b3(x,y)  over \u03b3: X\u00d7 Y \u2192 [0,\u221e)\n    subject to     \u03b3\u2208\u03a0(\u03bc_1,\u03bc_2) :\u21d4\u2211_y \u2208 Y\u03b3(x_0,y) = \u03bc_1(x_0) \u2200 x_0 \u2208 X \n    \u2211_x \u2208 X\u03b3(x,y_0) = \u03bc_2(y_0) \u2200 y_0 \u2208 Y,\n\nwhere the measures \u03bc_1,\u03bc_2 and \u03b3 were identified with their densities with respect to the counting measures on their domains.\n\n  Suppose X and Y are discrete with |X|=\u2113_1, |Y|=\u2113_2. Then any extreme point of the Kantorovich polytope \u03a0(\u03bc_1,\u03bc_2) is supported on at most \u2113_1+\u2113_2-1 points. In particular, for any cost c  :   X\u00d7 Y\u2192 and any marginals, the OT problem (<ref>) possesses an optimizer supported on at most \u2113_1+\u2113_2-1 points. \n\n\nThis can be deduced from well known results on extremal solutions in linear programming. For a self-contained and simple proof using geometry of convex polytopes see <cit.>.\n\n\n\n\n\n\n\u00a7 GENETIC COLUMN GENERATION ALGORITHM\n\n\nThe algorithm doesn't deal with the full OT problem but only its restrictions to certain subsets of X\u00d7 Y whose size is of the order of the support size of optimizers from Theorem <ref>.  \n\nWe call a subset \u03a9\u2282  X\u00d7 Y a feasible subset of configurations if \u03a0(\u03bc,\u03bd) \u2229{\u03b3 : (\u03b3) \u2282\u03a9} is non-empty. Given such a subset, we define the reduced problem to be\n\n    ROTminimize     \u27e8 c,\u03b3\u27e9 over \u03b3: X\u00d7 Y \u2192 [0,\u221e)\n    subject to     \u03b3\u2208\u03a0(\u03bc_1,\u03bc_2) \n        and (\u03b3)\u2286\u03a9.\n\nBecause \u03b3 is a discrete measure, (\u03b3) is the set of all (x,y)\u2208 X\u00d7 Y with \u03b3(x,y)\u2260 0. Thus the reduced problem amounts to reducing the variables in the linear program to the values of \u03b3 on configurations in \u03a9 (and setting the values outside \u03a9 to zero), and not changing the constraints. As the values outside \u03a9 no longer need to be considered, this shrinks the size of the program to that of \u03a9.\n\nBefore we come to genetic column generation, let us describe  classical column generation. Unlike genetic column generation it does not restrict the size of \u03a9, and works as follows. Given a feasible initial set \u03a9, the first step is to solve the reduced problem. The second step is to generate a new configuration (x',y') \u2209\u03a9 which is added to \u03a9 and improves the solution. The two steps are iterated until no more improving configurations exist.\n\nThe second step relies on the dual of the reduced problem (<ref>), \n\n    D-ROT*maximize     \u27e8\u03bc_1,u_1\u27e9 + \u27e8\u03bc_2,u_2\u27e9 over  u_1 : X \u2192, u_2 : Y \u2192\n    such that     u_1(x) + u_2(y) \u2264 c(x,y)   \u2200 (x,y) \u2208\u03a9.\n\nIn comparison, the dual of the full problem (<ref>) has the same objective function, but more constraints:\n\n    D-OT*maximize     \u27e8\u03bc_1,u_1\u27e9 + \u27e8\u03bc_2,u_2\u27e9 over  u_1 : X \u2192, u_2 : Y \u2192\n    such that     u_1(x) + u_2(y) \u2264 c(x,y)   \u2200 (x,y) \u2208 X\u00d7 Y.\n\nHence every dual optimizer for the full problem is admissible in the reduced problem (<ref>), but a dual optimizer for the reduced problem might violate a constraint of the full problem (<ref>). If, however, a dual optimizer for the reduced problem is admissible for  (<ref>) then it is already optimal for (<ref>):\n\n    Let (\u03b3^\u22c6,(u_1^\u22c6,u_2^\u22c6)) be a pair of optimizers for the reduced problems (<ref>,\u00a0<ref>). If (u_1^\u22c6,u_2^\u22c6) is admissible for the dual of the full problem (<ref>),\n    then \u03b3^\u22c6 is optimal for (<ref>).\n\nFor a proof of this classical result translated into the present context and language of OT see <cit.>.\nHence new configurations (x',y')\u2209\u03a9 can be sought by checking if they violate the dual constraint of the full problem (<ref>), i.e. if they satisfy the following acceptance criterion:\n\n    Acc\n    u_1^\u22c6(x') + u_2^\u22c6(y') - c(x',y') > 0.\n\nDue to economic interpretations this difference is called gain. In classical column generation this gain is maximized over all configurations, constituting the  so-called pricing problem. \n\n\nThe following difficulties arise when applying column generation to large LPs, as already pointed out in the Introduction. (i) The pricing problem is too expensive; and the empirical strategy of instead generating configurations (x',y')\u2209\u03a9 independently at random until one of them satisfies (<ref>) requires too many trials, especially in the multi-marginal case. (ii) Regardless of how one searches for new configurations, the subset \u03a9 \ngrows in each iteration step, making the iterations slower and slower and lacking any convergence guarantee until the size\nhas reached the original LP size that one wanted to avoid.\n\nGenetic column generation <cit.>\n tackles these difficulties as follows.\n \n\n\n(i) Motivated by machine learning protocols in unsupervised learning, the algorithm first proposes new configurations originating from currently active configurations, i.e. (x,y) \u2208supp(\u03b3) \u2282\u03a9: one picks an active configuration at random (\u201cparent\u201d), then proposes an offspring (\u201cchild\u201d) by changing one entry of the parent configuration. \nThe offspring is then accepted if its gain is positive. The rough analogy to ML is that the proposal step mimics an SGD step and the acceptance mimics learning from an adversary (in this case, the current dual). In fact, the proposal step in the first version of GenCol was even more similar to SGD, in that entries were points on a regular grid and children were proposed from neighbouring sites of parents.\n\n(ii) The size of \u03a9 is restricted to remain of the order of the support size of optimizers from Theorem <ref>. More precisely, one introduces a  hyperparameter \u03b2 > 1 and a tail clearing rule which guarantees that \n\n    |\u03a9| \u2264\u03b2 (\u2113_1+\u2113_2).\n\nTail clearing means that whenever, after accepting a child, \u03a9 violates (<ref>), the oldest unused configurations are removed. In practice, one chooses 3 \u2272\u03b2\u2272 5 and discards a batch of \u2113_1+\u2113_2 configurations whenever |\u03a9| exceeds \u03b2\u00b7(\u2113_1+\u2113_2). \nThe hyperparameter \u03b2 does not depend on the sizes \u2113_1 and \u2113_2 of X and Y. \n\nSee Algorithm <ref> for a summary of the algorithm. \n\n\n\nThe extension to multi-marginal problems is straightforward. In this case, one has N marginals \u03bc_1,\u2026,\u03bc_N on discrete spaces X_1,\u2026,X_N of sizes \u2113_1,\u2026, \u2113_N, plans are nonnegative functions on the product space X_1\u00d7\u2026\u00d7 X_N to [0,\u221e), and one seeks to  \n\n    MMOTminimize     \u27e8 c,\u03b3\u27e9 := \u2211_(x_1,...,x_N) \u2208 X_1\u00d7 ... \u00d7 X_N c(x_1,...,x_N)\u03b3(x_1,...,x_N)\n    subject to     \u03b3\u2208\u03a0(\u03bc_1,...,\u03bc_N).\n\nThis problem possesses an optimizer \u03b3  :   X_1\u00d7 ... \u00d7 X_N \u2192 supported on at most \u2113_1+...+\u2113_N-1 points. Sampling of new configurations works as before, i.e. one picks an active configuration (parent) and proposes a child by changing one entry of the parent, which is accepted if the gain \u2211_i=1^N u_i(x_i)-c(x_1,...,x_N) is positive, where u=(u_1,...,u_N), u_i   :   X_i\u2192, is the current dual solution. Tail clearing is carried out whenever |\u03a9| exceeds \u03b2\u00b7 (\u2113_1+...+\u2113_N). See <cit.> for the details.\n\nIn practice this turned out to be an extremely fast and accurate method to solve high-dimensional OT problems. In various test examples with up to \u223c 10^30 variables, it converged to a global optimum of the full problem using active sets of only a few thousand unknowns. Besides the question why the algorithm is so efficient, it is not clear why it should find a global optimum at all. Can it happen \u2013 due to the genetic updating and the tail clearing \u2013 that it instead gets stuck in a local minimum? \n\nThe answer is No in the two-marginal case, as shown in the next section. Note that in this case every configuration (x',y') \u2209\u03a9 which belongs to the product of the supports of \u03bc_1 and \u03bc_2 is proposed by Algorithm <ref> with strictly positive probability, so the only question is if the tail-clearing prevents the algorithm from finding the optimal set of configurations.\n\n\n\n\n\n\n\u00a7 CONVERGENCE\n\n\nBefore giving the proof of convergence, we must specify line 2 (solving the reduced problem and its dual) and line 4 (sample a parent and a child) of Algorithm <ref> more precisely. \n\nLine 2. First, in degenerate cases optimal plans may not be unique, so for convergence it is mandatory that \u03b3^* in Algorithm <ref> is updated only if the previous plan is no longer minimizing. Second, we require the linear programming solver to provide \na solution \u03b3^* which satisfies the support size bound from Theorem <ref>. If one uses the simplex algorithm with a warm start, both these requirements are automatically guaranteed. \n\nLine 4. Second, to rigorously implement the second stopping criterion in line 5, one does not sample parents and children in each trial independently, but draws random permutations covering all possibilities  and then tries them one after another until the stopping criterion is satisfied. \n\n  Let X and Y be discrete with |X|=\u2113_1, |Y|=\u2113_2, let c  :   X\u00d7 Y\u2192 be any cost, let \u03bc_1\u2208(X), \u03bc_2\u2208(Y) be any marginals, and let \u03a9\u2282 X \u00d7 Y be any feasible subset of configurations. For any optimal solution \u03b3^\u22c6 for the reduced problem (<ref>) which is an extreme point of the Kantorovich polytope and which is not optimal for the full problem (<ref>), the GenCol proposal and acceptance routine (lines 2\u20139 of Algorithm <ref>) as detailed above finds with positive probability in subsequent steps a superset \u03a9\u0303\u2283\u03a9, whose size exceeds that of \u03a9 by at most \u2113_1+\u2113_2-1 elements, which reduces the total cost:\n    \n    min_\u03b3  :  (\u03b3) \u2286\u03a9\u0303\u2131[\u03b3] < min_\u03b3  :  (\u03b3) \u2286\u03a9\u2131[\u03b3].\n\n \n\n\nConsider an optimal solution (\u03b3^0,u^0) for the reduced problem (<ref>) with \u03b3^0 extremal which is not optimal for the full problem. By Theorem <ref> \u03b3^0 is sparse with at most \u2113_1+\u2113_2-1 non-zero entries (active configurations). In the following we write u^0=(u^0_1,u^0_2). Due to complementary slackness\n\n    u^0_1(x) \n       + u^0_2(y) - c(x,y) = 0   \u2200 (x,y) \u2208(\u03b3^0).\n \nBecause \u03b3^0 is not optimal for the full problem (<ref>), by Theorem <ref> there exists a family \u0393 = {(x_1,y_1),...,(x_k,y_k)}\u2282(\u03b3^0) and a permutation \u03c3\u2208 S_k such that\n\n    \u2211_i=1^k c(x_i,y_i) > \u2211_i=1^k c(x_i,y_\u03c3(i)).\n\nNote that the bound |(\u03b3)| \u2264\u2113_1+\u2113_2-1 yields the upper bound k \u2264\u2113_1+\u2113_2-1.\nBecause {(x_1,y_1),...,(x_k,y_k)}\u2282(\u03b3^0), complementary slackness implies\n\n    equation1\n            u^0_1(x_1) + u^0_2(y_1) - c(x_1,y_1) = 0 equation.1\n    \u22ee\n    \n            u^0_1(x_k) + u^0_2(y_k) - c(x_k,y_k) = 0equation.k.\n\nAfter summation,\n\n    \u2211_i=1^k (u^0_1(x_i) + u^0_2(y_i)) - \u2211_i=1^kc(x_i,y_i)_> \u2211_i=1^k c(x_i,y_\u03c3(i)) = 0 \n    \u27f9      \u2211_i=1^k( u^0_1(x_i) + u^0_2(y_\u03c3(i)) - c(x_i,y_\u03c3(i))) > 0 \n    \u27f9     max_i\u2208{1,...,k}{u^0_1(x_i) + u^0_2(y_\u03c3(i)) - c(x_i,y_\u03c3(i)) } > 0 .\n\n\nBecause, in the two-marginal case, all configurations (x',y')\u2209\u03a9 are proposed by GenCol with positive probability, the element of \u0393 where the maximum in (<ref>) is realized, let us call it  (x_i_1,y_\u03c3(i_1)), is proposed with positive probability, and accepted. In the next step the reduced OT problem is resolved on \u03a9 extended by this element, yielding a new optimal pair (\u03b3^1,(u_1^1,u_2^1)) and two cases.\n\nCase 1: The optimal plan changes: \u03b3^1 \u2260\u03b3^0. In that case, due to the rule that the plan only changes when it must, the optimal cost decreases and we are done. \n\nCase 2: The optimal plan does not change, \u03b3^1 = \u03b3^0. But the dual solution (u^1_1,u^1_2) must have changed. Because \u03b3^1 = \u03b3^0, we have (x_1,y_1),\u2026,(x_k,y_k) \u2208(\u03b3^1) and eqs. (<ref>) - (<ref>) still hold true with u^0_1, u^0_2 replaced by u^1_1, u^1_2. \nBut now, since u^1 must satisfy the dual constraints on the enlarged configuration set, we also have\n\n    u^1_1(x_i_1) +  u^1_2(y_\u03c3(i_1)) - c(x_i_1,y_\u03c3(i_1)) \u2264 0.\n\nSince eqs. (<ref>)\u2013(<ref>) all are also still true with u^0_1, u^0_2 replaced by u^1_1, u^1_2, we conclude that \n\n    max_i\u2208{1,...,k}\\{i_1}{u^1_1(x_i) + u^1_2(y_\u03c3(i)) - c(x_i,y_\u03c3(i))} > 0.\n\nBut in the next step, again either the optimal plan changes or the element of \u0393 realizing the maximum in (<ref>) is proposed with positive probability and accepted, and so on. After k enlargement steps of \u03a9, either a change of optimal plan has occurred in some step, or all elements of \u0393 have been accepted with positive probability. But then Case 1 occurs, since the mass min{\u03b3^0(x_i,y_i), i = 1,\u2026,k} > 0 can be moved from {(x_i,y_i)}_i=1^k to {(x_i,y_\u03c3(i)}_i=1^k, decreasing the total cost.\n\n\nOne can alternatively see that Case 1 occurs once all elements of \u0393 have been accepted by considering the dual solution u^k: otherwise we would have\n\n    u_1^k(x_i) + u_2^k(y_\u03c3(i)) - c(x_i,y_\u03c3(i)) \u2264 0  \u2200 i\u2208{1,...,k},\n\nbut on the other hand (<ref>) must hold with u^0_1, u^0_2 replaced by u^k_1, u^k_2, a contradiction.\n\nConvergence of Algorithm <ref> now follows as an easy consequence.\n\n Suppose X and Y are discrete spaces of finite cardinality, and the hyperparameter \u03b2 is \u2265 2. For any marginals, any cost function, and any feasible initial set \u03a9\u2282 X\u00d7 Y, GenCol converges with probability 1 to an exact solution of the OT problem (<ref>). \n\n\n\n    The total cost is monotonically decreasing. Moreover X\u00d7 Y is finite and every non-optimal plan is improved with positive probability:  Since \u03b2\u2265 2, after a tail-clearing the algorithm allows to add more than \u2113_1+\u2113_2-1  configurations, and by Theorem <ref> this suffices to  find a plan with lower cost. Therefore the algorithm converges with probability 1.\n \nRemarks. \n\n    \n  * Note the generality of the cost function.\n    \n  * Shorter families are found with higher probability.\n    \n  * Longer tails increase the probability to find also long chains, but slow down the simplex algorithm to solve the LP. In practice, a value slightly larger than the minimal value from theory (e.g. \u03b2=3)  works well, and was in fact used on empirical grounds in <cit.>.\n\n\n\n\n\n\u00a7 COUNTEREXAMPLE FOR THREE MARGINALS\n\nThe proof in the previous section cannot simply be transferred to the multi-marginal case, at least not for the original updating rule proposed in <cit.> where children differ from an active configuration by only 1 entry. We present a simple counterexample. Let X_1=X_2=X_3 = {1,2,3} and N=3. Let further \n\n    \u03bc_1 = \u03bc_2 = \u03bc_3 := \u2211_x=1^31/3\u03b4_x.\n\nThe cost function is chosen to be\n\n    c(x_1,x_2,x_3) := \n        \n        0     x_1 = x_2 = x_3\n    \n        1     x_1 \u2260 x_2  x_1 \u2260 x_3  x_2 \u2260 x_3\n    \n        2    else.\n\nThen the transport plan\n\n    \u03b3_0 = 1/3(\u03b4_(1,2,3)+\u03b4_(2,3,1)+\u03b4_(3,1,2))\n\nis a stationary state for GenCol. Obviously the global optimal plan is\n\n    \u03b3^\u22c6 = 1/3(\u03b4_(1,1,1)+\u03b4_(2,2,2)+\u03b4_(3,3,3)).\n\nGenCol proposes new configurations by updating one entry of one active configuration. Independently of the dual solution, all possible configurations that can be  proposed are\n\n    (1,2,2), (1,3,3), (2,2,3), (3,2,3), (2,1,1), (2,2,1), (2,3,2), (2,3,3), \n    \n        (1,3,1), (3,3,1), (3,2,2), (3,3,2), (3,1,1), (3,1,3), (1,1,2), (2,1,2).\n\nThe cost for all of them is 2, while the cost for all active configurations in \u03b3_0 is only 1. Hence for any subset of the configurations listed above added to the active configurations in \u03b3_0, the optimal plan is again \u03b3_0. Therefore the current solution \u03b3_0 never changes and \u03b3^* cannot be reached.\n\nThe example is designed so that one would have to update two entries of an active configuration to reduce the cost. Any update in just one entry increases the cost. \n\nSome interesting properties of this example are: \n\n\n\n\n \n    \n  * The problem is symmetric (i.e., all marginals are equal and the cost is symmetric in its variables), like the Coulomb OT problem arising in electronic structure.\n    \n  * One can replace {1,2,3} by a convex independent set (i.e. a set all of whose points are extreme points), in which case the cost c can even be chosen convex.\n\n\nIt is an interesting open problem whether modified updating rules exist for the multi-marginal case that maintain sparsity yet rigorously avoid non-minimizing stationary states, at least for costs of practical interest like the Coulomb cost or the Wasserstein barycenter cost. \n\n\n\n"}