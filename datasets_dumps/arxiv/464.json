{"entry_id": "http://arxiv.org/abs/2303.06670v1", "published": "20230312142410", "title": "DINO-MC: Self-supervised Contrastive Learning for Remote Sensing Imagery with Multi-sized Local Crops", "authors": ["Xinye Wanyan", "Sachith Seneviratne", "Shuchang Shen", "Michael Kirley"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\nDINO-MC: Self-supervised Contrastive Learning for Remote Sensing Imagery with Multi-sized Local Crops\n    Xinye Wanyan\n\n\n\nxwanyan@student.unimelb.edu.au\n\n\n\n\n\nSachith Seneviratne\n\n\nsachith.seneviratne@unimelb.edu.au\nShuchang Shen \n\n\nchuchangs@student.unimelb.edu.au\nMichael Kirley \n\n\n\nmkirley@unimelb.edu.au\n\n    March 30, 2023\n=============================================================================================================================================================================================================\n\n\n\n\n\n\nDue to the costly nature of remote sensing image labeling and the large volume of available unlabeled imagery, self-supervised methods that can learn feature representations without manual annotation have received great attention.\nWhile prior works have explored self-supervised learning in remote sensing tasks, pretext tasks based on local-global view alignment remain underexplored.\nInspired by DINO <cit.>, which employs an effective representation learning structure with knowledge distillation based on global-local view alignment, we formulate two pretext tasks for use in self-supervised learning on remote sensing imagery (SSLRS).\nUsing these tasks, we explore the effectiveness of positive temporal contrast as well as multi-sized views on SSLRS. \nMoreover, we extend DINO and propose DINO-MC which uses local views of various sized crops instead of a single fixed size.\nOur experiments demonstrate that even when pre-trained on only 10% of the dataset, DINO-MC performs on par or better than existing state of the art SSLRS methods on multiple remote sensing tasks, while using less computational resources.\nAll codes, models and results are available at \n<https://github.com/WennyXY/DINO-MC>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nComputer vision models have been widely used in remote sensing to solve numerous real-world challenges, including disaster prevention <cit.>, forestry <cit.>, agriculture <cit.>, land surface change <cit.>, biodiversity <cit.>.\nGenerally, the models with deeper and more complex structures are able to extract more useful features from images, leading to superior performance in a wide range of applications.\nHowever, training large-scale computer vision models in a traditional supervised manner always requires large labeled datasets, which are costly and error-prone, especially in remote sensing domain <cit.>.\nTherefore, reducing the reliance of the model on labeled images is crucial for resolving specific downstream tasks.\nThe self-supervised learning (SSL) paradigm is a common solution which is able to train a general feature extraction model on unlabeled datasets.\nSSL is conducted in two phases: first, the model is pre-trained to learn general latent representations by solving complex pretext tasks. Then, the pre-trained model is fine-tuned on downstream tasks.\nSince different pretext tasks make the model learn different aspects of features, designing a suitable task whose labels can be automatically obtained from the data is one of the keys to SSL.\n\nAccording to the pretext tasks, SSL models can be classified into three categories.\nGenerative tasks allow the model learn feature representations by reconstructing or generating original images.\nFor example, the image inpainting <cit.> uses the original data as labels to train the model to recover several masked parts of the original image.\nThe discriminative task <cit.> trains the network to distinguish a set of categories.\nFor example, <cit.> trains the model to predict the relative position of a patch to its neighbors.\nIt is an eight-label classification task in which each image patch is defined to have up to eight adjacent patches.\nHowever, the feature representations learned by generative and discriminative methods are highly dependent on the used pretext task, and an ineffective pretext task might reduce the transfer-ability of a pre-trained model <cit.>.\nIn stead of solving a single pretext task, contrastive approaches train models by maximising the similarity between the feature representations of two positive samples, e.g., two augmented views of the same instance. \nHowever, simply following this approach will easily generate an identity map for each pair of positive samples, i.e., model collapse <cit.>.\n\nDINO <cit.>, a state-of-the-art contrastive self-supervised model, utilizes knowledge distillation and centering\nand sharpening of the teacher network <cit.> to handle this issue.\nDINO has shown impressive performance in numerous computer vision tasks, including image retrieval, copy detection and video instance segmentation. \nWhile some previous work has applied DINO to remote sensing domain <cit.>, a thorough evaluation and extension of the self-supervised objective for remote sensing imagery has not attempted. \nIn particular, the size of instances observed during pre-training on traditional natural scenes shows wider variation than seen in remote sensing imagery. \nHence, the alignment of the latent representation of the multi-sized local augmented crops against the global augmented views is of interest in remote sensing imagery as it leads to a more challening pretext task.\n\n\nIn SeCo <cit.>, the temporal information plays a crucial role in learning transferable features from remote sensing imagery, building primarily on the contrastive model MoCo-V2 <cit.>. \n\nIn this work, we explore whether using temporal views as positive instances can further improve the performance of DINO in remote sensing domain.\nFurthermore, inspired by the inherent characteristics of the size variation of semantic content observed between traditional natural scenes and remote sensing imagery, we propose DINO-MC which uses size variation in local crops to drive better representation learning of the semantic content of remote sensing imagery.\n\nWe evaluate our representations on different backbones including two transformer networks and two convnets.\nWhile most existing self-supervised methods for remote sensing employ ResNet and Vision Transformers (ViTs) as backbone models, the self-supervised feature extraction potential of Wide ResNets (WRN) and Swin Transformers is of particular interest in remote sensing, and we thus include them in our analysis.\n\nIn the linear probing evaluation, the findings demonstrate that DINO-MC has great transfer-ability, as it achieves 2.56% higher accuracy with a smaller pre-trained dataset than SeCo.\nBesides, DINO-MC outperforms DINO and SeCo when fine-tuned on two remote sensing classification tasks as well as on change detection (a segmentation task).\n\nIn conclusion, our main contributions are summarized as follows:\n\n\n  \u2219 We apply temporal views as positive instances to recent contrastive self-supervised models (DINO-TP). We analyze different backbone networks to explore their effectiveness on different remote sensing tasks when pretrained under this setting.\n\n  \u2219 We combine a new multi-sized local cropping strategy with DINO and propose DINO-MC. We pre-train DINO-MC with different backbones on a satellite imagery dataset SeCo-100K to learn a general representations.\n\n  \u2219 DINO-MC outperforms SeCo with only 10% pre-training dataset, and achieves state-of-the-art results on BigEarthNet multi-label and EuroSAT multi-class land use classification, as well as OSCD change detection task.\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n \u00a7.\u00a7 Contrastive Self-supervised Learning.\n\n\n\n\nThe self-supervised pre-training aims to learn a general features which can generalise well to different downstream tasks.\nAs a branch of self-supervised methods, contrastive learning has recently gained attention and shown promise in performance. It learns feature representations by maximizing the similarity of two positive instances and the distance between positive and negative instances.\nIn generative and discriminative models, the encoder is trained to learn feature representations by optimizing the loss function calculated on the ground truth and the prediction, while the loss of contrastive model is calculated in the latent space <cit.>.\n\n\n\nInstance discrimination <cit.> is a simple but effective pretext task.\nIt is widely used in contrastive learning which sets augmented views of the same input as positive samples and different instances as negative samples, and then trains the model to keep positive pairs close and negative pairs far in the representation space <cit.>.\n\n\n\n<cit.> proposes Momentum Contrast model (MoCo-V1) with a momentum encoder to effectively maintain multiple negative samples for representation learning.\nSimCLR <cit.> employs very large batch sizes instead of momentum encoders, and provides experimental results on 10 forms of data augmentation.\nInspired by SimCLR, MoCo-V1 is extended to MoCo-V2 <cit.>, and then MoCo-V3 <cit.> is proposed by applying ViT as the backbone.\n\n\nClustering method is another line of contrastive learning, which demands no precise indication from the inputs <cit.>. \n\n\n<cit.> introduces a contrasting cluster assignment that learns features by Swapping Assignments between multiple Views of the same image (SwAV).\nSwAV is easily applicable to different sizes of datasets since it employs clustering instead of pairwise comparisons.\nFurthermore, <cit.> proposes a new data augmentation named multi-crop to increase the instances without drastically additional memory and computation.  \nSpecifically, when doing multi-crop, images will be cropped to a collection of views with lower resolutions instead of the full-resolution views.\nMulti-crop shows both efficiency and effectiveness since several self-supervised models <cit.> perform well with it.\n\n\n\n\n\n\nKnowledge distillation can be used in contrastive learning for feature extraction without separating between images <cit.>.\n<cit.> proposes Bootstrap Your Own Latent (BYOL) based on the online and target networks whose weights are updated with each other.\nBYOL experiments with ResNet of different sizes.\n\nInspired by BYOL, <cit.> explores the further synergy between SSL and different backbones, especially ViTs, and proposes a simple form of self-distillation with no labels (DINO).\nDINO uses the same two networks architecture as BYOL but with different loss function and backbone models.\nWhen pre-trained on ImageNet <cit.>, DINO achieves better linear and KNN probing evaluation results than other self-supervised methods with fewer computation resources <cit.>.\n\n\n\n\n \u00a7.\u00a7 Self-supervised Learning in Remote Sensing.\n \n\n\n\n<cit.> experiments with self-supervised models on different pretext tasks, including image inpainting <cit.>, context prediction <cit.>, and instance discrimination <cit.> on remote sensing imagery tasks. \n<cit.> proposes to learn practical representations from satellite imagery by reconstructing the visible colors (RGB) from its high-dimensionality spectral bands (Spectral).\n\n<cit.> explores the application of contrastive representation learning to large-scale satellite image datasets.\n\n\n<cit.> proposes the seasonal contrast (SeCo) method, which significantly improves the performance of MoCo-V2 on three remote sensing tasks.\nSimilar to <cit.>, SeCo utilizes temporal information from remote sensing images to set two kinds of experiments MoCo-V2 with TP and SeCo.\nMoCo-V2 with TP is a positive temporal contrast, which regards the temporal views as the positive instances and trains models match their representations allowing the model to learn essential features that do not change over time.\nWhile SeCo is a negative temporal contrastive model regarding temporal views as the negative instances to capture the changes or differences because of the time changing.\n<cit.> uses Swin Transformer as the backbone of DINO and applies it to remote sensing imagery tasks.\nDINO-MM <cit.> extends DINO by combining synthetic-aperture radar (SAR) and multispectral (optical) images and is applied to BigEarthNet land use classification task.\n\n\n\nExisting studies have demonstrated the value and feasibility of self-supervised models for practical applications in remote sensing image tasks.\nHowever, the potential of SSL in remote sensing has not been fully unlocked.\nOur work targets to bridge this gap and extend existing self-supervised model by generating more effective contrastive instances.\n\n\n\n\n\u00a7 METHOD\n\nOur work is mainly based on a contrastive self-supervised model DINO, which has been applied to both natural and remote sensing imagery <cit.>.\nWe aim to learn useful, transferable features for remote sensing imagery by exploring positive temporal contrastive self-supervised model DINO-TP (<ref>) and introducing a self-supervised learning method DINO-MC (<ref>).\nIn addition, we also experiment with the feature extraction ability of different backbones in SSL.\n\n\n\n\n\n \u00a7.\u00a7 DINO-TP\n\n\nArchitecture \n<ref> shows the model structure.\nThe student and teacher networks in DINO are two neural networks with the same architecture g but different weights \u03b8_t and \u03b8_s.\nTwo sets of augmented views x_1 and x_2 are generated from the same image.\nThe student network receives both global and local crops as inputs, whereas the teacher network only receives global crops.\nSpecifically, the global view covers the majority of the initial image and the local view only contains a small portion of it.\nIn this way, the model is trained to match the individual local views to global views in the feature space.\n\nRather than a pre-trained and frozen teacher network used in previous study <cit.>, DINO dynamically updates the teacher weights by using EMA on student weights  \n\n    \u03b8_t\u03b8_t\u03bb+(1-\u03b8_s)\n\n, where \u03b8_t is the current weight of the teacher network and \u03b8_s is the current weight of the student network.\nThe \u03bb values adhere to a cosine schedule between 0.996 and 1, indicating that the teacher network is less dependent on the present student and more dependent on the integration of the student network in each round; hence, the weights are updated slowly.\n\nDINO uses centering and sharpening to prevent model collapse.\nCentering is adding a bias term c to the features of the output of the teacher model, i.e., \n\n    g_t(x) = g_t(x) + c\n\nAs <ref> shown, the bias term c is dynamically updated by the EMA, where m > 0 denotes the update rate and B is the batch size.\n\n    c mc+(1- m)1/B\u2211_i=1^B g\u03b8_t(x_i)\n\nDINO achieves sharpening by performing softmax normalization using low temperature in the teacher network to avoid consistent distribution.\nThe teacher network in DINO consistently performs better than the student network during pre-training, so it is used as the feature extractor in downstream tasks after pre-training <cit.>.\n\n\n\n\nTemporal Contrastive  \nThis work performs self-supervised pre-training on SeCo-100K <cit.> to exploit temporal information in representation learning.\nSeCo-100K is an unlabeled remote sensing imagery dataset with 100K instances, and each instance is composed of five different temporal views of the same location.\nIn DINO-TP, we regard the temporal views of the same location as the positive instances, i.e., we pre-train the teacher and student networks to match temporal views in the feature space.\nThe exact process is as follows.\nIn the pre-training phase, we randomly select three temporal views and name them as t_0, t_1, t_2.\nAs shown in <ref>, t_1, t_2 can be regarded as the temporal augmentation views of t_0.\nThe temporal image t_0 is directly used as q to generate 6 local crops of various sizes, including 184^2, 164^2, 144^2, 124^2, 104^2 and 84^2.\nAfter applying color jittering, grayscale, and Gaussian blur to each of the three temporal perspectives, we acquire k_0, k_1, k_2 views.\nWe scale k_0, k_1, k_2 within a specific range and resize them into 224^2 to get three global crops, which are used as the input of the teacher network.\nIn prior work, different techniques are paired with different crop scaling ranges.\nSwAV uses (0.05, 0.14), (0.14, 1) <cit.> and DINO chooses (0.05, 0.32), (0.32, 1) <cit.>.\nThis study follows DINO and uses (0.05, 0.32), (0.32, 1) as the scaling range for local crops and global crops.\nFollowing this method, DINO-TP not only learns the relationship between the whole and the pieces of the image, but also discovers the correlation between various temporal perspectives.\n\n\n\n \u00a7.\u00a7 DINO-MC\n\n\nThe structure of DINO-MC is nearly identical to DINO-TP.\nIntuitively, DINO-MC can be regarded as DINO with different multi-crop and color transformation augmentations, and as DINO-TP without temporal views.\nIn our work, we change the cropping strategy for local crops in DINO to create a more challenging pretext task.\nWe use multi-sized crops instead of the fixed-size local crops used in DINO. \nKeeping the number of global and local crops same as DINO, DINO-MC outperforms DINO on both linear and KNN probing as well as end-to-end evaluation on multiple downstream tasks.\nColor-related pretext tasks have been proven to be effective in many image tasks in the field of remote sensing <cit.>.\nThis is due to the strong connection between color and semantics.\nTherefore, we set two strategies of color transformations for global and local crops respectively.\nThe global views are augmented by random color jittering and GaussianBlur.\nThe local views are augmented by random color jittering, random grayscale shifting, and random Gaussian blur all together.\nRandom color jittering can change the brightness, contrast, saturation and hue of the image with a specific probability within a certain range.\nIt is a commonly used color transformation method because it can simulate the effect of shooting in different lighting environments and other real-world shooting situations.\nRandom grayscale converts an image into a grayscale image randomly.\nThis enhancement method can reduce the impact of color, which is beneficial for some specific application scenarios, and can learn aspects other than color properties.\nWe use different cropping strategy and different settings for color transformation.\nOur strategy is simple but effective, and the experiments verify that DINO-MC outperforms DINO on both linear probing and end-to-end fine-tuning on three downstream tasks (classification and change detection).\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\nIn this study, we evaluate the features learned from the self-supervised pre-training on two downstream tasks: a land use classification on EuroSAT <cit.> and a change detection task on OSCD <cit.>.\n\nSelf-supervised Pre-training  \nWe adopt and extend DINO for doing experiments since it is not only flexible and effective but also greatly declines the computational requirements.\nThe models are pre-trained on SeCo-100K dataset <cit.>, which is collected from Sentinel-2 <cit.> for self-supervised representation learning.\nIt is an unlabeled remote sensing dataset containing 100K instances from different locations around the world, each consisting of five temporal views taken from the same location.\nIn the pre-training phase, two distinct sets of temporal views of the same region are supplied to the student and teacher networks, respectively, then the self-supervised model matches these views in feature space to generate representations that do not vary over time.\nIn DINO-MC and DINO-TP, the scaling range of multi-crop is (0.05, 0.32) for local crops and (0.32, 1) for global crops.\nOur self-supervised models, pretrained on 100k images over 300 epochs, is compared against several baselines, including DINO, MoCo-V2, and SeCo, on three different remote sensing downstream tasks, while using less overall computation than the current state of the art (SeCo-1M).\n\n\n\n\n\n\n \u00a7.\u00a7 DINO with Different Backbones\n\nBeing the backbone of DINO, ViT is superior to ResNet <cit.>.\nIn this work, there are four different networks applied as the backbone of DINO, DINO-TP, and DINO-MC, including ViT, Swin Transformer, ResNet, and WRN.\n\n\nViT preserves the Transformer structure used in NLP as much as possible and performs very well in computer vision tasks <cit.>.\nThe input of transformer encoder is the embedding formed by adding the patch embedding with the position encoding.\nThe transformer encoder of ViT consists mainly of layer normalization (LN) which is applied before each block, multi-head attention, and multi-layer perceptron block (MLP).\n\n\nInspired by ViT, Swin Transformer is proposed with a hierarchical transformer, which computes representation with both regular and shifted windows to capture features from different levels and resolutions. \n\n\n\n\n\nA deep residual learning framework (ResNet) is proposed to overcome the degradation issue by <cit.>.\nThey add shortcut connections to transmit the information of shallow layers directly to the deeper layers of the neural network and require no additional computation.\nWith this mechanism, the residual net is deepened to 152 layers and achieves state-of-the-art results in multiple computer vision tasks. \nFurthermore, with the development of self-supervised representation learning, ResNet also becomes an effective backbone widely used in multiple self-supervised architectures <cit.>. \n\n\nWRN <cit.> is proposed to improve the performance of ResNet by increase the width of the residual block, i.e., widening the convolutional layers.\nWhen the same number of parameters are utilized, wide residual block can get superior outcomes with less training time compared to the original residual block.\nWRN adds a widening factor to a block denoted by k, and the initial residual block can be represented as k=1. \nAlthough the parameters numbers and computational complexity are quadratic in k, this method is more effective than expanding the number of layers in ResNet since large tensors are able to make better use of the parallel-computing ability of GPUs <cit.>.\nWRN is proved to achieve new state-of-the-art results in a supervised manner in terms of classification F1-Score and training efficiency <cit.>.\nTherefore, this project intends to employ it with a self-supervised training framework.\n\nImplementation Details \nThere are different sizes of ViT models, and we use ViT-small as the backbone model and implement it following DINO.\nIn the experiments, we load different backbones as the teacher and student network without pre-trained weights.\nWRN-50-2 has similar structure to ResNet, with the exception of the bottleneck number of channels, which is twice as large in each block.\nWe follow the recommendation from DINO to use AdamW optimizer.\n\n\n\nWe use cross-entropy as the loss function to calculate the distance between feature representations output by two networks.\nWe evaluate the learned representations by applying KNN and linear probing on EuroSAT land use classification task.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantitative Results\n\n\nIn order to evaluate the representations independently, the feature extraction model is frozen and only linear and KNN classifiers are trained on EuroSAT land use classification task.\nThe results are shown in <ref>.\nFrom the table, DINO-MC performs better than both DINO and DINO-TP when using ViT-samll, WRN-50-2, and ResNet-50 backbones.\nDINO-MC with WRN-50-2 pre-trained on 100K data is even 2.56% higher than the linear probing accuracy of SeCo pre-trained on 1 million data.\nBesides, DINO-MC with ViT-samll has 2.59% higher accuracy than DINO with ViT-samll which indicates the effectiveness of our strategy.\nIn comparison to the other three backbones, Swin-tiny performs less well.\nOne possible reason could be that the model size of Swin-tiny is much smaller than the other three models.\nAnother interesting observation is that the DINO-TP performs worse with two convnets than the DINO, but better with two transformer models.\nAmong the different self-supervised models, ViT-small and Swin-tiny performed more consistently than ResNet-50 and WRN-50-2.\nOverall, DINO-MC is particularly effective.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Land Use Classification on EuroSAT\n\nEuroSAT is a widely used benchmark dataset for remote sensing land use classification tasks.\nIt is used for representation evaluation in this study, allowing the results of models based on it to be easily compared to those of other models.\nThe dataset, which collects 27,000 remote sensing images from the Sentinel-2 satellite, is divided into 21,600 and 5,400 images for training and evaluation in the experiment, respectively.\n\nImplementation Details \nThe pre-trained backbone models in self-supervised learning are evaluated as feature extractors in this land use classification downstream task.\nBased on the pre-trained backbone models, we add a fully-connected layer as the classifier to output the classification results.\nBoth the feature extractor and the classifier are fine-tuning on this supervised classification task.\nWe train the models for around 200 epochs with a batch size of 32.\nThe learning rate is 1e-3 or 3e-3 for different models.\nWe use the CosineAnnealingLR scheduler to update the learning rate.\nSame as SeCo <cit.>, we use SGD optimizer without weight decay to update weights.\n\nQuantitative Results\n<ref> compares our pre-trained models against three supervised baseline models on EuroSAT land use classification task.\n\nDINO-MC with WRN-50-2 achieves similar results to the three supervised models, in particular, it was pre-trained on only 100K images, while the three supervised models were pre-trained on 1M images.\nThis further confirms the effectiveness of the representations learned by DINO-MC.\n\n\n\n\n\n\n \u00a7.\u00a7 Land Use Classification on BigEarthNet\n\nBigEarthNet <cit.> is a widely used benchmark dataset for land use classification task, containing a total of 590,326 images.\nThis paper uses BigEarthNet-S2 <cit.>, which collects remote sensing images from Sentinel-2 only, and each image is annotated by multiple land use categories.\nThe dataset provides 12 spectral bands for each image and a JSON file with its multi-labels and metadata information.\nFollowing SeCo <cit.>, we employ a new nomenclature of 19 classes introduced in <cit.>, and around 12% of the patches that are totally masked by seasonal snow, clouds, or cloud shadows are eliminated in this experiment.\nWe used the training/validation splitting strategy suggested by <cit.> with 311,667 instances for training and 103,944 images for validation.\n\nImplementation Details \nWe add a linear classification layer on top of the backbone model as the output layer and then fine-tune it on 10% and 100% BigEarthNet, respectively, to evaluate the features learned by the self-supervised models.\nIn this experiment, the Adam and AdamW optimizer with default hyper-parameters are used to update the weights of the models. \nIdentical to SeCo, we set the learning rate to 1e-5 and scale it down by ten in epochs of 60% and 80%, respectively.\nWe use the MultiLabelSoftMarginLoss as the loss function, which allows assigning a different number of target classes to each sample.\n\nQuantitative Results\n<ref> provides the results of each pre-trained model in the end-to-end BigEarthNet classification task.\nWe measure the performance of each model by mean average precision (MAP).\n\n\nWhen fine-tuned on the 10% BigEarthNet, DINO-MC outperforms SeCo-100K with the same backbone.\nBesides, DINO-MC with ViT-small achieves 1.65% higher MAP than with ResNet-50, 2.48% higher MAP than SeCo-100K, and even 1.58% higher MAP than SeCo-1M.\nThe performance of DINO-MC with each of the four backbone models exceeds that of SeCo-100K, and even outperforms SeCo-1M except for ResNet-50.\n\nWhen fine-tuned on the whole BigEarthNet, DINO-MC achieves comparable result to SeCo-100K with the same backbone.\nInterestingly, DINO-MC with Swin-tiny achieves 1.89% higher MAP than with ResNet-50, 1.63% higher MAP than SeCo-100K, and 0.94% higher than SeCo-1M.\n\nThe results demonstrate that the representations learned by DINO-MC can generalize well on the multi-label classification task on BigEarthNet.\n\n\n\n\n\n \u00a7.\u00a7 Change Detection on OSCD\n\n\n\n\n\n\n\nThe Onera Satellite Change Detection (OSCD) is a benchmark dataset of which the images are collected from Sentinel-2 satellites. \nIt focuses primarily on urban growth and disregards natural changes <cit.>. \nChange detection is a fundamental problem in the field of earth observation image analysis.\nThe input is a pair of images captured at the same location, while the label is a mask map highlighting the change parts.\nIt is a binary classification task in which labels are assigned to each pixel based on a series of images sampled at different times: change (positive) or no change (negative).\nThe results are measured by F1 Score, precision and recall.\n\nImplementation Details \nThe OSCD dataset is divided into fourteen and ten pairs for training and validation separately, as recommended by prior research <cit.>.\nIn addition, the categories of change and unchanged in this dataset are unbalanced due to the property of the task.\nWe use the same U-net architecture as SeCo for the change detection task, which employ the pre-trained backbone models to extract features as the encoder in the U-net.\nWe apply the pre-trained WRN-50-2 and ResNet-50 backbone models as the encoder and select the first convolution layer, and Layer 1 to 4 as the copy and cropping layers.\nAs for the decoder module, it is  constructed  to rebuild an image with the same width and height as the input.\nAnd the input and output sizes of its layers are set according to the input and output of the specific encoder layers, in order to concatenate them in the direction of the channel.\nUpsampling here is achieved by interpolate function, which can be understood simply as a technology to increase the resolution of the output.\nThe last layer of the U-net is a 1\u00d7 1 convolutional layer, which is used to map the channel of the feature vector to the number of output classes.\nDuring fine-tuning, in order to avoid overfitting, only the U-net weights are updated.\nIn this task, the pre-trained WRN are fine-tuned with a batch size of 32 and learning rate of 0.0006.\nThe loss function used is BCEWithLogitsLoss + SoftDiceLoss\n\nQuantitative Results\n<ref> provides the results of DINO, DINO-MC, and DINO-TC with ResNet-50 and WRN-50-2 respectively, compared against some supervised and self-supervised baselines on OSCD dataset.\n\nThe F1 score of DINO-MC with ResNet-50 is 5.52% higher than that of SeCo and almost 3% higher than that of DINO.\nWhen using WRN-50-2 as the backbone, the F1 score of DINO-MC is 5.76% higher than that of SeCo and similar with DINO.\nIt is no surprise that DINO-TP does not perform as well as DINO and DINO-MC, because DINO-TP receives images of the same location taken at different times as positive instances, so it aims to learn features that do not change over time, which is not very suitable for change detection task <cit.>.\n\n\nIn the results of OSCD task, the backbones pre-trained in DINO-MC can capture the subtle differences of image changes over time after a simple fine-tuning on the change detection dataset, which indicates that DINO-MC is able to learn general and effective features using only very simple data augmentation methods.\n\nQualitative Results\n<ref> gives the visualization masks of DINO-MC on OSCD task.\nTo do comparison to SeCo, we select two identical examples from the OSCD validation set.\nThe masks generated by DINO-MC outperform SeCo-1M since they cover more changed pixels without excessive false predictions.\nWe observe that the performance of DINO-TP on OSCD task is unstable since it achieves particularly high F1 score on the first instance, which is 4.35 higher than SeCo-1M, but much lower in the second one, which is 2.25 lower than SeCo-1M. \nAlthough positive temporal contrast used in DINO-TP has been proved to be undesirable for change detection task <cit.>, the performance of DINO-TP is comparable to SeCo-1M when evaluated on the whole validation set.\n\n\n\n\n\n\n\u00a7 CONCLUSIONS\n\nIn this work, we introduce DINO-TP and DINO-MC, which extend DINO in two ways: (1) DINO-TP employs a positive temporal contrast strategy and (2) DINO-MC utilizes a new cropping strategy and color transformations for local views.\nExperimental results of KNN and linear probing evaluation on EuroSAT demonstrate the effectiveness of our cropping strategy, as well as the unstable performance of the positive temporal contrast on remote sensing imagery.\nWe evaluate and compare our models with some self-supervised and supervised baselines on three remote sensing tasks.\nThe results of three end-to-end tasks indicate the superiority and efficiency of DINO-MC over the existing state-of-the-art models.\n\n\n\n\n\nAcknowledgement\nThis research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne. This Facility was established with the assistance of LIEF Grant LE170100200.\n\n\nieee_fullname\n\n\n\n"}