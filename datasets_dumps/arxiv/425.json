{"entry_id": "http://arxiv.org/abs/2303.06724v1", "published": "20230312183120", "title": "Gr\u00f6bner and Graver bases for calculating Opportunity Cost Matrices", "authors": ["Yuchen Ge", "Janosch Ortmann", "Walter Rei"], "primary_category": "math.OC", "categories": ["math.OC", "math.AC", "90C15(Primary), 90C10, 13P25 (secondary)"], "text": "\n\n\tOpportunity cost matrices, first introduced in <cit.>, are interesting in the context of scenario reduction. We provide new algorithms, based on ideas from algebraic geometry, to efficiently compute the opportunity cost matrix using computational algebraic geometry. We demonstrate the efficacy of our algorithms by computing opportunity cost matrices for two stochastic integer prorams.\n\n\t\nProbing the Origin of Changing-look Quasar Transitions with Chandra\n    [\n    \n===================================================================\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nIn many real-world applications, decision-making under uncertainty is a challenging problem that requires the consideration of multiple possible scenarios. However, the number of scenarios can grow exponentially with the number of uncertain parameters, making the decision problem intractable. Scenario reduction methods\n\naim to address this issue by reducing the number of scenarios while preserving the key characteristics of the original problem.\n\nRecently <cit.> there has been progress in developing problem-driven scenario reduction methods. All of these methods require the computation of a variant of the opportunity cost matrix introduced in <cit.>: assign a decision x^\u2217_j to each scenario s_j. In <cit.>  this is done by choosing an optimal solution to the single-scenario problem. The opportunity cost matrix is computed by evaluating the solution associated to scenario j against scenario i. Intuitively, suppose that we hae an oracle that predicts that scenario j will occur. The (i,j) entry of the opportunity cost matrix gives the cost incurred by trusting the oracle and taking a decision under the viewpoint of scenario j if it then turns out that in fact scenario i occurs.\n\nThe opportunity cost matrix naturally leads to a decision-based distance on the scenario set: two scenarios i and j can be considered to be close if the (i,j) entry of the opportunity cost matrix is small (and hence the cost of predicting scenario j when actually scenario i occurs leads to a small cost) and far away if the (i,j) entry is large.\n <cit.> propose clustering algorithms based on this distance, which has shown to lead to good approximative solutions and tight bounds on the true objective value.\n\n\n\nIn this paper, we present a novel approach to efficiently compute opportunity cost matrices using Gr\u00f6bner and Graver bases. Gr\u00f6bner <cit.> and Graver <cit.> bases  are algebraic objects that can be used to solve systems of polynomial equations and inequalities. These have been successfully applied to optimisation <cit.> \n\nand have seen a wide range of applications \n\n\n\nOur approach leverages the structure of opportunity cost matrices to formulate them as polynomial systems, which can then be solved using Gr\u00f6bner and Graver bases. The resulting method is fast, accurate, and scales with the number of scenarios and variables.\n\nWe demonstrate the effectiveness of our approach through numerical experiments on a well-known integer program from <cit.>. Our results show that the Gr\u00f6bner basis approach is particularly efficient for a large number of scenarios, whereas the Graver basis approach stays stable with a large number of decision variables.\nOur paper contributes to the literature by providing two new approaches to computing opportunity cost matrices. We also prove theoretical results about Graver bases for deterministic and stochastic integer programs.\n\n\nThe rest of the paper is organized as follows. In Section <ref>, we provide background on opportunity cost matrices and their usefulness. In Section <ref>, we introduce the Gr\u00f6bner and Graver bases and present our mathematical results. Section <ref> describes the algorithms that we then apply to generate our numerical results in Section <ref>. Finally, Section <ref> concludes.\n\n\n\n \u00a7.\u00a7 Notation\n Throughout this paper,  denotes the set of integers and  the subset of non-negative integers.\n\n\n\n\n\n\u00a7 OPPORTUNITY COST MATRICES\n\n\n\n\n\nConsider the following two-stage stochastic optimisation problem\n\n    min_x\u2208 F(x,\u03be) =  min_x\u2208\u03b3^\u22a4 x +  Q(x,\u03be) \n    \twhere \u03b3\u2208^d and  is the intersection of the integer lattice with a convex polyhedron. The random part Q of the objective function is given by\n    \tQ(x,\u03be)     =  min c_\u03be^\u22a4 y   T x + A y = h_\u03be y\u2208^d\n\nwhere c_\u03be\u2208^d  and b_\u03be\u2208^k for all \u03be and T and A are integer matrices of the required dimension. \nObserve that we assume T and A to be deterministic matrices, i.e. they are not random.\n\n\nIn general, it is not obvious how to obtain an explicit solution to (<ref>). The scenario approach in stochastic programming <cit.> consists of approximating the probability measure  on \u03be that gives rise to the expectation operator  by a discrete measure assigning probabilities[i.e. p_n\u2265 0 for all n and \u2211_n=1^N p_n=1] p_1,...,p_N on a finite set of outcomes \u03be_1,...,\u03be_N(often called scenarios) for \u03be. The set =\u03be_1,\u2026,\u03be_N is then referred to as the scenario set.\n\nFor example, this can be achieved by the sample average approximation <cit.>, where the \u03be_i are independent samples from  and are each assigned the same probability 1/N of occurring. In any case, the scenario approach yields optimisation problems of the form\n\n    min\u03b3 ^\u22a4 x+\u2211_i=1^N p_i c_i^\u22a4 y_i: x\u2208, A y_i=h_i-T x, y_i\u2208\u2124_+^n \u2200  i.\n\nIn order to properly model the incertainty represented by , often a large number of scenarios must be generated. On the other hand solving problems of the form <ref> when N is large can be very computationally challenging. This motivates the idea of scenario reduction: can one find a small subset  of the scenario set =\u03be_1,\u2026,\u03be_N such that replacing  by  in (<ref>) only incurs a small approximation error. \n\nGenerally speaking, scenario reduction methods can be separated into distribution-based and  problem-based methods. The former seek to minimise the difference (defined in some suitable sense) between the empirical measure of the subset and the original scenario set. See for example <cit.> for examples of this approach. On the other hand, problem-based methods <cit.> look to minimise the difference between the solutions associated to the larger and smaller problems, in other words scenarios are considered to be similar if they lead to similar decisions according to (<ref>). In <cit.>, the opportunity cost matrix was introduced. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\n\tConsider a set of feasible decisions x_1,...,x_N\u2208 such that decision x_j is associated to scenario s_j. The opportunity cost matrix is the N\u00d7 N matrix  whose (i,j) entry is _ij=F(x_i,\u03be_j).\n\n\n\nClearly there are many interesting choices for the solutions x_j. \n\n\nIn <cit.> and <cit.>, the decisions x_n were chosen to be the single-scenario solutions of (<ref>), that is\n\n    x_j\u2208 F(x,\u03be_j),\n\n\n\nIn this paper, we are looking for efficient methods of computing the matrix  in the context of the stochastic linear two-stage problem defined in (<ref>).\nSince the functions Q and F only differ by the linear term \u03b3\u00b7 x, we turn our attention to efficiently computing \n\n    Qx_i,\u03be_j    = minc_j^\u22a4 y  A y = b_ij,\n\nwhere c_j=c(\u03be_j), b_ij=b(x_i,\u03be_j). Here, we write b(x,\u03be)=h_\u03be-T_\u03be x.\nSince we had assumed A to be deterministic deterministic, the feasible region on the right-hand side of (<ref>) changes only through b_ij as i and j vary. \n\n\n\n\n\u00a7 GR\u00d6BNER AND GRAVER BASES\n\n\n\n\nIn this section we develop the  theory of Gr\u00f6bner and Graver bases that we require in order to justify and motivate the algorithms that will be described in Section <ref>.\n\n\nIn the previous section we saw that in order to compute the opportunity cost matrix, we are looking to solve integer optimisation problems of the form\n\n\n    cb:   minc^\u22a4 z: A z=b, z \u2208^d\n\nfor a given integer matrix A but with varying cost vectors c\u2208 and constraint vectors b\u2208$\u0331. Here,and$\u0331 are finite sets corresponding to the values that the c(\u03be_m) and b(x,\u03be_m) of (<ref>) can take. Since we consider the matrix A to be fixed throughout, we do not include it in the notation.\n\nNote that the optimal solution to (<ref>) may not be unique, which is sometimes inconvenient. Whenever required, we can therefore replace (<ref>) by its refinement >_cb that we will define now. We begin my defining a total order[A total order on a set S is a binary relation \u227a on S such that a\u227a b or b\u227a a for all a,b\u2208 S with a b.]  on ^d.\n\n\n\n\t\n\tLet > be any total order on ^n and fix c\u2208^d. We define another order relation on ^n by saying that x>_c y  and only if\n\t\n\t\t\n  * c\u00b7 x>c\u00b7 y or\n\t\t\n  * c\u00b7 x=c\u00b7 y and x>y.\n\t\n\n\nUnless otherwise specified, the total order \u227a in Definition <ref> will be the lexicographical order, under which x>y if and only if the leftmost non-zero entry of x-y is positive.\n\n\n\n\tA  set _c\u2286^d  is called a test set for the family of problems  cb b \u2208\u211d^l if\n\t\n\t\t\n  * c\u00b7 t>0  for all  t \u2208\ud835\udcaf_c\n\t\t\n  * for every  b \u2208\u211d^l  and for every non-optimal feasible solution  z_0\u2208\u2124_+^d  to  A z=b , there exists a vector  t \u2208\ud835\udcaf_c  such that  z_0-t  is feasible. Such a vector is called an augmentation vector.\n\t\n\n\n\n\nGiven a test set  \ud835\udcaf_c, the augmentation algorithm (Algorithm <ref>) provides a way to compute the optimum of >cb for any b\u2208$\u0331. Intuitively, while  there ist \u2208\ud835\udcaf_cwithc^\u22a4 t>0such thatz_0-tis feasible, we repetitively iteratez_0by assigningz_0:=z_0-t.\nThus, we are looking to efficiently construct a test set_cfor eachc\u2208. Observe that_c\u2286(A)since bothz_0andz_0-tare feasible for anyt\u2208_cwhich implies that\n\n    A t     = A(z_0 - (z_0-t)) = Az_0 - A(z_0-t) = b-b = 0.\n\n\nIn the following we will exhibit two approaches to constructing such a test set. The first uses Gr\u00f6bner bases and is described in Section <ref>. The second, based on Graver bases, goes one step further and constructs a universal test set, that is, a setcontaining a test set_cfor each cost vectorc\u2208.\n\n\n\n\n\n \u00a7.\u00a7 The Gr\u00f6bner Basis Approach\n\n\nIt is now well known <cit.>\nthat Gr\u00f6bner bases <cit.> yield test sets. The Buchberger algorithm<cit.> takes as input a set of polynomialsf_1,\u2026, f_Mand returns the reduced Gr\u00f6bner basis of the ideal generated by thef_j. A geometric version of the Buchberger algorithm was given in <cit.>. \nThis algorithm is at its most efficient if the generating set is small. We achieve this by using the toric ideal and in particular the methodology of <cit.>. The advantage is that this can be done once for all cost vectors, which is efficient in the context of computing opportunity cost matrices.\n\t\n\t\n\n\n\t\n\t\n\t\n\t\n\tWe will require the following result, which is a direct consequence of Dickson's Lemma <cit.> and can be found as Lemma 2.1.4 of <cit.>.\n\t\n\t\n\t\tLet A\u2208^l\u00d7 d and let _c,b denote the set of non-optimal feasible solutions of cb.\n\t\tThen there exist \u03b1(t),\u2026,\u03b1(t)\u2208^d such that\n\t\t\n    \u22c3_b \u2208^l_c,b=\u22c3_i=1^t(\u03b1(i)+N^n)\n\n\n\n\t\n\tThe vectors\u03b1(1),\u2026,\u03b1(t)allow us to compute a test set for the family of integer programscbb\u2208, via the the following result, which follows from the Gordon-Dickson Lemma and is stated as Corollary 2.1.10 of <cit.>. See also Lemma 2.1 in <cit.>.\n\t\n\t\n\t\tFix c\u2208^d, let \u03b1(1),\u2026,\u03b1(t) as in Lemma <ref> and let \u03b2(i) be the (unique) optimum of the program >_cA \u03b1 (i). Then the set\n\t\t\n    \ud835\udca2_A,c=(\u03b1(i)-\u03b2(i)),i=1,2,...,t\n\n\t\tis a minimal test set for the family of problems cb b\u2208^l.\n\t\n\tThe set  \ud835\udca2_A,c is called the Gr\u00f6bner basis for the matrix A. \n\n\n\tWhile this result gives a formula for the desired test set, it requires computation of the\u03b1(i)from Lemma <ref> and, crucially, of the\u03b2(i), which are given in terms of solutions of integer problems itself. A geometric construction of the test set was given in <cit.>.  \n\t\n\tAs explained above, we need to compute the solutions to>_cbascvaries. If we would like to take the Gr\u00f6bner basis approach, we would need to recompute the Gr\u00f6bner basis for every cost vectorc\u2208. Therefore, we proceed in a different manner, based on ideas from the theory of polynomial rings.\n\t\n\tLetR=xndenote the ring of polynomials innvariables. We will make frequent use of a close relation between vectors of integers and differences of monomials inR. Anyn-vector of non-negative integers\u03b1=(\u03b1_1,\u2026,\u03b1_n)\u2208^ncan be identified with a monomialx^\u03b1\u2208Rdefined by \n\t\n    x^\u03b1 = x_1^\u03b1_1\u22ef x_n^\u03b1_n = \u220f_j=1^n x_j^\u03b1_j,\n\n\tand this mapping can be extended to integer-valued vectors as follows. For any\u03b1=(\u03b1_1,\u2026, \u03b1_n)^n, define\u03b1^+=(\u03b1_1^+,\u2026,\u03b1_n^+)and\u03b1^-=(\u03b1_1-,\u2026,\u03b1_n^-), recalling that the positive part of a real numberxis defined to bex^+=max(x,0)and the negative part ofxisx^-=(-x)^+. Then both\u03b1^+, \u03b1^-\u2208^nand\u03b1=\u03b1^+-\u03b1^-.  We now define a mapping\n\t\n    \u03d5^n\u27f6 R   by  \u03d5(\u03b1) = x^\u03b1^+- x^\u03b1^-.\n \n\n\tThe notion of a Gr\u00f6bner basis associated to a matrix from Definition <ref> is actually a special case of a more general concept, namely that of a Gr\u00f6bner basis of a polynomial ideal.  In the following, we give a very brief account of the definition and basic properties of Gr\u00f6bner bases. For more details see for example Chapter 1 of <cit.>. \n\t\n\t\n\t\tA subset I\u2286 R is said to be an ideal of R if it satisfies the following two properties:\n\t\t\n  *  For any x,y\u2208 I we have x-y\u2208 I (that is, I is an additive subgroup of R),\n\t\t\t\n  *  For any x\u2208 I and r\u2208 R we have rx \u2208 I.\n\t\t\n\t\tGiven polynomials f_1,\u2026,f_n, the ideal generated byf_1,\u2026,f_n is the set of all R-linear combinations of the f_j:\n\t\t\n    \u27e8 f_1,\u2026,f_n\u27e9    = \u2211_j=1^n r_j f_j  r_1,\u2026,r_n \u2208 R .\n\n\t\tThe toric idealI_A of a matrix A\u2208^d\u00d7 n is generated by differences of monomials x^u-x^v such that u-v lies in the kernel of A:\t\t\t\t\n\n\t\t\n    I_A     = \u27e8 x^u-x^v  Au=Av \u27e9.\n\n\t\nSee Section 4 of <cit.> for the theory of toric ideals. A polynomialf\u2208Rconsists of monomialsx^\u03b1with degree\u03b1\u2208^n. Throughout, we fix a total order (recall Definition <ref> and the discussion surrounding it)\u227aon^n. In principle, any total order will do, but for us,\u227awill always be the total order<_cdefined in Definition <ref>. Given\u227a, we can compare the different monomials by comparing their degrees. This allows us to define the initial monomial offto be that with the greatest degree (with respect to\u227a). The initial monomial offis denoted\u227af. Given an idealIofR, the initial ideal ofIis the  ideal generated by the initial monomials of the elements ofI:\n\n    \u227a f     = \u27e8\u227a f  f\u2208 I \u27e9.\n\n\t\n\tWe are now in a position to define Gr\u00f6bner bases for polynomial ideals. \n\t\n\t\n\t\tLet I be an ideal of R. A subset  of I is said to be a Gr\u00f6bner basis of I if the inital ideal of I is generated by the inital monomials of the elements of :\n\t\t\n    \u227a I     = \u27e8\u227a g  g\u2208\u27e9.\n\n\tA Gr\u00f6bner basis  of I is said to be minimal if (<ref>) no longer holds whenever  any one element is removed from \t. Also,  is said to be reduced if for any distinct g_1,g_2\u2208 it holds that no term of g_1 is a multiple of \u227ag_2.\n\t\n\nGiven an idealIofRand a total order on^n, there is only one reduced Gr\u00f6bner basissuch that the coefficients of\u227agare equal to1for eachg\u2208. In other words, reduced Gr\u00f6bner bases are unique up to multiplying the elements by constants. \n\t\n\t\n\t\tRecall the vectors \u03b1(i) and \u03b2(i) from <ref>. The set of polynomials\n\t\t\n    _I_A   = x^\u03b1(j)-x^\u03b2(j) j\u22081,\u2026,t\n\n\tis the reduced Gr\u00f6bner basis of the toric ideal I_A with respect to the ordering <_c.\n\t\n\n\tWe conclude that the two notions of Gr\u00f6bner basis (of a matrix on the one hand and of a polynomial ideal on the other) are actually the same: they are related via the map\u03d5defined in (<ref>). In fact, the original Buchberger algorithm <cit.> is formulated in this setting.\n\t\nIn <cit.>, an efficient algorithm to compute a small generating set of the toric idealI_A, see Algorithm <ref>. Observe that the toric ideal only depends on the matrixA, and not on the cost vectorc. This motivates our kernel algorithm, described in detail in Algorithm <ref>:\n\n\n  *  Given the matrix A, compute a small generating set of the toric ideal  I_A following <cit.>.\n\t\n  *  For each j\u22081,\u2026,N, apply Buchberger's algorithm to compute the Gr\u00f6bner base with respect to A and <_c_j.\n\t\n  *  Use the correspondence (x^u-x^v) \u27f7 u-v from (<ref>) to obtain the test set with respect to (A,c_j).\n\t\n  *  For each i,j\u22081,\u2026,N, apply the augmentation algorithm (Algorithm <ref>) to compute Q(x_i,\u03be_j), and hence the (i,j)-entry of the opportunity cost matrix.\n\n\t\n\n\n\n\t\n\t\n\t\n\t\n\t\n\t\n\n\n\n\t\n\t\n\n \u00a7.\u00a7 The Graver basis approach\n\n\t\n\n\nIn the previous section, computing theN\u00d7Nopportunity cost matrix still required computingNtest sets via the Gr\u00f6bner bases. This motivates the question whether it is possible to construct a single test set that not only covers all right-hand side vectorsbbut also all cost vectorsc. \n\n\n\tA set \u2286 is an universal test set for the matrix A if it contains a test set _c for every cost vector c.\n\n\n\nClearly, the Gr\u00f6bner basis approach alone is insufficient for computing a universal test set, as this would require computing an infinite number of Gr\u00f6bner bases. Instead, the Graver basis approach will be used. We begin by stating the definition of a Graver basis.\n\n\n\n\tAssume a=(a_1,a_2,...,a_n), b=(b_1,b_2,...,b_n) \u2208\u2124^n. Then a \u2291 b if  \u2200 i:  a_i b_i\u2265 0 and |a_i| \u2264|b_i|. \n Fix a integer matrix A. The Graver Basis of A is the set  of  \u2291-minimal elements in  Ker_\u2124(A): ={u \u2208\u2124^n: A \u00b7 u=0,u\u2260 0}, denoted by \u0393_A. \n\nIt is not immediately obvious that\u0393_Ais finite. However, finiteness of\u0393_Afollows from the following result gathered from <cit.>.\n\n\n\tLet \ud835\udca2_A, c be the reduced  Gr\u00f6bner basis of A with respect to >c. Then \u22c3_c \u2208\u2124^n\ud835\udca2_A, c\u2286\ud835\udca2\u211b_A .\n\n\n\n\n\n\nNext, we state and prove a result about the relationship between the Graver Basis of the stochastic integer program <ref> and that of the deterministic integer program obtained by choosing a single scenario and assigning probability1to it:\n\n    min{c^\u22a4 x+ p q^\u22a4 y_: A x = b, x  \u2208\u2124_+^m, T x+W_ y_=h_, y_\u2208\u2124_+^n}\n\nThen we assert a new result, which connects the Graver Basis of the SIP problem and the Graver Basis of the IP problem:\n\n Denote the Graver Basis of the SIP problem (5) by \u0393_N, and that of the IP problem (<ref>) by \u0393_1  for  i=1,2,...,N. If Ker_\u211d(A)=0, we have the following relationship \u0393_N={(0,0,...,v_i,...,0):(0,v_i)\u2208\u0393_1,i=1,2,...,n}\n\tThe corresponding coefficient matrix of (<ref>) is of the form\n\t\n\tA_N:=([ A 0 0 \u22ef 0; T W 0 \u22ef 0; T 0 W \u22ef 0; \u22ee \u22ee \u22ee \u22f1 \u22ee; T 0 0 \u22ef W ])\n\t\n\tand  the corresponding coefficient matrix of (6) is of the form \n\t\n\tA_1:=([ A 0; T W;   ]).\n\t\n\tSince Ker_\u211d(A)=0, we have Ker_\u2124(A_N)=(0,v_1,v_2,...,v_n):(0,v_i)\u2208Ker_\u2124(A_1) }. From the definition of the Graver Basis, we can arrive at the conclusion.\n\n\n\u00a7 ALGORITHMS\n\n\n\n\n\tIn this section, we describe numerical algorithms that follow from the mathematical development of the previous section. We first introduce the augmentation algorithm that computes an optimal solutions tocbfrom a test set_cand a feasible solutionz_0.\n\n\n\nFor details about Algorithm <ref> see <cit.>.\t\n\n\nThe kernel algorithm takes as input the matrixA, a sequence of cost vectorscand an array of right-hand side vectorsband computes the corresponding opportunity cost matrix, by first applying Algorithm <ref> and then repeatedly the Buchberger algorithm. In this way, a Gr\u00f6bner basis forAis computed with respect to each>_c. The augmentation algorithm <ref> then gives the entries of the opportunity cost matrix for everycand everyb.\n\n\n\n\n\nFor the Graver basis approach, we apply the modified augmentation algorithm <ref> for a family of integer problems of the formminc^\u22a4xAx=bas bothbandcvary.\n\n\n \n\n\n\u00a7 PRELIMINARY RESULTS\n\n\n\nIn this section we test our approach on some numerical examples. The first is a well-known purely combinatorial stochastic program  introduced in <cit.>. The second is a stochastic network design problem. \n\n\n\n \u00a7.\u00a7 The example of <cit.>\n\n\nIn  <cit.>, the following stochastic integer program was introduced and studied.\n\n\n\n    min   {35 x_1+40 x_2+1/N\u2211_\u03bd=1^N 16 y_1^\u03bd+19 y_2^\u03bd+47 y_3^\u03bd+54 y_4^\u03bd}\n    s.t.     x_1+y_1^\u03bd+y_3^\u03bd\u2265\u03be_1^\u03bd\n        x_2+y_2^\u03bd+y_4^\u03bd\u2265\u03be_2^\u03bd\n        2 y_1^\u03bd+y_2^\u03bd\u2264\u03be_3^\u03bd\n        y_1^\u03bd+2 y_2^\u03bd\u2264\u03be_4^\u03bd, \n        x_1, x_2, y_1^\u03bd, y_2^\u03bd, y_3^\u03bd, y_4^\u03bd\u2208\u2124_+\n\n\n\nAs in <cit.> the scenarios\u03be^\u03bdare chosen uniformly from the four-dimensional squares [300,12000] \u00d7[300,12000]\u00d7[200,12000] \u00d7[200,12000]. \nIn order to bring the problem in the form (<ref>), we introduce slack variablesu_j^\u03bdforj\u22081,2,3,4and obtain\n\n\n    min   {1/N\u2211_\u03bd=1^N 35 x_1+40 x_2+ 16 y_1^\u03bd+19 y_2^\u03bd+47 y_3^\u03bd+54 y_4^\u03bd}\n    s.t.     x_1+y_1^\u03bd+y_3^\u03bd -u_1^\u03bd = \u03be_1^\u03bd\n        x_2+y_2^\u03bd+y_4^\u03bd -u_2^\u03bd  = \u03be_2^\u03bd\n        2 y_1^\u03bd+y_2^\u03bd +u_3^\u03bd = \u03be_3^\u03bd\n        y_1^\u03bd+2 y_2^\u03bd  + u_4^\u03bd =  \u03be_4^\u03bd, \n        x_1, x_2, y_1^\u03bd, y_2^\u03bd, y_3^\u03bd, y_4^\u03bd, u_1^\u03bd, u_2^\u03bd, u_3^\u03bd, u_4^\u03bd\u2208\u2124_+.\n\n\n\n\nIn order to test our approach, we calculated the seconds of CPU time used by the programs on the Apple Silicon M1 chip. We observe that it is easy to read off a  feasible solution to the problem (<ref>)\n    x_1=x_2=y_1^\u03bd=y_2^\u03bd=0, y_3^\u03bd=\u03be_1^\u03bd, y_4^\u03bd=\u03be_2^\u03bd   (\u03bd=1, \u2026 N)\n\n\n\n\n\nThe task is to compare the computational costs of the opportunity-cost matrix in the SIP problem in the last section. In other words, we compute a family of integer programming problems with three algorithms. We test the program's running time in CPU seconds on an Apple Silicon M1 chip with Python'spackage. Also, we introduce the modern MINLP solver, specially designed for mixed-integer programming problems, implemented by Python'spackage. We shall compare the computational costs of  our algorithms <ref> and <ref> with the MINLP solver in two cases. One is when we fix the sub-problem, and the other is when we fix the number of scenarios.\n\nWe shall compute the computational time when the quantity of scenarios equals 1, 20, 40, 60, 80, 100, 120, 140, 160, 180, and 200. We gather the numerical results in the table below. \n\n\n\n\nIt is remarkable that it only took about 0.1 seconds to compute the generating set of the toric ideal ofA, essential for the Gr\u00f6bner Bases computation in all scenarios. By contrast, it took approximately 130 seconds to compute the Graver Basis ofA. Also we visualize the table in a line chart below. The horizontal axis represents the increasing quantity of scenarios, and the vertical axis represents the running time. The three curves with different colors in the picture represent three different methods.\n\n\n\nFigure <ref> shows that Algorithm  <ref> took almost 130 seconds to initialize and then the curve rises slowly. The computational cost of the MINLP solver grows extremely fast and starts to overpower the Algorithm   <ref> whenN=20. Generally, the curve of Algorithm <ref> shows superior performance when compared with the other two algorithms.\n\n\n\nNext, we compare when the complexity of the sub-problems increases. Below is the graph showing the trends of computational costs when the size of scenarios is fixed to 200 but the number of scenarios is increased. In Figure <ref>, with a fixed 200 scenarios, var*xrepresentsxvariables in the particular programming problem. As can be seen, by comparing the curves, the Gr\u00f6bner Basis approach (the blue curve) is overall superior to the Graver Basis approach (the grey curve) and the traditional MINLP solver (the yellow curve). Also, regarding details, the Graver Basis method begins to suffer from the curse of dimensionality, starting from the problem's complexity being 20 variables, and so is the Gr\u00f6bner Basis method but more dramatically. However, the MINLP solver does not reflect the apparent complexity when it suffers from the curse. Generally, the MINLP solver applies to situations where the sub-problem is large, and the number of scenarios is small. In contrast, the Gr\u00f6bner Basis applies to situations where the sub-problem has little complexity, and the number of scenarios is significant.\n\n\n\n\nWe have yet to reflect one thing in the experiments. By testing our algorithm hundreds of times, we have come to the empirical conclusion that the time increase of our proposed algorithm in computing Grobner Basis will be dramatic forN > 15. And there will be geometric growth for the computational time regarding Graver Basis when the complexity is between 10 and 15 variables. Also, surprisingly, we can no longer compute the Gr\u00f6bner Basis after 15 variables' complexity. We refer to this phenomenon as the curse of dimensionality in computational algebraic geometry regarding SIP problems. We conjecture that the crux is that the number of ideal minimal generating elements will increase dramatically as the dimension increases, thus making Bunchberger's algorithm computationally costly.\n\nFinally, the crux of the problem, that two algebraic methods don't function when the variables' number exceeds 15, is the computation of the Gr\u00f6bner Basis of some specific ideals, specifically the toric ideal of the coefficient matrixA, unavoidable in each of the two algorithms. We have tried many applications specialized in algebraic calculations, for example, SageMath, Macaulay2, etc. Still, when the variables' number exceeds 15, usually the exponential of the generating elements of the polynomial ideal, which is required in the algorithm, is too great to be computable. For example, the computation of the Gr\u00f6bner Basis of the matrixA=(B, I)whereIdenotes the identity matrix andB = [  1  1  1  1  1  1  1  1  1  1;  1  2  3  4  5  6  7  8  9 10;  2  3  4  5  6  7  8  9 10 11;  3  4  5  6  7  8  9 10 11 12;  4  5  6  7  8  9 10 11 12 13;  5  6  7  8  9 10 11 12 13 14;  6  7  8  9 10 11 12 13 14 15;    ]involves the computation of the Gr\u00f6bner Basis of the toric ideal belowideal(x_8^2-x_7*x_9,\u00a0x_7*x_8-x_6*x_9, x_6*x_8-x_5*x_9, x_5*x_8-x_4*x_9, x_4*x_8-x_3*x_9,\u00a0x_3*x_8-x_2*x_9,\u00a0x_2*x_8-x_1*x_9,\u00a0x_1*x_8-x_0*x_9,\u00a0 x_7^2-x_5*x_9,\u00a0x_6*x_7-x_4*x_9, x_5*x_7-x_3*x_9, x_4*x_7-x_2*x_9, x_3*x_7-x_1*x_9,\u00a0x_2*x_7-x_0*x_9,\u00a0x_1*x_7-x_0*x_8, x_6^2-x_3*x_9,\u00a0x_5*x_6-x_2*x_9, x_4*x_6-x_1*x_9, x_3*x_6-x_0*x_9, x_2*x_6-x_0*x_8,\u00a0x_1*x_6-x_0*x_7,\u00a0 x_5^2-x_1*x_9,\u00a0x_4*x_5-x_0*x_9, x_3*x_5-x_0*x_8, x_2*x_5-x_0*x_7, x_1*x_5-x_0*x_6, x_4^2-x_0*x_8,\u00a0x_3*x_4-x_0*x_7, x_2*x_4-x_0*x_6, x_1*x_4-x_0*x_5, x_3^2-x_0*x_6,\u00a0x_2*x_3-x_0*x_5, x_1*x_3-x_0*x_4, x_2^2-x_0*x_4,\u00a0x_1*x_2-x_0*x_3, x_1^2-x_0*x_2,\u00a0 x_8*x_11*x_12*x_13*x_14*x_15*x_16-x_9, x_7*x_11*x_12*x_13*x_14*x_15*x_16-x_8, x_6*x_11*x_12*x_13*x_14*x_15*x_16-x_7,\nx_5*x_11*x_12*x_13*x_14*x_15*x_16-x_6, x_4*x_11*x_12*x_13*x_14*x_15*x_16-x_5,\nx_3*x_11*x_12*x_13*x_14*x_15*x_16-x_4, x_2*x_11*x_12*x_13*x_14*x_15*x_16-x_3,\nx_1*x_11*x_12*x_13*x_14*x_15*x_16-x_2, x_0*x_11*x_12*x_13*x_14*x_15*x_16-x_1,\nx_9*x_10*x_14*x_15^2*x_16^3-x_0*x_6*x_11^2*\u00a0x_12, x_8*x_10*x_14*x_15^2*x_16^3-x_0*x_5*x_11^2*\u00a0x_12, x_7*x_10*x_14*x_15^2*x_16^3-x_0*x_4*x_11^2*\u00a0x_12,\nx_6*x_10*x_14*x_15^2*x_16^3-x_0*x_3*x_11^2*\u00a0x_12, x_5*x_10*x_14*x_15^2*x_16^3-x_0*x_2*x_11^2*\u00a0x_12, x_4*x_10*x_14*x_15^2*x_16^3-x_0*x_1*x_11^2*\u00a0x_12,\nx_3*x_10*x_14*x_15^2*x_16^3-x_0^2*x_11^2*\u00a0x_12, x_0*x_5*x_11^3*x_12^2*x_13-x_9*x_10*x_15*x_16^2, x_0*x_4*x_11^3*x_12^2*x_13-x_8*x_10*x_15*x_16^2,\nx_0*x_3*x_11^3*x_12^2*x_13-x_7*x_10*x_15*x_16^2, x_0*x_2*x_11^3*x_12^2*x_13-x_6*x_10*x_15*x_16^2, x_0*x_1*x_11^3*x_12^2*x_13-x_5*x_10*x_15*x_16^2,\nx_0^2*x_11^3*x_12^2*x_13-x_4*x_10*x_15*x_16^2, x_2*x_10*x_13*x_14^2*x_15^3* x_16^4-x_0^2*x_11, x_1*x_10*x_12*x_13^2*x_14^3*x_15^4* x_16^5-x_0^2,\nx_10*x_11*x_12^2*x_13^3*x_14^4*x_15^5* x_16^6-x_0),In future work, we will study how the situation can be improved by  optimising the algorithm for computation of Gr\u00f6bner bases, for example, the Faug\u00e8reF_4<cit.> andF_5<cit.> algorithms.\n\n\n\n\n \u00a7.\u00a7 Stochastic network design\n\n\nWe have also tested our approach on a small stochastic network design problem, formulated as follows:\n\n    min   \u2211_a \u2208 A c_a x_a+1/N\u2211_i=1^N\u2211_c \u2208 C\u2211_a \u2208 A q_a c y_a c^i\n     s.t.    \u2211_a \u2208 A \n     a(0)=v y_a c^i-\u2211_a \u2208 A \n     a(1)=v y_a c^i=d_v, c^i  \u2200(v, c, i) \u2208 V \u00d7 C \u00d7{1, \u2026, N}\n       \u2211_c \u2208 C y_a c^i\u2264 u_a x_a  \u2200(a, i) \u2208 A \u00d7{1, \u2026, N}\n        x_a\u2208{0,1}, y_a c^i\u2208\n\n\nFollowing the same procedure as in Section <ref>, we compare our two algorithms with a commercial solver. Our instances concern a small problem, with three vertices and three arcs. Figure <ref> shows the time taken to compute the opportunity cost matrix for each approach.\n\nWe once more observe the superior efficiency of the kernel approach when the number of decision variables is small.\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nIn this paper, we have presented new algorithms for computing the opportunity cost matrix of <cit.> using algebraic methods using Gr\u00f6bner and Graver bases. We have also provided mathematical results showing the relationship of Graver bases for deterministic and stochastic integer programs. Our numerical results show that the Gr\u00f6bner basis is very efficient as the number of scenarios increases if the number of variables of the underlying integer program does not grow too fast. On the other hand, the Graver basis approach is interesting when there are a large number of variables.\n\nIn future work, we propose to include more recent methods of computing Gr\u00f6bner basis, such as the Faug\u00e8re algorithms. We will also study how to combine the algebraic approach with approximations such as Lagrangian cuts in order to increase further the number of decision variables that our approach can handle."}