{"entry_id": "http://arxiv.org/abs/2303.06842v1", "published": "20230313041642", "title": "Scene Graph Generation from Hierarchical Relationship Reasoning", "authors": ["Bowen Jiang", "Camillo J. Taylor"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\n\nScene Graph Generation from Hierarchical Relationship Reasoning\n    Bowen Jiang, Camillo J. Taylor\n\nUniversity of Pennsylvania\n\n\n{bwjiang, cjtaylor}@seas.upenn.edu\n\n\n\n\n\n\n\n\n\n\n\n    Received: date / Accepted: date\n==============================================================================================================\n\n\n\nempty\n\n\n\n\n\n\n\n\nThis paper describes a novel approach to deducing relationships between objects in a visual scene. It explicitly exploits an informative hierarchical structure that can be imposed to divide the object and relationship categories into disjoint super-categories. Specifically, our proposed scheme implements a Bayes prediction head to jointly predict the super-category or type of relationship between the two objects, along with the detailed relationship within that super-category. This design reduces the impact of class imbalance problems. We present experimental results on the Visual Genome and OpenImage V6 datasets showing that this factorized approach allows a relatively simple model to achieve competitive performance, especially on predicate classification and zero-shot tasks.\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nThis work considers the Scene Graph Generation problem proposed by Johnson et al. in \u00a0<cit.>. In this framing, the system's goal is to deduce the objects in a given image and the relationships between them. \n\nStandard object detection\u00a0<cit.> or segmentation\u00a0<cit.> algorithms focus on identifying each object instance in an image separately. Scene graph generation algorithms, in contrast, represent the whole scene as a directed graph. They regard each object instance as a node in the graph and capture possible relationships between each pair of instances as an edge. Triplets of a subject, a relationship, and an object, like <person, wear, skirt> and <car, on, road>, form essential components of scene graphs, and such regularly appearing structures in images are also called motifs\u00a0<cit.>. The rich knowledge encoded in scene graphs can be used for various downstream tasks, including image retrieval\u00a0<cit.> and visual scene understanding\u00a0<cit.>.\n\n\n\nZeller et al.\u00a0<cit.> observe that the categories associated with the subject and object instances in an image are strongly predictive of the relationship between them. For example, the relationship between a \u201cman\" and a \u201cshirt\" is likely to be \u201cwears\". The authors go further and show that when the objects in a dataset are divided into super-categories such as \u201cAnimal\", \u201cVehicle\", and \u201cFurniture\", and the relationships between objects are similarly divided into super-categories such as \u201cGeometric\", \u201cPossessive\", and \u201cSemantic\", the object categories and super-categories become strongly predictive of the relationship super-categories. For example, the relationship between a person and an article of clothing is likely to be possessive in nature. Based on this observation, the authors strongly argue for approaches that condition relationship predictions on the subject and object categories.\n\nWe build on this idea by proposing a new approach to scene graph construction, which directly exploits an informative hierarchical structure that can be imposed to divide the object and relationship categories into disjoint super-categories. We show that a relatively straightforward network that exploits this hierarchical structure can outperform state-of-the-art methods.\nSpecifically, we propose a network that considers each subject-object pair in an image and produces several outputs, including a scalar value indicating whether two objects are connected by a relationship, \nthe probability that the relationship, if it exists, belongs to each relationship super-category,\nand the conditional probability that the relationship has a specific class given the relationship super-category. The resulting network effectively captures a structure from the Bayes' rule where the probabilities associated with the super-categories serve to steer the attention of the network to the most profitable lines of interpretation.\nNumerical experiments demonstrate that our method HierMotif significantly improves performance, especially on the predicate classification tasks, by this form of hierarchical relationship reasoning.\n\nThe structure of this manuscript is as follows. Section\u00a0<ref> discusses some related work. Section\u00a0<ref> discusses details of our model, including the local Bayes predictor and an optional transformer encoder that can be used to refine the local results. Finally, Section\u00a0<ref> presents the experiments.\n\n\nTo summarize our contributions:\n\n\n\n    \n    \n  * We show that a simple scene graph generation model can achieve superior performance by leveraging object and relationship hierarchies.\n    \n\n    \n  * We propose a novel classification scheme inspired by Bayes' rule that jointly predicts the relationship super-category probabilities and the conditional probabilities of the relationships within each super-category.\n    \n\n    \n  * We propose a direction-aware multiplicative masking scheme for our classifier, and a dynamic batch sizing algorithm for efficient batch-wise training.\n    \n    \n    \n    \n    \n    \n    \n\n\n\n\n\n\u00a7 RELATED WORK\n\n\nOur work considers the scene graph generation problems proposed in <cit.> and <cit.>. Many authors have approached the problem from the graphical neural networks perspective\u00a0<cit.>. IMP <cit.> uses iterative message passing to update node and edge features in a bipartite graph. Graph-RCNN\u00a0<cit.> prunes unlikely relationships and uses an attentional graph convolution network\u00a0<cit.> to integrate global contexts and update nodes and edges. \n\nRecurrent neural networks\u00a0<cit.> have also played important roles in scene graph generation as a means for integrating global context information across the image. For example, node and edge features in <cit.> are represented by two GRUs\u00a0<cit.>, which take incoming messages and produce new hidden states. Neural Motifs\u00a0<cit.> investigates the repeated motif structures in scene graphs and captures global contexts with bidirectional LSTMs\u00a0<cit.>. \n\nIn their work on Neural Motifs, Zellers et al. <cit.> analyze scene graphs in the Visual Genome\u00a0<cit.> dataset and divide the 150 most frequently encountered object categories into 17 super-categories and the 50 most frequently encountered relationship categories into 3 relationship super-categories: geometric, possessive, and semantic, as illustrated in Table\u00a0<ref>. However, their proposed algorithm does not explicitly exploit the super-categories they identify. Our work follows their super-category classifications and fully utilizes this hierarchy information. We discover that the median relative frequency of a relationship label is only 0.17%, but when we divide them into super-categories, this median frequency increases to 0.79% (geometric), 1.4% (possessive), and 2.3% (semantic), respectively.\n\n\n\nIn recent years, BGT-Net\u00a0<cit.> utilizes a bidirectional GRU for instance-level communications, an object transformer encoder to predict instance categories, and another edge transformer encoder to generate edge information for every instance. HC-Net\u00a0<cit.> works on scene graph generation with hierarchical contexts, rather than the hierarchies of objects and relationships we exploit. GPS-Net\u00a0<cit.> focuses on recovering the direction of the relationship edges and understanding the relative priority of the nodes in the graph within a novel message-passing scheme. RTN\u00a0<cit.> presents a relation transformer network with a node-to-node encoder and a node-to-edge decoder. RelTR\u00a0<cit.> also employs a Detection Transformer (DETR)\u00a0<cit.> backbone, but it proposes a complicated triplet decoder network with three sub-modules and takes subject and object queries as input tokens. <cit.> establishes a unified conditional random field to model the distribution of objects and relationships jointly. Besides, <cit.> offers an energy-based framework instead of cross-entropy loss to incorporate the structure of the scene graph.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 METHOD\n\n\n\n\n\n\n \u00a7.\u00a7 Notation\n\nA scene graph G={V, E} is a graphical representation of a given image I. The set of vertices V consists of n object instances, including their bounding boxes {b_i |b_i \u2208\u211d^4}, the object categories {c_i | c_i \u2208\u2115}, and the object super-categories {sc_i | sc_i \u2208\u2115} for each c_i. We sometimes use the word \u201cinstances\" to cover \u201csubjects\" and \u201cobjects\". The set of edges, E, consists of relationships {r_ij| r_ij\u2208\u2115} between each pair of instances, and the associated relationship super-category {sr_i | sr_i \u2208\u2115}. Both i and j ranges from 1 to n, so the total number of possible r_ij is n^2. For every possible edge we predict a connectivity score {e_ij| e_ij\u2208 [0, 1]} which models whether a particular subject and object are connected.\n\n\n\n\n\n\n \u00a7.\u00a7 Scene Graph Construction\n\nOur basic scene understanding system in Figure\u00a0<ref> constructs a scene graph by considering each pair of object instances in turn and predicting whether or not they are related and, if so, what kind of relationship pertains. We refer to this as a local prediction system to distinguish it from other approaches that consider more global scene information.\n\n\n\n\n  \nObject detection backbone\nWe follow the most common two-stage model design\u00a0<cit.> to first predict object bounding boxes and labels by a pre-trained object detection backbone, and then predict relationships between objects. We adopt the DETR\u00a0<cit.> object detection module as the front end for our scene graph generation system. DETR has a ResNet-101 feature extraction backbone\u00a0<cit.>, followed by a transformer encoder\u00a0<cit.>. \nNext, a transformer decoder and a feed-forward head\u00a0<cit.> predict the final set of object predictions in parallel. Once DETR outputs instance categories, we associate them with their super-categories. Finally, MiDaS\u00a0<cit.> single-image depth estimation network is applied to the input image.\n\n\n\n  \nDirection-aware multiplicative masking After we have extracted the image features, we consider each pair of object instances as shown in Figure <ref>. Considering every permutation of instance pairs allows the system to cover all options. For every subject-object pair, we construct a combined feature tensor by converting the bounding boxes associated with the subject and object into binary masks, and then multiplying them elementwise with the image features to produce two feature tensors, one for the subject and one for the object, which is concatenated and passed on to the next stage. Note that the order of the two feature tensors is crucially important since motifs such as <bike, has, wheel> and <wheel, of, bike> hinge on which instance is considered the subject and which the object. \nGiven two object proposals i and j, we do two separate passes through our relationship network, one with i as the subject and j as the object, and the other with j as the subject and i as the object.\nWe avoid using a simple union mask of each subject-object pair, since the two bounding boxes may have significant overlap and the edge direction information would be lost.\nSome works crop instances by their regions of interest and resize them to a uniform size to form the object feature vectors\u00a0<cit.>. Our direction-aware multiplicative masking strategy maintains the instance size information, the spatial locations of each instance in the image, and the relative locations between each subject-object pair. Our ablation studies show that this approach significantly improves performance.\n\nThe subject-object feature tensors are passed to a network with 2 convolutional layers and 2 linear layers which produces a 512-dimensional hidden state vector X_ij as shown in Figure <ref>. This hidden state vector is passed to the Bayes prediction head along with 4 one-hot vectors that encode the categories and super-categories associated with the subject and object.\n\n\n\n\n\n\n  \nBayes prediction head\nThe Bayes prediction head is structured to produce several interpretable quantities, inspired by the Bayes' rule. The first output e_ij\u2208 [0, 1] is a scalar connectivity score, indicating the likelihood that object instances i and j are related. The network also produces three scalar values [p(g),  p(p),  p(s)], representing the probabilities that the relationship, if it exists, belongs to the geometric, possessive or semantic super-categories respectively. \nFinally, the network produces three vectors that are interpreted as conditional probability distributions: p(r\u0303_ij^g|g) \u2208^15 denotes the probability that the relationship should be labeled as each of the 15 categories under the geometric relationship super-category, conditioned on the relationship belonging to that super-category. Similarly, p(r\u0303_ij^p|p) \u2208^11 and p(r\u0303_ij^s|s) \u2208^24 denote the conditional probabilities associated with the possessive and semantic super-categories respectively.\n\nFigure\u00a0<ref> and Equations\u00a0<ref>-<ref> show how these quantities are combined to produce final classification results trainable via back-propagation. The output score vectors associated with each relationship category, r_ij^g\u2208^15, r_ij^p\u2208^11, and r_ij^s\u2208^15, are simply the conditional probability vectors multiplied by the associated super-class probabilities.\n\n    e_ij   = Sigmoid{X_ij^\u22a4W^conn}\n    \n    \t[p(g),  p(p),  p(s)]    = Softmax{X_ij^\u22a4 W^sc}\n    \n    \tr_ij^g := p(r\u0303_ij^g|g) * p(g)    = Softmax{X_ij^\u22a4 W^g} * p(g) \n    \n    \tr_ij^p := p(r\u0303_ij^p|p) * p(p)    = Softmax{X_ij^\u22a4 W^p} * p(p) \n    \n    \tr_ij^s := p(r\u0303_ij^s|s) * p(s)    = Softmax{X_ij^\u22a4 W^s} * p(s)\n\nThe Ws in these equations are the learnable linear layer parameter tensors.\n\nThe local prediction network finally outputs three relationship predictions, one for each super-category, by simply considering the maximum entry in each of the vectors r_ij^g, r_ij^p and r_ij^s.The associated maximum entries are multiplied by the connectivity score, e_ij, to produce a final score for each relationship hypothesis. These scores are used to rank order the relationship hypotheses from all subject-object pairs, and this ordering is used to suggest the most likely relationships in the scene.\nNote that in this structure, the super-category probabilities act to focus the network's attention on the appropriate conditional output heads. \n\nEquation\u00a0<ref>-<ref> can be summarized as follows:\n\n    [r_ij^g, r_ij^p, r_ij^s, e_ij] = BayesHead{X_ij}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \nDynamic batch sizing\n\nOne challenge in training our local prediction module efficiently stems from the fact that the images in the dataset typically contain different numbers of object instances and, therefore, contribute different numbers of ground-truth relationships to the training process. This can be a problem when we want to parallelize the training process with batch-wise operations.\n\nTo handle this problem, we propose a dynamic batch sizing scheme that can be effectively parallelized across multiple images in a training batch. \nThe dynamic batching process is described in Algorithm\u00a0<ref>, where max_index denotes the maximum number of objects detected in any image in the current batch.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Global context via motif-level attention\n\nThe local prediction scheme described in the previous sections constructs a scene graph by considering each pair of objects in the scene in turn and in isolation. \nWe propose an optional third stage with a transformer encoder\u00a0<cit.> to extract global context information, which can be used to refine the local prediction outputs.\n\nSince most scene graphs are sparse, we select a small subset S of hidden states {X_ij| X_ij\u2208 S} associated with the most highly scored relationship predictions, so that the model can focus its computational effort on the most informative relationship predictions.\n\nValues V, keys K, queries Q\u00a0<cit.> for all X_ij in S and the associated self-attention weights can be calculated from:\n\n    V = X,  K = X^\u22a4 W^K,  Q = X^\u22a4 W^Q,\n       Attention(Q, K, V)=SoftMax(Q K^\u22a4/\u221a(d)) V,\n\nwhere X is matrix constructed by concatenating all the {X_ij| X_ij\u2208 S}. W^V and W^K are two learnable matrices, and the scale factor d is used to stabilize the gradients\u00a0<cit.>. The dot-product Q K^\u22a4 between each pair of key and query produces the attention scores, representing how much attention each motif should pay to any other motifs. We also follow <cit.>'s suggestion on multi-head attention and add-and-norm layers. The resulting block is repeated several times to form the entire transformer encoder network.\n\nThe transformer encoder outputs a set of new embedding vectors {Y_ij| X_ij\u2208 S}. And these vectors are added to the corresponding X_ij as a residual connection to refine the relationship predictions. More specifically, for every relationship in S, the refined relationship is computed from:\n\n    [r\u0302_ij^g, r\u0302_ij^p, r\u0302_ij^s, e_ij] = BayesHead{X_ij + Y_ij}.\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n\n\n\n\n\n\n \u00a7.\u00a7 Dataset\n\n\n\n  \nVisual Genome We perform comprehensive experiments on the Visual Genome dataset\u00a0<cit.>, following the same pre-processing procedure used in\u00a0<cit.> to clean up object annotations. We consider the most frequent 150 object categories and 50 relationship predicates in the dataset. These are divided into 17 object super-categories and 3 relationship super-categories as defined in\u00a0<cit.>. The dataset contains around 75.7k training images and 32.4k testing images. On average, each image in the dataset contains 11.5 object instances and 6.2 relationships.\n\n\n\n  \nOpenImage V6 We follow the same data pre-processing provided by <cit.> to obtain a subset of the OpenImage V6 dataset\u00a0<cit.> with relationship annotations. It has 601 object categories, 30 relationship categories, about 53.9k training images, and around 3.2k testing images.\n\n\n\n \u00a7.\u00a7 Evaluation metrics\n\n\n\n  \nVisual Genome We employ the Recall@k (R@k)\u00a0<cit.> and mean Recall@k (mR@k)\u00a0<cit.> evaluation metrics that are widely used for this dataset. R@k measures the fraction of the ground-truth predicates that appear in the top k most confident predictions per image, the mR@k metric averages the recall score over all 50 relationship categories and, thus, provides an idea of how well the method performs across all of the relationship categories. A ground truth predicate matches a hypothesized relationship if the predicted relationship is correct, the subject and object labels match, and the bounding boxes associated with the subject and object both have IOU\u2265 0.5 with the ground-truth boxes.\n\n\n\nIn our hierarchical relationship scheme, each edge has three predictions per direction under three disjoint super-categories. Therefore, each directed edge outputs three individual candidates to be ranked in the top k most confident predictions instead of one, as shown in Figure\u00a0<ref>. Note that allowing three candidates per edge does not make the ranking problem easier, because the total number of candidates will be three times larger, and it is still challenging to put correct predicates in the top k ranking.\n\nWe also provide two more evaluation metrics, R@k^* and mR@k^*, which offer more insight into the accuracy of the prediction heads associated with each relationship super-category.\nFor these metrics, if any of the three super-category output heads, r_ij^g, r_ij^p or r_ij^s, correctly predicts the relationship, we score it as a match.\nWe contrast this with the NG-R@k described in \u00a0<cit.>, where \u201cNG\" represents \u201cno graph constraint\". This score significantly surpasses R@k, but it loosely defines a \u201cmatch\" as long as any of the 50 predicates with non-zero scores match with the target without further restrictions. In contrast, our R@k^* is still quite strict, since predicates within the same super-category remain exclusive. In other words, each relationship hypothesis would have at most three chances to be marked correct.\n\nZero-shot recall (zsR@k)\u00a0<cit.> estimates the model's ability to generalize its performance to unseen data. It calculates the R@k score for those <subject, relationship, object> triplets that only appear in test data, but not in the training dataset.\n\n\n\n  \nOpenImage V6 On this challenge we consider the standard evaluation metrics: Recall@50 (R@50), weighted mean average prevision of relationships (wmAP_rel), weighted mean average prevision of phrases (wmAP_phr), and the final score=0.2 \u00d7 R@50+ 0.4 \u00d7wmAP_rel + 0.4 \u00d7wmAP_phr. The weighted AP addresses the predicate class imbalance\u00a0<cit.>. wmAP_rel counts a match if both the subject and the object bounding boxes have IOU\u2265 0.5 with their ground-truths, and the <subject, relationship, and object> labels match with the target. Same for wmAP_phr except it considers the union bounding box of the subject and object.\n\n\n\n\n\n \u00a7.\u00a7 Evaluation modes\n \n\n\n\n\n\n\n\n  \nPredicate classification\nPredicate classification (PredCLS) separates the relationship prediction task from other procedures. Given a ground-truth set of object instances labels and bounding boxes, the model predicts relationships.\n\n\n\n  \nScene graph classification In this task the model predicts subject, object, and relationship categories, given ground-truth subject and object bounding boxes. In our experiment, the DETR detection backbone outputs object bounding boxes and labels simultaneously without a separate region proposal network\u00a0<cit.>, so we match each ground-truth bounding box with the predicted bounding box with the largest IOU and use the predicted object label.\n\n\n\n  \nScene graph detection\nScene graph detection (SGDET) requires the model to predict instance bounding boxes, object labels, and relationships. \nSGDET requires additional adaptations to the detection backbone because the predicted bounding boxes and labels are noisy and can degrade the model's performance. For example, we add per-class NMS post-processing to avoid many unnecessary relationship predictions and dramatically reduce computation time. We also dismiss all subject-object pairs whose bounding boxes have no overlap\u00a0<cit.>. Predicted instance categories are often inaccurate. The prediction will be regarded as wrong if the subject or object category is wrong, regardless of the relationships. To combat this, we duplicate the predicted bounding boxes twice in each image and consider the top two most confident object categories to improve the likelihood of detecting the correct relationships. Furthermore, we account for the fact that some object categories are ambiguous. For example, if the dataset annotates an instance as \u201capple\", but the detector predicts it as \u201cfruit\", the result should still be accepted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Training details\n\nAll models are trained on 4 NVIDIA V100 GPUs. The time to run inference on a typical image is around 1.5s. We leverage the pretrained DETR backbone provided by\u00a0<cit.> under object detection tasks. The relationship prediction network is trained with a batch size of 32 for 3 epochs on Visual Genome, or 1 epoch on OpenImage V6, by the SGD optimizer with a learning rate of 0.002 and a step scheduler. We freeze the parameters of the DETR object detector while training the local predictor module. The optional transformer encoder is trained with the same batch size for 3 more epochs by the AdamW\u00a0<cit.> optimizer with a learning rate of 0.005 and warmup steps \u00a0<cit.>, having other module parameters being frozen.\n\n\n\n \u00a7.\u00a7 Results\n\n\n\n  \nVisual Genome Table\u00a0<ref> compares our results on Visual Genome to other state-of-the-art methods. Our model, HierMotif, significantly outperforms previous methods on R@50 and R@100 for the PredCLS task by 6 and 9 percent, respectively. The penultimate row of each sub-table reports our R@k^* and mR@k^* that highlight the strong performance of the super-category prediction subsystems. These results show that the individual Bayes prediction heads have successfully learned how to predict the relationships within each super-category. The strong mR@k scores indicate improved performance across all relationship categories. Furthermore, it suggests that the scheme allows the specialized prediction heads to concentrate more effectively on discriminating within their relationship super-categories.\n\n\n\n\nFigure\u00a0<ref> shows some generated scene graphs for the PredCLS task on Visual Genome. The model generates a diverse set of predicates with high confidence to describe the visual scenes. We note that existing evaluation metrics may severely underestimate the model's actual capabilities due to insufficient ground-truth annotations in the dataset. The model produces many reasonable predictions that align with human intuition but are not annotated in the dataset. They are marked with blue arrows in the figure. We believe generating a rich set of reasonable predicates is helpful in practical scene understanding.  \n\nThe model sometimes gets confused on repeated instances of the same type. For example, it sometimes connects one cow to another nearby cow's tail. \nWe have also noticed cases in the dataset where the relationship between a subject and an object may be ambiguous; for example, a particular relationship may be annotated as <tail, of, cow> or <tail, on, cow>. Typically the two contending relationships belong to different super-categories, which our model handles quite naturally by reporting that two super-categories have likely predictions for an edge. \n\nTable\u00a0<ref> shows the strong zero-shot performance of our model on Visual Genome. The bottom row shows an ablation study where we do not use the Bayes prediction head and observe an inferior performance.\n\n\n\n\n  \nOpenImage V6 Table\u00a0<ref> demonstrates the experimental results on the OpenImage V6 dataset. It has an outstanding recall score, similar to the results on Visual Genome. Our model makes predictions on all subject-object pairs and outputs the top 20 most confident predictions for each image, so its precision wmAP_rel still has space to improve, but wmAP_phr and the final score are already competitive.\n\n\n\n\n\n\n\n  \nAblation studies \nWe provide ablation studies on Visual Genome in Table\u00a0<ref> to validate our key design decisions. Firstly, we investigate how our system would fare if we simply used a flat relationship categorization, instead of the hierarchical organization into three relationship super-categories with the Bayes prediction head. The performance is shown on the second row of the table, and it is much worse than the hierarchical baseline. Suggests that our hierarchical organization is improving performance, allowing our on-purposely simple relationship prediction scheme to perform competitively to other methods.\n\nThe third row shows the results if we ignore the super-categories associated with the object instances. It shows that object super-categories are useful, but not as critical as the relationship super-categories. Nevertheless, the decreases in mR@k suggest that the object super-category information helps share information across related classes and thus improves the performance, especially when the object categories are less frequently seen. The fourth row indicates that the image depth maps also help with the predictions.\n\nIn the fifth row, we examine the performance of a variant that does not use the direction-aware multiplicative masking scheme to extract subject and object features, but instead uses union bounding boxes. This variant does not perform as well as our baseline, indicating that the direction-aware scheme is beneficial.\n\nThe fifth and sixth rows investigate the value of adding the optional transformer structure with two slightly different parameter settings to refine the relationship predictions. These results indicate that this additional structure improves mR@k and mR@k^* but sacrifices a small amount of performance on the R@k and R@k^*. We conclude that our standard local prediction system is already strong and hypothesize that the DETR object detection backbone may already provide some global context via its transformer network.\n\nThe final several rows of the table show results obtained by varying various hyperparameters related to the SGDET task, specifically by changing the number of times we duplicate each predicted bounding box, as explained in Section\u00a0<ref>. We also show the result of varying the amount of non-maximal suppression applied to the bounding boxes produced by the DETR system during object detection.\n\n\n\n\n\u00a7 CONCLUDING REMARKS\n\n\nWe present a simple but powerful scene graph generation algorithm that exploits a hierarchical categorical structure imposed on the relationship and object labels.\nThe resulting system has a Bayesian structure with separate heads that predict the likelihood of a relationship between a pair of objects, the super-category of that relationship if it exists, and the specific relationship within each super-category. This representation effectively factorizes the final probability distribution over the relationship categories.\n\nOur results show that the hierarchical organization significantly outperforms a baseline that predicts a flat softmax over all relationship types. Our scheme performs competitively with state-of-the-art algorithms, especially in predicate classification and zero-shot learning. We hypothesize that this structure\n allows related labels to more effectively share the information common to each super-category.\n\nWe note that the proposed hierarchical approach could also be applied to other classification tasks and we plan to explore this possibility in future work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nieee_fullname\n\n\n\n"}