{"entry_id": "http://arxiv.org/abs/2303.07224v1", "published": "20230313155815", "title": "Efficient Semantic Segmentation by Altering Resolutions for Compressed Videos", "authors": ["Yubin Hu", "Yuze He", "Yanghao Li", "Jisheng Li", "Yuxing Han", "Jiangtao Wen", "Yong-Jin Liu"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\n\nEfficient Semantic Segmentation by Altering Resolutions for Compressed Videos\n    Yubin Hu1  \n    Yuze He1  \n    Yanghao Li1  \n    Jisheng Li1  \n    Yuxing Han2  \n    Jiangtao Wen3  \n    Yong-Jin Liu1Corresponding author.\n\n    1Department of Computer Science and Technology, Tsinghua University \n\n    2Shenzhen International Graduate School, Tsinghua University  \n\n    3Eastern Institute for Advanced Study\n\n    {huyb20, hyz22, liyangha18}@mails.tsinghua.edu.cn, jas0n1ee@icloud.com, \n\n    yuxinghan@sz.tsinghua.edu.cn, jtwen@eias.ac.cn, liuyongjin@tsinghua.edu.cn\n\n    March 30, 2023\n=======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\nVideo semantic segmentation (VSS) is a computationally expensive task due to the per-frame prediction for videos of high frame rates. In recent work, compact models or adaptive network strategies have been proposed for efficient VSS. However, they did not consider a crucial factor that affects the computational cost from the input side: the input resolution. In this paper, we propose an altering resolution framework called AR-Seg for compressed videos to achieve efficient VSS.  \nAR-Seg aims to reduce the computational cost by using low resolution for non-keyframes. To prevent the performance degradation caused by downsampling, we design a Cross Resolution Feature Fusion (CReFF) module, and supervise it with a novel Feature Similarity Training (FST) strategy. Specifically, CReFF first makes use of motion vectors stored in a compressed video to warp features from high-resolution keyframes to low-resolution non-keyframes for better spatial alignment, and then selectively aggregates the warped features with local attention mechanism. Furthermore, the proposed FST supervises the aggregated features with high-resolution features through an explicit similarity loss and an implicit constraint from the shared decoding layer. \nExtensive experiments on CamVid and Cityscapes show that AR-Seg achieves state-of-the-art performance and is compatible with different segmentation backbones. On CamVid, AR-Seg saves 67% computational cost (measured in GFLOPs) with the PSPNet18 backbone while maintaining high segmentation accuracy. Code:  <https://github.com/THU-LYJ-Lab/AR-Seg>.\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nVideo semantic segmentation (VSS) aims to predict pixel-wise semantic labels for each frame in a video sequence. In contrast to a single image, a video sequence is a series of consecutive image frames recorded at a certain frame rate (usually 25fps or higher). Applying image-based segmentation methods <cit.> to a video\nframe by frame consumes considerable computational resources.\nTo improve the efficiency of VSS, existing methods mainly focus on the design of network architectures. A class of methods proposes compact and efficient image-based architectures to reduce the computational overhead per-frame <cit.>. Another class of methods applies a deep model to keyframes and a shallow network for non-keyframes to avoid the repetitive computation <cit.> for videos. \n\n\n\nThe work presented in this paper is based on an important observation: the above existing works ignored a crucial factor that affects the computational cost from the input side: the input resolution. For image-related tasks, the input resolution directly determines the amount of computation, e.g., the computational cost of 2D convolution is proportional to the product of image width and height. Once we downsample the input frame by 0.5\u00d70.5, the computational overhead can be reduced by 75%. On the other hand, decreasing resolution often leads to worse segmentation accuracy due to the loss of information <cit.>. \nIn this paper, we propose to prevent the accuracy degradation by using temporal correlation in the video.\nSince the contents of video frames are temporally correlated, the local features lacking in low-resolution (LR) frames can be inferred and enriched by finding correspondences in sparse high-resolution (HR) reference frames based on motion cues. In a compressed video, the motion vectors contain such motion cues and can be obtained along with the video frames from video decoding at almost no additional cost.\n\n\n\nMotivated by the above observation, we propose an altering resolution framework for compressed videos, named AR-Seg, to achieve efficient VSS. As shown in Figure <ref>fig:ar-seg, AR-Seg uses an HR branch to process keyframes at high resolution and an LR branch to process non-keyframes at low resolution. \nIn particular, to prevent performance drop caused by downsampling, we insert a novel Cross Resolution Feature Fusion (CReFF) module into the LR branch and supervise the training with a Feature Similarity Training (FST) strategy to enrich local details in the LR features.\nCReFF fuses the HR keyframe features into LR non-keyframe features in two steps: 1) Align the spatial structures of features from different frames by feature warping with motion vectors, which can be readily obtained from compressed videos at almost no additional cost; 2) Selectively aggregate the warped features (which may be noisy after warping) into LR features with the aid of local attention mechanism. Since local attention assigns different importance to each \nlocation in the neighborhood, it is an effective way to avoid misleading by noisy warped features.\n\nFurthermore, our proposed FST strategy guides the learning of the CReFF aggregated features.\nFST consists of an explicit similarity loss (between the aggregated features and HR features inferred from non-keyframes) and an implicit constraint from the shared decoding layer across the HR and LR branches. Such a training strategy helps the LR branch to learn from features extracted from the HR branch, which is reliable and effective. Integrated with CReFF and FST, AR-Seg efficiently compensates for the accuracy loss of LR frames.\nOverall, AR-Seg significantly reduces the computational cost of VSS by altering input resolutions, while maintaining high segmentation accuracy.\n\nTo sum up, we make three contributions in this paper: 1) We propose an efficient framework for compressed videos, named AR-Seg, that uses altering input resolution for VSS and significantly reduces the computational cost without losing segmentation accuracy. 2) We design an efficient CReFF module to prevent the accuracy loss by aggregating HR keyframe features into LR non-keyframe features. 3) We propose a novel FST strategy that supervises the LR branch to learn from the HR branch through both explicit and implicit constraints. Experiment results demonstrate the effectiveness of AR-Seg with different resolutions, backbones, and keyframe intervals. On both CamVid <cit.> and Cityscapes <cit.> datasets, compared to the constant-resolution baselines, AR-Seg reduces the computational cost by nearly 70% without compromising accuracy. \n\n\n\n\n\u00a7 RELATED WORKS\n\n\n\nAs a fundamental task of scene understanding, semantic segmentation has been an active research area for many years <cit.>, which also attracts considerable attention in the study of deep neural networks, e.g., FCN <cit.>, DeepLabs <cit.> and PSPNet <cit.>. In order to obtain accurate results in real-time applications, several methods have been proposed to improve the efficiency of semantic segmentation, which we summarize as follows. \n\nEfficient Image Segmentation Methods. Many compact architectures have been proposed for efficient image segmentation. DFANet <cit.> adopted\na lightweight backbone to reduce computational cost and designed cross-level aggregation for feature refinement. DFNet <cit.> utilized a partial order pruning\nalgorithm to search segmentation models for a good trade-off between speed and accuracy. ICNet <cit.> used a cascade fusion module and transformed part of the computation from high-resolution to low-resolution. Wang et al. <cit.> designed super-resolution learning to improve image segmentation performance. BiSeNets <cit.>\nused two-stream paths for low-level details and high-level context information, respectively. ESPNet <cit.> used an efficient spatial pyramid to accelerate the convolution computation. These efficient backbone networks reduce the computational burden of single-image segmentation, and can be applied to temporal or spatial frameworks in VSS.\n\n\nTemporally Correlated Video Segmentation. Another group of methods focus on utilizing temporal redundancy in videos. They proposed various mechanisms that propagate the deep features extracted from keyframes to reduce the computation for non-keyframes. Clockwork <cit.> directly reused the segmentation result from keyframes, while Mahasseni et al. <cit.>\ninterpolated segmentation results in the neighborhood. Noticing the lack of information from non-keyframes, Li et al. <cit.> extracted shallow features from non-keyframes, and fused them into the propagated deep features by spatially variant convolution. \nTo compensate for the spatial misalignment between video frames, Zhu et al. <cit.> and Xu et al. <cit.> warped the intermediate features from keyframes by optical flow to produce segmentation results for non-keyframes. Jain et al. <cit.> fused the shallow features of non-keyframe into the warped features, and decoded them into better results. With global attention mechanism, TD-Net <cit.> aggregated the features from different time stamps and replaced the deep model with several shallow models distributed across the timeline. All the above methods mainly reduced the depth of backbone networks, but neglected the factor of input resolution considered in this paper. \nInstead of processing the image frames as a whole, Verelst et al. <cit.> split the frame into blocks and chose to copy or process them by a policy network. This block-based method reduces computational overhead from the spatial dimension, but lacks global information on non-keyframes. Kim et al. <cit.> \nattempted to improve efficiency by reducing resolution. But they directly used the LR segmentation results, thus suffering from severe performance degradation. Compared to these methods, our proposed AR-Seg keeps the global information of LR non-keyframes, and enhances LR frames by selectively aggregating intermediate features from HR keyframes. \n\nCompressed-Domain Video Analysis. Compressed video formats have been recently utilized in computer vision tasks. The motion vectors and residual maps are treated as additional modalities and directly fed into networks for video action recognition <cit.> and semantic segmentation <cit.>.\nSuch motion information also helps compensate for the spatial misalignment of features from different frames. Wang et al. <cit.> leveraged motion vectors to warp features in previous frames for object detection. Fan et al. <cit.> conditionally executed the backbone network for pose estimation depending on the residual values. For VSS, several methods have been proposed for efficient segmentation in the compressed domain. Jain et al. <cit.> warped the former and latter keyframe features using motion vectors, and predicted the non-keyframe features by interpolation. Tanujaya et al. <cit.> warped the results of keyframe segmentation for non-keyframes, and refined the warped segmentation by guided inpainting. Feng et al. <cit.> replaced a block of warped features with a local non-keyframe feature patch for further refinement. These methods reduced the computational cost of VSS, but suffered from performance degradation due to the limited capability of their feature refinement modules designed for non-keyframes.\nIn our proposed AR-Seg, the warped features are refined by local attention mechanism according to the LR features of non-keyframes. Our attention-based refinement selectively aggregates the warped features and effectively suppresses the noise in motion vectors, achieving good segmentation accuracy with little computational overhead.\n\n\n\n\n\u00a7 METHOD\n\n\n\n\nIn order to achieve efficient VSS for compressed videos, we propose an altering resolution framework named AR-Seg (Section <ref>). AR-Seg uses two branches to process HR keyframes and LR non-keyframes in the video separately. To compensate for the information loss due to downsampling the non-keyframes, we design a novel CReFF module that aggregates HR keyframe features into LR non-keyframe features (Section <ref>). To guide the learning of aggregated features, we further propose a novel feature similarity training (FST) strategy containing both explicit and implicit constraints (Section <ref>).\n\n\n\n\n \u00a7.\u00a7 AR-Seg Framework\n\n\n\nFor image/video semantic segmentation tasks, the input resolution directly determines the amount of computation, no matter what type of algorithms are applied, e.g., Conditional Random Fields, CNNs, and Transformers. Although reducing resolution has been studied in the context of video recognition <cit.>, altering resolution in dense prediction tasks like VSS remains unexplored. Based on this observation, we design the AR-Seg framework for VSS, which inputs video frames with altering resolutions; i.e., in AR-Seg, only a few keyframes are processed at high resolution to preserve fine details, while other non-keyframes are processed at low resolution to reduce the computational cost. \n\nTo identify the keyframes, we make use of the frame structure in a group of pictures (GOP) encoded in compressed videos <cit.>. A GOP includes L consecutive frames of three types: I frame, P frame, and B frame. \nI frames are encoded in intra mode, while P and B frames are encoded in inter mode that computes motion vectors for motion compensation. \nIn each GOP, we treat the first I frame as a keyframe and process it at high resolution. The remaining L-1 frames in GOP are non-keyframes and processed at low resolution. To simplify the description of our method, following the previous works <cit.>, we only consider the GOP structure without B frames in this section. The full treatment including B frames is presented in Appendix A4.\n\nDue to the domain gap between images with different resolutions, it is difficult for a single network to extract features suitable for both HR and LR resolution images. Thus we design two branches with shared backbone architecture in the model and train them separately for each resolution. \nFigure <ref>fig:ar-seg\nsummarizes the proposed AR-Seg framework. It consists of two branches: an HR branch for keyframes and an LR branch for non-keyframes. \n\nThe HR branch adapts an image segmentation network, which consists of a feature sub-network N_feat, a task sub-network N_task, and a final 1x1 convolutional layer[We follow the modular division of backbone networks in <cit.>. Two backbones are discussed and compared in Section <ref>.]. It predicts segmentation results in high-resolution and simultaneously provides intermediate features before the final convolution as a reference for the LR branch. \nThe LR branch is equipped with the same backbone network as the HR branch. To prevent performance degradation caused by downsampling, we design a CReFF module that aggregates HR features of the keyframe into the LR branch and place it before the final convolution layer of the backbone network. \nCReFF aggregates the HR reference features into the extracted LR features, yielding estimated HR features for the non-keyframes, which are further converted into pixel-wise semantic labels by the final convolution layer.\n\n\nAs illustrated in Figure <ref>,\ndifferent from the previous Accel framework <cit.>, AR-Seg performs feature fusion before the last 1x1 convolution layer, instead of before the task sub-network N_task. The reason is twofold: 1) Since feature maps before the final convolution have basically the same spatial layout as the input images and segmentation outputs, we can utilize motion vectors to compensate for the spatial misalignment of features at such position; and \n2) As the CReFF module actually upsamples the LR features, such a placement allows almost all convolution layers to benefit from the low resolution, thus reducing most of the computational cost.\n\n\n\n\n \u00a7.\u00a7 CReFF:  Cross Resolution Feature Fusion\n\n\nIn the AR-Seg framework, CReFF aims to prevent the performance degradation caused by the lack of fine local details in LR non-keyframes.\nUnlike a single image, video frames are intrinsically temporally correlated, so missing details in LR non-keyframes can be retrieved from the corresponding regions in HR keyframes according to motion cues. Motion vectors (MVs) in the compressed video exactly provide such motion cues at the block level, i.e., pixels inside a macroblock share the same motion vector. Almost all mainstream video compression standards use motion vectors for inter-prediction, including H.26x series <cit.>, AOMedia series <cit.> and AVS series <cit.>. Such block-wise MVs are readily available in compressed videos and can be used to assist the LR branch.\n\n\nSpecifically, as depicted in Figure <ref>, the HR branch of AR-Seg extracts the feature F_I\u2208\u211b^C\u00d7 H \u00d7 W from an I frame, and the LR branch extracts the feature f_P\u2208\u211b^C\u00d7 h \u00d7 w from a P frame. \nAlthough P frames are processed in low resolution, CReFF takes F_I, M_P, and f_P as input to generate the aggregated feature F\u0303_P, where M_P \u2208\u211b^2\u00d7 H \u00d7 W denotes the MVs from P frame to I frame. The two channels of M_P correspond to x and y dimensions of motion vectors, denoted by c_x and c_y.\nInside the CReFF module, the MV-based feature warping operation \ud835\udcb2_MV firstly warps F_I to the spatial layout of the P frame, which can be formulated as per-pixel shifting: \n\n    F\u0302_I^(x,y) = F_I^(x+M_P^(c_x,x,y),y+M_P^(c_y,x,y)),\n\nwhere F\u0302_I \u2208\u211b^C\u00d7 H \u00d7 W denotes the warped HR feature that will be further fused into LR features. \n\nDue to the coarse-grained MVs (block-level instead of pixel-level) and the varying occlusion relationships across video frames, the warped features F\u0302_I are often noisy and misleading. \nInspired by the success of non-local operation <cit.> and attention mechanism <cit.> in video-based applications, we propose to assign different fusion importance weights to the (x,y) locations in noisy features F\u0302_I by attention mechanism.\nSince F\u0302_I is roughly spatially aligned to f_p after warping, we use local attention to efficiently fuse the features as follows. \n\nIn the local-attention-based feature fusion module \u2131_LA, \nwe firstly generate the Value and Key feature maps from the warped HR features F\u0302_I, and the Query maps from the upsampled LR features F\u0305_P. The 3 \u00d7 3 grouped convolution \ud835\udc02\ud835\udc28\ud835\udc27\ud835\udc2f_g with groups=C is selected to efficiently encode the feature maps into attention representations V_I , K_I, Q_P \u2208\u211b^C\u00d7 H \u00d7 W. Note that attention representations share the same channel size as the intermediate features.\nDenote the n\u00d7 n neighborhood centered at (x,y) as Nbhd_(x,y). Within Nbhd_(x,y), the output of local attention A_P at the position (x,y) is generated by \n\n    A_P^(x,y) = V_I^Nbhd_(x,y)\ud835\udcae (K_I^Nbhd_(x,y), Q_P^(x,y)),\n\nwhere A_P^(x,y), Q_P^(x,y)\u2208\u211b^C\u00d7 1 denote the feature vectors at the position (x,y) of A_P and Q_P respectively, and V_I^Nbhd_(x,y), K_I^Nbhd_(x,y)\u2208\u211b^C \u00d7 n^2 are the re-arranged feature vectors within Nbhd_(x,y) in V_I and K_I, respectively. The similarity operation \ud835\udcae(K, Q) is formulated as \n\n    \ud835\udcae(K, Q) = \ud835\udc46\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 (K^T Q/\u221a(C)).\n\nFurthermore, the aggregated feature F\u0303_P for P frame is obtained in a residual fashion:\n\n    F\u0303_P = F\u0305_P + A_P = \ud835\udc48\ud835\udc5d\ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52(f_P) + A_P.\n\n\nIn summary, using the CReFF module, the feature details from the I frame are firstly aligned to the P frame, and then aggregated into the LR branch according to the pixel-wise similarity between Q_P and K_I. The verification of the architecture design of CReFF is presented in Section <ref>. The reader is referred to Appendix A1 for more details on the visualization of attention weights in \u2131_LA.\n\n\n\n\n \u00a7.\u00a7 FST:  Feature Similarity Training\n\n\n\nIn order to effectively train the CReFF module, we propose a feature similarity training (FST) strategy. FST utilizes the HR features of P frame F_P (extracted from the HR branch) to guide the learning of the aggregated features F\u0303_P in the LR branch.\nSince F_P contains sufficient details to produce high-quality segmentation results, CReFF can learn how to aggregate F\u0305_P and F\u0302_I into effective HR features from it under the supervision of FST. \nSpecifically, FST supervises the training process of the LR branch both explicitly and implicitly in the following ways.\n\nThe explicit constraint is to use the feature similarity loss \u2112_fs. We use mean square error (MSE) to measure the difference between F\u0303_P and F_P, which serves as an additional regularization for the LR model:\n\n    \u2112 = \u2112_seg + \u2112_fs = CE(S_P, G_P) + MSE(F\u0303_P,F_P),\n\nwhere S_P\u2208\u211b^C_out\u00d7 H \u00d7 W denotes the segmentation result produced by LR branch, G_P \u2208\u211b^H \u00d7 W denotes the ground-truth segmentation of P frame and the segmentation loss \u2112_seg is the standard cross entropy loss CE(S_P,G_P). \n\nThe implicit constraint of FST is the shared decoding layer of F\u0303_P and F_P. In the segmentation backbone model trained on the HR images, the final convolution layer acts as the segmentation decoder, which contains deep semantic information about high-quality HR features. To utilize such information, we directly transfer the final 1\u00d71 convolution layer of the HR branch to the LR branch with fixed parameters. Since the parameters are trained on HR features, they produce better segmentation results S_P when F\u0303_P is closer to the HR feature F_P. \n\n\n\n\nIn summary, with the explicit and implicit constraints, FST effectively transfers the knowledge of HR features from the HR branch to the LR branch, enabling high-quality segmentation based on the aggregated features of CReFF.\nFigure <ref> shows the overall training strategy for the LR branch. The HR I frame provides the features F_I for feature fusion in CReFF, and the HR P frame provides the features F_P for the explicit supervision in FST. Parameters of the LR branch are trained using the total loss \u2112 via backpropagation, with fixed parameters of the HR branch and the shared final convolution layer. \n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\nWe evaluate the proposed AR-Seg framework on CamVid <cit.> and Cityscapes <cit.> datasets for street-view video semantic segmentation. Below we present experiments to demonstrate the efficiency of AR-Seg and its compatibility with different backbone models, resolutions, and video compression configurations.\n\n\n\n \u00a7.\u00a7 Experimental Setup\n\n\nDatasets & Pre-processing. \nThe CamVid <cit.> dataset consists of 4 videos of 720\u00d7960 resolution captured at 30 fps, with semantic annotations at 1Hz and, in part, 15Hz. The Cityscapes <cit.> dataset contains street view videos of 1024\u00d72048 resolution captured in 17 fps, from which 5,000 images are densely annotated. We use the official train/validation/test split for both datasets. Following the previous works <cit.>, we select 11 and 19 classes for training and evaluation on these two datasets, respectively.\n\nTo simulate a real video compressed scenario, we compress the videos at reasonable bit-rates of 3Mbps for CamVid and 5Mbps for Cityscapes with the HEVC/H.265 <cit.> standard. The reader is referred to Appendix A3.1 for the detailed pre-processing steps.\n\n\n\n\n\n\n\nModels & Baselines. \nTo demonstrate the compatibility of AR-Seg with different backbones, we select two representative image segmentation models in our experiments: PSPNet <cit.> and BiseNet <cit.>, which is similar to settings in the previous work <cit.>. The former is a widely used classical model, and the latter is a lightweight model that achieves state-of-the-art performance.\nWe use AR^\u03b1- as the prefix of AR-Seg with specified backbone networks, where \u03b1 denotes the downsample scale for the LR branch. \n\nTraining & Evaluation Details. \nGiven a GOP length L, we train the LR branch with image pairs (i, p), where p refers to the P frame with annotation and i = p-(L-1) refers to the I frame as a reference. We denote the distance between \nthe annotated and the reference frames as d, then d=L-1 for the training pairs. \n\nFor evaluation, we test AR-Seg with different distances d between the target frame p and the reference keyframe i. For d = 0, we treat frame p as the keyframe and process it by the HR branch. Otherwise, we feed frame p into the LR branch for d \u2208 (0,L-1]. The average of \ud835\udc5a\ud835\udc3c\ud835\udc5c\ud835\udc48_\ud835\udc51 for each distance d is reported as the mIoU result. \nWe measure FLOPs by PyTorch-OpCounter <cit.> following the previous methods <cit.>. \nAll the comparisons are evaluated on the compressed videos. More training and evaluation details are presented in  Appendix A3.2 and A3.3.\n\n\n\n\n\n\n \u00a7.\u00a7 Experiment Results\n\n\n\nComparison with image-based methods. We first compare AR-Segs (with PSPNet <cit.> and BiseNet <cit.> as backbone) to their image-based counterparts of 1.0x resolution. \nAs shown in Table <ref>, on both CamVid and Cityscapes datasets, the proposed AR^0.5- models achieve on-par or better performance than the 1.0x resolution baselines while saving \u00a067% computational cost. Different from the low-resolution baselines that lead to significant performance degradation, AR-Seg successfully preserves the segmentation accuracy with the help of the CReFF module and the FST strategy. More comparisons between AR-Seg and the LR baselines under different resolutions are presented in Appendix A2.1.\nFurthermore, these experiment results with PSPNet and BiseNet also demonstrate the compatibility of AR-Seg for different backbone networks. \n\n\nComparison with video-based methods. Taking temporal coherence into consideration, we compare AR-Seg with the recent state-of-the-art video-based methods for efficient VSS. Besides the accuracy and computational cost, we also follow the previous work <cit.> to report the relative changes compared to their single-frame backbone models. Specifically, \u0394mIoU denotes the relative change of mIoU, and \u0394GFLOPs denotes the relative change of GFLOPs. As shown in Table <ref>, existing video-based methods usually improve accuracy (\u0394mIoU>0) at the cost of more computation (\u0394GFLOPs>0), e.g., TDNet <cit.> and Accel <cit.>. Other methods, including BlockCopy <cit.>, TapLab <cit.> and Jain et al. <cit.>, reduce the computational cost (\u0394GFLOPs<0) but the accuracy also decreases 3%-7%  (\u0394mIoU<0). As a comparison, our proposed models AR^0.6-  can reduce the computational cost (\u0394GFLOPs<0) by more than 55% and preserve the accuracy of single-frame backbone models (\u0394mIoU\u22650). With the lightweight backbone model BiseNet, AR^0.6-Bise18 achieves good performance in both accuracy and computational cost. More results of video-based methods and their single-frame backbone models are presented in Appendix A2.2.\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Ablation Study\n\n\n\nWe perform ablation studies to show the importance of each component in CReFF and FST, as well as the location of CReFF. We also evaluate AR-Seg in terms of different resolutions and GOP lengths, which reflects the influence of hyper-parameters \u03b1 and L, respectively.\nWe conducted ablation studies on the 30fps CamVid dataset, and used PSPNet18 as the default backbone model, L=12 as the default GOP length, and HEVC@3Mbps as the default codec. \n\nArchitecture of CReFF.  We first validate the necessity of \ud835\udcb2_MV and \u2131_LA. The method without CReFF directly applies FST to the upsampled features F\u0305_P and serves as a baseline. As shown in Table <ref>, simply warping the keyframe features (+ \ud835\udcb2_MV) saves the most amount of computation by skipping processing non-keyframes with the segmentation network, but receives poor accuracy. Directly fusing the keyframe features using local attention (+ \u2131_LA) does not perform very well, because the feature maps are not well-aligned due to object motion in videos. \n\nWe further evaluate the design of \u2131_LA by replacing 7x7 local attention (LA) with other operations, including LA with different neighborhood sizes, LA with non-grouped convolution encoders (\u2131_LA^*), global attention (\u2131_GA) and one-layer convolution (\u2131_Conv). \nFor \u2131_GA, we downsample the Value and Key maps by 1/32 to save the computation. \n\u2131_Conv processes the concatenated feature [F\u0302_I, F\u0305_P] with a 3x3 convolution. Due to the large channel number in deep layers, such an operation brings considerable computation overhead.\nResults in Table <ref> show that \u2131_LA with a 7x7 neighborhood achieves a good balance between computation and accuracy. Other designs increase the computational cost without improving the accuracy.\nFurthermore, removing the direct connection (DC) path from CReFF reduces the mIoU to 69.14%. This result implies that CReFF is more likely to learn the residuals of HR features than the absolute values. \n\n\n\n\n\nLocation of CReFF. As specified in Section <ref>, we place CReFF before the final 1x1 convolution layer, which is different from the previous Accel framework <cit.>. To evaluate the influence of the split point location, we insert CReFF before different layers and evaluate the performance. Split points include the final convolution layer (C_1\u00d71), the task sub-network N_task and the feature sub-network N_feat. For the N_feat case, we insert CReFF after the first convolution layer of ResNet. \nAs shown in Table <ref>, placing CReFF before C_1\u00d71 results in the best performance, which affirms our design in Section <ref>. We note that fusing features at an early stage does not improve accuracy, but rather considerably increases computational cost.  \n\nFeature Similarity Training (FST). The proposed FST strategy consists of the MSE Loss and the shared final convolution layer C_1\u00d71. We train AR^0.5-PSP18 with or without these components and report the results in Table <ref>. Both components improve the segmentation performance compared to the model trained without FST. We also replace the MSE Loss with the Kullback-Leibler (KL) Divergence Loss, but the resulting segmentation performance is poor.\n\n\n\nResolution of LR Branch. By adjusting the resolution of the LR branch, AR-Seg can tailor the setting adaptive to different computational budgets. Here, we train and evaluate AR-Seg with different resolutions for the LR branch, ranging from 0.3x to 1.0x. We also train and evaluate the constant-resolution baselines of each resolution for comparison. As shown in Figure <ref>fig:resolution-table-1, AR-Seg improves both backbones under all resolutions, demonstrating the effectiveness of CReFF and FST. To quantify the average improvement, we utilize two metrics BD-FLOPs and BD-mIoU following the design of BD-Rate and BD-PSNR <cit.> in video compression. The results show that (1) with the same computational budget, AR-Seg improves the absolute accuracy for two backbones by 3.67% and 3.02% respectively, and (2) with the same accuracy, AR-Seg reduces the computational cost by 76.73% and 51.06% respectively. Both metrics are described in detail in Appendix A5.\n\nThe Temporal Gap. To investigate the influence of the distance d to the keyframe, we plot the \ud835\udc5a\ud835\udc3c\ud835\udc5c\ud835\udc48_\ud835\udc51 results for AR^0.5-PSP18 and the constant-resolution baselines in Figure <ref>fig:resolution-table-2. As mIoU_0 is determined by the HR branch, AR^0.5-PSP18 shares the same point with PSPNet18(1.0x) at d=0. The \ud835\udc5a\ud835\udc3c\ud835\udc5c\ud835\udc48_0 for PSPNet18(0.5x) is much lower due to downsampling.\nWhen d>0, the accuracy of PSPNet18(1.0x) decreases since the compression artifacts in P frames are more severe than those in I frames. As a comparison, the AR^0.5-PSP18 benefits from the CReFF module and maintains high accuracy for all the d values.\n\nKeyframe Intervals. To validate the long-range reference, we extend our evaluation to different keyframe intervals without re-training. As shown in Table <ref>, AR^0.5-PSP18 trained with L=12 maintains good performance with different GOP lengths. Moreover, even for L=30, which stands for 1s in 30fps videos and is larger than the discussion in previous works <cit.>, AR-Seg outperforms the 1.0x baseline using only 29.2% FLOPs. \n\n\n\nVideo Compression Configurations. \nAs shown in Table <ref>, we also train and evaluate our model with different realistic bit-rates (3Mbps and 1Mbps) and configurations for HEVC/H.265 encoders. We use x265-medium and x265-ultrafast to represent different presets for x265, which apply simplified motion search algorithms and larger macro-blocks. These configurations are widely used in traditional video encoders. The results show that AR^0.5-PSP18 consistently outperforms the 1.0x constant resolution counterpart using only 33% GFLOPs under different configurations.\n\n\n\n \u00a7.\u00a7 Running Time\n\n\nWe measure the running time of AR-Seg with PSPNet18 on both CamVid and Cityscapes datasets, and the results are reported in Table <ref>. Our AR-Seg models run 2x-3x times faster than the constant resolution counterparts. All tests are executed on a single GeForce RTX 3090 GPU.\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nIn this paper, we propose AR-Seg, an altering resolution framework for compressed video semantic segmentation, which innovatively improves the efficiency of video segmentation from the perspective of input resolution. By jointly considering the design of architecture and training strategy, our proposed CReFF module and FST strategy effectively prevent the accuracy loss caused by downsampling. Results evaluated on two widely used datasets show that AR-Seg can achieve competitive segmentation accuracy with a reduction of up to 67% computational cost. Our current study only uses two alternating resolutions (i.e., \nHR and LR). Future work that applies more complicated scheduling of multi-resolutions and keyframe gaps will be considered to further improve the VSS performance.\n\n\n\nAknowledgements:\nThis work was partially supported by Beijing Natural Science Foundation (L222008), Tsinghua University Initiative Scientific Research Program, the Natural Science Foundation of China (61725204), BNRist, and MOE-Key Laboratory of Pervasive Computing.\n\n\n\nieee_fullname\n\n\n\n"}