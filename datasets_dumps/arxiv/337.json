{"entry_id": "http://arxiv.org/abs/2303.06859v1", "published": "20230313050418", "title": "Learning Distortion Invariant Representation for Image Restoration from A Causality Perspective", "authors": ["Xin Li", "Bingchen Li", "Xin Jin", "Cuiling Lan", "Zhibo Chen"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.MM", "eess.IV"], "text": "\n\n\n\n\n\n\nLearning Distortion Invariant Representation for Image Restoration \n from A Causality Perspective\n    Xin Li1[1], Bingchen Li1[1], Xin Jin2, Cuiling Lan3[2], Zhibo Chen1[2]\n\n1University of Science and Technology of China   2EIT Institute for Advanced Study \n\n3Microsoft Research Asia \n\n{lixin666, lbc31415926}@mail.ustc.edu.cn, jinxin@eias.ac.cn, \n culan@microsoft.com,  chenzhibo@ustc.edu.cn\n    Received: date / Accepted: date\n======================================================================================================================================================================================================================================================================================================\n\n\n[1]Equal contribution\n[2]Corresponding Author\n\n\nIn recent years, we have witnessed the great advancement of Deep neural networks (DNNs) in image restoration. \nHowever, a critical limitation is that they cannot generalize well to the real-world degradations with different degrees or types. \nIn this paper, we are the first to propose a novel training strategy for image restoration from the causality perspective, to improve the generalization ability of DNNs for unknown degradations.\nOur method, termed Distortion Invariant representation Learning (DIL), treats each distortion type and degree as one specific confounder, and learns the distortion-invariant representation by eliminating the harmful confounding effect of each degradation. We derive our DIL with the back-door criterion in causality by modeling the interventions of different distortions from the optimization perspective. \nParticularly, we introduce the counterfactual distortion augmentation to simulate the virtual distortion types and degrees as the confounders. Then, we instantiate the intervention of each distortion with a virtually model updating based on corresponding distorted images, and eliminate them from the meta-learning perspective.\nExtensive experiments demonstrate the effectiveness of our DIL on the generalization capability on unseen distortion types and degrees. Our code will  be available at <https://github.com/lixinustc/Casual-IR-DIL>.\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nImage restoration (IR) tasks\u00a0<cit.>, including image super-resolution\u00a0<cit.>, deblurring\u00a0<cit.>, denoising\u00a0<cit.>, compression artifacts removal\u00a0<cit.>, etc, have achieved amazing/uplifting performances, powered by deep learning.\n\nA series of backbones are elaborately and carefully designed to boost the restoration performances for specific degradation. Convolution neural networks (CNNs)\u00a0<cit.> and transformers\u00a0<cit.> are two commonly-used designed choices for the backbones of image restoration. However, these works inevitably suffer from severe performance drops when they encounter unseen degradations as shown in Fig.\u00a0<ref>, where the restoration degree in training corresponds to the noise of standard deviation 20 and the degrees in testing are different. The commonly-used training paradigm in image restoration, i.e., empirical risk minimization (ERM), does not work well for out-of-distribution degradations.\n\nParticularly, the restoration networks trained with ERM merely mine the correlation between distorted image I_d and its ideal reconstructed image I_o by minimizing the distance between I_o and the corresponding clean image I_c. However, a spurious correlation\u00a0<cit.> is also captured which introduces the bad confounding effects of specific degradation d.\n\nIt means the conditional probability P(I_o|I_d) is also conditioned on the distortion types or degrees d (i.e., d \u22a5\u22a5 I_o|I_d). \n\n\n\n\nA robust/generalizable restoration network should be distortion-invariant (i.e., d \u22a5\u22a5 I_o|I_d). For instance, given two distorted images with the same content I_c but different degradations d_1 and d_2, the robust restoration network is expected to recover the same reconstructed image I_o from these two distorted images (i.e., P(I_o|I_d, d=d_1)=P(I_o|I_d, d=d_2)), respectively. Previous works for the robustness of the restoration network can be roughly divided into two categories, distortion adaptation-based schemes, and domain adaptation/translation-based schemes. Distortion adaptation-based schemes\u00a0<cit.> aim to estimate the distortion types or representations, and then, handle the different distortions by adaptively modulating the restoration network. Domain adaptation/translation-based \nschemes\u00a0<cit.> regard different distortions as different domains, and introduce the domain adaptation/translation strategies to the image restoration field. Notwithstanding, the above works ignore the exploration of the intrinsic reasons for the poor generalization capability of the restoration network. In this paper, we take the first step to the causality-inspired image restoration, where novel distortion invariant representation learning from the causality perspective is proposed, to improve the generalization capability of the restoration network.\n\nAs depicted in\u00a0<cit.>, correlation is not equivalent to causation. Learning distortion invariant representation for image restoration requires obtaining the causal effects between the distorted and ideal reconstructed images instead of only their correlation. From the causality perspective, we build a casual graph for the image restoration process. As shown in Fig.\u00a0<ref>, the distortions D including types D_t or degrees D_l are the confounders in IR, which introduces the harmful bias and causes the restoration process I_dI_o condition on D, \nsince a spurious relation path is established via I_dDI_o. The causal connection we want between distorted and ideal reconstructed image is I_dI_o, of which the casual conditional probability can be represented as P(I_o|do(I_d)). Here, a \u201cdo\" operation\u00a0<cit.> is exploited to cut off the connection from the distortion D to I_d, thereby removing the bad confounding effects of D to the path I_dI_o, and learning the distortion-invariant feature representation (i.e., D \u22a5\u22a5 I_o|I_d).  \n\nThere are two typical adjustment criteria for causal effects estimation\u00a0<cit.>, the back-door criterion, and the front-door criterion, respectively. In particular, the back-door criterion aims to remove the bad confounding effects by traversing over known confounders, while the front-door criterion is to solve the challenge that confounders cannot be identified. To improve the generalization capability of the restoration network, we propose the Distortion-Invariant representation Learning (DIL) for image restoration by implementing the back-door criterion from the optimization perspective. \nThere are two challenges for achieving this.  The first challenge is how to construct the confounder sets (i.e., distortion sets). From the causality perspective\u00a0<cit.>, it is better to keep other factors in the distorted image invariant except for distortion types. However, in the real world, collecting the distorted/clean image pairs, especially with different real distortions but the same content is impractical.\n\nInspired by counterfactual\u00a0<cit.> in causality and the distortion simulation\u00a0<cit.>, \nwe propose the counterfactual distortion augmentation, which \n selects amounts of high-quality images from the commonly-used  dataset\u00a0<cit.>, and simulate the different distortion degrees or types on these images as the confounders. \n \n \n \n \n \n \n \n\n\n\n\n\nAnother challenge of implementing DIL stems from finding a stable and proper instantiation scheme for back-door criterion. Previous works\u00a0<cit.> have incorporated causal inference in high-level tasks by instantiating the back-door criterion\u00a0<cit.> with attention intervention\u00a0<cit.>, feature interventions\u00a0<cit.>, etc, which are arduous to be exploited in the low-level task of image restoration. \nIn this work, we theoretically derive our distortion-invariant representation learning DIL by instantiating the back-door criterion from the optimization perspective. Particularly, we model the intervention of simulated distortions for the restoration process by virtually updating the restoration network with the samples from the corresponding distortions. Then, we eliminate the confounding effects of distortions by introducing the optimization strategy from Meta Learning to our proposed DIL. In this way, we can instantiate the causal learning in image restoration and enable the DIL based on the back-door criterion. \n\n\nThe contributions of this paper are listed as follows:\n\n\n\n    \n  * We revisit the image restoration task from a causality view and pinpoint that the reason for the poor generalization of the restoration network, is that the restoration network is not independent to the distortions in the training dataset. \n    \n\n    \n  * Based on the back-door criterion in causality, we propose a novel training paradigm, Distortion Invariant representation Learning (DIL) for image restoration, where the intervention is instantiated by a virtually model updating under the counterfactual distortion augmentation and is eliminated with the optimization based on meta-learning. \n    \n    \n\n    \n  * Extensive experiments on different image restoration tasks have demonstrated the effectiveness of our DIL for improving the generalization ability on unseen distortion types and degrees. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORKS\n\n\n\n\n\n\n \u00a7.\u00a7 Image Restoration\n\n\n\nImage Restoration (IR)\u00a0<cit.> aims to recover high-quality images from the corresponding distorted images, which plays a prominent role in improving the human visual experience. With the advancement of deep learning, a series of works have achieved remarkable progress in lots of IR tasks, including image denoising\u00a0<cit.>, deblurring\u00a0<cit.>, super-resolution (SR)\u00a0<cit.>, etc.  Particularly, most of them are devoted to elaborately designing the frameworks for different IR tasks based on their degradation process, which can be roughly divided into two categories, CNN-based framework\u00a0<cit.>, and Transformer-based framework\u00a0<cit.>. Despite that, the above works only explore how to improve the ability of inductive bias toward specific degradation, which lacks enough generalization capability. \n\nTo improve the model's robustness, some works seek to incorporate the domain translation\u00a0<cit.> or distortion-adaptive learning\u00a0<cit.> into image restoration. In contrast, we introduce causal learning\u00a0<cit.> to image restoration. We answer the reason for the bad robustness of the restoration network and propose distortion-invariant representation learning from a causality perspective. \n\n\n\n\n \u00a7.\u00a7 Causal Inference\n\n\n\nCausal Inference is proposed to eliminate the harmful bias of confounders and discover the causal relationship between multiple variables\u00a0<cit.>. A do operation is implemented with adjustment criteria, e.g., front-door or back-door, to estimate the causal effects\u00a0<cit.>. In recent years, deep learning has boosted the vast development of a series of intelligent tasks, e.g., image classification\u00a0<cit.>, segmentation\u00a0<cit.>, detection\u00a0<cit.>, low-level processing\u00a0<cit.>. However, prominent works focus on fitting the correlation between inputs and their outputs while ignoring the causation. Due to the existence of confounders, the networks are easy to  capture the spurious correlation between inputs and their outputs. For instance, if most lions lie in the grass in the training data, the model inevitably mistakes the grass for a lion. To get rid of the harmful bias of confounders, some studies seek to incorporate casual inference into deep learning. <cit.> model the interventions of confounders from the feature perspective\n<cit.> integrate the front-door criterion to vision-language task from the attention perspective. To improve the generalization capability, <cit.> introduce the causal learning to domain adaptation/generalization. However, the above causality-inspired methods merely focus on the high-level tasks. In this paper, for the first time, we investigate the causality-based image restoration, which aims to improve the generalization capability of restoration networks on different distortion types and degrees. \n\n\n\n\u00a7 METHODS\n\n\n\n\n\n \u00a7.\u00a7 A Causal View for Image Restoration\n\n\n\nImage restoration aims to restore the distorted images, of which the degradation process can be represented as a function I_d = g(I_c, D). Here, I_c, I_d, D denotes the clean, distorted images, and distortions, respectively. A restoration network f is trained with the loss function to minimize the difference between its ideal reconstructed images I_o and the original clean image I_c. We model this whole process with a causal structure graph as shown in Fig.\u00a0<ref>. Here, DI_d I_c denotes the degradation process of I_d = g(I_c, D).  I_c I_o denotes I_o is learned with the supervision of I_c by maximizing the probability of P(I_c|I_o). \n\nIn addition, DI_o refers to the knowledge learned from D to I_o. I_dI_o means the restoration process with restoration network f. \n\n\nFrom the causality perspective, the casual representation of image restoration requires that the restoration network f obtains the causal relationship between I_dI_o (i.e., P(I_o|do(I_d))). However, there are two extra paths I_dDI_o and I_dI_cI_o introducing the spurious correlation to I_d and I_o, where I_c and D are confounders in causality. Importantly, the I_c are commonly diverse in the datasets and bring more vivid textures to reconstructed image I_o, which is a favorable confounder. We do not take into account of the confounder I_c\nin our paper. \n\n\nWe aim to improve the robustness of the restoration network to unseen or unknown distortions, which are inhibited by the bad confounding effects from confounders D. \nBut, how do the confounders D limit the generalization capability of the restoration network? As shown in Fig.\u00a0<ref>, the existing of I_dDI_o causes the conditional probability P(I_o|I_d) learned by restoration network f is also condition on distortions D, i.e., the fitting conditional probability of f is in fact as P(I_o|I_d, D). Consequently, the restoration network f is not robust to different distortions due to that it is not independent of different distortions D. \n\nA robust restoration network f should be independent of different distortions (i.e., D\u22a5\u22a5 I_o|I_d). To achieve this, we adopt the back-door criterion in causal inference to realize distortion-invariant learning (DIL). We formulate the back-door criterion in image restoration as Equ.\u00a0<ref>. \n\n    P(I_o|do(I_d)) = \u2211_d_i\u2208 D P(I_o|I_d, d_i)P(d_i), P(d_i)=1/n,\n\nwhere the casual conditional probability P(I_o|do(I_d)) is the optimization direction for restoration network f towards distortion invariant learning. To simplify the optimization, we set the probability of each distortion d_i as 1/n, where n is the number of distortion types and degrees that existed in confounders. From Equ.\u00a0<ref>, two crucial challenges for achieving it arise.  1) How to construct the virtual confounders (i.e., different distortion types or degrees)? since collecting different real distorted images with the same contents are nontrivial in the real world. 2) How to instantiate the intervention of different distortions to the reconstruction process (i.e., the P(I_o|I_d, d_i)) in image restoration. We achieve this through counterfactual distortion augmentation and distortion-invariant representation learning as described in the following sections.\n\n\n\n\n \u00a7.\u00a7 Counterfactual Distortion Augmentation\n\nTo learn the distortion-invariant representation for the restoration network, it is vital to construct the distortion set D (i.e., confounders). For instance, if we expect the restoration network to have the generalization capability for different distortion degrees, we require to construct the distortion set D with the distortions at different levels. Similarly, we can increase  the generalization capability of the restoration network for unknown distortion types by constructing the D with different but related distortion types. Furthermore, to avoid the effects of different image contents, it is better for each clean image to have corresponding distorted images with various distortion types or degrees in D. However, it is non-trivial to collect datasets that satisfy the above principles in the real world, which is labor-intensive and arduous.\n\nIn this paper, we construct the distortion set D with synthesized distortions, which we can call them virtual confounders in causality. Concretely, we collected a series of high-quality images I_c, and generated the distorted images by modifying the degradation process as I_d=g(I_c, d_i), d_i\u2208 D. We can also prove the rightness of the above distortion augmentation from the counterfactuals in causlity\u00a0<cit.>, where we  answer the counterfactual question that \u201cif D is d_i, what the I_d would be with I_c invariant?\". The proof can be found in the Supplementary. \n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Distortion-invariant Representation Learning\n\nAfter constructing the virtual confounders/distortions set D={d_i|1 \u2264 i\u2264 n}. We are able to achieve the distortion-invariant representation learning by implementing the back-door criterion as Eq.\u00a0<ref> for image restoration. Let us first introduce the relationship between the probability P(I_o|I_d) and the commonly-used training paradigm ERM (empirical risk minimization). In image restoration, an ideal reconstruction I_o is expected to learn by maximizing the condition probability P(I_o|I_d) with loss function as \u2112(f_\u03b8(I_d), I_c), where f_\u03b8 is the restoration network with the parameters \u03b8 and L denotes the loss function, such as the commonly-used \u2112_1 or \u2112_2 loss. The ERM is used to optimize the network f_\u03b8 (with parameters denoted by \u03b8) by minimizing the loss function overall training dataset \ud835\udc9f={I_d, I_c|d\u2208 D} as:\n\n    \u03b8^* = min_\u03b8\ud835\udd3c_(I_d,I_c)\u223c\ud835\udc9f [\u2112 (f_\u03b8(I_d), I_c)],\n\n\nwhere \u03b8^* enables the restoration network f to maximize the P(I_o|I_d)\u2248 P(I_c|I_d). \nHowever, the above training process also leads the P(I_o|I_d) to be not independent to the distortions d \u2208 D in the training dataset \ud835\udc9f, which eliminate the generalization ability of f on the out-of-distribution distortions (i.e.,, when d \u2209D). \nTo achieve the distortion-invariant representation learning, we aim to maximize the causal conditional probability P(I_o|do(I_d)) as instead of P(I_o|I_d). The key challenge stems from how to model the conditional probability P(I_o|I_d, d_i) in Eq.\u00a0<ref> (i.e., how to model the intervention from the distortion d_i\u2208 D for the restoration process P(I_o|I_d)).\n\nIn this paper, we propose to model the intervention  from d_i\u2208 D to the restoration process (i.e., P(I_o|I_d, d_i)) through the optimization of the network parameters \u03b8. \n\n\nFrom the above analysis, we know that the restoration network f_\u03b8 trained with ERM on the paired training data (I_d_i, I_c) is condition on the distortion d_i. This inspires us to instantiate the intervention of different distortion types or degrees d_i\u2208 D through updating the model parameter \u03b8 to \u03d5_d_i based on ERM with the training distorted-and-clean image pairs (I_d_i, I_c) \nas:\n\n    \u03d5_d_i = \u03b8 - \u03b1\u2207_\u03b8\u2112(f_\u03b8(I_d_i), I_c),\n\nwhere \u03d5_d_i denotes the parameters of the restoration network after one-step update, which is conditioned on the confounder d_i. \nConsequently, the maximum of the conditional probability P(I_o|I_d, d_i) can be obtained by minimizing the loss \u2112(f_\u03d5_d_i(I_d), I_c).\nThe optimization direction toward maximizing the causal condition probability P(I_o|do(I_d)) in Eq.\u00a0<ref> can be derived as:\n\n    \u03b8^* = min_\u03b8\ud835\udd3c_(I_d, I_c)\u223c\ud835\udc9f[ \u2211_d_i\u2208 D\u2112(f_\u03d5_d_i(I_d), I_c)],\n\n\n\n\n\n\n\n\n\n\nwhere D denotes the confounder set which contains n distortion degrees or types. Based on the above optimization objective, we learn distortion-invariant representation learning from a causality perspective.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Implementations of DIL from Meta-Learning\n\nAn interesting finding is that the derived optimization direction of DIL from causality perspective in Eq.\u00a0<ref> is consistent with one typical meta-learning strategy termed as MAML\u00a0<cit.>, even they have different purposes. MAML aims to enable the fast adaptation capability of a network for few-shot tasks, while ours aims to improve the generalization capability of the restoration network. We facilitate our DIL in image restoration based on this meta-learning strategy.\n\nHowever, it is arduous to directly incorporate the optimization direction of Eq.\u00a0<ref> into the practical training process, which is computationally prohibitive. The reason is that it requires multiple gradient computing and updating, which is expensive, especially for the pixel-wise image restoration. To simplify this process, we utilize the Talyor expansion and inverse expansion to derive Eq.\u00a0<ref> as:\n\n\n    \u03b8^* = min_\u03b8\ud835\udd3c_(I_d, I_c)\u223c\ud835\udc9f [  \u2112(f_\u03d5_d(I_d), I_c)], \n        where \u00a0\u03d5_d= \u03b8 - \u03b1\u2207_\u03b8\u2211_d_i\u2208 D1/n\u2112(f_\u03b8(I_d_i), I_c),\n\nwhere \u03d5_d denotes the parameters of restoration network f that is virtually updated with loss function with samples overall all distortions D={d_i}, 1\u2264 i \u2264 n. We define it as parallel sampling for DIL, which reduces the complex training process of DIL to two steps. In this paper, we call the original sampling strategy as serial sampling. The comparison between serial sampling and parallel sampling are shown in Fig.\u00a0<ref>. The detailed derivation for Eq.\u00a0<ref> are described in the Supplementary. \n\nWe also investigate two different gradient updating strategy for DIL. From Eq.\u00a0<ref> and Eq.\u00a0<ref>, we can observe that they require the second-order gradient since the gradient is computed with two-step forward through \u03d5_d_i, which is shown in Fig.\u00a0<ref>. To simplify it, Reptile\u00a0<cit.> proposes an alternative strategy (i.e., approximating the second-order gradient by the sequential parameter updating with one-order gradient. The optimization direction (i.e., gradient) is computed with the deviation between the initial and last-step parameters. We integrate it into our DIL and call it first-order optimization. In contrast, the original optimization in Eq.\u00a0<ref> is termed second-order optimization.\nIn summary, we propose four variants for DIL following the above two strategies.  DIL_sf adopts the serial sampling and first-order gradient optimization. DIL_pf utilizes the parallel sampling and first-order optimization. DIL_ss/DIL_ps exploits the second-order optimization and serial/parallel sampling. \n\n\n\n\n\n\u00a7 EXPERIMENTS\n\nIn this section. we first describe the implementation details. Then, we validate the effectiveness of our DIL from two typical out-of-distribution settings, i.e., Cross Distortion Degrees, and Cross Distortion Types. Particularly, for cross-distortion degrees, we train the restoration network with seen distortion degrees while testing it with unseen distortion degrees. For cross-distortion types, the restoration network is trained with synthesized distortions and validated on the corresponding real-world or other distortions.  \n\n\n\n\n\n \u00a7.\u00a7 Implementation\n\n We adopt the typical RRDB\u00a0<cit.> as our image restoration backbone, which has demonstrated remarkable performances towards various low-level image tasks\u00a0<cit.>. All the experiments are done with four NVIDIA 2080Ti GPUs. \n \n Adam optimizer is adopted to optimize network parameters in both ERM and DIL training paradigms. More details are given in the Supplementary.\n\n\n\n \u00a7.\u00a7 Cross Distortion Degrees\n\nResults on Image Denoising.\nFor image denoising, the training data are composed of distorted images with noise levels [5, 10, 15, 20] and their corresponding clean images. After training the restoration network, we validate it on the test datasets with unseen noise degrees, including [30, 40, 50]. We compare the empirical risk minimization (ERM) and four variants of our proposed DIL, i.e., DIL_sf, DIL_pf, DIL_ss, and DIL_ps, respectively. \n\n\n\n\n\nThe experimental results are shown in Table\u00a0<ref>. We can observe that all four variants of DIL achieve great generalization ability on multiple unseen noise levels compared with commonly-used empirical risk minimization (ERM). On several typical scenarios, including natural images (i.e., CBSD68\u00a0<cit.>, Kodak24\u00a0<cit.>, McMaster\u00a0<cit.>), building images (Urban100\u00a0<cit.>), cartoon images (i.e., Manga109\u00a0<cit.>), our DIL even outperforms the ERM by a promising/amazing gain of 8.74 dB at most. Moreover, with the increase of the distribution gap between training and testing data, ours can achieve larger improvements for ERM. Furthermore, for cross distortion degree, DIL_sf shows the best generalization capability compared with the other three variants by serial sampling and first-order optimization. We also visualize the reconstructed images of the above methods in Fig.\u00a0<ref>.\n\nFor the unseen distortion degree (\u03c3=30), the ERM cannot remove the noise well and the reconstructed image also contains obvious noise distortion. However, our DIL_sf enables the restoration network to recover more vivid and clean images from the unseen noise degrees, which validates the correctness and effectiveness of our proposed DIL. \n\nResults on Image Deblurring.\n\nWe also validate the generalization capability of our DIL on the challenging image deblurring. \n\nUnder this scenarios, we train the restoration network with our proposed DIL with the gaussian blurring level [1.0, 2.0, 3.0, 4.0], and validate its generalization capability on the more severe and difficult blurring levels, including 4.2, 4.4, 4.6, 4.8, and 5.0. \n\nAs shown in Table.\u00a0<ref>, we validate our DIL on five benchmark datasets, including Set5\u00a0<cit.>, Set14\u00a0<cit.>, BSD100\u00a0<cit.>, Urban100\u00a0<cit.>, and Manga109\u00a0<cit.>.  With the increase of blurring level, the restoration network trained with ERM suffers from a severe performance drop, since the unseen blurring levels are far away from the blurring levels used for training. But our DIL can improve ERM on each unseen blurring level for five datasets. In particular, we achieve the gain of 2.09 dB for the cartoon scene Manga109\u00a0<cit.> on the blurring level 5.0. \n\n\n\nResults on Hybrid-distorted Image Restoration.\nExcept for the above single distortion, we also explore the generalization capability of our DIL on hybrid-distorted image restoration. Following\u00a0<cit.>, \nthe hybrid distorted images are degraded with  blur, noise, and Jpeg compression in a sequence manner. Based on the distortion degree, it can be divided into three levels from low to high, i.e., mild, moderate, and severe. In this setting, the restoration network is trained with severe hybrid distortions and validated on the mild and moderate levels. \n\n\nAs shown in Table\u00a0<ref>, our DIL achieves an average gain of 1.05 dB, and 0.66 dB on the mild-level, and moderate-level hybrid distortions than ERM, which has a large distribution gap with severe-level hybrid distortions. We can also notice that with the increase of the distribution gap, ours can preserve more performances on the restoration of the out-of-distribution distortions. We also conduct the subjective comparison of our methods with the commonly-used ERM in Fig.\u00a0<ref>. We can observe that the restoration network trained with ERM suffers from new artifacts for unseen hybrid-distorted images. But our DIL can eliminate the artifacts well and generate more promising results.\n\n\n\n\n\n\n \u00a7.\u00a7 Cross Distortion Types\n\nIn this section, we investigate the effects of our proposed DIL on the cross-distortion type setting, which is more challenging than the cross-degree setting. \n\nResults on Real Image Super-resolution\nReal Image Super-resolution (RealSR) has attracted \ngreat attention since it is urgently required in real life, where the distorted image contains complex hybrid distortions, such as blurring, low resolution, noise, etc. However, the distorted/clean pairs for RealSR are hard to be collected. Simulating distortions like Real-world distortion has been a popular solution for RealSR\u00a0<cit.>. In this paper, we follow the Real-ESRGAN\u00a0<cit.> and utilize its proposed RealSR distortion simulating to generate image pairs as training datasets. Then we test the restoration network on the out-of-distribution datasets, RealSR V3\u00a0<cit.>, DRealSR\u00a0<cit.>, which are two commonly-used datasets for RealSR evaluation. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe show the experimental results on RealSR in Table.\u00a0<ref>. Without access to any training samples in RealSR V3, DRealSR, our DIL_sf can outperform the ERM by 0.29dB on RealSR V3\u00a0<cit.> and 0.26dB on DRealSR dataset\u00a0<cit.>. Particularly, we  notice that DIL_ps is more suitable for cross-distortion type scenarios than DIL_sf, which exceeds the ERM by a 0.47dB on RealSR V3, and 0.85dB on DRealSR dataset. \nThe reason for that we guess is that  DIL_ps is more capable of improving the generalization for the large distribution gap in image restoration. We also visualize the comparison corresponding to the subjective quality for different methods. As shown in Fig.\u00a0<ref>, Real-ESRNet\u00a0<cit.> and BSRNet\u00a0<cit.> cause the overshooting at the edge \nof the text. But our DIL_ps can eliminate the artifacts and achieve a high-quality restoration\n\n\n\nResults on Real Image Denoising.\nWe also study the generalization capability of our training paradigm DIL on the Real Image Denoising task. Concretely, we select four synthesized distortions based on four categories of color space among camera ISP process\u00a0<cit.>, and generate training image pairs from DF2K\u00a0<cit.> in an online manner. Then we verify its generalization on the commonly-used Real Denoising dataset SIDD\u00a0<cit.> and DND\u00a0<cit.>.  \nAs Table\u00a0<ref> illustrated, our DIL_ps achieves the PSNR of 39.92 dB, which outperforms the ERM by 1.02dB, which is almost the same with DIL_sf.\n\n\nResults on Image Deraining.\nAs an extension experiment, we introduce our DIL to the experiments of\nimage deraining task. Particularly, the raining types and degrees between different datasets are severely different in image deraining. Here, we optimize the restoration network with three image deraining datasets, including DID-MDN\u00a0<cit.>, Rain14000\u00a0<cit.>, and Heavy Rain Dataset\u00a0<cit.>. Then we validate the generalization capability of the restoration network on three unseen deraining datasets, i.e., Rain100L\u00a0<cit.>, Rain12\u00a0<cit.>, and Rain800\u00a0<cit.>. We report the experimental results in Table\u00a0<ref>. Our DIL (DIL_ps) enables the restoration network to have a better generalization capability than ERM, which obtains a gain of 0.76dB on Rain100L\u00a0<cit.> and 1.63dB on Rain12\u00a0<cit.> dataset. \n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Ablation Studies\n\nImpact of different restoration networks.\nWe demonstrate the effectiveness of DIL across different network backbones. In addition to the convolution-based RRDB\u00a0<cit.> network, we also incorporate our DIL into the transformer-based SwinIR\u00a0<cit.>. The performances are reported in Table\u00a0<ref>, which reveals that our DIL can also improve the generalization capability of Transformer-based backbones. This study reveals our DIL is a general training paradigm for different backbones. \n\n\n\nEffects of different variants for DIL\n\nAs shown in Table.\u00a0<ref>,and\u00a0<ref>, we can observe that DIL_sf is more proper for cross-distortion degrees. But for cross-distortion types, DIL_ps achieves better performance for RealSR and Image Deraining. It is noteworthy that the distribution gap of different distortion types is larger than different degrees. The first-order optimization is more stable but lacks enough capability for a severe distribution gap compared to second-order optimization. But all of them are competent in improving the generalization capability.  \n\n\n\n\n\n\n\n\n\u00a7 DISCUSSION ON LIMITATIONS\n\n\n\nThe performance on training data.\nWe also report the performance of our DIL on the seen training data in Table\u00a0<ref>. It can be seen that our DIL will cause a slight performance drop but the generalization capability is improved obviously. The reason for that is our DIL implements distortion invariant representation learning, which prevents the restoration network from over-fitting to the training data.\n\nThe impact of different distortion augmentation. As shown in Table\u00a0<ref>, despite that our DIL achieves the improvement of the generalization capability. The final generalization performance is still related to the distortion augmentation strategy. It is vital to find a universal distortion augmentation strategy, which requires more exploration. We believe it will be a potential/important direction to improve the generalization ability of the restoration network. \n\n\n\n\n\n\u00a7 CONCLUSION\n\n\n\nIn this paper, we propose a novel distortion invariant representation learning (DIL) training paradigm for image restoration from the causality perspective. In particular, we provide a causal view of the image restoration process, and clarify why the restoration network lacks the generalization capability for different degradations. Based on that, we treat the distortion types and degrees as confounders, of which the confounding effects can be removed with our proposed DIL. Concretely, we produce the spurious confounders by simulating the different distortion types and degrees. Then, an instantiation of the back-door criterion in causality is introduced from the  optimization perspective, which\nenables the restoration network to remove the harmful bias from different degradations. \nExtensive experiments on the settings, cross distortion degrees, and cross distortion types, have demonstrated that our DIL improves the generalization capability of the restoration network effectively.   \n\n\n\n\u00a7 ACKNOWLEDGEMENTS\n\nThis work was supported in part by NSFC under Grant U1908209, 62021001, and  ZJNSFC under Grant LQ23F010008.\n\n\n\n\nieee_fullname\n\n\n\n\n\n\u00a7 APPENDIX\n\n\nSection\u00a0<ref> provides the systematic introduction for the related notations of the back-door criterion in casual learning.\n\nSection\u00a0<ref> explains the counterfactual distortion augmentation from the causality perspective. \n\nSection\u00a0<ref> theoretically derives the parallel sampling in Eq. 5 of our paper.\n\nSection\u00a0<ref> clarifies the implementations of four variants of our DIL, which can help the readers to reproduce our methods more easily. \n\nSection\u00a0<ref> describes the more detailed experimental settings and the construction of distortion/confounder set D in different image restoration tasks.\n\nSection\u00a0<ref> visualizes more subjective comparisons on different image restoration tasks.\n\n\n\n\n\n\n\u00a7 THE BACK-DOOR CRITERION IN CAUSALITY.\n\n\nIn this section, we clarify the related notations and derivations for the back-door criterion in causality.\n\nStructure causal Model.\nAs described in\u00a0<cit.>, we can describe the causal relationship between different vectors with a directed Structural Casual Model (SCM) like Fig.\u00a0<ref>. A directed arrow XY represent X is the cause of the Y. The difference between correlation and causation is as follows: \n1) In causation, given XY, changing the X will cause the effect on Y. But changing Y does not have an effect on X since Y is not the cause of X. \n2) In correlation, we can compute the correlation between X and Y with conditional probability P(Y|X) and P(X|Y) no matter whether there is causation between X and Y. In general, model training in deep learning is a process to fit the correlation between inputs and their labels instead of the causation. \n\nConfounder.\nThe confounder is defined based on the SCM, which represents the variables (e.g., C in Fig.\u00a0<ref>) that are the common cause between two other variables (e.g., X and Y in Fig.\u00a0<ref>). The fork connection XCY causes the spurious correlation for X and Y, which has a confounding effect on the estimation of the causal relationship between X and Y. In other words, the correlation between X and Y learned by the model also is implicitly conditioned on the confounder C. \n\ndo operation. A do operation means to cut off the connection from the CX, which is shown in Fig.\u00a0<ref>. In this way, the correlation introduced from the path XCY is removed from do operation. Then the correlation learned by the model is only from the XY, which are represented as P(Y|do(X)). And this causal correlation is independent of the confounder C and is what we expect the model to learn. \n \n\n\nBack-door criterion.\nThe back-door criterion is proposed in\u00a0<cit.>, which aims to implement the do operation and eliminate the spurious correlation existed in XCY. It removes the confounding effects of confounder C by computing the average casual effects between XY by traversing all values of C as:\n\n    P(Y|do(X)) = \u2211_c P(Y|X,C=c)P(C=c)\n \nBased on Eq.\u00a0<ref>, we can achieve the do operation in Fig.\u00a0<ref> (b).\n\nThe back-door criterion in Image Restoration\nAs shown in Fig. 2 in our paper, we model the image restoration process as a structural causal model, where D={d_i|1\u2264 i \u2264 n} are the confounders between the distorted images I_d and the expected reconstruction images I_o, which satisfies the back-door criterion. Therefore, we can derive the back-door criterion in image restoration as: \n\n    P(I_o|do(I_d)) = \u2211_i=1^n P(I_o|I_d,D=d_i)P(D=d_i)\n \n\n\n\n\n\n\n\n\n\u00a7 A PROOF FOR COUNTERFACTUAL DISTORTION AUGMENTATION.\n\n\nThe conterfactuals aims to answer the question \"\u201cif X been x, in the situation U, what Y_X=x(U) would be?\". The three variables are in the same structural causal model (SCM), and X and U are the cause of Y. As described in \u00a0<cit.>, the calculating of counterfactuals follows three steps:\n1) Abduction: Use evidence e to determine the value of U.\n2) Action: Remove the structural equations for the variables X to modify the model M (i.e., the SCM). Then, set the X as X=x to obtain the modified M_x.\n3) Prediction: Use the M_x and U=u to compute the value of Y (i.e., the consequence of the counterfactual).\n\nConsidering the generation process of the distorted images I_d = g(I_c, d), where I_c and d are the clean images and distortion type/degree, respectively. g is the degradation process. The generation process can be modeled as a structural causal model I_cI_dd. To construct the datasets for the training of DIL, it is better to collect various distorted/clean image pairs with different distortions but the same content. However, in the real world, it is non-trivial to collect the datasets to satisfy this. Therefore, we can construct the ideal datasets by answering the counterfactual question \u201cif D is d_i, what the I_d would be with I_c invariant?\". We call the construction counterfactual distortion augmentation.\n\nAnalogously,\nthe computing of counterfactuals in distortion augmentation follows a three-step procedure. 1) Abduction: Use the distorted image I_d to determine the value of I_c, i.e., P(I_c|I_d). 2) Action: Modify the degradation model, g, so that D is adjusted to the counterfactual value d_i, that might rarely existed in real-world (e.g., the synthesised distortions). 3) Prediction: Compute the consequence I_d_i of the counterfactual based on estimated I_c and modified degradation model g_d_i.\n\nIt is fortunate that there are amounts of high-quality images captured by professional devices, that are only degraded by some extremely mild distortions. We can regard these images as clean images I_c. Therefore, the first step in counterfactuals is unnecessary and can be ignored. We can implement the counterfactual distortion augmentation by adding different synthetic distortion types or degrees to the same image contents I_c. \n\n\n\n\n\u00a7 THE DERIVATION OF THE PARALLEL SAMPLING.\n\n\nIn this section, we will give the derivation of our parallel sampling in Eq. 5 of our paper.\nFrom Eq. 3 and Eq. 4 in our paper as:\n\n    \u03b8^* =    min_\u03b8\ud835\udd3c_(I_d, I_c)\u223c\ud835\udc9f[ 1/n\u2211_d_i\u2208 D\u2112(f_\u03d5_d_i(I_d), I_c)], \n        where   \u03d5_d_i = \u03b8 - \u03b1\u2207_\u03b8\u2112(f_\u03b8(I_d_i), I_c)\n\nwe can conduct the Taylor expansion for the above equation at position \u03b8 as:\n\n    \u03b8^*    =  min_\u03b8\ud835\udd3c_(I_d, I_c)\u223c\ud835\udc9f{1/n\u2211_d_i\u2208 D[ \u2112(f_\u03b8(I_d), I_c)  \n            - \u03b1\u2207_\u03b8\u2112(f_\u03b8(I_d_i), I_c) \u2207_\u03b8\u2112(f_\u03b8(I_d), I_c) \n            +o(\u2207_\u03b8\u2112(f_\u03b8(I_d), I_c))] }\n        = min_\u03b8\ud835\udd3c_(I_d, I_c)\u223c\ud835\udc9f{\u2112(f_\u03b8(I_d), I_c) \n            -1/n\u2211_d_i\u2208 D\u03b1[\u2207_\u03b8\u2112(f_\u03b8(I_d_i), I_c)]\u2207_\u03b8\u2112(f_\u03b8(I_d), I_c)\n            +o(\u2207_\u03b8\u2112(f_\u03b8(I_d), I_c))] }}\n        = min_\u03b8\ud835\udd3c_(I_d, I_c)\u223c\ud835\udc9f{\u2112(f_\u03b8(I_d), I_c) \n            -\u03b1\u2207_\u03b8 [\u2211_d_i\u2208 D1/n\u2112(f_\u03b8(I_d_i), I_c)]\u2207_\u03b8\u2112(f_\u03b8(I_d), I_c)\n            +o(\u2207_\u03b8\u2112(f_\u03b8(I_d), I_c))] }}\n\n\nThen we conduct the Taylor inverse expansion for the Eq.\u00a0<ref>. The Eq.\u00a0<ref> can be derived as:\n\n    \u03b8^*    = \n            min_\u03b8\ud835\udd3c_(I_d, I_c)\u223c\ud835\udc9f[ \n       \u2112(f_(\u03b8-\u03b1\u2207_\u03b8\u2211_d_i\u2208 D1/n\u2112(f_\u03b8(I_d_i), I_c))(I_d), I_c)]\n\nLet \u03d5_d= \u03b8 - \u03b1\u2207_\u03b8\u2211_d_i\u2208 D1/n\u2112(f_\u03b8(I_d_i), I_c), we can obtain the final equation as Eq. 5 of our paper as:\n\n    \u03b8^* = min_\u03b8\ud835\udd3c_(I_d, I_c)\u223c\ud835\udc9f [  \u2112(f_\u03d5_d(I_d), I_c)], \n        where \u00a0\u03d5_d= \u03b8 - \u03b1\u2207_\u03b8\u2211_d_i\u2208 D1/n\u2112(f_\u03b8(I_d_i), I_c),\n\n\n\n\u00a7 THE DETAILED ALGORITHMS ON FOUR VARIANTS OF DIL.\n \n\nWe further demonstrate the algorithm details of four variants of our proposed DIL in the Alg.\u00a0<ref> (DIL_ps), Alg.\u00a0<ref> (DIL_ss), Alg.\u00a0<ref> (DIL_pf), and Alg.\u00a0<ref> (DIL_sf). As derived in Eq.\u00a0<ref>, we can utilize the parallel data sampling for all distortions D to substitute the serial sampling based optimization. The implementation differences between the two sampling strategies can be observed by comparing the Line 5-6 in the Alg.\u00a0<ref> and Line 5-9 in the Alg.\u00a0<ref>. We can find that parallel sampling can reduce the number of parameter updating by 1/n. By comparing the Alg.\u00a0<ref> and Alg.\u00a0<ref>, we can find that only first-order gradient existed in the DIL_pf, which is an approximation of the second-order optimization in Alg.\u00a0<ref>. The related proof can be found in the\u00a0<cit.>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 IMPLEMENTATION DETAILS.\n\n\n\n\n \u00a7.\u00a7 Overall Settings.\n\nFor all image restoration tasks (except for the image deraining task) in this paper, we use 800 images from DIV2K\u00a0<cit.> and 2650 images from Flickr2K\u00a0<cit.> as the clean images to construct the datasets for training.\n \n Following the common setting\u00a0<cit.>, In the training process, we randomly crop the distorted/clean image pairs with the size of 64\u00d764 from the training images, and feed them to the restoration network to optimize the parameters. In the process of the counterfactual distortion augmentation, the distorted patches I_d are generated online according to distortion set D of different image restoration tasks. For ERM, we use Adam optimizer with \u03b2_1=0.9 and \u03b2_2=0.999. For DIL_sf and DIL_pf training paradigms, the same Adam optimizer with ERM is used for the training optimization for the above two variants. For the virtual updating process, we adopt the Adam optimizer with \u03b2_1=0 and \u03b2_2=0.999 following\u00a0<cit.>. \n \n For DIL_ss and DIL_ps, we utilize the same two Adam optimizers as that used in ERM for the virtual updating step and training optimization step. \n  We set the batch size to 8 on each GPU. The total training iterations and initial learning rate are set to 400K and 1e-4, respectively. The learning rate will reduce by half at [200K, 300K]. \n  \n  All the tasks are optimized with the L1 loss if not mentioned. In the image deraining task, we utilize Charbonnier\u00a0<cit.> Loss as:\n\n    \u2112_char = \u221a(I_o - I_c^2 + \u03f5^2)\n\nwhere I_o and I_c denotes the reconstructed images and clean images, respectively. Following previous works\u00a0<cit.>, we set \u03f5 to 1e-3.\n\n\n\n\n \u00a7.\u00a7 The distortion/confounders set D for different tasks.\n\nIn this section, we describe the specific construction of the distortion/confounder set D in the counterfactual distortion augmentation strategy.\n\n\n\n  \u00a7.\u00a7.\u00a7 Cross distortion degrees\n\nImage Denoising.\nFor image denoising, the distortion/confounder set is composed of Additive White Gaussian Noise (AWGN) with the noise intensity of [5, 10, 15, 20], which is added to the clean images from DF2K\u00a0<cit.> to construct the training data. For testing, we utilize several unseen noise intensities, including [30, 40, 50] to estimate the generalization capability of different schemes. \n \nImage Deblurring.\nFor image deblurring, we obtain I_d by applying the distortion/confounding set D to I_c, which contains the 2D gaussian filter with different blurring sigma of [1.0, 2.0, 3.0, 4.0]. For testing, we validate the generalization capability of different schemes on the sigma [4.2, 4.4, 4.6, 4.8, 5.0]. \n\nHybrid distortion restoration.\nFollowing the\u00a0<cit.>, the hybrid distortions are degraded with the superposition of blur, noise, and Jpeg compression artifacts in a sequence manner.  The distortion/confounder set D for training is composed of multiple levels of severe hybrid distortions. The test datasets are composed of unseen distortion levels, including mild and moderate hybrid distortions. \n\n\n\n\n  \u00a7.\u00a7.\u00a7 Cross distortion types\n\n\nReal Image Super-resolution.\nFor real image super-resolution, we utilize the degradation model introduced by\u00a0<cit.> for training. To simplify the training process, we adopt the one-order distortion synthesis mode in \u00a0<cit.> to construct the distortion/confounder set D, where different d_i\u2208 D are divided with different noise types and blur types in the degradation model of\u00a0<cit.>. To validate the generalization capability of different schemes for the \u201ccross distortion types\", we exploit the RealSR\u00a0<cit.> and DRealSR\u00a0<cit.> for the real image super-resolution as our test data. \n\n\nReal Image Denoising.\nFor real image denoising, we obtain I_d based on the ISP process introduced by\u00a0<cit.>. We divide this degradation model into four different distortion types based on the different color filter arrays (CFA) to construct D. We follow previous works\u00a0<cit.> and utilize real denoising datasets DND\u00a0<cit.> and SIDD\u00a0<cit.> as the benchmarks to validate the generalization capability of different schemes.\n\nImage Deraining.\nFor image deraining, we utilize three datasets with three different raining types, including  Rain14000\u00a0<cit.>, DID-MDN\u00a0<cit.>, and Heavy Rain Dataset\u00a0<cit.>, to construct the training data \ud835\udc9f, and test the generalization capability of the restoration network on other three unseen raining types, including 100 rainy images from Rain100L\u00a0<cit.>, 100 rainy images from Rain800\u00a0<cit.>, and 12 rainy images from Rain12\u00a0<cit.>. It is noteworthy that the synthesis strategies of the above raining types are rarely released. Therefore, in this task, we relax the content consistency between different raining types for training. And our DIL is still effective for improving the generalization capability of the restoration network. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 MORE SUBJECTIVE VISUALIZATION\n\n\nWe provide more visual comparisons for different image restoration tasks in this section. As shown in Fig.\u00a0<ref>, the commonly-use ERM and our proposed DIL all achieve similar reconstructed results on the seen noise level (\u03c3=15). But ERM fails to restore high-quality images on unseen noise levels well, (e.g., \u03c3=40 and \u03c3=50), which indicates that ERM lacks of enough generalization ability for the unseen distortion degrees. In contrast,  our DIL_sf can recover noise-free and structure-preserved images despite the distortion degrees do not exist in the training data. This further proves the correctness and effectiveness of our proposed DIL. \n\nWe show more visual comparisons of image deblurring in Fig.\u00a0<ref>. When dealing with unseen blur degrees, our proposed DIL can restore the clear structures, while ERM produces overshooting artifacts on the edges. More visualizations for real image denoising can be found in Fig.\u00a0<ref>. And more visualizations for image deraining can be found in Fig.\u00a0<ref>. We also visualize the subjective comparisons on hybrid-distorted image restoration in Fig.\u00a0<ref>.\n\n\n\n\n"}