{"entry_id": "http://arxiv.org/abs/2303.07243v1", "published": "20230313161542", "title": "Sim-to-Real Deep Reinforcement Learning based Obstacle Avoidance for UAVs under Measurement Uncertainty", "authors": ["Bhaskar Joshi", "Dhruv Kapur", "Harikumar Kandath"], "primary_category": "cs.RO", "categories": ["cs.RO"], "text": "\n\n\n\n\n\nSim-to-Real Deep Reinforcement Learning based Obstacle Avoidance for UAVs under Measurement Uncertainty\n    Bhaskar Joshi*, Dhruv Kapur*, Harikumar Kandath ^\u2020\n    March 30, 2023\n=======================================================================================================\n\n\n\n*These authors contributed equally to this work\n ^\u2020Bhaskar Joshi, Dhruv Kapur and Harikumar Kandath are with RRC, IIIT Hyderabad { }.\n\n\nDeep Reinforcement Learning is quickly becoming a popular method for training autonomous Unmanned Aerial Vehicles (UAVs). Our work analyzes the effects of measurement uncertainty on the performance of Deep Reinforcement Learning (DRL) based waypoint navigation and obstacle avoidance for UAVs. Measurement uncertainty originates from noise in the sensors used for localization and detecting obstacles. Measurement uncertainty/noise is considered to follow a Gaussian probability distribution with unknown non-zero mean and variance. We evaluate the performance of a DRL agent, trained using the Proximal Policy Optimization (PPO) algorithm in an environment with continuous state and action spaces. The environment is randomized with different numbers of obstacles for each simulation episode in the presence of varying degrees of  noise, to capture the effects of realistic sensor measurements. Denoising techniques like the low pass filter and Kalman filter improve performance in the presence of unbiased noise. Moreover, we show that artificially injecting noise into the measurements during evaluation actually improves performance in certain scenarios. Extensive training and testing of the DRL agent under various UAV navigation scenarios are performed in the PyBullet physics simulator. To evaluate the practical validity of our method, we port the policy trained in simulation onto a real UAV without any further modifications and verify the results in a real-world environment.\n\n\n\n\nAutonomous navigation, deep reinforcement learning, measurement noise,  obstacle avoidance, proximal policy optimization, unmanned aerial vehicle.\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nUnmanned aerial vehicles (UAVs) are commonly used for various critical missions that often require navigation in unpredictable environments with obstacles and potential threats to the safety of the UAV<cit.>. While autonomous UAVs serve countless applications, navigation remains challenging due to a plethora of problems, such as environmental perception, limited sensing and processing capabilities, etc. Methods like velocity obstacles, artificial potential fields, etc. are used for obstacle avoidance in environments with low obstacle density, and accurate UAV localization and obstacle information <cit.>. Methods like SLAM <cit.> are effective in high obstacle-density environments, using data from sensors like cameras and LIDAR. This makes it too computationally complex for real-time implementation in UAVs.\n\n\n\nAn increasingly popular approach for waypoint navigation amidst obstacles is to use Reinforcement Learning (RL)<cit.>. RL allows an agent to learn the optimal behaviour for a particular task through trial-and-error interactions with the environment. This makes RL a promising method for improving the autonomy of agents in various robotics applications <cit.>.\n\nPham et al. <cit.> provide a framework for using reinforcement learning to enable UAVs to navigate a discretized unknown environment successfully. S. Ouahouah et al. <cit.> propose probabilistic and Deep Reinforcement Learning based algorithms for avoiding collisions in a discrete space environment. Villanueva et al. <cit.> present a method for better UAV exploration during training for navigational tasks by injecting Gaussian noise into the action space of a DRL agent. They found this to decrease the number of navigation steps, resulting in a shorter time of flight per task. All of the above approaches assume the data obtained from the sensors to be perfect.\n\nAll the real-world sensors are noisy, and to the best of our knowledge, no studies have been conducted on the effects of measurement noise in the context of training and implementation of DRL-based algorithms for UAVs. Measurement noise makes the relative position of the obstacles and the target from the UAV uncertain. \n\nThe environment is simulated in PyBullet <cit.>, a realistic physics simulator, where we train a DRL agent to control the UAV to avoid obstacles and reach a target location, subject to varying types of Gaussian noise and a varying number of obstacles. It is observed that training the agent with a certain level of measurement noise improves its obstacle-avoidance capabilities.\n\nIn this context, the key contributions of this paper are given below.\n\n    \n  * This is the first study that systematically analyzes the effects of noisy sensor inputs on DRL-based waypoint navigation and obstacle avoidance for UAVs.\n    \n  * The measurement noise is modelled as a random variable sampled from a Gaussian distribution. Both training and evaluation of the well-known DRL algorithm, Proximal Policy Optimization (PPO) is performed in the presence of measurement noise with different levels of the unknown mean and variance. The performance of the DRL agent trained with perfect measurements is compared with other agents trained with different levels of measurement noise.\n    \n  * We show that artificially injecting noise with carefully chosen variance into the existing measurement error improves the performance of the DRL agent when the measurement error has some unknown bias.\n    \n  * The policy trained in simulations can be directly deployed to a real-world environment for waypoint navigation and obstacle avoidance, using a CrazyFlie 2.1.\n\n\nThe rest of the paper is organized as follows. Section II contains an overview of the background concepts for our work. Section III provides the problem formulation. In Section IV, we explain our methodology. Section V includes all experimental results, both in simulation and in the real world. We summarize the results and discuss their implications, drawing the primary conclusion in Section VI.\n\n\n\n\u00a7 PRELIMINARIES\n\nIn this section, we introduce the key background concepts of this work.\n\n\n \u00a7.\u00a7 UAV Model\n\nA first-order linear UAV model with position coordinates [x, y]^T as output and velocity [v_x, v_y]^T as the control input is used here, as given by Eq. (<ref>).\n\n    \u1e8b = v_x,    \u1e8f = v_y\n\n\nOur current work assumes that the altitude z is held constant throughout the flight, constraining the UAV navigation to the XY plane.\n\n\n\n \u00a7.\u00a7 Deep Reinforcement Learning\n\n\nReinforcement Learning is a method for an agent to learn the desired behavior through trial-and-error interactions with the environment. The agent receives an observation O_t i.e representation of the current state S_t  and chooses an action A_t, receiving a new observation and a reward that determines the value of the action taken in that state. The agent's choice of action is dictated by its internal policy at the time \u03c0_t. A policy is simply a probability distribution over the set of all possible actions in the given state, describing the likelihood of choosing each action.\n\n\n\n    \u03c0_t(a|s) = \u2119(A_t = a | S_t = s)\n\n\nThe return is calculated as the sum of rewards, discounted by a factor that determines the relative importance of short-term rewards. The objective of RL is to maximize the expected return from its interactions with the environment. In Deep Reinforcement Learning, the policy is typically represented by a deep neural network, allowing interactions with more complex environments. \n\n\n\n \u00a7.\u00a7 Proximal Policy Optimization (PPO)\n\n\nProximal Policy Optimization (PPO) <cit.> is a deep reinforcement learning algorithm that iteratively updates a policy to maximize a reward signal. PPO employs a form of trust region optimization, which allows for better handling of non-stationary environments by limiting the size of policy updates.\n\n\n    r_t(\u03b8) = \u03c0_\u03b8(a_t | s_t)/\u03c0_\u03b8_old(a_t | s_t)\n \n\nPPO uses a clipped surrogate objective function to ensure that policy updates do not deviate too far from the previous policy, thereby avoiding large changes that could lead to destabilization of the learning process. This constraint encourages more stable learning and better convergence properties compared to other policy optimization algorithms. \n\n\n\n \u00a7.\u00a7 Measurement Noise Model and Denoising Algorithms\n\nThe noise used in all of our simulations and experiments is sampled from Gaussian distributions with different combinations of the mean (\u03bc) and standard deviation (\u03c3).\n\n    f(x) = 1/\u221a(2\u03c0\u03c3^2)e^-1/2(x-\u03bc/\u03c3)^2= \ud835\udca9(\u03bc, \u03c3)\n\n\nThere are several approaches to denoising (reducing the effect of measurement noise) a signal corrupted by Gaussian noise. Our primary focus is on two of them: The Bessel Low Pass Filter <cit.>, and the Kalman Filter <cit.>\n\n\n\n\u00a7 PROBLEM FORMULATION\n\n \nThe objective of the agent is to minimize the distance between the UAV and the target (d_target) within some acceptable error (\u03f5_success), while at all times maintaining some minimum safety distance (\u03f5_safe) to all obstacles d_o_i if possible. As a heuristic, at any given time, we only consider the obstacle closest to the UAV, asserting the minimum safety distance constraint on it (d_o > \u03f5_safe), trivially satisfying the constraint on all other obstacles in the environment.\n\nSince sensors tend to be noisy, the localization estimate of the UAV is imprecise, as given below.\n\n    x\u0302 = x + \u03b7_x        \u0177 = y + \u03b7_y \n    \u03b7_x, \u03b7_y    \u223c\ud835\udca9(\u03bc, \u03c3)\n\n\nHere x and y denote the true position of the UAV and \u03b7_x and \u03b7_y are localization errors caused due to sensor noise. \n\nThe position estimate deviating from the true position due to sensor noise makes the problem of obstacle avoidance a lot harder to tackle since uncertainty takes hold of the state observation at every step. \n\nOur study looks into how inherently robust a policy learned through deep reinforcement learning is to such noise, and if we can leverage certain techniques during training and evaluation to improve its performance.\n\n\n\n\n\u00a7 METHODOLOGY\n\nThe architecture of the methodology followed is shown in Fig. <ref>.\n\n\n\n\n \u00a7.\u00a7 The Environment\n\n\n\nThe environment consists of multiple obstacles, the target location, and the initial position of the UAV.\nAt the start of every episode, the UAV is randomly spawned at:\n\n    x_0    = [ x_min + r_minor, y_g_0, z_min + z_max/2] \n    \n    y_g_0   \u223c Uniform(y_min + r_minor, y_max - r_minor)\n\n\nNext, the goal position is set as\n\n    x_g = [\n    x_max - r_minor, y_min + y_max/2, z_min + z_max/2]\n\n\nHere, m_min and m_max specify the environment dimensions along the m-axis (m \u2208{x,y,z}), and r_minor is the safety bound added to avoid the UAV from going out of bounds.\n\nFinally, the environment randomly generates a series of obstacles. It first chooses the total number of obstacles between a specified upper and lower bound. The obstacles are then positioned by distributing them along the x-axis based on the uniform distribution, along the y-axis based on the normal distribution, and at the default altitude (midpoint) along the z-axis.\n\n\n\n  \u00a7.\u00a7.\u00a7 Observation Space\n\n\nThe observation space of the environment is a continuous, distilled form of the state at any given time, encoding only the information required by the agent to learn a good policy.\n\n    O_t = [ \u0394x_g_t \u0394y_g_t \u0394x_o_t \u0394y_o_t ]\n  \n\n\u0394x_g_t and \u0394y_g_t are distances to the goal at time t along the x and y axes. Similarly, \u0394x_o_t and \u0394y_o_t are distances to the surface of the nearest obstacle (or wall). \n\nWith localization noise, the x and y positions of the UAV are affected by noise and become x\u0302, \u0177. As such, the observation is also perturbed as given below.\n\n    \u00d4_t = [ \u0394x\u0302_\u0302\u011d_t  \u0394\u0177_\u0302\u011d_t \u0394x\u0302_\u0302\u00f4_t  \u0394\u0177_\u0302\u00f4_t ]\n\n\n\nIn the presence of a denoiser, the noisy position estimates, x\u0302 and \u0177, are denoised to yield x\u0303 and \u1ef9. This leads to the denoised form of the observation.\n\n\n    \u00d5_t = [ \u0394x\u0303_\u0303g\u0303_t  \u0394\u1ef9_\u0303g\u0303_t  \u0394x\u0303_\u0303\u00f5_t   \u0394\u1ef9_\u0303\u00f5_t ]\n\n\n\n\u00d5_t is the observation returned from the environment at time step t.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Action Space\n\n\nThe action space of our environment is also continuous. A valid action is any vector of the form.\n\n\n    A = [   v_x   v_y v_mag ]\n\n\n\nHere, v_x and v_y are the velocities of the UAV along the x and y directions, respectively. v_mag is the magnitude of the velocity vector. The velocity command can be computed from the action in the following way:\n\n\n    v\u20d7 = [ v_x v_y ]/\u221a(v_x^2 + v_y ^ 2)\u00b7 v_mag\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Reward Function\n\n\nThe environment has a dense reward function, giving the agent continuous feedback to learn from. At time step t, the reward is given by:\n\n\n    R_t = \n    R_s    if dist to target < \u03f5_success\n    \n    R_f    if collided with an obstacle\n    \n    R_f    if out of time\n    \n    (-R_d \u2016[ \u0394 x_t \u0394 y_t ]\u2016\n    \n    -R_major1_major\n    \n    - R_minor1_minor)    otherwise\n\n\n\n\nHere, R_s > 0 is the success reward, R_f < 0 is the failure penalty, R_d > 0 is the distance penalty coefficient, R_minor > 0, and R_major > 0 are the minor and major bound breach penalties respectively. The agent is said to be \"out of time\" if it has neither successfully reached the target nor collided with an obstacle within a given time limit. 1_major and 1_minor are boolean variables that are set if the major and minor safety bounds of radius r_major and r_minor respectively have been breached and are unset if they have not. Each non-terminal time step has a small negative reward which is a function of its distance from the target, incentivizing the agent to get to the target location in the fewest possible steps.\n\n\n\n\n \u00a7.\u00a7 The Agent\n\n\nThe agent is an instance of the Proximal Policy Optimization algorithm from stablebaselines3 <cit.>. Both, the actor and the critic, take as input the observation from the environment. The critic tries to learn the state value function, so its output is the perceived value of the input state. On the other hand, the actor tries to learn the policy. For the given input state, it tries to predict the mean value for each scalar in the action vector. It then samples an action value using the mean from a Gaussian distribution.\n\nIt is important to note here that the focus of our work is not to create the best possible policy to navigate the environment, but rather to study the effects of noise on an arbitrary policy with an acceptable success rate. As such, we have chosen this vanilla PPO implementation without any modification for our experiments.\n\n\n\n\n\n \u00a7.\u00a7 The Effects of Noise\n\n\nThe primary focus of our work is on the effects of observation space noise on the performance of the policy. We first trained multiple policies on different levels of unbiased noise and compared their training results, such as mean reward and mean episode duration over 100 episodes. For each trained policy, we run a baseline test by evaluating the policy in an environment with no noise (\u03bc = 0, \u03c3 = 0). Once the baselines were established, we evaluated these policies in environments with different types of measurement noise as given below:\n\n\n\n  * unbiased noise (\u03bc = 0, \u03c3\u2260 0)\n\n  * bias-only noise (\u03bc\u2260 0, \u03c3 = 0)\n\n  * biased noise (\u03bc\u2260 0, \u03c3\u2260 0).\n\n\nWhile the effects of unbiased noise with unknown standard deviation (\u03c3) can be mitigated quite effectively with the use of denoisers, as shown by our results in Section V, the same cannot be said about bias-only noise and biased noise with unknown mean (\u03bc). \n\nOur experiments reveal that the policies trained using the above-described way tend to perform better when faced with biased noise, rather than with bias-only noise, indicating that performance in the presence of the latter can be improved by injecting carefully chosen unbiased noise on top of the biased localization estimate. \n\nAgain, It is important to note here that we aren't treating biased noise in the same way as the other two noises; rather, we treat it as two separate noises \u2014 the first is the biased noise with none-to-low variance plaguing sensor readings, which cannot be fixed by using a filter, and the second is some unbiased noise that we inject into the existing sensor noise to improve the performance of the policy.\n\n\n\n \u00a7.\u00a7 Sim-to-Real Transfer\n\n\nFor the physical experiments, we use a Crazyflie 2.1 with motion capture localization. The Crazyflie is equipped with motion capture markers, which allow us to localize the UAV in real-time with high precision, having full control over the perturbation of the observations. The motion capture data is streamed to our server, which first corrupts the position data using the noise generator, denoises it if necessary and then computes the observation, before passing it onto the policy. The policy sees the perturbed observation and generates the appropriate action. This action is converted into a velocity command for the UAV to follow and is then sent to the UAV. All of the above steps constitute a single time step. We define an episode as successful if the UAV reaches within some distance of the target. We are able to port the trained agent network to the UAV without any further modifications from training. \n\n\n\n\u00a7 RESULTS\n\n\nThis section covers the experimental results obtained during training and during evaluation, both in simulation and in a physical environment. The policies were trained on an RTX 2080 Ti GPU, taking around 5 hours to run for 5 million timesteps. Evaluation of a policy for 1000 episodes on a given combination of \u03bc, \u03c3, and denoiser takes around 10 minutes. The specific environment variables used during experimentation can be found in TABLE <ref>.\n\n\n\nAll measurements are in meters. \n\n\n\n\n\n \u00a7.\u00a7 Training\n\n\nWe trained 3 policies in environments with different degrees of unbiased noise to compare the effects of noise added to the observation during training as given below.\n\n\n    \n  * Policy 1: No Noise (\u03bc=0, \u03c3 = 0)\n    \n  * Policy 2: Low Noise (\u03bc=0, \u03c3 = 0.1)\n    \n  * Policy 3: High Noise (\u03bc=0, \u03c3 = 1.0)\n\n\nEach policy was trained for 5 million time steps, in an environment with anywhere between 1 and 3 obstacles, placed at random at the start of every episode. Policy hyperparameters are mentioned in TABLE <ref>.\n\n\n\nWe use two criteria to judge the quality of the training process for our policies \u2014 mean reward over the last 100 episodes and mean episode duration over the last 100 episodes.\n\n\n\n  \u00a7.\u00a7.\u00a7 Mean Reward\n\nFrom Figure <ref>(a), we see that the mean reward for all three policies increases as training goes on. This indicates that the desired behaviour of avoiding obstacles and reaching the target in the fewest possible steps is being learned.\n\nPolicy 2 (orange) has the highest mean reward, marginally beating out Policy 1. This is because the small noise (\u03c3 = 0.1) in the observation space during the training of Policy 2 caused the agent to explore more effectively, yielding a better policy. Policy 3 (green) performs the worst because the high variance (\u03c3 = 1.0) makes it difficult for the policy to identify patterns in its observations.\n\n\n\n  \u00a7.\u00a7.\u00a7 Episode Duration\n\nFigure <ref>(b) shows the mean episode duration also reducing over training. This is because each non-terminal time step in the environment leads to a negative reward for the agent, incentivizing the agent to finish the episode as quickly as possible.\n\n\n\n \u00a7.\u00a7 Simulated Evaluations of Trained Policies\n\n\nWe evaluated the trained policies in environments with varying degrees of bias and variance in the noise, with and without a denoiser (LPF and KF). Each evaluation took place in a randomly generated environment with anywhere between 0 to 3 obstacles placed at random at the start of every episode, for 1000 episodes, to guarantee confidence in our results.\n\nThe LPF we have used for our experiments is the 2^nd order Bessel Low Pass filter with a cutoff frequency of 2, for which the transfer function <cit.> evaluates to\n\n\n    H(s) = 3/s^2+3s+3\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Unbiased Noise\n\n\nThis subsection contains the evaluation results of the three trained policies in environments with varying degrees of unbiased noise (\u03bc = 0, 0 \u2264\u03c3\u2264 3.0, \u0394\u03c3 = 0.1), comparing their performances with and without a denoiser. We will be referring to the case with no standard deviation in the noise (\u03c3 = 0) as baseline for the remained of this subsection.\n\nPolicy 1: Figure <ref>(a) visualizes the performance of Policy 1. At baseline, Policy 1 achieves a success rate of around 60%. We see a very quick dropoff in success rate as the standard deviation increases, with it falling to near 0% around \u03c3 = 2.0. This drop-off is slowed down significantly by the Low Pass and Kalman Filters, still achieving around 30% success rate at \u03c3 = 3.\n\nPolicy 2: From Figure <ref>(b), to begin with, Policy 2 outperforms Policy 1 at baseline, achieving around 72% success rate. With a slight increase in \u03c3, Policy 2's performance increases, peaking around 76% at \u03c3 = 0.5. Further increase in \u03c3 causes the performance to drop quickly, yet even at \u03c3 = 3.0, Policy 2 achieves a success rate of 56%, almost twice the success rate of Policy 1 even with the help of a denoiser. Augmenting Policy 2 with a denoiser creates a very robust controller, with a near-consistent 70% success rate across the whole standard deviation range.\n\nPolicy 3: Figure <ref>(c) shows Policy 3 with near 0% success rate at baseline and for very small values of \u03c3. Then at \u03c3 = 0.3 it jumps right up to 50% success rate, and then gradually falls off with a further increase in \u03c3. When compared to Policy 1, Policy 3 performs much worse for smaller noises, but significantly better for large values of \u03c3, performing nearly identical to Policy 1 with a denoiser beyond \u03c3 = 1.8. Stacking the LPF on top of Policy 3 gives us a similar trend, seemingly scaled horizontally. It takes slightly longer to climb up to 50% success rate, but its drop-off is also significantly more gradual, maintaining a near-constant 50% success rate after \u03c3 = 0.1. No such trend is seen for the Kalman Filter within our experimental range.\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Bias-only Noise\n\n\nThis subsection contains the evaluation results of the three trained policies in environments with varying degrees of bias-only noise (0 \u2264\u03bc\u2264 0.3, \u03c3 = 0, \u0394\u03bc = 0.01), comparing their performances with and without a denoiser.\n\nFigure <ref> confirms the fact that the denoisers have no way of mitigating the negative effects of bias, as we see the same trend across the three cases for each policy. The success rate for Policy 1 decreases as bias increases (Figure <ref>(a)). Interestingly, Policy 2 is robust to bias up to a significant extent, all the way up to \u03bc=1.2, beyond which it falls drastically (Figure <ref>(b)). The success rate for Policy 3 (Figure <ref>(c)) is too low to make any systematic inferences and can be attributed entirely to randomness. \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Biased Noise\n\n\nThis subsection contains the evaluation results of the three trained policies in environments with varying degrees of biased noise (0 \u2264\u03bc\u2264 0.3, 0 \u2264\u03c3\u2264 3.0,\u0394\u03bc=0.01,\u0394\u03c3=0.1), comparing their performances without a denoiser.\n\n Policy 1: There are three regions of interest in Figure <ref>(a) \u2014 first, the small blue region on the bottom left indicates that the policy performs poorly for large values of \u03bc with small values of \u03c3. Second, the large blue region on the right indicates the policy's failure in the presence of high variance noise. Finally, the Red region spread diagonally downwards indicates that the negative effects of high bias can be mitigated to an extent by injecting unbiased noise with a certain value of \u03c3, picked from this region. Consider performance at \u03bc=2.0 \u2014 with \u03c3 = 0, the success rate is at 0%, but picking the right value, say \u03c3=1.2 brings the success rate up to over 40%. We see it fade out as we move down, indicating the reduced effectiveness of this approach for larger values of \u03bc.\n\nPolicy 2: Policy 2 shows high success rates for a decent range of \u03bc and \u03c3 (Figure <ref>(b)). We see the blue region in the bottom left coincides with the fall-off point in Figure <ref>(b) at \u03bc=1.2. However, the red contour around the blue region suggests that the negative effects of the bias can be neutralized by carefully choosing a value for \u03c3, increasing the success rate from around 35% to over 70%. Again, as expected, if bias and variance increase too much, the success rate starts to drop off as indicated by the fading to the bottom and to the right.\n\nPolicy 3: Figure <ref>(c) aligns the trend set in Figure <ref>(c). For small values of \u03c3, the policy performs very poorly, as indicated by the blue strip on the left. Beyond \u03c3 = 0.3, we see a quick spike in the success rate for smaller values of \u03bc, with the success rate dropping as we move radially outward. The drop is a lot faster for an increase in \u03bc as compared to an increase in \u03c3, since Policy 3 was found to perform quite consistently for very high values of \u03c3, but to perform poorly for all values of \u03bc.\nThe code for training and evaluation of the policies, with detailed results, can be found at https://github.com/dkapur17/DroneControl and the demo video could be found at https://youtu.be/ALTblQmQtHM\n\n\n\n\n\n\n\n \u00a7.\u00a7 Sim to Real\n\n\n\n\nDue to the high-level nature of our control policy, we were able to run the trained policy on a physical Crazyflie 2.1 without any further modifications. Figure <ref> shows the real-world environment we carried out our experiments in. The floating spherical obstacles in the simulated environment were replaced with cylindrical pipes of a diameter of 10 centimeters. The bounds of the environment were marked with white lines, defining the operating region for the UAV. The start and target positions are also marked, with the target position having a 10-centimeter radius around it to allow for a small completion error, similar to how it was in simulation (\u03f5_success = 0.1). \n\nDue to the constraint on the number of trials, it is feasible to run in the physical environment, our primary focus was on evaluating the performances of Policy 1 and Policy 2, since they yielded promising results in simulation. The results obtained were consistent with those seen in the simulations. For confidence in our results, each test was run 5 times.\n\nWe verify the improvement in performance for the policies against unbiased noise when they are provided with some support by a denoiser. More importantly, we verify the unexpected result of being able to improve performance in the presence of bias by adding variance to the localization estimate. Both policies were tested with a bias of 0.15 m. In the absence of any variance, the success rate is quite low as expected. Adding unbiased noise with \u03c3 = 0.8 improved performance, and both policies reached the target on all trials. Consistent with our simulation results, further increasing the standard deviation to \u03c3=1.3 caused the success rate to plummet again. The results for the physical experiments can be found in TABLE <ref>.\n\n\n\n\n\u00a7 CONCLUSION\n\n\n\n\nOur work studies the interaction between various forms of Gaussian Noise, and a Proximal Policy Optimization agent trained to control a UAV for obstacle avoidance in a continuous state and action space. This was done by both, training policies with different levels of unbiased noise, as well as by evaluating the policies on different kinds of noise \u2014 unbiased, bias-only and biased noise.  We verified the function of a denoiser, by testing the trained policy in the presence of unbiased noise and saw an improvement in performance with the addition of a denoiser, such as the Low Pass Filter or the Kalman Filter.\n\nThe key results from our work are two-fold \u2014 first, training the PPO agent with a small amount of state space noise leads to it learning a very stable policy, outperforming a policy trained without noise across the board when evaluated in noisy environments. Second, and the more surprising result, is that we can leverage the inherent robustness of the trained policy to unbiased noise to improve its performance in environments with high bias low variance noise. This can be done by artificially injecting unbiased noise into the sensor measurements, yielding perturbed observations, which are then fed into the policy, greatly improving the success rate.\n\nOur current study uses a simplified environment wherein the UAV's motion was constrained to a plane. In our future work, we look to allow the agent to control the UAV's altitude as well while also having it deal with obstacles at varying heights as well as with multiple agents.\n\n00\n\nb4 A. Budiyanto, A. Cahyadi, T. B. Adji and O. Wahyunggoro, \"UAV obstacle avoidance using potential field under dynamic environment,\" 2015 International Conference on Control, Electronics, Renewable Energy and Communications (ICCEREC), Bandung, Indonesia, 2015, pp. 187-192.\n\nb41 C.Y. Tan, S. Huang, K.K. Tan and R.S.H. Teo, \"Three-Dimensional Collision Avoidance for Multi Unmanned Aerial Vehicles Using Velocity Obstacle,\" Journal of Intelligent and Robotic Systems, 97, 227\u2013248 (2020).\n\nb42 A. Steenbeek and F. Nex, \"CNN-based dense monocular visual SLAM for real-time UAV exploration in emergency conditions,\" Drones, 2022 Mar 18;6(3):79.\n\nb5 Azar, A. T., Koubaa, A., Ali Mohamed, N., Ibrahim, H. A., Ibrahim, Z. F., Kazim, M., ... & Casalino, G. (2021). Drone deep reinforcement learning: A review. Electronics, 10(9), 999.\n\nb7 Kober, J., Bagnell, J. A., & Peters, J. (2013). Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11), 1238-1274.\n\nb6 Pham, H. X., La, H. M., Feil-Seifer, D., & Nguyen, L. V. (2018). Autonomous uav navigation using reinforcement learning. arXiv preprint arXiv:1801.05086.\n\nb9 Ouahouah, S., Bagaa, M., Prados-Garzon, J., & Taleb, T. (2021). Deep-reinforcement-learning-based collision avoidance in uav environment. IEEE Internet of Things Journal, 9(6), 4015-4030.\n\n\nb8 Villanueva, A., & Fajardo, A. (2019, December). Deep reinforcement learning with noise injection for UAV path planning. In 2019 IEEE 6th International Conference on Engineering Technologies and Applied Sciences (ICETAS) (pp. 1-6). IEEE.\n\npb Erwin Coumans and Yunfei Bai (2016-2019). PyBullet, a Python module for physics simulation for games, robotics and machine learning.\n\n\nbessel Bowman, F. (2012). Introduction to Bessel functions. Courier Corporation.\n\nkalman Kalman, R. E. (1960). A new approach to linear filtering and prediction problems.\n\nPPO Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.\n\nsb3 Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., & Dormann, N. (2021). Stable-baselines3: Reliable reinforcement learning implementations. The Journal of Machine Learning Research, 22(1), 12348-12355.\n\n\n\n"}