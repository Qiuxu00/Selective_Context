{"entry_id": "http://arxiv.org/abs/2303.06966v1", "published": "20230313100813", "title": "A new methodology to predict the oncotype scores based on clinico-pathological data with similar tumor profiles", "authors": ["Zeina Al Masry", "Romain Pic", "Cl\u00e9ment Dombry", "Christine Devalland"], "primary_category": "stat.AP", "categories": ["stat.AP", "q-bio.QM", "stat.ML"], "text": "\n\nFirst results from the JWST Early Release Science Program Q3D: Ionization cone, clumpy star formation and shocks in a z=3 extremely red quasar host\n    Lillian Whitesell\n    March 30, 2023\n===================================================================================================================================================\n\n\n\n\nIntroduction: The Oncotype DX (ODX) test is a commercially available molecular test for breast cancer assay that provides prognostic and predictive breast cancer recurrence information for hormone positive, HER2-negative patients. The aim of this study is to propose a novel methodology to assist physicians in their decision-making. \n\nMethods: A retrospective study between 2012 and 2020 with 333 cases that underwent an ODX assay from three hospitals in Bourgogne Franche-Comt\u00e9 was conducted. Clinical and pathological reports were used to collect the data. A methodology based on distributional random forest was developed using 9 clinico-pathological characteristics. This methodology can be used particularly to identify the patients of the training cohort that share similarities with the new patient and to predict an estimate of the distribution of the ODX score. \n\nResults: The mean age of participants id 56.9 years old. We have correctly classified 92% of patients in low risk and 40.2% of patients in high risk. The overall accuracy is 79.3%. The proportion of low risk correct predicted value (PPV) is 82%. The percentage of high risk correct predicted value (NPV) is approximately 62.3%. The F1-score and the Area Under Curve (AUC) are of 0.87 and 0.759, respectively. \n\nConclusion: The proposed methodology makes it possible to predict the distribution of the ODX score for a patient and provides an explanation of the predicted score. The use of the methodology with the pathologist\u2019s expertise on the different histological and immunohistochemical characteristics has a clinical impact to help oncologist in decision-making regarding breast cancer therapy. \n\n\n\nKeywords: Breast Cancer, Oncotype DX, Clinico-pathological data, Machine Learning, Distributional Random Forest.\n\n\n\n\u00a7 INTRODUCTION\n\nThe Oncotype DX (ODX) test is a commercially available molecular test for breast cancer assay (Genomic Health) that provides prognostic and predictive breast cancer recurrence information for hormone positive, HER2-negative patients. The ODX test is based on 1A-level evidence and it is included in the main international clinical guidelines recommendations such as those of the American  Society of Clinical Oncology (ASCO <cit.>) or the National Comprehensive Cancer Network (NCCN) as well as in the last staging guidelines of AJCC 8th edition <cit.>.\nThe ODX test is the most widely available molecular test used in the world. This assay analyzes 21 genes by RT-qPCR (16 cancer-related genes and 5 housekeeping genes) and aims to predict the risk of recurrence at 10 years by providing a recurrence score ranging from 0 to 100 and to estimate the benefit of adjuvant chemotherapy. Several retrospective and prospective studies have validated this test and its clinical utility. <cit.> have shown a correlation between ODX score and disease-free survival in patients with ER-positive/HER2-negative, node negative, tamoxifen-treated breast cancer, based on the NSABP B-14 trial. As for the chemotherapy benefits, <cit.> and <cit.> have evaluated the test using the studies related to NSABP-B20 and SWOG 8814. The prospective phase III trial TAILORx study <cit.> has modified the ODX score\u2019s cutoff values (low risk <11, intermediate risk 11-25 and high risk >25) in order to avoid under-treatments of cancer. To be more precise, in the low group, the risk of recurrence at 5 years is very low (<10%) with hormonal therapy, which confirms the uselessness of adding a chemotherapy <cit.>. For the intermediate group, chemotherapy has a benefit only for women younger than 50 years old. For the high-risk group, the chemotherapy is highly recommended. \nNevertheless, one third of women with  hormone-receptor positive  breast  cancer have a lymph node disease. Thus, the prospective trial RxPONDER trial study analyzes the capacity of the ODX test to predict the benefit of chemotherapy for women with positive lymph node disease <cit.>. RxPONDER showed that postmenopausal patients with node involvement and an ODX score between 0 and 25 did not benefit from chemotherapy, whereas premenopausal patients with node involvement with 1-3 nodes and ODX scores between 0 and 25 benefited significantly from chemotherapy. \n\nDespite its proven value, the ODX test is not routinely used due to its high cost. For this reason, less than 20% of patients in Europe have access to the ODX test. Health-related economic study are performed to understand for which patients the assay is the most useful <cit.>.  From this economic point of view, many alternative tools have been developed to predict this score. These tools are based on clinico-pathological data such as Magee equations <cit.> and the IHC4 score <cit.>. Indeed, many studies have shown the correlation between the results of the latter tools and the ODX score <cit.>. \nFew works used features with machine learning techniques in order to provide an ODX-based methodology to divide the patients into categories corresponding to low or high risk of cancer <cit.>.\n\nThe aim of this paper is to propose a novel methodology to assist physicians in their decision-making.  It is based on random forests for distributional regression as presented in              <cit.> and <cit.>. This methodology creates links between a new patient and the cohort used for training based on clinico-pathological characteristics. These links can be used particularly to identify the patients of the training cohort that share similarities with the new patient and to predict an estimate of the distribution of the ODX score. This information is available to clinicians to help them better understand the probable clinical evolution of the tumor in order to optimize the treatment. \n \nMoreover, it enables  knowledge capitalization by feedback and analysis of patient history. One of the consequences of this study is to weight the variability of the anatomo-pathological data, so this new methodology can adapt to the specificities of a cohort.\n\n\n\n\n\u00a7 MATERIALS AND METHODS\n\n\n\n\n \u00a7.\u00a7 Dataset description\n\nThe cohort is a retrospective study between 2012 and 2020 with 333 cases that underwent an ODX assay from three hospitals in Bourgogne Franche-Comt\u00e9: Besan\u00e7on, Belfort and Dijon. All patients have ER-positive and HER2-negative early breast cancer. Clinical and pathological reports were used to collect the data such as the age at diagnosis, the menopausal status, the treatment, the recurrence, the tumor size, the lymph node status, the histological type, the Nottingham grade, hormone receptors for estrogen (ER) expression, hormone receptors for progesterone (PR) expression, the human epidermal growth factor receptor 2 (HER2) status and the protein p53 and Ki67 proliferation index. Immunohistochemical staining was performed (Ventana Benchmark XT system\u00ae, Roche\u2122) on the tumor block of ODX testing with UltraView Universal DAB detection with ER antibody (clone SP1; Roche/Ventana Medical Systems, Tucson, USA), PR antibody (clone 1E2; Roche/Ventana Medical Systems, Tucson, USA), HER2 antibody (clone 4B5; Roche/Ventana Medical Systems, Tucson, USA), Ki67 antibody (clone Mib-1, Dako, Glostrup, Denmark) and p53 antibody (clone DO-7, Dako, Glostrup, Denmark). \nThe HER2 immunostaining was interpreted using the 2018 American Society of Clinical Oncology/College of American Pathologists guidelines <cit.>. The Ki67 proliferation index was evaluated by manual counting with counter on at least 200 tumor cells. The protein p53 was assessed by immunohistochemistry. The positive threshold is greater than 10% of the tumor cells' nuclei.\nThe ODX test was realized by Genomic Health (Redwood City, CA, USA) and analyzed 21 genes by RT-qPCR from paraffin-embedded blocks of tumor tissue. The ODX score was obtained from the clinical reports. The three  ODX categories were the same as the ones defined in the ODX\u2018s assays using TAILORx and RxPONDER: low risk (< 16), intermediate risk (16-25) and high risk (> 25). \nThe institution review board approved this study.\n\nThe cohort contains more than 50 features, from which we selected the most critical ones using feature importance in random forest and physicians' assessments. Table <ref> describes the tumor characteristics using the  features selected for our study.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Distributional Random Forest\n\nRandom Forest <cit.>  is a powerful machine learning algorithm that can be used for prediction in various settings and has been successfully applied in the field of medicine <cit.>. Our goal here is to predict the result of the expensive ODX test based on clinico-pathological features. We propose the use of Distributional Random Forest that provides a predictive distribution for the ODX score based on the clinico-pathological features. We shall expose the methodology for Random Forest and Distributional Random Forest.\n\nStandard regression links the mean of the response variable Y to a set of features X based on observations from a training sample of feature\u2013response pairs, say (X_i,Y_i) for i=1,\u2026,n. Random Forest (RF) prediction is an ensemble method that consists of the bootstrap aggregation <cit.> of randomized classification and regression trees (CART, <cit.>).\nThe predictive mean can be written as the average\n\n    \u0176=1/B\u2211_b=1^B T_b(X),\n\nwhere T^1(X),\u2026,T^B(X) corresponds to the prediction of the different trees built on different bootstrap samples. Each single tree prediction takes the form of an average across a neighborhood of X in the tree, i.e.\n\n    T_b(X)=1/|R_b(X)|\u2211_X_i\u2208 R_b(X) Y_i,\n\nwith R_b(X) being the region of the feature space that contains X in the tree T_b and |R_b(X)| the numbers of observations that fall into this region. Consequently, the Random Forest prediction (<ref>) has the equivalent form\n\n    \u0176=\u2211_i=1^n w_i(X) Y_i,\n\nwith the Random Forest weights defined by\n\n    w_i(X)=1/B\u2211_i=1^B 1_{X_i\u2208 R(X)}/|R_b(X)|,   1\u2264 i\u2264 n,\n\nand these weights are non-negative with sum 1 (probability weights).\n\nThe main idea of Distributional Random Forest (DRF) relies on Equation\u00a0(<ref>): the prediction \u0176 is the sample mean of the weighted sample Y_i with weights w_i(X) which can  be seen as an approximation of the conditional distribution of Y given X. The cumulative distribution function F(y| X)=\u2119(Y\u2264 y|X) is thus approximated by\n\n    F\u0302(y|X)=\u2211_i=1^n w_i(X) 1_{Y_i\u2264 y}.\n\nThis idea was first suggested by Meinshausen <cit.> who proposed the construction of quantile regression forest by approximating the conditional quantile of Y given X  by the  quantiles of the weighted empirical distribution (<ref>).\n\nFigure\u00a0<ref> presents a synthetic representation of the DRF procedure with the different steps: subsampling of the original sample, tree construction on each subsample, computation of the neighborhood/weight at the point to predict, averaging of weights given by the different trees that finally provide the predictive distribution.\n\nThe Random Forest weights (<ref>) are interesting in themselves and provide relevant information in terms of similar/influential observations. Given a new feature X, the weight w_i(X) is interpreted as the proportion in which the observation Y_i contributes to the prediction of Y given X. Observations with the largest weights are interpreted as the nearest neighbors of X in terms of an implicit metric on the predictor space that is tailored  for predicting the response, see <cit.>. The random forest weights make it possible to identify the most similar/influential individuals in the training data. Comparing X to these similar observations can help understand the relationship between X and Y.\n\n\n\nFinally, let us mention that the weights (<ref>)-(<ref>) depend on the specific structure of the trees that are used for prediction. Trees are grown by recursive binary splitting, maximising a homogeneity criterion; the goal is to partition the feature space into different regions that are as homogeneous as possible. In the standard CART algorithm, the variance is used as the homogeneity criterion, resulting in a partition adapted to the prediction of the mean. Several different splitting rules have been considered in the statistical literature that put the emphasis on the prediction of quantiles <cit.> or on the overall distribution <cit.>. \n\nA Distributional Random Forest is fitted to the whole data set. The software R with the package  (Generalized Random Forest) is used to compute the random forest and the associated weights. When no new test set is provided, the  method performs out-of-bag prediction on the training set. This means that, for each training example, all the trees that did not use this example during the training are identified (the example was \u2018out-of-bag\u2019), and a prediction for the test example is then made using only these trees.\n \n\n\n\n\n\n\n \u00a7.\u00a7 Applications of Distributional Random Forest\n\nDRF is a fully non-parametric and model-free method that performs probabilistic forecast and distributional regression. For a set of features X, it provides the full predictive distribution of the response variable Y, that is to say exhaustive information for its possible fluctuations knowing the features. The method is very informative and powerful (see Figure <ref>) as it provides:\n\n    \n  * (distributional regression) a predictive distribution for each new case that can be represented by a histogram;\n    \n  * (mean or median prediction)  a predictive mean or median when a point estimate is needed - the mean is commonly used while the median is more robust to outliers; \n    \n  * (uncertainty assessment) a graphical assessment of the uncertainty with the shape of the histogram (either peaked or flat) or numerical statistics  such as standard error or confidence interval for the prediction;\n    \n  * (classification)  the probability of classes of particular interest can be instantly computed - for the ODX score, the classes ODX\u2264 25 and ODX> 25 are considered;\n    \n  * (similar/influential patients) the  patients in the cohort (training set) that are the most similar to a new case can be easily identified through the random forest weights that are interpreted as a measure of proximity - this proximity is meant in the sense of an implicit distance that is learnt by the model and that gives more importance to the relevant features; this information can allow the practitioner to make meaningful and informative comparisons between the new case and the patients from the cohort.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Evaluation of predictive performance\n\n\n\nIn order to evaluate the distributional random forest algorithm and compare it with concurrent methods, the theory of a proper scoring rule  <cit.> is used. In probabilistic forecasting, a scoring rule compares a predictive distribution F and the outcomes y. It plays the role of a measure of error similar to  the mean squared error in regression  or the misclassification rate in  classification. A scoring rule is strictly proper if the expected score is minimal when the predictive distribution F matches the outcome distribution. A strictly proper scoring rule can be used for the evaluation of probabilistic forecast and distributional regression <cit.>. \n\nThe most popular scoring rule is the Continuous Ranked Probability Score (CRPS) <cit.> and is defined by \n\n    CRPS(F,y)=\u222b_\u211d (F(z)-1_{y\u2264 z})^2 dz.\n\n In a case where the predictive distribution F corresponds to a weighted sample (y_i)_1\u2264 i  \u2264 n with weights (w_i)_1\u2264 i\u2264 n, the CRPS is easily computed by \n\n    CRPS(F,y)=\u2211_i=1^n w_i|y_i-y| -\u2211_1\u2264 i<j\u2264 nw_iw_j|y_i-y_j|.\n\nThe first term compares the predictive distribution F and observation y (calibration) while the second term assesses the precision of the prediction (sharpness). This expression also shows that   CRPS(F,y) is reported in the same unit as the observation y and that it generalizes the absolute error to which it is reduced if F is a deterministic forecast, that is to say a point measure.\n\nIn order to evaluate the generalization capacity of the model, that is to say its predictive performance on a new sample, different validation methods can be used to assess the prediction error. Simple validation uses a training set to fit the model and an independent test set to compute error (CRPS). K-fold cross validation is more involved and splits  the data into  K groups that successively play the role of the test set. More precisely, K different models are fitted on training sets consisting of all folds but one  which is left-out during training and used as a test set to compute the CRPS; this results in K different test errors which are averaged so as to obtain the K-fold cross validation error.\nIn the specific case of bagging including our random forest method, the out-of-bag (OOB) method can be used instead. It usually provides similar results as  K-fold cross validation but is much more numerically efficient since only one fit of the model is required. Indeed, due to resampling, a given observation does not belong to all the subsamples and one can consider the submodel aggregating all the trees that were trained without this observation; this submodel is then evaluated at the observation and the error (CRPS) is computed; averaging all these errors yield the OOB error.  \n\n\n\n\n\n\n\u00a7 RESULTS\n\nThe DRF was applied to 333 patients to predict the ODX score  using the 9 features presented in Table <ref>. In order to compare with the literature, we emphasize the classification into two classes (ODX\u2264 25 and ODX>25). \n\nBefore presenting the results of the DRF, we shall first present the evaluation of our model. Simple graphical diagnostics can be performed by considering the regression model deduced from DRF. The results of the regression are presented in Figure\u00a0<ref>, where the predictive mean (Figure\u00a0<ref>a) and predictive median (Figure\u00a0<ref>b) versus the real ODX score are plotted. We can observe a rather good fit, and that an important proportion of the observations are within in their confidence intervals. In Figure <ref>a,  the grey ribbon has a  semi-amplitude equal to the standard error and accounting for uncertainty. The grey ribbon in Figure <ref>b represents the credibility interval with a level of 90%. \n\n\nAdditionally, in order to assess the ODX probabilistic forecast, we compared the OOB predictive distribution and the actual observation for the ODX, for each observation. The prediction error is measured in terms of the CRPS introduced in Section\u00a0<ref>. The different scores are represented in Figure\u00a0<ref>a. The smaller the CRPS, the more accurate the forecast. We can observe that most of the predictions have a small or medium CRPS, which indicates the overall good quality of prediction. A smaller number of observations have a large CRPS, indicating individuals for whom the ODX score notably differs from what we might expect in comparison with the overall population. Together with the CRPS, the figure provides the results for the binary classification task (ODX\u2264 25 or ODX> 25): classification errors are indicated with the color orange while the color blue corresponds to correctly classified observations. We can observe a good match between classification errors and a large CRPS, which confirms the ability of the CRPS to assess forecast quality. Then, for each patient, the DRF provides a predictive distribution represented by a histogram that can be compared with the actual ODX score. We also indicate the two class probabilities corresponding to the light-green/left or dark-green/right classes. \n\nWe have selected three patients respectively  with a low  (Figure\u00a0<ref>b, Patient A), medium  (Figure\u00a0<ref>c, Patient B)  and large CRPS (Figure\u00a0<ref>d, Patient C). The predictions associated to these patients can be considered \"good\", \"average\" and \"bad\", respectively. In Figure\u00a0<ref>b we can observe a sharp predictive distribution (peaked histogram) and an ODX score close to the peak. In Figure\u00a0<ref>c, the histogram is flatter, indicating more uncertainty, and the true ODX score is contained in a high probability region. In Figure\u00a0<ref>d, the predictive distribution has also a large dispersion and the ODX score is contained in a low probability region, which means that the match between the two is poor. We insist on the fact that a large CRPS does not necessary mean a miss-classification of a patient as it can be seen for some of the higher CRPS values in Figure\u00a0<ref>a. The CRPS considers the distributional regression and is not explicitly related to the binary classification presented here.\n\n\nDue to the impact of the classification of ODX in the two classes ODX\u2264 25 and ODX>25, we shall present the detailed evaluation of the classification model deduced from DRF (see Table <ref>).  This evaluation is based on the standard classification metrics such as confusion matrix and standard metrics. The standard metrics are as follows:\n\n    Accuracy = TP+TN/TP +FP + FN + TN,\n\n\n    Sensitivity = TP/TP +FN,\n\n\n    Specificity = TN/FP + TN,\n\n\n    Positive Predictive Value = TP/TP +FP,\n\n\n    Negative Predictive Value = TN/FN+TN,\n\n\n    F1-score = 2*Positive Predictive Value*Sensitivity/Positive Predictive Value+Sensitivity\n\nwhere TP is the number of patients correctly classified as ODX \u226425, FP is the number of patients incorrectly classified as ODX \u226425, TN is the number of patients correctly classified as ODX >25 and FN is the number of patients incorrectly classified as ODX >25.\n\n\n\n\n\nWe have correctly classified 231 out of 251 patients (92%) in low risk and 33 of 75  patients (40.2%) in high risk. The overall accuracy is 79.3% and the p-value is less than 0.05. The proportion of low risk correct predicted value (PPV) is 82%. The percentage of high risk correct predicted value (NPV) is approximately 62.3%. The F1-score and the Area Under Curve (AUC) are of 0.87 and 0.759, respectively. The DRF will provide additional information such as the nearest neighbor patients, the distribution of the ODX score and the uncertainty prediction (see Figure <ref>). We now consider the 69 miss-classified patients with low and high risks. First of all, we notice that the majority of these patients have predictions that are close to the decision border (i.e. close to ODX=25). These patients are miss-classified because of the binary decision and additional information available with the DRF method shows either that the patient's ODX score is close to the decision border or that the neighborhood of the patient is not realistic because of limitations of the training cohort. This first part of the miss-classified patients might have a small CRPS as the CRPS accounts for the dispersion of the prediction and its bias. The second part of the miss-classified patients correspond to extreme values of the ODX score within our cohort. The nearest patients provided by the DRF for these miss-classified patients are thus less informative as they are taken within the cohort that is not representative of these outlier patients. In order to give more quantitative results, we compared the mean absolute difference for the ODX score, Ki67 and p53 between the 69 miss-classified patients and the weighted average value of their neighborhoods. The miss-classified patients have a mean absolute difference of ODX score compared to their neighborhood of 9.84 where the correctly classified patients have an average absolute difference of 6.29. In terms of Ki67 and p53, the average absolute difference is 24.56% and 5.77% respectively when the average absolute difference for the correctly classified patient is 16.77% for Ki67 and 5.84% for the p53 respectively.\n\nThese classification results are then compared with state-of-the art techniques <cit.>. A detailed comparison is given in Table <ref>.\n\n\n\n\n\n\u00a7 DISCUSSION\n\nODX is the most commonly availabe breast genomic test used in early stage ER postive/HER2-negative breast cancer. It makes it possible to define patients who are unlikely to benefit from chemotherapy. The ODX score is based on 6 gene groups. These groups correspond to the markers analysis in pathological reports. Some have compared the ODX score to this immuno-histological data and proved the predictive relationship with the ODX score. Several studies were published using this clinicopathological data to predict the ODX score with different methods (see Table <ref>). The present study was realised to predict the ODX score from a specific regional cohort of 333 patients with clinical and immuno-histological data using Distributional Random Forest. This prediction is associated with a predictive error on the one hand, and the ability to determine the similar patients on the other hand. The proposed DRF model detected 82% of lower risk patients (ODX\u2264 25) and 62.3% of high risk patients (ODX> 25). \n\nA few studies have proposed some prediction tools for the ODX score  <cit.>. Each study is based on the specific categorization of patients according to the original ODX categories and TAILORx (see ODX Prediction Threshold in Table <ref>). The prediction results of the different studies are similar and based on clinico-pathological data. The tumor size, tumor grade and PR are used in all the six selected published studies as well as for our current study. The Ki67 is not used in <cit.> and <cit.>. In our study, we integrated the p53.  The  threshold  used for the ODX score is different from one study to another. Our DRF model performs as well as the other prediction tools.  The novelty is in providing additional information to the prediction (see Figure <ref>) such as the probability of classes (low and high risk), the similar profiles and the uncertainty prediction.\n\nThe correct predicted values are 82.5% and 62.3% for low and high risk, respectively. We used the CRPS score to distinguish the best and worst prediction. The best results were obtained for ODX profiles below 16. The average Ki67, for the first best ten results, is under 14%, which corresponds to the low-risk profile of our previous study <cit.>. The average percentages of ER and PR are 93% and 77%, respectively, which fits into the same low risk profiles. When looking at the surrounding family and the profile of close patients, we observe that similar profiles vary between 0 and 25 for ODX. The similarities fall in the low risk profile. The Ki67 scores of similar profiles for the first ten results are below 25%.\n\n\n\n\n\n\n\n \n\n\n\nAs for the results that are discordant, they lie in the high risk class. The averages of the ODX score,  Ki67 and PR are respectively 46%, 36% and 22%. In addition, a negative correlation between ODX and PR for the best and worst results can be observed.  The similar profiles for such cases have a high PR. This behavior is due to the small number of cases in the high risk category.  An example of the worst prediction is a patient with a high ODX score and probability of lying in the high class of 50%.  The real ODX score is 49 and the predicted ODX score is near to the cut-off. The average ODX score for the 10 first similar profiles is 31 and the distribution is centered around 25. The similar profiles are very dispersed, which is difficult to analyze. Most of the nearest neighbors have an SBR grade of 3. The prediction is bad, but nevertheless, the similar profiles have a low ODX score and a high SBR grade.\n The size of the cohort and the training and testing phase could impact the prediction results. In addition, we have an unbalanced cohort in our study, since we have less patients in the high risk class. In that case, the factors of the similar profiles that influences ODX score such as PR, Ki67 and p53 should be considered. The distribution of identical profiles allows the clinician to retrieve similar historical cases in terms of evolution. The proposed model can be applied even when there is missing data. It makes it possible to predict the low risk class with high certitude, which means no chemotherapy to plan. \nOur study is related to the dataset and it is therefore difficult to generalize to a different cohort because of known inter-cohort variability, especially on some biomarkers such as Ki67. \n\n\n\n\u00a7 CONCLUSION\n\nThis paper proposes a new methodology for oncotype scoring prediction. This methodology is based on distributional random forest and using 9 clinico-pathological features. It makes it possible to predict the distribution of the ODX score for a patient and provides an explanation of the predicted score by computing the probability of belonging to the low or high risk category and identifying the nearest similar profiles. The proposed Distributional Random Forest model detects 82% of lower risk patients (\u2264 25) and 62.3% of patients with high risk (> 25).  However, DRF presents certain limitations.  The use of DRF with the pathologist\u2019s expertise on the different histological and immunohistochemical characteristics has a clinical impact to help oncologist in decision-making regarding breast cancer therapy. \nThe medico-economic interest of this strategy is obvious. Additional studies are needed to further validate the DRF method and improve knowledge extraction from pathological data.\n\n\n\nplainnat\n\n\n\n\n\n"}