{"entry_id": "http://arxiv.org/abs/2303.07027v1", "published": "20230313114408", "title": "Adaptive Dereverberation, Noise and Interferer Reduction Using Sparse Weighted Linearly Constrained Minimum Power Beamforming", "authors": ["Henri Gode", "Simon Doclo"], "primary_category": "eess.AS", "categories": ["eess.AS"], "text": "\n\n\n\n\n\nAdaptive Dereverberation, Noise and Interferer Reduction Using Sparse Weighted Linearly Constrained Minimum Power Beamforming\n\n\n\nThis work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) \u2013 Project ID 390895286 \u2013 EXC 2177/1.\n\n    Henri Gode and Simon Doclo\nDepartment of Medical Physics and Acoustics and Cluster of Excellence Hearing4all,\nUniversity of Oldenburg, Germany \n\n{henri.gode, simon.doclo}@uni-oldenburg.de\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    March 30, 2023\n====================================================================================================================================================================================================================================================================\n\n\n\n\nInterfering sources, background noise and reverberation degrade speech quality and intelligibility in hearing aid applications. In this paper, we present an adaptive algorithm aiming at dereverberation, noise and interferer reduction and preservation of binaural cues based on the wBLCMP beamformer. The wBLCMP beamformer unifies the multi-channel weighted prediction error method performing dereverberation and the linearly constrained minimum power beamformer performing noise and interferer reduction into a single convolutional beamformer. We propose to adaptively compute the optimal filter by incorporating an exponential window into a sparsity-promoting lp-norm cost function, which enables to track a moving target speaker. Simulation results with successive target speakers at different positions show that the proposed adaptive version of the wBLCMP beamformer outperforms a non-adaptive version in terms of objective speech enhancement performance measures. \n\n\n\n\nnoise reduction, dereverberation, online processing, convolutional beamformer, multi-microphone\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nIn many hands-free speech communication systems such as hearing aids, mobile phones and smart speakers, interfering sounds, ambient noise and reverberation may degrade the speech quality and intelligibility of the recorded microphone signals\u00a0<cit.>. \n\nTo enhance speech quality and intelligibility, many multi-microphone speech enhancement methods aiming at noise and interferer reduction and dereverberation have been proposed in the last decades\u00a0<cit.>. For many of these methods, both non-adaptive versions with time-invariant parameters as well as adaptive versions with time-varying parameters exist. When considering binaural hearing aids, it is often desired to preserve the binaural cues, which provide spatial awareness of the acoustic scene for the listener\u00a0<cit.>.\n\n\nA commonly used multi-microphone noise reduction method is the MPDR beamformer\u00a0<cit.>, which aims at minimizing the output power while leaving the desired speech component undistorted. The LCMP beamformer generalizes the MPDR beamformer, providing the possibility of multiple linear constraints, e.g., to perform controlled reduction of the interfering sources\u00a0<cit.>. Often the constraints are formulated in terms of the RTF vectors of the target speaker and interfering sources\u00a0<cit.>. \n\n\n\nTo achieve dereverberation, the WPE method\u00a0<cit.> and its generalization using sparse priors \u00a0<cit.> are commonly employed. \nWPE uses a convolutional filter, applied to a number of past frames in the STFT domain, to estimate and subtract the late reverberation component.\n\nSince the WPE cost function does not have an analytic solution, it has been proposed to use iterative alternating optimization schemes. In\u00a0<cit.> adaptive versions of the WPE algorithm have been proposed, e.g., by incorporating an exponential window into the cost function and incorporating an additional constraint to prevent overestimation of the late reverberation\u00a0<cit.>.\n\n\nAiming at joint dereverberation and noise reduction, it has been proposed to perform MIMO-WPE as a preprocessing stage before MPDR beamforming, in a cascade system\u00a0<cit.>. By unifying the optimization of the convolutional WPE filter and the MPDR beamformer, the so-called WPD beamformer\u00a0<cit.> \nand its generalization using sparse priors\u00a0<cit.> were shown to outperform cascade systems. The unified WPD beamformer \nis optimized similarly to the WPE filter with an additional distortionless constraint using the RTF of the target speaker.\nIn\u00a0<cit.> two adaptive versions of the WPD algorithm have been proposed.\n\n\n\n\n\n\n\n\n\nAiming at joint dereverberation, reduction of interfering sources and noise and preservation of the binaural cues of all sources, the wBLCMP beamformer in\u00a0<cit.> generalizes the WPD beamformer by unifying the optimization of the convolutional WPE filter and the LCMP beamformer. \nSimilarly to\u00a0<cit.>, in this paper, we derive an adaptive version \n\n\nby incorporating an exponential window into the cost function, which enables tracking of a moving target speaker. In addition, similarly to\u00a0<cit.>, we explicitly control the sparsity of the STFT coefficients by using an \u2113_p-norm cost function. For a complex acoustic scenario featuring a target speaker which suddenly switches position, an interfering source at a fixed position and diffuse babble noise, simulation results show that the adaptive version of the wBLCMP beamformer clearly outperforms its non-adaptive version in terms of objective speech enhancement performance measures and RTF vector estimation accuracy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 SIGNAL MODEL\n\n\nWe consider J acoustic sources captured by a binaural microphone array setup with M/2 microphones on each of two head-worn hearing devices (e.g. left and right hearing aid)  in a noisy and reverberant acoustic environment (with J < M). Without loss of generality, the first source (j=1) is considered to be the target speaker and the remaining J-1 sources are considered to be interfering sources. The STFT coefficients of the microphone signals at time frame t are denoted as\n\n    \ud835\udc32_t = [ y_1,t     \u22ef y_M,t ]^T\u2208\u2102^M\u00d7 1,\n\nwith (\u00b7)^T denoting the transpose operator. In (<ref>) the frequency index has been omitted since it is assumed that each frequency subband is independent and hence can be processed individually. \n\n\n\n\nSimilarly to\u00a0<cit.>, the multi-channel microphone signal \ud835\udc32_t in (<ref>) is modeled as the sum of each source signal s_j,t convolved with its possibly time-varying multi-channel CTF matrix \ud835\udc00_j,t= [     \ud835\udc1a_j,t,0           \u22ef \ud835\udc1a_j,t,L_a-1 ]\u2208\u2102^M\u00d7 L_a plus background noise \ud835\udc27_t \u2208\u2102^M\u00d7 1, i.e.\n\n    \ud835\udc32_t = \u2211_j=1^J\u2211_l=0^L_a-1\ud835\udc1a_j,t,l s_j,t-l +  \ud835\udc27_t,\n\nwhere L_a denotes the number of taps of the CTF. By splitting the CTF into the early reflections and late reverberation using the integer parameter \u03c4, the reverberant signal for the j-th source can be decomposed into its direct component \ud835\udc1d_j,t\u2208\u2102^M\u00d7 1 (including early reflections)\nand its late reverberation component \ud835\udc2b_j,t\u2208\u2102^M\u00d7 1, i.e.\n\n    \ud835\udc32_t =  \u2211_j=1^J\u2211_l=0^\u03c4-1\ud835\udc1a_j,t,l s_j,t-l_\ud835\udc1d_j,t +  \u2211_j=1^J\u2211_l=\u03c4^L_a-1\ud835\udc1a_j,t,l s_j,t-l_\ud835\udc2b_j,t +  \ud835\udc27_t.\n\n\nThe direct component for the j-th source \ud835\udc1d_j,t can be approximated using the MTF vector \ud835\udc2f_j,t\u2208\u2102^M\u00d7 1 as\u00a0<cit.>\n\n    \ud835\udc1d_j,t\u2248\ud835\udc2f_j,t s_j,t = \ud835\udc2f_j,m,t d_j,m,t,    m \u2208{1,...,M},\n\nwhere d_j,m,t\n\ndenotes the direct component of the j-th source in the reference microphone m at time frame t.\n\nThe vector\n\n    \ud835\udc2f_j,m,t = \ud835\udc2f_j,t / v_j,m,t\u2208\u2102^M\u00d7 1\n\ndenotes the possibly time-varying RTF vector for the j-th source, where v_j,m,t is the m-th entry of \ud835\udc2f_j,t. \n\n\n\n\u00a7 SPARSE WBLCMP FILTER\n\nTo obtain an estimate \nof the direct target speech component d_1,\u03bd,t in the left and right \nreference microphone denoted by m = \u03bd\u2208{L,R}, it has been proposed in\u00a0<cit.> to apply a convolutional filter \ud835\udc21_\u03bd,t\u2208\u2102^M(L_h-\u03c4+1)\u00d7 1 to the stacked noisy STFT vector \ud835\udc32_t, i.e.\n\n    d\u0302_1,\u03bd,t = \ud835\udc21_\u03bd,t^H\ud835\udc32_t,\n\nwhere (\u00b7)^H denotes the conjugate transpose operator and the stacked noisy STFT vector \ud835\udc32_t is defined as\n\n    \ud835\udc32_t   = [       \ud835\udc32^T_t           |     \ud835\udc32^T_t-\u03c4           \u22ef \ud835\udc32^T_t-L_h+1 ]^T\u2208\u2102^M(L_h-\u03c4+1)\u00d7 1,\n\nwhere L_h denotes the filter length.\nIt should be noted that the vector \ud835\udc32_t only includes a subset of the L_h most recent frames, i.e. it includes the current frame but excludes the preceding \u03c4-1 frames, aiming at preserving the early reflections.\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Non-Adaptive Version\n\n\nBy assuming that all CTF and MTF and the convolutional filter \ud835\udc21_\u03bd,t do not change over time, i.e. \ud835\udc21_\u03bd,t = \ud835\udc21_\u03bd for all time frames t\u2208{1,\u2026,T}, a non-adaptive version of the wBLCMP beamformer aiming at joint dereverberation, noise and interferer reduction \nhas been derived in\u00a0<cit.>.\n\n\nIn\u00a0<cit.>, assuming that the direct component of the target speaker follows a zero mean complex Gaussian distribution with a time-varying variance \u03bb_n=d_1,\u03bd,n^2, the convolutional filter in (<ref>) is computed by minimizing the negative log-likelihood function\n\n    _\ud835\udc21_\u03bd\u2211_n=1^Tln\u03bb_n+ d\u0302_1,\u03bd,n^2/\u03bb_n = \u2211_n=1^Tln\u03bb_n+\ud835\udc21_\u03bd^H\ud835\udc32_n^2/\u03bb_n,\n\nsubject to a linear constraint for each source using their RTF defined in (<ref>), i.e.\n\n    \ud835\udc21_\u03bd^H\ud835\udc2f_j,\u03bd =   \u03b2_j            \u2200 j \u2208{1,\u2026,J}\n    \ud835\udc2f_j,\u03bd =   [ \ud835\udc2f_j,\u03bd^T     0^T ]^T,\n\nwhere 0 denotes a vector containing M(L_h-\u03c4) zeros and \u03b2_j denotes a scaling factor for the direct component of the j-th source. The scaling factor \u03b2_1 is usually set to 1, corresponding to a distortionless constraint for the target speaker, whereas all other scaling factors are usually chosen to be close to 0, aiming at suppressing the interfering sources.\n\nIn this paper, we aim at explicitly taking into account that the STFT coefficients of the direct target speech component are sparser than the STFT coefficients of the noisy reverberant mixture recorded by the microphones\u00a0<cit.>. \nHence, similarly to the WPE variant in\u00a0<cit.> and the WPD variant in\u00a0<cit.>, we propose to minimize the convolutional filter in (<ref>) using an \u2113_p-norm cost function instead of (<ref>), i.e.\n\n    _\ud835\udc21_\u03bd\u2211_n=1^Td\u0302_1,\u03bd,n^p = \u2211_n=1^T\ud835\udc21_\u03bd^H\ud835\udc32_n^p\n\nwhere p \u2208 (0,2] denotes the so-called shape parameter. This parameter determines the sparsity of the cost function, where small values of p promote sparsity. It should be noted that for 0 < p < 1 this cost function is non-convex. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Adaptive Version\n\n\nTo deal with time-varying acoustic scenarios, e.g. moving sources, in this paper we derive an adaptive version of the wBLCMP beamformer. Similarly as in\u00a0<cit.>, we propose to incorporate an exponential window into the cost function in (<ref>). The resulting minimization problem for each time frame t is given by \n[box=]align\n    \n      _\ud835\udc21_\u03bd,t \u2211_n=1^t \u03b3^t-nd\u0302_1,\u03bd,n^p =  \u2211_n=1^t\u03b3^t-n\ud835\udc21_\u03bd,t^H\ud835\udc32_n^p\n    s.t.  \ud835\udc21_\u03bd,t^H\ud835\udc2f_j,\u03bd,t = \u03b2_j  \u2200j \u2208{1,\u2026,J},\n   \n\n\nwhere the smoothing parameter \u03b3\u2208 (0,1] allows adaptation to possibly time-varying CTF and MTF. Note that the cost function in (<ref>) reduces to the cost function in (<ref>) for \u03b3 = 1 and t=T. Therefore, the following derivations based on the adaptive cost function in (<ref>) for the adaptive version also hold for the cost function in (<ref>) for the non-adaptive version.\n\n\n\n\n\n\n \u00a7.\u00a7 Filter Optimization\n\n\n\nSimilarly as in\u00a0<cit.>, we propose to use an IRLS procedure to minimize the cost function in\u00a0(<ref>) subject to the constraints in\u00a0(<ref>). The basic idea is to replace the non-convex \u2113_p-norm minimization problem with a series of convex \u2113_2-norm minimization subproblems, which have an analytic solution. In this paper we used only the first iteration of IRLS, since preliminary results indicated sufficient convergence. \n\n \n\n\n\n  \u00a7.\u00a7.\u00a7 Constrained -Norm Subproblem Minimization\n\n\ntest \n In each frame, the non-convex cost function in (<ref>) is replaced with a convex weighted \u2113_2-norm cost function, i.e.\n\n    _\ud835\udc21_\u03bd,t\u2211_n=1^t\u03b3^t-nw_nd\u0302_1,\u03bd,n^2 =  \u2211_n=1^T\u03b3^t-nw_n\ud835\udc21_\u03bd,t^H\ud835\udc32_n^2\n\n\n\n\n\n\n\n\n\n\n\nwhere the weights w_n are real-valued and positive. The filter minimizing (<ref>) subject to the linear constraints in (<ref>) is equal to\n\n    \ud835\udc21_\u03bd,t = \ud835\udc11_y,t^-1\ud835\udc02_t(\ud835\udc02_t^H\ud835\udc11_y,t^-1\ud835\udc02_t)^-1\ud835\udc01\ud835\udc02_t^H\ud835\udc1e_\u03bd,\n \nwhere\n\n    \ud835\udc11_y,t = \u2211_n=1^t\u03b3^t-n w_n\ud835\udc32_n\ud835\udc32_n^H\n\ndenotes the weighted noisy STCM of the stacked microphone signals, \ud835\udc02_t =[ \ud835\udc2f_1,\u03bd,t       \u22ef \ud835\udc2f_J,\u03bd,t ] denotes the constraint matrix containing the RTF vectors for all sources, \ud835\udc01 = diag([ \u03b2_1   \u22ef \u03b2_J ]^T) denotes the diagonal scaling matrix containing the scaling factors for all sources, and \ud835\udc1e_\u03bd is a selection vector with its entry corresponding to the left or right reference microphone equal to 1 and all other entries equal to 0. Assuming that the weights w_n of past frames n\u2208{1,\u2026,t-1} are well estimated during processing of these past frames, the weighted noisy STCM \ud835\udc11_y,t in (<ref>) can be effectively computed by an recursive update in each frame, i.e. \ud835\udc11_y,t = \u03b3\ud835\udc11_y,t-1 + \n    w_t\ud835\udc32_t\ud835\udc32_t^H.\nHowever, since only the inverse of the weighted noisy STCM is required in (<ref>) it is more effective to use an update formula for \ud835\udc11_y,t^-1 based on the Woodbury matrix identity, i.e.\n\n    \ud835\udc11_y,t^-1 = 1/\u03b3(\ud835\udc11_y,t-1^-1 - w_t\ud835\udc11_y,t-1^-1\ud835\udc32\u0305_t\ud835\udc32\u0305_t^H\ud835\udc11_y,t-1^-1/\u03b3 + w_t\ud835\udc32\u0305_t^H\ud835\udc11_y,t-1^-1\ud835\udc32\u0305_t)\n\n\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Weight Estimation\n\n\ntest \n Similarly as in\u00a0<cit.>, in each frame t the weight w_t in (<ref>) is estimated as\n\n    w_t = ( \u2211_\u03bdd\u0302_1,\u03bd,t^2)^p/2-1 = ( \u2211_\u03bd\ud835\udc21_\u03bd,t^H\ud835\udc32_t^2)^p/2-1,\n\n\n\n\n\nsuch that (<ref>) is a first-order approximation of (<ref>). Note that the shape parameter p only affects the weight update in (<ref>) of the algorithm, where it is possible to set p=0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 RTF Estimation\n\n\nThe wBLCMP beamformer in\u00a0(<ref>) requires estimates of the RTF for each source, which can be obtained using the covariance whitening method\u00a0<cit.>. It has been shown in\u00a0<cit.> that performing RTF estimation on multi-channel dereverberated signals \ud835\udc33_t, obtained by a MIMO-WPE preprocessing stage, is beneficial, since the MTF-based model in (<ref>) assumes short transfer functions for the direct component. The block diagram in Fig.\u00a0<ref> shows an overview of the complete algorithm. Note that the computation time is not significantly increased by the MIMO-WPE preprocessing stage, since the wBLCMP filter can be effectively computed using the MIMO-WPE filter, because both are based on the convolutional signal model in (<ref>) and can be derived using the \u2113_p-norm cost function in (<ref>) <cit.>. The RTF vector of the j-th source can then be estimated based on the generalized eigenvalue decomposition of the dereverberated covariance matrix \ud835\udc11_j,t of that source and the dereverberated covariance matrix \ud835\udc11_v,j,t of all other sources and the background noise. Since accurately estimating all of these covariance matrices is far from trivial, in this paper, we will assume oracle knowledge about a noise-only period and a noise-plus-interferer period in the beginning of the signal, which are used to compute fixed covariance matrices of an interfering source and noise. In contrast, the covariance matrix and RTF vector of the target are tracked.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n\n\nIn this section, we compare the performance of the proposed adaptive version of the wBLCMP beamformer (Sec.\u00a0<ref>) with the non-adaptive version (Sec.\u00a0<ref>) using different shape parameters p for a spatially non-stationary acoustic scenario where the target speaker suddenly switches position.\n\n\n\n\n \u00a7.\u00a7 Acoustic Scenario\n\n\n\n\nWe considered 2 BTE hearing aids with 2 microphones each, mounted on a dummy head located approximately in the center of an acoustic laboratory (7\u00d76\u00d72.7) with a reverberation time T_60\u2248510.\nThe acoustic scenario consists of one target speaker (which suddenly switches position), one interfering speaker (at a fixed position) and background noise. The target and interfering speech components at the microphones were generated by convolving clean speech signals with room impulse responses measured from loudspeakers at about 2 from the dummy head. The target speaker at position 1 (0, front of dummy head) is a male speaker which is active in the interval [2,20.4], whereas the target speaker at position 2 (90, right of dummy head) is a female speaker which is active in the interval [20.4,39]. The interfering speaker is a male speaker which is located at -120 and is active in the interval [1,39].\nQuasi-diffuse babble noise, which is constantly active, was generated by playing back cafeteria noise \nusing 4 loudspeakers facing the corners of the laboratory.\n\nThe noisy mixture is constructed at a broadband SNR of 0 and a broadband SIR of 0 for both target positions. Note that there is a noise-only period in the 1^st second and a noise-plus-interferer period in the 2^nd second.\n\nThe sampling frequency was equal to 16.\n\n\n\n \u00a7.\u00a7 Algorithm Settings\n\n\nWe applied the wBLCMP beamformer within an STFT framework with a frame length of 32, a frame shift of t_s = 16 and a sqrt-Hann window for analysis and synthesis. We compared the performance of two shape parameters p={0,0.5}, since it has been shown in\u00a0<cit.> that a shape parameter of p=0.5 can be beneficial. The filter length L_h in (<ref>) was set to 16 frames corresponding to 256 covering about half of the T_60. The prediction delay \u03c4 was set to 3 frames corresponding to 48 aiming at preserving the early reflections.\nThe scaling factors of the target speaker and the interfering source in (<ref>) were set to \u03b2_1 = 0dB and \u03b2_2 = -20, respectively. Since preliminary results indicated reasonable convergence after the initial iteration of the alternating optimization described in Sec.\u00a0<ref>, we chose to stop after the first iteration to reduce computational cost. For the adaptive versions, different time constants are evaluated between t_\u03b3 = [100, 1500]. The smoothing parameter \u03b3 can be computed using the time constant as \u03b3 = e^-t_s/t_\u03b3. The noise-plus-interferer covariance matrix \ud835\udc11_v,2 and RTF vector of the interfering source \ud835\udc2f_2,\u03bd are fixed after the first 2, whereas the covariance matrix and RTF vector of the target speaker are adaptively tracked.\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Objective Speech Enhancement Measures\n\n\nAs objective performance measures we used the FWSSNR\u00a0<cit.>, and the SRR\u00a0<cit.> averaged across the left and right output signal.\n\n\n\n\n\n\n\n\nAs reference signal for FWSSNR and SRR we used the direct target speech component including early reflections (first 50 of the RIR) at the reference microphones.\n\nIn addition, we evaluate the RTF vector estimation accuracy based on the  Hermitian angle\n\n    \u03c6 = acos( \u1e7d\u0302_j,t^H\u1e7d_j,t/\u1e7d\u0302_j,t\u1e7d_j,t)\n\nbetween the estimated RTF vector \u1e7d\u0302_j,t of the target speaker and the oracle RTF vector \u1e7d_j,t averaged across frequency bands. The Hermitian angle \u03c6 is a scale-invariant error measure for complex vectors, with lower values indicating smaller errors. The oracle RTF vectors are computed as the principal eigenvector of the covariance matrices of a white noise signal convolved with the early part (50) of the respective multi-channel RIR of the target speaker. Note that for each target speaker position there is a unique oracle RTF vector. \n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Results\n\n\nFig.\u00a0<ref> compares the FWSSNR and SRR improvements (difference between scores for input and output signals) for different time constants t_\u03b3 of the adaptive and non-adaptive version of the wBLCMP beamformer using two different shape parameters p={0,0.5}. It can be clearly observed that for the considered switching-target scenario the adaptive version of the wBLCMP beamformer outperforms the non-adaptive version in both performance measures for almost all time constants. The best SRR improvement is obtained using a time constant of roughly t_\u03b3 = 450, whereas the FWSSNR improvement is higher for shorter time constants. Using the shape parameter p=0.5 yields better SRR improvements especially for larger time constants, whereas using the shape parameter p=0, corresponding to the conventional cost function in (<ref>), yields slightly better FWSSNR improvements.\n\nFor the adaptive and the non-adaptive version Fig.\u00a0<ref> shows the average Hermitian angle between the oracle RTF vector of the active target speaker and the estimated target RTF vector. Note that the non-adaptive version only provides one RTF vector estimate for the whole signal in contrast to the adaptive version which estimates the RTF vector of the target speaker in each time frame. It can be observed that the adaptive wBLCMP beamformer outperforms the non-adaptive version in almost all time frames in terms of RTF vector estimation accuracy.\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nIn this paper, we derived an adaptive version of the wBLCMP beamformer capable of tracking a moving target speaker in a noisy environment with interfering sources. In addition we generalized the conventional method using sparse priors. The evaluation in terms of objective performance measures clearly shows that the adaptive version outperforms the non-adaptive version in the considered acoustic scenario. This can be explained partly by the ability to track the time-varying RTF vector and covariance matrix of a moving target speaker. \n\n\n\n\n\n\nIEEEtran\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}