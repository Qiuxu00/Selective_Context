{"entry_id": "http://arxiv.org/abs/2303.06885v3", "published": "20230313060518", "title": "DR2: Diffusion-based Robust Degradation Remover for Blind Face Restoration", "authors": ["Zhixin Wang", "Xiaoyun Zhang", "Ziying Zhang", "Huangjie Zheng", "Mingyuan Zhou", "Ya Zhang", "Yanfeng Wang"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n[\n\nDR2: Diffusion-based Robust Degradation Remover for Blind Face Restoration\n    \nZhixin Wang1  Xiaoyun Zhang1[2] Ziying Zhang1   Huangjie Zheng2 \n\nMingyuan Zhou2  Ya Zhang1,3   Yanfeng Wang1,3[2]\n\n\n1Shanghai Jiao Tong University, 3Shanghai AI Laboratory, 2The University of Texas at Austin\n\n{dedsec_z, xiaoyun.zhang, zyzhang2000, ya_zhang, wangyanfeng} @sjtu.edu.cn \n\nhuangjie.zheng@utexas.edu, mingyuan.zhou@mccombs.utexas.edu \n\n\n    March 30, 2023\n==================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n    \n    < g r a p h i c s >\n\n    figureDR2 uses Denoising Diffusion Probabilistic Models to remove degradation. The generative process is conditioned on the low-quality input after being diffused into a noisy status. As a result, DR2 predicts coarse faces _0 regardless of the degradation type. On severely degraded images, our final restoration results achieve high quality with fewer artifacts than previous arts <cit.>.\n    \n\n\n]\n\n\n\n\n[2]Corresponding author.\nBlind face restoration usually synthesizes degraded low-quality data with a pre-defined degradation model for training, while more complex cases could happen in the real world. This gap between the assumed and actual degradation hurts the restoration performance where artifacts are often observed in the output. However, it is expensive and infeasible to include every type of degradation to cover real-world cases in the training data. To tackle this robustness issue, we propose Diffusion-based Robust Degradation Remover (DR2) to first transform the degraded image to a coarse but degradation-invariant prediction, then employ an enhancement module to restore the coarse prediction to a high-quality image. By leveraging a well-performing denoising diffusion probabilistic model, our DR2 diffuses input images to a noisy status where various types of degradation give way to Gaussian noise, and then captures semantic information through iterative denoising steps. As a result, DR2 is robust against common degradation (blur, resize, noise and compression) and compatible with different designs of enhancement modules. Experiments in various settings show that our framework outperforms state-of-the-art methods on heavily degraded synthetic and real-world datasets.\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nBlind face restoration aims to restore high-quality face images from their low-quality counterparts suffering from unknown degradation, such as low-resolution <cit.>, blur <cit.>, noise<cit.>, compression <cit.>, . Great improvement in restoration quality has been witnessed over the past few years with the exploitation of various facial priors. Geometric priors such as facial landmarks <cit.>, parsing maps <cit.>, and heatmaps <cit.> are pivotal to recovering the shapes of facial components. Reference priors <cit.> of high-quality images are used as guidance to improve details. Recent research investigates generative priors <cit.> and high-quality dictionaries <cit.>, which help to generate photo-realistic details and textures.\n\nDespite the great progress in visual quality, these methods lack a robust mechanism to handle degraded inputs besides relying on pre-defined degradation to synthesize the training data. When applying them to images of severe or unseen degradation, undesired results with obvious artifacts can be observed. As shown in <ref>, artifacts typically appear when 1) the input image lacks high-frequency information due to downsampling or blur (1^st row), in which case restoration networks can not generate adequate information, or 2) the input image bears corrupted high-frequency information due to noise or other degradation (2^nd row), and restoration networks mistakenly use the corrupted information for restoration. The primary cause of this inadaptability is the inconsistency between the synthetic degradation of training data and the actual degradation in the real world.\n\n\n\nExpanding the synthetic degradation model for training would improve the models\u2019 adaptability but it is apparently difficult and expensive to simulate every possible degradation in the real world. To alleviate the dependency on synthetic degradation, we leverage a well-performing denoising diffusion probabilistic model (DDPM) <cit.> to remove the degradation from inputs. DDPM generates images through a stochastic iterative denoising process and Gaussian noisy images can provide guidance to the generative process <cit.>. As shown in <ref>, noisy images are degradation-irrelevant conditions for DDPM generative process. Adding extra Gaussian noise (right) makes different degradation less distinguishable compared with the original distribution (left), while DDPM can still capture the semantic information within this noise status and recover clean face images. This property of pretrained DDPM makes it a robust degradation removal module though only high-quality face images are used for training the DDPM.\n\nOur overall blind face restoration framework DR2E consists of the \nDiffusion-based Robust Degradation Remover (DR2) and an Enhancement module. In the first stage, DR2 first transforms the degraded images into coarse, smooth, and visually clean intermediate results, which fall into a degradation-invariant distribution (4^th column in <ref>). In the second stage, the degradation-invariant images are further processed by the enhancement module for high-quality details. By this design, the enhancement module is compatible with various designs of restoration methods in seeking the best restoration quality, ensuring our DR2E achieves both strong robustness and high quality.\n\n\n\nWe summarize the contributions as follows. \n\n(1) We propose DR2 that leverages a pretrained diffusion model to remove degradation, achieving robustness against complex degradation without using synthetic degradation for training. \n(2) Together with an enhancement module, we employ DR2 in a two-stage blind face restoration framework, namely DR2E. The enhancement module has great flexibility in incorporating a variety of restoration methods to achieve high restoration quality.\n\n(3) Comprehensive studies and experiments show that our framework outperforms state-of-the-art methods on heavily degraded synthetic and real-world datasets.\n\n\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n\n\n\n\n\nBlind Face Restoration Based on face hallucination or face super-resolution <cit.>, blind face restoration aims to restore high-quality faces from low-quality images with unknown and complex degradation. Many facial priors are exploited to alleviate dependency on degraded inputs. Geometry priors, including facial landmarks <cit.>, parsing maps <cit.>, and facial component heatmaps <cit.> help to recover accurate shapes but contain no information on details in themselves. Reference priors <cit.> of high-quality images are used to recover details or preserve identity. To further boost restoration quality, generative priors like pretrained StyleGAN <cit.> are used to provide vivid textures and details. PULSE <cit.> uses latent optimization to find latent code of high-quality face, while more efficiently, GPEN <cit.>, GFP-GAN <cit.>, and GLEAN <cit.> embed generative priors into the encoder-decoder structure. Another category of methods utilizes pretrained Vector-Quantize <cit.> codebooks. DFDNet <cit.> suggests constructing dictionaries of each component (eyes, mouth), while recent VQFR <cit.> and CodeFormer <cit.> pretrain high-quality dictionaries on entire faces, acquiring rich expressiveness.\n\n\n\n\n\n\n\n\n\nDiffusion Models Denoising Diffusion Probabilistic Models (DDPM) <cit.> are a fast-developing class of generative models in unconditional image generation rivaling Generative Adversarial Networks (GAN) <cit.>. Recent research utilizes it for super-resolution. SR3 <cit.> modifies DDPM to be conditioned on low-resolution images through channel-wise concatenation. However, it fixes the degradation to simple downsampling and does not apply to other degradation settings. Latent Diffusion <cit.> performs super-resolution in a similar concatenation manner but in a low-dimensional latent space. ILVR <cit.> proposes a conditioning method to control the generative process of pretrained DDPM for image-translation tasks. Diffusion-based methods face a common problem of slow sampling speed, while our DR2E adopts a hybrid architecture like <cit.> to speed up the sampling process.\n\n\n\n\u00a7 METHODOLOGY\n\n\n\nOur proposed DR2E framework is depicted in <ref>, which consists of the degradation remover DR2 and an enhancement module. Given an input image  suffering from unknown degradation, diffused low-quality information _t-1 is provided to refine the generative process. As a result, DR2 recovers a coarse result _0 that is semantically close to  and degradation-invariant. Then the enhancement module maps _0 to the final output with higher resolution and high-quality details.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Preliminary\n\n\n\nDenoising Diffusion Probabilistic Models (DDPM) <cit.> are a class of generative models that first pre-defines a variance schedule {\u03b2_1, \u03b2_2,...,\u03b2_T} to progressively corrupt an image _0 to a noisy status through forward (diffusion) process:\n\n\n    q(_t | _t-1) = (_t; \u221a(1 - \u03b2_t)_t-1, \u03b2_t )\n\n\nMoreover, based on the property of the Markov chain, for any intermediate timestep t\u2208{1,2,...,T}, the corresponding noisy distribution has an analytic form:\n\n    q(_t|_0)    = (_t; \u221a(\u03b1\u0305_t)_0,   (1 - \u03b1\u0305_t)) \n       = \u221a(\u03b1\u0305_t)_0 + \u221a(1 - \u03b1\u0305_t)\n\n\nwhere \u03b1\u0305_t := \u220f_s=1^t (1 -\u03b2_s) and \u223c(, ). Then _T \u223c(, ) if T is big enough, usually T = 1000.\n\nThe model progressively generates images by reversing the forward process. The generative process is also a Gaussian transition with the learned mean _\u03b8:\n\n\n    p_\u03b8(_t-1 | _t) = (_t-1; _\u03b8(_t, t), \u03c3_t^2 )\n\n\nwhere \u03c3_t is usually a pre-defined constant related to the variance schedule, and _\u03b8(_t, t) is usually parameterized by a denoising U-Net _\u03b8(_t, t) <cit.> with the following equivalence:\n\n    _\u03b8(_t, t) = 1/\u221a(\u03b1_t)(_t - 1 - \u03b1_t/\u221a(1 - \u03b1\u0305_t)_\u03b8(_t, t))\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Framework Overview\n\n\n\nSuppose the low-quality image  is degraded from the high-quality ground truth \u223c\ud835\udcb3() as = (, ) where  describes the degradation model. Previous studies constructs the inverse function ^-1(\u00b7, ) by modeling p(| , ) with a pre-defined  <cit.>. It meets the adaptation problem when actual degradation ^' in the real world is far from .\n\nTo overcome this challenge, we propose to model p(| ) without a known  by a two-stage framework: it first removes degradation from inputs and get x\u0302_0, then maps degradation-invariant x\u0302_0 to high-quality outputs. Our target is to maximize the likelihood:\n\n  p_,(| )    = \u222bp_(|_0) p_(_0|) d_0 \n\n   = __0 \u223cp_(_0|) [ p_(|_0)] \n,\n \n\np_(_0|) corresponds to the degradation removal module, and p_(|_0) corresponds to the enhancement module. For the first stage, instead of directly learning the mapping from  to _0 which usually involves a pre-defined degradation model , we come up with an important assumption and propose a diffusion-based method to remove degradation.\n\nAssumption. For the diffusion process defined in <ref>, (1) there exists an intermediate timestep \u03c4 such that for t > \u03c4, the distance between q(_t|) and q(_t|) is close especially in the low-frequency part; (2) there exists \u03c9 > \u03c4 such that the distance between q(_\u03c9|) and q(_\u03c9|) is eventually small enough, satisfying q(_\u03c9|)\u2248q(_\u03c9|).\n\nNote this assumption is not strong, as paired  and  would share similar low-frequency contents, and for sufficiently large t \u2248 T, q(_t|) and q(_t|) are naturally close to the standard (, ). This assumption is also qualitatively justified in <ref>. Intuitively, if  and  are close in distribution (implying mild degradation), we can find \u03c9 and \u03c4 in a relatively small value and vice versa. \n\nThen we rewrite the objective of the degradation removal module by applying the assumption q(_\u03c9|)\u2248q(_\u03c9|):\n\n p_(_0|)    = \u222bp(_0|_\u03c4) p_\u03b8(_\u03c4|_\u03c9) q(_\u03c9| ) d_\u03c4d_\u03c9\n\n     \u2248\u222bp(_0|_\u03c4) p_\u03b8(_\u03c4|_\u03c9) q(_\u03c9| ) d_\u03c4d_\u03c9\n\n      p_\u03b8(_\u03c4|_\u03c9) = \u220f_t=\u03c4+1^\u03c9 p_\u03b8(_t-1|_t)\n\n\n\nBy replacing variable from _\u03c9 to _\u03c9, <ref> and <ref> naturally yields a DDPM model that denoises _\u03c9 back to _\u03c4, and we can further predict _0 by the reverse of <ref>. _0 would maintain semantics with  if proper conditioning methods like <cit.> is adopted. So by leveraging a DDPM, we propose Diffusion-based Robust Degradation Remover (DR2) according to  <ref>.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Diffusion-based Robust Degradation Remover\n\n\n\nConsider a pretrained DDPM p_\u03b8(_t-1|_t) (<ref>) with a denoising U-Net _\u03b8(_t, t) pretrained on high-quality face dataset. We respectively implement q(_\u03c9 | ), p_\u03b8(_\u03c4|_\u03c9) and p(_0|_\u03c4) in <ref> by three steps in below.\n\n\n\n\n\n\n(1) Initial Condition at \u03c9. We first \u201cforward\" the degraded image  to an initial condition _\u03c9 by sampling from <ref>  and use it as _\u03c9: \n\n\n    _\u03c9 := _\u03c9 = \u221a(\u03b1\u0305_\u03c9) + \u221a(1 - \u03b1\u0305_\u03c9),\n\n\n\u03c9\u2208{1,2,...,T}. This corresponds to q(_\u03c9|) in <ref>. Then the DR2 denoising process starts at step \u03c9. This reduces the samplings steps and helps to speed up as well.\n\n\n\n\n(2) Iterative Refinement. After each transition from _t to _t-1 (\u03c4 + 1 \u2a7d t \u2a7d\u03c9), we sample _t-1 from  through <ref>. Based on Assumption (1), we replace the low-frequency part of _t-1 with that of _t-1 because they are close in distribution, which is fomulated as:\n\n\n    _t-1 := \u03a6_N(_t-1) + (\ud835\udc08 - \u03a6_N)(_t-1)\n\n\nwhere \u03a6_N(\u00b7) denotes a low-pass filter implemented by downsampling and upsampling the image with a sharing scale factor N. We drop the high-frequency part of  for it contains little information due to degradation. Unfiltered degradation that remained in the low-frequency part would be covered by the added noise. These conditional denoising steps correspond to p_(_\u03c4|_\u03c9) in <ref>, which ensure the result shares basic semantics with y. \n\nIterative refinement is pivotal for preserving the low-frequency information of the input images. With the iterative refinement, the choice of \u03c9 and the randomness of Gaussian noise affect little to the result. We present ablation study in the supplementary for illustration.\n\n\n\n\n\n\n(3) Truncated Output at \u03c4. As t gets smaller, the noise level gets milder and the distance between q(_t|) and q(_t|) gets larger. For small t, the original degradation is more dominating in q(_t|) than the added Gaussian noise. So the denoising process is truncated before t is too small. We use predicted noise at step \u03c4 (0 < \u03c4 < \u03c9) to estimate the generation result as follows:\n\n\n\n    _0 = 1/\u221a(\u03b1\u0305_\u03c4) (_\u03c4 - \u221a(1 - \u03b1\u0305_\u03c4)_\u03b8(_\u03c4, \u03c4))\n\nThis corresponds to p(_0|_\u03c4) in <ref>. _0 is the output of DR2, which maintains the basic semantics of  and is removed from various degradation. \n\n\n\nSelection of N and \u03c4. Downsampling factor N and output step \u03c4 have significant effects on the fidelity and \u201ccleanness\" of _0. We conduct ablation studies in <ref> to show the effects of these two hyper-parameters. The best choices of N and \u03c4 are data-dependent. Generally speaking, big N and \u03c4 are more effective to remove the degradation but lead to lower fidelity. On the contrary, small N and \u03c4 leads to high fidelity, but may keep the degradation in the outputs. While \u03c9 is empirically fixed to \u03c4 + 0.25T. \n\n\n\n\n\n\n\n \u00a7.\u00a7 Enhancement Module\n\n\n\nWith outputs of DR2, restoring the high-quality details only requires training an enhancement module p_(|_0) (<ref>). Here we do not hypothesize about the specific method or architecture of this module. Any neural network that can be trained to map a low-quality image to its high-quality counterpart can be plugged in our framework. And the enhancement module is independently trained with its proposed loss functions.\n\nBackbones. In practice, without loss of generality, we choose SPARNetHD <cit.> that utilized no facial priors, and VQFR <cit.> that pretrain a high-quality VQ codebook <cit.> as two alternative backbones for our enhancement module to justify that it can be compatible with a broad choice of existing methods. We denote them as DR2 + SPAR and DR2 + VQFR respectively.\n\nTraining Data. Any pretrained blind face restoration models can be directly plugged-in without further finetuning, but in order to help the enhancement module adapt better and faster to DR2 outputs, we suggest constructing training data for the enhancement module using DR2 as follows: \n\n\n    = DR2(;N,\u03c4) \u229b k_\u03c3\n\n\nGiven a high-quality image , we first use DR2 to reconstruct itself with controlling parameters (N, \u03c4) then convolve it with an Gaussian blur kernel k_\u03c3. This helps the enhancement module adapt better and faster to DR2 outputs, which is recommended but not compulsory. Noting that beside this augmentation, no other degradation model is required in the training process as what previous works <cit.> do by using <ref>.\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Datasets and Implementation\n\n\nImplementation. DR2 and the enhancement module are independently trained on FFHQ dataset <cit.>, which contains 70,000 high-quality face images. We use pretrained DDPM proposed by <cit.> for our DR2. As introduced in <ref>, we choose SPARNetHD <cit.> and VQFR <cit.> as two alternative architectures for the enhancement module. We train SPARNetHD backbone from scratch with training data constructed by <ref>. We set N = 4 and randomly sample \u03c4, \u03c3 from {50, 100, 150, 200}, {1 : 7}, respectively. As for VQFR backbone, we use its official pretrained model. \n\nTesting Datasets. We construct one synthetic dataset and four real-world datasets for testing. A brief introduction of each is as followed:\n\n\u2219 CelebA-Test. Following previous works <cit.>, we adopt a commonly used degradation model as follows to synthesize testing data from CelebA-HQ <cit.>:\n\n    = [(\u229b k_\u03c3) \u2193_r + n_\u03b4]_JPEG_q\n\n\nA high-quality image  is first convolved with a Gaussian blur kernel k_\u03c3, then bicubically downsampled with a scale factor r. n_\u03b4 represents additive noise and is randomly chosen from Gaussian, Laplace, and Poisson. Finally, JPEG compression with quality q is applied. We use r = 16, 8, and 4 to form three restoration tasks denoted as 16\u00d7, 8\u00d7, and 4\u00d7. For each upsampling factor, we generate three splits with different levels of degradation and each split contains 1,000 images. The mild split randomly samples \u03c3, \u03b4 and q from {3:5}, {5:20}, {60:80}, respectively. The medium from {5:7}, {15:40}, {40:60}. And the severe split from {7:9}, {25:50}, {30:40}.\n\n\u2219 WIDER-Normal and WIDER-Critical. We select 400 critical cases suffering from heavy degradation (mainly low-resolution) from WIDER-face dataset <cit.> to form the WIDER-Critical dataset and another 400 regular cases for WIDER-Normal dataset.\n\n\u2219 CelebChild contains 180 child faces of celebrities collected from the Internet. Most of them are only mildly degraded.\n\n\u2219 LFW-Test. LFW <cit.> contains low-quality images with mild degradation from the Internet. We choose 1,000 testing images of different identities.\n\nDuring testing, we conduct grid search for best controlling parameters (N,\u03c4) of DR2 for each dataset. Detailed parameter settings are presented in the suplementary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Comparisons with State-of-the-art Methods\n\n\n\nWe compare our method with several state-of-the-art face restoration methods: DFDNet <cit.>, SPARNetHD <cit.>, GFP-GAN <cit.>, GPEN <cit.>, VQFR <cit.>, and Codeformer <cit.>. We adopt their official codes and pretrained models.\n\nFor evaluation, we adopt pixel-wise metrics (PSNR and SSIM) and the perceptual metric (LPIPS <cit.>) for the CelebA-Test with ground truth. We also employ the widely-used non-reference perceptual metric FID <cit.>.\n\nSynthetic CelebA-Test. For each upsampling factor, we calculate evaluation metrics on three splits and present the average in <ref>. For 16\u00d7 and 8\u00d7 upsampling tasks where degradation is severe due to low resolution, DR2 + VQFR and DR2 + SPAR achieve the best and the second-best LPIPS and FID scores, indicating our results are perceptually close to the ground truth. Noting that DR2 + VQFR is better at perceptual metrics (LPIPS and FID) thanks to the pretrained high-quality codebook, and DR2 + SPAR is better at pixel-wise metrics (PSNR and SSIM) because without facial priors, the outputs have higher fidelity to the inputs. For 4\u00d7 upsampling task where degradation is relatively milder, previous methods trained on similar synthetic degradation manage to produce high-quality images without obvious artifacts. But our methods still obtain superior FID scores, showing our outputs have closer distribution to ground truth on different settings.\n\n\n\nQualitative comparisons from are presented in <ref>. Our methods produce fewer artifacts on severely degraded inputs compared with previous methods.\n\n\n\n\nReal-World Datasets. We evaluate FID scores on different real-world datasets and present quantitative results in <ref>. On severely degraded dataset WIDER-Critical, our DR2 + VQFR and DR2 + SPAR achieve the best and the second best FID. On other datasets with only mild degradation, the restoration quality rather than robustness becomes the bottleneck, so DR2 + SPAR with no facial priors struggles to stand out, while DR2 + VQFR still achieves the best performance.\n\nQualitative results on WIDER-Critical are shown in <ref>. When input images' resolutions are very low, previous methods fail to complement adequate information for pleasant faces, while our outputs are visually more pleasant thanks to the generative ability of DDPM.\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Comparisons with Diffusion-based Methods\n\n\n\n\n\n\n\n\nDiffusion-based super-resolution methods can be grouped into two categories by whether feeding auxiliary input to the denoising U-Net. \n\nSR3 <cit.> typically uses the concatenation of low-resolution images and _t as the input of the denoising U-Net. But SR3 fixes degradation to bicubic downsampling during training, which makes it highly degradation-sensitive. For visual comparisons, we re-implement the concatenation-based method based on <cit.>. As shown in <ref>, minor noise in the second input evidently harm the performance of this concatenation-based method. Eventually, this type of method would rely on synthetic degradation to improve robustness like <cit.>, while our DR2 have good robustness against different degradation without training on specifically degraded data.\n\nAnother category of methods is training-free, exploiting pretrained diffusion methods like ILVR <cit.>. It shows the ability to transform both clean and degraded low-resolution images into high-resolution outputs. However, relying solely on ILVR for blind face restoration faces the trade-off problem between fidelity and quality (realness). As shown in <ref>, ILVR Sample 1 has high fidelity to input but low visual quality because the conditioning information is over-used. On the contrary, under-use of conditions leads to high quality but low fidelity as ILVR Sample 2. In our framework, fidelity is controlled by DR2 and high-quality details are restored by the enhancement module, thus alleviating the trade-off problem.\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Effect of Different N and \u03c4\n\n\n\n\nIn this section, we explore the property of DR2 output in terms of the controlling parameter (N, \u03c4) so that we can have a better intuitions for choosing appropriate parameters for variant input data. To avoid the influence of the enhancement modules varying in structures, embedded facial priors, and training strategies, we only evaluate DR2 outputs with no enhancement. \n\nIn <ref>, DR2 outputs are generated with different combinations of N and \u03c4. Bigger N and \u03c4 are effective to remove degradation but tent to make results deviant from the input. On the contrary, small N and \u03c4 lead to high fidelity, but may keep the degradation in outputs.\n\n\n\n\n\nWe provide quantitative evaluations on CelebA-Test (8\u00d7, medium split) dataset in <ref>. With bicubically downsampled low-resolution images used as ground truth, we adopt pixel-wise metric (PSNR\u2191) and identity distance (Deg\u2193) based on the embedding angle of ArcFace <cit.> for evaluating the quality and fidelity of DR2 outputs. For scale N = 4, 8, and 16, PSNR first goes up and Deg goes down because degradation is gradually removed as \u03c4 increases. Then they hit the optimal point at the same time before the outputs begin to deviate from the input as \u03c4 continues to grow. Optimal \u03c4 is bigger for smaller N. For N = 2, PSNR stops to increase before Deg reaches the optimality because Gaussian noise starts to appear in the output (like results sampled with (N, \u03c4)=(2,350) in <ref>). This cause of the appearance of Gaussian noise is that _t sampled by <ref> contains heavy Gaussian noise when t (t > \u03c4) is big and most part of _t is utilized by <ref> when N is small.\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Discussion and Limitations\n\nOur DR2 is built on a pretrained DDPM, so it would face the problem of slow sampling speed even we only perform 0.25T steps in total. But DR2 can be combined with diffusion acceleration methods like inference every 10 steps. And keep the output resolution of DR2 relatively low (256^2 in our practice) and leave the upsampling for enhancement module for faster speed.\n\nAnother major limitation of our proposed DR2 is the manual choosing for controlling parameters N and \u03c4.\n\nAs a future work, we are exploring whether image quality assessment scores (like NIQE) can be used to develop an automatic search algorithms for N and \u03c4.\n\nFurthermore, for inputs with slight degradation, DR2 is less necessary because previous methods can also be effective and faster. And in extreme cases where input images contains very slight degradation or even no degradation, DR2 transformation may remove details in the inputs, but that is not common cases for blind face restoration. \n\n\n\n\n\u00a7 CONCLUSION\n\n\n\nWe propose the DR2E, a two-stage blind face restoration framework that leverages a pretrained DDPM to remove degradation from inputs, and an enhancement module for detail restoration. In the first stage, DR2 removes degradation by using diffused low-quality information as conditions to guide the generative process. This transformation requires no synthetically degraded data for training. Extensive comparisons demonstrate the strong robustness and high restoration quality of our DR2E framework.\n\n\n\n\n\n\u00a7 ACKNOWLEDGEMENTS\n\nThis work is supported by National Natural Science Foundation of China (62271308), Shanghai Key Laboratory\nof Digital Media Processing and Transmissions (STCSM 22511105700, 18DZ2270700), 111 plan (BP0719010), and State Key Laboratory of UHD Video and Audio Production and Presentation.\n\n\nieee_fullname\n\n\n\nfiguresection\ntablesection\n\n\n\n\n\n\n\n\u00a7 APPENDIX\n\n\nIn the appendix, we provide additional discussions and results complementing <ref>. In <ref>, we conduct further ablation studies on initial condition and iterative refinement to show the control and conditioning effect  these two mechanisms bring to the DR2 generative process. In <ref>, we provide (1) our detailed settings of DR2 controlling parameters (N,\u03c4) for each testing dataset, and (2) show more qualitative comparisons on each split of CelebA-Test dataset in this section to illustrate how our methods and previous state-of-the-art methods perform over variant levels of degradation.\n\n\n\n\n\n\n\n\u00a7 MORE ABLATION STUDIES\n\n\nIn this section, we explore the effect of initial condition and iterative refinement in DR2. To avoid the influence of the enhancement modules varying in structures, embedded facial priors, and training strategies, we only conduct experiments on DR2 outputs with no enhancement. To evaluate the degradation removal performance and fidelity of DR2 outputs, we use bicubic downsampled images as ground truth low-resolution (GT LR) image. This is intuitive as DR2 is targeted to produce clean but blurry middle results.\n\n\n\n\n\n\n \u00a7.\u00a7 Conditioning Effect of Initial Condition with Iterative Refinement Enabled\n\n\n\n\n\n\n\n\nDuring DR2 generative process, diffused low-quality inputs is provided through initial condition and iterative refinement. The latter one yields stronger control to the generative process because it is performed at each step, while initial condition only provides information in the beginning with heavy Gaussian noise attached. To quantitatively evaluate the effect of initial condition, we follow the settings of S<ref> by calculating the pixel-wise metric (PSNR) and identity distance (Deg) between DR2 outputs and ground truth low-resolution images on CelebA-Test (8\u00d7, medium split) dataset. Quantitative results are shown in <ref>. We fix (N, \u03c4) = (4, 300) and change the value of \u03c9. When \u03c9 = 1000 = T, no initial condition is provided because \ud835\udc32_1000 is pure Gaussian noise. As shown in the table, with iterative refinement providing strong control to DR2 generative process, the quality and fidelity of DR2 outputs are not evidently affected as \u03c9 varies. \n\nQualitative results are provided in <ref>. With fixed iterative refinement controlling parameters, \u03c9 has little visual effect on DR2 outputs. Although the initial condition provides limited information compared with iterative refinement, it significantly reduces the total steps of DR2 denoising process. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Conditioning Effect of Initial Condition with Iterative Refinement Disabled\n\n\nWe conduct experiments without iterative refinement in this section to show that generative results bear less fidelity to the input without it. Without iterative refinement, DR2 generative process relies solely on the initial condition to utilize information of low-quality inputs, and generate images through DDPM denoising steps stochastically from initial condition. \u03c9 now becomes an important controlling parameter determining how much conditioning information is provided. We also calculate PSNR and Deg between DR2 outputs and ground-truth low-resolution images on  CelebA-Test (8\u00d7, medium split) dataset. Quantitative results with different (\u03c9, \u03c4) are provided in <ref>. Note that PSNR and Deg are all worse than those in <ref>, and have a negative correlation with \u03c9 because less information of inputs is used as \u03c9 increases. \n\n\nQualitative results are shown in <ref>. When \u03c9\u2a7e 400, added noise in initial condition is strong enough to cover the degradation in inputs so the output tends to be smooth and clean. But as \u03c9 increases, the outputs become more irrelevant to the input because the initial conditions are weakened. Compared with results that were sampled with iterative refinement, the importance of it on preserving semantic information is obvious.\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 DETAILED SETTINGS AND COMPARISONS\n\n\n\n\n\n\n \u00a7.\u00a7 Controlling Parameter Settings\n\nAs introduced in <ref>, to evaluate the performance on different levels of degradation, we synthesize three splits (mild, medium, and severe) for each upsampling task (16\u00d7, 8\u00d7, and 4\u00d7) together with four real-world datasets. During the experiment in <ref>, different controlling parameters (N, \u03c4) are used for each dataset or split. Generally speaking, big N and \u03c4 are more effective to remove the degradation but lead to lower fidelity and vice versa. We provide detailed settings we employed in <ref>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 More Qualitative Comparisons\n\nFor more comprehensive comparisons with previous methods on different levels of degraded dataset, we provide qualitative results on each split of CelebA-Test dataset under each upsampling factor in <ref>. As shown in the figures, for inputs with slight degradation, DR2 transformation is less necessary because previous methods can also be effective. But for severe degradation, previous methods fail since they never see such degradation during training. While our method shows great robustness even though no synthetic degraded images are employed for training.\n\n\n\n\n\n\n\n\n\n"}