{"entry_id": "http://arxiv.org/abs/2303.06810v1", "published": "20230313015653", "title": "Dynamic Clustering and Cluster Contrastive Learning for Unsupervised Person Re-identification", "authors": ["Ziqi He", "Mengjia Xue", "Yunhao Du", "Zhicheng Zhao", "Fei Su"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\n\nDynamic Clustering and Cluster Contrastive Learning for Unsupervised Person Re-identification\n    Ziqi He, Mengjia Xue, Yunhao Du, Zhicheng ZhaoCorresponding author, Fei Su\n\nBeijing University of Posts and Telecommunications, Beijing, China\n\n{heziqi, xuemengjia, dyh_bupt, zhaozc, sufei}@bupt.edu.cn\n\n\n\n\n\n\n\n\n\n\n\n    \n========================================================================================================================================================================================================================\n\n\nempty\n\n\n\nUnsupervised Re-ID methods aim at learning robust and discriminative features from unlabeled data. However, existing methods often ignore the relationship between module parameters of Re-ID framework and feature distributions, which may lead to feature misalignment and hinder the model performance. To address this problem, we propose a dynamic clustering and cluster contrastive learning (DCCC) method. Specifically, we first design a dynamic clustering parameters scheduler (DCPS) which adjust the hyper-parameter of clustering to fit the variation of intra- and inter-class distances. Then, a dynamic cluster contrastive learning (DyCL) method is designed to match the cluster representation vectors' weights with the local feature association. Finally, a label smoothing soft contrastive loss (L_ss) is built to keep the balance between cluster contrastive learning and self-supervised learning with low computational consumption and high computational efficiency. Experiments on several  widely used public datasets validate the effectiveness of our proposed DCCC which outperforms previous state-of-the-art methods by achieving the best performance. Code is available at <https://github.com/theziqi/DCCC>.\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nPerson Re-identification (Re-ID) aims to identify the desired target pedestrian from a large number of cross-camera surveillance images. Initially, researchers have mostly worked on supervised Re-ID methods based on deep network models<cit.>. However, as supervised Re-ID methods are widely arranged in real-world scenarios<cit.>, the volume of data becomes larger and the time costs of manual annotation become more expensive, leading to limitations in the development of supervised Re-ID. Therefore, in order to reduce the cost of manual annotation, unsupervised Re-ID methods, which use unlabeled data for training, have received a lot of attention from researchers.\n\nUnsupervised domain adaptation (UDA) Re-ID methods and fully unsupervised learning (USL) Re-ID methods are two types of unsupervised re-identification techniques. UDA Re-ID mthods involve a source domain without any annotation information and a target domain fully annotated based on transfer learning. However, the introduction of the source domain limits the model's performance. On one hand, the model's performance in the target domain is influenced by the quality of the knowledge learned in the source domain<cit.>. On the other hand, the discrepancy in data distribution between the source and target domains hinders sufficient knowledge transfer, which in turn inhibits the model's performance<cit.>. USL Re-ID methods, in contrast, only employ unlabeled datasets for training, which are more adaptable and scalable without other external factors.\n\n\n\nRecent methods commonly utilize plug-and-play modules, including a clustering algorithm, a memory bank and contrastive loss functions, together with network models to build an efficient and available USL Re-ID framework. Its remarkable performance has attracted a great deal of attention and taken a dominant place. They mostly follow this procedure in USL Re-ID task: (1) generating the corresponding pseudo-labels by a clustering algorithm; (2)  computing the contrastive loss by the supervision of pseudo-labels for the input instances, called query instances, with the postive and negative from the memory bank; (3) updating the cluster representation vectors in the memory bank for the next iteration. Each iteration stage drives the network to learn robust and discriminative features. In this case, the feature distribution converges towards the ground truth distribution. However, these methods always ignore this actual variation in defining each module. For instance, the most usually applied clustering algorithm in USL Re-ID method is DBSCAN: Cheng<cit.> assign different values to the clustering parameter eps for different datasets, such as 0.5 on Market1501 and 0.6 on DukeMTMC-reID, while the values are 0.4 and 0.7 in ISE<cit.>. In general, the clustering parameters are usually taken empirically as the optimal fixed values<cit.>. However, this approach of setting clustering parameters may result in a mismatch between the parameters and the feature distribution, called feature misalignment. On one side, as Figure <ref> shows, during the training stage, the intra-class distances decrease, while the inter-class distances increase, and accordingly, the density of each cluster rises. On the other side, the clustering parameter eps of DBSCAN is a distance threshold determining whether two neighbouring instances are of the same class which represents the density of the clusters. Thus, eps is supposed to be adaptive to the distribution for prompting the clustering algorithm to generate high-quality pseudo-labels.\n\nThe observation inspires us to review the modules of the currently common USL Re-ID framework. We have found that feature misalignment also occurs in the memory bank and the contrastive loss functions. The cluster representation vectors is often the average centroid or the hardest instance in a mini-batch. However, they only reflect the local feature distribution. If the model learns the difference between them and the query features, it may cause a distribution shift. Moreover, contrastive methods do not cope well with the effects of distortion from data augmentation. Because of the traditional training way of a single network, after data augmentation, positive pseudo instances may be more different from query instances, negative pseudo instances may be more similar to query instances<cit.>, and the distribution of the same query features may be inconsistent.\n\nTo address the aforementioned problems, we propose a dynamic clustering and cluster contrastive learning (DCCC) method. DCCC designs a dynamic clustering parameters scheduler (DCPS) which continuously adjusts the clustering parameters to match the feature distribution as closely as possible during network training. We also propose a dynamic cluster contrastive learning (DyCL) method which optimizes the cluster representation vectors in the contrastive learning method by replacing the average centroid or the hardest positive instance with a dynamic cluster centroid. Based on the hard sample mining strategy<cit.>, the dynamic clustering centroid is the bridge between the weight parameters and the local feature distribution by assigning weights to each sample based on the feature distance in a mini-batch. In addition, DCCC also constructs a label smoothing soft contrastive loss (L_ss) function as the final loss function, which takes into account both cluster contrastive learning and self-supervised learning and reduces the computational cost.\n\nTo summarize, our contributions are: (1) We propose a dynamic clustering parameters scheduler, which enables the clustering hyper-parameters to dynamically decay with the training process. To our knowledge, our work is the first detailed study around the parameter settings of the clustering algorithm in unsupervised Re-ID research. (2) We propose a dynamic cluster contrastive learning method based on hard sample mining, which fully considers each instance in the mini-batch and assigns corresponding weights to them to update the cluter representation vectors in memory bank, solving the inconsistency problem. (3) We also use self-supervised methods combined with cluster contrast scheme to construct a simple and efficient label smooth soft contrastive loss function, which enhances the consistency in the distribution of same query features. (4) Experiments validate the effectiveness of our proposed method, which outperforms the state-of-the-art method in USL Re-ID on datasets such as Market1501 and DukeMTMC-reID.\n\n\n\n\u00a7 RELATED WORKS\n\n\n\n\n \u00a7.\u00a7 Unsupervised Person Re-ID\n\n\nThe most current unsupervised pedestrian Re-ID methods can be roughly divided into fully unsupervised (USL) methods and unsupervised domain adaptation (UDA) methods.\n\nDue to the additional data introduced, UDA Re-ID often achieves better results than fully unsupervised methods. It requires labeled source domain data and unlabeled target domain data for training. Several UDA methods will transfer images from the source domain to the target domain utilizing Generative Adversarial Networks (GAN)<cit.>. The pseudo-label-based UDA methods will first pre-train the network on the source domain and then fine-tune the network on the target domain using the generated pseudo-labels<cit.>. Both of these methods focus on the transfer of knowledge from one domain to another. And USL Re-ID is more challenging and flexible in terms of data requirements as it only uses fully unlabeled data for training. Traditional methods use metric learning for personal retrieval tasks<cit.>. The performance bottleneck has been removed by clustering algorithms, and numerous USL Re-ID methods based on these algorithms have emerged<cit.>. Pseudo-labels generated by clustering algorithms or similarity estimation allow the model to train unlabeled data in a similar way to a labeled training model<cit.>. SpCL<cit.> is a  unsupervised Re-ID self-paced contrastive learning framework based on instance-level memory. Cluster Contrast<cit.> employs cluster-level memory for addressing inconsistent updates of memory class centroids. ICE<cit.> incorporates the concepts of camera-aware, hard-sample mining, and soft-label in contrastive learning. However, the hard sample mining strategies in Re-ID's contrastive learning framework are often limited to a single hard sample and do not fully exploit global information.\n\n\n\n \u00a7.\u00a7 Clustering Algorithm\n\n\nA clustering algorithm's fundamental idea is dividing a dataset into clusters based on some criterion (typically distance) without any annotations. Clustering algorithms are frequently used in deep unsupervised learning due to their unsupervised capability and good performance. Clustering algorithms often calculate the similarity between samples using metrics Euclidean distance, Manhattan distance, and Jaccard similarity coefficient<cit.>. Clustering algorithms can be broadly categorised into Partition-based Methods, Density-based methods, Hierarchical Methods and so on. K-Means<cit.>, a partition-based method, is first used in unsupervised Re-ID, but it requires the number of clusters as a hyper-parameter, which is difficult to estimate for unsupervised learning. DBSCAN<cit.>, a density-based method, require the definition of two parameters eps and min_samples, which denote the neighbourhood radius of the density and the neighbourhood density threshold. The property of not requiring the number of clusters has led many unsupervised Re-ID algorithms to adopt DBSCAN. Nevertheless, fixed parameters of DBSCAN which is popular in cluster-based USL Re-ID method may lose the key context information at each training epoch.\n\n\n\n\n\n\u00a7 PROPOSED APPROACH\n\n\n\n\n \u00a7.\u00a7 Overview\n\n\n\nWe propose a USL Re-ID framework that combines dynamic clustering algorithms and cluster contrastive learning. The framework is based on a general framework for self-supervised learning<cit.> and Cluster Contrast<cit.>. Figure <ref> shows an overview of the framework. It includes backbone networks, a clustering algorithm, a memory bank, and other components. The overall training procedure is as follows.\n\nIn USL Re-ID methods, a dataset is commonly denoted as X={x_1,x_2,...,x_N}, where each x_i is an unlabeled image, containing N pedestrian images totally. \n\nIn each epoch of training, the initial stage is carried out first. The backbone networks consists of a teacher network and a student network. The student network is a regular network updated by gradient back-propagation, and its network parameters are denoted by \u03b8_s. Given a picture x, f_\u03b8_s(x) represents the output features of the student network. The teacher network's parameters \u03b8_t are updated by the parameters of the student network by exponential moving average (EMA)<cit.>, which is formulated as follows:\n\n\n    \u03b8_t=\u03bb\u03b8_t^' + (1-\u03bb)\u03b8_s\n\n\nwhere \u03b8_t^' is the teacher network parameters from the previous iteration and \u03bb is a momentum update hyper-parameter. DBSCAN then divides the output features of the student network into sets of instances that can be clustered and outliers that are not clustered, based on Jaccard similarity coefficient<cit.> pairs, and assigns them the corresponding pseudo-labels. The parameters of the clustering algorithm are determined by the dynamic clustering parameter scheduler (DCPS) that matches the global distribution of features. The mean value of the features in each instance set will be used as the initial cluster representation vector in the memory bank.\n\nIn the training stage, we use PK sampling<cit.> where P pedestrian IDs are randomly selected and K pedestrian images are randomly drawn for each ID. Subsequently, the same query instances are fed into the student network and the teacher network after two different data augmentations. The dynamic custering contrastive learning (DyCL) performs a momentum update of the cluster representation vectors utilizing a hard sample mining strategy by the correlation between the student output features and the corresponding cluster representation vectors.\n\nEventually, our proposed label smoothing soft contrastive Loss (L_ss) is the final loss function, calculated from the output features and the cluster representation vectors.\n\n\n\n \u00a7.\u00a7 Dynamic Clustering Parameter Scheduler\n\n\n\nThe hyper-parameters of the clustering algorithms in USL Re-ID methods control the classification efficiency, eps in DBSCAN. However, in machine learning task, especially unsupervised Re-ID<cit.>, these hyper-parameters are static, which does not match the features that are constantly changing during the training process. The density of the clusters and the value of eps are closely related: if eps is too large, there will be too much noise in the clusters, and if eps is too small, many valid samples will be excluded. \n\nCoinciding with this paper, the learning rate<cit.> which is the most fundamental hyper-parameter in neural network dynamically decays too. Motivated by learning rate, a dynamic eps parameter strategies are given.\n\nWe strive to ensure that the eps parameter variation curves fits the inter-class and intra-class distance variation curves in order to further align them. Then the inter- and intra-class distance variation curves are convex functions, which is demonstrated in Figure <ref>. We thus choose the monotonical exponential function. The eps value \u03f5 increases with each epoch to become a multiple of \u03c3_e from the previous epoch. It can be formulated as follows: \n\n\n    \u03f5 = \u03f5_beign*\u03c3_e^epochs\n\nwhere \u03c3_e\u2208[0,1]  is the decay ratio, and \u03f5_beign is the initial eps value, witch is generally taken to be a larger number such as 0.7. Then epochs is the current number of epochs in training.\n\nFigure <ref> shows function plots for different dynamic clustering parameter scheduler where our proposed exponential dynamic parameter scheduler converges more closely to the feature distribution than the other two. In Figure <ref>, it can be seen that the eps parameter continues to decay without end, while the curves of inter-and intra-class distance first fall and rise steeply, and finally level off. If eps decreases when the feature distance is stable in the late training period, some information may be lost, which affects the quality of pseudo-labels. Therefore, we make the eps parameter terminate when reaching a certain value (one half of the initial value).\n\n\n\n\n\n \u00a7.\u00a7 Dynamic Cluster Contrastive Learning\n\n\n\nClassical USL Re-ID methods based on contrastive learning calculate the loss in the context of a mini-batch. In order to break the constraint which the methods pick positive and negative samples locally, the features of SpCL<cit.> are stored in the global memory and updated gradually with the training process. However, the batch training approach allows only some of the instances in a class to be updated in each iteration, leading to the unbalanced updating pace, which will shift the feature distribution. To solve this problem, Cluster Contrast<cit.> stores the cluster representation vectors directly in the memory bank and updates them in a uniform momentum manner. Where the ClusterNCE loss<cit.> can be formulated as:\n\n\n    L_cluster = \ud835\udd3c[-logexp(q \u00b7 c_+ / \u03c4)/\u2211 ^C_i=1exp(q \u00b7 c_i/\u03c4)]\n\n\nwhere c_i is the i-th cluster representation vector stored in memory bank, c_+ is the cluster representation vector corresponding to q, \u03c4 is the temperature factor and C is the total number of clusters in pseudo-labels. Cluster representation vectors speed up the convergence of the model and also alleviate the inevitable drawbacks of the false positive due to the uncertainty of the clustering algorithm. Cluster representation vectors should reflect as much information about the class as possible to ensure learning accuracy. However, Cluster Contrast uses the average centroid or the hardest sample that do not reflect the holistic distribution.\n\nInspired by adaptive weight triplet loss<cit.>, we propose a dynamic clustering contrastive learning (DyCL) method. Contrary to adaptive weight triplet loss, which adopts the hard sample mining strategy in pairwise loss to obtain the distance between positive and negative pairs, we employ dynamic weights to memory momentum updating, so that the model can fully exploit the valid information in the global context. According to the hard sample mining strategy, we assign a corresponding weight to similar instances of each query instance, with the harder instances having a greater weight. A softmax function is used to gain the weights of the samples in order to emphasize the significance of hard instances and prevent entering a local optimum due by:\n\n\n    w_ij^dy = exp(-<c_i \u00b7 z_j>/\u03c4_w)/\u2211_m=1^N_iexp(-<c_i \u00b7 z_m>/\u03c4_w)\n\n\nwhere \u03c4_w is the temperature coefficient hyper-parameter that affects the proportion of weights for hard instances, N_i is the instance number of i-th class in a mini-batch and z_m is the m-th instance feature of N_i. Note that the sum of weights is \u2211_j=1^N_iw_ij^dy = 1. Thus, the i-th dynamic cluster centroid is the weighted mean in the mini-batch:\n\n\n    \u0109_\u0302\u00ee = \u2211_j=1^N_i w_ijz_j\n\n\n\n    c_i \u2190\u03b3c_i + (1-\u03b3)\u0109_\u0302\u00ee\n\n\nwhere \u03b3 is the hyper-parameter of momentum updating. A comparison with the other two contrastive learning methods is shown in Figure <ref>. It can be seen that DyCL is more responsive to the details of the feature distribution and that the adjustable \u03c4_w hyper-parameter can further balance the global and local information.\n\n\n\n\n\n \u00a7.\u00a7 Label Smoothing Soft Contrastive Loss\n\n\n\nWe have investigated the loss functions of existing USL Re-ID methods and found two potential improvements: Firstly, these methods tend to use more complex loss functions in order to enable the model to learn multiple tasks simultaneously. ICE<cit.> incorporates proxy loss, hard instance contrastive loss, and soft instance contrastive loss to reduce intra-class distribution differences and mitigate the distortion due to data augmentation. HDRCL<cit.> combines pseudo-label-based local-to-global contrastive loss and self-supervised probabilistic regression loss to enable the model to generate more discriminative features. PPLR<cit.> is even more targeted at the proposed part-based unsupervised Re-ID framework with four loss functions containing camera loss, local feature loss, global feature loss, and triplet loss. Besides the relatively heavier computational cost of loss and back-propagation, the overly complicated hyper-parameters and weights also make it difficult to tune. Secondly, since data augmentation may cause a degree of distortion, the features of the same query instances may differ, resulting in the similarity change within features, which affects the consistency of the feature distribution and possibly undermines the model performance. MMT<cit.> proposes a soft classification loss based on the 'Teacher-Student' model<cit.>, where computes the temporally averaged similarity probability of the teacher network output as a soft label to supervise the the student network, and the output of both networks can be consistent. This self-supervised method avoids the error amplification from perturbations in the feature distribution during training in a non-parametric manner<cit.>.\n\nTo achieve the above goals, we try to combine contrast learning and self-supervised learning with only one loss function. We propose a label smoothing soft contrastive loss (L_ss) based on pseudo-labels refinement for the prediction of teacher features and clustering results. Specifically, we generate a refined smoothing soft label y^sm_k for each query instance:\n\n\n    y^sm_k=\u03bc_sy^t_k+(1-\u03bc_s) \u1ef9_\u0303k\u0303\n\n\nwhere \u1ef9_\u0303k\u0303 represents the clustering-generated pseudo-label of the query instance which is an one-hot label. \u03bc_s\u2208[0,1] is a weight parameter controlling the ratio of the soft label y^t_k to the one-hot pseudo-label \u1ef9_\u0303k\u0303. Soft label y^t_k  is the similarity probability of the query feature to the k-th class representation vector, which can be formulated as:\n\n\n    y^t_k=exp(q_t \u00b7 c_k / \u03c4)/\u2211 ^C_i=1exp(q_t \u00b7 c_i/\u03c4)\n\nwhere q_t represents the query features from the teacher network, c_i is the i-th cluster representation vector, and \u03b1 is the temperature coefficient hyper-parameter. In contrast to the one-hot pseudo-label which only exploits class information, the refined smoothing soft label additionally considers the consistency of feature distribution of the same query instance. Since ClusterNCE loss is effectively a cross-entropy loss<cit.>, plugging y^sm_k into Eq. <ref>, we can obtain the formula for the label smoothing soft contrastive loss as:\n\n\n    L_ss=\u2211_k=1^C -y^sm_k \u00b7 log(y^s_k)\n\nwhere y^t_k denotes the similarity probability of the teacher network, which also has the same expression as Eq. <ref>. Unlike previous work with a loss function consisting of multiple parts, the label smoothing soft contrastive loss L_ss integrates the contrastive and self-supervised methods with high computational efficiency and low computational consumption.\n\n\n\n\n\n\u00a7 EXPERIMENT\n\n\n\n\n \u00a7.\u00a7 Dataset and Evaluation Protocol\n\n\nWe validate the proposed method on three generic person Re-ID datasets, Market1501<cit.>, DukeMTMC-reID<cit.>, and MSMT17<cit.>, respectively.\n\nMarket-1501 is collected from 6 cameras at the entrance of supermarkets in Tsinghua University campus. 12936 pedestrian images from 751 pedestrian IDs are included in the training set, and 19732 pedestrian images from 750 pedestrian IDs are included in the test set.\n\nDukeMTMC-reID is a subset of the DukeMTMC dataset, which is derived from 8 non-overlapping cameras. 16522 images from 702 pedestrian IDs in the dataset are training images, 2228 images from another 702 pedestrian IDs are query images, and 17661 are gallery images.\n\nMSMT17 is a recently proposed large person Re-ID dataset, containing a total of 126441 images from 4101 pedestrian IDs captured from 15 cameras, of which the training set contains 32621 images from 1041 pedestrian IDs and the test set contains 93820 images from 3060 pedestrian IDs.\n\nOur experiments validate the model performance using Rank-1, Rank-5 and Rank-10 precision of cumulative matching characteristics (CMC)<cit.> and mean average precision (mAP)<cit.> metrics. We don't use any post-processing operations, such as re-ranking<cit.>.\n\n\n\n \u00a7.\u00a7 Implementation\n\n\nNetwork structure. We used 4 Tesla V100 GPUs and Resnet50<cit.> as the encoder backbone for feature extraction,  pre-trained on ImageNet<cit.>. We removed all modules after the fourth convolutional layer of the backbone networks and added a global average pooling layer (GAP), followed by a batch normalization layer<cit.> and a L2 normalization layer, with 2048-dimensional output features.\n\nParameter settings. We use a warm-up strategy, where the learning rate grows linearly to 0.00035 for the first 20 epochs, and then stops decaying. We use Adam with a weight decay factor of 5e-4. The network performs 200 iterations in one epoch, for a total of 70 epochs. Each mini-batch contains 64 images from 4 pedestrian IDs, which means the batch size is 256. Before being fed into the network, each image is scaled down to a pixel size of 256\u00d7128 with data augmentation, including random inverting, random cropping and random erasing. We set the momentum update parameters  \u03b3 to 0.1 in Eq. <ref>, \u03c4 of contrastive loss to 0.05 in Eq. <ref> and \u03c4_w of dynamic weights to 0.09 in Eq. <ref>. Before the start of each epoch, we calculate the feature distances by Jaccard coefficients with a nearest neighbour parameter k of 30, and then DBSCAN generates the corresponding pseudo-labels for all features in the dataset, with the minimal number of neighbors set to 4.\n\n\n\n \u00a7.\u00a7 Ablation Study\n\n\nTo validate the effectiveness of our proposed method, we conducted detailed comparative experiments on Market1501 and DukeMTMC-reID.  We choose Cluster Contrast<cit.> as the baseline method for our experiments, which updates the cluster representation vectors directly with query instances. We verify the effectiveness of our proposed three components: the dynamic clustering parameter scheduler (DCPS), the dynamic cluster contrastive learning (DyCL) and the label smoothing soft contrastive loss (L_ss), and the results are shown in Tab. <ref>. We intuitively visualize the features extracted by baseline model and our DCCC model utilizing T-SNE<cit.> as shown in Figure <ref> which shows our cluster compactness and independence.\n\n\n\n\n\nEffectiveness of dynamic clustering parameter scheduler (DCPS). The effectiveness of DCPS is demonstrated by the comparison between the results in (\u266f 1 and \u266f 3), (\u266f 4 and \u266f 5), and (\u266f 6 and \u266f 8), as shown in Tab.<ref>. DCPS enables the clustering parameter to no longer be fixed, but to vary dynamically with the feature distribution. The experimental results imply that DCPS benefits the clustering algorithm in generating high quality pseudo-labels. DCPS eventually uses the exponential EPS scheduler (ExpoES) from Sec. <ref> for DBSCAN. To further validate the effectiveness of ExpoES, we compare it with the step EPS scheduler (StepES) and the linear EPS scheduler (LinearES). As shown in Tab. <ref>. ExpoES has the most significant improvement on baseline, with mAP improving by 0.8 on Market1501 and DukeMTMC-reID.\n\n\n\nEffectiveness of dynamic cluster contrastive learning (DyCL). As shown in Tab. <ref>, the effectiveness of DyCL is illustrated by the comparison between the results in (\u266f 1 and \u266f 2), (\u266f 4 and \u266f 6) and (\u266f 6 and \u266f 8). DyCL enables cluster representation vectors to be aligned with the feature distribution locally, enabling the model to learn finer-grained knowledge. We also experiment with the two non-dynamic cluster contrastive learning methods in Sec. <ref>. The results of the baseline, the cluster average contrastive learning (AvgCL) and the hardest sample contrastive learning (HardestCL) are not as good as the performance of DyCL in Tab. <ref>.\n\nEffectiveness of label smoothing soft contrastive loss (L_ss).  The efficiency of L_ss can be observed in the comparison between (\u266f 1 and \u266f 4), (\u266f 2 and \u266f 6), (\u266f 3 and \u266f 5) and (\u266f 7 and \u266f 8) in Tab. <ref>. In terms of the performance, the baseline is greatly improved by 3.9%/1.7% and 1.0%/0.1% mAP/Rank-1 on Market1501 and DukeMTMC-reID via L_ss. In order to lessen the interference brought on by data augmentation, L_ss is built on a dual network topology including probabilistic distillation and contrastive loss based on label refinement. Additionally, the comparison of L_ss and cross-entropy loss can be seen in the appendix.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Parameter Analysis\n\n\nWe analyze the sensitivity of the hyper-parameter \u03c4_w and \u03bc_s. The value of \u03c4_w affects the weighting of the difficult instances in the cluster representation vectors. A larger value of \u03c4_w means a larger proportion of difficult instances, but the cluster representation vectors will contain less global information of query instances. Conversely, the smaller the value of \u03c4_w taken, the more the model will be biased towards locally optimal solutions and will not achieve higher performance. To find the optimal \u03c4_w in Eq. <ref>, we designed experiments to analyze the indicator curves of mAP and Rank-1 for \u03c4_w ranging from 0.01 to 0.13 and intervals of 0.02, as shown in Figure <ref>. The best results were obtained at \u03c4_w=0.03 on Market1501 and at \u03c4_w=0.07 on DukeMTMC-reID. In Eq. <ref>, \u03bc_s controls the significance of the redefined label in L_ss for soft labels. We tune this parameter finely with the others fixed. Large \u03bc_s will bias the model more towards eliminating errors due to image distortion, but the model will have more trouble in learning intra- and inter-class information. Small \u03bc_s will result in a reduction in noise immunity. If \u03bc_s = 0, the loss function will decomposes down to be a cross-entropy loss function and cause the performance drop. Based on the experimental results in Figure <ref>, we set \u03bc_s to 0.3 and 0.1 on Market1501 and DukeMTMC-reID, respectively.\n\n\n\n\n\n\n \u00a7.\u00a7 Comparison with State-of-the-Art Methods\n\n\nOn three common datasets, Market1501, DukeMTMC-reID, and MSMT17, we compared our proposed DCCC with various advanced USL Person Re-ID methods. The results are given in Tab. <ref>. It can be shown from the comparison that DCCC outperforms previous methods including LOMO<cit.>, BOW<cit.>, BUC<cit.>, MMCL<cit.>, JVTC<cit.>, JVTC+, HCT<cit.>, UGA<cit.>, CycAs<cit.>, SpCL<cit.>, CAP<cit.>, ICE<cit.>, Cluster Contrast<cit.>, and IIDS<cit.>. Just like previous methods, DCCC do not use pre-trained data other than ImageNet. The baseline method Cluster Contrast utilizes clustering centroids too, but DCCC completely exploits the feature distribution of query instances. DCCC surpasses Cluster Contrast by 4.5%/1.8%, 1.4%/0.5%, 4.0%/6.3% for mAP/Rank1 on Market1501, DukeMTMC-reID and MSMT17. Moreover, unlike ICE, CAP and IIDS, DCCC don't use any camera information and achieves better performance with more limited information. Even more, DCCC outperformed some well-known supervised methods. Ultimately, our proposed method achieves 86.6%/94.1%, 74.0%/85.4%, 31.6%/62.3% results on Market1501, DukeMTMC-reID and MSMT17. Considering the different settings of backbone in ISE<cit.>, Cluster Contrast and HDCRL<cit.>, we provide the results of DCCC with generalized-mean pooling (GeM) and IBN-Net for a fair comparison and discuss their effectiveness in the appendix.\n\n\n\n\u00a7 CONCLUSION\n\n\nIn this paper, we propose a novel unsupervised Re-ID framework based on dynamic clustering and dynamic cluster contrastive learning. We design a dynamic cluster contrastive learning method with adaptive weights to store cluster representation vectors in cluster-level memory to solve the inconsistency problem. Then, we focus on the often overlooked clustering hyper-parameters by deploying a dynamic EPS scheduler to DBSCAN, resulting in a more stable clustering process. And we proposed a label smoothing soft contrastive loss to consider contrastive learning and self-supervised learning together with less computaional cost. Finally, we conduct experiments to validate the performance of the proposed method. Experiments' results show that our method has achieved the best performance comparing with those state-of-the-art methods. \n\nunsrt\n\n\n\n\n\n\n\n\u00a7 APPENDIX\n\n\n\n \u00a7.\u00a7 Different Settings of Backbone\n\n\nIBN-Net<cit.> uses instance normalization and batch normalization jointly on the basis of ResNet, which greatly improves the generalization and learning ability of the model, and solves the problem of domain transfer well on the Re-ID task, so we conducted related experiments using IBN-ResNet50-a. GeM<cit.> is capable of adaptively implementing feature space mapping, with a more robust feature representation compared to average pooling and maximum pooling. We also replaced the global average pooling (GAP)in the vanilla ResNet50 with generalized-mean pooling (GeM) for comparison. As shown in Tab. <ref>, both IBN-Net and GeM pooling gave a considerable improvement to the network.\n\n\n\n\n\n \u00a7.\u00a7 Comparison between L_ss and L_ce+L_ss.\n\n\nComparison tests are conducted to compare with the losses under the traditional single network configuration, and the findings are presented in Tab. <ref>. InfoNCE loss is a cross-entropy loss. displays the baseline cross-entropy loss (L_ce) based on a single st row network topology, while line 2nd row displays the situation in which L_ce and L_ss (+L_ss) are applied, with weights of 0.7. The experiments show that using L_ss alone achieves better results with less number of parameters and computational effort than with the former.\n\n\n\n\n\n \u00a7.\u00a7 Sensitivity analysis of hyper-parameter step\n\n\nWe validate the simplest step EPS scheduler to exploit how does it work like the step learing rate scheduler. We conducted experiments on the step EPS scheduler with step=1,5,10,15, and the experimental results are shown in Figure <ref>. In particular, the step EPS scheduler degenerates to a linear EPS scheduler when step=1. It is easy to see that the performance degrades as step grows for both the Market1501 and DukeMTMC-reID. Therefore, the linear EPS scheduler is better than the step EPS scheduler.\n\n\n\n"}