{"entry_id": "http://arxiv.org/abs/2303.06827v1", "published": "20230313030003", "title": "Kernel Density Bayesian Inverse Reinforcement Learning", "authors": ["Aishwarya Mandyam", "Didong Li", "Diana Cai", "Andrew Jones", "Barbara E. Engelhardt"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "text": "\n\nLarge statistical learning models effectively forecast diverse chaotic systems\n    William Gilpin\n    March 30, 2023\n==============================================================================\n\n\n\n\nInverse reinforcement learning\u00a0(IRL) is a powerful framework to infer an agent's reward function by observing its behavior, but IRL algorithms that learn point estimates of the reward function can be misleading because there may be several functions that describe an agent's behavior equally well. A Bayesian approach to IRL models a distribution over candidate reward functions, alleviating the shortcomings of learning a point estimate. However, several Bayesian IRL algorithms use a Q-value function in place of the likelihood function. The resulting posterior is computationally intensive to calculate, has few theoretical guarantees, and the Q-value function is often a poor approximation for the likelihood. We introduce kernel density Bayesian IRL (KD-BIRL), which uses conditional kernel density estimation to directly approximate the likelihood, providing an efficient framework that, with a modified reward function parameterization, is applicable to environments with complex and infinite state spaces. We demonstrate KD-BIRL's benefits through a series of experiments in Gridworld environments and a simulated sepsis treatment task.\n\n\n\n\n\u00a7 INTRODUCTION\n\nReinforcement learning (RL) methods find policies that maximize an agent's long-term expected reward within a Markov decision process (MDP). In many observational data settings, we observe a sequence of states and actions for an agent carrying out a policy driven by an unknown reward function. It can be useful to learn this reward function to identify the factors driving the agent's behavior. For example, in a hospital setting, we see a patient's treatment schedule and measurements of physiological state. To make sense of the underlying factors influencing treatment decisions, we can identify the clinician's reward function (i.e., objectives), and examine how this function drives treatment decisions given a patient's state. It is particularly difficult to infer the reward function in this setting because the vector of observed covariates for a patient at a given time is often noisy and partially missing, and there may be several candidate reward functions that can explain the doctor's behavior. \n\nInverse reinforcement learning (IRL) methods infer an agent's reward function given observations of the agent's behavior. \nEarly IRL algorithms were used to identify point estimates of the reward function that best explained an agent\u2019s behavior\u00a0<cit.>, and were applied to problems in path planning\u00a0<cit.>, urban navigation\u00a0<cit.> and robotics\u00a0<cit.>. A point estimate can also aid in imitation learning, where the inferred reward function is used to fit RL policies that replicate desired behavior. \n\nDespite the success of early IRL approaches, there are limitations to inferring a point estimate. First, the IRL problem is often non-identifiable\u00a0<cit.>, meaning there may be multiple (and possibly infinite) reward functions that explain a set of behaviors equally well. Second, for finite demonstration data, point estimates fail to capture the uncertainty and noise in the data-generating process. Thus, it is advantageous to take a Bayesian approach, which treats the reward function as inherently random and communicates a degree of uncertainty that relies on the dataset distribution. A Bayesian approach to IRL computes a posterior distribution that places mass on reward functions proportional to how well they explain the observed behavior\u00a0<cit.>.\n\nHowever, existing Bayesian IRL methods can be computationally demanding. In Bayesian modeling, likelihood specification has a large impact on the resulting posterior distribution. The formulation of the likelihood function (i.e., the function describing the probability of observing a state-action pair given a reward function) is unknown in the IRL setting. Existing approaches replace it with an optimal Q-value function, denoted by Q^\u22c6, which best approximates the long-term expected reward for a given state-action tuple(<cit.>). The Q-value function must be learned using Q-learning\u00a0<cit.>, a \u201cforward RL\u201d algorithm, or one that solves an environment's MDP. The original algorithm pioneered by \u00a0<cit.> and the majority of its successors use Markov chain Monte Carlo (MCMC) sampling to compute a posterior over the reward function, and every iteration of MCMC requires forward RL for each sampled reward function. This is computationally expensive, especially with infinite or high-dimensional state spaces. Additionally, a posterior that uses Q^\u22c6 as a likelihood is equivalent to a Gibbs posterior\u00a0<cit.> and lacks desirable theoretical properties\u00a0<cit.>. \n\nWe address these challenges with kernel density Bayesian inverse reinforcement learning (KD-BIRL), a method that (1) estimates the likelihood function directly, leading to theoretical guarantees for the consistency of the resulting posterior distribution, and (2) disassociates the number of times forward RL is required from the number of iterations of MCMC sampling, thus reducing computational complexity. The contributions of our work are as follows: \n\n\n    \n  * We propose KD-BIRL, a Bayesian IRL method that uses conditional kernel density estimation to directly approximate the likelihood function (Section <ref>).\n    \n  * We justify our method theoretically by proving posterior consistency, and demonstrating that the posterior contracts to the equivalence class of the expert reward function (Section <ref>).\n    \n  * We show that KD-BIRL's posterior estimates efficiently and accurately capture agent priorities in Gridworld environments (Section <ref>).\n    \n  * We demonstrate that, with a feature-based reward function, KD-BIRL can successfully infer rewards in complex state spaces such as a sepsis management task (Section <ref>).\n\n\n\n\n\n\n\n\u00a7 PRELIMINARIES\n\n\n\n\n\n \u00a7.\u00a7 Background: Inverse reinforcement learning (IRL)\n\nThe goal of IRL methods is to infer the reward function of an agent, given its behavior. An RL agent interacts with and responds to an environment that can be defined using an MDP. An MDP is represented by (\ud835\udcae, \ud835\udc9c, P, R), where \ud835\udcae is the state space; \ud835\udc9c is the set of actions; P(\ud835\udc2c^e_t+1 | \ud835\udc2c^e_t, \ud835\udc1a_t) defines state-transition probabilities from time t to t+1; and R : \ud835\udcae\u2192\u211d is a reward function, where R \u2208\u211b and \u211b denotes the space of reward functions. The input to an IRL algorithm is a set of expert demonstrations, {(s^e_t, a^e_t)}_t=1^n, where each demonstration is a 2-tuple (s^e_t, a^e_t) representing an agent's state and chosen action at time t. These demonstrations are assumed to arise from an agent acting according to policy, \u03c0^\u22c6:\ud835\udcae\u2192\ud835\udc9c, that is optimal for a fixed but unknown reward function R^\u22c6. Given these demonstrations, IRL algorithms seek a reward function R^\u22c6 such that \u03c0^\u22c6 is optimal with respect to R^\u22c6.\n\nBayesian approaches to IRL treat the reward function R as inherently random. By specifying a prior distribution over R and a likelihood function for the observed data,\nthese methods then infer a posterior distribution over R given n expert demonstrations of an agent {(s_i^e,a_i^e)}_i=1^n. \nUsing Bayes rule, the posterior density is equivalent to the product of the prior distribution on the reward, p(R), and the likelihood of the expert demonstrations given the reward function, with a normalizing constant that corresponds to the probability of the expert demonstrations:\n\n\n    p(R  | {(s_i^e,a_i^e)}_i=1^n)\n    \n    =  p(R)\u220f_i=1^n p(s_i^e,a_i^e|R)/p({(s_i^e,a_i^e)}_i=1^n).\n\n\nIn the initial formulation of Bayesian IRL (BIRL)\u00a0<cit.>, the authors propose using a Q-value function to calculate the likelihood in <Ref>. The Q-value function for a given policy \u03c0 at time t is Q^\u03c0(s_t, a_t) = r_t + \u03b3\ud835\udd3c_s^'\u223c P[V^\u03c0(s^')],\nwhere s_t, a_t, and r_t are the state, action, and reward at time t, and \u03b3\u2208 [0, 1] is a discount factor. This approach uses\nan optimal Q-value function, Q^\u22c6,  as a component of the likelihood within a Gibbs posterior framework.\nThe \u201clikelihood\u201d takes the form\n\n    p(s, a   |  R) \u221d e^\u03b1 Q^\u22c6(s, a, R),\n\nwhere Q^\u22c6(s, a, R) is the optimal Q-value function for reward function R, and \u03b1 > 0 is an inverse temperature parameter that represents confidence in the agent's ability to select optimal actions. \n\nThere are several potential challenges with \nlearning the aforementioned BIRL posterior.\nFirst, the optimal Q^\u22c6 is found using Q-learning, a forward RL algorithm, and Q^\u22c6 is typically expensive to estimate for a new R. BIRL uses MCMC, which requires learning Q^\u22c6 on every iteration of sampling for a new R.\nIn addition, because Equation <ref> is a loss-based function (rather than a true likelihood), the resulting function is not a classical Bayesian posterior\u00a0<cit.> and does not have theoretical guarantees regarding posterior contraction (more details in Appendix Section 2,3). \nAdditionally, Q-value estimates can be incorrect for states that are very rarely visited, as often happens in infinite state spaces, leading to incorrect likelihood estimates that can affect the accuracy of the BIRL posterior.\n\n\n \u00a7.\u00a7 Related Work\n\n\nSeveral extensions to the original BIRL algorithm\u00a0<cit.> have been proposed. The first set of methods identifies nonparametric reward functions\u00a0<cit.>. These algorithms use a variety of strategies such as Gaussian processes\u00a0<cit.>, Indian buffet process (IBP) priors\u00a0<cit.>, and Dirichlet process mixture models\u00a0<cit.> to learn reward functions for MDPs with large state spaces that may include sub-goals. Other methods reduce computational complexity by using either more informative priors\u00a0<cit.>, different sampling procedures (e.g., Metropolis-Hastings\u00a0<cit.> or expectation-maximization\u00a0<cit.>), variational inference to approximate the posterior\u00a0<cit.>, or by learning several reward functions that each describe a subset of the state space\u00a0<cit.>. However, all of these approaches use a Q-value function in place of the likelihood and hence still suffer from consistency and computational issues; this construction is both inefficient and limits desirable Bayesian behaviors with regard to posterior sampling and uncertainty quantification. \n\nTo address these computational and consistency issues, it is necessary to either directly estimate the likelihood, reduce the number of times forward RL is performed, or modify the reward function parameterization. Recent work proposes a variational Bayes framework, Approximate Variational Reward Imitation Learning (AVRIL)\u00a0<cit.>, to approximate the full posterior. This method improves upon existing work by avoiding re-estimating Q^\u22c6 for every sampled reward function, allowing it to bypass some of the computational inefficiencies of <cit.>'s initial formulation. However, AVRIL still requires the use of a Q-value function, resulting in a misspecified optimization objective. One method avoids using a Q-value function entirely for the likelihood\u00a0<cit.> and instead approximates it using real-time dynamic programming or action comparison. Other work proposes a feature-based reward function, which parameterizes the reward as a linear combination of a set of weights and a low-dimensional feature encoding of the state\u00a0<cit.>. This approach can be beneficial because the posterior inference is over a lower dimensional reward vector. More recent work builds on this approach and proposes a method that enables imitation learning in complex control problems\u00a0<cit.>. All of these techniques are best suited for environments with a closed-loop controller that provides instant feedback.\n\n\n\n\u00a7 METHODS\n\n\n\n\n \u00a7.\u00a7 Conditional kernel density estimation\n\nAs discussed earlier, directly estimating the likelihood function can lead to theoretical guarantees of posterior consistency. To estimate the likelihood p(s,a  |  R), we first observe that it can be viewed as the conditional density of the state-action pair given the reward function. Thus, any appropriate conditional density estimator could be applied; examples include the conditional kernel density estimator (CKDE,\u00a0<cit.>) and Gaussian processes (GPs). We adopt the CKDE because it is nonparametric, has a closed form, and is straightforward to implement\u00a0<cit.>. \nMotivated by the conditional probability equation p(y|x)=p(x,y)/p(x) (where x and y are two generic random variables), the CKDE estimates the conditional density p(y|x) by \napproximating the joint distribution p(x,y) and marginal distribution p(x) separately via kernel density estimation (KDE). \nGiven pairs of observations {(x_j,y_j)}_j=1^m, the KDE approximations for the joint and marginal distributions are\n\n    p(x,y)=1/m\u2211_j=1^m\n    K(x-x_j/h)\n    K'(y-y_j/h'),\n        \n    p(x)=1/m\u2211_j=1^m\n    K(x-x_j/h),\n\nwhere K and K' are kernel functions with bandwidths h, h' > 0, respectively. To approximate the conditional density, the CKDE simply takes the ratio of these two KDE approximations:\n\n    p(y|x) =p(x,y)/p(x)\n    =\u2211_j=1^mK(x-x_j/h)\n    K'(y-y_j/h')/\u2211_\u2113=1^m\n    K(x-x_\u2113/h).\n\n\n\n\n \u00a7.\u00a7 Kernel Density Bayesian IRL\n\nWe propose kernel density Bayesian inverse reinforcement learning (KD-BIRL), which uses a CKDE approximation p_m(s,a  |  R) to estimate the likelihood p(s,a  |  R). While the standard form of the CKDE (<Ref>) uses the difference between two samples (e.g., x - x_j) as input to the kernel functions, this difference can be replaced by any suitable distance metric\u00a0<cit.>. To estimate the joint and marginal distributions, p(s, a, R) and p(R), we must specify two distance functions: one for comparing state-action tuples and one for comparing reward functions. We denote these as d_s: (\ud835\udcae\u00d7\ud835\udc9c) \u00d7 (\ud835\udcae\u00d7\ud835\udc9c)  \u2192\u211d_+ and d_r: \u211b\u00d7\u211b\u2192\u211d_+, respectively, and we discuss specific choices for them later.\nThe CKDE approximation is then\n\n    p_m(s,a  |  R)\n    =\n    p_m(s,a, R)/p_m(R)\n    =\n    \u2211_j=1^m\n    K(d_s((s,a), (s_j,a_j))/h)\n    K'(d_r(R, R_j)/h')  \n    /\u2211_\u2113=1^m K(d_r(R, R_l)/h') ,\n\nwhere h, h' > 0 are the bandwidth hyperparameters. \n\nNote that fitting a CKDE for the likelihood requires estimating the density across a range of reward functions and state-action pairs. To better enable this,\nwe construct an additional set of demonstrations and reward functions \u2013 which we call the training dataset {(s_j,a_j, R_j)}_j=1^m \u2013\nto augment the observed expert demonstrations\n{(s_i^e, a_i^e)}_i=1^n (from an\nagent acting according to the data generating reward R^\u22c6). \nThe training dataset \ncontains demonstrations from agents whose policies optimize for reward functions that are likely distinct from those of the expert. Each sample in the training dataset is a state-action pair associated with a reward function. There will be many state-action pairs that correspond to the same reward function; therefore, R_j is not unique. In a simulated setting where the training demonstrations are not available already, we choose k training set reward functions, learn k optimal policies that optimize for each of these functions, and generate \u230a m/k \u230b demonstrations from each policy. We sample the reward functions R_1, \u2026, R_k \u223c u,\nwhere u is a distribution on the space of reward functions\n\u211b. The resulting density estimate is more accurate when R_1, \u2026, R_k are uniformly distributed across \u211b. As such, in our experiments with simulated environments, we use a uniform distribution for u.  \n\nUsing the CKDE in <Ref>,\nwe can now estimate the posterior density function of R given n expert demonstrations, m training demonstrations, and prior p(R):\n\n    p_m^n(R|{s_i^e,a_i^e}_i=1^n) \u221d p(R)\n    \u220f_i=1^n p_m(s_i^e,a_i^e  |  R)\n    \n    = p(R)\n    \u220f_i=1^n\n    \u2211_j=1^m\n    K(d_s((s_i^e,a_i^e), (s_j,a_j))/h)\n    K'(d_r(R, R_j)/h') \n    /\u2211_\u2113=1^m K(d_r(R, R_l)/h') .\n\nThe choice of the prior, p(R), and the distance metrics d_s, d_r can be altered depending on information known about the reward function or state space in advance\u00a0<cit.>. For example, if the reward function is assumed to be a linear function of the state, the cosine distance is more appropriate for d_r. Several non-uniform priors may be appropriate for p(R) depending on the characteristics of the MDP, including Gaussian\u00a0<cit.>, Beta\u00a0<cit.>, and Chinese restaurant process (CRP)\u00a0<cit.>. In KD-BIRL, the kernel K(x)=exp(-x^2) is chosen to be Gaussian. We choose a Gaussian kernel because it can approximate bounded and continuous functions well. The bandwidth hyperparameters can be chosen using rule-of-thumb procedures\u00a0<cit.>.\n\nTo infer the posterior estimate in <Ref>,\nwe sample rewards using a Hamiltonian Monte Carlo algorithm\u00a0<cit.> (additional details in Appendix Section 9).\nNote a key computational gain of our approach over BIRL, which is also a sampling-based algorithm: we only use forward RL to generate the training dataset (in the case of simulated environments or when it is not already present), and avoid it in each iteration of MCMC. This is possible because <Ref> does not depend on Q^\u22c6. \n\n\n\n  \u00a7.\u00a7.\u00a7 Feature-based reward function\n\n\nWhile KD-BIRL can be applied as-is in many environments, the CKDE is known to scale poorly to high-dimensional functions. Thus, before KD-BIRL can work in environments with large state spaces where the corresponding reward function has a large number of parameters, it is necessary to re-parameterize the reward function. In our existing formulation, the reward function is parameterized as a vector where each index corresponds to the reward in one of the states. Under this formulation, in a 10\u00d710 Gridworld environment, the reward function would be represented as a vector of length 100. In practice, the CKDE increases in computational cost with respect to both the length of the vector and the number of samples in the expert and training datasets\u00a0<cit.>, and it would not be suited to learn 100 parameters. \n\nWe propose a formulation of KD-BIRL that uses a feature-based reward function\u00a0<cit.>. This method of parameterizing a reward is one of three broad categories of IRL formulations\u00a0<cit.>. The feature-based reward function R(s,a) = w^\u22a4\u03d5(s,a) where w \u2208\u211b^q and \u03d5: S \u00d7 A \u2192\u211b^q is advantageous because it does not scale with the dimensionality of the state s and does not rely on the state space being discrete like our earlier approach. Here, \u03d5 is a known function that maps a state-action tuple to a feature vector of length q. Intuitively, this feature vector is a low-dimensional representation of the original state that facilities reward inference. In this setup, the goal is to find w^\u22c6 such that:\n\n    w^\u22c6\u22a4 E[\u2211_t=0^\u221e\u03b3^t \u03d5(s_t, a_t) | \u03c0^\u22c6] \u2265 w^\u22c6\u22a4 E[\u2211_t=0^\u221e\u03b3^t \u03d5(s_t, a_t) | \u03c0]\n\nwhere \u03b3 is a discount factor, \u03c0 is a policy, and s_t, a_t is the state and action at time t. In a Bayesian setting, the resulting posterior is over w rather than R. \n\nNow we generate n expert and m training demonstrations. Recall that the CKDE requires as input the reward function parameters corresponding to each training dataset sample. A given training dataset sample here is\n{(s_j,a_j,w_j)}  where w_j is the weight vector of length q associated with the reward function that was used by the agent to generate the sample s_j, a_j. w_j can be repeated and is not unique. A sample from the expert demonstration dataset is still\n{(s_i^e, a_i^e)}_i=1^n, where the data-generating weights w^\u22c6 are used to generate these demonstrations. The procedure for learning the CKDE and the resulting posterior inference then stays the same. The CKDE formulation is now:\n\n    p_m(s,a  |  w)\n    =\n    p_m(s,a, w)/p_m(w)\n     = \n    \u2211_j=1^m\n    K(d_s((s,a), (s_j,a_j))/h)\n    K'(d_r(w,w_j)/h') \n    /\u2211_\u2113=1^m K(d_r(w,w_l)/h') ,\n\nwhere d_r measures the similarity between weight vectors, and d_s is the distance between state-action tuples. The posterior is then:\n\n    p_m^n(w|{s_i^e,a_i^e}_i=1^n) \u221d p(w)\n    \u220f_i=1^n p_m(s_i^e,a_i^e  |  w)\n    \n     = p(w)\n    \u220f_i=1^n\n    \u2211_j=1^m\n    K(d_s((s_i^e,a_i^e), (s_j,a_j))/h)\n    K'(d_r(w,w_j)/h') \n    /\u2211_\u2113=1^m K(d_r(w,w_l)/h') .\n\n\n\n\n\u00a7 THEORETICAL GUARANTEES OF KD-BIRL\n\n\nKD-BIRL estimates the density function of a true Bayesian posterior distribution (<Ref>), so we can reason about the posterior's asymptotic behavior. In particular, we want to ascertain that this posterior estimate contracts as it receives more samples. Because the IRL problem is non-identifiable, the \u201ccorrect\u201d reward function as defined by existing methods\u00a0<cit.>, may not be unique. In this work, we assume that any two reward functions that lead an agent to behave in the same way are equivalent. Said another way, if a set of observations is equally likely under two reward functions, the functions are considered equal: R_1\u2243 R_2 if p(\u00b7|R_1)-p(\u00b7|R_2)_L_1 = 0. We can then define the equivalence class [R^\u22c6] for R^\u22c6 as [R^\u22c6] = {R \u2208\u211b : R \u2243 R^\u22c6}. An ideal posterior distribution places higher mass on reward functions in the equivalence class [R^\u22c6]. \n\nWe first focus on the likelihood estimation step of our approach and show that, when the size of the training dataset m approaches \u221e and the m samples arise from sufficiently different reward functions that cover the space of \u211b, the likelihood estimated using a CKDE (Equation (<ref>)) converges to the true likelihood p(s,a  |  R). \n\n\nLet h_m, h_m'>0 be the bandwidths chosen for the CKDE. Assume that both p(s,a  |  R) and p(R) are square-integrable and twice differentiable with a square-integrable and continuous second order derivative, and that  mh_m^p/2\u2192\u221e and mh_m^'^p/2\u2192\u221e as m\u2192\u221e. Then, \n\n    p_m(s,a|R)p(s,a  |  R),\u00a0\u2200(s,a,R)\u2208\ud835\udcae\u00d7\ud835\udc9c\u00d7\u211b.\n\n\n\n<Ref> verifies that we can estimate the likelihood using a CKDE, opening the door to Bayesian inference. We now show that as n, the size of expert demonstrations, and m, the size of the training dataset, approach \u221e, the posterior distribution generated using KD-BIRL contracts to the equivalence class of the expert demonstration generating reward [R^\u22c6].\n\n Assume the prior for R, denoted by \u03a0, satisfies \u03a0({R:KL(R^\u22c6,R)<\u03f5})>0 for any \u03f5>0, where KL is the Kullback\u2013Leibler divergence. \nAssume \u211b\u2286\u211d^d is a compact set. Then, the posterior measure corresponding to the posterior density function p_m^n defined in <Ref>, denoted by \u03a0_m^n, is consistent w.r.t. the L_1 distance; that is, \n\n    \u03a0^n_m({R:p(\u00b7|R)-p(\u00b7|R^\u22c6)_L_1<\u03f5)}) 1.\n\n\n<Ref> implies that the posterior \u03a0_m^n assigns almost all mass to the neighborhood of [R^\u22c6]. This means that the reward function the KD-BIRL posterior contracts to with a large enough sample size is practically equivalent to the data-generating reward function R^\u22c6. Note that this not a statement regarding the posterior contraction rate, just a certification of contraction. Proofs for both <Ref> and <Ref> are in Appendix Section 4. \n\n\n\n\u00a7 EXPERIMENTS\n\n\nHere, we evaluate the accuracy and computational efficiency of KD-BIRL. We compare KD-BIRL to AVRIL\u00a0<cit.>, a recent method that that simultaneously learns an imitator policy and performs reward inference on the expert demonstrations, and the original Bayesian IRL algorithm (BIRL)\u00a0<cit.>. We demonstrate results using a Gridworld environment\u00a0<cit.> and a sepsis management clinical environment\u00a0<cit.>. \n\nTo quantitatively evaluate the reward functions learned by IRL methods, previous studies have used Expected Value Difference (EVD)\u00a0<cit.>. EVD is defined as |V^\u22c6(r^A) - V^\u03c0^\u22c6(r^L)(r^A)| where V^\u03c0 = \u2211_s p_0(s) V^\u03c0 is the value of policy \u03c0 with initial state distribution p_0, r^A is the true data-generating reward, r^L is the learned reward, and V^\u22c6 is the value function associated with the optimal policy \u03c0^\u22c6. Intuitively, the EVD measures the difference in reward obtained by an agent whose policy is optimal for the true reward and the reward obtained by an agent whose policy is optimal for the learned reward. We use EVD because it allows us to compare KD-BIRL to related methods without needing to directly compare two reward function samples based on their functional form. The lower the EVD, the better our learned reward recapitulates the expert reward (see Appendix Section 10 for more details). \n\n\n\n \u00a7.\u00a7 Gridworld environment\n\n\nWe begin our analysis in a Gridworld environment. The MDP here is defined by the grid's g\u00d7 g discrete state space \ud835\udcae where a given state is represented as a one-hot encoded vector in \u211b^g\u00d7 g, e_i, where the i'th index is 1 and corresponds to the state in which the agent is in, and g is the size of the grid; the action space contains 5 possible actions {, , , , }, each represented as a one-hot encoded vector; and the true reward function R^\u22c6, which is unobserved by the IRL algorithms, is a vector of length g \u00d7 g. We structure the reward function such that each state has an independent scalar reward parameter. We specify the domain of each of these parameters to be the unit interval; thus, each feasible reward function can be represented by a vector R \u2208 [0, 1]^g\u00d7 g.\n\nTo fit KD-BIRL we use Stan\u00a0<cit.>, which uses a Hamiltonian Monte Carlo algorithm. To fit the BIRL and AVRIL posteriors, we first generate the same number of expert demonstration trajectories as used for KD-BIRL. BIRL and AVRIL use an inverse temperature hyperparameter, \u03b1; we set \u03b1=1 for all methods. AVRIL uses two additional hyperparameters \u03b3, \u03b4, which we set to 1. Unless otherwise specified, KD-BIRL uses a uniform prior for the reward r_s \u223cUnif(0, 1) for s=1,\u2026,g \u00d7 g and Euclidean distance for d_s, d_r. \n\n\n  \u00a7.\u00a7.\u00a7 Visualizing KD-BIRL's posterior distribution\n\nFirst, we visualize KD-BIRL's posterior distribution in comparison to those recovered by AVRIL and BIRL. To do this, we show density plots of samples from all three distributions marginalized at each state in the 2\u00d72 Gridworld environment. All methods use a data-generating reward function R^\u22c6 = [0, 0, 0, 1]. We find that the posterior samples from KD-BIRL and BIRL are more concentrated around R^\u22c6 than those from AVRIL (<Ref>).\n\n\n\n  \u00a7.\u00a7.\u00a7 KD-BIRL requires fewer instances of Q-learning\n\nNext, we quantify the computational complexity associated with performing reward inference in a 4\u00d74 Gridworld in which only the state [3,3] contains a reward of 1. As discussed earlier, much of the computational cost associated with learning a posterior distribution in existing methods arises from repeated instances of forward RL. BIRL requires forward RL during every iteration of MCMC sampling; several thousand iterations are required for the sampler to converge. AVRIL uses one instance of forward RL to learn an approximate posterior. KD-BIRL also minimizes the use of forward RL, only using it during dataset generation, in the case that these observations are not already available. \n\nHere, we vary the number of iterations of forward RL and plot the EVDs for reward samples from the resulting posterior distributions for the three methods. Our results indicate that with fewer instances of forward RL, KD-BIRL reward samples better replicate the behavior of the expert demonstrations than those of BIRL (partly because the x-axis implies too few iterations of MCMC sampling); consequently, even though AVRIL requires fewer instances of forward RL, it is at the expense of accuracy in the posterior distribution, as highlighted by the stagnant EVD (<Ref>). \n\n\n\n\n \u00a7.\u00a7 Limitations of the CKDE\n\nIt is well known that CKDE has difficulty scaling to high-dimensional probability density functions\u00a0<cit.>. Regardless, we want to identify the limits of the CKDE used in the original KD-BIRL setup without a feature-based reward function. To do so, we use a 5\u00d75 Gridworld environment. In <Ref>, despite the fact that the number of reward parameters is larger than what we expect the CKDE to successfully model, KD-BIRL is able to estimate a posterior whose mean is in the equivalence class of R^\u22c6. That is, the posterior mean and R^\u22c6 encourage the same behavior in an agent, which implies that the expert demonstrations are equally likely under both. However, there are states ([2,3], [4, 3] in <Ref>, Panel 2) in the 5\u00d75 Gridworld in which the mean estimated reward is notably incorrect, which indicates that the CKDE struggles to learn 25 independent reward parameters successfully. \n\n\n\n \u00a7.\u00a7 Feature-based rewards\n\nWe now study three methods of reward function featurization that can enable KD-BIRL to perform reward inference in environments with large state spaces. \n\n\n  \u00a7.\u00a7.\u00a7 Using known features in a 10x10 Gridworld\n\nAs discussed in <Ref>, without using feature-based rewards, the original KD-BIRL algorithm would not be able to perform inference in the 10\u00d710 Gridworld because the reward vector length (100) is too high. In the 10\u00d710 Gridworld, the MDP is identical to the earlier Gridworld settings, except the state space is the series of one-hot encoded vectors of length 100. In this setting, we select \u03d5(s) = [x, y] to be a simple function that ignores the action and maps the state vector of length 100 to the spatial coordinates of the agent. In this way, we treat the coordinates of the agent as a \u201cfeature vector\u201d. Then, we choose weights w^\u22c6 such that R^\u22c6 is a linear combination of the features and w^\u22c6. <Ref> visualizes the resulting posterior distributions for two choices of w^\u22c6. We use a Normal prior for p(w) with mean 0 and variance 1 for w^\u22c6=[-1, 1], and a Normal prior with mean 0.5 and variance 0.5 for w^\u22c6=[1, 1]. We find that KD-BIRL accurately recovers the relative magnitude and sign of the individual components of w^\u22c6 for both chosen reward functions. \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Manually curated features in a sepsis treatment environment\n\n\nNow we perform inference in a sepsis treatment simulator based on de-identified data from MIMIC-III\u00a0<cit.>, a database of electronic health records (EHR). Sepsis arises when the body responds to infection in a way that is harmful to its own tissues and organs\u00a0<cit.>. In the original simulator\u00a0<cit.>, the state is a vector of length 46, where each element contains information about a given physiological covariate. There are 25 possible actions, each corresponding to a different combination of treatments. The transition function was learned using deep RL models\u00a0<cit.> (see Appendix Section 12 for details). \n\nSepsis treatment depends heavily on organ failure metrics, and fast increases in these metrics warrant urgent care\u00a0<cit.>. Since we have observed that in the Gridworld environment, KD-BIRL can successfully model a reward that is a function of a small state space, we choose \u03d5 to be a function that ignores the action and extracts three Sequential Organ Failure Assessment (SOFA)\u00a0<cit.> covariates present in the state: sofa, quick sofa, and quick sofa systolic blood pressure score. The result is a feature-based reward function with manually selected features based on prior knowledge. Our reward is now a linear combination of the difference between the state features at time t and t+1,\n\n    R(s_t) = [ a; b; c ]^\u22a4[  s(cov_1)_t-s(cov_1)_t+1;  s(cov_2)_t-s(cov_2)_t+1; s(cov_3)_t-s(cov_3)_t+1, ]\n\nwhere [a, b, c] are the weights, and s(cov_1), s(cov_2), s(cov_3) are the three organ failure features in state s. We choose the true (unobserved) weights to be [a=0.8, b=0.6, c=0.4]. We compare our method to AVRIL and avoid fitting BIRL due to computational constraints. Our results indicate that KD-BIRL generates reward samples with lower and more concentrated EVDs than the AVRIL trajectories (<Ref>). This indicates that KD-BIRL estimates a posterior distribution that is concentrated around the equivalence class of R^\u22c6.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Using a VAE to identify features in the sepsis environment\n\nFinally, we explore the use of a variational auto-encoder (VAE) to learn \u03d5 in the sepsis environment. More specifically, we use a VAE to learn a low-dimensional representation of state-action tuples, and aim to learn the set of weights that modifies this representation to form the reward function. To do this, we first learn \u03d5 on a set of state-action tuples independent of the training or expert demonstrations. The input dimension to the VAE is 47 (46 state features + 1 action), and the low dimensional representation has 3 features. The VAE uses 4 linear layers for the encoder and decoder, and optimizes for a downsampled representation with low reconstruction error using Adam. \n\nOnce \u03d5 is known, it can be used to generate the required datasets. To do this, we first select a set of weights w^\u22c6 for the expert demonstrations, and generate state-action tuples that optimize R(s,a) where R(s,a) = \u03d5(s,a) \u00d7 w^\u22c6. We repeat this procedure for several sets of uniformly selected weights w_0, \u22ef, w_c to generate the training dataset. Finally, we fit KD-BIRL and evaluate the learned weights using EVD as before. We report results in <Ref>, and find that across a variety of w^\u22c6 values, KD-BIRL's posterior samples generate comparable, if not much lower values than AVRIL. This, coupled with the additional theoretical guarantees, makes KD-BIRL a good choice for performing IRL in complex environments. \n\n\n\n\u00a7 DISCUSSION AND CONCLUSION\n\n\nIn this work, we present kernel density Bayesian inverse reinforcement learning (KD-BIRL), an efficient IRL algorithm that improves upon existing methods by estimating a posterior distribution on the reward function while avoiding Q-learning for every iteration of MCMC sampling, and by providing theoretical guarantees of posterior consistency. We show that KD-BIRL generates concentrated posteriors and is more computationally efficient than existing methods in a Gridworld environment. Additionally, we demonstrate that with a feature-based reward function, KD-BIRL can perform inference in a complex healthcare environment, and the resulting posterior outperforms a leading method. Taken together, our results suggest that, in complex environments, KD-BIRL can enable an accurate probabilistic description of clinician objectives that is not possible with current methods.\n\nSeveral future directions remain. This work is best-suited for on-policy (i.e., simulation) environments, and additional work is necessary to apply it directly to off-policy environments such as retrospective clinical decision-making settings. In particular, we would need behavior demonstrations from multiple agents in order to define a training dataset. Thus, it will be necessary to be able to associate actions with an agent (i.e., clinician). Additionally, the particular choices of distance metrics and hyperparameters used in the CKDE depend on the environment and reward function parameterization; additional experimentation is required to adapt this to different environments. Furthermore, a limitation of the conventional CKDE is that it performs poorly in high dimensions. One solution is to consider a modified version of the CKDE to speed it up\u00a0<cit.>. Another solution is to replace the CKDE with another nonparametric conditional density estimator\u00a0<cit.>. \n\nBecause KD-BIRL is a framework that estimates the likelihood as a conditional density, it can be easily modified to accommodate other choices for the CKDE. Finally, in this work, we discuss efforts to re-parameterize the reward function, and it is of interest to apply this work in additional environments with continuous or infinite state spaces, such as real-world EHR.\n\n \nWe thank Alex Chan for providing code associated with the AVRIL method. This work was funded by the Helmsley Trust grant AWD1006624, NIH NCI 5U2CCA233195, NIH NHLBI R01 HL133218, and NSF CAREER AWD1005627. BEE is on the SAB of Creyon Bio, Arrepath, and Freenome.  A. Mandyam was supported in part by a Stanford Engineering Fellowship. D. Cai was supported in part by a Google Ph.D. Fellowship in Machine Learning.\n\n\n\n\n\n\n\u00a7 CODE\n\n\nOur experiments were run on an internally-hosted cluster using a 320 NVIDIA P100 GPU whose processor core has 16 GB of memory hosted. Our experiments used a total of approximately 200 hours of compute time. Our code uses the MIT License and is available at https://github.com/bee-hive/kdbirlhttps://github.com/bee-hive/kdbirl.\n\n\n\n\u00a7 RATIONALITY OF BAYESIAN IRL\n\n\nThe original Bayesian IRL algorithm uses a Q-value function as a component of the likelihood calculation. \nWhile this can be sufficient for imitation learning, it does not allow for posterior predictive sampling, and updates to the Gibbs posterior are not rational unless the Q-value function satisfies certain conditions\u00a0<cit.> (see <Ref>). Intuitively, rational updates imply that new evidence is appropriately incorporated to modify the posterior from the prior. The Q-value function can be approximated using any real-value prediction method from neural networks to linear regression, and the rationality of the resulting posterior updates is not discussed in prior work. A rational posterior update implies that if the updating function, in this case, Q^\u22c6, has a lower value, the posterior probabilities should be lower, and vice versa. However, there is no way to confirm that Q^\u22c6 satisfies Assumption 3 of the guidelines for updating belief distributions in <cit.>.  Furthermore, a Q-value function is a poor approximation of density or likelihood. A higher Q-value associated with a state means that the Q-learning algorithm finds that this state is more likely to generate a reward. However, this does not imply that an agent is more likely to visit that state. Bayesian IRL formulations that use a Q-value function equate these two phenomena, and this can be incorrect depending on (1) the optimality of such an agent, (2) the quality of the Q-learning estimator, and (3) whether the agent has even visited the state. \n\n\n\n\u00a7 RATIONALITY CONDITIONS FOR POSTERIOR DISTRIBUTION UPDATES\n\n\n\u00a0<cit.> make several assumptions before guaranteeing rational updates to a posterior distribution. These are:\n\n  \n    \u03c8[l(\u03b8, x_2), \u03c8{l(\u03b8, x_1), \u03c0(\u03b8)}] \u2261\u03c8{l(\u03b8, x_1) + l(\u03b8, x_2), \u03c0(\u03b8)}\n\n  \n  where \u03b8 is the parameter of interest, \u03c0(\u03b8) is the prior distribution on \u03b8, l is a loss function, \u03c8 is the update function, and x_1, x_2 are data points. \n\n\n\nFor any set A \u2282\u0398, \n\n    \u03c8{l(\u03b8, x), \u03c0(\u03b8)}/\u222b_A \u03c8{l(\u03b8, x), \u03c0(\u03b8)} d\u03b8 = \u03c8{l(\u03b8, x), \u03c0_A(\u03b8)}\n\nwhere \u03c0_A is \u03c0 normalized to A. \n\n\nLower evidence for a state should yield smaller posterior probabilities under the same prior. So, if for some A\u2282\u0398, l(\u03b8, x) > l(\u03b8, y) for \u03b8\u2208 A \u2282\u0398 and l(\u03b8, x) = l(\u03b8, y) for \u03b8\u2208 A^c, then \n\n    \u222b_A \u03c8{l(\u03b8, x), \u03c0(\u03b8)} d\u03b8 < \u222b_A \u03c8{l(\u03b8, y), \u03c0(\u03b8)}d\u03b8.\n\n\n\nIf l(\u03b8, x) \u2261 constant, then \u03c8{l(\u03b8, x), \u03c0(\u03b8)}\u2261\u03c0(\u03b8).\n\n\nIf l\u0303(\u03b8, x) = l(\u03b8, x) + c for some constant c, then \n\n    \u03c8{l\u0303(\u03b8, x), \u03c0(\u03b8)} = \u03c8{l(\u03b8, x), \u03c0(\u03b8)}.\n\n\nIf \u03c8 is a Q-value function, which can be parameterized by anything from a linear model to a deep neural network, the rationality of the subsequent posterior updates is not discussed in previous work. In particular, it is not possible to verify these assumptions for a Q-value function. \n\n\n\n\u00a7 PROOFS FOR LEMMA 4.1, THEOREM 4.2\n\n\n\nThe proof associated with Lemma 4.1 follows. \n\nWe now use the continuity of the likelihood, the finite sample analysis of multivariate kernel density estimators in <cit.>[Section 4.4, Equation 4.16](<Ref>), which defines the Mean Integrated Square Error (MISE) of the density function, and Theorem 1(<Ref>) of  <cit.>[Section 2.6-2.9], which asserts that as the sample size increases, the mean of the density estimator converges and variance prevents the mean from exploding. We can use Theorem 1 because we assume that the density function is square-integrable and twice differentiable and that the bandwidth approaches 0 as the dataset size increases. Then, up to a constant, for a given state-action pair (s, a),\n\n    1/m\u2211_j=1^m e^-d_s((s,a),(s_j,a_j))^2/(2h)e^-d_r(R,R_j)^2/(2h')p(s,a,R).\n\nThe same holds true for d_r, 1/m\u2211_\u2113=1^m e^-d_r(R,R_\u2113)^2/(2h')p(R). By the Continuous Mapping Theorem\u00a0<cit.>, we conclude that \n\n    p_m(s,a|R)p(s,a,R)/p(R)=p(s,a|R).\n\n\n\n\n\n\nThe proof associated with Theorem 4.2 follows. \n\n\nBy Lemma 4.1, as m\u2192\u221e, p_m converges to the true likelihood, so we can adopt existing tools from Bayesian asymptotic theory.\n\nWe first define an equivalence relation on \u211b, denoted by \u2243: \n\n    R_1\u2243 R_2\u00a0iff\u00a0p(\u00b7|R_1)=p(\u00b7|R_2),\u00a0a.e.\n\nNote that \u2243 satisfies reflexivity, symmetry, and transitivity and is, therefore an equivalence relation. We denote the equivalence class by [\u00b7], that is, [R]={R':R'\u2243 R}, and the quotient space is defined as \u211b\u211b / \u2243 = {[R]:R\u2208\u211b}. The corresponding canonical projection is denoted by : \u211b\u2192\u211b,\u00a0 R\u21a6[R]. Then, the projection  induces a prior distribution on \u211b denoted by \u03a0: \u03a0(A)\u03a0(^-1(A)). Moreover, \u211b admits a metric d:\n\n    d([R_1],[R_2])p(\u00b7|R_1)-p(\u00b7|R_2)_L^1.\n\nBecause this metric uses the L^1 norm, it satisfies symmetry and triangular inequality. Additionally, it is true that\n\n    d([R_1],[R_2])=0\u27fa p(\u00b7|R_1)=p(\u00b7|R_2),\u00a0a.e.\u00a0\u27f8 R_1\u2243 R_2\u27fa [R_1]=[R_2],\n so d fulfills the identity of indiscernibles principle.\nAs a result, d is a valid distance metric on \u211b.\n\nThen consider the following Bayesian model: \n\n    (s,a)|[R]\u2243 p(s,a|[R]),\u00a0[R]\u2208\u211b,\u00a0[R]\u2243\u03a0.\n \nThis model is well-defined since p(s,a|[R]) is independent of the representative of [R] by the definition of the equivalence class. Observe that KL(R,R^*)=KL([R],[R^*]) by the definition of the equivalence class. Then, let A={[R]:KL([R],[R^*])<\u03f5}\u2282\u211b. We can define ^-1(A) = {R\u2208\u211b:KL(R,R^*)<\u03f5}\u2282\u211b. As a result, \u03a0({[R]:KL([R],[R^*])<\u03f5})=\u03a0({R:KL(R,R^*)<\u03f5})>0 for any \u03f5>0, that is, the KL support condition is satisfied . Moreover, the mapping [R]\u2192 p(\u00b7|R) is one-to-one. Because the Bayesian model is parameterized by [R] and we assume that \u211b is a compact set, by \u00a0<cit.>[Lemma 10.6](<Ref>) there exist consistent tests as required in Schwartz's Theorem\n\n. Then, by <cit.>(<Ref>), the posterior \u03a0_n on \u211b is consistent. That is, for any \u03f5>0, \u03a0^n_m({[R]:d([R],[R^*])<\u03f5}) 1. Put in terms of the original parameter space,\n\n    \u03a0_m^n({R:p(\u00b7|R)-p(\u00b7|R^\u22c6)_L_1<\u03f5)})=\u03a0^n_m({[R]:d([R],[R^*])<\u03f5}),\u00a0\u2200\u03f5>0.\n\n\n\n\n\n\u00a7 ASYMPTOTIC EXPANSION OF THE MEAN INTEGRATED SQUARED ERROR THEOREM 1\n\n\n\n(i) The integrated squared bias of the kernel density estimator can be expanded as\n\n    ISB{f\u0302(\u00b7;H)} = 1/4 c_2(K)^2vec^\u22a4R(D^\u2297 2f)(vec H)^\u2297 2 + o(||vec H||^2).\n\n\n(ii) The integrated variance of the kernel density estimator can be expanded as \n\n    IV{f\u0302(\u00b7;H)} = m^-1 |H|^-1/2 R(K) + o(m^-1|H|^-1/2).\n\n\n\nHere, f\u0302 is the estimated density function, H is a matrix of bandwidth values, c_2(K) = \u222b_\u211b^d z_i^2 K(z) dz for all i=1, \u2026, d is the variance of the kernel function K, and m is the size of the training dataset. In our work, H is a diagonal matrix where every element on the diagonal is the same bandwidth h_m. In this work, we assume that f is square-integrable and twice differentiable and that the bandwidth matrix H\u2192 0 is m \u2192\u221e. Because we use a Gaussian kernel for K, we know that it is square integrable, spherically symmetric, and has a finite second-order moment. \n\n\n\n\u00a7 WAND AND JONES, EQUATION 4.16\n\n\nThe mean integrated squared error\u00a0(MISE) of a multivariate kernel density estimator is defined as:\n\n    MISE{f\u0302(\u00b7;H)} = n^-1 (4\u03c0)^-d/2 |H|^-1/2 + w^\u22a4{(1-n^-1)\u03a9_2 - 2\u03a9_1 + \u03a9_0}w\n\nwhere H is a matrix of bandwidth values, n is the size of the dataset, \u03a9_a denotes the k\u00d7 k matrix with (l, l') entry equal to \u03d5_a H + \u03a3_l + \u03a3_l' (\u03bc_l - \u03bc_l'), \u03d5_d is a d-variate Normal kernel, w=(w_1, \u2026, w_k)^\u22a4 is a vector of positive numbers summing to 1, and for each l=1, \u2026, k, \u03bc_l is a d \u00d7 1 vector and \u03a3_l is a d\u00d7 d covariance matrix.  \n\n\n\n\u00a7 SCHWARTZ'S THEOREM\n\n\nIf p_0 \u2208 KL(\u03a0) and for every neighborhood \ud835\udcb0 of p_0 there exist tests \u03d5_n such that P_0^n\u03d5_n \u2192 0 and sup_p \u2208\ud835\udcb0^c P^n(1 - \u03d5_n) \u2192 0, then the posterior distribution \u03a0_n(\u00b7| X_1, \u2026, X_n) in the model X_1, \u2026, X_n | p \u223c^iid p and p \u223c\u03a0 is strongly consistent at p_0. \n\n\n\n\u00a7 ASYMPTOTIC STATISTICS, LEMMA 10.6\n\n\n\nSuppose that \u0398 is \u03c3-compact, P_\u03b8\u2260 P_\u03b8' for every pair \u03b8\u2260\u03b8', and the maps \u03b8\u2192 P_\u03b8 are continuous for the total variation norm. Then there exists a sequence of estimators that is uniformly consistent on every compact subset of \u0398.\n\nHere, \u0398 is the space of parameters, P is the probability density function, and \u03b8\u2208\u0398 is a parameter.\n\n\n\n\u00a7 KD-BIRL ALGORITHM\n\n\n<Ref> is the general version of the KD-BIRL algorithm where the expert demonstrations and training dataset are already available. A version of the algorithm that is suited for simulated datasets which require dataset generation can be seen in <Ref>.\n\n\n\n\n\n\u00a7 CALCULATING EXPECTED VALUE DIFFERENCE (EVD)\n\n\nThe procedure to calculate EVD varies depending on the method. For all methods, this process requires a set of reward samples. Because KD-BIRL and BIRL both use MCMC sampling, we can use the reward samples generated from each iteration. AVRIL does not use MCMC, so we have to modify the approach to generating samples depending on the structure of the reward function. When the reward function is a vector with length equal to the cardinality of the state space, we use the AVRIL agent to estimate the variational mean and standard deviation of reward at each state in the environment. Using these statistics, we then assume that the reward samples arise from a multivariate normal distribution and use numpy to generate samples according to the mean and standard deviation information collected. Once we have samples, we can then calculate EVD and 95% confidence intervals. Recall that EVD is defined as |V^*(r^A) - V^\u03c0^*(r^L)(r^A)| where r^A is the ground truth known reward and r^L is the learned reward. For a given method, we calculate 95% confidence intervals across the EVD for each of the reward samples. Given a reward sample, we can calculate EVD, where r^L is the sample, and r^A is the known reward. To determine the value of the policy optimizing for a particular reward function, we train an optimal agent for that reward function, and generate demonstrations characterizing its behavior; the value is then the average reward received across those demonstrations. Finally, we calculate the difference between the value of the policy for r^A and r^L, and report confidence intervals across these values for all samples for a given method. \n\nIn the Sepsis environment, we use two methods of reward function featurization. In both, because the reward function does not consider individual discretized states, we cannot use the earlier approach to calculate EVD for AVRIL. To generate EVDs, we used the trained AVRIL agent to generate trajectories using agent-recommended actions starting from an initial state; the EVD here is the difference between the total value of these trajectories and the total value of trajectories generated using the correct reward function (independent of AVRIL).  \n\n\n\n\n\u00a7 CHOOSING BANDWIDTH HYPERPARAMETERS FOR KD-BIRL\n\n\nNow, we investigate the effect of the bandwidth hyperparameters h and h' on KD-BIRL's posterior distribution for the reward function R^\u22c6= [0, 0, 0, 1], marginalized at state [1, 1] (<Ref>). Recall that h and h' correspond to the bandwidth for the state-action pairs and reward functions, respectively. Note that the density of reward samples varies more with h' than h, but it is important to tune both these hyperparameters because the resulting posterior distributions can change substantially.\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 SEPSIS ENVIRONMENT\n\n\nThe Sepsis environment is a simulation setting that models Sepsis treatment. There are 48 state features(<Ref>) in the original environment, comprised of 46 physiological covariates and an action and state index. \n\n\n\n \n\n Feature     Description\n \n Albumin     Measured value of Albumin, a protein made by the liver\n \n Anion Gap     Measured difference between the negatively and positively charged electrolytes in blood\n\n Bands     Measuring band neutrophil concentration \n\n Bicarbonate     Measured arterial blood gas \n\n Bilirubin     Measured bilirubin \n\n BUN     Measured Blood Urea Nitrogen \n\n Chloride     Measured chloride \n\n Creatinine     Measured Creatinine \n\n DiasBP     Diastolic blood pressure \n\n Glucose     Administered glucose \n\n Glucose     Measured glucose \n\n Heart Rate     Measured Heart Rate \n\n Hematocrit     Measure of the proportion of red blood cells \n\n Hemoglobin     Measured hemoglobin \n\n INR     International normalized ratio \n\n Lactate     Measured lactate \n\n MeanBP     Mean Blood Pressure \n\n PaCO2     Partial pressure of Carbon Dioxide \n\n Platelet     Measured platelet count \n\n Potassium     Measured potassium \n\n PT     Prothrombin time \n\n RespRate     Respiratory rate \n\n Sodium     Measured sodium \n\n SpO2     Measured oxygen saturation \n\n SysBP     Measured systolic blood pressure \n\n TempC     Temperature in degrees Celsius \n\n WBC     White blood cell count \n\n age     Age in years  \n\n is male     Gender, true or false \n\n race     Ethnicity (white, black, hispanic or other) \n\n height     Height in inches \n\n Weight     Weight in kgs \n\n Vent     Patient is on ventilator \n\n SOFA     Sepsis related organ failure score \n\n LODS     Logistic organ disfunction score \n\n SIRS     Systemic inflammatory response syndrome \n\n qSOFA     Quick SOFA score \n\n qSOFA Sysbp Score     Quick SOFA that incorporates systolic blood pressure measurement \n\n qSOFA GCS Score     Quick SOFA incorporating Glasgow Coma Scale \n\n qSofa Respirate Score     Quick SOFA incorporating respiratory rate \n\n Elixhauser hospital     Hospital uses Elixhauser comorbidity software \n\n Blood culture positive     Bacteria is present in the blood \n\n \n \n\n\nThe original Sepsis environment uses the features described in <Ref> to represent the patient state. In the first method of featurization we consider in our experiments, we consider only three features that directly affect the reward received in a given state. These are SOFA, qSOFA, and qSOFA Sysbp Score. Intuitively, these can be thought of as features of the state that are most relevant to the reward function. In the second method of featurization, we consider all of the state features as well as the action chosen, and use a variational auto-encoder to generate a low dimensional representation. \n"}