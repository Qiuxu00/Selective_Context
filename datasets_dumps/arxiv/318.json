{"entry_id": "http://arxiv.org/abs/2303.07354v1", "published": "20230313063938", "title": "MetaTroll: Few-shot Detection of State-Sponsored Trolls with Transformer Adapters", "authors": ["Lin Tian", "Xiuzhen Zhang", "Jey Han Lau"], "primary_category": "cs.CL", "categories": ["cs.CL"], "text": "\n\n\n\n\n\n\n\nState-sponsored trolls are the main actors of influence campaigns on social media and automatic troll detection is important to combat misinformation at scale. \nExisting troll detection models are developed based on training data for known campaigns (e.g. the influence campaign by Russia's Internet Research Agency on the 2016 US Election), and they fall short when dealing with novel campaigns with new targets. \n\nWe propose MetaTroll, a text-based troll detection model based on the meta-learning framework that enables high portability and parameter-efficient adaptation to new campaigns using only a handful of labelled samples for few-shot transfer. We introduce campaign-specific transformer adapters to MetaTroll to \u201cmemorise\u201d campaign-specific knowledge so as to tackle catastrophic forgetting, where a model \u201cforgets\u201d how to detect trolls from older campaigns due to continual adaptation. \n\n\nOur experiments demonstrate that MetaTroll substantially outperforms baselines and state-of-the-art few-shot text classification models. Lastly, we explore simple approaches to extend MetaTroll to \n\nmultilingual and multimodal detection. Source code for MetaTroll is available at: https://github.com/ltian678/metatroll-code.git\n\n\n\n\n<ccs2012>\n   <concept>\n       <concept_id>10010147.10010178.10010179</concept_id>\n       <concept_desc>Computing methodologies\u00a0Natural language processing</concept_desc>\n       <concept_significance>500</concept_significance>\n       </concept>\n   <concept>\n       <concept_id>10010147.10010178.10010179.10003352</concept_id>\n       <concept_desc>Computing methodologies\u00a0Information extraction</concept_desc>\n       <concept_significance>300</concept_significance>\n       </concept>\n </ccs2012>\n\n\n[500]Computing methodologies\u00a0Natural language processing\n[300]Computing methodologies\u00a0Information extraction\n\n\n\nDirect tomography of quantum states and processes via weak\nmeasurements of Pauli spin operators on an NMR quantum processor\n    Arvind\n    March 30, 2023\n===========================================================================================================================\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nState-sponsored trolls, along with bots, are the main actors in influence operations and misinformation campaigns on social media.\nThe 2016 US Presidential election for example, highlighted how foreign states can carry out mass influence campaign, and in this case by Russia's Internet Research Agency <cit.>.[<https://www.adelaide.edu.au/newsroom/news/list/2021/12/09/understanding-mass-influence-activities-is-critical>.]\n\nIn another context, <cit.> studied how influence operations undermined the vaccine debate in public health. \n\nTo help combat misinformation on social media, Twitter began releasing user accounts associated with state-sponsored influence activities. Our work uses this dataset for troll detection, and as such a troll in our context is a state-sponsored agent with links to an influence campaign.\n\n\n\n\n\n\n\n\n\n\nExisting troll detection models focus on extracting signals from user online posts and user activities <cit.>.\n\n\n\nRecent neural approaches explore fusing different feature representations for troll detection, e.g. social structure, source posts and propagated posts such as retweets on Twitter\u00a0<cit.>.\nThese studies generally formulate the task as a standard supervised learning problem which requires a substantial amount of labelled data for known campaigns. \nAs such, they are ill-equipped to detect  novel campaigns sponsored by another state for a different target.\n\n\nMeta-learning is a well-established framework for few-shot learning\u00a0<cit.>.\nThe idea of meta-learning is to leverage the shared knowledge from previous tasks to facilitate the learning of new tasks.\nIn our context, meta-learning has the potential to quickly adapt a troll detection model to a new campaign via few-shot transfer, once it's meta-trained on a set of known campaigns. However, in a continual learning setting where the troll detection model needs to be constantly updated to work with new campaigns over time, the standard meta-learning framework suffers from catastrophic forgetting \u2014 the problem where the model \u201cforgets\u201d how to detect trolls from the older campaigns as they are updated <cit.>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe main contributions of this work are as follows:\n\n\n\n  * We introduce a troll detection problem under a realistic setting where novel campaigns continually emerge, mimicking real-world events on social media platforms. \n\n  * We propose MetaTroll, a text-based meta-learning framework for troll detection, which includes a three-stage meta-training process that allows it to learn knowledge across different  campaigns for fast adaptation to new campaigns. \n\n  * MetaTroll tackles catastrophic forgetting by introducing campaign-specific transformer adapters <cit.> (i.e. each campaign has its own set of adapter parameters), \nwhich can \u201cmemorise\u201d campaign-specific knowledge.\n\n  * MetaTroll has the ability to to work with multiple languages (multilingual) by using a pretrained multilingual model and incorporate images  as an additional input (multimodal) by either encoding them using pretrained image classifiers or converting them into text via optical character recognition.\n\n  * Large-scale experiments on a real-world dataset of 14 Twitter campaigns showed the superior performance of MetaTroll.\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\nOur related work comes from three areas, troll detection, meta-learning and few-shot text classification. \n\n\n\n \u00a7.\u00a7 Troll detection\n\nEarly studies of troll detection focus on extracting hand-engineered features from the textual contents of user posts for troll detection\u00a0<cit.>. \nSignals such as writing style, sentiment as well as emotions have been explored\u00a0<cit.>.\nUser online activities have also been used to detect trolls\u00a0<cit.>.\n<cit.> presents a study on anti-social behavior in online discussion communities, focusing on users that will eventually be banned.\n\n\n\nMore recently, approaches that combine user posts and online activities for troll detection have emerged\u00a0<cit.>.\n<cit.> identify 49 linguistic markers of deception and measure their use by troll accounts. They show that such deceptive language cues can help to accurately identify trolls.  <cit.> propose a detection approach that relies on users' metadata, activity (e.g. number of shared links, retweets, mentions, etc.), and linguistic features to identify active trolls on Twitter.\n<cit.> show that Russian trolls aim to hijack the political conversation to create distrust among different groups in the community.\n\n<cit.> similarly demonstrate how trolls act to accentuate disagreement and sow division along divergent frames, and this is further validated by <cit.> in relation to Russian ads on Facebook. \nIn <cit.>, the authors study the effects of manipulation campaigns by analysing the accounts that endorsed trolls' activity on Twitter.\nThey find that conservative-leaning users re-shared troll content 30 times more than liberal ones. \n<cit.> compare troll behavior with other (random) Twitter accounts by recognising the differences in the content they spread, the evolution of their accounts, and the strategies they adopted to increase their impact.\n<cit.> leverage both social structure and textual contents to learn user representations via graph embedding.\n\n\n\n\nWe focus on how to detect trolls in emergent social activities with limited labelled data.\n\n\n\n \u00a7.\u00a7 Meta-learning\n\nMeta-learning  aims to extract some transferable knowledge from a set of tasks to quickly adapt to a new task.\nThese approaches can be divided into three categories: metric-based, optimisation-based, and model-based.\nMetric-based meta-learning approaches\u00a0<cit.> aim to learn an embedding function to encode the input and a metric function to learn the distance (e.g. cosine distance and euclidean distance) between the query data and support data.\n\nOptimisation-based approaches\u00a0<cit.> are designed to learn good parameter initialisation that can quickly adapt to new tasks within a few gradient descent steps.\nModel-based approaches\u00a0<cit.> use neural networks to embed task information and predict test examples conditioned on the task information.\n\nOur approach combines both optimisation and model-based ideas in that we adopt <cit.> to update model parameters and a novel architecture that involves adapters and adaptive classification layers to learn task information.\n\n\n\n\n\n \u00a7.\u00a7 Few-shot text classification\n\n\n\nText classification has shifted from task-specific training to pre-trained language models (such as BERT) followed by task-specific fine-tuning\u00a0<cit.>. \nRecently, the language model GPT-3\u00a0<cit.> has shown strong few-shot performance for many natural language processing tasks.\nFew-shot text classification refer to a text classification setting where there are novel unseen tasks (domains) \nwith only a few labelled examples for training a classification model, and \nmeta-learning have been shown to produce strong performance \n\u00a0<cit.>. \n<cit.> introduce distributional signatures, such as word frequency and information entropy, into a meta-learning framework.\n<cit.> combine attention mechanism with prototypical network to solve noisy few-shot relation classification task. \n<cit.> leverage the dynamic routing algorithm in meta-learning to solve sentiment and intent text classification tasks for English and Chinese datasets.\n<cit.> propose an adaptive metric-based method that can determine the best weighted combination automatically.\n\n\nIn our problem setting, our focus is on adapting the classifier to a new task (i.e. campaign) under the meta-learning framework.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 PROBLEM STATEMENT\n\n\n\n\n\nWe first explain some preliminaries of meta-learning, and contextualise it under our problem setting, before describing our model in approach.\nLet \u2130 = {e_0, e_1, e_2,...,e_l} be a set of campaigns. These campaigns are split into two partitions: \n\u2130_train for meta-training and \u2130_test for meta-testing (in our case, we have 6 campaigns for \u2130_train, and 4 for \u2130_test; see Table\u00a0<ref>). \nFor each  campaign e, we have labeled users \ud835\udcb0, where each user \ud835\udc2e consists of a list of text posts and images, defined as \ud835\udc2e = {(c_0, m_0, p_0), ..., (c_n, m_n, p_n)}, where c refers to the textual content of the post,  m a set of images, and p the timestamp of the post.\n\n\nEach user \ud835\udc2e is associated with a ground-truth label y \u2208 Y, where Y represents the label set (troll or non-troll; binary). \nOur goal is to leverage the knowledge learned from past (meta-train) campaigns (\u2130_train) to adapt quickly to new (meta-test) campaigns (\u2130_test)  via few-shot learning (e.g. 5 or 10 users for the new campaign).\n\n\n\nIn our problem setting, the notion of a task (from meta-learning) is the binary troll detection task for a particular campaign.\n\n\n\nEach task \ud835\udcaf_i =  ( \ud835\udc9f^s, \ud835\udc9f^q  ) includes a support set \ud835\udc9f^s with S data points and a query set \ud835\udc9f^q with Q data points, where \ud835\udc9f^s = {\ud835\udc2e_i^s, y_i^s}_i=1^S , \ud835\udc9f^q = {\ud835\udc2e_i^q, y_i^q}_i=1^Q, and \ud835\udc2e_i is a user with a list of posts and images and y_i\u2208{ 0,1 } corresponding to troll/non-troll labels (as defined above).\nDuring meta-training, we  iterate through different campaign in \u2130_train and sample support and query instances to train the model. Once that's done, to adapt to a new campaigns in \u2130_test, we follow a similar training process but we update the model parameters using only the support instances from the new campaign, and reserve the query set for evaluation (i.e. the query set is not used for model update).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\tMetaTroll\n\t\n\t\n\n\t Input/hyper-parameters: meta-train  campaigns \u2130_train, meta-test  campaigns \u2130_test, learning rate \u03b2, \u03b3 and \u03b4, model M \n\n\t Parameters:\n\tcampaign-general adapter \u03a8, campaign-specific adapter  \u03a8_e, BERT \u03a6, linear classifier \u03a9, learning rate \u03b1\n\n\t\n\t\n\t\n\t\n    \n\n    \n\t\n\t \n \n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 APPROACH\n\n\n\n\n\n\n\n\n\nWe present the overall architecture of MetaTroll in Figure\u00a0<ref>. MetaTroll has two modules, a BERT-based \u00a0<cit.> feature extractor and an adaptive linear classifier. \n\nWe first explain at a high level how we train MetaTroll in three stages for the feature extractor and linear classifier (Algorithm\u00a0<ref>).\nIn the first stage, we fine-tune an off-the-shelf BERT and update all its parameters (\u03a6) \n\nusing all training samples from the meta-train campaigns \u2130_train (line 1 in Algorithm\u00a0<ref>). At this stage we do not introduce adapters <cit.> to BERT and it is optimised to do binary classification of trolls and non-trolls. The idea of this step is to train a feature extractor for the troll detection task (in other words, this step is standard fine-tuning to adapt BERT for binary troll detection).\n\n\n\n\n\n\nIn the second stage, we introduce adapters <cit.> to BERT and train the adapter parameters (\u03a8) using model agnostic\nmeta-learning (MAML: <cit.>; line 3\u201316 in Algorithm\u00a0<ref>). Note that the adapter is shared across all campaigns (i.e. the adapter is not campaign-specific), and the idea of this stage is to learn a good initialisation for the adapter to do general troll detection; in the next stage, the learned adapter parameters will be used to initalise campaign-specific adapters.\n\n\n\n\n\n\n\nDuring this stage, only the adapter parameters (\u03a8) are updated while the BERT parameters (\u03a6) are frozen. \n\n\n\n\nIn the third and final stage, we introduce campaign-specific adapters \nand adaptive linear classifiers to MetaTroll, creating the full model. The campaign-specific adapters are initialised using the campaign-general adapter from stage 2. The idea of using campaign-specific adapters is to address catastrophic forgetting when MetaTroll is continuously updated for new campaigns that emerge over time in an application setting: the campaign-specific adapter solves the `forgetting' problem because the knowledge of detecting older/past campaigns are stored in their adapters which will not be overwritten as MetaTroll is continually updated. As for the adaptive linear classifiers, they are also campaign-specific and designed to encourage MetaTroll to learn campaign representations that are distinct for different campaigns.[The adaptive linear classifier parameters are initialised randomly for a new campaign.]\nWe update the campaign-specific adapter  (\u03a8_e) and classifier parameters (\u03a9_e) via meta-learning, similar to stage 2 training (line 17\u201332 in  Algorithm\u00a0<ref>), noting that the BERT parameters (\u03a6) are frozen in this stage.\n\n\nAfter MetaTroll is trained (over the 3 stages), to adapt it to trolls of a new campaign at test time, we follow the third stage training to learn campaign-specific adapter (\u03a8_e) and classifier (\u03a9_e) parameters for the new campaign. Once adapted, MetaTroll can be used to classify users from this new campaign. We next describe the training stages and test inference in detail.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Stage One Training\n\n\n\n\n\nIn the first stage, MetaTroll is a standard BERT fine-tuned with standard cross entropy loss to do binary classification of trolls vs. non-trolls (ignoring the campaigns). Given a user \ud835\udc2e with posts {c_0,...,c_n}: \n\n    v    = BERT([CLS] \u2295 c_0 \u2295 ... \u2295 c_n) \n    \u0177   = softmax (W v + b)\n\nwhere v is the contextual embedding of [CLS] and \u2295 is the concatenation operation. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Stage Two Training\n\n\n\nIn the second stage, we add adapters to BERT (see Figure\u00a0<ref>).\nFollowing\u00a0<cit.>, we insert two adapter modules containing bottleneck layers into each transformer layer of BERT.\nNote that the adapter is shared across all campaigns here, as the goal in this stage is to learn good set of initial adapter parameter values that can be used to initialise campaign-specific adapters in the next stage.\n\n\nCorrespondingly, Equation <ref> is now modified to:\n\n    v = AdapterBERT([CLS] \u2295 c_0 \u2295 ... \u2295 c_n)\n\n\n\nWe meta-train the adapter parameters using MAML (line 3\u201316 in Algorithm\u00a0<ref>).\nWe first sample a task \ud835\udcaf_i for campaign e from  \u2130_train to create the support D^s and query set  D^q.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDenoting M as the model, \u03a6 the BERT parameters and \u03a8 the adapter parameters, we next compute the inner loop to update \u03a8 as follows <cit.>:\n\n    \u2112_\ud835\udcaf_i(M_\u03a6,\u03a8, \ud835\udc9f^s)    = 1/|\ud835\udc9f^s|\u2211_\ud835\udc2e_i \u2208\ud835\udc9f^s-log p(y_i|\ud835\udc2e_i ; \u03a6, \u03a8) \n    \u03a8^'   = \u03a8 - \u03b1\u2207_\u03a8\u2112(M_\u03a6,\u03a8, \ud835\udc9f^s)\n\nwhere \u03b1 is the learning rate (computed next in the outer loop)\n\nand \u2112_\ud835\udcaf_i(M_\u03a6,\u03a8, \ud835\udc9f^s) is the cross entropy loss over the support set. Next we compute the cross-entropy loss based on the query set, using the updated parameters:\n\n    \u2112_\ud835\udcaf_i(M_\u03a6,\u03a8^', \ud835\udc9f^q) = 1/|\ud835\udc9f^q|\u2211_\ud835\udc2e_i \u2208\ud835\udc9f^q-log p(y_i|\ud835\udc2e_i ; \u03a6, \u03a8^')\n\n\nThis inner loop is carried out for multiple steps of gradient descent (using several tasks from the same campaign). Note that in this stage  \u03a6 (BERT parameters) is frozen. Once that's done, we update the adapter parameters \u03a8 and inner loop learning rate \u03b1:[In our implementation, \u03b1 is layer-specific, i.e. we have a separate learning rate for each adapter in different layers.]\n\n    \u03a8\u2190\u03a8-\u03b2\u2207_\u03a8\ud835\udd3c_\ud835\udcaf_i \u223c\ud835\udcaf[\u2112_\ud835\udcaf_i(M_\u03a6,\u03a8^', \ud835\udc9f^q)]\n            \n    \u03b1\u2190\u03a8-\u03b2\u2207_\u03b1\ud835\udd3c_\ud835\udcaf_i \u223c\ud835\udcaf[\u2112_\ud835\udcaf_i(M_\u03a6,\u03a8^', \ud835\udc9f^q)]\n\nwhere \u03b2 is the learning rate for the outer loop (set to 1e^-5 in our experiments).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Stage Three Training\n\n\n\nIn this stage, we introduce campaign-specific adapters for each campaign, and they are all initialised using the campaign-general adapter learned from the previous stage. Formally, Equation <ref> is now updated to:\n\n    v = AdapterBERT_e([CLS] \u2295 c_0 \u2295 ... \u2295 c_n)\n\nwhere e is the campaign.\n\nWe introduce campaign-specific adapters to our model for two reasons: (1) they  are  more efficient to train  and less vulnerable to overfitting (which is important in a few-shot learning setting), since they contain only a small number of parameters compared to the alternative where we have one BERT model for every campaign); and (2)  they alleviate catastrophic forgetting in a continual learning setting, as each campaign has its own adapter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInspired by <cit.>, we next introduce an adaptive linear classifier network that replaces W and b in the linear layer used for classification (Equation <ref>). Intuitively, this adaptive classifier works by first computing the aggregate troll and non-troll representation for each campaign, and then learn campaign-specific projections to classify between trolls vs. non-trolls.\n\n\nLet \ud835\udc9f_0^s and \ud835\udc9f_1^s denote the support set where the labels are trolls (y=0) and non-trolls (y=1) respectively, we compute W_e, b_e and \u0177 for troll campaign e as follows:\n\n    W_e^0   = 1/|\ud835\udc9f_0^s|\u2211_v_ \u2208\ud835\udc9f_0^s( v )    \n    \tb_e^0 = 1/|\ud835\udc9f_0^s |\u2211_v_ \u2208\ud835\udc9f_0^s( v ) \n    \n    \tW_e^1   = 1/|\ud835\udc9f_1^s|\u2211_v_ \u2208\ud835\udc9f_1^s( v )    \n    \tb_e^1 = 1/|\ud835\udc9f_1^s |\u2211_v_ \u2208\ud835\udc9f_1^s( v ) \n    \u0177   = softmax (W_e v + b_e)\n\nwhere W^i_e denotes the (i-1)^th column of W_e.\n\n\n\nIn other words, MetaTroll classifies a user based on whether its representation (v) is closer to the (average) troll or non-troll representation.\n\n\n\n\n\n\nThe campaign-specific adaptive linear classifier parameters \u03a9_e and the adapters parameters \u03a8_e are trained using MAML, just like stage 2 training (line 17\u201331 in  Algorithm\u00a0<ref>).\n\n\n\n \u00a7.\u00a7 Meta-testing\n\n\n\n\nAfter MetaTroll is trained, to adapt it to a new campaign e , we follow the process of the third stage training. To simulate few-shot learning, we sample only a small number of instances for the support and query set (e.g. 5 each), and use only the support set for updating the adapter  (\u03a8_e)  and classifier parameters (\u03a9_e) (line 25\u201326 in Algorithm\u00a0<ref>) and do not run the outer loop ((line 30\u201331). Here the query set is used only for computing performance (i.e. in line 27 we compute accuracy instead of loss over the query set).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTS AND RESULTS\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Datasets and models\n\n\n\n\n\nWe use the information operations dataset published by Twitter for our experiments.[<https://transparency.twitter.com/en/reports/information-operations.html>]\nThis dataset contains different groups of users banned by Twitter since October 2018 for engaging in state-sponsored information operations, and each group represents a campaign in our work.\n\nFor example, the \u201cIran-2018-Palestine\u201d campaign refers to trolls sponsored by Iran for an information campaign targeting Palestine in 2018.[<https://blog.twitter.com/en_us/topics/company/2018/enabling-further-research-of-information-operations-on-twitter>] To clarify, these campaigns are defined by Twitter when they release the data, and each campaign is associated with a blogpost that explains the information operation.\n\n\n\n\nFor each campaign, we filter users and keep only those who have posted a tweet within the 6-month event period (\u201cEvent Time\u201d in data) to remove users who are inactive during the event.[The event period is determined as follows: (1) the end date is the last post in the campaign; and the (2) start date is 6 months from the end date.] For each user, we also filter their tweets to keep only their most recent 20 posts that have a timestamp within the event period.\n\nTo create the non-troll users, we combine two sources: (1) \u201cRandom\u201d, random accounts that are sampled by generating random numeric user IDs and validating their existence following <cit.>; and (2) \u201cHashtag\u201d, users whose posts contain popular hashtags used by a campaign, where popular hashtags are defined as hashtags that collectively cover 75% of trolls' posts. The reason why we have two types of non-troll users is that if we only sample random users as non-trolls, the post content of non-trolls would be topically very different to that of the trolls, and the detection task would degenerate into a topic detection task. The non-troll users sampled using the \u201cHashtag\u201d approach is designed to circumvent this and makes the detection task more challenging.\n\n\n\nTable\u00a0<ref> present some statistics for trolls and non-trolls in different campaigns, where 6 are used for meta-training and 4 for meta-testing.\nNote that the non-trolls of a particular campaign are users sampled with the \u201cHashtag\u201d approach, and the last row corresponds to non-troll users sampled using the \u201cRandom\u201d approach.\nFor these campaigns, at least 80% of the trolls' posts are in English, and so they are used for the monolingual (English) experiments in the paper.[For non-troll users, we only keep English tweets (based on the predicted language given in the metadata).]\n\n\n\n\n\nThe trolls and non-trolls in Table\u00a0<ref> represent the pool of users which we draw from to construct the final training/testing data. In all of our experiments, we keep the ratio of trolls to non-trolls to 50/50 through sampling,\n\nand when sampling for non-troll users, the ratio from the two sources (\u201cRandom\u201d and \u201cHashtag\u201d) is also 50/50.[We also only sample \u201cRandom\u201d users whose most recent post is in the event time.] As an example,  \u201cUganda-2021-NRM\u201d has 334 troll users. We therefore sample 334 non-troll users, where 167 are from \u201cRandom\u201d and another 167 from \u201cHashtag\u201d.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe compare our MetaTroll model against the following baselines including state-of-the-art meta-learning methods and  few-shot text classification models:\n\n\n\n\n\n\n\n\n\n\n\n\n  * BERT\u00a0<cit.>[<https://huggingface.co/docs/transformers/model_doc/bert>]: BERT fine-tuned using the support set of meta-test data.\n\n  * KNN\u00a0<cit.>: K-nearest neighbour classifier with off-the-shelf BERT as the feature extractor.[K is selected from [5,10].]\n\n\n  * AdBERT\u00a0<cit.>[<https://adapterhub.ml/>]: BERT that fine-tunes an adapter for each campaign.\n\n  * GPT3\u00a0<cit.>[<https://gpt3demo.com/apps/openai-gpt-3-playground>]: a very large pretrained model adapted to our tasks using prompt-based learning <cit.>.[We use \u201ctext-davinci-002\u201d in our experiments. The prompts are a small set of instances (e.g. 5) with their labels added to the beginning of the input.]\n\n  * MAML\u00a0<cit.>[<https://github.com/tristandeleu/pytorch-meta/tree/master/examples/maml>]: BERT trained with the MAML algorithm for few-shot learning.\n\n  * CNP\u00a0<cit.>[<https://github.com/deepmind/neural-processes>]: a model-based meta-learning framework that consists of a shared encoder, aggregator and decoder.\n\n  * ProtoNet\u00a0<cit.>[<https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch>]: a deep metric-based approach using sample average as class prototypes and the distance is calculated based on euclidean-distance. \n\n  * Induct\u00a0<cit.>: a few-shot classification model that uses a dynamic routing algorithm to learn a class-wise representation.\n\n  * HATT\u00a0<cit.>[<https://github.com/thunlp/HATT-Proto>]: A classification model of metric-based meta-learning framework together with attention mechanism.\n\n  * DS\u00a0<cit.>[<https://github.com/YujiaBao/Distributional-Signatures>]: A few-shot text classification model that uses distributional signatures such as word frequency and information entropy for training.\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Results\n\n\n\n\nWe first present English troll detection performance. In Sections <ref> and <ref> we extend MetaTroll to work with non-English data and images. As we focus on English here, non-English posts are discarded (although this only reduces the data size by a negligible amount, as most data is in English, as explained in datasets).\n\n\n\nAll reported figures are an average accuracy performance over 5 runs with different random seeds.[Noting that the performance for one run is an average performance over the query sets from multiple tasks.\n\n]\n\n\nTable\u00a0<ref> presents 5-shot and 10-shot results for the 4 meta-test campaigns.[5-shot means only 5 labelled instances for each class are given in the support set.]\n\nMetaTroll is the best model, achieving an average of 76.35% accuracy over all campaigns. That said, some of the few-shot text classifiers (Induct e.g.73.57%) are not far behind.\n\n\n\n\nMost models only benefit marginally by seeing 5 more examples going from 5- to 10-shot, with the exception of GPT3 where we see a substantial performance boost (average 14.64% gain).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Continual learning performance\n\n\n\n\n\nIn continual learning\u00a0<cit.>, new tasks appear over time, \nwhere the goal of learning is to adapt the model accordingly to the new tasks without forgetting the previous tasks.\nThis is a more realistic setting for a troll detection system, as it should continually adapt to new campaigns that appear over time.\nBut in this setting it will suffer from catastrophic forgetting <cit.>, where after adapting to newer campaigns its performance to classify older campaigns will degrade.\n\nTo simulate this continual learning setting, we next evaluate the troll detection models on a past campaign after  \n\nit has been adapted for a number of campaigns in sequence. \n\n\nFor example, a system is first adapted to GRU-2020-NATO (G), and then to IRA-2020-Russia (I), Uganda-2021-NRM (U) and China-2021-Xinjiang (C) in sequence (denoted as G\u2192I\u2192U\u2192C). \nWe then test the system using trolls using the past campaigns, i.e. G, I and U.\n\n\n\n\n\n\n\nOne challenge with MetaTroll under this continual learning evaluation is that at test time it needs to know which adapter to use \u2014 information that most other systems do not require as they don't have campaign-specific parameters (exception: AdBERT  and GPT-3[GPT3 technically does not have campaign-specific parameters, but it needs to be given campaign-specific prompts, and so requires the campaign label.]). We experiment a simple approach to solve this: have MetaTroll classify a user using all its adapters, and select the outcome that has the highest probability.[We do the same for AdBERT and GPT3.]\n\n\nWe present (5-shot) troll classification results under this continual learning setting in \nTable\u00a0<ref>.\n\nAdBERT and GPT-3 suffers little catastrophic forgetting, as they have campaign-specific parameters or prompts (they are unaffected by continual learning as their base model is unchanged), although their performance is only marginally above random chance in the first place.\nMetaTroll is the clear winner here, with only <5% accuracy degradation over time.[Note that performance of these systems on older campaigns will still degrade slightly over more adaptations, as there are more campaign-specific adapters or prompts to select from.]\n\nIn contrast, the meta-learning methods and few-shot classifiers suffer from catastrophic forgetting and their performance on older campaigns drops substantially (e.g. MAML's performance for GRU-2020-NATO drops from 68.32 to 65.15 and 57.62 after several updates).\n\n\n\n\n\nAn unintended benefit of that MetaTroll classifies trolls using multiple adapters is that it is effectively doing multi-class rather than binary classification (with over 85% accuracy for campaign classification; Table\u00a0<ref>). What this means is that not only MetaTroll is able to retain its performance for classifying trolls vs. non-trolls from different campaigns over time, it can also predict which campaign an instance belongs to \u2013 an arguably more difficult task.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Multilingual performance\n\n\nWe extend MetaTroll to multilingual. To encode multilingual input text, we replace BERT with XLM-R\u00a0<cit.>.[Specifically, we use the following library that implements adapters into XLM-R: <https://docs.adapterhub.ml/classes/models/xlmroberta.html>] For comparison, we include baselines BERT, KNN, and AdBERT, and meta-learning methods ProtoNet and MAML as their base model can be replaced with XLM-R. We exclude the few-shot classifiers as they are designed for English and cannot be trivially adapted to other languages.\n\n\n\n\nIn terms of data, we expand the meta-test campaigns by adding four new campaigns (Thailand-2020-RTA, Mexico-2021-Election, Venezuela-2021-Gov, and China-2021-Changyu) where the predominant language is not in English and present their statistics in Table\u00a0<ref>. For the 10 English campaigns (data), we restore the previously discarded non-English posts and include them for meta-training and meta-testing.\n\n\nResults are presented in Table\u00a0<ref>.\n\n\nGenerally we see that all models' performance has somewhat degraded when their feature extractor is replaced with a multilingual model, although MetaTroll manages to keep its accuracy around 60%.\n\nInterestingly, China-2021-Changyu appears to be the most difficult campaign, and we suspect it may be due to its diverse set of languages (38% French, 12.3% English, 12.1% Simplified Chinese).\n\n\n\n\n\n\n \u00a7.\u00a7 Multimodal performance\n\n\nNext we consider incorporating images posted by users, as image is an effective communication device (e.g. memes). Note that we only present results for different variants of MetaTroll here, as we have demonstrated that it is the most competitive detection system.\n\nTo process images, we use pre-trained ResNet18\u00a0<cit.> as an off-the-shelf tool to extract image embeddings. We also explore using a multilingual OCR model to extract text information from images (which will be useful for processing memes).[https://github.com/JaidedAI/EasyOCR] \nAs we have multiple images for each user, we aggregate the ResNet18 image embeddings via max-pooling, and concatenate the max-pooled vector with the text representation (v in Equation <ref>). For texts that are extracted by OCR, we concatenate them into a long string and process them with another AdapterBERT (Equation <ref>; their parameters are not shared), and similarly concatenate the final CLS representation to the text representation.\n\nResults are in  Table\u00a0<ref>. \u201c+image#\u201d is a baseline where we concatenate a numeric feature that denotes the number of images posted by the user to the text representation. Interestingly, even with the baseline approach we see a small improvement, indicating that trolls use more images (e.g. average number of images used by trolls is 20 vs. 5 for non-trolls in GRU-2020-NATO).\n\nIncorporating either ResNet or OCR encodings boosts performance further (with OCR being marginally more beneficial), and that adding them both produces the best performance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\nWe propose MetaTroll, a few-shot troll detection model with\ncampaign-specific adapters that tackles catastrophic forgetting in a continual learning setting.\n\nExperimental results show that MetaTroll outperforms existing state-of-the-art meta-learning and few-shot text classification models, and it can be extended to handle multilingual and multimodal input.\n\n\n\n\n\n\n\n\n\n\n\u00a7 ACKNOWLEDGEMENTS\n\nThis research is supported in part by the Australian Research Council Discovery Project DP200101441.\nLin Tian is supported by the RMIT University Vice-Chancellor PhD Scholarship (VCPS).\n\n\n\nACM-Reference-Format\n\n\n\n\n\n\n\n\n\n\u00a7 IMPLEMENTATION DETAILS\n\nWe implement our models in PyTorch using the HuggingFace \nlibrary[<https://github.com/huggingface>] and their \npretrained BERT[<https://huggingface.co/bert-base-cased>] \nand XML-R[<https://huggingface.co/xlm-roberta-base>].\nThe adapter-based models are from AdapterHub[urlhttps://github.com/adapter-hub/adapter-transformers].\n\nTo handle images, we use ResNet18\u00a0[<https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html>] and EasyOCR\u00a0[<https://github.com/JaidedAI/EasyOCR>].\n\nWe set maximum token length = 320 and dropout rate = 0.2 for BERT embedding and dropout rate = 0.1 for meta-learning process. \nLearning rate is tuned in the range between [1e^-5, 5e^-5] for BERT in the first stage. Learning rate warmup is set up 10% of steps.\nThe learning rate \u03b2 for MAML outer loop is 1e^-5.\nIn the first stage, BERT uses the Adam optimiser\u00a0<cit.>.\nSearch space for learning rate \u03b3 and \u03b4 is from [1e^-4, 1e^-5, 2e^-5,3e^-5,4e^-5,5e^-5].\nThe outer loop learning rate for the third stage \u03b4 is set as 1e^-5.\nThe inner loop learning rate \u03b3 for BERT-based models is set as 2e^-5 and for XLM-R-based models is set as 5e^-5.\nWe experimented with the number of training tasks in the range of 60,000 to 100,000, with 80,000 tasks generally yielding the best results.\nOur experiments are running using A100 GPU with 40GB Memory.\n\n\n\n\n\u00a7 ABLATION STUDY\n\nWe conduct ablation study to justify the importance of our three stages training processes. By removing the adaptive classifier, we attach a standard linear layer for the final classification.\nFor \u201cs1+s2\u201d, we attach a standard linear layer for the final classification with removing the adaptive classifier. \u201cs2+s3\u201d is initialise the BERT text encoder without pre-training on any troll data. \u201cs1+s3\u201d drops the meta-trained task-specific adapter. We still allocate one adapter to each task with randomly initialised adaptor. \nWe report the average accuracy on four English meta-testing campaigns of different model variants in Table\u00a0<ref>. Results show that our model, the combination of all three training stages achieves the best average accuracy of 76.35%. Without pretraining the base text encoder, the framework performs the worst, resulting in 3.04% drops on average accuracy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 DATASET DETAILS\n\n\nWe further include a detailed statistics of our Twitter troll data with detailed topic for each campaign and online post link attached.\n\n\n\n\n\n"}