{"entry_id": "http://arxiv.org/abs/2303.06744v1", "published": "20230312201614", "title": "Ensemble Learning of Myocardial Displacements for Myocardial Infarction Detection in Echocardiography", "authors": ["Nguyen Tuan", "Phi Nguyen", "Dai Tran", "Hung Pham", "Quang Nguyen", "Thanh Le", "Hanh Van", "Bach Do", "Phuong Tran", "Vinh Le", "Thuy Nguyen", "Long Tran", "Hieu Pham"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDTT: An Example-Driven Tabular Transformer by Leveraging Large Language Models \n    Davood Rafiei\n    March 30, 2023\n===============================================================================\n\n\n\n\n\n\n\n\nBackground: Early detection and localization of myocardial infarction (MI) can reduce the severity of cardiac damage through timely treatment interventions. In recent years, deep learning techniques have shown promise for detecting MI in echocardiographic images. Existing attempts typically formulate this task as classification and rely on a single segmentation model to estimate myocardial segment displacements. However, there has been no examination of how segmentation accuracy affects MI classification performance and the potential benefits of using ensemble learning approaches. Our study investigates this relationship and introduces a robust method that combines features from multiple segmentation models to improve MI classification performance by leveraging ensemble learning.\n\nMaterials and Methods: Our method combines myocardial segment displacement features from multiple segmentation models, which are then input into a typical classifier to estimate the risk of MI. We validated the proposed approach on two datasets: the public HMC-QU dataset (109 echocardiograms) for training and validation, and an E-Hospital dataset (60 echocardiograms) from a local clinical site in Vietnam for independent testing. Model performance was evaluated based on accuracy, sensitivity, and specificity.\n\nResults: The proposed approach demonstrated excellent performance in detecting MI. It achieved an F1 score of 0.942, corresponding to an accuracy of 91.4%, a sensitivity of 94.1%, and a specificity of 88.3%. The results showed that the proposed approach outperformed the state-of-the-art feature-based method, which had a precision of 85.2%, a specificity of 70.1%, a sensitivity of 85.9%, an accuracy of 85.5%, and an accuracy of 80.2% on the HMC-QU dataset. On the external validation set, the proposed model still performed well, with an F1 score of 0.8, an accuracy of 76.7%, a sensitivity of 77.8%, and a specificity of 75.0%.\n\nConclusions: Our study demonstrated the ability to accurately predict MI in echocardiograms by combining information from several segmentation models. Further research is necessary to determine its potential use in clinical settings as a tool to assist cardiologists and technicians with objective assessments and reduce dependence on operator subjectivity. Our research codes are available on GitHub at <https://github.com/vinuni-vishc/mi-detection-echo>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeywords: Echocardiography, Image Segmentation, Deep Learning, Machine Learning, Myocardical Infarction, Motion Estimation, Regional Wall Motion Abnormality, Diagnostic Ability\n\n\n \n\n\n\n\n\n\u00a7 INTRODUCTION\n\nA myocardial infarction (MI), which is also called a heart attack, happens when blood flow to part of the heart is cut off due to a clot\u00a0<cit.> and severely damage the heart tissue. Most of the time, this happens because one or more of the coronary arteries, which bring blood to the heart, are blocked. \n\nMI is a serious and potentially life-threatening condition that is the leading cause of death worldwide, affecting 32.4 million people each year\u00a0<cit.>. In the US solely, about 4 million people visit the emergency room each year with heart symptoms\u00a0<cit.>.\n\nAccording to a study undertaken by the World Health Organization\u00a0<cit.>, cardiologists use multiple diagnostic indicators such as pathology outcomes, biochemical marker values, electrocardiography (ECG) findings, and other imaging modalities to diagnose patients with MI\u00a0<cit.>. However, pathology can only detect dead cells in the heart muscle\u00a0<cit.>. ECG cannot distinguish between MI and myocardial ischemia symptoms\u00a0<cit.>, and the specificity of biochemical marker values (cardiac enzymes) is quite low\u00a0<cit.>. Due to these limitations, none of these techniques are adequate for early MI detection. Therefore, the most valuable tool for early diagnosis is an imaging technique known as echocardiography, which is applicable for both clinical and research applications. Echocardiography (ECHO) is a pivotal tool for a safe and real-time functional assessment of the cardiovascular system <cit.>. It is based on ultrasonography, a noninvasive imaging technique that is incredibly valuable for monitoring and diagnosing patients who are exceedingly vulnerable. Moreover, echocardiography is fast, inexpensive, accessible, portable, and carries the lowest risk among imaging techniques\u00a0<cit.>.\n\n\nWith the availability of the MI datasets on echocardiography, machine learning (ML) algorithms have been used to detect MI by extracting features from echocardiography\u00a0<cit.>. Although they showed promising results, previous studies have largely focused on using features from segmentation models, but the assumption that good segmentation equates to strong classification has yet to be fully substantiated. In addition, current methods are still limited by using only features from a single model, which is a common problem in MI classification and can lead to poor performance on unseen data\u00a0<cit.>. We conducted experiments to determine the relationship between strong segmentation models and precise MI classification. Our results showed that utilizing the predictions from multiple models through ensemble methods can better identify the patterns and features from echocardiography, resulting in more trustworthy and accurate predictions. Therefore, in this work, we propose a new approach to MI classification by incorporating multiple segmentation models and ensemble learning techniques. Our main contributions are summarized as follows:\n\n    \n  * Our experiment results showed that there is no strong correlation between good segmentation models and accurate MI classifiers. The finding indicates that highly accurate segmentation of the left ventricle (LV) is not a key condition for accurate MI detection.\n    \n    \n  * We proposed an ensemble method to combine multiple features produced by different LV wall segmentation models. The experimental results show that the proposed ensemble method consistently outperformed the state-of-the-art methods based on single models across all metrics on both the public and external test sets. This result suggests that ensemble learning is successful at complementing the features of multiple feature extractors in a way that a single feature model could not.\n\n    \n  * The effectiveness of our method was tested on an external dataset obtained from a local clinical source. The results revealed a decline in the model's performance compared to public data test sets. Possible reasons for the decrease in performance are presented, along with an examination of the implications of these results. Recommendations are also given for enhancing the model's performance in a clinical environment.\n    \n    \n  * The proposed method showed a higher agreement score (Cohen's kappa value) than single-feature methods, regardless of whether they used different sets of features. This high level of agreement is an important advantage of the proposed approach as it suggests that the model predictions are subject to variation due to different feature extractors. The high Cohen's kappa value indicates that our proposed method is reliable and well-suited for use in the classification of MI.\n    \n\n\n\n\u00a7 RELATED WORK\n\n\nMI detection has been a focus of research in the field of medical imaging, with various techniques being proposed to detect abnormalities in LV wall motion\u00a0<cit.>. With the aim to reduce the cost and time of diagnosis, computer-assisted diagnostic techniques have been developed in recent decades that aim to automate the detection of MI by using image processing and ML techniques\u00a0<cit.>. Very first studies use active contour-based models, such as\u00a0<cit.> the snake technique introduced by Kass <cit.>, which uses an elastic curve to detect lines, boundaries, and edges in an image. However, these methods can be impractical or even impossible to use in cases where the LV wall is not visible due to low contrast or a portion of the wall is missing\u00a0<cit.>.  Other MI detection techniques include motion estimation methods\u00a0<cit.>, which track the displacement of the LV wall, but the accuracy of these methods can depend on the performance of speckle tracking and can lead to unreliable results\u00a0<cit.>.\n\nDue to limitations in extracting features while using solely image processing techniques, recent studies shift to deep learning to extract hidden features from echocardiography images and detect MI. Neural networks such as U-Net <cit.> and U-Net++ <cit.> have been widely used for semantic segmentation. <cit.> presented a large dataset of 2D echocardiography images and proposed a U-Net-based model to accurate segmentation of LV wall. \u00a0<cit.> utilized the accurate segmentation of LV wall to detect MI. While studies have shown that deep learning models can be used to detect MI, to the best of our knowledge, no work has explored the direct correlation between a strong segmentation of the LV wall and MI detection.\n\nIn addition to developing individual deep learning models, ensemble techniques, which combine the predictions of multiple models, have been explored as a way to improve performance and robustness in the analysis of cardiac functions. For example, <cit.> improved the performance of heart's morphological and functional assessments by using majority vote from three ML models: support vector machine, random forest, and deep learning. <cit.> also increased the diagnostic performance of coronary heart disease screening by stacking ML models. While these studies have shown that ensemble techniques can improve the performance of medical problems, there is still no work that has explored the use of ensemble techniques for MI detection in echocardiography images.\n\n\n\n\nAnother major concern while using deep learning models for MI detection is the inconsistency of the results\u00a0<cit.>. The performance of deep learning models is highly dependent on the quality of the training data, which can be difficult to obtain<cit.>. In addition, the performance of deep learning models can be affected by the choice of the model's architecture and hyperparameters, which can be difficult to determine\u00a0<cit.>. To address these issues, we propose a strategy for combining features from different models in order to improve the diagnostic performance of MI detection. The proposed approach is based on the idea that by combining the strengths of various techniques, we can overcome their individual limitations and achieve more accurate and reliable results. We believe that this approach has the potential to significantly advance the field of MI detection and improve patient care.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe remainder of this paper is organized as follows. Section <ref> introduces the benchmark HMC-QU dataset, our in-house dataset from E-Hospital, and the framework for determining LV wall motion for MI identification. We will also describe in this section the experimental setting used in this study. Next, Section <ref> presents the quantitative and qualitative evaluations of the proposed approach on the HMC-QU dataset and the E-Hospital dataset. We analyze the MI detection performance using various segmentation architectures and classifiers. Finally, Section <ref> discusses the experimental results and concludes the paper with some potential research directions.\n\n\n\n\n\n\n\n\n\u00a7 MATERIALS AND METHODS\n\n\n\n\n\n\n\nIn this section, we introduce the proposed ensemble learning framework that addresses the challenge of MI detection. Figure <ref> illustrates  the  overall  architecture,  consisting  of three main phases: LV wall segmentation, feature engineering from myocardial segmentation displacement, and MI detection by traditional classification ML methods from ensemble models by weighting features from different segmentation models. Below we explain each phase in detail.\n\n\n\n\n    \n  * Phase 1 - LV wall segmentation: The goal of the first phase is to identify the myocardial boundary, which is a key indicator of heart function. Previous works often used only a single segmentation model to contour the LV. We further use multiple segmentation models and assess the accuracy of each segmentation model on each segment and on the whole LV.\n    \n    \n\n    \n  * Phase 2 - Feature engineering from myocardial segmentation displacement: In this phase, we extract features from the segmented myocardial regions based on displacement over time. These features include measures such as strain, strain rate, and torsion, which are important indicators of myocardial function and can provide insight into the presence of MI. Because we used multiple segmentation models in Phase 1, we proposed a method to combine features from multiple segmentation results.\n    \n\n    \n  * Phase 3 - MI classification: In the final phase, traditional classification ML methods such as support vector machines, random forests, and logistic regression, are explored to detect MI from the extracted features. To further improve the performance of the proposed model, we also implement an ensemble learning approach by weighting the features from the different segmentation models. The weighting step takes into account the segmentation accuracy of each segmentation model. The performance of the proposed framework will be evaluated using common metrics, including sensitivity, specificity, precision, F1 score, and accuracy.\n\n\n\n\n\n\n\n \n\n \n\n\n\n \u00a7.\u00a7 DATASET\n\nIn this study, we used the public HMC-QU dataset <cit.> as the training and validation sets. The HMC-QU dataset 2D echocardiography recordings for the detection of MI and was established by cardiologists from the Hamad Medical Corporation, researchers from Qatar University and Tampere University. The ultrasound machines used to acquire the data were made by GE Healthcare, and the recordings have spatial and temporal resolutions that vary from 422\u00d7636 to 768\u00d71024 and 25 frames per second, respectively. The collection includes 162 Apical Four Chambers (A4C) echos acquired between 2018 and 2019, but for the purposes of this study, a subset of 109 echos was used, resulting in a total of 2,349 images from 72 MI patients and 37 non-MI subjects. The remaining 53 echos were excluded because they did not include the entire LV wall, which was necessary for cardiologists to evaluate. As depicted in Figure <ref>a, the non-MI and MI cases have two frames: end-systolic and end-diastolic. The MI case has a significantly larger overlap region compared to the non-MI sample.\n\n\n\n\n\n\nTo evaluate the effectiveness of our proposed method, we collected a dataset of patient records from E-Hospital, a local clinical site in Vietnam. The institutional review board at the clinical site approved the study, and we followed ethical guidelines in collecting and processing the data. We obtained 200 patient records from E-Hospital for this study, without specific selection criteria, covering the period from 2020 to 2021. The data were acquired using the Philip Affiniti 70G ultrasound imaging system, and we carefully examined all the data to ensure high-quality and visible anatomy structures. Only 60 data samples met our standards and were retained for further analysis. Three experienced doctors annotated the MI region in each sample, and another group of three doctors checked the annotations for accuracy and consistency. We used the annotated data for training and validation of our proposed model, which we stored in the Digital Imaging and Communications in Medicine (DICOM) format. The dataset includes only MI annotations and was used as a separate test set for evaluating the performance of our model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 FEATURE ENGINEERING\n\nSimilar to <cit.>, we extract the motion feature by performing two consecutive steps: LV (LV) segmentation and feature calculation. Below we explain these two steps in detail.\n\nLV segmentation. We use a U-Net inspired convolutional network that consists of an encoding and down-sampling path, followed by a decoding and up-sampling path with skip links <cit.>. The model's output is a mask of the LV obtained through the Softmax function. During the training phase, we use cross-entropy as the loss function to optimize the model's parameters. Cross-entropy, which is defined as a measure of the difference between two probability distributions for a given random variable or set of events, is widely used for classification objectives. The binary cross-entropy is defined as\n\n    L_BCE(y, \u0177)=-(y log (\u0177)+(1-y) log (1-\u0177)),\n\nwhere \u0177 is the predicted value by the prediction model.\n\n\n\n\n\n\n\n\n\n\nFeature calculation. We analyze the segments of the LV wall to detect possible MI signatures. A standardized model\u00a0<cit.> recommends dividing the LV wall into seven segments, as depicted in Figure <ref>b. However, in our analysis, we only consider six of them, as the apical cap does not exhibit inward motion activity and should be skipped for the A4C view. For MI detection, we extract the displacement of the endocardial boundary points from the six segments. By evaluating the rate of displacement from the captured global motion of the LV wall, we aim to mimic the typical diagnosis of cardiologists, who assess segments that show a lack of motion as abnormal.\n\n\nAfter segmenting the LV wall, we further extract its inner border to define the endocardial boundary, which is then divided into standardized six segments. The boundary segment displacements are calculated using the L_1 norm as follows\n\n    d_L1 = |x^t - x^t_r| + |y^t - y^t_r|,\n\nwhere x and y are the pixel coordinates of the current frame t and the reference frame t_r (the first frame of one cycle). In order to capture the boundary segment motion more precisely, we take N uniformly sampled pixels p \u2208(x_1, y_1), (x_2, y_2),..., (x_N, y_N) on each frame t for each segment s, and calculate the pair-wise distances d_s_t between t_r and t. The segment displacement for each frame is then calculated as \n\n    d_s_t = 1/N\u2211_n=1^N |x_n^t_r-x_n^t|+|y_n^t_r-y_n^t|.\n\n\nFinally, we take the maximum pixel displacement of each segment, d_s^max = max(d_s_t), from the displacement curves, and normalize it to unity. The motion feature, MF, is then calculated as\n\n    MF=d_s^max.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 ENSEMBLE OF FEATURES\n\n\n\nEnsemble learning, which involves training a group of different classifiers and combining their predictions, has been shown to improve model performance and robustness <cit.>. During the process of feature engineering, we discovered that different segmentation models perform better on different segments (as shown in Table <ref>). This meant that the performance of the classifier was limited by the performance of the segmentation model. To enable the classifier to benefit from multiple features, we calculated the accumulated vector by summing multiple feature vectors based on the accuracy of the segmentation models on the validation set. This approach has several benefits: 1) the weighting method does not require any training, so it can be used universally with different classifiers; 2) conventionally, stacking multiple feature vectors can increase the dimensionality of the accumulated feature vector, making it harder to train the classifier and limiting the number of features that can be used. In contrast, the weighted sum vector has a fixed dimension regardless of the number of segmentation models used.\n\n\n\n\nFormally, given n motion features vector MF \u2208\u211d^6 extracted from n segmentation model with an objective metric for different segments \ud835\udd44\u2208\u211d^6. The weighted coefficient \ud835\udd4e is calculated as\n\n    \ud835\udd4e_i =  \ud835\udd44_i/\u2211_i=i^n\ud835\udd44_i.\n\n\nAccumulated feature vector MF_acc\u2208\u211d^6 is calculated as\n\n    MF_acc = \u2211_i=i^n MF_i \u2299\ud835\udd4e_i.\n\n\n\n\n\n\n\n \u00a7.\u00a7 MI DETECTION\n\nIn the final step of the pipeline, we use ML to detect MI in an echocardiogram. To do this, we employ a variety of conventional supervised ML techniques, including \nsupport vector machine <cit.>, logistic regression <cit.>, decision trees <cit.>, and k-nearest neighbor <cit.>. These techniques were chosen over more complex deep learning methods due to the small and imbalanced nature of our dataset, as well as the fact that the extracted features are more suited to simpler analysis. To fairly evaluate the performance of these classifiers, we use a stratified 5-fold cross-validation scheme. The details of their configuration, training, and testing are discussed in the following section.\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 EVALUATION METRICS\n\n\nLV Segmentation. The Intersection over Union (IoU) metric, also known as the Jaccard index, is used to measure the overlap between the target mask and the prediction output. This metric is similar to the Dice coefficient, which is frequently used as a loss function during training. The IoU is calculated by dividing the intersection of the target and prediction by their union. It is written as\n\n\n    IoU=target\u2229prediction/target\u222aprediction.\n\n\nMI Detection. For the MI detection, we classify infarcted subjects as class-positive (MI) and normal, non-MI subjects as class-negative. In this case, the confusion matrix is formed as follows: TN is the number of correctly predicted non-MI subjects, TP is the number of correctly predicted MI patients, FN is the number of incorrectly detected MI patients as non-MI subjects, and FP is the number of incorrectly detected non-MI subjects as MI patients. The elements of the confusion matrix are calculated per video for MI detection. The standard performance evaluation metrics are defined as\n\n\n    Sensitivity   =TP/TP + FN,\n\n\n    Specificity   =TN/TN + FP,\n\n\n    Accuracy   =TP + TN/TP + TN + FP + FN,\n\n\n    Precision   =TP/TP + FP,\n\n\n    F(\u03b2)=(1+\u03b2^2)  Precision \u00d7 Sensitivity /\u03b2^2\u00d7 Precision + Sensitivity ,\n\n\nwhere TP, FP, TN, and FN denote the numbers of true positive, false positive, true negative, and false negative cases, respectively. Sensitivity (also known as recall) is the ratio of correctly detected MI patients to all MI patients. Specificity is the ratio of correctly classified non-MI subjects to all non-MI subjects. Precision refers to the number of correctly detected MI patients over the total number of correctly detected samples. Accuracy is the ratio of correctly detected samples. F1 score is calculated as the harmonic average of precision and sensitivity, with a weighting of \u03b2=1 in the dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrediction Reliability. In addition to the performance of the prediction model, the reliability of the model is a crucial criterion for ML models used in medical applications. In this case, reliability can be interpreted as the consistency of the classifier's output regardless of the use of different feature extractors or combinations of feature extractors. We quantify this criterion by calculating the Cohen's Kappa\u00a0<cit.> coefficient of the model's output based on the feature extractor. In this work, the Cohen's Kappa coefficient is calculated for three scenarios: \n\n\n\n  * Agreement score between different predictors that use a single feature extractor. \n\n  * Agreement score between ensemble model using a different set of feature extractors.\n\n  * Agreement score between cardiologist experts on single echo.\n\n\n\n\n \u00a7.\u00a7 EXPERIMENTAL SETTINGS\n\n\nLV wall segemtation. To evaluate the model, we employed a stratified 5-fold cross-validation scheme, in which we used 80% of the available echos in the dataset to train the model and tested it on the remaining 20% holdout (unseen) echos. During the training process, we trained all parameters in the network for 50 epochs using a learning rate of 1e-4. The model was implemented in Pytorch and optimized using the Adam optimizer with parameters \u03b2_1 = 0.9 and \u03b2_2 = 0.999. The training and testing were conducted on a GTX 3090 GPU, and the number of total parameters ranged from 1 million to 23 million.\n\nIn our experiments, we explored a variety of architectures, including UNet <cit.>, UNet++ <cit.>, LinkNet <cit.>, DeepLabV3 <cit.>, and PAN <cit.>, as well as several encoder architectures, including resnet18 <cit.>, vgg11 <cit.>, densenet121 <cit.>, efficientnet-b0 <cit.>, and inceptionv4 <cit.>. The segmentation performance was evaluated on a pixel-level using the IoU score metric described in Section <ref>.\n\n\n\n\nMI detection. \nIn this experiment, we applied various supervised ML techniques for binary classification, including support vector machine (SVM), logistic regression (LR), decision trees (DT), and k-nearest neighbor (KNN). To begin, we collected a dataset consisting of input ensemble features and corresponding binary labels. Next, we split the dataset into a training set and a test set. The training set was used to train the different classification models, while the test set was used to evaluate their performance. \n\nFor each of the models, we first fitted the model to the training data using default hyperparameter values. We then used the trained model to predict the labels for the instances in the test set. The performance of each model was evaluated using a variety of metrics, such as accuracy, precision, and recall.\nFinally, we compared the performance of the different models to determine which one achieved the best results. For models used in our experiments, we used the default hyperparameters provided by the scikit-learn\u00a0<cit.> library, which is a widely used ML library in Python. This decision was made to ensure that our model could be easily reproduced and to facilitate comparisons with other studies that used similar methods.\n\nModel reliability analysis. To evaluate the reliability of the predictions made by the different classification methods, we first randomly selected 20 examples from the test set. We then calculated the agreement score within each classification method (SVM, LR, DT, and KNN) that used only features derived from a single segmentation model. Each classification method was trained using four different sets of features. For the ensemble methods, we also calculated the agreement score between the four models, each of which was created using a different pair of features. Finally, we computed the human expert agreement score using the labels provided by three experts on the same echo samples. This allowed us to measure the similarity of the output of the feature extractors and classifiers on the same input.\n\n\n\n\n\n\n\u00a7 RESULTS\n\n\n\n\nIn this section, we present the results of our experiments on LV wall segmentation and MI classification using our proposed method on the HMC-QU validation test and a separate E-Hospital test set, respectively. We also evaluate the performance of our method through various metrics and techniques. First, we report the performance of our model on LV wall segmentation on the HMC-QU validation test on different LV wall segments, where we compare the state-of-the-art methods in the field. Next, we present the MI classification results on the HMC-QU and E-Hospital datasets and compare our model's performance against the baseline and other existing approaches. Finally, we discuss the reliability of our model by analyzing its agreement scores compared to existing methods and human experts.\n\n\n \u00a7.\u00a7 LV WALL SEGMENTATION ON HMC-QU VALIDATION TEST\n\n\n\n\n\nTable <ref> shows the LV wall segmentation averages (mean) results  for 5-fold CV  with different network architectures and encoders. The results indicate a stable IoU score between various types of architectures and encoders. The maximum IoU score can get is 0.876% when using UNet architecture and resnet18 encoder. Despite the fact that there is no significant variation in segmentation performance during validation, the segmentation models have different scores on different heart segments, as we have discussed in section <ref>. \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 MI CLASSIFICATION ON TEST SET\n\n\n\n\n\nTable <ref> demonstrates that the performance of concatenated features from multiple segmentation models is superior compared to a single segmentation model and the performance across several classifications is consistent. Overall, the model achieved better performance by using ensemble features.\n\n\n\n  * Models with the same classifier performed similarly when using only one feature, on the other hand, ensemble models show better performance. Using SVM models, for instance, PAN or UNet segmentation features yielded comparable F1 scores of 0.829 for PAN and 0.831 for UNet. In contrast, the ensemble SVM model performed better with F1 score of 0.925. This improvement is rather uniform across classifiers by a substantial margin.\n\n\n  * Upon comparing the performance of several classifiers, we found that there exist discrepancies in the approaches employed. Using an ensemble of two or three features, LR provided the best performance with F1 score of 0.942, while SVM, DT, and KNN only achieved F1 scores of 0.925, 0.910, and 0.894, respectively. When more features are added to the ensemble, however, this result becomes inconsistent, with LR's F1 score dropping to 0.905 and DT's F1 score reaching 0.931. This finding suggests that classifier selection should be based on experimentation with all classifiers.\n\n\n  * Regarding the number of features used to construct the model ensemble. We have experimented with combining two and three features. We discovered that combining additional features does not produce better results. In reality, adding more features to a model might diminish its performance. In the case of the SVM classifier, adding three features decreased the F1 score from 0.925 to 0.903. This could indicate that adding more features to an ensemble could eventually increase the noise to the final features, eradicating the effectiveness of the advantage segment.\n\n\n  * Our ensemble method showed significantly better performance compared to the average ensemble approach. our method achieved an F1 score of 0.925, while the average ensemble approach only achieved an F1 score of 0.806. This demonstrates the effectiveness of choosing the right regions from the LV wall, rather than average regions altogether.\n\n\n\n\nCorrelation between good segmentation models and\naccurate MI classifications. In our experiment, we evaluated the performance of four different MI detection algorithms. Each algorithm used features from a weaker segmentation approach (PAN), and more accurate segmentation techniques (UNet and LinkNet). As shown in the table <ref>, While LinkNet and UNet achieved higher IoU scores of 0.871 and 0.867, respectively, algorithms that use PAN features still performs better than LinkNet and UNet with F1 score of 0.836 for DT-PAN and 0.788 DT-LinkNet. These results suggest that, while good segmentation is important for MI detection, it is not the only factor that determines the overall performance of the algorithm. In this case, the combination of advanced segmentation and effective feature extraction/classification techniques appears to be crucial for achieving optimal results.\n\nComparison with previous state-of-the-art method. Table <ref> shows the performance of our ensemble models and the previous state-of-the-art method from <cit.>. In general, our ensemble LR model with two PAN features outperforms the state-of-the-art model presented in <cit.> by a significant margin. The F1 score for our model is 0.09 higher than the previous model, indicating that it is able to more accurately predict the outcome of interest. Additionally, when considering other evaluation metrics such as sensitivity, specificity, precision, and accuracy, our model consistently outperforms the model from <cit.>. This suggests that using a larger number of features can be beneficial for improving the performance of a ML model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerformance of classification models on external test set. Table <ref> shows the result of our ensemble model in comparison with the single-feature model on the local clinical site, E-Hospital. The performance of the ensemble model on the external local clinical test set was found to be highly correlated with the results obtained on the public data test set. In terms of F1 score, the ensemble model achieved a score of 0.824 on the local clinical test set, which was significantly higher than the score from single-feature and average ensemble models. This demonstrates the consistant of our methods on different dataset. Additionally, the best ensemble model had a sensitivity, of 0.806 on the local clinical test set, which was again higher than single feature model and average ensemble model. Overall, these results suggest that the MI classification model is reliable and effective in identifying MI events in both familiar and novel datasets.\n\nWe also discovered that the external test on the local clinical dataset yielded lower scores compared to the public test set. Specifically, the model performed with F1 score of 0.824 on the local clinical dataset, while it achieved F1 score of 0.942 on the public test set. This suggests that the model may not generalize as well to the local clinical dataset, possibly due to differences in the distribution of the data or the specific characteristics of the patient population represented in the dataset. Solving this problem may be a task for future work. There are several potential approaches that could be pursued in order to address this issue, some possible approaches could include using different types of ML algorithms, collecting additional data, or fine-tuning the model parameters in order to improve performance. Ultimately, it will be important to carefully evaluate the strengths and limitations of different approaches in order to identify the most promising direction for future work.\n\n\n\n\n\n \u00a7.\u00a7 MODEL RELIABILITY\n\n\n\n    \nTable <ref> shows the agreement scores between the single-feature model and ensemble-feature models for different classifiers and a human expert. A comparison between the single-feature model and ensemble-feature models shows that the prediction of ensemble-feature models is more consistent than that of the single-feature model. For example, the agreement score of the model's predictions by SVM single-feature models is 0.74, which is not so consistent with the human expert (0.96). The agreement score of ensemble models ranges from 0.79 to 0.82, the prediction by ensemble models is more similar to that of a human expert than that of a single-feature model, but still not consistent enough to replace a human expert. \n\nWe observed no significant difference in consistency among single-model classifiers using SVM, LR, and DT. The same is true when we used ensembles, but we found that the DT ensemble had slightly better performance than the SVM and LR ensembles, as reflected in agreement scores of 0.82 for LR, and 0.81 and 0.79 for SVM and LR, respectively.\n\n\nThe above assessment show that ensemble models can be more accurate than a single model because it can capture a wider range of patterns and relationships in the data. In this case, it appears that the ensemble model has a higher agreement score than the single-feature model, which suggests that it is making more accurate predictions. This could also be due to the fact that the ensemble model is able to consider multiple features, rather than just one, which allows it to better capture the complexity of the data. Overall, the higher agreement score of the ensemble model indicates that it is a more effective model for MI classification problem.\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nWe proposed an accurate and robust deep learning-based approach for the MI detection on echocardiograms. We showed that accurate segmentation models are not fully correlated with accurate MI classifiers, indicating that highly accurate segmentation of the LV is not a key factor for building an accurate MI detection system. We then proposed an ensemble method for combining multiple features provided by different LV segmentation models, which outperformed both single models and state-of-the-art methods on two echocardiogram datasets. Compared to these existing approaches, the proposed method demonstrated significant improvement across all evaluation metrics. We further illustrated that the proposed method shows a higher agreement score (Cohen's kappa value) than single-feature methods, regardless of the features used. This high level of agreement suggests that our predictions are subject to less variation due to different feature extractors, making our method reliable and well-suited for use in the classification of MI.\n\nOur work opens up several potential directions for further exploration. First, an end-to-end system for MI detection, rather than the current three-stage approach, should be developed. Second, further studies can be conducted to investigate the causes of performance drop on the external dataset and implement methods for addressing these issues. Third, it might be interesting to extend our training method into  the  pretraining  stage  to  produce better pre-trained models. Additionally, further exploration of the factors that contribute to the success of ensemble learning in MI detection could be beneficial for improving the performance of future models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CONFLICT OF INTEREST STATEMENT\n\n\nTN, and HP were employed by VinUni-Illinois Smart Health Center, VinUniversity. PN, VL, TN, and LT were employed by VNU University of Engineering and Technology. DT was employed by E Hospital. HP, QN, TL, BD, and PT were  employed by Bach Mai Hospital. The authors declare that this research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\n\n\n\n\n\n\u00a7 AUTHOR CONTRIBUTIONS\n\nTN, LT and HP designed the research. TN and PN drafted the manuscript. TN, PN, LT, VL, TN  and HP revised the manuscript. DT, HP, QN, TL, BD, and PT collected and annotated the data. All authors have approved the final version.\n\n\n\n\n\u00a7 FUNDING\n\nThe research is supported by the Vingroup Innovation Foundation (VINIF) under project code VINIF.2019.DA02, and it is also supported by the VinUni-Illinois Smart Health Center at VinUniversity.\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 DATA AVAILABILITY STATEMENT\n\nThe HMC-QU dataset used in this study can be found and freely downloaded on Kaggle at <https://www.kaggle.com/datasets/aysendegerli/hmcqu-dataset>.\n\n\n\n\u00a7 ETHICS STATEMENT\n\nIn the handling and use of medical data, we are committed to upholding the ethical principles of respect for persons, beneficence, and non-maleficence. We recognize that medical data is highly sensitive and personal, and we will take all necessary measures to protect the confidentiality and privacy of individuals. We will only use medical data for the purposes for which it was collected, and we will obtain informed consent from individuals before collecting or using their data. We will also ensure that the use of medical data is necessary and justified, and we will take steps to minimize any potential harm to individuals. Overall, our aim is to use medical data in a responsible and ethical manner that respects the rights and dignity of individuals.\n\n\n\n\nabbrv \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}