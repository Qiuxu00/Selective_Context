{"entry_id": "http://arxiv.org/abs/2303.07143v1", "published": "20230313141134", "title": "Multi-Microphone Speaker Separation by Spatial Regions", "authors": ["Julian Wechsler", "Srikanth Raj Chetupalli", "Wolfgang Mack", "Emanu\u00ebl A. P. Habets"], "primary_category": "eess.AS", "categories": ["eess.AS", "cs.LG", "cs.SD"], "text": "\n\n\nSublinear drag regime at mesoscopic scales in\n  viscoelastic materials\n    C. L. N. Oliveira\n    March 30, 2023\n======================================================================\n\n\n\nWe consider the task of region-based source separation of reverberant multi-microphone recordings.\nWe assume pre-defined spatial regions with a single active source per region.\nThe objective is to estimate the signals from the individual spatial regions as captured by a reference microphone while retaining a correspondence between signals and spatial regions.\nWe propose a data-driven approach using a modified version of a state-of-the-art network, where different layers model spatial and spectro-temporal information.\nThe network is trained to enforce a fixed mapping of regions to network outputs.\nUsing speech from LibriMix, we construct a data set specifically designed to contain the region information.\nAdditionally, we train the network with permutation invariant training.\nWe show that both training methods result in a fixed mapping of regions to network outputs, achieve comparable performance, and that the networks exploit spatial information.\nThe proposed network outperforms a baseline network by 1.5\u00a0dB in scale-invariant signal-to-distortion ratio.\n\n\n\nSpeaker Separation, PIT, Multi-Channel, Regions\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nWhen capturing a mixture of multiple speakers with an array of microphones, the signals contain information related to the speakers' identities and their positions.\nThe speakers' identities, thereby, are encoded in the spectral structure of the individual microphone signals. The positions are implicitly represented in time differences of arrival and attenuation differences of the received speech signals caused by the relative positioning of the source and array. This information can be exploited to separate the speakers from each other, as required, e.g., by automatic speech recognition systems <cit.>, hearing aids <cit.>, or communication systems <cit.>. \n\nSpeaker separation systems are typically implemented as DNN that estimate a mask for each speaker from a feature representation of the microphone signal(s). Subsequently, the mask is applied element-wise to a feature representation of the microphone signal(s) to estimate the features of the individual speakers.\nEach speaker's features are transformed back to the time domain such that they can be processed separately.\nDuring training the DNN, the so-called permutation problem occurs.\nAs all speakers are instances of the class \u201cspeaker\u201d, there are multiple possibilities (speaker permutations) to assign the masks to the individual speakers.\nA prominent solution to address the permutation problem is deep clustering, where mixture features are mapped to vectors <cit.>.\nThe permutation problem is addressed by using a vector-affinity-based loss.\nAn alternative solution is PIT <cit.>, where a reconstruction loss of all permutations is computed, and only the smallest loss amongst the permutations is used to update the DNN.\nHowever, PIT does not scale well with the number of speakers. \nIn <cit.>, the authors avoided the permutation problem by consistently selecting a permutation based on the relative speaker positions (distance or angle) w.r.t. the microphone array. Note that fixing the permutation based on relative positions does not enable to infer the speaker positions from the estimated outputs. \nTo extract a single source from a defined target area,  <cit.> train DNN that exploit location information implicitly given in the DNN inputs.  Alternative approaches <cit.> provide the target DOA explicitly to the separation system such that it extracts the respective source. \n\nIn some application scenarios, the control rights of speakers can be defined by their positions.\nConsider, for example, a car environment where only the driver should be able to control specific applications with their voice (e.g., an autopilot system).\nSingle-source extraction methods <cit.> may not be suitable for such scenarios, as all sources are required, whereas methods that are explicitly provided with the target DOA as input may exhibit problems with spatially extended or angularly overlapping regions <cit.>. \n\nTo relate a DNN output to a specific spatial region, the main contributions of this paper are 1) to introduce the task of multi-source separation with sources located in fixed pre-defined regions, where the desired output signals are assigned to associated regions by enforcing a one-to-one correspondence between regions and network outputs, 2) to propose a training data generation scheme with a task-based spatial data distribution, i.e., to construct the training data such that the speaker positions are restricted to pre-defined spatial regions, and 3) to apply the recently proposed triple-path structure <cit.> for multi-source separation in the time domain, where individual DNN layers model inter-channel, short-term, and long-term information.\n\n\n\n\u00a7 PROBLEM FORMULATION\n\n\n\n\nConsider an in-car scenario where the speech of S \u2208\u2115^+ speakers is captured by a ULA with M \u2208\u2115^+ microphones mounted, e.g., on the rear mirror.\nThe set of positive integers is given by \u2115^+.\nThe m^th\u2208{1,\u2026,M} discrete time-domain microphone signal \ud835\udc32_m \u2208\u211d^T\u00d7 1 is a mixture of the S reverberant speech signals \ud835\udc31_m^(s)\u2208\u211d^T\u00d7 1, i.e., \n\n    \ud835\udc32_m = \u2211_s=1^S\ud835\udc31_m^(s),\n\nwhere s \u2208{1, \u2026, S} is the speaker index, T \u2208\u2115^+ is the length of the signals in time samples, and \u211d is the set of real numbers.\nIn a car, the speaker positions can be restricted to R \u2208\u2115^+ distinct regions with index r \u2208{1, \u2026, R} (e.g., the passenger seats). \nWe denote the set of speakers from region r by \ud835\udcae^(r) and define the mixture in region r by\n\n    \ud835\udc31^(r)_m = \u2211_s \u2208\ud835\udcae^(r)\ud835\udc31^(s)_m.\n\n\nIn the present work, we tackle the problem of separating speech sources belonging to different pre-defined, fixed spatial regions. In addition, each output signal is associated with one region. This association allows to directly identify, e.g., the driver, such that region-specific control rights can be implemented in a human-machine interface. This problem can be understood as a specific MIMO signal estimation problem. \nMore formally, the objective is to obtain estimates \ud835\udc31^(r)_m_ref of the reverberant speech mixtures per region at a reference microphone m_ref based on the M microphone signals \ud835\udc32_m. In this work, we assume a single active speaker per region (i.e., R=S).\n\n\n\n\u00a7 SEPARATION ARCHITECTURE\n\n\n\nThe employed DNN is a modified version of the recently proposed AmbiSep <cit.>, referred to as SpaRSep, where masking in a learned TF domain <cit.> is used for separation  (see Figure\u00a0<ref>).\nThe encoder performs short-time analysis and converts the M time-domain inputs to sub-sampled, positive TF representations, i.e.,\n\n    \ud835\udc18_m = Encoder(\ud835\udc32_m),\u00a0\u00a0\u00a0\ud835\udc18_m\u2208^N\u00d7 F_+,\n\nwhere N is the number of time frames, and F is the number of features.\nThe masking network generates one positive real-valued mask M_r for each source region r, which is multiplied with the encoder output of the reference microphone channel, \ud835\udc18_ref.\nThe masked representations are converted back to the time domain by the decoder. \n\n\n\nEncoder-Decoder:\nA 1D convolution layer of F filters with a kernel-size equivalent to 1\u00a0ms duration and 50% stride, followed by ReLU activation, is used as the encoder.\nThe decoder comprises a transpose-1D convolution layer with the same parameters as the encoder.\nThe encoder and decoder are applied independently to the input and output channels, respectively.\nNote that the weights are shared across the channels.\n\n\n\nMasking network:\nFigure <ref> shows a block diagram of the masking network.\nThe input multi-channel TF representation is first passed through a LayerNorm layer <cit.> followed by a linear layer, and is then segmented into Q chunks of size K with 50% overlap.\nThe chunked TF representation is fed to  P concatenated TPB.\nEach TPB comprises three transformer encoder layers <cit.>, one inter-channel, one intra-chunk, and one inter-chunk transformer.\nThe inter-channel transformer models the relationships across the microphone channels (i.e., spatial information) in the input at each time frame.\nThe intra-chunk transformer models the short-time relations in the input, while the inter-chunk transformer models the global relations across the chunks.\n\nThe output of the last TPB is passed through a PReLU layer.\nAs opposed to <cit.>, the signals are flattened along the channel dimension before being fed to a linear layer to compute R outputs.\nThe chunking operation is inverted using overlap-add, and the corresponding output is passed through an output gate.\nThe gated output is then passed through a linear layer with ReLU activation to predict positive-valued masks.\n\nAll layers in the architecture, except for the inter-channel transformers, process the inputs independently across the channels, making the architecture amenable to a parallel implementation across the channels. \nAn important feature of the architecture is that the number of parameters in the model is independent of the number of input microphones M (which is the sequence dimension in the inter-channel transformer) and sources S.\nOnly the output size of the linear layer after all TPB depends on the number of regions R.\n\n\n\n\u00a7 PROPOSED REGION-BASED SEPARATION\n\n\n\n\n\n\nTo preserve the region information on the R separated signals \ud835\udc31^(r)_m_ref, we propose to assign each of the R output channels to a specific (different) region r.\nWe propose to implement that assignment in two steps.\nFirst, we propose to generate training data where the speaker positions are limited to the respective regions (one speaker per region).\nWe train SpaRSep to maximize the commonly used SI-SDR <cit.> (also, cf. <cit.>).\nTypically, speaker separation systems are trained with PIT, where DNN updates are based on the speaker permutation that yields the lowest loss.\nAn illustration of permutations for three speakers is given in Figure\u00a0<ref>.\nAs the proposed training data generation yields data where the speaker permutation can be fixed according to spatial regions, PIT could converge to a permutation that preserves the region information.\nIn other words, the estimated speech signals may consistently be ordered according to the regions.\nHowever, it is unclear how PIT determines the order and whether PIT will converge to a region-dependent permutation.\nConsequently, we propose to consistently assign each region to a specific DNN output during training.\nThat way, we ensure that the DNN is trained with a region-dependent permutation.\nAdditionally, the computational load during training is reduced compared to PIT, most notably when the number of regions increases:\nWhile PIT scales factorially, the fixed mapping scales linearly with the number of regions R.\nFor example, when training with a fixed mapping for three regions, we always update the DNN with one of the six possible permutations, e.g., one shown in Figure\u00a0<ref>.\n\n\n\n\u00a7 EXPERIMENTAL SETUP\n\n\n\nThis section briefly describes the simulation of the data sets and gives the parameters of the employed DNN architecture.\n\n\nDatasets:\nTo model an in-car environment, we simulated RIR for a car-sized shoe-box room (3\u00a0m,   2\u00a0m, 1.5\u00a0m) with typical in-car reverberation times from 0.05-0.1 seconds in steps of 0.01 <cit.> (slightly exceeding values from <cit.> to accommodate for different materials like glass ceilings).\nThe room contains a ULA with M=3 microphones and 8\u00a0cm inter-microphone spacing where the central microphone was placed at (0.5\u00a0m, 1\u00a0m, 1\u00a0m).\nAdditionally, there are three distinct cuboid regions where speakers can be active (referred to as `driver' (D), `co-driver' (C), `backseats' (B)). The 'driver' and `co-driver' region-centers (cubes with an edge length of 0.5\u00a0m) were placed at (1.25\u00a0m, 0.5\u00a0m, 1\u00a0m) and (1.25\u00a0m, 1.5\u00a0m, 1\u00a0m), respectively. The `backseats' region consists of a cuboid region three times the size of one individual front region, placed 1\u00a0m behind the `driver'/`co-driver' regions (see Figure\u00a0<ref>). Fifty points were randomly sampled from the two smaller regions (D and C), 150 from the bigger one (B). Per region, the source positions were split into three disjoint subsets (60%, 20%, 20%) as basis for training, validation, and test sets, respectively.\n\nBased on source and microphone positions, RIR were simulated using <cit.>.\nThey were convolved with speech according to the Libri3Mix <cit.> description in the configurations `mix_clean' and `min' to obtain the reverberant speech signals at the microphones.\nThe sampling frequency was 16\u00a0kHz, and the length of training and validation files was four seconds. To form a single training/validation/test example, three reverberant speech signals, one per region, were added. We refer to that test set as FC. For the test, we additionally created a set with simulated microphone self-noise by adding spatio-temporal white Gaussian noise with a signal-to-noise ratio \u2208 [20,30]\u00a0dB w.r.t. all reverberant speakers.  We refer to this test set as WN. To show that the DNN can assign the speakers to the regions even if not all regions have an active speaker simultaneously, we create a third test set referred to as PO. There, the speakers start speaking successively with a delay of two seconds. The speaker order is determined randomly. Overall, the training, validation, and each test set consist of  9'300, 3'000, and 3'000 samples, respectively.\n\nModel Details:\nWe trained two copies of the SpaRSep architecture as described in Section\u00a0<ref> for 100 epochs using an initial learning rate of 15e-5 and a batch size of one sample.\nOne copy was trained using PIT (as in <cit.>), and one copy was trained using the fixed mapping as motivated and proposed in Section\u00a0<ref>, both times employing SI-SDR as loss function.\nFor the variant without PIT, we enforced the source from the `driver region' be mapped to output 1, `co-driver' to 2, `backseats' to 3.\nThe encoder was set to encode the waveform signals using a window length of 16 samples and a hop size of 8 samples, outputting a representation with F=128 features.\nThe resulting features were chunked with a chunk size of K=250 and passed through a concatenation of P=4 TPB.\nThe model comprises approximately 4.2 million parameters.\n\n\n\n\n\u00a7 PERFORMANCE EVALUATION\n\n\n\n\n\n\n\n\n\n\nFor all datasets, our performance evaluation is based on two metrics that we calculate by averaging over all 3'000 test samples per set[Audio examples can be found at <https://www.audiolabs-erlangen.de/resources/2023-ICASSP-SSBRIPS>.].\nWe report the SI-SDR <cit.> of the reconstructed estimates and the SI-SDRi over the mixture, namely both the mean over all regions and per region (denoted by, e.g., `driver' as indicated in Figure\u00a0<ref>).\nThe model comprises approximately 4.2 million parameters, and training the models on an NVIDIA A100 SXM4 reserved only for this training took 109.5 h and 107.4 h for the variants with and without PIT, respectively.\nFurther, as a multi-channel separation baseline where spatial information is not explicitly modeled, we trained an implementation of the MC ConvTasNet <cit.> to maximize the SI-SDR of the reconstructed signals in combination with PIT.\nNote that MC ConvTasNet is not a task-specific baseline, i.e., it is not designed for region-based separation. The model comprises approximately 4.9 million parameters, and training on an NVIDIA GeForce RTX2080 Ti took 53.1 h.\n\nTable <ref> shows the evaluation results for the three test sets.\nFor the FC set, which matches the training condition, MC ConvTasNet obtained an average SI-SDRi of 16.8 dB, whereas SpaRSep <cit.> obtained an average SI-SDRi of 18.3 dB for both training conditions, with PIT and with the proposed fixed permutation at the output.\nNote that SpaRSep consistently outperforms MC ConvTasNet, indicating that the triple-path structure is indeed advantageous for modeling spatial information.\nThe SI-SDRi is approximately equal for all regions, which is also illustrated in Figure\u00a0<ref> for the SpaRSep model trained with the proposed fixed mapping.\nEach dot represents the SI-SDRi per test point as described in Section\u00a0<ref>.\nThe mean SI-SDRi values per point range between 15.9 and 20.4 decibels, represented by the diameter.\nSince the input SI-SDR is higher for regions D and C compared to region B, accordingly,the output SI-SDR is also higher for regions D and C.\nThis difference stems from the mixing procedure, where the anechoic input signals are normalized and scaled as per the LibriMix definitions <cit.>, but the reverberation introduces different direct-to-reverberation ratios for the spatial regions. \n\nFurther, even for the short reverberation times of the car scenario, the backseat signals have a lower direct-to-reverberation ratio and it is challenging to recover the reverberation tail.\n\n\nThe performance is degraded by roughly 1.0 dB for the WN set, which is a condition not seen during training.\nThe model trained on noise-free data can thus be expected to generalize to real measurements, including microphone self-noise out of the box.\nFor the PO set, we see that the model trained with fixed output mapping is superior to the PIT model.\nFor both trained models, the metrics are higher by 4 - 5 dB compared to the FC set.\nSince partial overlap is a more likely scenario in a natural human conversation than fully concurrent speakers, this is a welcome finding.\n\nThe performance similarity between the models with and without PIT suggested that the PIT model may have learned a fixed output mapping.\nTable\u00a0<ref> shows the different permutations obtained.\nFor 99.8 % of the examples (for the FC set), the PIT model output a common permutation of C-B-D. \nInterpreting this as a fixed permutation, we see that the confusions happen only between regions D\u2194B and C\u2194B.\nThe output permutation is found to be only slightly sensitive to the evaluation condition.\nFor all test sets, \u226599.8 % of the samples were output in the `majority permutation'.\n\nThe fixed-mapping model generates output with the desired fixed permutation `D-C-B' most of the time (\u226599.9 %).\nFor the PO set, there were no confusions at all, indicating that spatial information was properly learned.\nNote that due to the random order of speaker onsets, this indicates that the assignment of speakers to regions also reliably works when only one source is active.\nWhen confusion occurs for the fixed-mapping model, it is found to be between one of the front regions (D, C) and the back region (B).\nIntuitively, this is expected since the possible ranges of DOA of sources from the two front regions are non-overlapping, and the proposed architecture with fixed output mapping can generate region-specific outputs.\nFor each of the two front regions, respectively, and the back region, there exists a range of DOA that is common for both regions.\nIn Figure\u00a0<ref>, these overlap regions are indicated by blue dotted lines merging at the array center.\nNote that the distinguishability of two sources located in such an overlap region is influenced by their different direct-to-reverberation ratios.\nSince random combinations of one point from every region were simulated, the mean SI-SDRi does not degrade noticeably.\n\nTo interpret the representations computed by the separation network, and especially to probe the spatial processing discussed before (i.e., the inter-channel modeling), we studied the inter-channel attention for scenarios where only one source is active in one of the regions and the sources in the other regions are silent.\nFigure\u00a0<ref> shows the average attention weights (M \u00d7 M matrices) computed in the eight heads of the inter-channel transformer of the first TPB, which is also the first transformer layer in the network, directly after the encoder.\nThe attention weights are averaged over all time frames of 10 different test samples.\nWe see that irrespective of the active source region, the average attention weights have a fixed pattern, i.e., the network computes representation based on fixed spatial processing in the first inter-channel transformer layer.\nThis behavior is observed for the model trained with fixed output mapping as well as the model trained with PIT.\nSpecific patterns computed by a few heads are also similar between the two models, e.g., head 8 of the fixed-order model and head 2 of the PIT model.\nWe interpret this behavior as a sort of signal-independent beamforming, where the network learns a fixed spatial pre-processing of the encoded signals before signal-dependent separation is performed.\nThe inter-channel transformers in the other TPB are found to compute active source region-specific attentions.\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nWe formulated the task of source separation that preserves region information. We proposed to address that task by a special training data generation scheme and by training with fixed correspondences of regions to separated sources on the network output. The employed transformer-based network architecture was shown to learn spatial features. We conclude that networks can be trained to separate sources and preserve regional information.\n\n\n\n\u00a7 ACKNOWLEDGMENT\n\n\nThe authors gratefully acknowledge the scientific support and HPC resources provided by the Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg (FAU).\n\n\n[SI-SDR]\n DNNdeep neural network\n DOAdirection-of-arrival\n DOAdirections-of-arrival\n \n FCfull concurrent test set\n \n MC ConvTasNetmulti-channel extension of ConvTasNet\n MIMOmultiple-input multiple-output\n \n PITpermutation invariant training\n POtest set with partial overlap\n PReLUParameterized ReLU\n \n ReLUrectified linear unit\n RIRroom impulse response\n \n SI-SDRscale-invariant signal-to-distortion ratio\n SI-SDRiimprovement in SI-SDR\n SpaRSepseparator network based on spatial regions\n \n TFtime-feature\n TPBtriple-path block\n \n ULAuniform linear array\n \n WNfull concurrent with white noise test set\n\n\n\n\n\n\n\n\n\nIEEEbib\n\n\n"}