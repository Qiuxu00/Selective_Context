{"entry_id": "http://arxiv.org/abs/2303.07096v1", "published": "20230313133059", "title": "Prototype-based Embedding Network for Scene Graph Generation", "authors": ["Chaofan Zheng", "Xinyu Lyu", "Lianli Gao", "Bo Dai", "Jingkuan Song"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrototype-based Embedding Network for Scene Graph Generation\n    \nChaofan ZhengEqual contribution.\nXinyu Lyu^\u2217 \nLianli GaoCorresponding author. \nBo Dai\nJingkuan Song \n\n\n\n\n\n\n School of Computer Science and Engineering, \nUniversity of Electronic Science and Technology of China, China\n\n mailto:zheng_chaofan@foxmail.comzheng_chaofan@foxmail.com,\nmailto:lianli.gao@uestc.edu.cnlianli.gao@uestc.edu.cn\n\n\n    \n==================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurrent Scene Graph Generation (SGG) methods explore contextual information to predict relationships among entity pairs.\nHowever, due to the diverse visual appearance of numerous possible subject-object combinations, \nthere is a large intra-class variation within each predicate category, , \u201cman-eating-pizza, giraffe-eating-leaf\u201d, \nand the severe inter-class similarity between different classes, , \u201cman-holding-plate, man-eating-pizza\u201d, in model's latent space. \nThe above challenges prevent current SGG methods from acquiring robust features for reliable relation prediction.\nIn this paper, we claim that the predicate's category-inherent semantics can serve as class-wise prototypes in the semantic space for relieving the challenges.\n\nTo the end, we propose the Prototype-based Embedding Network (PE-Net), which models entities/predicates with prototype-aligned compact and distinctive representations and thereby establishes matching between entity pairs and predicates in a common embedding space for relation recognition.\nMoreover, Prototype-guided Learning (PL) is introduced to help PE-Net efficiently learn such entity-predicate matching, and Prototype Regularization (PR) is devised to relieve the ambiguous entity-predicate matching caused by the predicate's semantic overlap.\nExtensive experiments demonstrate that our method gains superior relation recognition capability on SGG, achieving new state-of-the-art performances on both Visual Genome and Open Images datasets. The codes are available at\n<https://github.com/VL-Group/PENET>. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\nScene Graph Generation (SGG) is a fundamental computer vision task that involves detecting the entities and predicting their relationships in an image to generate a scene graph, where nodes indicate entities and edges indicate relationships between entity pairs. Such a graph-structured representation is helpful for downstream tasks such as Visual Question Answering\u00a0<cit.>, Image Captioning\u00a0<cit.>, and Image Retrieval\u00a0<cit.>. \n\n\n\n\n\nExisting SGG models\u00a0<cit.> typically start with an object detector that generates a set of entity proposals and corresponding features. Then, entity features are enhanced by exploring the contextual information taking advantage of message-passing modules. Finally, these refined entity features are used to predict pairwise relations. \nAlthough many works have made great efforts to explore the contextual information for robust relation recognition, they still suffer from biased-prediction problems, preferring common predicates (,\u201con\u201d, \u201cof\u201d) instead of fine-grained ones (,\u201cwalking on\u201d, \u201ccovering\u201d).\n\nTo address the problem, various de-biasing frameworks\u00a0<cit.> have been proposed to obtain balanced prediction results. \nWhile alleviating the long-tailed issue to some extent, most of them only achieve a trade-off between head and tail predicates.\nIn other words, they sacrifice the robust representations learned on head predicates for unworthy improvements in the tail ones\u00a0<cit.>, which do not truly improve the model's holistic recognition ability for most of the relations.  \n\n\n\n\n\n\n\n\n\n\nThe origin of the issue lies in the fact that current SGG methods fail to capture compact and distinctive representations for relations. \nFor instance, as shown in Fig.\u00a0<ref>, the relation representation, derived from Motifs' latent space, is heavily discrete and intersecting. \n\nHence, it makes existing SGG models hard to learn perfect decision boundaries for accurate predicate recognition. \nAccordingly, we summarize the issue as two challenges: large Intra-class variation within the same relation class and severe Inter-class similarity between different categories.\n\nIntra-class variation. The intra-class variation arises from the diverse appearance of entities and various subject-object combinations.\n\nSpecifically, entities' visual appearances change greatly even though they belong to the same class.\nThus, represented as the union feature containing subject and object entities, relation representations significantly vary with the appearances of entity instances, , various visual representations for \u201cpizza\u201d in Fig.\u00a0<ref>(c) vs. Fig.\u00a0<ref>(d).\nBesides, the numerous subject-object combinations of predicate instances further increase the variation within each predicate class, , \u201cman-eating-pizza\u201d vs. \u201cgiraffe-eating-leaf\u201d in Fig.\u00a0<ref>(c) and Fig.\u00a0<ref>(e). \n\n\n\n\n\nInter-class similarity. The inter-class similarity of relations originates from similar-looking interactions but belongs to different predicate classes. For instance, as shown in Fig.\u00a0<ref>(a) and Fig.\u00a0<ref>(c), the similar visual appearance of interactions between \u201cman-pizza\u201d and \u201cman-plate\u201d make current SGG models hard to distinguish \u201ceating\u201d from \u201cholding\u201d, even if they are semantic irrelevant to each other.\n\nThe above challenges motivate us to study two problems:\n\n1) For the intra-class variation, how to capture category-inherent features, producing compact representations for entity/predicate instances from the same category.\n\n\n\n\nMoreover, 2) for the inter-class similarity, how to derive distinctive representations for effectively distinguishing similar-looking relation instances between different classes.  \n\n\n\nOur key intuition is that semantics is more reliable than visual appearance when modeling entities/predicates.\n\n\nIntuitively, although entities/predicates of the same class significantly vary in visual appearance, they all share the representative semantics, which can be easily captured from their class labels.\n\nDominated by the representative semantics, the representations of entities and predicates have smaller variations within their classes in the semantic space.\n\nBesides, the class-inherent semantics is discriminative enough for visual-similar instances between different categories. \n\n\n\n\n\nTherefore, in conjunction with the above analysis, modeling entities and predicates in the semantic space can provide highly compact and distinguishable representations against intra-class variation and inter-class similarity challenges.\n\n\n\n\n\n\n\n\n\nInspired by that, we propose a simple but effective method, Prototype-based Embedding Network (PE-Net), which produces compact and distinctive entity/predicate representations for relation recognition. \nTo achieve that, the PE-Net models entity and predicate instances with compact and distinguishable representations in the semantic space, which are closely aligned to their semantic prototypes.\nPractically, the prototype is defined as the representative embedding for a group of instances from the same entity/predicate class. \n\n\n\nThen, the PE-Net establishes matching between entity pairs (, subject-object (s, o)) and their corresponding predicates (p) for relation recognition \n\n(, \u2131(s, o) \u2248p).\nBesides, a Prototype-guided Learning strategy (PL) is proposed to help PE-Net efficiently learn this entity-predicate matching. \n\nAdditionally, to alleviate the ambiguous entity-predicate matching caused by the semantic overlap between predicates (, \u201cwalking on\u201d and \u201cstanding on\u201d), Prototype Regularization (PR) is proposed to encourage inter-class separation between predicate prototypes for precise entity-predicate matching.\n\n\n\n\n\n\n\n\nFinally, we introduce two metrics, , Intra-class Variance (IV) and Intra-class to Inter-class Variance Ratio (IIVR), to measure the compactness and distinctiveness of entity/predicate representations, respectively.\n\n\n\n\nIn summary, the main contributions of our work are three folds:\n\n    \n    \n    \n  * We propose a simple yet effective method, , Prototype-based Embedding Network (PE-Net), which produces compact and distinctive entity/predicate representations and then establishes matching between entity pairs and predicates for relation recognition.\n    \n    \n  * Moreover, Prototype-guided Learning (PL) is introduced to help PE-Net efficiently learn such entity-predicate matching, and Prototype Regularization (PR) is devised to relieve the ambiguous entity-predicate matching caused by the predicate's semantic overlap.\n    \n    \n  * Evaluated on the Visual Genome and Open Images datasets, our method significantly increases the relation recognition ability for SGG, achieving new state-of-the-art performances.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\nWe categorize the related works of SGG into the following fields: Vanilla Scene Graph Generation Model and Unbiased Scene Graph Generation Framework. \n\nVanilla Scene Graph Generation Model. Numerous models have been proposed to solve the scene graph generation task from different perspectives in recent years. Early methods\u00a0<cit.> attempted to detect objects and relations with independent networks, ignoring the rich contextual information. Afterward, \u00a0<cit.> firstly proves that the contextual information can significantly improve the relation prediction and hence introduces an iterative message-passing mechanism to refine the features of objects and relations. \u00a0<cit.> further emphasizes the importance of contextual information between objects and utilizes the BiLSTM to encode the object and edge contextual information. Moreover, to avoid suffering from noisy information during message passing,\u00a0<cit.> and\u00a0<cit.> design sparse structures to improve the model's context modeling capability. \nIn addition, prior knowledge is also helpful for relation prediction. \u00a0<cit.> explores the statistical patterns of object co-occurrence for refining relation predictions. Besides, \u00a0<cit.> encodes the commonsense knowledge into the model to improve the few-shot recognition ability. \nHowever, due to the imbalanced data distribution, the vanilla SGG models struggle to recognize the fine-grained tail predicates.\n\nUnbiased Scene Graph Generation Framework.\nRecently, various de-biasing SGG frameworks have been proposed to tackle the biased predictions problem. \u00a0<cit.> proposes a counterfactual causality method to remove the effect of context bias. \u00a0<cit.> constructs a hierarchical tree structure from the cognitive perspective to make the tail predicates receive more attention. \u00a0<cit.> compensates the disadvantages of over-sampling and under-sampling and proposes a bi-level sampling method. \u00a0<cit.> creates a balanced learning process by constructing a balanced predicate learning space and semantic adjustment. \u00a0<cit.> explicitly cleans the noisy annotations on the datasets to balance the data distribution. \u00a0<cit.> introduces a predicate lattice to figure out the fine-grained predicate pairs that are hard to distinguish. Despite alleviating the biased problem to some extent, these methods improve the prediction performance of tail predicates at the expense of head ones, which do not truly improve the model\u2019s holistic recognition ability.\n\nOur work generates compact and distinctive entity/predicate representations by utilizing a prototype-based modeling method and cleverly-designed learning strategies, which achieves superior relation recognition performance on both head predicates and tail ones with a simple but effective framework. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 METHOD\n\n\n\n\n\n\n\nThe whole pipeline of our Prototype-based Relation Embedding (PE-Net) is illustrated in\u00a0<ref>.\nFollowing the previous works\u00a0<cit.>, we utilize an object detector (, Faster R-CNN\u00a0<cit.>) to generate a set of entity proposals with corresponding features.\nMoreover, the features extracted from the union box between two entities are used to represent their corresponding predicates.\nGiven entity and predicate features, the Prototype-based Embedding Network (PE-Net)\n\nmodels subject (s), object (o), and predicate (p) instances with prototype-based compact and distinguishable representations.\nThen, the PE-Net matches subject-object pairs ((s, o)) with the corresponding predicates (, \u2131(s, o) \u2248p) in the common embedding space for relation recognition.\nTo achieve that, we propose a Prototype-guided Learning (PL), to help PE-Net learn the entity-predicate matching. \nFurthermore, to relieve the ambiguous matching problem caused by the predicate's semantic overlap, Prototype Regularization (PR) is proposed to encourage inter-class distinction for accurate entity-predicate matching. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Prototype-based Embedding Network\n\n\nThe procedure of Prototype-based Embedding Network (PE-Net) can be divided into two steps: 1) Prototype-based Modeling for producing compact and distinctive entity and predicate representations. 2) Prototype-guided Entity-Predicate Matching for relation recognition.\n\nPrototype-based Modeling.\nThe Prototype-based Embedding Network (PE-Net) models entity/predicate instances with prototype-based compact and discriminative representations shown in Fig.\u00a0<ref>.\n\nConcretely, the representations of subject (s), object (o), and predicate (p) are modeled below:\n\n    s    = \ud835\udc16_s t_s + v_s, \n    o    = \ud835\udc16_o t_o + v_o, \n    p    = \ud835\udc16_p t_p + u_p,\n\nwhere \ud835\udc16_\ud835\udc2c, \ud835\udc16_\ud835\udc28, and \ud835\udc16_\ud835\udc29 are learnable parameters.\nMoreover, \ud835\udc16_s t_s, \ud835\udc16_o t_o and \ud835\udc16_p t_p are the class-specific semantic prototypes obtained from their class labels' word embedding (GloVe\u00a0<cit.>), , t_s, t_o and t_p. \nBased on the class-specific prototypes, the instance-varied semantic contents v_s, v_o and u_p are utilized to model the diversity of each instance from the same subject, object, and predicate class.\nPractically,  v_s are obtained as:\n\n    g_s    = \u03c3(f((\ud835\udc16_\ud835\udc2ct_s) \u2295 h(x_s))) \n    v_s    = g_s\u2299 h(x_s),\n\nwhere f(\u00b7) is a fully connected layer, h(\u00b7) is the visual-to-semantic function used to transform the visual feature into semantic space, and \u2295 is the concatenation operation. Moreover, \u03c3(\u00b7) is the sigmoid activation function, \u2299 is the element-wise product, and x_s is the visual features of subject instances from the detector.\nUtilizing the gate mechanism in <ref>, the class-irrelevant information is eliminated from the original visual feature x_s producing consistent representations within class. In addition, we derive v_o in the same way following <ref>. \n\nSimilarly, predicate's instance-varied semantic content u_p is defined as:\n\n    g_p    = \u03c3( f( \u2131(s,o) \u2295 h(x_u)  )  ) \n    u_p    = g_p\u2299 h(x_u),\n\nwhere x_u is the union feature of subject and object, and \u2131(\u00b7, \u00b7) denotes the feature fusion function. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrototype-guided Entity-Predicate Matching.\nThen, for relation recognition, we match subject instance (s) and object instance (o) with the corresponding predicate instance (p) in the common semantic space.\nPractically, the entity-predicate matching is shown below:\n\n    \u2131(s, o) \u2248p,\n\nwhere \u2131(s, o) is defined as: ReLU(s+o) - (s-o)^2.\n\nHowever, the predicate representation varies with the subject-object pair, which prevents PE-Net from efficiently learning the matching.\nTherefore, we perform an equivalent transformation on <ref>, deriving a deterministic matching objective as follows:\n\n    \u2131(s, o) - u_p\u2248\ud835\udc16_p t_p,\n\nwhere \u2131(s, o) - u_p is defined as relation representation r, which should be matched to its corresponding predicate prototype \ud835\udc16_p t_p (represented as c in the following sections).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Prototype-guided Learning\n\n\n\nTo help PE-Net efficiently match relation representations with corresponding predicates in <ref>, we devise a learning strategy, , Prototype-guided Learning (PL), which makes relation representations close to their corresponding prototypes. In practice, PL consists of two constraints: cosine similarity and Euclidean distance.\n\nFirstly, we have to increase the cosine similarity between the relation representation r and its corresponding prototype c_t, which is implemented as the following loss function:\n\n    \u2112_e_sim =  -   logexp(\u27e8r, c_t\u27e9 / \u03c4)/\u2211_j=0^Nexp(\u27e8r, c_j\u27e9 / \u03c4),\n\nwhere \u00b7 denotes the unitary operation, \n\u03c4 is a learnable temperature hyper-parameter, t is subscript for the ground truth class, and N is the number of predicate categories. \n\nSince the cosine similarity only considers angle-based relative distance, it may fail to make relation representations and corresponding prototypes close to each other in the Euclidean space.\nTo the end, we further impose the Euclidean distance constraint.\nIt encourages the relation representation r close to its corresponding prototypes c_t while keeping the distances with others in Euclidean space.\nPractically, we first calculate the distances between the relation representation r and each class prototype c_i obtaining the distances set G = { g_i}_i=0^N, where g_i is computed as:\n\n    g_i = r - c_i_2^2.\n\nThen, we sort the distance set B = G\u2216{g_t} (excluding g_t) in increasing order and obtain the sorted distance set as B' = {b'_i} _i = 0^N-1.\nFurthermore, the top k_1 smallest distances of B' are averaged as the distance g^- to negative prototypes:\n\n    g^- = 1/k_1\u2211_i=0^k_1-1 b'_i.\n\nTogether with the distance to the positive prototype g^+ = g_t, we further construct the triplet loss:\n\n    \u2112_e_euc = max(0, g^+ - g^- + \u03b3_1),\n\nwhere \u03b3_1 is a hyper-parameter to adjust the distance margins between relation representations and the negative prototypes.\n\n\n\n \u00a7.\u00a7 Prototype Regularization\n\nTo alleviate the ambiguous matching caused by the semantic overlap between predicates, we propose a Prototype Regularization (PR) to encourage inter-class separation by enlarging the distinction between prototypes for precise entity-predicate matching.\n\n\n\n\n\nCorrespondingly, according to the constraints imposed in <ref>, we first calculate the cosine similarity between predicate prototypes obtaining the similarity matrix as follows:\n\n    S = C\u00b7C^\u22a4 = (s_ij) \u2208\u211d^(N+1) \u00d7 (N+1),\n\nwhere C=[c_0; c_1;...; c_N] is the predicate prototype matrix, and\nC is obtained by normalizing the vectors in it. Moreover, s_ij represents the cosine similarity between prototype c_i and c_j.\n\nThen, we should reduce each pair of prototypes' cosine similarity to make them distinctive in the semantic space.\n\n\nTherefore, we introduce the l_2,1-norm of S and minimize it:\n\n    \u2112_r_sim = S_2,1= \u2211_i=0^N\u221a(\u2211_j=0^N s_ij^2).\n\n\n\nHowever, only regularized by the cosine similarity, some predicates are still not distinctive enough against others. Thus, we enlarge their distances in the Euclidean space for further distinction. \n\nTo achieve that, we calculate the Euclidean distance between two prototypes obtaining the distance matrix \nD = (d_ij) \u2208\u211d^(N+1) \u00d7 (N+1) with d_ij computed as:\n\n    d_ij = c_i - c_j_2^2,\n\nwhere d_ij indicates the Euclidean distance between prototype c_i and c_j.\n\n\n\n\n\n\nFor each prototype, we should distance it from others. \nTherefore, we sort the elements in each row of matrix D in increasing order, obtaining D' =  (d'_ij) \u2208\u211d^(N+1) \u00d7 (N+1), and select the top k_2 smallest values of each row to widen them:\n\n    d^-     = 1/(N+1)k_2\u2211_i=0^N\u2211_j=1^k_2 d'_ij, \n    \u2112_r_euc    = max (0, - d^- + \u03b3_2),\n\nwhere \u03b3_2 is another hyper-parameter used to adjust the distance margins.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Scene Graph Prediction\n\n\nDuring the training stage, the final loss function \u2112 for our PE-Net is expressed as:\n\n    \u2112 = \u2112_r_sim + \u2112_e_sim + \u2112_r_euc + \u2112_e_euc .\n\n\nDuring the testing stage, with the relation representation r, we choose the class of prototypes with the highest cosine similarity as the prediction result: \n\n    res_r = iarg max ({q_i | q_i = \u27e8r, c_i\u27e9 / \u03c4}),\n\nwhere q_i indicates the similarity between relation representation r and prototype c_i. \n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Datasets\n\n\nVisual Genome (VG).  The Visual Genome (VG) dataset consists of 108,077 images with average annotations of 38 objects and 22 relationships per image. In this paper, we adopt the most widely used split\u00a0<cit.>, which contains the most frequent 150 object categories and 50 predicate categories.\nSpecifically, the dataset is divided into a training set with 70% of the images, a testing set with the remaining 30%, and 5k images from the training set for validation. \n\nOpen Images (OI).\nWe conduct experiments on Open Image V6 dataset, which has 126,368 images for training, and 1813 and 5322 images for validation and testing. It contains 301 object categories and 31 predicate categories. \n\n\n\n\n \u00a7.\u00a7 Evaluation Protocol\n\nVisual Genome (VG).\nWe evaluate our method on three sub-tasks, including Predicate Classification (PredCls), Scene Graph Classification (SGCls), and Scene Graph Detection (SGDet).\n\n\n\nFollowing the recent works\u00a0<cit.>, we take Recall@K (R@K) and mean Recall@K (mR@K) as the primary evaluation metrics.\n\nMoreover, we also report the zero-shot Recall@K (zs-R@K) that measures the model's generalization in dealing with the unseen relation triplets during training. \n\n\n\n\nDue to the imbalanced data distribution of VG dataset, R@K focuses on the common predicates with abundant samples, and mR@K prefers the tail predicates.\nTherefore, we introduce the Mean@K (M@K), which averages the R@K and mR@K for evaluating the model's overall performance on SGG.\nIn addition, the Intra-class Variance (IV) and Intra-class to Inter-class Variance Ratio (IIVR) are introduced to measure the compactness and distinctiveness of entity/predicate representations. Intuitively, lower values of IV and IIVR indicate higher quality for representations. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Images (OI).\nFollowing the previous works\u00a0<cit.>, we utilize the Recall@50 (R@50), weighted mean AP of relations (wmAP_rel), weighted mean AP of phrase (wmAP_phr) as the evaluation metrics. The score_wtd is calculated as:\nscore_wtd = 0.2 \u00d7R@50 + 0.4 \u00d7wmAP_rel + 0.4 \u00d7wmAP_phr. \n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n \u00a7.\u00a7 Implementation Details\n\n\nFollowing the previous works\u00a0<cit.>, we adopt the Faster R-CNN\u00a0<cit.> with ResNeXt-101-FPN\u00a0<cit.> provided by\u00a0<cit.> to detect objects in the image. The parameters of the detector are frozen during training. \nIn particular, we set k_1, k_2, \u03b3_1, and \u03b3_2 as 10, 1, 1, and 7.\nAdditionally, the PE-Net is trained by an SGD optimizer with 60k iterations. The initial learning rate and the batch size are set to 10^-3 and 8. All experiments are implemented with PyTorch and trained with an NVIDIA GeForce RTX 3090 GPU. \n\n\n\n \u00a7.\u00a7 Comparisons with State-of-the-art Methods\n\n\n\nVisual Genome.\nTo evaluate PE-Net's capability on scene graph generation, we compare it with several state-of-the-art SGG methods on Visual Genome dataset under all three sub-tasks. The results are shown in <ref>. \n\n\n\n\n\n\nGenerally, our method achieves superior performance compared to other SGG models.\n\n\nConcretely, PE-Net(P) outperforms the VCTree by 7.5%, 6.5%, and 3.7% at mR@100 and by 2.7%, 0.7%, and 0.9% at R@100 on PredCls, SGCls, and SGDet.\nIt also outperforms the recent SGG model, RU-Net, by 0.5% and 1.2% at R@100 and mR@100 on PredCls.\nIn addition, the full PE-Net outperforms \n\nVCTree by 15.9%, 10.6%, 7.2%,  \nand RU-Net by 9.6%, 4.3%, 3.7%,\nat mR@100 on three subtasks, respectively. \nThe results demonstrate the effectiveness of our model.\n\n\n\n\n \n\nMoreover, to explore PE-Net's potential capability of solving the long-tail problem for SGG, we equip it with the advanced re-weighting method\u00a0<cit.>.\n\nThen, we compare PE-Net-Reweight with Motifs\u00a0<cit.> de-biased by several existing state-of-the-art de-biasing methods.\n\nThe results are summarized in <ref>.\nWe find that our PE-Net-Reweight pushes the performance on unbiased SGG to a new level.\n\nFor instance, compared with Motifs-GCL, we achieve an absolute performance advantage, outperforming it by 17.0%, 10.2%, and 8.9% at R@100 on PredCls, SGCls, and SGDet tasks, and by 2.5%, 1.7% at mR@100 on PredCls, SGCls tasks, respectively.\nBenefiting from the prototype-aligned distinctive representations, the PE-Net has the potential to tackle the biased problem in SGG.\n\n\nIn addition, we report the zero-shot recall results to verify the generalization of our method in handling the unseen relation triplets in the training set. \nAs shown in <ref>, our model outperforms the vanilla Motifs and VCTree by 15.53%, 5.40%, 3.49%, and 15.38%, 4.45%, 2.91% at zs-R@100 on PredCls, SGCls, and SGDet. \nAlthough TDE significantly improves the zero-shot performance by removing the effect of context bias, Motifs-TDE and VCTree-TDE are still surpassed by our PE-Net with 2.69%, 2.03%, 0.7%, and 3.29%, 2.53%, 0.4% on three tasks, respectively. \nWe owe the strength to the Prototype-based Modeling of our PE-Net, which models entity and predicate in the semantic space, significantly improving the model's analogical reasoning capability on unseen relation triplets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Images.\nTo verify the generality of our method on different datasets, we conduct experiments on Open Images and present the results in <ref>. \nConsistent with the performance on VG, PE-Net also achieves competitive results on Open Images dataset.\nSpecifically, our method exceeds the BGNN with a large margin of 2.7% on average at four metrics, and outperforms RU-Net by 1.2%, 2.5%, and 1.4% at wmAP_rel, wmAP_phr, and score_wtd, respectively. \nIt powerfully confirms PE-Net's generalization on handling relation recognition under different data distributions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Measuring Representation Modeling of PE-Net\n\n\nTo certify the assumption that our PE-Net is capable of producing compact and distinctive representations for entity and predicate, we conduct both quantitative and qualitative studies in <ref> and <ref>, respectively.\nNotably, we only conduct experiments on the PredCls task, which eliminates the impact of entities' mis-classification made by detectors. \n\nQuantitative Analysis.\nTo quantitatively evaluate the quality of the entity's and predicate's representations (, degree of intra-class compactness and the inter-class distinctiveness), we evaluate and make comparisons between PE-Net and previous methods\u00a0<cit.> with IV (Intra-class Variance) and IIVR (Intra-class to Inter-class Variance Ratio). Moreover, the experimental results are shown in <ref>.\n\n\n\n\n\n\n\n\n\n\n\n\nFirstly, our PE-Net yields more compact entity and predicate representations than previous methods, , 0.74 9.73 on IV-O and 1.06 1.41 on IV-R compared with Motifs. \n\nThat illustrates the effectiveness of our Prototype-based Modeling in PE-Net. \n\nAlso, the representations learned by our model are more distinguishable, , 0.24 1.93 on IIVR-O and 1.67 2.72 on IIVR-R compared with Motifs. \n\nWe owe it to the effectiveness of our PR, which significantly alleviates the ambiguous entity-predicate matching by encouraging predicate prototypes away from each other. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQualitative Analysis.\nFor an intuitive illustration of PE-Net's capability of yielding compact and distinguishable representations, we visualize the feature distribution of entities and predicates taking advantage of the t-SNE technique, shown in <ref>. \n\nComparing <ref>(a) with <ref>(b), we observe that PE-Net produces more compact and distinctive entity representations than Motifs, intuitively illustrating the advantages of modeling instances in the semantic space than from visual appearances. \n\n\nIn addition, the relation feature distribution of Motifs shown in <ref>(c) is of large intra-class variance and severe inter-class overlap. In this case, it is hard for SGG models to learn a perfect decision boundary for accurate relation recognition. \nOn the contrary, the relation representations learned by our PE-Net are of high-level inter-class distinctiveness and intra-class compactness, which intuitively demonstrates our method's superiority and explains why our method achieves excellent relation prediction performance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Ablation Studies\n\n\n\n\n\nTo verify the effectiveness of each component of the proposed PE-Net, we conduct ablation studies on PL and PR under the VG dataset, and the results are summarized in <ref>. \nExp 1, PE-Net is trained without PL and PR, which directly uses a linear classifier to classify the relation representation defined in <ref>. \nExp 2, PE-Net is trained with PL, which discards the \u2112_r_sim and \u2112_r_euc in <ref>. \nExp 3, PE-Net is trained with both PL and PR, , <ref>.\n\n\n\n\nWhen constrained by PL in Exp 2, the model outperforms the baseline (, Exp 1) on all metrics under three sub-tasks (, 25.4% 20.0% at mR@100, and 70.1% 68.2% at R@100 on PredCls).\n\nThis verifies that PL effectively helps PE-Net to establish matching between entities and predicates for accurate relation recognition.\n\n\nFurthermore, after being integrated with PR in Exp 3, our PE-Net obtains significant gains on mR@K (, 33.8% 25.4% at mR@100 on PredCls), which demonstrates PR's effectiveness in enlarging the distinction between prototypes achieving reliable entity-predicate matching.\nHowever, we observe that the improvement of mR@K brings a slight drop at R@K. \nThat is because our model can reasonably classify some predicates (head classes) into their corresponding fine-grained ones (tail classes), , from \u201con\u201d to \u201cstanding on/laying on/walking on\u201d. And the drops of Recall on those head predicates are inevitable, which is also observed in fine-grained classification\u00a0<cit.> and long-tailed tasks\u00a0<cit.>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Visualization Results\n\n\n\n\n\n\nTo verify that our proposed PE-Net is capable of making reliable relation recognition, we make a comparison between scene graphs generated by Motifs\u00a0<cit.> (in purple) and our method (in green) in <ref>. \n\nIn the first example, our method predicts informative relations such as \u201ccar-parked on-street\u201d and \u201cbuilding1-across-street\u201d instead of \u201ccar-on-street\u201d and \u201cbuilding1-near-street\u201d.\nSimilarly, in the second example, our method generates fine-grained predicates, , \u201cwearing\u201d and \u201criding\u201d.\nThese results demonstrate that our method has a stronger predicate recognition ability than Motifs,\nwhich generates accurate relations for comprehensive scene understanding.\n\n\n\n\n\u00a7 CONCLUSION\n  \nIn this work, we propose a novel Prototype-based Embedding Network (PE-Net), which produces compact and distinctive entity/predicate representations for SGG task. Towards this end, the PE-Net models entity and predicate instances with prototype-based representations and then matches entity pairs with predicates for relation recognition. Moreover, we propose a Prototype-guided Learning strategy (PL) and Prototype Regularization (PR) to help PE-Net efficiently learn entity-predicate matching. Finally, our method achieves new state-of-the-art performances on both Visual Genome and Open Images datasets, which demonstrates the effectiveness of our methods.\n\nBroader Impact and Limitations.\n\nOur work presents a powerful and efficient SGG method, which predicts relations between entities without message-passing module. The merit greatly reduces the computational complexity and enables SGG to be widely used in real-world applications, such as autonomous driving and intelligent robotics. However, our method is sensitive to the detector's recognition ability for entities, which limits its performance on SGCls and SGDet subtasks. Therefore, a more robust modeling method should be explored in further work.\n\nAcknowledgment.\n\nThis study is supported by grants from National Key R&D Program of China (2022YFC2009903/2022YFC2009900), the National Natural Science Foundation of China (Grant No. 62122018, No. 62020106008, No. 61772116, No. 61872064), Fok Ying-Tong Education Foundation(171106), SongShan Laboratory YYJC012022019, and Open Research Projects of Zhejiang Lab (No. 2019KD0AD01/011).\n\n\nieee_fullname\n\n\n\n"}