{"entry_id": "http://arxiv.org/abs/2303.07033v1", "published": "20230313114724", "title": "SelfPromer: Self-Prompt Dehazing Transformers with Depth-Consistency", "authors": ["Cong Wang", "Jinshan Pan", "Wanyu Lin", "Jiangxin Dong", "Xiao-Ming Wu"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\nSelfPromer: Self-Prompt Dehazing Transformers with Depth-Consistency\n    Cong Wang^1, Jinshan Pan^2, Wanyu Lin^1, Jiangxin Dong^2, Xiao-Ming Wu^1 \n\n^1The Hong Kong Polytechnic University, ^2Nanjing University of Science and Technology\n\n{supercong94;sdluran;dongjxjx}@gmail.com, {wan-yu.lin;xiao-ming.wu}@polyu.edu.hk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    March 30, 2023\n=============================================================================================================================================================================================================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis work presents an effective depth-consistency self-prompt Transformer for image dehazing. It is motivated by an observation that the estimated depths of an image with haze residuals and its clear counterpart vary. Enforcing the depth consistency of dehazed images with clear ones, therefore, is essential for dehazing. For this purpose, we develop a prompt based on the features of depth differences between the hazy input images and corresponding clear counterparts that can guide dehazing models for better restoration. Specifically, we first apply deep features extracted from the input images to the depth difference features for generating the prompt that contains the haze residual information in the input. Then we propose a prompt embedding module that is designed to perceive the haze residuals, by linearly adding the prompt to the deep features. Further, we develop an effective prompt attention module to pay more attention to haze residuals for better removal. By incorporating the prompt, prompt embedding, and prompt attention into an encoder-decoder network based on VQGAN, we can achieve better perception quality. As the depths of clear images are not available at inference, and the dehazed images with one-time feed-forward execution may still contain a portion of haze residuals, we propose a new continuous self-prompt inference that can iteratively correct the dehazing model towards better haze-free image generation. Extensive experiments show that our method performs favorably against the state-of-the-art approaches on both synthetic and real-world datasets in terms of perception metrics including NIQE, PI, and PIQE.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\nRecent years have witnessed advanced progress in image dehazing due to the development of deep dehazing models.\n\nMathematically, the haze process is usually modeled by an atmospheric light scattering model\u00a0<cit.> formulated as:\n\n\n\n    I(x) = J(x)T(x) + (1-T(x))A,\n\n\n\n\n\nwhere I and J denote a hazy and haze-free image, respectively, and A denotes the global atmospheric light, x denotes the pixel index, and the transmission map T is usually modeled as T(x) = e^-\u03b2d(x) with the scene depth d(x), and the scattering coefficient \u03b2 reflects the haze density.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost existing works develop various variations of deep Convolutional Neural Networks (CNNs) for image dehazing\u00a0<cit.>.\nThey typically compute a sequence of features from the hazy input images and directly reconstruct the clear ones based on the features, which have achieved state-of-the-art results on benchmarks\u00a0<cit.> in terms of PSNRs and SSIMs.\n\nHowever, as dehazing is ill-posed, very small errors in the estimated features may degrade the performance.\nExisting works propose to use deep CNNs as image priors and then restore the clear images iteratively. However, they cannot effectively correct the errors or remove the haze residuals in the dehazed images as these models are fixed in the iterative process\u00a0<cit.>.\n\n\n\nIt is noteworthy that the human visual system generally possesses an intrinsic correction mechanism that aids in ensuring optimal results for a task. This phenomenon has been a key inspiration behind the development of a novel dehazing approach incorporating a correction mechanism that guides deep models toward better haze-free results generation.\n\n\n\n\n\n\nSpecifically, if a dehazed result exists haze residuals, a correction mechanism can localize these regions and guide the relevant task toward removing them.\nNotably, NLP-based text prompt learning has shown promise in guiding the models by correcting the predictions\u00a0<cit.>.\nHowever, text-based prompts may not be appropriate for tasks that require solely visual inputs without accompanying text.\nRecent works\u00a0<cit.> attempted to address this issue by introducing text-free prompts into vision tasks.\nFor instance, PromptonomyViT\u00a0<cit.> evaluates the adaptation of multi-task prompts such as depth, normal, and segmentation to improve the performance of the video Transformers.\nNevertheless, these prompts may not be suitable for image dehazing tasks, as they could not capture the haze-related content.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo better guide the deep model for better image dehazing, this work develops an effective self-prompt dehazing Transformer. Specifically, it explores with the depth consistency of hazy images and their corresponding clear ones as a prompt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn particular, our study is motivated by the substantial difference between the estimated depths of hazy images and their clear counterparts, , the same scene captured in the same location should be consistent regarding depth.\n\nDepth is typically related to the transmission map in the atmospheric light scattering model as shown in\u00a0(<ref>).\nThus, if the dehazed images can be reconstructed accurately, their estimated depths should be close to those of their clear counterparts at large.\nHowever, haze residuals often degrade the accuracy of depth estimation, resulting in significant differences between hazy and clear images, as illustrated in Fig.\u00a0<ref>(e).\nYet, the difference map of estimated depths from images with haze residuals and clear images often points to the regions affected by haze residuals.\n\nBased on the above observation, we design a prompt to guide the deep models for perceiving and paying more attention to haze residuals.\nOur prompt is built upon the estimated feature-level depth differences, of which the inconsistent regions can reveal haze residual locations for deep models correction.\n\n\n\n\nOn top of the prompt, we introduce a prompt embedding module that linearly combines input features with the prompt to better perceive haze residuals.\nFurther, we propose a prompt attention module that employs self-attention guided by the prompt to pay more attention to haze residuals for better haze removal.\n\nOur encoder-decoder architecture combines these modules using VQGAN\u00a0<cit.> to enhance the perception quality of the results, as opposed to relying solely on PSNRs and SSIMs metrics for evaluation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs the depths of clear images suffer from unavailability at inference and dehazed images obtained via one-time feed-forward execution may have haze residuals, we introduce a continuous self-prompt inference to address these challenges.\nSpecifically, our proposed approach feeds the hazy input image to the model and sets the depth difference as zero to generate clearer images that serve as the clear counterpart.\nThe clear image participates in constructing the prompt to conduct prompt dehazing.\nThe inference operation is continuously conducted as the depth differences can keep correcting the deep dehazing models toward better haze-free image generation.\n\n\n\n\n\n\n\n\n\nThis paper makes the following contributions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  * We make the first attempt to formulate the prompt by considering the cues of the estimated depth differences between the image with haze residuals and its clear counterpart in the image dehazing task.\n\n\n\n\n\n\n\n  * We propose a prompt embedding module and a prompt attention module to respectively perceive and pay more attention to haze residuals for better removal.\n\n\n\n\n\n\n  * We propose a new continuous self-prompt inference approach to iteratively correct the deep models toward better haze-free image generation.\n\n\n\n\n\n\n  * \nExperiments demonstrate that our method performs favorably against state-of-the-art approaches on both synthetic and real-world datasets in terms of perception metrics including NIQE, PI, and PIQE.\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\nIn this section, we overview image dehazing, VQGAN image restoration, and prompt vision applications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage Dehazing.\n\nTraditional solutions usually design various hand-crafted priors captured deterministic and statistical properties of hazy and haze-free images to remove haze, such as dark channel\u00a0<cit.>, color-line\u00a0<cit.>, haze-line\u00a0<cit.>, .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecently, CNN-based dehazing approaches are gradually developed\u00a0<cit.>, , MSCNN\u00a0<cit.> use CNN to estimate the transmission map.\nOne limitation of these algorithms is not flexible as they are not end-to-end.\nTo address this issue, end-to-end dehazing networks\u00a0<cit.> are proposed.\nConsidering the haze physics model\u00a0(<ref>), physics-based CNNs\u00a0<cit.> are suggested.\nMotivated the powerful generation ability of CycleGAN\u00a0<cit.>, cycle-based methods\u00a0<cit.> are adapted.\nAlthough these efforts, these methods usually tend to produce unsatisfactory results as they cannot effectively perceive haze residuals.\n\n\n\nVQGAN for Image Restoration.\n\nRecent research\u00a0<cit.> has shown that VQGAN\u00a0<cit.> is an effective tool to generate more realistic results.\n\n\n\n\n\n\n\nVQGAN-based restoration methods estimate latent clear images but often neglect deep model prior cues, which can limit their performance.\nZhou\u00a0. propose CodeFormer\u00a0<cit.>, which inserts regular Transformers into VQGAN for face restoration.\nDifferent from this work, our approach incorporates the estimated depth inconsistency between the image with haze residuals and its clear version by using prompt embedding and prompt attention to iteratively correct deep models with a self-prompt inference scheme for image dehazing.\n\n\n\n\n\nPrompt Learning for Vision.\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt learning is first studied in natural language processing\u00a0<cit.>.\nDue to its high effectiveness, prompt learning is recently used in vision-related tasks\u00a0<cit.>, , domain generalization\u00a0<cit.>, multi-modal learning\u00a0<cit.>, action understanding\u00a0<cit.>, and visual prompt tuning\u00a0<cit.>.\nTo our knowledge, there is no effort to exploit prompts for dehazing. This paper aims to investigate this new path.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 PROPOSED METHOD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur method comprises two branches: the prompt branch and the self-prompt dehazing Transformer branch. The prompt branch generates a prompt by using the deep depth difference and deep feature extracted from the hazy input.\nThe other branch exploits the generated prompt to guide the deep model for image dehazing.\nWe incorporate a prompt embedding module and prompt attention module to respectively perceive and pay more attention to the haze residuals for better removal. The proposed modules are formulated into an encoder-decoder architecture based on VQGAN for better perception quality\u00a0<cit.>.\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Overall Framework\n\n\nFig.\u00a0<ref> illustrates our method at the training stage.\nGiven a hazy images I, we first utilize trainable encoder Enc(\u00b7) to extract features:\n\n\n\n\n    \ud835\udc05_Enc = Enc(I).\n\n\n\n\n\n\nThen, we compute the depth difference of the hazy image I and its corresponding clear image J in feature space: \n\n\n\n\n\n    D_1 = DE(I);\u00a0D_2 = DE(J),\n       \ud835\udc05_D_1 = Enc_pre^frozen(D_1);\u00a0\ud835\udc05_D_2 = Enc_pre^frozen(D_2),\n       \ud835\udc05_D_diff = |\ud835\udc05_D_1 -\ud835\udc05_D_2|,\n\n\n\n\n\n\n\n\n\nwhere DE(\u00b7) denotes the depth estimator[We chose DPT_Next_ViT_L_384 to balance accuracy, speed, and model size: https://github.com/isl-org/MiDaS.]\u00a0<cit.>.\nEnc_pre^frozen(\u00b7) denotes the pre-trained VQGAN encoder which is frozen when training our dehazing models.\n\n\nNext, we exploit \ud835\udc05_D_diff to build the Prompt, and develop a prompt embedding module and a prompt attention module in Transformers, , PTB (see details in Sec.\u00a0<ref>) to better generate haze-aware features:\n\n\n\n\n\n\n    Prompt = \ud835\udc05_D_diff\u00b7\ud835\udc05_Enc,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0Prompt\u00a0\n       \ud835\udc05_ProEmbed = Prompt + \ud835\udc05_Enc,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0Prompt\u00a0Embedding\n       \ud835\udc05_PTB = PTB(Prompt, \ud835\udc05_ProEmbed),\u00a0#\u00a0Prompt\u00a0Transformer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhere \ud835\udc05_ProEmbed means the features of prompt embedding.\n\n\n\n\nThe generated feature \ud835\udc05_PTB is further matched with the learned haze-free  at the pre-trained VQGAN stage by the Lookup method\u00a0<cit.>:\n\n\n\n\n    \ud835\udc05_mat = Lookup(\ud835\udc05_PTB, ).\n\n\n\n\n\n\n\n\n\n\nFinally, we reconstruct the dehazing images J from the matched features \ud835\udc05_mat by decoder of pre-trained VQGAN Dec_pre^frozen(\u00b7) with residual learning\u00a0<cit.> by mutual deformable fusion module MDFM (see details in Sec.\u00a0<ref>):\n\n\n\n\n    J = Dec_pre^frozen(\ud835\udc05_mat) + MDFM(F_Enc^s, F_PTB^s,u),\n\n\n\n\n\n\n\n\nwhere F_Enc^s means the encoder features at s scale, while F_PTB^s,u denotes the s\u00d7 upsampling features of PTB.\nWe conduct the residual learning with MDFM in {1, 1/2, 1/4, 1/8} scales between the encoder and decoder like FeMaSR\u00a0<cit.>.\nHere, F_Enc^1/8 denotes the F_Enc in (<ref>).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoss Functions.\nWe use pixel reconstruction loss \u2112_rec, codebook loss \u2112_code, perception loss \u2112_per, and adversarial loss \u2112_adv to measure the error between the dehazed images J and the corresponding ground truth J:\n\n\n\n\n    [ \u2112 =\u2112_rec+\u03bb_code\u2112_code + \u03bb_per\u2112_per + \u03bb_adv\u2112_adv, ]\n\n\n\n\n\n\n\n\nwhere\n\n\n\n\n\n\n    \u2112_rec =||J-J||_1 + \u03bb_ssim(1-SSIM(J,J)),\n       \u2112_code = ||z\u0305_\ud835\udc2a-z_\ud835\udc2a||_2^2,\n       \u2112_per = ||\u03a6(J)- \u03a6(J)||_2^2,\n       \u2112_adv= \ud835\udd3c_J[log\u00a0\ud835\udc9f(J)] + \ud835\udd3c_J[1-log\u00a0\ud835\udc9f(J)],\n\n\n\n\n\n\n\n\n\nwhere SSIM(\u00b7) denotes the structural similarity\u00a0<cit.> for better structure generation.\n\nz_\ud835\udc2a is the haze-free codebook features by feeding haze-free images J to pre-trained VQGAN while z\u0305_\ud835\udc2a is the reconstructed codebook features.\n\u03a6(\u00b7) denotes the feature extractor of VGG19\u00a0<cit.>.\n\ud835\udc9f is the discriminator\u00a0<cit.>.\n\u03bb_code, \u03bb_per, \u03bb_adv, and \u03bb_ssim are weights.\n\nFor inference, we propose a new self-prompt inference approach (see details in Sec.\u00a0<ref>) as our training stage involves the depth of clear images to participate in forming the prompt while clear images are not available at testing.\n\n\n\n \u00a7.\u00a7 Self-Prompt Transformers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe proposed self-prompt Transformer contains the prompt generated by the prompt branch, a prompt embedding module, and a prompt attention module which is contained in the prompt Transformer block.\nIn the following, we introduce the definition of the prompt, prompt embedding module and prompt attention module, and prompt Transformer block in detail.\n\n\n\n\n\n\n\n\nPrompt (Definition).\n\n\nThe prompt is based on the estimated depth difference between the input image and its clear counterpart.\nIt is defined in (<ref>) which can better contain haze residual features as \ud835\udc05_D_diff with higher response value reveals inconsistent parts which potentially correspond to the haze residuals in the input hazy image.\n\n\n\n\n\n\n\n\n\n\nPrompt Embedding.\nExisting Transformers\u00a0<cit.> usually use the position embedding method (Fig.\u00a0<ref>(a)) to represent the positional correlation, which does not contain haze-related information so that it may not effectively perceive the haze residual information well.\nMoreover, image restoration requires processing different input sizes at inference while the position embedding is defined with fixed parameters at training\u00a0<cit.>.\nHence, position embedding may be not a good choice for image dehazing.\nTo overcome these problems, we propose prompt embedding which is defined in (<ref>).\n\n\n\nBy linearly adding the extracted features \ud835\udc05_Enc with Prompt, the embedded feature \ud835\udc05_ProEmbed perceives the haze residual features as Prompt extracts the haze residual features.\nNote that as \ud835\udc05_ProEmbed has the same size as \ud835\udc05_Enc, it does not require fixed sizes like position embedding.\n\n\n\n\nPrompt Attention.\n\nExisting Transformers usually extract Query \ud835\udc10, Key \ud835\udc0a, and Value \ud835\udc15 from input features to estimate scaled-dot-product attention shown in Fig.\u00a0<ref>(c).\nAlthough Transformers are effective for feature representation, the standard operation may be not suitable for image dehazing.\n\nTo ensure the Transformers pay more attention to haze residuals for better removal, we propose prompt attention ProAtt(\u00b7) by linearly adding the query with Prompt:\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Q =  Q + Prompt,\n    \n       ProAtt(Q, K, V) = Softmax(QK^T/\u221a(d_head))V,\n\n\n\n\n\n\n\n\n\n\nwhere d_head denotes the dimension of head.\nFig.\u00a0<ref>(d) illustrates the proposed prompt attention.\nNote that as Q in attention is to achieve the similarity relation for expected inputs\u00a0<cit.>, our prompt attention by linearly adding the prompt Prompt with the Query Q can pay more attention to haze residuals for better removal.\n\n\n\n\n\nPrompt Transformer Block.\n\nAccording to the above attention design, our prompt Transformer block (PTB) can be sequentially computed as:\n\n\n\n\n\n    Q, K, V = LN(\ud835\udc17^l-1), \n       \ud835\udc17\u0302^l =ProAtt(Q, K, V) + \ud835\udc17^l-1,\n    \n       \ud835\udc17^l =  MLP(LN(\ud835\udc17\u0302^l)) + \ud835\udc17\u0302^l,\n\n\n\n\n\n\n\n\n\n\nwhere \ud835\udc17^l-1 and \ud835\udc17^l mean the input and output of the l^th prompt Transformer block.\nSpecially, \ud835\udc17^0 is the \ud835\udc05_ProEmbed.\nLN and MLP denote the layer normalization and multilayer perceptron.\nThe PTB is shown in the right part of Fig.\u00a0<ref>.\n\n\n\n\n\n\n\n\n\nIt is worth noting that our prompt embedding and prompt attention are flexible as we can manually set \ud835\udc05_D_diff=0, the network thus automatically degrade to the model without prompt, which will be exploited to form our continuous self-prompt inference (see Sec.\u00a0<ref>).\n\n\n\n \u00a7.\u00a7 Mutual Deformable Fusion Module\n\n\nAs VQGAN is less effective for preserving details\u00a0<cit.>, motivated by the deformable models\u00a0<cit.> that can better fuse features, we propose a mutual deformable fusion module (MDFM) by fusing features mutually to adaptively learn more suitable offsets for better feature representation:\n\n\n\n\n\n    off_1 = Conv(\ud835\udc9e[F_Enc^s, F_PTB^s,u]);off_2 =  Conv(\ud835\udc9e[F_PTB^s,u, F_Enc^s]),\n       \ud835\udc18_1 = DMC(F_Enc^s, off_1);\ud835\udc18_2 = DMC(F_PTB^s,u, off_2),\n       \ud835\udc05_MDFM = Conv(\ud835\udc9e[\ud835\udc18_1, \ud835\udc18_2]),\n\n\n\n\n\n\n\n\n\n\nwhere Conv(\u00b7), \ud835\udc9e[\u00b7], and DMC(\u00b7) respectively denote the 1\u00d71 convolution, concatenation, and deformable convolution.\noff_k\u00a0(k=1,2.) denotes the estimated offset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Continuous Self-Prompt Inference\n\n\n\n\n\n\n\n\n\nOur model requires the depth of clear images during training, but these images are unavailable at inference. Additionally, dehazed images generated by a one-time feed-forward execution may still contain some haze residuals. To address these issues, we propose a continuous self-prompt inference approach that leverages prompt embedding and prompt attention through linear addition, as discussed in Sec.\u00a0<ref>. By setting feature-level depth difference \ud835\udc05_D_diff to zero, we can feed hazy images to our trained network and obtain clearer dehazed results which participate in building the prompt to conduct prompt dehazing.\nThe iterative inference is conducted to correct the deep models to ensure the deep models toward better haze-free image generation:\n\n\n\n\n\n    J_i^w/o\u00a0prompt = \ud835\udca9^w/o\u00a0prompt(J_i-1^w/o\u00a0prompt),\u00a0set\u00a0\ud835\udc05_D_diff=0,    #\u00a0Step 1\n       Prompt = \ud835\udc05_D_diff\u00b7\ud835\udc05_Enc;  \ud835\udc05_Enc = Enc(J_i-1^w/o\u00a0prompt),   #\u00a0Step 2\n       J_i^prompt = \ud835\udca9^prompt(J_i-1^w/o\u00a0prompt,Prompt),   #\u00a0Step 3\n       J_i^w/o\u00a0prompt = J_i^prompt,\u00a0\u00a0\u00a0(i=1,2, \u22ef),   #\u00a0Step 4\n\n\n\n\n\n\n\n\n\n\nwhere \ud835\udca9^w/o\u00a0prompt denotes our trained network without prompt by setting \ud835\udc05_D_diff as zero, while \ud835\udca9^prompt means our trained network with prompt.\n\ud835\udc05_D_diff = |Enc_pre^frozen(DE(J_i-1^w/o\u00a0prompt)) -Enc_pre^frozen(DE(J_i^w/o\u00a0prompt))|.\nJ_0^w/o\u00a0prompt denotes the original hazy images, while J_i-1^w/o\u00a0prompt is regarded as the image with haze residuals and J_i^w/o\u00a0prompt in (<ref>) is regarded as the clear counterpart of J_i^w/o\u00a0prompt.\nJ_i^prompt means the i^th prompt dehazing results.\n\nAccording to (<ref>), the inference is a continuous self-prompt scheme, , we get the clear images from the hazy image itself by feeding it to \ud835\udca9^w/o\u00a0prompt to participate in producing the prompt and the inference is continuously conducted.\nFig.\u00a0<ref> better illustrates the inference process.\n\n\nFig.\u00a0<ref> shows our continuous self-prompt at 2^nd and 3^rd prompts outperforms the baseline which uses ground-truth (GT) to participate in forming the prompt like the process of the training stage.\nHowever, GT is not available in the real world.\nMore detailed explanations are presented in Sec.\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n\n\nIn this section, we evaluate the effectiveness of our method against state-of-the-art ones (SOTAs) on commonly used benchmarks and illustrate the effectiveness of the key components in the proposed method.\n\n\nImplementation Details.\n\n\nWe use 10 PTBs, , l=10, in our model.\nThe details about the VQGAN are presented in the supplementary materials.\n\nWe crop an image patch of 256\u00d7256 pixels. The batch size is 10.\nWe use ADAM\u00a0<cit.> with default parameters as the optimizer.\n\nThe initial learning rate is 0.0001 and is divided by 2 at 160K, 320K, and 400K iterations. The model training terminates after 500K iterations.\nThe weight parameters \u03bb_code, \u03bb_per, \u03bb_adv, and \u03bb_ssim are empirically set as 1, 1, 0.1, and 0.5.\nOur implementation is based on the PyTorch using one Nvidia 3090 GPU.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSynthetic Datasets.\nFollowing the protocol of\u00a0<cit.>, we use the RESIDE ITS\u00a0<cit.> as our training dataset and the SOTS-indoor\u00a0<cit.> and SOTS-outdoor\u00a0<cit.> as the testing datasets.\n\n\n\nReal-world Datasets.\n\nIn <cit.>, Li\u00a0. also collect large-scale real-world hazy images, called UnannotatedHazyImages.\n\n\nWe use these images as a real-world hazy dataset.\n\n\n\nEvaluation Metrics.\nAs we mainly aim to recover images with better perception quality, we use widely-used Natural Image Quality Evaluator (NIQE)\u00a0<cit.>, Perceptual Indexes (PI)\u00a0<cit.>, and Perception-based Image Quality Evaluator (PIQE)\u00a0<cit.> to measure restoration quality.\nSince the distortion metrics Peak-Signal-to-Noise-Ratio (PSNR)\u00a0<cit.> and Structural SIMilarity (SSIM)\u00a0<cit.> cannot model the perception quality well, we use them for  reference only.\nNotice that all metrics are re-computed for fairness.\nWe use the grayscale image to compute the PSNR and SSIM.\nWe compute NIQE and PI by the provided metrics at https://pypi.org/project/pyiqa/.\nThe PIQE is computed via https://github.com/buyizhiyou/NRVQA.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Results on Synthetic Datasets\n\n\nTab.\u00a0<ref> and Tab.\u00a0<ref> respectively report the comparison results with SOTAs on the SOTS-indoor and SOTS-outdoor datasets\u00a0<cit.>.\nOur method achieves better performance in terms of NIQE, PI, and PIQE, indicating the generated results by our method possesses higher perception quality.\nFig.\u00a0<ref> and Fig.\u00a0<ref> show that our method restores much clearer images while the evaluated approaches generate the results with haze residual or artifacts.\n\nAs we train the network with a one-time feed-forward process, PSNRs and SSIMs are naturally decreased (Ours_1 Ours_3 in Tabs.\u00a0<ref> and <ref>) when inference is conducted iteratively.\nWe argue distortion metrics including PSNRs and SSIMs are not good measures for image dehazing as Figs.\u00a0<ref> and <ref> have shown methods with higher PSNR and SSIMs cannot recover perceptual results, , Dehamer\u00a0<cit.> and D4\u00a0<cit.>, while our method with better perception metrics is able to generate more realistic results.\n\n\n\n \u00a7.\u00a7 Results on Real-World Datasets\n\n\nTab.\u00a0<ref> summarises the comparison results on the real-world datasets\u00a0<cit.>, where our method performs better than the evaluated methods.\nFig.\u00a0<ref> illustrates that our method generates an image with vivid color and finer details.\n\n\n\n \u00a7.\u00a7 Analysis and Discussion\n\n\nWe further analyze the effectiveness of the proposed method and understand how it works on image dehazing.\nThe results in this section are obtained from the SOTS-indoor dataset if not further mentioned. Our results are from the 1^st prompt inference for fair comparisons, , i=1 in (<ref>) if not further specifically mentioned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffectiveness of prompt.\n\n\n\n\n\nInitially, we assess the effect of the prompt on image dehazing.\nNotably, various prospective prompt candidates exist, such as image-level depth difference as the input of the VQGAN encoder or concatenation between deep features extracted from the input and depth features as the input of the Transformers.\nOur proposed prompt is compared with these candidates, as illustrated in Tab.\u00a0<ref>(b) and <ref>(c), demonstrating that none of these candidates outperforms our proposed prompt.\n\nNote our method without prompt leads to a similar model with CodeFormer\u00a0<cit.> which directly inserts regular Transformers into VQGAN.\nTab.\u00a0<ref> shows prompt help yield superior perception quality than the model without prompt (Tab.\u00a0<ref>(a)).\nThe efficacy of our model with the prompt is further affirmed by Fig.\u00a0<ref>, indicating that the model with the prompt generates better results, while the model without prompt fails to remove haze effectively.\n\n\nEffectiveness of prompt embedding.\n\n\n\nOne might ponder the relative efficacy of our prompt embedding in contrast to the prevalent technique of position embedding (Fig.\u00a0<ref>(a)).\nIn this regard, we assess the effect of these embedding approaches in Tab.\u00a0<ref>. The table reveals that our prompt embedding proves more advantageous over the position embedding, since the former is associated with haze residual information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffectiveness of prompt attention.\n\n\n\n\nAnalyzing the efficacy of prompt attention proves intriguing.\nTab.\u00a0<ref> indicates that our prompt attention yields better results as compared to commonly used attention methods (Fig.\u00a0<ref>(c)).\nThese findings signify that incorporating prompts in enhancing Query estimation accounts for the haze information, thereby culminating in more effective image dehazing results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffect of the number of steps in continuous self-prompt.\n\n\n\nThe inference stage involves several steps to generate the prompt for better image dehazing.\nWe thus examine the effect of the number of steps in the continuous self-prompt.\nFig.\u00a0<ref> reveals that the optimal performance is achieved with a number of steps equal to 3 in the continuous self-prompts (, i=3 in (<ref>)), in terms of NIQE.\nNotably, additional prompts do not improve the dehazing performance any further.\nOne real-world example in Fig.\u00a0<ref> demonstrates that our continuous self-prompt method can gradually enhance dehazing quality.\n\n\n\n\nContinuous self-prompt recurrent dehazing.\n\n\n\n\n\n\n\nWe use the continuous self-prompt approach to restore clear images progressively at inference.\nTo determine whether a recurrent method that is training our model without prompt achieves similar or better results, we compare our proposed method with it in Fig.\u00a0<ref>, demonstrating that the recurrent method is not as good as our continuous self-prompt.\n\n\nContinuous self-prompt GT guidance.\n\n\n\n\n\n\nFig.\u00a0<ref> compares the NIQE performance of ground truth (GT) guidance with that of the continuous self-prompt algorithm.\nResults show that while GT guidance performs better than the 1^st prompt, it falls short of the effectiveness of the 2^nd and 3^rd prompts.\nThis is likely due to GT guidance's limited ability to handle haze residuals which may still exist in the dehazed images, which are addressed by the self-prompt's ability to exploit residual haze information to progressively improve dehazing quality over time.\nMoreover, as GT is not available in the real world, these findings may further support the use of self-prompt as a more practical alternative.\n\n\nDepth-consistency.\n\n\nFig.\u00a0<ref> shows heat maps of depth differences obtained by the continuous self-prompt inference with different prompt steps.\nThe results demonstrate both image-level and feature-level depth differences decrease as the number of prompt steps increases, indicating the depths obtained with the prompt, , (<ref>), become increasingly consistent with those obtained without it, , (<ref>).\n\n\nModel size and running time.\n\n\n\n\nTab.\u00a0<ref> compares our model sizes and running time against recent Transformer-based SOTAs: Uformer\u00a0<cit.> and Dehamer\u00a0<cit.>.\nOur model is comparable with these leading methods on model sizes.\nWhile the single-iteration time speed of our method remains comparable to these two feed-forward models\u00a0<cit.>, it requires slightly more time for multiple iterations since our method involves estimating depths.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nWe have proposed a simple yet effective self-prompt Transformer for image dehazing by exploring the prompt built on the estimated depth difference between the image with haze residuals and its clear counterpart.\nWe have shown that the proposed prompt can guide the deep model for better image dehazing.\nTo generate better dehazing images at the inference stage, we have proposed continuous self-prompt inference, where the proposed prompt strategy can remove haze progressively.\nWe have shown that our method generates results with better perception quality in terms of NIQE, PI, and PIQE.\n\n\n\nLimitations.\nOur model is influenced by the estimated depth of J_i-1^w/o\u00a0prompt and J_i^w/o\u00a0prompt in (<ref>).\nOur continuous self-prompt approach may not work well if the depth difference is not significant enough.\n\nSlight performance degradation occurs when multiple prompts in the SOTS-outdoor in terms of NIQE and PI (Tab.\u00a0<ref>).\nWe argue this may be because the depth generated from J_i-1^w/o\u00a0prompt and J_i^w/o\u00a0prompt in (<ref>) in SOTS-outdoor is not significant enough.\nYet, the 1^st prompt still outperforms existing methods.\n\n\n\n\n\n\n\nieee_fullname\n\n\n\n"}