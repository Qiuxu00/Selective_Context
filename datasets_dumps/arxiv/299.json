{"entry_id": "http://arxiv.org/abs/2303.06925v1", "published": "20230313084124", "title": "Super-Resolution Information Enhancement For Crowd Counting", "authors": ["Jiahao Xie", "Wei Xu", "Dingkang Liang", "Zhanyu Ma", "Kongming Liang", "Weidong Liu", "Rui Wang", "Ling Jin"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\nAnalytics for \u201cinteraction with the service\u201d: Surreptitious Collection of User Interaction Data\n    Feiyang Tang, Bjarte M. \u00d8stvold\n    March 30, 2023\n===============================================================================================\n\n\n\nCrowd counting is a challenging task due to the heavy occlusions, scales, and density variations. Existing methods handle these challenges effectively while ignoring low-resolution (LR) circumstances. The LR circumstances weaken the counting performance deeply for two crucial reasons: 1) limited detail information; 2) overlapping head regions accumulate in density maps and result in extreme ground-truth values. An intuitive solution is to employ super-resolution (SR) pre-processes for the input LR images. However, it complicates the inference steps and thus limits application potentials when requiring real-time. We propose a more elegant method termed Multi-Scale Super-Resolution Module (MSSRM). It guides the network to estimate the lost details and enhances the detailed information in the feature space. Noteworthy that the MSSRM is plug-in plug-out and deals with the LR problems with no inference cost. As the proposed method requires SR labels, we further propose a Super-Resolution Crowd Counting dataset (SR-Crowd). Extensive experiments on three datasets demonstrate the superiority of our method. The code will be available at <https://github.com/PRIS-CV/MSSRM.git>.\n\n\n\n\ncrowd counting, super-resolution, multi-scale super-resolution module, plug-in plug-out\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nCrowd counting is an essential task for understanding crowd scenes, which aims to estimate the number of persons involved in given images or videos. Traditional crowd counting algorithms\u00a0<cit.> achieve this task mainly by detecting individuals or regressing density-related features.\n\n\n\n\n\n\nWe have witnessed the rise of convolutional neural networks (CNN). Current mainstream counting methods\u00a0<cit.> adopt the CNN to regress a density map. The predicted count equals the integral of the density map, which can perform better than the traditional methods.\nHowever, crowd counting is still a challenging task due to the large-scale variation, complex background, and blur. \nRecent methods\u00a0<cit.> usually focus on coping with the scale variation and background noise while ignoring the blur. Generally, low-resolution (LR) images usually present blurry phenomena, i.e., losing the massive content details of the crowd. Actually, in practical applications, LR images are really common, e.g., aging image capture devices, limited image storage space, downsampling to achieve real-time inference, perspective effect, occasional jitters, and so on. Networks drawback in the LR scenes are hard to extract precise semantic information when being deployed in the mentioned situations, leading to disappointing application potentials. Specifically, as shown in Fig.\u00a0<ref> (a), the LR image loses content details, resulting in a low-quality density map. Instead, the HR image maintains the detailed information, and the resulting density map is clear and sharp. The challenge in generating sharp density maps from LR images is the lack of detailed information.\n\nSuper-resolution\u00a0<cit.> aims to reconstruct a visually natural HR image from its degraded LR image and has achieved great success in these years. It is a plausible method to tackle the mentioned problem.\nHowever, employing the SR methods in the pre-processes to obtain HR images introduces extra inference costs and limits the application potentials in occasions requiring real-time estimation.\n\nIn this paper, we propose to employ the SR task as the auxiliary task to enhance the detailed information in the feature space. Specifically, we propose a Multi-Scale Super-Resolution Module (MSSRM), which enables the networks to generate HR density maps. The MSSRM works in the training phase, and guides the network to estimate reliable details even when given LR images. It could be removed during the inference phase, compensating for the loss of detail caused by blurry images without changing the original network structure. We further combine the MSSRM with mainstream backbones to propose Multi-Scale Super-Resolution Guided Network (MSSRGN). We measure the counting performance of the MSSRGN in LR circumstances to evaluate the effectiveness of our methods.\n\nNotably, SR tasks require corresponding HR and LR labels, but existing crowd datasets capture crowd images at a single resolution level. To address this issue, we propose a new Super-Resolution Crowd dataset (SR-Crowd). \nIn summary, the contributions of this paper are as follows:\n\n1. We propose an MSSRM, which guides the network to estimate reliable detail information and boosts the counting performance in LR circumstances with no inference costs.\n\n2. We introduce SR-Crowd, a large-scale super-resolution crowd dataset. To our knowledge, this is the first super-resolution dataset in the crowd counting area.\n\n3. Extensive experiments on the SR-Crowd and two mainstream crowd counting datasets demonstrate the effectiveness of our method.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 METHODS\n\nWe propose to employ the SR task as an auxiliary task to enhance the SR information for crowd counting. In this section, we introduce the SR-Crowd dataset and the MSSRGN.\n\n\n\n\n\n \u00a7.\u00a7 Data Collection and Analysis\n\nThe SR-Crowd images are collected from two challenging crowd datasets: NWPU-Crowd\u00a0<cit.> and UCF-QNRF\u00a0<cit.>. We first extract images of height or width greater than 2048 from these two datasets. Then we adjust the corresponding ratio to make the height or width equal to 2048 as 4\u00d7 upscaling images. Subsequently, we downsample the images by 2\u00d7 and 4\u00d7, respectively. Downsampling source images to construct a new SR dataset is a standard way in SR tasks.\n\nThen we generate the ground-truth density maps for LR images by resizing the head center coordinates according to scaling factors. The 2\u00d7 and 4\u00d7 upscaling images are used as SR ground truth.\n\nEach set of SR-Crowd data consists of 3,542 image pairs with 2,000,527 annotations. It consists of 2,909 images used for training and 633 images for testing. In the dataset, the minimum and maximum counts are 0 and 17,726, respectively, and the mean counts are 564. The average resolution of LR images, 2\u00d7 upscaling images, and 4\u00d7 upscaling images are 350 \u00d7 504, 670 \u00d7 1008, and 1397 \u00d7 2016, respectively.\n\n\n\n\n \u00a7.\u00a7 MSSRGN\n We focus on improving counting performance in the LR circumstances compared to the previous work\u00a0<cit.>. As shown in Fig.\u00a0<ref>, we achieve the SR task in the training phase. Following the usual practice <cit.>, we choose the pre-trained VGG-16\u00a0<cit.> as the backbone. We remove the last pooling layer of the original VGG-16 to preserve spatial information and remove the full connection layer to adapt the network structure to arbitrary resolution inputs. The output size of VGG-16 is 1/8 of the original input size. Then feed the output feature to the back-end network to generate the density map. The back-end structure consists of a 3\u00d73 convolution layer and a 1\u00d71 convolution layer. \nFurthermore, we feed the features to the MSSR module to estimate the SR image and enhance the detailed information. We introduce the details of the MSSR module next.\n\n\n\nMSSRM. We design a multi-scale super-resolution module to extract detailed information from LR images and assist the backbone network to generate high-quality density maps. MCNN\u00a0<cit.> proposes the multi-column convolution method to solve the multi-scale problem in the crowd. Although this method can alleviate the scale problem to a certain extent, it has a complex structure and low efficiency. Meanwhile, the number of columns limits the scale diversity of features. In this paper, we concatenate multi-scale features from the backbone as the input of the MSSRM. The super-resolution module consists of a 3 \u00d7 3 convolution layer and a sub-pixel convolution layer\u00a0<cit.>. The 3 \u00d7 3 convolution layer reshapes the input feature to be r^2 channels where r represents upscaling factor. Then, the sub-pixel convolution rearranges the elements of tensors of H \u00d7 W \u00d7 C \u00d7 r^2 shape into tensors of rH \u00d7 rW \u00d7 C shape according to periodic shuffling, where H, W, C denote the height, width and channels of the LR image respectively.  This method can fill in the LR features and map them to HR features. The mathematical expression can be present as:\n\n    I^SR=f^n(\ud835\udc08^L R ; W_n, b_n)=\u03be(W_n * f^n-1(\ud835\udc08^L R)+b_n),\n\nwhere W_n, b_n, n represent network weights, bias, and the number of layers n, respectively. \u03be is a periodic shuffling operator that rearranges elements with LR features.\n\n\n\nThe MSSRM contains shallow layers, which leads to limited decoding capabilities. However, it is not designed as an ideal head structure to achieve desirable SR performance on purpose. We propose the MSSRM to guide the network to enhance the detailed information in the feature space. Specifically, we supervise the SR image estimation. Transmit ground-truth lost details information to the backbone structure. Shallow layers shorten the SR information flow. It forces the encoded feature from the backbone structure to contain rich detailed information. In this way, while facing the LR circumstances, the estimated lost detail feature boosts the sharper density maps estimation performance. Note that the MSSRM will be removed after the crowd counter is well-trained. It improves the counting performance with no inference cost. Instead of employing the SR pre-processes before crowd counting, the proposed MSSRM is more elegant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoss Function.\nWe use the L2 loss to supervise the density map and HR image estimations. Specifically, given the i-th image X_i, the loss L(\u0398) can be presented as:\n\n    L(\u0398)=1/2 N\u2211_i=1^NP(X_i ; \u0398)-P_i^G T_2^2,\n\nwhere \u0398 is a learnable set of parameters. P(X_i ; \u0398) refers to the estimated density map generated by MSSRGN. P_i^G T is the ground-truth density map of image X_i. N is the number of training images. L(\u0398) denotes the loss between the ground-truth density map and the estimated density map.\n\n    L(\u03f5)=1/2 N\u2211_i=1^Nf(I_i^LR ; \u03f5)-I_i^HR_2^2,\n\nwhere I_i^HR and I_i^LR refer to the HR and LR images, respectively. \u03f5 is a learnable set of parameters. f(I^LR ; \u03f5) is the SR feature generated by MSSRGN. L(\u03f5) denotes the loss between the HR feature and SR feature. The final objective function is defined as:\n\nL = L(\u0398) + \u03b1 L(\u03f5),\n\n\nwhere \u03b1 is a balance weight.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\nThis section introduces implementation details, evaluation metrics, ablation studies, and comparisons.\n\nImplementation Details.\nThe MSSRGN is an end-to-end structure. We flip each image randomly. The first 10 convolutional layers are fine-tuned on the basis of the VGG-16 pre-training model. The other layers are initialized by a Gaussian initialization with a 0.01 standard deviation. We use the adaptive moment estimation optimizer. The learning rate as 10^-5. The number of training epochs is set to 500. \n\nEvaluation Metrics.\nWe generally use the mean absolute error (MAE) and the root mean squared error (RMSE) as evaluation metrics in the crowd counting task. Their definition is as\nMAE=1/M\u2211_j=1^M|b_j-b_j^G T|,\nRMSE=\u221a(1/M\u2211_j=1^M|b_j-b_j^G T|^2),\nwhere M is the number of test images, b_j^G T is the ground truth of counting, b_j represents the estimated count in the j-th image.\n\nAblation Studies. We collect LR images from the original images using down-sampling methods. We first compare the performance of different down-sampling strategies. As shown in Table\u00a0<ref>, linear interpolation has the best performance. \n\nIt achieves 5.959 and 2.224 lower MAE than the cubic and lanczos4 interpolation. \nWe employ the linear interpolation in our experiments.\nFeature fusion of different scales facilitates the extraction of multi-scale features. We then conduct experiments to analyze the effectiveness of different features fusion. As shown in Table\u00a0<ref>, the MAE of fusing stages 3, 4, 5 is 148.171. It achieves 3.499 lower MAE and 55.480 lower RMSE than the second place. We fuse stages 3, 4, and 5 in the following.\nWe present visualizations from the baseline method and the MSSRGN in Fig.\u00a0<ref>. Three images are captured in the sparse, crowded, and dense senses, respectively. As we can see that the baseline method predicts density maps with serious noises in background regions. The proposed MSSRGN output density maps clearer and sharper.\n\nComparisons. \nAs shown in Table\u00a0<ref>, we evaluate our method on the SR-Crowd, ShanghaiTech Part_A, and UCF-QNRF datasets. The experimental results demonstrate that our MSSR module is plug-in plug-out and efficient. Specifically, on the SR-Crowd dataset, for CSRNET and FPN, MAE is reduced by 7.284 and 6.807, respectively, when adopting the upscale factor as 2\u00d7. For the 4\u00d7 upscale factor, MAE is reduced by 8.705 and 1.631, respectively. We also conducted comparative experiments with CLTR\u00a0<cit.> and MAN\u00a0<cit.> on the SR-Crowd, which achieve 170.684 and 151.890 MAE, respectively. For the ShanghaiTech Part_A and the UCF-QNRF dataset, we downsample the images 2\u00d7 and 4\u00d7 as input. Use the original images as the ground truth for the SR task. Resize the original coordinates of head centers according to the upscale factors to generate new gt centers. On the ShanghaiTech Part_A, the MSSRGN\u2020\u00a0and MSSRGN\u2020\u2020\u00a0achieve 6.506 and 6.687 MAE improvement. \nWe also conduct experiments on the UCF-QNRF dataset, a challenging dataset with dramatic variations both in crowd density and image resolution. Concretely, we resize the long side of images to 512, 1024, and 2048. And resize the short of images corresponding multiples to mitigate the enormous resolution variations. As shown in Table\u00a0<ref>, the MSSRGN\u2020\u00a0achieves the best performance in the UCF-QNRF dataset. It reduces the MAE and RMSE values by 3.379 and 3.495. These experiments demonstrate the effectiveness of our method. \n\n\n\n\n\n\u00a7 CONCLUSION\n\nIn this paper, we propose to employ the SR task as an auxiliary task to deal with the LR circumstances in crowd counting. We propose an elegant method, MSSRM, to overcome the low-resolution barriers with no inference cost. The MSSRM is plug-in plug-out. It works in the training phase and guides the network to estimate the lost details as compensation for the blurry presentations. Besides, we propose an SR-Crowd dataset containing hierarchical-resolution images to achieve the SR task. Extensive experiments on SR-Crowd, ShanghaiTech Part_A, and UCF-QNRF datasets demonstrate the effectiveness of our method. We believe our contributions broaden the application potentials of existing crowd counting methods. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIEEEbib\n\n\n"}