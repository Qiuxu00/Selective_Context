{"entry_id": "http://arxiv.org/abs/2303.07065v1", "published": "20230313123959", "title": "MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID", "authors": ["Jianyang Gu", "Kai Wang", "Hao Luo", "Chen Chen", "Wei Jiang", "Yuqiang Fang", "Shanghang Zhang", "Yang You", "Jian Zhao"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\nMSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID\n    Jianyang Gu^1 Kai Wang^2 Hao Luo^3 Chen Chen^4 Wei Jiang^1*\n Yuqiang Fang^5 Shanghang Zhang^6 Yang You^2 Jian Zhao^7Co-corresponding authors \n\n^1Zhejiang University ^2National University of Singapore ^3Alibaba Group\n\n^4OPPO Research Institute ^5Space Engineering University ^6Peking University\n\n^7Institute of North Electronic Equipment \n\n{gu_jianyang, jiangwei_zju}@zju.edu.cn zhaojian90@u.nus.edu\n\n\n\n\n\n\n\n\n\n\n\n    March 30, 2023\n=============================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\nNeural Architecture Search (NAS) has been increasingly appealing to the society of object Re-Identification (ReID), for that task-specific architectures significantly improve the retrieval performance. Previous works explore new optimizing targets and search spaces for NAS ReID, yet they neglect the difference of training schemes between image classification and ReID. In this work, we propose a novel Twins Contrastive Mechanism (TCM) to provide more appropriate supervision for ReID architecture search. TCM reduces the category overlaps between the training and validation data, and assists NAS in simulating real-world ReID training schemes. We then design a Multi-Scale Interaction (MSI) search space to search for rational interaction operations between multi-scale features. In addition, we introduce a Spatial Alignment Module (SAM) to further enhance the attention consistency confronted with images from different sources. Under the proposed NAS scheme, a specific architecture is automatically searched, named as MSINet. Extensive experiments demonstrate that our method surpasses state-of-the-art ReID methods on both in-domain and cross-domain scenarios. Source code available in https://github.com/vimar-gu/MSINethttps://github.com/vimar-gu/MSINet. \n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nObject re-identification (Re-ID) aims at retrieving specific object instances across different views\u00a0<cit.>, which attracts much attention in computer vision community due to its wide-range applications. Previous works have achieved great progresses on both supervised\u00a0<cit.> and unsupervised ReID tasks\u00a0<cit.>, most of which adopts backbone models originally designed for general image classification tasks\u00a0<cit.>. \n\n\n\nRecent literature\u00a0<cit.> has shown that applying different architectures on ReID leads to large performance variations. Some works employ Neural Architecture Search (NAS) for ReID\u00a0<cit.>. The proposed optimizing targets and search spaces stably improve the model performance, yet the main search scheme still follows traditional NAS methods designed for general classification tasks\u00a0<cit.>. \nAs an open-set task, ReID contains different categories in the training and validation sets\u00a0<cit.>, while the two sets share exactly the same categories in standard classification tasks\u00a0<cit.>, which is also followed by traditional NAS methods. \nThe incompatibility between search schemes and real-world training schemes makes the searched architecture sub-optimal for ReID.\nMoreover, ReID is required to distinguish more subtle distinctions among fine-grained instances compared with image-level classification\u00a0<cit.>. Some previous works\u00a0<cit.> have manifested that local perspectives and multi-scale features are discriminative for ReID. \nHowever, current utilizations of these features are mostly empirically designed, which can be more flexible according to the characteristics of different network layers. \n\nIn this work, we propose a novel NAS scheme aiming at addressing the aforementioned challenges. \nIn order to simulate the real-world ReID training schemes, a Twins Contrastive Mechanism (TCM) is proposed to unbind the categories of the training and validation sets. \nAn adjustable overlap ratio of categories builds up the compatibility between NAS and ReID, which provides more appropriate supervision for ReID architecture search. \nMoreover, to search for more rational utilizations of multi-scale features, we design a Multi-Scale Interaction (MSI) search space. The MSI space focuses on interaction operations between multi-scale features along the shallow and deep layers of the network, which guides the features to promote each other.\nAdditionally, to further improve the generalization capability, we propose a Spatial Alignment Module (SAM) to enhance the attention consistency of the model confronted with images from different sources. \nWith the above NAS scheme, we obtain a light-weight yet effective model architecture, denoted as Multi-Scale Interaction Net (MSINet). \n\n\nWe visualize the example activation maps of our proposed MSINet and ResNet50\u00a0<cit.> trained on VeRi-776\u00a0<cit.> in Fig.\u00a0<ref>. \nCompared to ResNet50, MSINet focuses on more unique distinctions with specific semantic information to recognize instances. \nBesides, MSINet largely increases the distance margin between query image and corresponding negative samples, reflecting extraordinary discriminative capability.\nExtensive experiments demonstrate that MSINet surpasses state-of-the-art (SOTA) ReID methods on both in-domain and cross-domain scenarios. Our source codes are available in the supplementary material. \n\nOur contributions are summarized as follows:\n\n    \n  * To the best of our knowledge, we are the first to build the NAS search scheme according to the real-world ReID training schemes, which provides more appropriate supervision for the ReID architecture search. \n    \n  * We propose a novel search space based on the Multi-Scale Interaction (MSI) operations and a Spatial Alignment Module (SAM) to improve the model performance on in-domain and cross-domain scenarios. \n    \n  * We construct a light-weight yet effective architecture for ReID tasks, denoted as MSINet. With only 2.3M parameters, MSINet surpasses ResNet50\u00a0<cit.> by 9% mAP on MSMT17\u00a0<cit.> and 16% mAP on MSMT17\u2192Market-1501\u00a0<cit.>. \n\n\n\n\n\u00a7 RELATED WORKS\n\n\nNeural Architecture Search.\nNAS has been increasingly appealing to the computer vision society, due to its automatic architecture designing characteristics. NAS methods can be roughly separated into four categories: reinforcement learning\u00a0<cit.>, evolutionary algorithms\u00a0<cit.>, gradient desent\u00a0<cit.> and performance prediction\u00a0<cit.>. Liu et al. establish a differentiable architecture search (DARTS) method\u00a0<cit.>, which improves the practicability of NAS by a large extent. Some later works further improve the structure through sampling strategy\u00a0<cit.>, network pruning\u00a0<cit.>, progressive learning\u00a0<cit.>, collaborative competition\u00a0<cit.>, etc. \nMost of NAS works focus on general image classification tasks, where the training and validation sets share the exact same categories. \nFollowing the setting, however, leads to incompatibility with the real-world training schemes of object ReID. \nIn this work, we unbind the category bond between the two sets and propose a novel search scheme suitable for ReID. \n\nReID Network Design.\nCurrent ReID works mostly adopt backbones designed for image classification\u00a0<cit.>. Some works\u00a0<cit.> design attention modules based on the common backbones to unearth their potential on distinguishing local distinctions. However, these methods usually lead to large calculation consumption. \n\nThere are also several works focusing on designing ReID-specific architectures. \nLi et al. present a Filter Pairing Neural Network to dynamically match patches in the feature maps\u00a0<cit.>. Wang et al. separate and regroup the features of two samples with a WConv layer\u00a0<cit.>. Guo et al. extract multi-scale features to directly evaluate the similarity between samples\u00a0<cit.>. However, the siamese structure is inconvenient when conducting retrieval on large galleries. \nZhou et al. aggregate multi-scale information to achieve high accuracy with small computing consumption\u00a0<cit.>. Quan et al. introduce a part-aware module into the DARTS search space\u00a0<cit.>. Li et al. propose a new search space in regard to receptive field scales\u00a0<cit.>. These methods have excellent performance on limited parameter scales, but fail to surpass those networks with complex structures. \nDifferent from previous works, we design a light-weight searching structure focusing on rational interaction operations between multi-scale features. \nThe searched MSINet surpasses SOTA methods on both in-domain and cross-domain tasks. \n\n\n\n\n\u00a7 METHODS\n\nOur goal is to construct an effective NAS scheme to search for a light-weight backbone architecture suitable for ReID tasks. Based on the training schemes of ReID, we propose a novel Twins Contrastive Mechanism to provide more appropriate supervision for the search process. Aiming at rational interaction between multi-scale features, we design a Multi-Scale Interaction search space. We further introduce a Spatial Alignment Module to improve the generalization capability with limited parameter growth. \n\n\n\n\n\n \u00a7.\u00a7 Twins Contrastive Mechanism\n\n\nNAS aims at automatically searching for the optimal network architecture for certain data.\nInspired by\u00a0<cit.>, a basic differentiable architecture search scheme is established. \nWe define the ordinary model parameters as \u03c9, and architecture parameters as \u03b1. For network layer i with a search space of \ud835\udcaa, \u03b1_i controls the weight of each operation o in the space. The features are parallelly passed through all the operations, and the final output is formulated by the softmax-weighted sum of operation outputs:\n\n    f(\ud835\udc31_i)=\u2211_o\u2208\ud835\udcaaexp{\u03b1^o_i}/\u2211_o^'\u2208\ud835\udcaaexp{\u03b1^o^'_i}\u00b7 o(\ud835\udc31_i).\n\nThe search process is conducted in an alternative manner. Training data is utilized to update the model parameters, and validation data is then employed to update the architecture parameters. For most NAS methods designed for image classification tasks, the training and validation data share exactly the same categories and a linear classification layer for loss calculation. \n\nDifferent from standard image classification, as an open-set retrieval task, ReID has different categories in the training and validation sets. \nThe incompatibility between search schemes and real-world training schemes might lead to sub-optimal searching results. \n\nAccordingly, we propose a novel Twins Contrastive Mechanism (TCM) for NAS ReID training. \nSpecifically, we employ two independent auxiliary memories \ud835\udc9e_tr and \ud835\udc9e_val to store the embedded features of the training and validation data, respectively. The memories are initialized with the centroid features, which are calculated by averaging the features of each category. \nAt each iteration, the training loss is first calculated with \ud835\udc9e_tr for model parameter updating. Given an embedded feature \ud835\udc1f with category label j, the contrastive classification loss is calculated with:\n\n    \u2112^cls_tr=-logexp(\ud835\udc1f\u00b7\ud835\udc1c^j_tr/\u03c4)/\u2211^N^c_tr_n=0exp(\ud835\udc1f\u00b7\ud835\udc1c^n_tr/\u03c4),\n\nwhere \ud835\udc1c^n_tr represents the memorized feature of category n, N^c_tr stands for the total number of categories in the training set, and \u03c4 is the temperature parameter, which is set as 0.05 empirically\u00a0<cit.>.\nAfter updating the model parameters, the embedded feature \ud835\udc1f with category label j is integrated into the corresponding memorized feature \ud835\udc1c^j_tr by:\n\n    \ud835\udc1c^j_tr\u2190\u03b2\ud835\udc1c^j_tr + (1-\u03b2) \ud835\udc1f,\n\nwhere \u03b2 is set as 0.2 empirically\u00a0<cit.>.\nThen the updated model is evaluated on the validation data to generate to validation loss with \ud835\udc9e_val replacing \ud835\udc9e_tr in Eq.\u00a0<ref>. \nThe architecture parameter is then updated with the validation loss to finish an iteration. \n\nAs the loss calculation does not rely on the linear classification layer, the categories of the training and validation sets are unbound. We are able to dynamically adjust the category overlap ratio in these two sets. \nThe advantages of a proper overlap ratio are summarized as two folds. \nFirstly, TCM better simulates the real-world training of ReID and helps the model focus on truly discriminative distinctions. The differences between the training and validation data improves the generalization capability of the model. \nSecondly, a relatively small proportion of overlapped categories stabilizes the architecture parameter update through a consistent optimizing target with the model parameter update. \n\n\n\n\n\n \u00a7.\u00a7 Multi-Scale Interaction Space\n\n\nAlthough the local perspective and multi-scale features have already been investigated in previous ReID works\u00a0<cit.>, the utilization of these information is mainly empirically designed aggregation, which is monotonous and restrained. \nWe argue that on the one hand, the rational utilization of multi-scale features should be dynamically adjusted along the shallow and deep layers of the network. \nOn the other hand, introducing interaction other than aggregation creates direct information exchange, and makes fuller use of multi-scale features. \nTherefore, we propose a novel Multi-Scale Interaction (MSI) search space to establish a light-weight architecture suitable for ReID. \n\nAs shown in Fig.\u00a0<ref>, the network is mainly grouped with MSI cells and down-sample blocks, which is generally consistent with OSNet\u00a0<cit.>. In each cell, the input features are passed through two branches with different receptive field scales. \nTo reduce the calculation burden of the network, for the layers inside each branch, we adopt the stack of 1\u00d71 convolution and multiple depth-wise 3\u00d73 convolution to implement specific scales. A scale ratio \u03c1 of 3:1 is selected for the two branches. \nThese two branches do not share model parameters, except for the Interaction Modules (IM). IM introduces information exchange for the two branches. There are 4 operation options for the IM. \nWith the two-branch input features defined as (\ud835\udc31_1, \ud835\udc31_2), the operations can be formulated as:\n\nNone. None operation involves no parameters, and outputs exactly the input features (\ud835\udc31_1, \ud835\udc31_2). \n\nExchange. Exchange acts as the strongest interaction among all options. It directly exchanges the features for the two branches and outputs (\ud835\udc31_2, \ud835\udc31_1). \nExchange contains no extra parameters, as well. \n\n\n\n\n\n\nChannel Gate. Channel gate introduces a Multi-Layer Perceptron (MLP) to generate a channel-wise attention gate\u00a0<cit.> as:\n\n    G(\ud835\udc31)=\u03c3(MLP(\ud835\udc31))),\n\nand returns (G(\ud835\udc31_1)\u00b7\ud835\udc31_1, G(\ud835\udc31_2)\u00b7\ud835\udc31_2). \nThe MLP is composed of 2 fully connected layers and its parameters are shared for both branches. Thereby it achieves interaction by jointly screening discriminative feature channels. \n\nCross Attention. \nTraditional channel attention module calculates the channel correlation inside a single feature map<cit.>. The original feature map \ud835\udc31\u2208\u211b^C\u00d7 H\u00d7 W is firstly reshaped into the query feature \ud835\udc31\u0303\u2208\u211b^C\u00d7 N, where N=H\u00d7 W. Then the correlation activation is calculated by performing a matrix multiplication between the query feature \ud835\udc31\u0303 and the key feature \ud835\udc31\u0303^\u22a4. We propose to exchange the keys of the two branches to explicitly calculate the correlation between each other. The correlation activation is then transformed to a mask, and is added up to the original features with a learnable proportion. \n\nAfter interaction, the multi-scale branches are fused through a sum operation. \nIt is worth noting that the extra parameters brought by multiple interaction options are limited, which enables searching for each cell along the whole network independently. \nAt the beginning of the network, we employ the same stem module as that in OSNet\u00a0<cit.>, containing a 7\u00d7 7 convolutional layer and a 3\u00d7 3 max pooling with a stride of 2. \nAfter the searching process, the interaction operation o with the largest weight \u03b1_i^o at each layer is reserved to form the searched architecture. \n\n\n\nAfter searching the architecture, the model is validated on various Re-ID tasks. \nThe training is constrained by the classification id loss and the triplet loss, formulated by:\n\n    \u2112_id=1/N\u2211_i=1^N-log(exp\ud835\udc16^\u22a4_i\ud835\udc1f_i/\u2211_jexp\ud835\udc16^\u22a4_j\ud835\udc1f_i),\n\nwhere \ud835\udc1f_i is a feature vector, the corresponding classifier weight of which is \ud835\udc16_i, and\n\n    \u2112_tri=[\ud835\udc9f(\ud835\udc1f_a,\ud835\udc1f_p)-\ud835\udc9f(\ud835\udc1f_a,\ud835\udc1f_n)+m]_+,\n\nwhere \ud835\udc1f_a, \ud835\udc1f_p, \ud835\udc1f_n are the embedded features for the anchor, the hardest positive and negative samples in a mini-batch, \ud835\udc9f(\u00b7,\u00b7) is the Euclidean distance, m is the margin parameter, and [\u00b7]_+ is the max(\u00b7,0) function. \n\n\n\n \u00a7.\u00a7 Spatial Alignment Module\n\n\nThe retrieval precision of object ReID tasks are largely affected by the variation of appearances such as poses, illumination and occlusion when the camera conditions change. In order that the model correctly and consistently focuses on the discriminative spatial positions, we design a Spatial Alignment Module (SAM) to explicitly align the spatial attention between images, as shown in Fig.\u00a0<ref>.  \n\nSpecifically, we first calculate the position-wise correlation activation map \ud835\udc00 between the feature maps in a mini-batch. The activation between sample i and j can be formulated as: \ud835\udc00(i, j)=\ud835\udc31\u0303_j^\u22a4\u00d7\ud835\udc31\u0303_i, where \ud835\udc31\u0303\u2208\u211b^C\u00d7 N is reshaped from the original feature \ud835\udc31\u2208\u211b^C\u00d7 H\u00d7 W. Then we take the maximum activation for each position of sample i as:\n\n    \ud835\udc1a(i,j)=max_dim=1\ud835\udc00(i,j).\n\nThe above process is denoted as \u201cMutual Conv\u201d in Fig.\u00a0<ref>. \nWe evaluate the consistency between activation vectors with cosine similarity. \nFor negative samples specifically, there can be many different hints for recognition, some of which might be inappropriate, such as the backgrounds. \nBy aligning all the correlations for sample i, we hope that the network can correct some attention bias and consistently focus on discriminative positions. \n\nHowever, through aligning positive sample pairs, the ID-related features are expected to be emphasized, which cannot be achieved by aligning negative pairs. \nTherefore, we introduce an extra position activation module (PAM) to generate supervision for the alignment between positive pairs. \nThe spatial alignment loss is formulated as:\n\n    \u2112_sa(i)=1/N_+\u2211_p\u2208\u2110_+(1-S(\ud835\udc1a\u0302(i), \ud835\udc1a(i,p)))+\n    1/N_-\u2211_n_1,n_2\u2208\u2110_-(1-S(\ud835\udc1a(i,n_1), \ud835\udc1a(i,n_2))),\n\nwhere \u2110_+ contains positive indices for sample i, the total number of which is N_+, and vice versa. \ud835\udc1a\u0302(i) stands for the generated activation vector for positive sample alignment, and S(\u00b7,\u00b7) is the cosine similarity. \n\n\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n \u00a7.\u00a7 Datasets and Evaluation Metrics\n\nOur proposed method is evaluated on two person ReID datasets Market-1501\u00a0<cit.>, MSMT17\u00a0<cit.>, and two vehicle ReID datasets VeRi-776\u00a0<cit.> and VehicleID\u00a0<cit.>. \nFor simplicity, the four datasets are denoted as M, MS, VR and VID in the following sections, respectively. \nEvaluation metrics include Cumulative Matching Characteristic (CMC) and mean average precision (mAP), which are commonly utilized on ReID tasks. \n\n\n\n \u00a7.\u00a7 Architecture Search\n\n\nWe conduct the searching process on MSMT17. SGD is adopted for model parameter update with an initial learning rate of 0.025. The model is trained for 350 epochs in total. We adopt a warm-up strategy for the first 10 epochs. Then the learning rate is decayed by 0.1 at 150, 225 and 300 epochs, respectively. Adam\u00a0<cit.> is adopted for the architecture parameter update with an initial learning rate of 0.002. The learning rate is decayed at the same pace. The images are reshaped to 256\u00d7128 for person and 256\u00d7256 for vehicles. Data augmentation includes random flip, random crop and random erasing\u00a0<cit.>. The searched architecture is presented in Tab.\u00a0<ref>. The \u201cMSINet\u201d in the following experiment sections refers to this architecture. \n\nWe visualize the feature maps extracted by each MSI cell in Fig.\u00a0<ref>. At the shallow layers of the network, the kernels mainly focus on overall contour information. Channel gate helps to filter out inferior information, such as the background. \nAs we approach deeper layers, the extracted features each have specific semantic information, where cross attention is more likely to be selected for the interaction. It indicates that cross attention is more rational for exchanging high-level semantic information. \n\n\n\n \u00a7.\u00a7 Comparison with Other Backbones\n\n\n\n\n\n\n\nWe first compare our proposed MSINet with ResNet50 and recent proposed light-weight backbones in both in-domain and cross-domain ReID scenarios. \n\nIn-Domain Tasks. \nWe adopt a two-group supervised evaluation scheme similar to that in\u00a0<cit.>: training from scratch and fine-tuning ImageNet<cit.> pre-trained models. The training parameters for both schemes are kept the same as that in architecture search, except for an initial learning rate of 0.065. Triplet loss and cross entropy loss are adopted for the parameter update. \nThe margin m in Eq.\u00a0<ref> is set as 0.3. \n<cit.> adopts an FBLNeck. We also employ the same structure. The results are shown in Tab.\u00a0<ref>. \n\nResNet50 is the most commonly utilized backbone network in ReID tasks, yet holds the worst performance. \nMoreover, ResNet50 largely depends on ImageNet pre-training, while MSINet without pre-training has already surpassed pre-trained ResNet50 on all metrics. \nCompared with the other datasets, MS contains more variations on illumination, background and camera pose, and brings a large performance gap between ResNet50 and other methods. It also validates the inadequacy of image classification networks on ReID tasks. \nOSNet\u00a0<cit.> and CDNet\u00a0<cit.> are recently proposed architectures designed specifically for ReID tasks. Both architectures focus on fusing multi-scale features to better suit ReID. CDNet employs a traditional NAS scheme to search for the proper receptive field scales for each cell. MSINet fixes the receptive field scale and instead selects optimal interaction operations inside each cell. With only a bit more parameters, MSINet surpasses all the other backbones by a large margin. \n\n\n\n\n\n\n\nCross-Domain Tasks.\nCross-domain experiments verify the generalization capability of the model. Following previous domain generalizable ReID works\u00a0<cit.>, data augmentation is adjusted to random flip, random crop and color jittering. The model is pre-trained and fine-tuned for 250 epochs to avoid over-fitting. The other settings are kept the same as supervision scenes. With no present pre-trained models for CDNet\u00a0<cit.>, it is excluded from this section. \n\nTab.\u00a0<ref> shows that ResNet50 can be easily interfered by different image styles confronted with new image domains. \nOSNet learns multi-scale features with specific semantic information for ReID, which is domain invariant to some extent. \nOur proposed search scheme also takes into account the generalization capability of the model. By partly separating the categories for training and validation sets, the searched interaction operations generalize well confronted with new image domains. Except for discrimination, MSINet also surpasses the other backbones on cross-domain tasks by a large margin with faster inference speed. \n\nAdditionally, we introduce SAM into the model, which aligns the spatial correlations between images. A weighted sum of ReID loss and spatial alignment loss is utilized when training the network with SAM. The weight of spatial alignment loss is set as \u03bb_sa=2.0. Without extra inference consumption or damages on the supervised performance, SAM further boosts the generalization capability of MSINet. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Comparison with State-of-the-art Methods\n\n\nTab.\u00a0<ref> further illustrates the supervision performance comparison of our proposed MSINet with the SOTA methods on M and MS datasets. \nWith much less parameters than most of the compared methods, MSINet achieves a retrieval accuracy comparable to that of more complicated ones. \nAuto-ReID\u00a0<cit.> first designs a NAS scheme for ReID, yet the DARTS-style architecture contains 13M parameters. \nRGA-SC\u00a0<cit.> carefully designs a relation-aware global attention module. \nMSINet achieves even higher performance with less training consumption, which validates the superiority of selecting rational interaction. \n\nWe also evaluate the model performance replacing the backbone network from ResNet50 to MSINet for SOTA unsupervised ReID methods in Tab.\u00a0<ref>. \nFor purely unsupervised learning (USL) method GS\u00a0<cit.> on M dataset, MSINet performs slightly lower on rank-1, yet has a large superiority on mAP. \nFor HDCRL\u00a0<cit.>, MSINet shows obvious superiority over ResNet50. \nFor unsupervised domain adaptation (UDA) method IDM\u00a0<cit.> on M\u2192MS task, MSINet surpasses ResNet50 by a large margin, which further proves that the TCM brings outstanding generalization capability to the searched architecture. \n\n\n\n \u00a7.\u00a7 Ablation Studies\n\n\nEffectiveness of Architecture Search.\nTo verify the effectiveness, we conduct supervised training on MS with different search schemes in Fig.\u00a0<ref> (a). \nUnder the standard classification scheme (\u201cCE Overlap\u201d), the searched model performs poor. Replacing the cross entropy loss to contrastive loss (\u201cTCM Overlap\u201d) only brings a slight improvement. The complete TCM framework unbounds the categories between the training and validation set, and thereby improves the performance by a large margin. \n\n\nIn Fig.\u00a0<ref> (b) we compare the performance of different architectures. \nFirstly, we validate 4 models each with a unique interaction operation from the 4 options in the search space. \nNone and Exchange, with no trainable parameters, achieve poor performance. Channel gate introduces channel-wise attention, whose model performs the best among 4 options. Cross attention exchanges the key features for the two branches. Over-frequent exchange interferes the ordinary feature extraction and degrades the network performance. \nThrough appropriately arranging interaction operations along the architecture, MSINet surpasses all the above 4 models. Random architecture, on the other hand, shows no rational appliance of interaction operations, which validates that the proposed search scheme helps find suitable architectures for ReID. \n\nEffectiveness of Spatial Alignment Module.\nWe validate the effectiveness of each components of SAM on the VR\u2192VID cross-domain experiment in Tab.\u00a0<ref>. \nFirstly, we introduce the spatial alignment for positive and negative sample pairs, respectively. Each of them brings certain performance improvements. \nHowever, a unified alignment for all sample pairs damages ID-related features and degrades the performance instead. \nTherefore, we separate the alignment of positive and negative samples, which retains some discriminative features and integrates the effect of both aspects. \nThe extra PAM for positive sample alignment further guarantees the focus on ID-related positions and achieves the best performance.\nWe also conduct in-domain experiment on VR to prove that SAM improves the generalization capability without sacrificing the supervision performance. \nAdding SAM to OSNet receives similar results, which validates the universality of SAM. \n\n\n\n\n\nFusing Operation.\nAfter interaction, the multi-scale features are fused by sum operation. We investigate several fusing options on MS training from scratch in Fig.\u00a0<ref> (c). Subtracting (\u201cMinus\u201d) a branch from the other leads to about the same results as \u201cSum\u201d while multiplication (\u201cMul\u201d) performs poorly. \n\nComparison with Transformer.\nTransformer, as a new architecture, has recently been continuously making progresses in many computer vision domains\u00a0<cit.>, including ReID\u00a0<cit.>. We compare the model performance with some baseline Transformer models in Tab.\u00a0<ref>. \nDeiT-B and ViT-B\u00a0<cit.> achieves higher performance on MSMT17, with much larger calculation burden compared with our proposed MSINet. On VeRi-776, MSINet surpasses all the baseline Transformer methods. \nIt proves that rational interaction operations between multi-scale features are capable to assist light-weighted pure-CNN models to obtain comparable performance with complex Transformers. \n\nParameter Analysis.\nFirstly, we study the influence of different receptive field scale ratios \u03c1 inside an MSI cell on MS training from scratch in Fig.\u00a0<ref> (d).\nIntroducing scale differences between branches improves the model performance significantly, and subsequent increases brings more modest impacts. \nConsidering both parameter scales and model performance, the ratio of 3:1 is selected for MSINet. \n\nSecondly, the model performance fluctuation influenced by spatial alignment weight is visualized in Fig.\u00a0<ref> (e). The experiment is conducted on the VR\u2192VID cross-domain scenario. \nEmploying the alignment generally makes a positive impact on the generalization capability of the model. The optimal loss weight \u03bb_sa locates at 2.0. \n\nVisualization Results.\nWe visualize the top-15 retrieved sequences and the corresponding distances from an query image on VR in Fig.\u00a0<ref>. By comparison, ResNet50 mainly focuses on general appearance features, where the top-rated negative samples share similar car bodies. MSINet, oppositely, concentrates on discriminative distinctions, an empty car hopper in this case, and creates an evident distance gap between positive and negative samples. More details can be seen in the supplementation material. \n\n\n\n\u00a7 CONCLUSION\n\nIn this paper, we design a Twins Contrastive Mechanism for NAS to build the compatibility with ReID. The task-specific search scheme provides the searching process with more appropriate supervision. A Multi-Scale Interaction search space is proposed to establish rational and flexible utilization of multi-scale features. With a Spatial Alignment Module, our proposed MSINet achieves SOTA performance on both supervision and cross-domain scenarios with limited parameter amount. \nWe hope the proposed approach could inspire more works focusing on designing network architectures suitable for ReID tasks. \n\n\n\n\n\n\n\n\n\u00a7 ACKNOWLEDGEMENT\n\nThis work is partially supported by National Natural Science Foundation of China (62173302, 62006244), China Scholarship Council (202206320302), Young Elite Scientist Sponsorship Program of China Association for Science and Technology (YESS20200140), and AI Singapore Programme (AISG2-PhD-2021-08-008).\n\n\nieee_fullname\n\n\n\n\n\n\n\n\u00a7 SEARCH ON VERI-776\n\n\n\n\n\n\n\n\n\nWe select a training-validation ratio of 60%-80% in the searching process on MSMT17\u00a0<cit.>. Without changing any specific configurations, we directly search for the rational interaction operations on VeRi-776\u00a0<cit.> dataset. The searched architecture is denoted as MSINet-VR. We compare the structure of MSINet and MSINet-VR in Tab\u00a0<ref>. Generally, the two searched architecture have common characteristics: Channel Gate is preferred in shallow layers, while Cross Attention is employed for more thorough information interaction in deep layers. \n\nQuantitatively, we also conduct relevant supervision and cross-domain experiments with MSINet-VR in Tab.\u00a0<ref>. All the experiment configurations are kept the same as those of MSINet training. Although there are some fluctuations, generally MSINet-VR has similar performance to MSINet, and the retrieval accuracy still surpasses ResNet50\u00a0<cit.> by a large margin. \n\n\n\n\u00a7 SEARCH WITH DIFFERENT OVERLAP RATIOS\n\n\nWith the identities of training and validation sets unbound, we conduct a series of experiments utilizing different data separation ratios in Tab.\u00a0<ref> to find the appropriate interaction operations for the network. \nFirstly, we separate the training and validation sets completely with no identity overlaps. It can be observed that a balanced train-validation ratio generally brings better performance. \nFor the two extremes of data distribution, an over-small validation set makes the architecture optimizer stuck in local minima and achieves poor performance. On the contrary, an over-large validation set brings no severe damage to the architecture search process, despite that the model is still not optimal. It demonstrates that abundant validation data is essential for ReID NAS. \n\n\n\n\n\nSecondly, we randomly select part of identities, and evenly divide their images into the training and validation sets. \nThe experiment results suggest that having a relatively small proportion of overlapped identities, whose images have been partly utilized for model parameter update, stabilizes the searching process and leads to a better architecture. However, when the overlap increases to a certain extent, the resemblance between the training and validation sets will bring negative influence to the ReID architecture search. \nAs a comparison, we conduct the search task with traditional NAS scheme where a linear classification layer and cross entropy loss are employed for the training and validation data, the searched model of which performs worse than our proposed TCM.\n\nCombined with above rules and the model performance, we select the architecture searched with the train-validation split of 60%-80% as the proposed MSINet.\n\n\n\n\n\n\u00a7 SEARCH WITH SOFTMAX LOSS\n\n\nWe further compare the detailed interaction operations between MSINet and the architecture searched under traditional NAS scheme, where softmax loss and a unified linear classification layer are utilized for the training and validation sets\u00a0<cit.> (denoted as MSINet-S) in Tab.\u00a0<ref>. Compared with MSINet and MSINet-VR, where direct information exchange mainly appears at deep layers, MSINet-S contains a large amount of Exchange and Cross Attention along the whole network. The over-frequent information exchange fails to focus on discriminative features. It also validates the effectiveness of our proposed Twins Contrastive Mechanism on searching for architectures suitable for ReID. \n\n\n\n\u00a7 VISUALIZATION RESULTS\n\n\nSome additional visualization results are illustrated to further manifest the effectiveness of our proposed architecture. \nFirstly, we visualize an example comparison of the top-20 retrieved sequences between ResNet50 and MSINet on MSMT17 in Fig.\u00a0<ref>. ResNet50 mainly focus on general appearance information, while our proposed MSINet concentrates on discriminative distinctions, the hand bag in this case. Even though positive samples have large appearance differences from the query image, MSINet is still capable to distinguish them. \n\nSecondly, example activation maps of ResNet50 and our proposed MSINet on Market-1501\u00a0<cit.> are visualized in Fig.\u00a0<ref>. ResNet50 mainly focuses on the right part of the image, including some background areas. Our proposed MSINet, oppositely, is capable to dynamically focus on discriminative distinctions of each image. \n\nThirdly, to intuitively demonstrate the effectiveness of the Spatial Alignment Module (SAM) on enhancing the attention consistency of the model confronted with images from different sources, we visualize example activation maps on the task of VR\u2192VID. As shown in Fig.\u00a0<ref>, without alignment, the model can have different activated positions on different images of the same identity, even if they share similar appearances. \n\n\n\n\u00a7 COMPARISON AND ADVANTAGES TO OSNET\n\n(1) OSNet simply sums up the features of each branch, without detailed exploration on the interaction between branches. In comparison, MSINet practically select rational interaction operations for different network layers. Consequently, MSINet surpasses OSNet not only in supervised, but also in domain generalization performance by a large margin. \n(2) OSNet contains 4 branches with different receptive field scales, where there exists certain parameter redundancy. We validated in the early exploring that removing the branches with receptive field scales of 3 and 5 has little influence to the model performance. \nMSINet reduces the number of branches, and increases the scale difference between two branches, which increases the parameter amount by a little bit but significantly reduces the inference time. \n\n\n\n\u00a7 DETAILED ANALYSIS ON SAM\n\nWe compare the proposed SAM module to some previous attention-based methods and analyze it in detail. \n<cit.> regularizes the attention generated at different network layers for the same image; <cit.> explicitly enforces the longitudinal activation distribution to be the same for two images, which may lead to misalignment if the objects are not properly detected. \nFor negative samples, there can be many different hints for recognition, some of which might be inappropriate, such as the backgrounds. By aggregating the information from different negative samples, the network is driven to only focus on discriminative regions. \nThe motivation of SAM is different from the above two works. \nFor the in-domain setting, the camera condition diffrences are directly addressed by supervised learning. \nThus, SAM brings limited improvements, yet doesn't defect the performance, compared to techniques like instance normalization. \n\n\n\n\n\n\n\n\n\n\u00a7 MORE ABLATION STUDY\n\nAdditional cross-domain experiments. \nWe add the M\u2192MS and VID\u2192VR experiment results to Tab.\u00a0<ref>. MSINet surpasses OSNet on all metrics. \n\nAblation study on softmax operation. \nSAM aligns the activation values in the feature map, where the discriminative positions are actually matched between different samples. \nAs the \u201cMutual Conv\u201d operation is non-parametric, it is not proper to apply the softmax-squeezed position attention values for direct alignment, which may result in scale inconsistency. \n\nThe experiment results in Tab.\u00a0<ref> also suggest slight influence on this detail. \n\n\n\n\u00a7 LIMITATIONS AND FUTURE WORK\n\nThe designed interaction operations only include forward and exchange in the direct and attention forms, which restricts the size of the search space. \nIn the future works, there are still exploration room for more elaborate and complicated interaction operations and search spaces. \nThere are still exploration room for more complicated search schemes and spaces. \n\n"}