{"entry_id": "http://arxiv.org/abs/2303.06834v1", "published": "20230313033129", "title": "DarkVisionNet: Low-Light Imaging via RGB-NIR Fusion with Deep Inconsistency Prior", "authors": ["Shuangping Jin", "Bingbing Yu", "Minhao Jing", "Yi Zhou", "Jiajun Liang", "Renhe Ji"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\nLabel Information Bottleneck for Label Enhancement\n    Qinghai Zheng^1,\u00a0 Jihua Zhu^2[1]Corresponding author, E-mail: zhujh@xjtu.edu.cn,\u00a0 Haoyu Tang^3 \n\n\t^1College of Computer and Data Science, Fuzhou University, China \n\n\t^2School of Software Engineering, Xi'an Jiaotong University, Xi'an, China \n\n\t^3School of Software, Shandong University, Jinan, China \n\n\t\n\n\n\n\n\n\n    Received: date / Accepted: date\n========================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\n\n\nRGB-NIR fusion is a promising method for low-light imaging. \nHowever, high-intensity noise in low-light images amplifies the effect of structure inconsistency between RGB-NIR images, which fails existing algorithms. \nTo handle this, we propose a new RGB-NIR fusion algorithm called Dark Vision Net (DVN) with two technical novelties: Deep Structure and Deep Inconsistency Prior (DIP). \nThe Deep Structure extracts clear structure details in deep multiscale feature space rather than raw input space, which is more robust to noisy inputs. \nBased on the deep structures from both RGB and NIR domains, we introduce the DIP to leverage the structure inconsistency to guide the fusion of RGB-NIR. \nBenefiting from this, the proposed DVN obtains high-quality low-light images without the visual artifacts. \n\nWe also propose a new dataset called Dark Vision Dataset (DVD), consisting of aligned RGB-NIR image pairs, as the first public RGB-NIR fusion benchmark. Quantitative and qualitative results on the proposed benchmark show that DVN significantly outperforms other comparison algorithms in PSNR and SSIM, especially in extremely low light conditions.\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\nHigh-quality low-light imaging is a challenging but significant task. On the one hand, it is the cornerstone of many important applications such as 24-hour surveillance, smartphone photography, etc. On the other hand, though, massive noise of images in extremely dark environments hinders algorithms from the satisfactory restoration of low-light images. RGB-NIR fusion techniques provide a new perspective for the challenge: it enhances the low-light noisy color (RGB) image through rich, detailed information in the corresponding near-infrared (NIR) image (The high quality of NIR images in dark environments comes from invisible near-infrared flash), which greatly improves the signal-to-noise ratio (SNR) of the restored RGB image. Under the constraint of cost, size and other factors, RGB-NIR fusion becomes the most promising technique to restore the vanished textual and structural details from noisy RGB images taken in an extremely low-light environments, as shown in Figure <ref>(a).\n\n\n\n\n\n\n\n\nHowever, the existing RGB-NIR fusion algorithms suffers from the problem of structure inconsistency between RGB and NIR images, resulting in unnatural appearance and loss of key information, which limits the application of RGB-NIR fusion algorithm in low-light imaging. \nFigure <ref> illustrates two typical examples of structure inconsistency between RGB and NIR images: Figure <ref>(b) shows the absence of NIR shadows in the RGB image (grass shadows only appear on the book edge in the NIR image), and the nonexistence of RGB color structure in the NIR image (text \u2018complete\u2019 almost disappears on the book cover in the NIR image). Fusion algorithms need to tackle these structure inconsistency to avoid visual artifacts in output images. There are two categories of RGB-NIR fusion methods currently, i.e. traditional methods and neural-network-based methods, and modeling the structure of the paired RGB-NIR images plays an important role in both of them. Traditional methods, such as Scale Map <cit.>, tackle the structure inconsistency problem by manually designed functions. Some neural-network-based methods <cit.>, on the other hand, utilize deep learning techniques to automatically learn the structure inconsistency by a large amount of data. Both of them perform well under certain circumstances.\n\nHowever, when confronted with extreme low-light environments, existing methods fail to maintain satisfactory performance, since the structure inconsistency is dramatically exacerbated by massive noise in the RGB image. As shown in Figure <ref>(b), the dense noise in the RGB image makes it difficult for Scale Map to extract structural information, causing the failure of distinguishing which structures in the NIR image should be eliminated, and result in the unnatural ghost images on the book edge. Deformable Kernel Networks (DKN) <cit.> falsely weakens gradients of input RGB image that do not exist in the corresponding NIR image, which leads to the blurriness of letters on the book cover. Even though these structural inconsistency of corresponding RGB and NIR images can be captured by human eyes effortlessly, they still confuse most of the existing fusion algorithms.\n\n\n\nIn this paper, we focus on improving the RGB-NIR fusion algorithm for extremely low SNR images by tackling the structure inconsistency problem. Based on the above analysis, we argue that the structure inconsistency under extremely low light can be handled well by introducing prior knowledge into deep features. To achieve this, we propose a deep RGB-NIR fusion algorithm called Dark Vision Net (DVN), which explicitly leverages the prior knowledge of structure inconsistency to guide the fusion of RGB-NIR deep features, as shown in Figure <ref>. With DVN, two technical novelties are introduced: (1) We find a new way, referred to as deep structures, to represent clear structure information encoded in deep features extracted by the proposed Deep Structure Extraction Module (DSEM). Even facing images with low SNR, the deep structures can still be effectively extracted and represent reliable structural information, which is critical to the introduction of prior knowledge. (2) We propose Deep Inconsistency Prior (DIP), which indicates the differences between RGB-NIR deep structures. Integrated into the fusion of RGB-NIR deep features, the DIP empowers the network to handle the structure inconsistency. Benefiting from this, the proposed DVN can obtain high-quality low-light images.\n\nIn addition, to the best of our knowledge, there is no available benchmark dedicated for the RGB-NIR fusion task so far. The lack of benchmarks to evaluate and train fusion algorithms greatly limits the development of this field. To fill this gap, we propose a dataset named Dark Vision Dataset (DVD) as the first RGB-NIR fusion benchmark. Based on this dataset, we give qualitative and quantitative evaluations to prove the effectiveness of our method.\nIn summary, the main contributions of this paper are as follows:\n\n\n\t\n  * We propose a novel RGB-NIR fusion algorithm called Dark Vision Net (DVN) With Deep Inconsistency Prior (DIP). The DIP explicitly integrates the prior of structure inconsistency into the deep features, avoiding over-relying on NIR features in the feature fusion. Benefits from this, DVN can obtain high-quality low-light images without visual artifacts.\n\t\n  * We propose a new dataset Dark Vision Dataset (DVD) as the first public dataset for training and evaluating RGB-NIR fusion algorithms.\n\t\n  * The quantitative and qualitative results indicate that DVN is significantly better than other compared methods.\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Image Denoising. \n\n\n\nIn recent years, denoising algorithms based on deep neural networks have continually emerged and overcome the drawbacks of analytical methods <cit.>. The image noise model is gradually improved simultaneously <cit.>. \n<cit.> applied an encoder-decoder network to suppress the noise and recover high-quality images. \n<cit.> presented a denoising network to process blind noise denoising.\n<cit.> attempted to remove the noise from real noisy images.\nThere are also deep denoising algorithms trained without clean data supervision <cit.>. \nHowever, in extremely dark environments, fine texture details damaged by the high-intensity noise are very difficult to restore. In that case, denoising algorithms tend to generate over-smoothed outputs. By the way, low-light image enhancement algorithms <cit.> try to directly restore high-quality images in terms of brightness, color, etc. However, these algorithms cannot deal with such high-intensity noise as well. \n\n\n\n  \u00a7.\u00a7.\u00a7 RGB-NIR Fusion. \n\n\nTo obtain high-quality low-light images, researchers <cit.> try to fuse NIR images with RGB images. \n\n\n\nRecently, <cit.> pointed out the gradient inconsistency between RGB-NIR image pairs, and proposed Scale Map to try to solve it. \nAmong the methods based on deep neural network, Joint Image Filtering with Deep Convolutional Networks (DJFR) <cit.> constructs a unified two-stream network model for image fusion, CU-Net <cit.> combines sparse encoding with Convolutional Neural Networks (CNNs), DKN <cit.> explicitly learns sparse and spatially-variant kernels for image filtering. <cit.> innovatively constructs a network that directly decouples RGB and NIR signals for 24-hour imaging. \nIn general, current RGB-NIR fusion algorithm has two main problems: insufficient ability to deal with RGB-NIR texture inconsistency, leading to heavy artifacts on the final fusion images, inadequate noise suppression capability especially when dealing with high-intensity noise in extremely low-light environments. \n\n\n\n\n  \u00a7.\u00a7.\u00a7 Datasets. \n\nThere is only a small amount of data that can be used for RGB-NIR fusion studies because of the difficulty to obtain aligned RGB-NIR image pairs. Some studies <cit.> focus on obtaining hyperspectral datasets, and strictly aligned RGB-NIR image pairs can be obtained by integrating hyperspectral images on the corresponding band. \n<cit.> present a prototype camera to collect RGB-NIR image pairs. However, these datasets are too small to be used to comprehensively measure the performance of fusion algorithms. \nMore importantly, due to the lack of data on actual scenarios, they cannot encourage follow-up researchers to focus on the valuable problems that RGB-NIR will encounter in applications. \n\n\n\n\n\u00a7 APPROACH\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Prior Knowledge of Structure Inconsistency\n\nAs previously described, the network needs to be aware of the inconsistent regions on the two inputs. We design an intuitive function to measure the inconsistency from image features. Firstly binary edge maps are extracted from each feature channel. Then the inconsistency is defined as\n\n    \u2131 (edge^C_:, edge^N)    = \u03bb (1 - edge^C_:)(1 - edge^N) \n          + edge^C_:\u00b7 edge^N\n\nwhere C_:\u2208\u211d^H \u00d7 W and N \u2208\u211d^H \u00d7 W denote R/G/B channel of the clean RGB image and NIR image, edge^C_: and edge^N respectively represent the binarized edge maps of C_: and N, which is obtained by binarizing its mean value as a threshold after Sobel filtering. \n\nAs shown in Figure <ref>, \u2131(\u00b7,\u00b7) equals to 0 in the regions where edge^C_: and edge^N shows severe inconsistency. \nOn the contrary, \u2131(\u00b7,\u00b7) equals to 1 in the regions where the structures of RGB and NIR are consistent. \nAnd in other regions, \u2131(\u00b7,\u00b7) is set to a hyperparameter \u03bb (0 < \u03bb < 1), indicating that there is no significant inconsistency. \nUtilising the output inconsistency map of \u2131, the inconsistent NIR structures can be easily suppressed by a direct multiplication.\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Extraction of Deep Structures\n\nEven though function \u2131 subtly describes the inconsistency between RGB and NIR images, it cannot be applied directly in extremely low light cases. \nAs shown in Figure <ref>, the calculated inconsistency map contains nothing but non-informative noise when facing extremely noisy RGB image. \nTo avoid the influence of noise in the structure inconsistency extraction, we propose the Deep Structure Extraction Module (DSEM) and Deep Inconsistency Prior (DIP), where we compute the structure inconsistency in feature space. \nConsidering the processing flow of RGB and NIR are basically the same, we give a unified description here to keep the symbols concise. \n\n\n\n\n\n\n\n\nThe detailed architecture of DSEM is shown in Figure <ref>(a). \nDSEM takes multi-scale features feat_i (i represents scale) from restoration network R and outputs multi-scale deep structure maps struct_i. \nIn order for DSEM to predict high-quality deep structure maps, we introduce a clear supervision signal struct_i^gt (addressed later) for DSEM and the training loss is calculated as:\n\n    \u2112_stru = \u2211_i = 1^3\u2211_c = 1^Ch_i Dist(struct_i, c, struct_i, c^gt),\n\nwhere Ch_i is the channel number of the deep structures in the ith scale, Dist is Dice Loss <cit.>, struct_i, c is the cth channel of the predicted deep structures in the ith scale and struct_i, c^gt is the corresponding ground-truth. The Dice loss is given by\nDist(P, G) = ( \u2211_j^N p_j^2+\u2211_j^N g_j^2 ) / (2 \u2211_j^N p_j g_j), where p_j, g_j is the value of the jth pixel on the predicted structure map P and ground-truth G. \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Supervision of DSEM\n\nConsidering that it is almost impossible for DSEM to naturally output feature maps that only contain structural information, we have to introduce a clear supervision for the output of DSEM to predict high-quality deep structure maps. \nThe supervision signal is set up following the idea of Deep Image Prior <cit.> and struct_i, c^gt is acquired from a pretrained AutoEncoder <cit.> network AE [See the training details in supplementary material.]. \nThe base architecture of AE is exactly the same as R with skip connections removed, as Figure <ref>(b) shows. \nMulti-scale decoder features dec_i, c are extracted from the pretrained AutoEncoder network AE and the supervision signal is calculated by:\n\n    struct_i, c, j^gt =\n    \n    0    if  (\u2207 dec_i, c, j - m_\u2207 dec_i, c) <= 0,\n    \n    1    if  (\u2207 dec_i, c, j - m_\u2207 dec_i, c) > 0.\n\nwhere struct_i, c, j^gt is the jth pixel of struct_i, c^gt, \u2207 represents the Sobel operator, \u2207 dec_i, c, j is the jth pixel in \u2207 dec_i, c and m_\u2207 dec_i, c is the global average pooling result of \u2207 dec_i, c. The supervision signal obtained by this design effectively trains the DSEM and clear deep structure maps are predicted as shown in Figure <ref>.\n\n\n\n \u00a7.\u00a7 Calculation of DIP and Image Fusion\n\nThe extracted deep structures contain rich structure information and are robust to noise. With struct_i of the noisy RGB and NIR images, we can introduce inconsistency function \u2131 to obtain high-quality knowledge of structure inconsistency:\n\n    M^DIP_i, c   = \u2131(struct_i, c^C, struct_i, c^N)\n\nwhere C \u2208\u211d^H \u00d7 W \u00d7 3 and N \u2208\u211d^H \u00d7 W denote the noisy RGB image and NIR image, struct_i, c is the cth channel of the features from the ith scale and M^DIP_i, c is the corresponding inconsistency measurement. \nSince M^DIP_i, c represents the structure inconsistency instead of intensity inconsistency, we apply M^DIP_i, c directly to struct_i, c^N instead of feat^N_i, c in the form of:\n\n    \u015dt\u0302r\u0302\u00fb\u0109t\u0302_\u0302\u00ee,\u0302 \u0302\u0109^\u0302N\u0302 = M^DIP_i, c\u00b7 struct_i, c^N.\n\nUnder the guidance of the DIP, \u015dt\u0302r\u0302\u00fb\u0109t\u0302_\u0302\u00ee,\u0302 \u0302\u0109^\u0302N\u0302 discards the structures that are inconsistent with RGB, thus empowering the deep features with prior knowledge to tackle structure inconsistency. \n\nAs we will show in the experiments later, inconsistent structures in the NIR structure maps can be significantly suppressed after multiplying with M^DIP_i, c. \n\n\n\nTo further fuse the rich details contained in \u015dt\u0302r\u0302\u00fb\u0109t\u0302_\u0302\u00ee,\u0302 \u0302\u0109^\u0302N\u0302 into RGB features, we designed a multi-scale fusion module as shown in Figure <ref>(c). As pointed out in <cit.>, denoising first and fusion later can improve the fusion quality. So we follow <cit.> to reuse the denoised output of the restoration network R set up for noisy RGB as the input of the multi-scale fusion module.\n\n\n\n\n \u00a7.\u00a7 Loss Function\n\nThe total loss function we used is formulated as:\n\n    \u2112 = \u2112_rec^C + \u2112_rec^\u0108 + \u2112_rec^N + \u03bb_1 \u00b7\u2112_stru^C + \u03bb_2 \u00b7\u2112_stru^N\n\nwhere \u2112_stru^C and \u2112_stru^N are the loss function for RGB/NIR deep structures prediction, which is described above. \u03bb_1 and \u03bb_2 are the corresponding coefficients and set to 1/1000 and 1/3000. \n\u2112_rec^C, \u2112_rec^\u0108 and \u2112_rec^N represent the reconstruction loss of fused-RGB/coarse-RGB/NIR image respectively. All of them are Charbonnier loss <cit.> in the form of:\n\n    \u2112_rec   = \u221a(\ud835\udc17-\ud835\udc17_gt^2+\u03b5^2)\n\nwhere \ud835\udc17 and \ud835\udc17_gt represent the network output and the corresponding supervision. The constant \u03b5 is set to 10^-3 empirically. \n\n\n\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENT\n\n\n\n\n\n\n \u00a7.\u00a7 Datasets\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Data Collection. \n\nIn order to obtain the aligned RGB-NIR image pairs in the easiest and direct way, we collect all RGB-NIR image pairs by switching an optical filter placed directly in front of the camera without an IR-Cut, and we divide them into two types of image pairs for different usages.\nWe collect reference image pairs in normal-light environments. In order to obtain high-quality references, multiple still captures are averaged to remove noise and a matching algorithm <cit.> with manual double-check is applied to ensure the alignment of image pairs. In the following experiments, we add synthetic noise to these references to quantitatively evaluate the performance of fusion algorithms. To facilitate training, the collected images are cropped into 256*256 image patches.\nWe collect real noisy image pairs of 1920*1080 pixels in low-light environments. The post-processing steps are the same as those used in collecting references image pairs, except that multi-frame average is not performed to noisy RGB images. In the following experiments, we use these noisy image pairs to qualitatively evaluate the performances of fusion algorithms in handling real low-light images.\n\n\n\n  \u00a7.\u00a7.\u00a7 Dataset for Experiment. \n\nTo make the synthetic data closer to the real images, we follow <cit.> to add synthetic noise to reference image pairs for training and testing. Considering that all the comparison methods use sRGB (standard Red Green Blue) images as input, we convert the collected raw data into sRGB through a simple isp-pipeline (Gray World for white balance, Gamma correction, Demosaicing) <cit.>, to make a fair comparison. In the following experiments, we use 5k reference image pairs (256*256) as the training set. Another 1k reference image pairs (256*256) along with 10 additional real noisy image pairs (1920*1080) are used for testing. \n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Implementation Details\n\nAll experiments are conducted on a device equipped with two 2080-Ti GPUs. \nWe train the proposed DVN from scratch in an end-to-end fashion. \nBatchsize is set to 16. \nTraining images are randomly cropped in the size of 128*128, and the value range is [0, 1]. \nWe augment the training data following MPRNet <cit.>, including random flipping and rotating. \nAdam optimizer with momentum terms (0.9, 0.999) is applied for optimization. \nThe whole network is trained for 80 epochs, and the learning rate is gradually reduced from 2e-4 to 1e-6. \n\u03bb in function \u2131 is set to 0.5 for all configurations. \nThe AutoEncoder network used to provide supervision signals for DSEM is pretrained in the same way, except that it only trained for 5 epoches and the input is clean RGB and NIR images separately. See supplementary material for more training details. \n\nThe synthesis of low-light data for training includes two steps. \nThe first step is to reduce the average value of raw images taken under normal light to 5 (10-bit raw data). The second step is to add noise to the pseudo-dark raw images, including Gaussian noise with variance equals to \u03c3, and Poisson noise with a level proportional to \u03c3.\n\n\n\n \u00a7.\u00a7 Performance Comparison\n\n\n\n  \u00a7.\u00a7.\u00a7 Results on DVD Benchmark. \n\nWe evaluate and compare DVN with representative methods in related fields, including single-frame noise reduction algorithms NBNet <cit.> and MPRNet <cit.>, joint image filtering algorithms GIF <cit.>, DJFR <cit.>, DKN <cit.> and CUNet <cit.>, as well as Scale Map <cit.> which specially designed for RGB-NIR fusion. \n\n\nAll methods are trained or finetuned on DVD from scratch. \nWe use PSNR and SSIM <cit.> for quantitative measurement. \nQualitative comparison is shown in Figure <ref>, and quantitative comparison under different noise intensity settings (\u03c3 = 2, 4, 6, 8, the larger the \u03c3, the heavier the noise) on DVD benchmark is shown in Table <ref>. \n\n\nThe qualitative comparison in Figure <ref> clearly illustrates the superiority of the proposed DVN on noise removal, detail restoration and visual artifacts suppression.\nIn contrast, image denoising algorithms (i.e. NBNet and MPRNet) cannot restore texture details when the noise intensity becomes high, and the output turns into pieces of smear even though the noise is effectively suppressed. GIF and DJFR output images with heavy noise as the 3rd and 4th column in Figure <ref> shows, which greatly affects the fusion quality. The fusion effect of DKN and CUNet (5th and 6th column in Figure <ref>) under mild noise (e.g. \u03c3=2) is acceptable. But under heavy noise, obvious color deviation appears in the DKN output, and neither of them can deal with structure inconsistency (see the 4th row in Figure <ref>), resulting in severe artifacts in the fusion images. Scale Map outputs images with rich details. However, it cannot reduce the noise in the areas where texture is lacking in the NIR image. In addition, it is hard to achieve a balance between noise suppression and texture migration when applying Scale Map. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Generalization on Real Noisy RGB-NIR. \n\nTo evaluate the performance of algorithms when facing real low-light images, we conduct a qualitative experiment on several pairs of RGB-NIR images captured in real low-light environments.\n\nAs shown in Figure <ref>, outputs of DVN have obviously low noise, rich details, and are visually more natural when handling RGB-NIR pairs with real noise, even if the network is trained on a synthetic noisy dataset. \n\n\n\n  \u00a7.\u00a7.\u00a7 Comparison on Public Dataset. \n\nSo far, there is no high-quality public RGB-NIR dataset like DVD yet. For example, RGB-NIR pairs in IVRG <cit.> are not well aligned. Even so, we retrained DVN and other methods on IVRG and give quantitative comparison in Table <ref>. It is clear that DVN still performs well. \n\n\n\n  \u00a7.\u00a7.\u00a7 Comparison with Low-Light Enhancement Methods. \n\nWe also compare our method with the low-light enhancement methods. We retrained SID <cit.> and SGN <cit.>, the comparison can be seen in Table <ref>. It is clear that our proposed DVN still shows great superiority.\n\n\n\n \u00a7.\u00a7 Effectiveness of DIP\n\nIn this section, we verify that the proposed DIP is effective in handling the mentioned structure inconsistency. For comparison, we retrain a baseline, which is the same as the proposed DVN only without the DIP module. As Figure <ref>(a) shows, the NIR shadow of the grass still remains in the fusion result without DIP, but not in the fusion result with DIP. This directly proves that DIP can handle the structure inconsistency. Figure <ref>(b) shows that DIP can also deal with serious structure inconsistency caused by the misalignment between RGB-NIR images to a certain extent (this example pair cannot be aligned even after image registration). This has practical value because the problem of misalignment frequently occurs in applications. Taking into account the nature of DIP, the remaining artifacts are in line with expectations, since they are concentrated near the pixels \nwith gradients in the RGB image. \n\nIn addition, Figure <ref> also visualizes the deep structure of RGB, NIR, consistent NIR (DIP-weighted) as well as DIP Maps. It is obvious that even facing noisy input, the RGB deep structure still contains clear structures. The visual comparison between the NIR deep structure and the consistent NIR deep structure proves that the introduction of DIP can handle structure inconsistency in deep feature space.\n\n\n\n\n \u00a7.\u00a7 Ablation Study\n\nWe evaluate the effectiveness of each component in the proposed algorithm on the DVD benchmark quantitatively in this section. PSNR and SSIM are reported in Table <ref>. The baseline network directly fuse NIR features with RGB features (row 1 in Table <ref>). \n\n\n\nIntermediate supervision \u2112_rec^\u0108 and \u2112_rec^N effectively improve the performance as Table <ref> (row 1 and 2) shows.\nThis indicates the necessity of enhancing the noise suppression capability of the network for clean structure extraction. \n\nApplying DSEM to learn deep structures without DIP can improve performance as well as Table <ref> (row 1 and 3) shows. However, since the inconsistent structures are not removed, the benefits are not obvious, even if we use intermediate supervision and DSEM simultaneously as row 4 shows.\n\nAs Table <ref> (row 5) shows, after introducing DIP to deal with the structure inconsistency, the network performance can be further improved by a large margin. This demonstrates the effectiveness of our proposed algorithm and the necessity to focus on the structure inconsistency problem on RGB-NIR fusion problem.\n\n\n\n\n\u00a7 CONCLUSION\n\nIn this paper, we propose a novel RGB-NIR fusion algorithm called Dark Vision Net (DVN). DVN introduces Deep inconsistency prior (DIP) to integrate the structure inconsistency into the deep convolution features, so that DVN can obtain a high-quality fusion result without visual artifacts. In addition, we also proposed the first available benchmark, which is called Dark Vision Dataset (DVD), for RGB-NIR fusion algorithms training and evaluation. Quantitative and qualitative results prove that the DVN is significantly better than other algorithms.\n\n\n\n\u00a7 ACKNOWLEDGEMENTS\n\nThis paper is supported by the National Key R&D Plan\nof the Ministry of Science and Technology (Project No.\n2020AAA0104400) and Beijing Academy of Artificial Intelligence (BAAI).\n\n\n\n\n\n"}