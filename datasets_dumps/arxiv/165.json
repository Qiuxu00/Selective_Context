{"entry_id": "http://arxiv.org/abs/2303.07129v1", "published": "20230313135920", "title": "AdaptiveNet: Post-deployment Neural Architecture Adaptation for Diverse Edge Environments", "authors": ["Hao Wen", "Yuanchun Li", "Zunshuai Zhang", "Shiqi Jiang", "Xiaozhou Ye", "Ye Ouyang", "Ya-Qin Zhang", "Yunxin Liu"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.DC"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep learning models are increasingly deployed to edge devices for real-time applications. To ensure stable service quality across diverse edge environments, it is highly desirable to generate tailored model architectures for different conditions. However, conventional pre-deployment model generation approaches are not satisfactory due to the difficulty of handling the diversity of edge environments and the demand for edge information. In this paper, we propose to adapt the model architecture after deployment in the target environment, where the model quality can be precisely measured and private edge data can be retained. To achieve efficient and effective edge model generation, we introduce a pretraining-assisted on-cloud model elastification method and an edge-friendly on-device architecture search method. Model elastification generates a high-quality search space of model architectures with the guidance of a developer-specified oracle model. Each subnet in the space is a valid model with different environment affinity, and each device efficiently finds and maintains the most suitable subnet based on a series of edge-tailored optimizations. Extensive experiments on various edge devices demonstrate that our approach is able to achieve significantly better accuracy-latency tradeoffs (e.g. 46.74% higher on average accuracy with a 60% latency budget) than strong baselines with minimal overhead (13 GPU hours in the cloud and 2 minutes on the edge server).\n\n\n: Post-deployment Neural Architecture Adaptation for Diverse Edge Environments\n    M.\u00a0Kuich^1e-mail: mkuich@fuw.edu.pl, M.\u00a0\u0106wiok^1, W.\u00a0Dominik^1, A.\u00a0Fija\u0142kowska^1, M.\u00a0Fila^1, A.\u00a0Giska^1, Z.\u00a0Janas^1, A.\u00a0Kalinowski^1, K.\u00a0Kierzkowski^1, C.\u00a0Mazzocchi^1, W.\u00a0Okli\u0144ski^1, M.\u00a0Zaremba^1, D.\u00a0Grz\u0105dziel^2, J.\u00a0Lekki^2, W.\u00a0Kr\u00f3las^2, A.\u00a0Kuli\u0144ska^2, A.\u00a0Kurowski^2, W.\u00a0Janik^2, T.\u00a0Pieprzyca^2, Z.\u00a0Szklarz^2, M.\u00a0Scholz^2, M.\u00a0Turza\u0144ski^2, U.\u00a0Wi\u0105cek^2, U.\u00a0Wo\u017anicka^2, A.\u00a0Caciolli^3, M.\u00a0Campostrini^4, V.\u00a0Rigato^4, M.\u00a0Gai^5, H.\u00a0O.\u00a0U.\u00a0Fynbo^6\n^1Faculty of Physics, University of Warsaw, Warsaw, Poland\n\n^2Institute of Nuclear Physics Polish Academy of Sciences, Cracow, Poland\n\n^3University of Padova and INFN-PD, Padova, Italy\n\n^4Laboratori Nazionali di Legnaro, Legnaro, Italy\n^5University of Connecticut, CT, USA \n ^6Department of Physics and Astronomy, Aarhus University, Aarhus, Denmark\n\n\n    March 30, 2023\n=========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\n\nDeep learning has enabled and enhanced many intelligent applications at the edge, such as driving assistance <cit.>, face authentication <cit.>, video surveillance <cit.>, speech recognition <cit.>, etc.\n\nDue to latency and privacy considerations, it is an increasingly common practice to deploy the models to edge devices <cit.>, so that the models can be invoked directly without transmitting data to the server.\n\n\n\n\n\nThe diversity of execution environments is a unique characteristic of edge devices as compared with the cloud.\nFor example, a video app may deploy a super-resolution model on millions of smartphones, ranging from low-end devices to high-end ones, and their computational capacity may differ by up to 20 times. Generating a model for each type of device to guarantee the user experience is very time-consuming; an object detection model may run on different kinds of driving assistance systems, and the computational power may range from 20 to 1000 TOPS. To guarantee safety, the model inference usually has a strict latency budget.\n\nEven on the same type of devices, the model execution environments may also vary across instances and change over time due to different hardware states and concurrent processes.\nTo provide a good and uniform user experience, developers are usually required to generate tailored models for diverse edge environments.\n\n\n\n\n\n\n\n\n\nThere are many techniques proposed to automatically generate tailored models according to the target environments.\nMost of them are cloud-based approaches, in which the models are determined on the cloud side before distributing to edge devices. We call them \u201cpre-deployment approaches\u201d in this paper.\nNeural Architecture Search (NAS) <cit.> is the most popular technique of this type due to its superior flexibility to change network architectures.\nIt typically requires collecting information (about computational resources, runtime conditions, data distribution, etc.) from the target environments to guide the model architecture search and training processes in the cloud.\n\n\n\n\nDespite the effectiveness to find optimal model architecture based on the target environment, NAS approaches are less practical in many edge/mobile scenarios where the model execution environments may be very diverse and dynamic.\nSearching and maintaining the optimal model architecture in the cloud for each edge would be very compute- and labor-intensive.\nThus, a more economic and ideal solution is to let the model self-adapt to the target environment after deployment, which we call \u201cpost-deployment approach\u201d to distinguish with the conventional methods, as illustrated in Figure\u00a0<ref>.\nDoing so brings several other benefits - the quality of model architectures can be more precisely measured in the target environment, and user privacy can be better protected because there is no need to collect edge information. \n\n\n\n\n\nThe idea of adapting the model to the target device has been explored in both the mobile computing community <cit.> and the machine learning community <cit.>.\nThe mobile community is mainly focused on model scaling, adjusting the model complexity to fulfill certain latency requirements, while ML research mainly aims to deal with different data distributions or hard/easy samples.\nModel scaling approaches share a similar goal as ours, but prior work only shrinks the model size through pruning or quantization instead of changing the model architecture, which limits the opportunity to achieve optimal accuracy-latency tradeoffs.\n\n\n\n\n\n\n\n\n\n\n\n\nTo this end, we introduce , an end-to-end system to generate models for diverse edge environments through post-deployment on-device neural architecture adaptation.\nWe focus on two related challenges in .\nFirst, generating the search space of model architectures is non-trivial since the space must contain enough high-quality candidates that are suitable for different edge conditions.\nSecond, directly searching the optimal model architecture at the edge may be time-consuming due to the limited on-device resources.\n\naddresses the above challenges by training a supernet once and letting the edge devices choose the satisfactory subnet on their own. The method can be divided into two stages, the on-cloud elastification and on-device search. \n\nIn the first stage, we design an on-cloud model elastification method to generate a high-quality search space for edge devices. Specifically, the elastification takes an arbitrary pretrained model as the input and converts it to a multi-path supernet by adding branches into the pretrained model, ensuring each path in the supernet is a valid and useful model. \nWe introduce block-wise knowledge distillation to train the newly added branches, which consequently improves the quality of the subnets.\nOur supernet offers millions of model variations with different structures, and the edge side only needs to find the best structure iteratively without additional training.\n\nIn the second stage, to improve the efficiency of architecture search and update at the edge, we systematically optimize the search process according to edge characteristics.\n\nWe first build a performance model on the device by profiling each block in the supernet, which guides the candidate selection during the search, therefore reducing the number of iterations needed to find the optimal model architecture.\nThen we introduce a reuse-based model evaluation method, which caches intermediate features across model candidates to reduce the time required to evaluate the models in each iteration.\n\n\n\n\n\nTo evaluate our approach, we conduct experiments with various popular tasks (image classification, object detection, and semantic segmentation) and models (ResNet <cit.>, MobileNetV2 <cit.>, EfficientNetV2 <cit.>, etc.) on three edge devices including Jetson Nano, Android smartphone (Xiaomi 12), and edge server (NVIDIA 2080 Ti GPU).\nWe compare with several strong baselines including LegoDNN <cit.>, FlexDNN <cit.>, SkipNet <cit.>, and Slimmable Neural Networks <cit.>.\nThe results have shown that our method can achieve significantly better accuracy-latency tradeoffs than state-of-the-art baselines. For example, our method can generate models that have 46.74% higher accuracy than those produced by other methods with a latency budget of 60%.\nMeanwhile, the overhead of our method is minimal which only takes 13 GPU hours for elastification in the cloud and 2 minutes for adaptation on the edge server.\n\n\nOur work makes the following technical contributions:\n\n\n    \n  * We propose and develop the concept of on-device post-deployment neural architecture adaptation, and implement it with an end-to-end system.\n    \n  * We introduce a pretraining-assisted model elastification method that can effectively and flexibly generate a model search space, as well as edge-tailored strategies to search the optimal model from the space and maintain it at runtime.\n    \n    \n  * Our method achieves significantly better accuracy-latency tradeoffs than SOTA baselines according to experiments on various edge devices and common tasks. The tool and models will be open-sourced for edge AI developers to use.\n\n\n\n\n\n\u00a7 BACKGROUND AND MOTIVATION\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Current Practice and Related Work for Edge Model Generation\n\n\n\n\n\nDeploying deep neural networks (DNNs) at the edge is increasingly popular due to latency requirements and privacy concerns.\nSince DNN models are mostly computationally heavy, deploying them to the edge usually has to consider two characteristics of edge devices. First, edge devices are mostly resource-constrained. As a result, there are already a lot of efforts on improving the performance of DNN models on edge devices, including optimizing the DNN inference framework on heterogeneous edge devices <cit.>, designing lightweight model backbones <cit.> and compressing the models to be deployed <cit.>.\n\n\n\nBesides the resource limitation, another major challenge of edge environments is the diversity - model developers usually need to deploy a certain model to thousands even millions of devices that are different from each other.\nThe deployed models are usually expected to meet a certain budget of latency while achieving higher accuracy, or achieve certain expected accuracy while minimizing the latency.\nThus, customizing the model for different target devices becomes a necessity.\n\n\n\n\n\nThe current practices to handle edge environment diversity are mostly cloud-based pre-deployment approaches, the central server generates models for different edge devices before distributing them for deployment.\nSince manually designing models for diverse edge environments is cumbersome, the common practice is to use automated model generation techniques.\nNAS\u00a0<cit.> is the most representative and widely-used model generation method, which searches for the optimal network architecture in a well-designed search space.\nMost NAS methods require training the architectures during searching <cit.>, which is very time-consuming (10,000+ GPU hours) when generating models for a large number of devices.\n\n\nOne-shot NAS <cit.> is proposed to greatly reduce the training cost by allowing the candidate networks to share a common over-parameterized supernet.\nAmong them, several approaches also mention the concept of directly searching the architecture for target data and devices <cit.>.\nHowever, they require to collect much information from the edge devices to build accuracy and latency predictors, which are used to guide the search process in the cloud.\n\nThere are also several approaches proposed to scale models at the edge to provide a wider range of resource-accuracy trade-offs. \nMost of them apply structured pruning (or similar techniques) to generate various descendent models <cit.>, which can adjust the size of each network module without changing the architecture.\nHowever, they have limited abilities to generate optimal models for diverse edge environments due to the restricted model space. Dynamic Neural Networks <cit.> are a type of DNN that support flexible inference based on the difficulty of input. When the input is easy, Dynamic Neural Networks can reduce the computation by skipping a set of blocks <cit.> or exiting from the middle layers <cit.>. However, this kind of work only considers dynamically adjusting the depth of DNN models, and they are not completely suitable for situations where latency budgets are strict. \n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Limitations of Current Practice\n\n\nWe conduct several motivational studies to understand the limitations of the conventional model generation method.\n\nWe argue that the cloud-based pre-deployment model generation approaches underestimate the diversity of edge environments. We identify three types of diversity:\n\n\n\n  * Inter-device diversity.\nEdge devices are equipped with various types and grades of processors for DL inference, such as CPU, GPU, and AI accelerators. Even for devices with the same type of hardware, their conditions can be different. We measure the inference latency of two popular DNN models on four different mobile devices. As shown in Table\u00a0<ref>, the inference latency of a model varies a lot on different devices.\n\n\n   \n\n     \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  * Intra-device diversity.\n\nEven on the same device, the inference latency of a model may also be affected by various factors, including background processes, software versions, hardware aging, ambient temperature, etc. Table\u00a0<ref> shows the non-neglectable impact of varying conditions on inference latency.\n\n\n\n  * Data distribution diversity.\nNAS approaches need to search for the optimal architecture over a given dataset. However, edge devices are usually used in different locations and by different users, dealing with different data distributions <cit.>. For example, some smart cameras are deployed in outdoor scenarios while some are indoor, and the common classes of objects in the scene may be different across devices.\n\n\nSuch complex and ubiquitous diversity poses several difficulties for cloud-based model generation.\nFirst, tailoring models for diverse edge environments is a heavy task.\nTo generate optimal model architecture for each edge environment, the current practice requires repeating the search process for all types of environments and maintaining them in the cloud.\n\nThe required manual and computational effort are determined by the granularity of edge environment diversity to consider, which might be burdensome if the developers want to achieve optimal latency-accuracy tradeoffs on more devices.\nMeanwhile, handling the dynamicity of the edge environment is even more difficult since it requires frequent communications with each edge device and rapid reactive model updating in the cloud.\n\n\n\n\n\n\n\n\nSecond, modeling the edge environment may also be difficult. A necessary step in the cloud-based model generation is to estimate the performance of the candidate model, such that the model architecture can be optimized according to the target hardware and data. For example, existing NAS methods are usually based on accuracy and latency predictors <cit.>. Building the predictors requires collecting intensive edge information, which is not easy, especially for the accuracy predictor that depends on the potentially private edge data.\nThe compromise solution is to use a unified accuracy predictor for different edge devices <cit.>.\n\nHowever, the unified accuracy predictor may not perform well for edge devices with data distribution shifts. As shown in Figure <ref>, the accuracy values and rankings of candidate models predicted by the once-for-all accuracy predictor <cit.> are both different from the ground truth, which indicates that the predictors can be unreliable on edge distributions, leading to sub-optimal model generation.\n\n\n\n\n\n\n \u00a7.\u00a7 Post-deployment Neural Architecture Generation: Goal and Challenges\n\n\n\n\nThe limitations of existing edge model generation methods motivate us to think, can we directly search for the optimal neural architecture on the edge device after deployment?\n\nDoing so brings several key advantages. Unlike traditional on-cloud NAS which has to estimate the performance of subnets, edge devices can directly evaluate the performance of a given model architecture natively, which is more precise. Besides, searching on the device is a plug-and-play process and does not need to collect edge information to the cloud, bringing the benefits of protecting user privacy and reducing the computation overhead of the cloud.\n\n\nOn the other hand, finding the optimal model architecture directly at the edge is challenging.\nFirst, generating the model search space for edge devices is difficult. The search space should be flexibly and easily customizable to support diverse edge applications and different ranges of target devices. Meanwhile, since the training abilities of edge devices are usually weak, the search space should contain high-quality candidate models that can be used in different edge environments with minimal (or even no) further tuning.\n\nSecond, the model search process can be time-consuming at the edge. Existing architecture search methods require either training a lot of candidate models or repeatedly evaluating the performance of the candidates. Both are very heavy for the edge devices because of the limited computing resources and tight deadline of model initialization.\n\nDynamically updating the model according to environment changes is even more time-sensitive.\n\n\n\n\n\n\n\n\u00a7 OVERVIEW\n\n\n\n\n\n\nTo solve the aforementioned challenges and realize the vision of post-deployment model generation, we introduce , an on-device neural architecture adaptation approach for diverse edge environments. To the best of our knowledge, is the first end-to-end system to enable on-device architecture adaptation.\n\n\n\n\n\n\n\nThe main idea of is to generate high-quality model search spaces based on developer-specified pretrained networks through modular expansion and distillation, and efficiently search for the optimal architecture on the target device guided by performance modeling.\nFigure <ref> illustrates the architecture of , which includes an on-cloud model elastification and an on-device subnet search. \n\nOur model elastification is efficient by leveraging the guidance of a developer-specified pretrained model. It mainly consists of a granularity-aware graph expansion step and a distillation-based training step.\nGiven an arbitrary pretrained network, we first discover the repeating basic blocks and determine the replaceable paths in the computational graph. Then we add optional branches to the model to extend it into a supernet. The added branches include layers that can replace multiple original layers, or structured-pruned layers that reduce the computational cost of individual layers. Each path from the input to the output in the graph is a valid subnet, which consists of both original and newly added modules.\n\n\nThe supernet obtained by graph expansion contains a large variety of architectures with different levels of computational complexity. We then further improve the quality of each candidate architecture in the supernet through training, so that on-device training can be avoided to save computation cost.\nSince our supernet is generated from a pretrained model, we use branch-wise distillation to efficiently train the newly-added branches to mimic the original branches.\nThe distillation is followed by a whole-graph fine-tuning to further improve the overall accuracy of the subnets.\nWith all these techniques, the supernet would contain high-quality subnets that can fit in different edge environments, and it is deployed to the edge devices for further adaptation.\n\n\n\n\nThe on-device subnet search stage aims to find the most appropriate subnet (that can achieve the highest accuracy within the latency budget) on resource-limited edge devices.\nWe first build a latency model by profiling the blocks in the supernet to precisely estimate the latency of subnets in the native environment.\nBased on the latency model, we design a search strategy that initializes a set of promising candidate models and iteratively mutates the candidates around the latency budget.\nThe search efficiency is further improved by reusing the common intermediate features during candidate model evaluation.\nThe optimal model is also adaptively updated by the runtime monitor to handle environment dynamicity.\n\n\n\n\n\n\u00a7 ELASTIFICATION ON CLOUD\n\n\n\n\n\n\n\nThe input of the model elastification stage is a developer-specified pretrained model, similar to the common scenarios in edge AI deployment. The pretrained model is determined as the best-performing model that can fulfill (or slightly exceed) the latency budget on the highest-end target device.\n\nThe goal of elastification is to convert the given pretrained model into a supernet, by expanding alternative basic blocks, making connections between them, and letting each path in the supernet (namely subnet) behave correctly.\n\nIn this way, the supernet is granted with the elasticity: each subnet has the discriminative performance characteristics in terms of inference latency and accuracy. The supernet is then deployed onto the edge, where the particular edge device can search for the most suitable subnet according to its own hardware capacities and data distribution.\n\nThere are two main problems to solve in elastification. The first is how to generate the supernet architecture. Although prior work <cit.> has discussed hand-crafted supernets for certain models, it is still an open question to automatically generate supernets based on an arbitrary pretrained model, especially considering the diversity of DNNs.\n\nAnother problem is how to train the subnets in the supernet to improve their quality. A supernet typically contains millions of subnets, thus training them separately is time-consuming.\n\nWe propose two techniques to address these problems accordingly.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Granularity-aware Graph Expansion\n\n\n\n\n\n\n\nLet \ud835\udca9 denote the pretrained model we want to elasticize. The first step is to analyze the computational graph of \ud835\udca9 to determine how it can be expanded.\nWe call the smallest unit in the graph that be replaced as a basic block, and the block partitioning is determined by the following principles.\nFirst, the size of blocks determines the subnet search space size and the granularity of how the latency can be controlled. Thus, we limit the block parameter size to no more than \u03b3\u00b7 P_0 where \u03b3 is a parameter to control the granularity and P_0 is the parameter size of the original model N.\nSecond, the blocks should not span fusion layers. For example, Conv and ReLU can be fused in most inference frameworks <cit.>. \n\nThird, each basic block should be single-input and single-output in the original model graph.\nFollowing these principles, we can represent the supernet as a set of connected basic blocks \ud835\udca9 = graph {\u212c^(0), C}, where \u212c^(0) is the set of blocks and C denotes the connections between them.\n\n\n\n\n\n\nNext, we generate the supernet graph \ud835\udcae by expanding the graph of the pretrained model \ud835\udca9 through adding alternative blocks and connections. Particularly, we consider two expanding strategies including merging and shrinking, as shown in Figure <ref>.\nFirst, we add merged blocks B_i^(j) that can replace multiple basic blocks (j>0 represents the number of reduced blocks in the replacement).\n\nSuppose { B_i^(0), B_i+1^(0), ..., B_i+j^(0)} are the basic blocks in \ud835\udca9 that can be replaced by B_i^(j), The input shape of B_i^(j) is the same as B_i^(0), and output shape is the same as B_i+j^(0). The parameter size of B_i^(j) is determined by the largest among the replaced blocks.\nSecond, like traditional model scaling approaches <cit.>, we also add different levels of shrunk blocks \u212c_i^(-1), \u212c_i^(-2), ... for each basic block \u212c_i^(0)\u2208\u212c^(0) by reducing its size with structured pruning and network slimming techniques <cit.>.\nThe granularity of merged blocks and pruned blocks can be balanced to control the size of subnet search space.\n\n\n\n\n\n\n\nCompared to the existing model scaling methods <cit.>, our supernet has higher elasticity because it allows the subnets to have different architectures, rather than just different sizes of the same architecture. This is also the reason why NAS outperforms other model generation techniques on the server side <cit.>.\n\n\n\n\n\n\n \u00a7.\u00a7 Distillation-based Supernet Training\n\n\n\n\nNext, we need to train the generated supernet to improve the quality of its subnets, so that the subnet can be directly used at the edge without further training. We achieve efficient and effective training by fully utilizing the supernet. The whole training process includes a branch distillation phase and a whole-model tuning phase.\n\n\nBranch-wise distillation. In this phase, we first freeze the weights of B_i^0 so that the accuracy of the original pretrained model is preserved. Then we adopt feature-based knowledge distillation\u00a0<cit.> to let the added blocks imitate their corresponding original blocks. As illustrated in Figure <ref>, in each iteration, we randomly sample a subnet from the supernet and use \ud835\udca9 as the teacher model to train the new branches in the subnet. Specifically, let S_i denote the output feature map of a newly added block B_i^(j) and T_i denotes the output feature map of the last block that B_i^(j) replaces, we use the L2 distance as the distillation loss. The loss function is\n\n    \u2112_distillation=1/M\u2211_i=1^M T_i-S_i  _2 ^2,\n\nwhere M denote the number of new blocks in the sampled subnet. \nWith enough iterations applied, all new blocks in the supernet will be trained multiple times to improve their individual quality.\nSince we only train the new blocks and use the feature maps of the pretrained model as strong supervision, the distillation process is efficient and easy to converge.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFurther tuning.\nWe further train the supernet using labelled data to improve the end-to-end quality of the subnets. In each step of tuning, we randomly sample a subnet B_i^(j), forward a batch of samples, compute the Cross-Entropy\u00a0<cit.> loss between the subnet outputs and the labels, and update the parameters of the added blocks in B_i^(j) via gradient descent.\nThe performance of the supernet is measured by sampling a new set of random subnets and testing each of them on validation data. We use the latency-range accuracy as the training progress indicator, which records the average accuracy achieved by subnets in each latency range.\nThis phase starts from the distilled supernet, and thus the learning rates are relatively small and the convergence is fast.\n\nNotes on design rationale. Each phase in our design is indispensable to ensure training efficiency and effectiveness. Using distillation only will lead to suboptimal final accuracy, and direct training will significantly slow down the convergence. Merging the two phases together is also not desirable since it will make the loss design and training more difficult.\nThe experimental comparison can be found in Section\u00a0<ref>.\nWe also note that our method does not modify the parameters of the pretrained model, so it guarantees that the latency-accuracy tradeoffs will be better than or at least equal to the pretrained model.\n\n\n\n\n\n\n\n\n\u00a7 ADAPTATION ON EDGE\n\n\n\nThe supernet generated by model elastification is uniformly deployed to different edge devices, but it is not directly usable since each edge device has different characteristics and requirements.\nThus, we further introduce the on-device adaptation stage to obtain the optimal architecture adaptively in the target environment by searching the subnet space.\nSuch a search process is similar to traditional on-cloud NAS but has a higher requirement for efficiency.\n\n\n\n\n\n\nAccording to our analysis, using a normal search method as in NAS can cost more than 10 hours on edge devices.\nMost of the searching time is spent on evaluating the subnets.\n\nThis is because we have to perform model inference hundreds of times to get the accuracy of candidate models in each search iteration and use the accuracy results to guide the next search iteration.\n\n\nTo reduce the searching overhead, we introduce a latency model-guided search strategy and a reuse-based model evaluation method.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Model-guided Search Strategy\n\n\n\n\n\nWe first optimize the search strategy to find the optimal model architecture (the architecture that can achieve the highest accuracy within the latency budget) with fewer iterations. The core idea of the optimization is to prune the search with the guidance of a natively-built latency model.\n\nFormally, suppose T_budget denotes the latency budget in the target environment and D_edge is the edge dataset.\nOur goal is to find a subnet \ud835\udca9' = { B_i_1^(j_1), B_i_2^(j_2), ..., B_i_n^(j_n)} from the supernet \ud835\udcae whose latency T(\ud835\udca9') \u2264 T_budget and accuracy Acc(\ud835\udca9', D_edge) is optimal.\nDuring the search, the accuracy of the candidate model is directly measured on the edge dataset D_edge, and the latency is computed with a latency model.\n\nThe latency model is a table \ud835\udcaf={T_i^(j)} where T_i^(j) is the latency of basic block B_i^(j) in the supernet.\nThe block latency is precisely measured on the device through profiling after deployment. Note that this process is quick (within seconds) because the number of basic blocks is small (much smaller than the number of subnets).\nOur supernet generation strategy (Section\u00a0<ref>) ensures that the basic blocks will not be fused, thus the latency of a chosen subnet is the sum of all its blocks, T(\ud835\udca9') =\u2211_k=0^n T_i_k^(j_k).\nNote that we use the latency model to compute the latency rather than directly measure it because end-to-end latency measurement under the actual model operating condition is time-consuming.\n\n\n\n\n\n\n\n\n\n\n\n\nThe subnet search process contains two main steps, including candidate initialization and candidate mutation, where the initialization step produces a set of seed subnets and each mutation step changes the subnets iteratively to better fit the target environment.\nBoth the initialization and mutation are customized with the latency model in our approach.\n\nOur experiments show that the optimal subnets are often near the latency budget.\nTherefore, the initialization and mutation are designed to keep the search of candidates near the budget.\n\n\n\n\n\nSpecifically, we design two supporting functions NearbyInit and NearbyMutate. NearbyInit generates the initial candidate subnets by randomly sampling a group of subnets whose latencies lie in the range of [T_budget - \u0394 T, T_budget + \u0394 T]. The models with latency higher than T_budget are unlikely to be useful at the current moment, but they may be used later to handle dynamic environment change (see Section\u00a0<ref>).\nNext, when we change the candidate subnets in each iteration, we first randomly mutate a subnet by replacing a branch in it. If the latency of the subnet after mutation is out of [T_budget - \u0394 T, T_budget + \u0394 T], we iterate the rest branches in the supernet and find the best alternative branches that can reduce the latency change.\n\nBoth the initialization and mutation strategies are graph operations that do not involve heavy computation, but they can significantly reduce the evaluation overhead by improving search efficiency.\nMeanwhile, the model-guided initialization and mutation can be integrated into most standard search algorithms including evolutionary search <cit.> and simulated annealing <cit.>.\nAlgorithm\u00a0<ref> shows the subnet search strategy based on evolutionary search.\n\n\n\n\n\n\n\n\n\n\n\t\t\n\t\t\n\n\n\t\t\n\n\n\n\n\n\n\n\n\t\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n \u00a7.\u00a7 Reuse-based Model Evaluation\n\n\n\n\n\n\n\n\nWhile the model-guided search strategy reduces the required iterations, the model evaluation overhead in each iteration is still high.\n\nIn each iteration, we usually need to evaluate hundreds of candidate subnets with the edge data to find the most accurate ones. The candidate subnets usually share common prefix substructures, so we have the opportunity to save time by reusing common intermediate features across subnets.\n\nFor example, let \ud835\udca9'_1 = G_prefix\u222a G_1 and \ud835\udca9'_2 = G_prefix\u222a G_2 denote two different subnets and \ud835\udca9'_1 is evaluated before \ud835\udca9'_2,\n\n\nduring the evaluation of \ud835\udca9'_1, the output feature of G_prefix(D_edge) can be saved and reused when evaluating S_2, which saves the inference cost of computing G_prefix(D).\n\n\nHowever, saving all the common features during model evaluation is infeasible because it will take too much memory. Thus, we can only save part of the common features and improve the reuse ratio as much as possible. In order to achieve this, we introduce a tree-based feature cache to schedule the evaluation. The leaf nodes and non-leaf nodes in the tree represent subnets and common prefix substructures respectively. The leaf nodes (subnets) sharing the same parent node have the same prefix substructure represented by the parent node. And two non-leaf nodes with the same parent node have the same smaller prefix substructure. \n\n\nAfter building the feature cache tree, we evaluate all the subnets in the depth-first order. When we traverse to node N, we cache the output feature of that node in memory. Then, when evaluating the descendant leaf nodes (subnets) of N, we can reuse the cached feature. After evaluating all descendant leaf nodes (subnets) of N, we can release the feature from memory since it won't be reused by later subnets. As a result, the number of saved features is no more than the depth of the tree, and we can adjust the depth of the tree to control the size of the feature cache.\n\nAnother problem of model evaluation is that testing the models one by one may lead to too frequent data I/O operations. Thus, we adopt batch-wise model group evaluation, loading a batch of data and evaluating all candidate subnets using the batch. The performance of the subnets is the average of them on all batches. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Dynamic Model Update\n\n\n\n\n\nThe optimal subnet found by search is used in the target environment for serving. However, the subnet may become suboptimal at runtime upon environment change.\n\n\ndeals with environment change by dynamically paging in and paging out alternative blocks. In order to provide subnets of different latency-accuracy trade-offs, we maintain a subnet pool during searching (Section\u00a0<ref>) and save the [arch, latency, accuracy] tuples of all the searched subnets, where arch denotes the encoded architecture of the subnet. After searching, we save the subnet architectures that achieved the highest accuracy at different levels of latency (within the latency range [T_budget - \u0394 T, T_budget + \u0394 T]). For each of these subnet architectures, we save the relative latency as compared with the current optimal subnet.\n\n\nAt runtime, a latency monitor runs to detect the latency change of the running model. When the inference latency exceeds the budget, the latency monitor reports the latency scaling ratio r = actual latency/estimated latency, and searches in the subnet pool to find the subnet that achieves the highest accuracy within the scaled latency budget T_budget/r.\nSimilarly, when the actual latency is smaller than the largest relative latency in the subnet pool, the monitor also replaces the running model with a better one. \n\nIf the environment change is too significant and no subnet in the pool can fulfill the latency budget, we restart the search process to obtain the new optimal subnet and subnet pool.\n\n\n\n\n\n\n\n\n\n\u00a7 IMPLEMENTATION\n\n\n\n\n\n\n\nWe implement our method using Python and Java. The on-cloud elastification part and on-device searching part are developed with PyTorch and PyTorch Mobile <cit.>.\n\n\n\nHanding two-stage models.\n\nSome deep learning applications such as object detection and semantic segmentation often require two-stage training, pretraining the backbone on ImageNet <cit.> and fine-tuning on the smaller task dataset. When a DNN model needs to be trained on two datasets, uses a two-stage elastification strategy. Let \ud835\udca9 denote the DNN model well-trained on two datasets {D_1, D_2} in order. We first elasticize the backbone of \ud835\udca9 and train the newly added branches on D_1 based on feature-based distillation (Section 4.2.1) method. After distillation, we connect the elasticized backbone to the head of \ud835\udca9 to make it a supernet, and further train it on D_2 (Section 4.2.2).   \n\n\n\n\n\n\nDevices with limited memory. The supernet generated by our method is about 2\u00d7-5\u00d7 larger than the pretrained model, which may not fit in the memory of some low-end devices. We use block-wise loading and inference to reduce the memory overhead. Specifically, only the blocks required by the current subnet are loaded into the memory during searching, and others are retained in the disk. Therefore, requires no more memory in the on-edge stage than that required by the optimal subnet.\n\n\n\n\n\n\n\n\n\n\n\u00a7 EVALUATION\n\n\n\n\n\nWe conduct experiments to answer the following research questions:\n(1) Is able to generate models with better latency-accuracy tradeoffs? (<ref>, <ref>)\n\n(2) Can utilize the edge data distribution? (<ref>)\n(3) What's the efficiency of in both on-cloud and on-edge stages? (<ref>, <ref>)\n\n\n\n \u00a7.\u00a7 Experimental Setup\n\n\n\nEdge environments. \n\nWe use three edge devices including an Android Smartphone (Xiaomi 12) with Snapdragon 8 Gen 1 CPU and 8 GB memory, a Jetson Nano with 4 GB memory, and an edge server with NVIDIA 3090 Ti with 24 GB GPU memory.\nThe batch sizes are set to 1, 1, and 32 on the three devices to simulate real workloads.\nWe use different latency budgets to simulate intra-device hardware diversity. The data distribution diversity is not considered in most experiments to fairly compare with the baselines. In Section\u00a0<ref>, we use Dirichlet distribution to generate edge data, the same setting used in most Federated Learning research <cit.>.\n\n\nBaselines.\nLegoDNN <cit.> and NestDNN <cit.> are the most relevant baselines of our work. LegoDNN <cit.> is a pruning-based, block-grained technique for model scaling. NestDNN <cit.> generates multi-capacity DNN models using filter pruning and recovering methods.  Since the source code of NestDNN is unavailable and it underperforms LegoDNN <cit.>, we conduct a detailed comparison with LegoDNN.\nWe also include three methods that can be used for on-device model generation, including Slimmable Networks <cit.>, FlexDNN <cit.> and SkipNet <cit.>. \nSlimmable Networks <cit.> design models whose widths can be flexibly changed without retraining, FlexDNN <cit.> is an input-adaptive method that supports early exits, and SkipNet <cit.> is a representative dynamic neural network that can dynamically switch different routes for different inputs. We adapt SkipNet <cit.> by letting it search for an optimal fixed route on the target device as the generated model. And we adapt FlexDNN <cit.> by specifying an early exit layer for each latency budget.\nWe also compare with the EfficientNetV2 series <cit.>, which are examples of state-of-the-art models generated by on-cloud NAS.\n\n\nTasks, Models, and Datasets.\nWe evaluate the performance of on three common vision tasks.\n\n\n\n  * Image classification aims to recognize the category of an image. We select three popular classification models, MobileNetV2 <cit.>, ResNet50 <cit.>, and ResNet101 <cit.> to represent small, middle, and large models. The dataset used in this task is ImageNet2012 <cit.>.\n\n\n  * Object detection aims to detect objects in an image, predicting the object bounding boxes and categories. We choose EfficientDet <cit.> with ResNet50 <cit.> backbone as the detection model, which is one of the top-performing detection models, and COCO2017 <cit.> as the dataset.\n\nThe performance of detection models is measured by mean average precision over Intersection over Union threshold 0.5 (mAP@0.5).\n\n  * Semantic segmentation aims to predict the class label of each pixel in an image. \n\nWe choose FPN <cit.> model with ResNet50 <cit.> encoder pretrained on ImageNet2012 <cit.>. The dataset is CamVid <cit.>, a road scene understanding dataset. The performance is measured by Mean Intersection over Union (mIoU).\n\n\n\n\n\n\n\n \u00a7.\u00a7 General Model Scaling Performance\n\n\n\n\n\nWe first evaluate the quality of models generated by our method and the baselines.\nSpecifically, we elasticize MobileNetV2, ResNet50, and ResNet101 which represent small, medium, and large models respectively.\nFor small models, we elasticize them into supernets containing five types of replaceable blocks {B_i^(1), B_i^(2), B_i^(0), B_i^(-1), B_i^(-2)}. The pruning rate of B_i^(-1), B_i^(-2) are 0.5 and 0.25 respectively.\nFor medium and large models such as ResNet50 and ResNet101, we elasticize them into supernets that only contain original and merging optional blocks {B_i^(1), B_i^(2), B_i^(0)}. After elasticizing, the supernets contain 2.58\u00d710^8, 1.06\u00d710^5, 2.57\u00d710^17 subnets, respectively.\nTo make a fair comparison, we divide the validation set into two subsets, one smaller subset (3000 images) to search for the optimal subnet under 10 latency budgets, and the rest to evaluate the optimal subnet.\n\n\n\nThe result is displayed in Figure <ref>, AdaptiveNet achieves higher accuracy than baseline approaches at almost every latency budget, and increases accuracy by 10.44% and 28.03% on average compared to LegoDNN with 90% and 70% latency budget respectively. This is because our elastification creates better search space of subnets and the two-stage training technique allows subnets to learn from both the original pretrained model and the labels. Thus, AdaptiveNet can outperform LegoDNN which only trains the descendent blocks to mimic the original blocks.\n\nBesides, we observe that AdaptiveNet outperforms the baseline models more at a lower latency budget. At the 60% and 80% latency budget, AdaptiveNet achieves 42.53% and 29.16% higher accuracy on average respectively. This is because our approach includes merging two or more blocks into one replacement block compared to pruning-based model scaling techniques. Such block merging can save more latency with a smaller loss of accuracy than high-ratio pruning. \n\n\n\nWe also notice that the gap between and Slimmable Networks is small on smartphones and 3090 Ti. The main reason is that slimmed networks can better utilize the computational resources on such devices. However, because the slimmable models are based on custom backbones, they cannot support SOTA pretrained models and are not flexible for normal developers to use.\n\nFurther, can be used with multiple pretrained models to achieve more wide-range and fine-grained trade-offs. \nFigure\u00a0<ref>(a) shows the performance of models generated from two oracle EfficientNetV2 models, where provides over 20 meaningful latency-accuracy trade-offs between the oracle models.\nThus, developers can use as an effective supplement to manually-created or cloud-generated models to offer more choices for the edge with little overhead (dozens of hours).\n\n\n\n \u00a7.\u00a7 Performance on Other Tasks\n\n\n\nWe also test AdaptiveNet on object detection and semantic segmentation to evaluate its generalizability and performance on complex two-stage tasks (pretrained on ImageNet2012 <cit.>, fine-tuned on COCO2017 <cit.> and CamVid <cit.>). Our object detection model, EfficientDet <cit.>, consists of a backbone, neck, and head, among which the backbone takes up most of the inference latency (more than 90% according to our measurements), thus we only elasticize the backbones. For the same reason, we only elasticize the encoder of FPN <cit.>.\nSince the official code of our baseline LegoDNN <cit.> on object detection and semantic segmentation cannot run properly, we implement the training process of LegoDNN <cit.> on both tasks.\nTo make fair comparisons, and LegoDNN <cit.> start from the same pretrained model and train for the same GPU hours. After training, we randomly sample the same number (500) of subnets for both tasks and evaluate them on the test set.\n\nThe results are shown in Figure <ref>. Similar to the classification tasks, achieves reasonable scaling performance and outperforms the baseline. Some of the FPN subnets can even achieve better tradeoffs than the original pretrained model, which is because the original model is over-fitted. Our subnets generated by merging some original blocks together can reduce the parameter size of the original model, which reduces over-fitting and improves accuracy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Impact of Edge Data Distribution Shift\n\n\n\n\n\n\n\nSince generates the model directly on the target device, it can utilize edge data as compared with on-cloud NAS. \nWe examine the quality of models generated by on different edge datasets simulated with Dirichlet distributions.\nThe results are shown in Figure <ref>.\n\n\nWe notice that can outperform the EfficientNetV2 models that are generated by extensive on-cloud NAS <cit.> on unbalanced edge data distributions. \nSome of the subnets may improve the top-1 accuracy by 1.07% while saving 7.71% latency at the same time, which is a hard-won improvement since the original model has achieved excellent performance (with 86.99% top-1 accuracy and 5.03ms latency). We also observe that the advantage of increases when the data distribution is more unbalanced. \n\nGiven the fact that the edge data distributions are usually different from training ones <cit.>, we believe the post-deployment model generation mechanism of is a more promising direction to seek in edge AI scenarios.\n\n\n\n\n \u00a7.\u00a7 On-cloud Training Performance\n\n\n\n\n\n\nWe examine the efficiency and effectiveness of our on-cloud elastification stage.\nWe compare our supernet training method with a supervised training baseline (our method without distillation) and a distillation-only baseline (our model without whole-model tuning) and plot the progressive top-1 accuracy on ImageNet in Figure <ref>.\nAlthough all of the training methods can converge after 50 epochs, our two-step training technique is 7.15% and 7.84% higher than using supervised training only and distillation only.\n\n\nOur supernet training also converges faster than the baselines with only 60 epochs (about 13 hours), which is also significantly faster than on-cloud NAS (>1200 GPU hours <cit.>).\n\n\n\n\n \u00a7.\u00a7 On-device Adaptation Efficiency\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we evaluate the performance of the on-device search in .\n\nMost of the subnet search methods in conventional NAS are too heavy for edge devices (reinforcement learning <cit.> and gradient-based methods <cit.>).\nSo we choose normal evolutionary search <cit.> and simulated annealing <cit.> as our baselines.\nTo examine the effectiveness of our method, we conduct two ablation experiments and one end-to-end experiment. All the experiments use the same 500 images for searching.\n\nFigure <ref> shows the acceleration percentage of our reuse-based model evaluation method (Section\u00a0<ref>). We compare the time spent to evaluate 50, 100, and 200 subnets respectively. \n\nOur method saves up to 56.7% and 36.2% of search overhead compared to normal evaluation pipeline, and consumes 100-500MB of memory. It is notable that the memory cost of our method is controllable by adjusting the depth of the subnet tree. If there is no space for feature maps, we can set the depth to 0, which will reduce the GPU memory overhead to zero with some sacrifice of search efficiency.\n\nFigure <ref> shows the benefits of our model-guided search strategy. To achieve the same average accuracy, , evolutionary algorithm, and simulated annealing need to try 800, 3100, and 1800 subnets respectively.\n\nFigure <ref> shows the end-to-end search efficiency comparison between and baselines on NVIDIA 2080 Ti. We conduct three individual experiments with a population size of 50, 100, and 200, respectively, and show the best results for each strategy. , simulated annealing algorithm, and evolutionary algorithm to find optimal subnet in 117.6, 765.6, and 1656.9 seconds respectively, indicating that our method can improve search efficiency by more than 80%.\n\n   \n\nNetwork transmission overhead. Table <ref> shows the size of and pretrained models. Although increases the size of models, we believe it can actually save network overhead. only needs to transmit the supernet to edge devices once, which is 1.32\u00d7-2.72\u00d7 larger than the original pre-trained model. However, to achieve similar performance, conventional model deployment approaches have to collect device information and re-transmit the model when the edge environment changes, which is n\u00d7 larger than the original model, where n is the time of changes.\n\n\n\nReal-time Model Update Efficiency.\nWe further test the performance of our dynamic model update module and present the result in Figure <ref>. We choose ResNet50 <cit.> as our pretrained model and the experiment is conducted on NVIDIA 2080 Ti. We adjust the GPU usage by running and killing MobileNetV2 inference processes.\nOur dynamic model update is fast and responsive. After the latency budget is exceeded or under-utilized, can find the optimal model and recover the latency within 1 second. Specifically, we obtain a pool of optimal subnets for a range of latency budgets during the on-device search. The runtime update module only needs to switch to the proper model, instead of searching from scratch. Thus, it should be easy for our method to catch up with the workload dynamics. Besides, if the real workload is very dynamic, we can control the update frequency to avoid overreaction.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 DISCUSSION\n\n\nAn issue that may be a concern in the on-device model generation is the need for labeled edge data, which might be difficult if the data is not auto-labeled (like in many unsupervised tasks <cit.>).\nSuch a dataset can be generated by querying an oracle model with unlabelled edge data. Letting the cloud send public data to the edge is also an option, although the edge data characteristics will not be utilized in this way.\n\n\n\nAlthough AdaptiveNet is mainly evaluated on vision tasks, it should be able to generalize to other tasks such as NLP. Transformer models <cit.> are also composed of repeated blocks such as encoders and decoders, so we should be able to elasticize them into supernets and choose the optimal subnet from them at edge devices.\n\nWe also want to discuss the relationship between AdaptiveNet and on-device training, which can be used to improve model quality after deployment. First, on-device training typically requires heavy computation and sufficient training data to be effective, which does not require. Besides, on devices with good training conditions, a better architecture found by can also be beneficial.\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nThis paper proposes a novel approach for on-device, post-deployment, and environment-aware model architecture generation.\nThe approach is implemented as an end-to-end system equipped with on-cloud model elastification and on-device model adaptation techniques.\nExperiments have demonstrated the remarkable model quality and model generation efficiency of our method.\nDevelopers can scale their AI applications to diverse and dynamic edge environments with our system by simply specifying a pretrained model.\n\n\n\n\u00a7 ACKNOWLEDGEMENT\n\n\n\nThis work is supported by the National Key R&D Program of China (No.2022YFF0604501) and NSFC (No.62272261).\n\n\n\n\nACM-Reference-Format\n\n\n\n"}