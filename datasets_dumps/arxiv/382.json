{"entry_id": "http://arxiv.org/abs/2303.06800v1", "published": "20230313011050", "title": "Object-Centric Multi-Task Learning for Human Instances", "authors": ["Hyeongseok Son", "Sangil Jung", "Solae Lee", "Seongeun Kim", "Seung-In Park", "ByungIn Yoo"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\n\n\n\n\n\nObject-Centric Multi-Task Learning for Human Instances\n    Hyeongseok Son, Sangil Jung, Solae Lee, Seongeun Kim, Seung-In Park, ByungIn Yoo\n\nSamsung Advanced Institute of Technology\n\n{hs1.son, sang-il.jung, solae913.lee, se91.kim, si14.park, byungin.yoo}@samsung.com\n\n\n\n\n\n\n\n\n\n\n\n    March 30, 2023\n==============================================================================================================================================================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Human is one of the most essential classes in visual recognition tasks such as detection, segmentation, and pose estimation. Although much effort has been put into individual tasks, multi-task learning for these three tasks has been rarely studied. In this paper, we explore a compact multi-task network architecture that maximally shares the parameters of the multiple tasks via object-centric learning. To this end, we propose a novel query design to encode the human instance information effectively, called human-centric query (HCQ). HCQ enables for the query to learn explicit and structural information of human as well such as keypoints. Besides, we utilize HCQ in prediction heads of the target tasks directly and also interweave HCQ with the deformable attention in Transformer decoders to exploit a well-learned object-centric representation. Experimental results show that the proposed multi-task network achieves comparable accuracy to state-of-the-art task-specific models in human detection, segmentation, and pose estimation task, while it consumes less computational costs.\n \n\n\n\n\u00a7 INTRODUCTION\n\n\n\n\nCore tasks of visual recognition are detection, segmentation, and pose estimation in scenes for the various vision applications such as video surveillance and human computer interaction.\n\nInstead of training an individual model for each task, multi-task learning strategy using an unified architecture is beneficial in terms of cost-efficiency and inter-task synergy. In this paper, we focus on designing an effective unified architecture for the human-related multi-tasks.\n\n\nRecently, object-centric  learning\u00a0<cit.> was introduced benefiting from Transformer architecture. Object-centric learning enables to encode per-instance representations by mapping queries in Transformer to corresponding object instances in an image. \nDue to the per-instance representation, object-centric learning is well-suited with human instance-level recognition tasks.\nAlthough the object-centric learning has been applied in some studies on multi-task learning for detection and segmentation\u00a0<cit.>, few attempts have been made to apply the concept to human recognition problems. \n\nFurthermore, unlike general object recognition, human recognition needs to estimate poses that imply the structural information of the human instances. Thus, a na\u00efve introduction of object-centric learning would lead to performance degradation because different types of information are implicitly encoded in a mixed way, preventing each query from having tailored information for each task. \n\n\nTo tackle this problem, we propose a novel human-centric query (HCQ) design to extract more representative information of human instances. We segregate the structural information from the decoder embedding and represent it in explicit forms called learnable keypoints (<ref>d). The learnable keypoints contain the bounding box and the body joints of a human while the decoder embedding encodes the general representation of an instance. This separation enables different pieces of information of a human instance to be encoded in a decoupled way, allowing multiple target tasks to effectively utilize useful information from the query for their own purposes. \n\n\n\n\n\n\n\n\nThanks to the decoupled design of HCQ, only the light-weight prediction heads are needed because the information is already disentangled enough for each task. In addition, using both information in the prediction heads further improve the performance because it gives extra information that the other query does not have. For example, human pose can help segmentation\u00a0<cit.> and vice versa. Even though some previous methods (e.g., DAB-DETR\u00a0<cit.>) represent coarse information explicitly such as bounding boxes, but it does not give enough information to help other tasks (e.g., segmentation). Thus, most previous methods feed only decoder embeddings to the prediction heads to perform target tasks even though they have explicit box coordinates. \n\n\nFurthermore, our learnable keypoints can be interweaved with deformable cross-attention seamlessly. The deforamable attention\u00a0<cit.> predicts the sample locations from the decoder embedding directly instead of computing similarity between the query and the image features. We can just apply our learnable keypoints to the deformable cross-attention as the sampling locations. \nIt reduces computation for predicting the sampling locations and also the high-quality keypoints give extra structural information to be attended compared to the direct prediction from the decoder embeddings.\n\n\nWe demonstrate our method on the COCO dataset\u00a0<cit.> with various ablation studies. To our best knowledge, our method is the first Transformer-based unified architecture for human-related multi-tasks, and achieves the comparable accuracy with the state-of-the-art task-specific models while having compact design.\n\n\nTo summarize, our contributions are as follows:\n\n    \n  * We present a unified network architecture performing multiple human instance-level vision tasks with minimal task overheads. \n    \n    \n    \n    \n  * We propose a new object-centric query design to effectively represent the structural information of human instances. With the object-centric query, our model can leverage the disentangled high-level information directly in the prediction head. \n    In addition to that, the query design allows to interweave the structural information with deformation attention seamlessly. \n    \n  * We experimentally show that our method achieves the comparable accuracy with the state-of-the-art task-specific models while having cost-efficient architecture. \n    \n\n\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n\n\n\n\n\n\n\n  \nObject-centric learning\nThe concept of object-centric representation learning is presented in Locatello \u00a0<cit.> where they focus on learning object representations in order to segregate information in an image according to the individual object entities. A set of object representations, called slots, are mapped to object instances in an image using slot attention. \nWith the help of set prediction formulation, DETR\u00a0<cit.> proposed to learn object-centric representations with the Transformer architecture for object detection. They used a fixed-size set of learnable queries, called object queries, to infer the relations of the objects and the image feature (<ref>(a)). \n\n\n\n\n\n  \nObject query design\nMany follow-up approaches\u00a0<cit.> have tackled the object detection task based on DETR.\nAmong them, we focus representative methods in perspective of object query designs.\nIn Deformable DETR\u00a0<cit.>, queries are formulated as 2D reference points simply obtained by learned linear projection (<ref>(b)). By leveraging 2D reference points as anchor points, it allows faster convergence and better performance in object detection. DAB-DETR\u00a0<cit.> replaced the query formulation with dynamic anchor boxes containing both position and size information (<ref>(c)), hence each query conveys stronger positional priors to let decoders focus on a regions of interest. \nDistinct from the previous works, our learnable query explicitly and compactly carries the structural information of an object as the form of keypoints (<ref>(d)). Furthermore, we present effective ways to exploit the high-level information learned in the query for performing deformable attention and task-specific heads.\n\n\n\n\n\n  \nObject-centric multi-task learning\n\nObject-centric representation has been applied to general object multi-task learning of detection and segmentation. \nDETR\u00a0<cit.> is an object detector benefiting from the object queries communicating with image features, can be easily adapted to segmentation task by attaching mask heads. \n\nRecently proposed Mask-DINO\u00a0<cit.> further improved performance of the target tasks simultaneously by utilizing anchor box-guided cross-attention and denoising training scheme. In the segmentation point of view, three segmentation tasks including semantic segmentation, instance segmentation and panoptic segmentation\u00a0<cit.> can be regarded as different tasks, and some work\u00a0<cit.> conducted all these tasks in an unified architecture. Mask2Former\u00a0\u00a0<cit.> attended only the masked regions in cross-attention for fast convergence and K-net\u00a0<cit.> introduced learnable kernels with update strategy where each kernel is in charge of each mask.\n\n\n\n\n\n\n\n  \nMulti-task learning for human instances\nHuman instance segmentation and pose estimation\u00a0<cit.> of multi-person are the key tasks for human-related visual tasks. \nPersonLab\u00a0<cit.> adopted bottom-up approach, which first extracts multiple intermediate feature maps and associates those features to get instance-wise segmentation and pose. Pose2seg\u00a0<cit.> used the human poses instead of the bounding boxes to normalize the interest regions for better alignment and performed the target task for each proposal. PosePlusSeg\u00a0<cit.> used a shared backbone to get intermediate maps for each task and each task pipeline combines those maps to get the results. Unlike all these convolution-based methods mentioned above, we use Transformer decoder architecture with human-centric queries to perform these tasks simultaneously.\n\n\n\n\n\n\n\n\n\u00a7 METHOD\n\n\nWe propose a novel human-centric query design which allows to contain various information of humans so that the human-related tasks such as detection, segmentation and pose estimation are jointly performed. It allows a light-weightened computational model with a multi-task manner, while achieving comparable accuracy in each task. The human-centric query is employed for the prediction of each task and the structural information is transferred into deformable attention module seamlessly. We first overview our unified network architecture for human-centric multi-task learning (<ref>), and then present our human-centric query design (<ref>) and its utilization in detail (<ref> and <ref>).\n\n\n\n \u00a7.\u00a7 Overall architecture\n\n\nThe overall architecture (<ref>) consists of three components: image feature extractor, Transformer decoder and task-specific heads. The image feature extractor takes an image as input and produces multi-resolution features as output. The features are fed into the Transformer decoder to be attended. The Transformer decoder processes the human-centric queries to have various information of human instances, and the queries are fed into each light-weight task-specific head for final prediction.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Human-centric query design\n\n\n\n\nSince a single query vector for training multi-tasks represents tangled information and usually causes performance degradation, we design a novel human-centric query to decouple the structural information from the tangled embedding vector.\nThe proposed human-centric query consists of two distinct parts, decoder embeddings and learnable keypoints (<ref>). \nThe decoder embedding _q^l\u2208^1 \u00d7 D is the representation vector for each human instance where q and l are indices for query and layer, respectively, and D is the hidden dimension for each query.\nThe learnable keypoints _q^l\u2208^1 \u00d7 2(n+1) are the decoupled positional and structural information of a human instance and defined as\n\n    _q^l    =[ x_q,0^l, y_q,0^l, \u22ef, x_q,n^l, y_q,n^l]\n       =[ _q,0^l, \u22ef, _q,n^l] ,\n\nwhere _q,i^l := (x_q,i^l, y_q,i^l) is a 2D coordinate of the ith keypoint.\n\nEmphasizing that the formulation is identical for all queries and layers, we omit the indices q and l without loss of generalities.\n\n\n\nThe learnable keypoints  contains two different types of information; a bbox part (_0, _1) and a pose part (_2, \u2026, _n). Keypoints _0 and _1 in the bbox part give xy-coordinates of left-top and right-bottom of the bounding box, respectively. These two diagonal points are sufficient to represent a given bounding box and we define the center and side lengths of the bounding box using these two points:\n\n    _c = _0 + _1/2  and  \ud835\udc1d = _1 - _0 .\n\nUnlike the bbox part, keypoints in the pose part are regarded as coordinates in a canonical space defined by the bounding box, i.e., the set  of xy-coordinates of joints in the human pose are computed as\n\n    = {_0 + _i T | i=2, \u2026, n} ,\n\nwhere T:=diag(\ud835\udc1d) is a dilation operator. We represent the joint cooridnates of pose in the canonical space which is normalized by the box. It reduces the pose variance according to the size of the human so that it lessens the burden of the network. The effectiveness of the canonical space can be found in the supplementary material.\n\n\nInspired by the DAB-DETR\u00a0<cit.>, the corresponding structural embedding \u2208^1 \u00d7 D is obtained by successive operations over . First, a sine encoding \u03c3\u2192^1 \u00d7 D' maps each element of  to a vector and then, a multi-layer perceptron MLP is applied. In other words,\n\n\n    = MLP( \u03c3() )\n       = MLP(Cat( \u03c3(x_0), \u03c3(y_0), \u2026, \u03c3(x_n), \u03c3(y_n))) ,\n\nwhere Cat is a concatenation operator along with the last dimension. Here, MLP is a three-layer perceptron:\n\n    MLP(\ud835\udc31) = ReLU(ReLU(\ud835\udc31W_1 + \ud835\udc1b_1)W_2 + \ud835\udc1b_2)W_3 + \ud835\udc1b_3 ,\n\nwhere W_1\u2208\u211d^2(n+1)D'\u00d7D, W_2, W_3\u2208\u211d^D\u00d7D, and \ud835\udc1b_1, \ud835\udc1b_2, \ud835\udc1b_3\u2208\u211d^1 \u00d7 D are learnable weights and biases. \n\n\nOne notable point is that our learnable keypoints carries salient coordinates for not only bounding box but also joints of human pose. This allows each human-centric query to become an expert on position and structure of the corresponding human object.\n\n\n\n\n\n\n \u00a7.\u00a7 Query utilization in task-specific heads\n\n\nPrevious works do not use a learnable query as an input of task-specific head networks, even though the query contains high-level information pre-computed from previous layers, such as the coordinates of a bounding box (and also the joints of a pose in our approach). \nWe simply feed the concatenation of the coordinates along with decoder embeddings into task-specific heads as the form of conditional information. By doing so, each task head can consider useful information in other tasks for improving its accuracy.\n\nIn <ref>, regarding object detection and pose estimation tasks, as learnable keypoints themselves are the results of the tasks, their task heads perform in every layer in the transformer decoder. Other task heads perform only once, after the transformer decoder.\nFor all these task heads, we provide the information of learnable keypoints.\n\n\n\n\n\n  \nPose estimation head\nWhile other task-specific heads have conventional structures similar to DETR\u00a0<cit.> and MaskFormer\u00a0<cit.>, our pose estimation head network has a simple architecture, distinguished from previous pose estimation methods. Our head network receives object queries and produces the coordinates of pose joints directly. Off-the-shelf pose estimation methodologies\u00a0<cit.> used auxiliary expedients such as bounding box cropping & resizing or heat map extraction. Although these methods has advantages in terms of model performance, it has limitations in reducing computational cost. On the other hand, thanks to the advantages inherited from object-centric queries, the proposed method uses a vector-to-vector head network which directly regresses joints of pose from the human-centric query. This makes the computational cost of the head network small compared to the conventional task head receiving a cropped image and producing a heat map (304-to-34 in our head and 256\u00d7256\u00d73-to-56\u00d756\u00d717 in <cit.> for each instance).\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Query utilization in deformable attentions\n\n\n\nOur learnable query carries keypoint coordinates with high accuracy for a bounding box and a pose, and keypoints refer to salient points by definition. If we reuse such information as the sampling points in an attention module, we can reduce redundant computations for computing attention.\nFrom this perspective, we directly use our learnable keypoints as sampling locations in a deformable attention layer.\nTo handle potential regions that cannot be covered by the keypoints, we also use original sampling locations together, predicted by an MLP as done in Deformable DETR\u00a0<cit.>.\n\n\n\n\n\n  \nDeformable Attention with Keypoints\nLet m be an attention head index and let Q\u2208^1\u00d7 D be the query vector which is sum of  and an output of self-attention module. The set of proposed sampling locations _m is a union of two subsets; the set \ud835\udc9f_m of Deformable DETR sampling locations and the set _m of joint coordinates for pose\u00a0(Fig.\u00a0<ref>). The sampling locations in \ud835\udc9f_m are decided by sampling offsets \u0394_m which are generated by MLP over the query vertor Q:\n\n    \ud835\udc9f_m = {_c + \u0394_m|\u2200generated\u00a0\u0394_m} .\n\nNote that the center of the bounding box _c in Eq.\u00a0(<ref>) plays role of the reference point. The joint coordinates for each head _m are equally split from  in Eq.\u00a0(<ref>). For the number  of attention heads and the number  of all sampling locations in each head, let x\u2208^H \u00d7 W \u00d7 D be an image feature map, and let W_m\u2208^D\u00d7 D/ be a learnable weight. The value V_m\u2208^\u00d7 D/ is defined as a stack of sampled features at x W_m:\n\n    V_m = Cat( (x W_m())^T )^T ,\n\nwhere the concatenation is applied across for all \u2208_m. From this, the output of the deformable attention with keypoints can be represented as\n\n    DeformAttKey(Q, V, ) = Cat(A_1 V_1, \u2026, A_ V_) W ,\n\nwhere A_m\u2208^1\u00d7 is an attention coefficient obtained by linear operator over Q and W\u2208^D \u00d7 D is a learnable weight. \n\n\n\n\n  \nMulti-Scale Attention with Keypoints\nEq.\u00a0(<ref>) can naturally be used to the case of multi-scale attention module. Assume that there are  image feature maps x_s\u2208^H_s \u00d7 W_s \u00d7 D for scale index s. By repeatedly applying the sampling process of Eq.\u00a0(<ref>) for each x_s, sampled feature V_m,s is obtained for each scale s. The multi-scale attention value V_m\u2208^\u00d7 D/ is defined as\n\n    V_m = Cat( V_m,1^T, \u2026, V_m,^T )^T .\n\nNow, the multi-scale attention output can be obtained by the same Eq.\u00a0(<ref>) if one assumes the attention coefficient having wider dimension: A_m\u2208^1\u00d7.\n\nSpecifically, regarding 32 sampling locations per an object query, we obtain 16 locations from the keypoints and the remaining 16 locations following Eq.\u00a0(<ref>).\n\n\n\n\n\n\n\n\n\u00a7 EXPERIMENTAL RESULTS\n\n\n\n\n\n \u00a7.\u00a7 Settings\n\nWe use MS COCO 2017 dataset\u00a0<cit.> for network training and validation as all labels for human pose estimation, detection, and segmentation tasks are provided. \nThere might be a task-specific augmentation technique to obtain the best performance for each task. However, we applied one common data augmentation technique in <cit.> because the results of three tasks are obtained from a single image and we need to train all tasks at the same time.\nWe use the AdamW\u00a0<cit.> optimizer with the initial learning rate of 10^-4 and the batch size of 16. We train each model for 368,750 iterations, and apply a learning rate decay at the two iterations of 327,778 and 355,092 with the decay value of 0.1. \n\n\n\n  \nLoss function\nWe use a binary cross-entropy loss for human classification. For the segmentation, we use a binary cross-entropy and the dice loss\u00a0<cit.> as was done in <cit.>. For the regression of the bounding box and joint positions, we use a RLE loss function\u00a0<cit.> because RLE is known to be better than L_1 loss for regression problems. \nWe train multiple tasks together with our unified architecture. To this end, a total training loss is defined with the weighted summation of multiple loss functions for the tasks.\nRegarding bipartite matching, we use the classification and the mask loss\u00a0<cit.> instead of using all the task losses.\nWe empirically found that it is sufficient for matching.\nMore details of the experimental setting can be found in the supplementary material.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Component analysis\n\n\n\n\n\n  \nAblation study\nOur baseline network architecture is based on Mask2Former\u00a0<cit.> for segmentation using the Swin-B\u00a0<cit.> backbone, and we replace their cross-attention layers with deformable attention layers. To predict reference points for deformable attention, we replace learnable query with the bbox part of learnable keypoints.\n\nWe define two models BaseNet-DS and BaseNet-DPS as baseline (<Ref>). The BaseNet-DS conducts only detection and segmentation tasks with above settings. \n\nIn addition to BaseNet-DS, we consider na\u00efve multi-tasking with an additional human pose estimation task. We add a head network and a RLE loss fucntion for human pose estimation to the baseline model, and we consider this model as a baseline multi-tasking model\u00a0(BaseNet-DPS).\n\nIn this ablation study\u00a0(<Ref>), from the baseline models, we add our components of 1) learnable keypoints including also the joint coordinates of a pose, 2) query utilization in task-specific heads (Query util. in T.H.), and 3) query utilization in deformable attentions (Query util. in D.A.) one by one, and analyze their performance.\n\n\n\n\n\n\n\nSimply adding a task causes the degradation of overall performance showing a difficulty of learning shared representation for multi-tasking\u00a0(BaseNet-DS vs. BaseNet-DPS).\n\n\nWhen we employ our query design, the accuracy of detection and segmentation increase (BaseNet-DPS vs. HCQNet-\u03b1). It shows that explicit separation of mixed information can improve accuracies.\nHowever, in this case, the accuracy of human pose estimation drops significantly, because decoder embeddings now do not contain pose information sufficiently.\n\n\n\n\nWhen we provide learnable keypoints to task-specific heads, the accuracies of all tasks increase because the positional and structural information in the learnable keypoints can complement each other of the tasks (HCQNet-\u03b1 vs. HCQNet-\u03b2).\nWe found that this component is more effective in pose estimation than detection even though they have a similar prediction process using regression and iterative refinement.\nWe guess that, in detection, a bounding box has simpler representation so that the prediction head can estimate a proper displacement without the bounding box predicted in the previous layer, but not vice versa in pose estimation.\n\n\n\nBy using the learnable keypoints in deformable attentions, we can expect additional performance gain (HCQNet-\u03b2 vs. HCQNet).\nIn our experiment, this component significantly improves the accuracy of human pose estimation, \n\nbut the gain in segmentation is smaller and it slightly decreases the accuracy of detection.\n\nOverall, our all components (HCQNet) bring significant performance improvement for all target tasks, compared to the baseline multi-tasking model (BaseNet-DPS).\n\n\n\n\n\n  \nTraining speed\nIn addition to improvement of overall accuracy in all tasks, we found that the training speed of our approach is much faster than the baseline multi-tasking model (<ref>). Specifically, HCQNet trained for 100k iterations shows higher average accuracy than BaseNet-DPS trained for 300k iterations, showing more than three times faster training speed. It implies that our approach can be useful for training a larger multi-task model.\n\n\n\n\n\n\n  \nQualitative comparison\nThe effects of our proposed approach can be also found in qualitative results (<ref>).\nAs shown in the second column of <ref>, while task-specific method (Mask2Former) and na\u00efve multi-tasking model (BaseNet-DPS) suffer from segmenting an occluded object, our model (HCQNet) can produce better segmentation result by jointly utilizing the information of other tasks encoded in our HCQ.\n\n\n\n\n  \nVariants of the learnable keypoints\n\n\n\nIn the previous experiment, our proposed components successfully improve learnable keypoints and the performance of various tasks.\nMeanwhile, different forms of learnable keypoints as conditional information may lead more performance improvement because there can be a suitable form of the conditional information for each task.\nIn this experiment, we explore the possibility.\n\n\nWe empirically found that the current form using the coordinates of pose joints in the canonical space is suitable for learning pose estimation. \n\nTherefore, we test different forms for only object detection and segmentation tasks.\nFor the tasks, we test other forms such as the joint coordinates in the image space or keypoint embedding instead of coordinate. For keypoint embedding, we use the same process of obtaining structural embedding used in (<ref>).\n\nThe canonical space coordinate has better accuracy in pose estimation compared to the other variants while having the comparable accuracy on detection and segmentation tasks (<Ref>). This implies that our coordinate information in learnable keypoints can be generally applied to various tasks.\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Comparison\n\n\n\n\n\n\n\nTo show the effectiveness of our approach, we compare our method with various task-specific models\u00a0<cit.> and multi-task models\u00a0<cit.> handling a part of object dectection, instance segmentation, and human pose estimation tasks.\nWe use a residual network\u00a0<cit.> (R-50 and R-152), a feature pyramid network\u00a0<cit.> (fpn), and Swin-Transformer\u00a0<cit.> (Swin-B) as backbone models, pretrained on the ImageNet-1K dataset\u00a0<cit.>. \nAs there is no approach to handle three tasks together, we divide the tasks into two groups in terms of evaluation protocol for fair comparison. Specifically, when we evaluate tasks including pose estimation, the instances with a small size (< 32\u00d732 pixels) are excluded in computing mAP, as was done in <cit.>.\n\n\nNote that once we trained our HCQNet jointly for all tasks, we use it to evaluate with two types of protocals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn <Ref>, we compare our HCQNet with detection and segmentation models\u00a0<cit.>.\nFor a fair comparison, we retrain state-of-the-art task specific models with the Swin-B backbone for a human class only; DAB-DETR\u00a0<cit.> in object detection and Mask2Former\u00a0<cit.> in instance segmentation. \n<Ref> shows that, even though our model runs three tasks simultaneously, it achieves comparable performance to task-specific state-of-the-art models.\n\n\n\n\nIn <Ref>, we compare models handling tasks including pose estimation\u00a0<cit.>.\nIn this experiment, we exclude small-sized human instances in the validation set as mentioned above.\nOur model shows a significantly higher segmentation accuracy than them (<Ref>).\nThe pose estimation accuracy is lower than previous methods, because our models consider small objects in data augmentation, while poes estimation methods do not.\nWhen we finetune our model on the training set focusing on larger instances, our overall performance further improves.\nAs our model uses the simpler architecture of our pose-specific head compared to other pose-specific approaches and is not specialized for a human pose estimation task, the accuracy of our model in pose estimation is still inferior to those of state-of-the-art pose estimation methods\u00a0<cit.>.\n\nOur model is a simple combination of multiple tasks, and not currently optimized in terms of loss balancing, inter-task-correlated loss, and other task-specific designs.\nNonetheless, our model shows comparable performance to task-specific models.\nWe believe that exploring them would lead to the performance improvement of our approach.\n\nWe refer the reader to our supplementary material for results on the OCHuman dataset\u00a0<cit.>.\n\n\n\n\n \u00a7.\u00a7 Computational cost analysis\n\nIn previous experiments, we show that our query design enables a unified network to perform multiple task harmoniously in terms of accuracy.\nIn this section, we validate that the cost effectiveness of our multi-task network.\nThe computational costs of comparing methods are computed on an input image of 1024\u00d71024.\nIn our network, multiple tasks share most computations in backbone, pixel decoder (corresponding transformer encoder), and transformer decoder (<Ref>).\nThe overhead of each task head is negligible, showing the scalability of our approach in terms of increasing the number of target tasks.\nIn addition, it is noteworthy that performing our model alone is more cost-efficient than performing task-specific state-of-the-art models separately.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nWe present a novel human-centric representation for multi-task learning. To this end, a novel query is designed that carries positional coordinates of key points and represents structural information of human instances effectively. In order to exploit the queries with pre-computed, high-level information in task-specific heads, we use the learnable keypoints as conditional input for the task heads and combine the learnable keypoints with deformable attention. As the results, our proposed model shows comparable performance to individual state-of-the art models for multiple human recognition tasks such as pose estimation, segmentation, and detection, while it consumes a significantly small computational cost.\n\n\nWe refer the reader to our supplementary material for discussion about the limitation and potential negative social impact of our approach.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nieee_fullname\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 DETAILS ABOUT EXPERIMENTAL SETTING\n\n\n\n \u00a7.\u00a7 Network architecture\n\n\nAs described in Sec. 3.1 in the main paper, our network consists of image feature extractor, transformer decoder, and task-specific modules.\nOur main difference in the architecture is to concentrate on the transformer decoder.\nThe image feature extractor is based on the Mask2Former\u00a0<cit.>, which consists of a backbone network and a pixel decoder. We can select various backbone models such as residual network\u00a0<cit.> and Swin-transformer\u00a0<cit.>. Following <cit.>, we use multi-scale deformable attention Transformer (MSDeformAttn)\u00a0<cit.> as the pixel decoder.\n\nRegarding the transformer decoder, each individual human in the given image corresponds to each of proposed human-centric queries, and we use 100 queries for all experiments. The decoder consists of 8 decoder layers.\nDistinct form <cit.>, in each decoder layer, we use a deformable attention layer\u00a0<cit.> as a cross-attention layer, and a self-attention layer is placed before the deformable attention layer.\nEach deformable attention layer receives 3 scales of image feature (1/32, 1/16, 1/8 of the feature resolution) from the pixel decoder.\n\n\n\n\n\n \u00a7.\u00a7 Loss function\n\n\nAs described in Sec 4.1 in the main paper, we use four types of loss functions according to the target tasks: classification, segmentation, bbox, and pose.\nWe denote these loss functions as L_c, L_s, L_b, and L_p, respectively.  \nThen, the total training loss is defined as\n\n\n\n\n    L_total = \u03bb_c L_c + \u03bb_s L_s +\u03bb_b L_b + \u03bb_p L_p.\n\nwhere \u03bb_c, \u03bb_s, \u03bb_b and \u03bb_p are the mixing weights and are set to 2, 5, 0.2, and 0.2, respectively, in our experiment.\nBecause there are many possible combinations of mixing weights for multiple tasks, another best combination would exist.\nStill, our models with these weights show practically reasonable performance in the target tasks without sophisticated tuning to search the best hyper-parameters.\n\n\n\nThe training loss is applied to the matched pairs of instances between the prediction and the ground-truth.\n\nWe also use an auxiliary training loss by attaching the prediction layer to each transformer decoder layer, similar to <cit.>.\nThe auxiliary loss is same with L_total.\n\n\n\n \u00a7.\u00a7 Data augmentation\n\nAs described in Sec .4.1 in the main paper, we follow a data augmentation scheme used in <cit.>.\nSpecifically, for each image, we apply random scaling to the image with the range [0.1, 2.0] and crop the scaled image with the fixed size of 1024 \u00d7 1024. If the scaled image is smaller than the cropping size, we apply zero-padding to right- and bottom-side of the image to produce the result image of 1024 \u00d7 1024.\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 ADDITIONAL EXPERIMENTS\n\n\n\n\n\n\n \u00a7.\u00a7 Canonical space of the pose part of learnable keypoints\n\n\nAs described in Sec. 3.2 in the main paper, \nwe normalized the coordinates of the pose part in our learnable keypoints by the box coordinate of the bbox part; we refer it as the canonical space.\n\nWe can also define the coordinates of pose part in the image space instead of canonical space.\nWe empirically found that representing the pose part in the image space causes a large performance drop in pose estimation and it also degrades the accuracy of two other tasks (<Ref>).\n\n\n\n\n\n\n \u00a7.\u00a7 OCHuman dataset\n\nIn Fig. 6 in the main paper, we present several qualitative results on the OCHuman dataset\u00a0<cit.>.\nIn this section, we present quantitative results on the dataset.\nCompared to Pose2Seg\u00a0<cit.>, our method achieves better performance in both detection and segmentation tasks (<Ref>).\nCompared to a state-of-the-art segmentation method (Mask2Former\u00a0<cit.>), our method shows a comparable accuracy in segmentation.\nCompared to our baseline multi-tasking model (BaseNet-DPS), our approach still induces significant improvements in pose estimation and segmentation on this different dataset.\n\nWe show additional visual results of our methods on the COCO and OCHuman datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 ADDITIONAL DISCUSSION\n\n\n\n\n  \nLimitation\n\n\nWhile pose estimation, detection, and segmentation tasks share a common high-level perception and can help each other intuitively,\na mismatch in current task-wise objectives may make an ambiguity in the multi-task learning of them.\n\nFor example, human pose estimation should handle occluded regions while segmentation and detection only handle visible areas.\nAs shown in <ref>, some pose joints in an occluded region make the bounding box expanded, and this eventually degrades the quantitative accuracy of object detection because the ground-truth bounding box is defined by the enclosing box of the visible segments of the instance.\nAlso, all pose joints are computed for any partial human instance (the rightmost column in <ref>). In this case, even though the corresponding bbox and mask of the instance do not always cover the pose, the pose may still affect other tasks negatively.\nConsidering the visibility of pose joints may alleviate this problem.\nExploring this potential mismatch can be a clue to improve overall performance of multi-task learning with a unified architecture.\n\n\n\n\n\n\nWhile we focus on handling the human class only in this paper, our query design is not limited to a human class. Learnable keypoints are currently trained in a supervised manner. To deal with more general classes, we can consider manual labeling or unsupervised keypoint learning as future work.\n\n\n\n\n\n  \nPotential negative social impact\n\nWe use multiple GPUs for several days for training a model, and it induces a significant energy consumption.\nWhile we use the public dataset (MS COCO 2017), there can be a privacy-related problem when collecting an additional dataset.\nAlso, our approach and its extension can be utilized in developing recognition models used in surveillance cameras, having the risk of a detrimental effect on people's privacy.\n\n\n\n\n\n\n\n\n\n\n\n"}