{"entry_id": "http://arxiv.org/abs/2303.07012v1", "published": "20230313111841", "title": "AGTGAN: Unpaired Image Translation for Photographic Ancient Character Generation", "authors": ["Hongxiang Huang", "Daihui Yang", "Gang Dai", "Zhen Han", "Yuyi Wang", "Kin-Man Lam", "Fan Yang", "Shuangping Huang", "Yongge Liu", "Mengchao He"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI"], "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#1\n@ACM@anonymous@addto@macroauthornotes\n    footnote#1\n\nEqual Contribution.\nCorresponding author.\n\n\n 1South China University of Technology, Guangzhou, China\n \n \n 2Pazhou Laboratory, Guangzhou, China\n \n \n 3DAMO Academy, Alibaba Group, Hangzhou, China\n \n \n 4LMU Munich and Siemens AG, Munich, Germany\n \n \n 5ETH Zurich, Zurich, Switzerland\n \n \n 6The Hong Kong Polytechnic University, Hong Kong, China\n \n \n 7Anyang Normal University, Anyang, China\n \n \n \neehsp@scut.edu.cn, eehxhuang@mail.scut.edu.cn\n\n\nHongxiang Huang, Daihui Yang, Gang Dai, Zhen Han, Yuyi Wang, Kin-Man Lam, Fan Yang, Shuangping Huang, Yongge Liu, Mengchao He\n\n\n\n\n\n\nBoth authors contributed equally to this research.\n\n[1]\n\n[1]\neehxhuang@mail.scut.edu.cn\n\n  South China University of Technology\n  Guangzhou\n  China\n\n\n\n\n  Ludwig Maximilian University of Munich\n  Munich\n  Germany\n\n\n\n  Swiss Federal Institute of Technology\n  Zurich\n  Switzerland\n  \n\n  CRRC Institute\n  Zhuzhou\n  China\n  \n  \n\n\n\n  The Hong Kong Polytechnic University\n  Hong Kong\n  China\n\n\n\n  South China University of Technology\n  Guangzhou\n  China\n\n\nCorresponding author.\n\n\n  South China University of Technology\n  Pazhou Laboratory\n  Guangzhou\n  China\n\n\n\n\n  Anyang Normal University\n  Anyang\n  China\n  \n\n\n  DAMO Academy, Alibaba Group\n  Hangzhou\n  China\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe study of ancient writings has great value for archaeology and philology. Essential forms of material are photographic characters, but manual photographic character recognition is extremely time-consuming and expertise-dependent. Automatic classification is therefore greatly desired. However, the current performance is limited due to the lack of annotated data. \nData generation is an inexpensive but useful solution to data scarcity. Nevertheless, the diverse glyph shapes and complex background textures of photographic ancient characters make the generation task difficult, leading to unsatisfactory results of existing methods. To this end, we propose an unsupervised generative adversarial network called AGTGAN in this paper. By explicitly modeling global and local glyph shape style, followed by a stroke-aware texture transfer and an associate adversarial learning mechanism, our method can generate characters with diverse glyphs and realistic textures. \nWe evaluate our method on photographic ancient character datasets, e.g., OBC306 and CSDD. Our method outperforms other state-of-the-art methods in terms of various metrics and performs much better in terms of the diversity and authenticity of generated samples. With our generated images, experiments on the largest photographic oracle bone character dataset show that our method can achieve a significant increase in classification accuracy, up to 16.34%. The source code is available at https://github.com/Hellomystery/AGTGAN.\n\n\n\n\n\n\n\n\n<ccs2012>\n   <concept>\n       <concept_id>10010147.10010371.10010382.10010383</concept_id>\n       <concept_desc>Computing methodologies\u00a0Image processing</concept_desc>\n       <concept_significance>500</concept_significance>\n       </concept>\n   <concept>\n       <concept_id>10010147.10010178.10010224</concept_id>\n       <concept_desc>Computing methodologies\u00a0Computer vision</concept_desc>\n       <concept_significance>500</concept_significance>\n       </concept>\n </ccs2012>\n\n\n[500]Computing methodologies\u00a0Image processing\n[500]Computing methodologies\u00a0Computer vision\n\n\n\n\n\n\n\n\n\n\n\n  \n    < g r a p h i c s >\n\n  Seattle Mariners at Spring Training, 2010.\n  Enjoying the baseball game from the third-base\n  seats. Ichiro Suzuki preparing to bat.\n  \n\n\n\n\nAGTGAN: Unpaired Image Translation for Photographic Ancient Character Generation\n    Mengchao He\n    March 30, 2023\n================================================================================\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\nHuman curiosity for exploring the origins of civilization will never fade away even with the passage of time. Through the imprints left by ancestors, i.e., ancient writings, modern people can learn about ancient civilizations. There are several well-known ancient writings, such as oracle bone inscription, cuneiform, Mayan script, Indus script, and ancient Egyptian hieroglyphic. Recognizing the content of these writings is an important premise to boost academic research in both ancient civilizations and literature.\n\nTraditionally, research on ancient writings has been the prerogative of paleographical specialists. Usually, they transform the photographic characters (PC) into simulated characters (SC) by manually simulating the ancient characters on photographic ancient documents. The simulated characters are often included in the ancient character dictionaries\u00a0<cit.>, as the primary tools for the study of ancient writings. Fig.\u00a0<ref> shows some simulated and photographic characters of oracle bone inscription and cuneiform. However, the specialists identify the photographic characters, based on their experience and intuition, which results in low efficiency and great ambiguity. Automatic photographic ancient character recognition, using deep learning methods, has great potential for accelerating the process of ancient writing research, e.g., decipherment\u00a0<cit.> and restoration\u00a0<cit.>. Nevertheless, deep learning methods require large amounts of well-labeled data to learn an accurate classifier\u00a0<cit.>. Besides, collecting and annotating photographic ancient character data is time-consuming and expertise-dependent. Hence, the performance of existing photographic ancient character classifiers is still unsatisfactory, due to the scarcity of annotated data.\n\nSynthesizing new data is a solution to the data scarcity problem\u00a0<cit.>. Generative adversarial networks (GANs) <cit.> have opened a new door for image generation and brought many amazing results\u00a0<cit.>. However, generating photographic ancient characters is not a simple task. Take the photographic oracle bone characters (POC) shown in Fig.\u00a0<ref> as an example. Due to the differences in shooting angles and writers, POCs render various global and local shape variations, such as inclination or rotation, changes in the relative position of strokes, etc. In terms of texture, POCs are printed with complex backgrounds, because of irregular noise. Thus, diverse glyph shapes and complex background textures are important aspects to be considered in character generation, which make the task difficult.\n\n\n\nThe mainstream methods for character generation believe that character images contain content and style information, and styles can be divided into shape style and texture style\u00a0<cit.>. Some methods\u00a0<cit.> learn to extract content and style information in parallel based on disentangled representation, and then entangle the extracted information to generate characters of specific styles. These methods mainly regard the style of a character as its shape style. However, the complex texture of photographic ancient characters increases the difficulty of decoupling the content and style, which leads to degradation of the generation quality when using these methods. In addition, several two-stage methods\u00a0<cit.> divide the character image generation into a shape modeling stage and a stroke rendering stage. Nevertheless, these methods still require a considerable amount of paired data or class labels for training, which is costly and impractical for ancient writings. Moreover, the existing mainstream character generation methods have a common shortcoming that they model shape styles not explicitly enough, which makes it difficult for them to learn diverse global and local shape patterns without strong supervision. To sum up, the generation of photographic ancient characters needs to consider complex shape styles and texture styles. Besides, it is better to reduce the demand for unpaired data and other annotation information through unsupervised learning.\n\nTo this end, we propose a novel unsupervised generative model, called Associate Glyph-transformation and Texture-transfer GAN (AGTGAN), which learns a complex mapping from simulated characters to photographic characters, to synthesize diverse and realistic photographic character images. Our model cascades a glyph-transformation GAN (GTG) and a texture-transfer GAN (TTG), and is end-to-end trainable. The contributions of this paper are fourfold:\n\n\n\n    \n  * The proposed novel photographic character generation model, i.e., AGTGAN, is the first proposed method for enriching annotated photographic character data. This method integrates GTG and TTG, for generating glyph-shape variations and performing texture transfer, respectively. The whole network is end-to-end trainable by the novel associate adversarial training mechanism.\n    \n  * \n    We propose a glyph-shape generator that combines affine with thin-plate-spline (TPS) transformations to explicitly model the global and local shape styles of photographic ancient characters. In addition, noise injection is introduced to increase the randomness of shape transformation, and the signal-and-noise balanced regularization is proposed to guide the model to generate diverse and meaningful glyph shapes.\n    \n  * A new stroke-aware consistency loss for TTG is introduced to solve the blurring problem of the generated photographic characters in the texture transfer process.\n    \n  * Quantitative and qualitative evaluation results show that our generated samples have good diversity and optimal authenticity. \n    With our generated samples, experiments conducted on the largest photographic oracle bone character (POC) dataset, OBC306\u00a0<cit.>, show that our method achieves an absolute improvement of 16.34% in terms of POC glyph classification accuracy. \n\n\n\n\n\u00a7 RELATED WORK\n\n\nCharacter generation has long been considered an essential challenge, while generating photographic ancient characters has not received the attention it deserves. The current generation of ancient writings is limited to a certain type of ancient characters, and the generated samples do not show the original glyph and texture characteristics of the ancient characters\u00a0<cit.>. \nIn this section, we first review the work in similar fields of photographic ancient character generation and then introduce a framework commonly used in character generation.\n\n\n \u00a7.\u00a7 Handwritten Text Generation\n\nWe first review the methods of handwritten text generation, because ancient writings are essentially handwritten or hand-carved texts. The task of handwritten text generation is similar to that of photographic ancient character generation, aiming to imitate natural handwriting in human style. The early two-step methods\u00a0<cit.> generate isolated letters, and then concatenate them to produce a whole word. These methods rely on handcrafted rules and only generate handwriting with limited variations. Recently, deep generative models directly generate whole-word images. RNN and GAN were used to generate handwriting in a variety of styles\u00a0<cit.>. \n<cit.> and <cit.> generate text images on the condition of extracting style features in a few-shot setup and textual content of a predefined fixed length. In addition, <cit.> is an adversarial augmentation method that increases handwritten text images by transforming shapes.\nThe above-mentioned methods are designed to generate only Latin characters, and most of them require expensive text annotations to enhance the generation quality. \n\n\n\n \u00a7.\u00a7 Font generation\n\n\n\nFont generation is usually regarded as an style transfer task for text images, which is handled by image-to-image translation methods in many works. For example, zi2zi\u00a0<cit.> achieves font style transfer by a conditional GAN. Based on zi2zi, DCFont\u00a0<cit.> introduces a style classifier for better style representation. TET-GAN\u00a0<cit.> learns to disentangle and recombine the content and style features of text images by a stylization subnetwork and a destylization subnetwork, but it only transfers the texture style. It is difficult to model the complicated shape and texture style at the same time if only relying on a single auto-encoder for modeling the font style. Samaneh et al.\u00a0<cit.> proposed a two-stage font generation framework, MC-GAN, which divides the font style into shape style and texture style and uses two different networks to transform shapes and transfer texture successively. However, the method is only applicable to the 26 English letters, resulting in limited generalization ability. \n\nFor font generation of complex characters, e.g., Chinese characters, the glyph shape transformation has always been the focus of researchers' attention. <cit.> were proposed for the monochrome Chinese font generation task, which simplify font styles to shape styles. EMD\u00a0<cit.> used different encoders to extract the content vector and shape style vector of fonts based on the idea of disentanglement. SA-VAE\u00a0<cit.> demonstrated that domain knowledge of Chinese characters, e.g., the information of radicals and stokes, helps improve the output image quality. CalliGAN\u00a0<cit.> and StokeGAN\u00a0<cit.> added extra component codes of characters to train a conditional GAN and a CycleGAN\u00a0<cit.>, respectively, exploiting prior knowledge to maintain structural information. RD-GAN\u00a0<cit.> proposed a radical extraction module to extract radicals as prior knowledge, which can improve the performance of the discriminator and generate unseen characters in a fixed style. Different from previous methods, ChiroGAN\u00a0<cit.> and SCFont\u00a0<cit.> adopted different font generation paradigms, first extracting the skeleton and then rendering the strokes. However, in DG-Font\u00a0<cit.> and ZiGAN\u00a0<cit.>, it was argued that the abovementioned methods\u00a0<cit.> require expensive supervision information, e.g., stroke information or paired data. DG-Font introduced a feature deformation skip connection, achieved by deformable convolution\u00a0<cit.> to improve the ability of the network to produce shape deformation of the strokes or radicals. ZiGAN learned extra structural knowledge in unpaired data to strengthen the coarse-grained understanding of character content. Inspired by <cit.> and <cit.>, AGIS-Net\u00a0<cit.> divided the decoder of the disentanglement framework into two parts, the shape style reconstruction branch and the texture style reconstruction branch, which can realize the shape transformation and texture transfer of complex characters at the same time. \n\nNevertheless, existing font generation methods may not be suitable for the generation of photographic ancient characters for the following reasons. On the one hand, the intra-domain glyph shapes of photographic ancient characters are more diverse than those general font generation tasks. The reason is that photographic ancient characters were engraved or written by different people in different periods and photographed from different angles. On the contrary, the shape style of each font is consistent in general font generation tasks, because the characters of each font are written by one writer. On the other hand, although the texture features, e.g., color, brightness and grain, of photographic ancient Chinese characters are relatively stable, it is difficult to decouple the content (glyph) and texture style, because the texture of background noise is very similar to those of foreground strokes, which easily leads to confusion and misunderstanding.\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Image-to-Image Translation\n\nRecently, image-to-image translation has achieved impressive results\u00a0<cit.>.\n\n\n<cit.> first proposed a conditional GAN to obtain the desired outputs from reference inputs without manually tuning the mapping function. However, this method requires paired training data. To address the unavailable paired data, <cit.> used a cycle consistency regularization term in the loss function. These two works inspire many follow-up approaches\u00a0<cit.>. However, these methods only deal with one-to-one translations. To model one-to-many mapping, <cit.> and <cit.> proposed a disentangled representation framework to transfer the source content to a given style. \nIn CUT\u00a0<cit.>, it was argued that the cycle-consistency loss used by <cit.> assumes that the relationship between the two domains is bijective, which is often too restrictive. An alternative solution was proposed by introducing contrastive learning for unpaired image-to-image translation with a PatchNCE loss to maximize the mutual information between the corresponding patches of the input and output images. Nevertheless, the aforementioned I2I translation methods are capable of texture transfer, but are limited by the shape-variation translation\u00a0<cit.>. \n\nTo resolve the problem, some methods attempted to model the shape style by directly generating image pixels\u00a0<cit.>. <cit.> proposed a discriminator with dilated convolution\u00a0<cit.> to train a shape-aware generator, which achieves global shape transformation. <cit.> implemented local geometric transformations using different embedding networks for the comparative learning strategy. <cit.> proposed a spatial transformer network (STN)\u00a0<cit.> with thin plate spline (TPS) transformation to explicitly transform scene text images and a CycleGAN to transfer the texture style of the images. However, the discriminator connecting the STN and CycleGAN in\u00a0<cit.> is unable to discern shape difference under the interference of texture features. These methods only take global or local deformation into account. However, not only global transformation, but also local transformation is needed for generating glyph shapes of ancient writings because the details of glyph shapes, such as strokes and radicals, are diverse.\n\n\n\n\n\u00a7 PROPOSED METHOD\n\nOur goal is to learn a one-to-many mapping from the source simulated character (SC) domain d_ss to the target photographic character (PC) domain d_tp. This mapping should ensure that all multimodal PC outputs preserve the glyph classes of the input SCs, while yielding rich variations in glyph shapes and texture styles.\n\nInstead of establishing a single-step mapping from d_ss to d_tp, we divide this mapping into two stages: glyph shape mapping and texture mapping. This leads to two more intermediate domains, the transformed SC domain d_ts and the destylized PC domain d_dp. Additionally, we define the final generated PC domain as the realistic PC domain d_rp. All these domains and the overall model are depicted in Fig.\u00a0<ref>, where x, x_t, y_d, \u0177, and y denote the characters sampled from d_ss, d_ts, d_dp, d_rp, and d_tp, respectively.\n\nAs shown in Fig.\u00a0<ref>, our proposed model, called AGTGAN, is composed of a GTG and a TTG for glyph-shape transformation and texture transfer, respectively. Furthermore, we introduce an associate adversarial training mechanism for synergistically improving GTG and TTG, which makes the whole network end-to-end trainable.\n\n\n\n\n\n \u00a7.\u00a7 Glyph-Transformation GAN\n\nGlyph-transformation GAN (GTG), which consists of a subtly designed glyph-shape generator G_g and a CNN discriminator D_g, aims to generate diverse glyph shapes that resemble the PC glyph shapes. \n\n\n\n\n  \u00a7.\u00a7.\u00a7 Glyph Shape Generator\n \nAccording to our observations, \nit is difficult to explicitly model glyph shapes by directly predicting\nimage pixels to achieve global and local shape variations simultaneously without strong supervision. Therefore, we use the spatial transformer network (STN) to resample image pixels with predicted deformed grids, e.g., affine transformation matrix, to achieve explicit shape variations. Our G_g, as illustrated in the green dotted box in Fig.\u00a0<ref>, consists of an STN component\u00a0<cit.> and two reconstruction networks, R_z and R_x, for reconstructing the noises and the input images, respectively. The STN component \nincludes an Encoder and a Predictor, which are used together to estimate the affine and TPS transformation parameters, and a Sampler, which is used to generate x_t by resampling the input x with the estimated parameters. Different from those methods that only use affine or TPS transformation\u00a0<cit.>, our G_g combines affine and TPS transformations, to finely simulate PC glyph shapes at the global and local levels. Specifically, the affine transformation augments the overall glyph shape, producing global shape changes, such as rotation, translation, and scaling, etc., while TPS augments local shape changes, such as stroke length and distortion. We compare the effect of using both TPS and affine transformation with other situations in the Appendix.\n\n\n\nTo achieve glyph-shape variations, we inject Gaussian noise z to the SC features and then, obtain diversified outputs. \nHowever, as mentioned in <cit.>, GAN is prone to ignoring the added noise, thus producing outputs similar to each other. To this end, <cit.> reconstructed the noise vectors from the outputs, so as to preserve the influence of noise. However, they ignored the fact that noise will reduce the authenticity of the generated images, when the influence of noise far exceeds that of input features. To avoid this, we design two reconstruction networks, R_z and R_x, to restore the injected noise z and the input x, respectively, from the same estimated parameters during training. With our devised SNR loss (refer to \nthe section of `Signal-and-Noise Reconstruction Loss'), R_z and R_x make the influence of signal and noise compete against each other during training and finally, reach a balanced state.\nMore details of the architecture of G_g can be found in the Appendix.\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Signal-and-Noise Reconstruction Loss\n\nAs mentioned previously, the signal-and-noise reconstruction (SNR) loss is designed to balance the influence of the input signal and the noise. \nThe signal reconstruction loss ensures that the transformed SCs retain the original glyph structure, i.e., the generated SCs and the inputs should belong to the same character classes, while the noise reconstruction loss guarantees the output diversity.\n\nThe SNR loss contains three terms. The first two terms correspond to the input signal reconstruction error and the noise reconstruction error,  as follows:\n\n    L_1(x, x_rec)+ L_1(z, z_rec),\n\nwhere x_rec and z_rec denote the reconstructed signal and noise, respectively, and L_1 represents the L_1-distance. \n\nIt is worth noting that if the above two terms are not properly regularized, the loss function may be dominant by one of the loss terms. This may lead to either monotonous (not sufficiently) or completely random (meaningless) output.\nTo address this problem, we introduce a balance term, called reconstruction error ratio (RER), to balance the influence of the signal and noise, as follows: \n\n    RER = log( L_1(z, z_rec)/ L_1(x,x_rec)).\n\nIncluding this RER term, the SNR loss is defined as follows:\n\n    \n    L_snr(G_g)= L_1(x, x_rec)+ L_1(z, z_rec)+\u03b1\u00b7 RER,\n\n\nwhere \u03b1 is a dynamic coefficient. We further constrain RER with a hyperparameter M>1. During training, we set \u03b1=1 if RER \u00a0log M. In this case, the noise reconstruction is much worse than the signal reconstruction. This means that the signal is over-dominant, and we use a positive balance term to penalize a large ratio. On the contrary,  we set \u03b1=-1 if RER \u00a0-logM. In this case, the signal reconstruction is much worse than the noise reconstruction. This means that the noise is over-dominant, and we use a negative balance term to penalize a small ratio. If RER falls between the ideal range, i.e., [-log M, log M], we set \u03b1=0, i.e., without any additional penalty. In this case, the influence of the signal and noise will maintain a balance.\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Diversity Loss\n\n\nDiversity is positively correlated with the difference between the two transformation parameters, P(E(x),z_1) and P(E(x),z_2), which are estimated from two mixed signal vectors injected with noises z_1 and z_2, respectively, where z_1 and z_2 are randomly drawn from the same Gaussian distribution. The diversity loss is defined as follows:\n\n    \n    L_d i v(E,P)=- L_1(P(E(x), z_1), P(E(x), z_2)),\n\n\nwhere E and P represent the Encoder and the Predictor in STN, respectively.\n\n\n\n \u00a7.\u00a7 Texture-Transfer GAN\n\n\n\nWe use a cycle-structure GAN <cit.>, called texture-transfer GAN (TTG), to add texture styles to SCs, i.e., x and x_t. TTG consists of two generators, G_XY and G_YX, and two discriminators, D_Y and D_X (see the blue dotted box in Fig.\u00a0<ref>). In order to enhance the adaptability of TTG to PC generation, we propose a stroke-aware cycle consistency loss to prevent PCs from losing strokes or becoming blurred during the texture transfer process.\n\n\n  \u00a7.\u00a7.\u00a7 Stroke-aware Cycle Consistency Loss\n\nCompared to other images, the area occupied by the characters in a character image is usually small, and contains almost all the glyph information. This makes existing cycle-structured networks unsuitable for character generation, because they treat every pixel almost equally. Hence, the output is blurred, or even incomplete characters. \nTo solve this problem, we propose a stroke-aware cycle consistency loss to guide TTG to pay more attention to foreground characters, as follows:\n\n    \n    L_sacyc(G_X Y, G_Y X) =     \ud835\udd3c_x_t[W \u2299 L_1(G_Y X(G_X Y(....\n       .... x_t)),x_t)] \n    \n                +   \ud835\udd3c_y[ L_1(G_X Y(G_Y X(y))..\n       .., y)],\n\n\n\n    \n    L_sacyc(G_X Y, G_Y X) = \n       \ud835\udd3c_x_t[W \u2299 L_1(G_Y X(G_X Y(....\n       .... x_t)),x_t)] \n    \n                +   \ud835\udd3c_y[ L_1(G_X Y(G_Y X(y)), y)],\n\n\nwhere \u2299 denotes the element-wise product and W a weight matrix extracted from SC, as follows:\n\n    \n    W_ij={[ C/S_f g\u00b7 S_b g, x_t^ij\u2208 foreground;               1,x_t^ij\u2208 background ].,\n\n\nwhere S_fg and S_bg denote the area (in pixels) of the foreground region and the background region, respectively. The constant parameter C\u2265 1 determines the trade-off between enforcing foreground clarity and maintaining background authenticity. It is worth noting that the stroke-aware information does not need additional annotations, which can be easily obtained by image binarization.\n\n\n\n \u00a7.\u00a7 Associate Adversarial Training\n\n\nWe propose an associate adversarial training, which connects GTG and TTG through two transitional domains, d_ts and d_dp, to exchange the glyph-shape information and fuse it with texture styles, as shown in Fig.\u00a0<ref>. In this way, the two GANs are trained jointly to adapt to and promote each other. \n\nTo implement the glyph-shape mapping from the source SC domain d_ss to the target PC domain d_tp, G_g of the first GAN attempts to explore diverse glyph shapes, while D_g attempts to distinguish whether the glyph shapes come from the transformed SC domain, d_ts, or the destylized PC domain, d_dp. d_dp is produced via the second GAN with a cycle structure, acting as a bridging domain between the two GANs and guiding the glyph deformation of the input SCs. Through adversarial learning, GTG pushes the distribution of d_ts close to that of d_dp.\n\nWe use the transitional domain d_dp, instead of the target PC domain d_tp, to guide the glyph transformation, because there are some texture style differences between d_tp and d_ss. This will confuse GTG in the glyph transformation learning. d_dp is obtained after gradual destylization through training the TTG, retaining rich global and local glyph-shape patterns. When the texture style of d_dp is gradually reduced, GTG focuses on the glyph-shape differences and precisely learns the glyph shape features.\n\nThe loss function for GTG is a combination of the least squares generative adversarial loss\u00a0<cit.>,  the SNR loss, and the diversity loss, as follows:\n\n    \n    L_G_g=   \ud835\udd3c_x [D_g(G_g(x))-1]^2\n       +  L_snr(G_g)+ L_div(E,P),\n               \n     L_D_g=   1/2\ud835\udd3c_x[D_g(G_g(x))]^2\n       +1/2\ud835\udd3c_y_d[D_g(y_d)-1]^2.\n\n\nTo perform the style mapping from the source SC domain d_ss to the target PC domain d_tp, G_XY takes SCs from d_ts as its input and generates realistic PCs to deceive the corresponding D_Y. D_Y attempts to distinguish the domain of the PC samples, i.e., either the realistic PC domain d_rp or the target PC domain d_tp.\n\n\n\nWe use d_ts, instead of d_ss, as the input domain for TTG, because the glyph difference between d_ss and d_tp can easily confuse TTG when learning to transfer texture styles. With GTG training, the glyph shapes of the d_ts samples gradually approach the glyph of d_dp. In this way, TTG can focus on the texture style differences and capture texture features more accurately. GTG transmits glyph-shape information to TTG by d_ts, which encourages G_XY to generate PCs with realistic glyph shapes and texture styles.\n\nThe loss function for TTG is a combination of the least squares generative adversarial loss and the stroke-aware cycle consistency loss, as follows:\n\n    \n    L_G_XY,G_YX   =\ud835\udd3c_x_t[D_Y(G_XY(x_t))-1]^2\n       +\ud835\udd3c_y[D_X(G_YX(y))-1]^2\n       +\u03bb L_sacyc(G_XY , G_Y X),\n               \n    \n    \n    \n    \n             L_D_Y,D_X=   1/2\ud835\udd3c_x_t[D_Y(G_XY(x_t))]^2\n       +1/2\ud835\udd3c_y[D_Y(y)-1]^2\n       +1/2\ud835\udd3c_y[D_X(G_YX(y))]^2\n       +1/2\ud835\udd3c_x[D_X(x)-1]^2,\n\n\n\nwhere \u03bb is a hyperparameter, which controls the relative importance of the stroke-aware cycle consistency loss.\n\nWith the proposed associate adversarial training for GTG and TTG, they can be trained together harmoniously as a unified pipeline, i.e., AGTGAN.\n\n\n\n\u00a7 EXPERIMENT\n\n\n\n\n \u00a7.\u00a7 Datasets\n\n\n\nSOC5519\u00a0<cit.> contains 44,868 clean simulated oracle bone character (SOC) instances from 5,491 classes, covering almost all the classes that have been discovered. \n\nOBC306\u00a0<cit.> contains 309,511 samples from 306 classes, available from the open-source OBI database.\n\nIt is worth noting that most of the character classes in SOC5519 are unavailable in OBC306 since the number of classes in OBC306 is much fewer than that in SOC5519. \n\nHCCC\u00a0<cit.> is an image set that was manually simulated and arranged from existing hand copies of cuneiform tablets. This dataset contains 4,416 samples from 50 classes that most frequently appear in several corpora.\n\nCSDD\u00a0<cit.> provides bounding box annotations and class labels for signs on 81 tablets' images. We segmented 2,576 photographic cuneiform character (PCC) images according to the bounding box annotations over 233 classes.\n\n\n\n\n\n \u00a7.\u00a7 Training Details\n\nFor all experiments, we set \u03bb = 10, C=2, and M = 6. All the simulated character and photographic images are resized to 64\u00d764. We set the initial learning rate at 0.0001 for GTG and 0.001 for TTG, and use the Adam solver\u00a0<cit.> with a batch size of 64 for optimization. We keep the learning rates constant for the first 15,000 iterations, and linearly decay the rates to zero over the next 15,000 iterations.\n\n\n\n\n \u00a7.\u00a7 Quantitative Evaluation Metrics & User Study\n\n\nFID. \nwe use the Fr\u00e9chet inception distance (FID)\u00a0<cit.> to evaluate the authenticity of the generated images by measuring the distance between the generated distribution and the real distribution based on the features extracted by the inception network\u00a0<cit.>. The lower the FID, the better the quality of the generated images.\n\nNDB and JSD.\nWe use the number of statistically different bins (NDB) and the Jensen-Shannon divergence (JSD)\u00a0<cit.> to evaluate the authenticity of the generated images. Compared with FID, the NDB and JSD metrics are directly applied to image pixels and do not rely on the learned representation. This makes the metric more sensitive to pixel-level differences in images. We set the number of bins for NDB to 50.\n\nLPIPS. \nWe use the learned perceptual image patch similarity (LPIPS) metric\u00a0<cit.> to measure the diversity of generated images. We generate 1,000 samples for each class and compute the LPIPS distance between pairs of samples. The average LPIPS distance for all the classes is used as the final LPIPS value.\n\nUser Study.\n\nWe conduct a user study based on pairwise comparisons. Given the POC image groups generated by AGTGAN and other models, each subject needs to answer the question \u201cWhich POC image group is more realistic?\u201d with a real POC image as a reference.\n\n\n\n \u00a7.\u00a7 Comparison with State-of-the-Art Methods\n\n\nWe compare out proposed AGTGAN with state-of-the-art unsupervised image-to-image models, including CycleGAN\u00a0<cit.>, DRIT++\u00a0<cit.>, NICE-GAN\u00a0<cit.>, DCLGAN\u00a0<cit.>, and DG-Font\u00a0<cit.>, from the perspectives of visual quality, quantitative metrics, user study, and classification performance. \n\n\n\n  \u00a7.\u00a7.\u00a7 Generation Result\n\n\n\n\n\n\n\n\n\n\nFig.\u00a0<ref> demonstrates some generated photographic ancient characters for randomly selected character classes. \nWe can see that our method generates significantly diverse glyph shapes: the strokes and radicals show rich local variations, and the entire characters show different sizes or inclination appearances, while preserving the original glyph labels. In terms of texture style, our generated POC renders more natural background noise and the generated PCC renders a better three-dimensional effect. Furthermore, the characters of both oracle bone inscription and cuneiform generated by our method do not suffer from blurring.\n\nIn contrast, CycleGAN generates some implausible glyphs, e.g., some strokes are missing or wrong strokes are added. This is mainly because the transferred background texture confuses the character strokes, leading to blurring of the generated glyphs. DG-Font, DRIT++, and NICE-GAN generate worse results, such as unrecognizable glyphs and images with artifacts or fog effect. \nA possible reason is that they rely on the decoupled latent vectors of content and style, which are not easily obtained from photographic characters with complex glyph shapes and textures.\nAlthough DCLGAN generates clear glyphs, the background texture of the generated samples is very monotonous and is quite different from the target texture styles. The reason is that the adopted contrastive learning strategy is prone to capturing semantic information of glyphs, while ignoring most of the background details.\nIn addition, many generated samples, e.g., in the second and third rows under \u201cDRIT++\u201d, \u201cNICE-GAN\u201d and \u201cDCLGAN\u201d, still align with the source characters with less glyph-shape variations, indicating that the baselines have difficulties in learning the variety of glyphs from the target. Although DG-Font uses deformable convolution to learn shape styles, it can only produce weak local deformation. It is worth mentioning that we demonstrate the great potential of our method in zero-shot generation in the Appendix.\n\n\n\n\n\nTab.\u00a0<ref> summarizes the quantitative results, and similar conclusions to the above visual analysis can be reached. Our method achieves the best FID, NDB and JSD scores among all methods. \nAlthough DRIT++ and NICE-GAN obtain a higher LPIPS score than our method, the high diversity comes from chaotic textures and incomplete glyphs.  It is worth noting that the classification criteria of the HCCC and CSDD datasets are inconsistent and the samples generated by reference to HCCC cannot be evaluated with the real data of CSDD when applying the FID metric. The results of LPIPS, NDB, and JSD applied to the generated cuneiform also show that our method is superior to other comparison methods, which can be found in the Appendix.\n\nThe results of the user study in Fig.\u00a0<ref> show that more people choose the POCs generated by our AGTGAN to be closer to the target POCs, compared with other methods.\n\n\n\n  \u00a7.\u00a7.\u00a7 Classification Performance\n\n\nWe further evaluate the quality of the generated samples by conducting classification experiments on OBC306. Following the same protocol as in\u00a0<cit.>, we randomly select a quarter of the samples for testing while ensuring that each class has at least one test sample, and the rest form the real POC training set. \nMore setting details can be found in the Appendix.\n\nWe also measured the classification performance of the minority classes to further investigate the effect of the generated POCs.\nThose classes that contain one to ten samples form a few-shot (FS) subset, and those that do not contain any real training samples form a zero-shot (ZS) subset.\n\nTab.\u00a0<ref> summarizes the TOP-1 average class accuracy. The TOP-3 and TOP-5 accuracies are listed in the Appendix. Tab.\u00a0<ref> shows that, with the AGTGAN generated samples, the highest POC classification accuracy can be achieved on OBC306, as well as FS and ZS. \nCompared to \u201cSource only\u201d in Tab.\u00a0<ref>, the improvement achieved using our generated samples is at least 16.34%. By using the training data generated by AGTGAN, the classification accuracy is 81.25% on FS and 93.10% on ZS. The results show that our method has a significant effect on improving the recognition accuracy of the minority classes.\n \n\n\n \u00a7.\u00a7 Ablation Study\n\nFig.\u00a0<ref> summarized the ablation studies for evaluating the impact of different parts of AGTGAN. \n\nDiversity loss (w/o DL):\nOur model, without using the diversity loss, produces POC images with monotonous glyph patterns, which are almost aligned with the SOC glyphs. Thus, the diversity of the generated POCs is not satisfactory. This can also be seen from its smallest LPIPS value, i.e., 0.262, among all the methods compared.\n\nReconstruction error ratio (w/o RER):\nFrom the row of \u201cw/o RER\u201d in Fig.\u00a0<ref>, we can see two distinct types of generated glyphs. One is with little transformation, e.g., the first and third glyph classes, and the other is with large distortion, beyond the plausible glyph space, e.g., the second glyph class. This is because the balance between signal and noise may be broken in the optimization process, if RER is not imposed for balancing. This may cause the noise to become small, which is then filtered by the network, or to become large and dominate the signal.\n\nStroke-aware cycle consistency loss (w/o SA): \nThe \u201cw/o SA\u201d row in Fig.\u00a0<ref> shows that the generated samples have missing strokes or the background textures are incorrectly added to the foreground characters. This dramatically degrades the generation quality. The LPIPS of the generated samples reaches a high value of 0.301, which is mainly due to chaotic glyphs rather than plausible variations.\n\n\n\n\n\n\u00a7 CONCLUSION\n\nWe proposed a novel character generative model, namely AGTGAN, which, so far as we know, is the first method capable of automatically generating rich and realistic photographic ancient characters. \nHence, these generated images, to a certain extent, solve the most critical problem faced by the task of automatic classification of photographic ancient characters, due to lack of well-labeled data. \n\n\n\nA natural direction for future work is to extend our proposed method to more general and complex writing, e.g., handwritten text and formulas. Moreover, the creative font generation tasks, e.g., font design and calligraphy imitation, are other potential applications of our proposed method. \n\n\n\n\u00a7 ACKNOWLEDGEMENTS\n\nThe research is partially supported by National Nature Science Foundation of China (No. 62176093, 61673182, 61936003), Key Realm R&D Program of Guangzhou (No. 202206030001), Guangdong Basic and Applied Basic Research Foundation (No. 2021A1515012282), GD-NSF (No. 2017A030312006), and the Alibaba Innovative Research.\n\n\n\nACM-Reference-Format\n\n\n\n\n\n\n\n\u00a7 APPENDIX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Architecture Details\n\n\nThe glyph shape generator G_g contains a spatial transformer network (STN)\u00a0<cit.> component and two reconstruction networks, R_x and R_z. Herein, the STN is used to predict the thin plate spline (TPS) grid coordinates and affine transformation parameters. R_x and R_z are used to restore the input glyph and the noise, respectively. The details of the Encoder, Predictor of STN, as well as R_x & R_z are shown in Tab.\u00a0<ref>. The four convolutional layers in the first column constitute the Encoder, and the other four fully connected layers, after the reshape operation, constitute the Predictor. \nWe build the signal reconstruction network R_x using the inverse structure of the Encoder & Predictor, while the structure of the noise reconstruction network R_z is the same as the part of the fully connected layers in R_x. \n\n\n\n\n \u00a7.\u00a7 Effects of the combination of affine and TPS transformation\n\nAffine transformation can yield global deformations, e.g., rotation, translation, and scaling of characters, while TPS transformation tends to create local stroke modifications. It is demonstrated in Fig.\u00a0<ref> that the combination of affine and TPS transformations can yield global and local shape deformations simultaneously, while only using TPS transformation lacks  global shape deformation.\n\n\n\n \u00a7.\u00a7 Generated POCs of Unseen Character Classes\n\nCollecting paired ancient character data is costly or even impractical. Thanks to unsupervised learning, our method can generate samples that are unseen during training, which is of great significance to solve the problem of data scarcity.\nFig.\u00a0<ref> shows the comparison results between our AGTGAN and other models. Nine character classes unseen in OBC306 are randomly selected for comparison. We can see that our AGTGAN is still able to generate realistic and diverse photographic oracle bone characters (POCs), even if there are no real POC samples available for training. The POCs generated by other models are more or less flawed, as they do not have rich glyph shape variations, or have blurry strokes, fog effects, chaotic glyphs, etc.\n\n\n\n\n\n \u00a7.\u00a7 Quantitative Evaluation of Generated PCCs\n\nTab.\u00a0<ref> summarizes the quantitative results of generated photographic cuneiform characters (PCCs). \n\nIn terms of NDB and JSD, our method expresses the superior performance compared to other methods with a remarkable gap away from the second-best. Besides, our method also achieves the best results on LPIPS, which measures the diversity of generated samples.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Classification Setting Details\n\nThe classification performance is measured by the average class accuracy over all classes in the test set, rather than the accuracy of all samples. Otherwise, the overall accuracy will be dominated by the major classes. Using the the average class accuracy can equally reflect the accuracy of each character class, including the minority classes. For classes containing less than 750 training samples, we use the comparison methods to generate new POCs, to ensure that each class has at least 750 samples after combining the real and generated POCs. For classes with more than 750 samples, no extra generated images are used to train the classifier. It is worth noting that 750 is the average number of samples in the classes of OBC306. According to\u00a0<cit.>, we select the best-performing Inception-v4 as the backbone of the POC recognizer.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Classification Performance in terms of TOP-K\n\nThe TOP-3 and TOP-5 average class accuracy are tabulated in Tab.\u00a0<ref>.\nTraining the classifier with generated samples added, we achieve the best TOP-3 and TOP-5 performances, with the average class accuracies reaching 93.13% and 94.61%, respectively.\nThese results are significantly better than those achieved by all other models and `Source Only', demonstrating the superiority of our method.\nBesides, we can see that AGTGAN and NICE-GAN achieve the same TOP-3 and TOP-5 accuracy of 96.55% under the setting of ZS. \nThe reason is that there are only one test sample per class in 29 zero-shot classes of OBC306, and the prediction performance for 28 out of 29 classes, enhanced by these two methods, can reach 100% in terms of both TOP-3 and TOP-5 classification accuracies. \n\nTherefore, the ZS accuracy can be calculated as follow:  \n\n    28 \u00d7 100%/29 = 96.55%.\n\n\n\n\n\n"}