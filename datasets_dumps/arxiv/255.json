{"entry_id": "http://arxiv.org/abs/2303.07003v1", "published": "20230313110005", "title": "Review on the Feasibility of Adversarial Evasion Attacks and Defenses for Network Intrusion Detection Systems", "authors": ["Islam Debicha", "Benjamin Cochez", "Tayeb Kenaza", "Thibault Debatty", "Jean-Michel Dricot", "Wim Mees"], "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI"], "text": "\n\t\n\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\tmymainaddress,mysecondaryaddress]Islam Debichamycorrespondingauthor\n\t\t[mycorrespondingauthor]Corresponding author\n\t\tislam.debicha@ulb.be\n\t\t\n\t\tmymainaddress]Benjamin Cochezmycorrespondingauthor\n\t\t\n\t\tbenjamin.cochez@ulb.be\n\t\tmythirdaddress]Tayeb Kenaza\n\t\tmysecondaryaddress]Thibault Debatty\n\t\tmymainaddress]Jean-Michel Dricot\n\t\tmysecondaryaddress]Wim Mees \n\t\t\n\t\t[mymainaddress] Cybersecurity Research Center, Universit\u00e9 Libre de Bruxelles, 1000 Brussels, Belgium\n\t\t[mysecondaryaddress]Cyber Defence Lab, Royal Military Academy, 1000 Brussels, Belgium\n\t\t[mythirdaddress]Computer Security Laboratory, Ecole Militaire Polytechnique, Algiers, Algeria\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\tNowadays, numerous applications incorporate machine learning (ML) algorithms due to their prominent achievements. However, many studies in the field of computer vision have shown that ML can be fooled by intentionally crafted instances, called adversarial examples. These adversarial examples take advantage of the intrinsic vulnerability of ML models. Recent research raises many concerns in the cybersecurity field. An increasing number of researchers are studying the feasibility of such attacks on security systems based on ML algorithms, such as Intrusion Detection Systems (IDS). The feasibility of such adversarial attacks would be influenced by various domain-specific constraints. This can potentially increase the difficulty of crafting adversarial examples. Despite the considerable amount of research that has been done in this area, much of it focuses on showing that it is possible to fool a model using features extracted from the raw data but does not address the practical side, i.e., the reverse transformation from theory to practice. For this reason, we propose a review browsing through various important papers to provide a comprehensive analysis. Our analysis highlights some challenges that have not been addressed in the reviewed papers. \n\t\t\t\n\t\t\n\t\t\n\t\t\n\t\t\tAdversarial Machine Learning Intrusion Detection Systems Adversarial Attacks Adversarial defenses\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n\n\t\n\t\n\n\u00a7 INTRODUCTION\n\n\t\n\tWith the development of new technologies and the increasing evolution of Internet interconnections, security is now a crucial issue. To defend against the various existing attacks, some defensive systems use ML algorithms such as anomaly-based IDS (AIDS) which is currently a widely used security tool <cit.> due to its ability, among other benefits, to detect unknown attacks, i.e., zero-day  attacks<cit.>. Nevertheless, recent studies have shown that ML models in general, and deep neural networks in particular, are vulnerable to so-called adversarial attacks and that the addition of small specifically designed perturbations can mislead the classifier <cit.>.\n\t\n\tToday, problems related to the security of ML-based IDS are an active research topic <cit.>. In the context of network-based IDS, this means that it is possible to design specific perturbations to be added to network traffic by manipulating certain properties, such as packet size, or send/receive time and duration. These perturbations can mislead a classifier into identifying attack traffic as benign and thus evading the intrusion detection system.\n\t\n\t\n\t\n\tIn addition, a significant amount of research on the impact of adversarial learning in computer vision has been transferred into intrusion detection. Initial results have shown that the classifiers used in AIDS are also vulnerable to these algorithms. A typical approach used by researchers is to focus on the theoretical aspect of the problem by setting simplifying assumptions and focusing only on the feature space <cit.>. However, unlike computer vision where the created perturbations have relatively few constraints, a valid network traffic perturbation must satisfy many domain-specific constraints (both semantic and syntactic). These domain-specific constraints ensure that the added perturbation will generate valid network traffic enabling the transition from feature space to traffic space. Unfortunately, network-specific constraints are often not considered or only to a limited extent. This means that the feasibility of attacks from a realistic point of view is not fully considered. Some researchers <cit.> have decided to take a different approach to deal with the problem by limiting the need for feature knowledge by directly manipulating the traffic space.\n\t\n\t\n\tOur main contribution is therefore a revised review of the state of the art providing a new aspect based on the feasibility of attacks. We also provide an update on new contributions that have been produced recently concerning the feasibility of attacks in real settings:\n\t\n\t\t\n  * We propose a complete analysis, for each selected paper, on the real feasibility of the proposed attacks by demonstrating whether or not the constraints of the domain are respected. \n\t\t\n  * We propose an analysis of the defenses used in the papers studied to highlight the strengths and weaknesses of each. \n\t\t\n  * We identify some realistic aspects that should be considered for future studies of the impact of adversarial attacks on IDSs.\n\t\n\tWe believe this review will help future research in creating realistic attacks that consider the full context of the IDS domain. We also believe that it will help in the understanding and creation of new defense mechanisms that improve the robustness of the developed models. \n\t\n\tThe rest of the paper is structured as follows. Section <ref> gives a theoretical reminder introducing the key concepts used in the reviewed papers. Section <ref> summarizes previous reviews that have been conducted on the subject. Section <ref> describes the most commonly used state-of-the-art attacks in the literature. Section <ref> shows the most popular defense mechanisms used in the literature to counter the attacks described in Section <ref> . Section <ref> contains a detailed analysis of the realism of the selected papers. Section <ref> discusses the actual feasibility of the attacks present in Section <ref> and the challenges associated with them. Section <ref> concludes the paper by providing the key points that have been discussed.\n\t\n\t\n\n\u00a7  BACKGROUND AND RELATED WORK\n\n\t\n\t\n\t\n\t\n\n \u00a7.\u00a7 Anomaly-based IDS\n\n\tThe increasing development of new threats targeting network infrastructures worldwide has pushed researchers to develop new defenses. Due to the huge number of undiscovered attacks, most defense mechanisms are unable to cope with such threats. To mitigate this problem, solutions such as Anomaly-Based IDS ( AIDS ), which can detect some of these previously unknown attacks through the use of statistics and machine learning algorithms, are gaining popularity. AIDS have many properties, among which we note their ability to be deployed in a network (NIDS) or directly on a host (HIDS). As far as NIDS are concerned, two different types can be found: packet-based and flow-based. The data used by NIDS to collect this information can come from different sources such as network protocols like NetFlow/IPFIX, SNMP, or directly from an agent. NIDS can also use application logs from anti-virus or firewalls. To measure the performance of NIDS, different metrics are used such as true positive rate (TPR), true negative rate (TNR), false positive rate (FPR), false negative rate (FNR), accuracy, precision, F1 score, error rate, area under the curve (AUC). All these metrics are derived from the confusion matrix shown in <ref>.\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n \u00a7.\u00a7 Threat model\n\n\tAlthough there are several types of threats, an attacker often seeks to violate one of the following properties: confidentiality, integrity, authenticity, and availability.\n\t\n\tIn terms of threat modeling, there are two important points to consider, namely the knowledge restriction corresponding to the complexity of the attack and the objective of the attack, corresponding to the capability of the attack.\n\t\n\t\n\n  \nKnowledge restriction\n\tAs shown in Figure <ref>, attacks can be conducted in two forms, black box or white box. The white box attack means that the adversary knows everything about the training dataset and the model architecture, in particular all the parameters and meta-parameters which are for example the inputs, the gradients (for DNNs), the tree depth (for decision trees) or the number of neighbors (for K-nearest neighbors) as well as the chosen cost function or the type of optimizer (e.g., ADAM or RMSProp) in case of neural networks.  The black box refers to the fact that the attacker knows nothing about the target model, i.e., the architecture of the model and the dataset used. The attacker can only send requests to the targeted model and receive answers in the form of decisions or probabilities (logits). He must, therefore, without knowing any information about the model, approximate a decision boundary similar to that of the target model to be able to craft adversarial samples. Another option for black box attacks is exploiting transferability.  An attacker can create a surrogate model, similar in functionality to the targeted model, craft adversarial instances to fool the surrogate model, and then transfer those instances to the targeted model so that it will also be fooled. Black-box attacks are more complicated to perform since less knowledge is available, but also because more computational resources are needed to accommodate this accumulated knowledge (queries). \n\t\n\t\n\n  \nAttack objective\n\tAnother relevant property of an attack is its objective. There are two different types of objectives, the untargeted attack, and the targeted attack. A non-targeted attack is easier to perform since all the attacker has to do is trick the machine learning model without any particular considerations. Two possible scenarios can be expected. The first is confidence reduction, which means that the attacker simply wants to decrease the performance of the model while maintaining the overall functionality. The second scenario is misclassification. In this case, the adversary's goal is to trick the model into misclassifying without specific constraints. In a targeted attack, the adversary's goal is to force an ML model to produce the desired output by manipulating the input. This type of objective is therefore more complicated to achieve because it requires manipulating the model in a specific direction, unlike a non-targeted attack that is not limited to a certain target. There are two variants of targeted attacks that can be highlighted. The first is targeted misclassification, which means that an attacker wants to cause misclassification in a certain target class with any input. The other variant is source/target misclassification, which means that an attacker wants to cause misclassification in a certain target class with a certain input. This particular goal is the most difficult to achieve.\n\t\n\t\n\n \u00a7.\u00a7 Adversarial examples\n\n\tAdversarial learning refers to the problem of designing attacks against machine learning as well as defenses against these attacks. Depending on the phase in which the attack is carried out, adversarial attacks can be divided into poisoning and evasion attacks. This paper focuses on evasion attacks. This choice is due to the fact that this review wants to focus on the most realistic aspects of adversarial attacks against NIDS. The problem with poisoning attacks is that they require the ability to directly manipulate the model training data. It is clear that in a realistic scenario, the attacker's knowledge will be limited, and it will be less possible to manipulate the model before its training phase.\n\t\n\tThe creation of adversarial examples can be expressed as an initial problem formulation as defined in Eq. <ref>.\n\t\n    Minimize:  D(x, x + \u03b4)\n    Such that:  C(x + \u03b4) = t \u2014- constraint 1\n    \n    \t\t\tx + \u03b4\u2208 [0,1]^n \u2014- constraint 2\n\n\twhere we want to minimize the distance between the original element and the adversarial element D(x, x + \u03b4) respecting 2 constraints. The first is that the classification C of x + \u03b4 must be classified as the target label t. The second is that x + \u03b4 must be a valid element.\n\t\n\tAccording to the work of Szegedy et al. <cit.>, adversarial examples exploit the fact that neural networks have \"blind spots\". The cause of this \"blind spot\" effect would be due to the models being non-linear and trying to behave linearly as concluded by Goodfellow et al. <cit.>. These adversarial examples have certain properties described below. \n\t\n\t\n\n  \nLP norms\n\tTo compute the distance between the original element x and the perturbed element x_adv, an LP norm (i.e., distance metric) is used such as L_0,L_1, L_2 and L_\u221e allowing to define the boundary of adversarial examples. These norms are thus used to minimize the perturbation rate used to generate the adversarial example. The most common norms used by adversarial algorithms are:\n\t\n\tL_0: This distance metric counts the number of features of x modified in x_adv. This metric only takes into account the number of modified features regardless of the perturbation rate introduced in each feature.\n\t\n\tL_1: This norm represents the Manhattan distance between x and x_adv as defined in Eq. <ref>.\n\t\n    L_1 = |x_1 - x_1_adv| +  ... + |x_n - x_n_adv|\n\n\t\n\tL_2: This norm calculates the Euclidean distance or the mean-squared error between x and x_adv as shown in Eq. <ref>.\n\t\n    L_2 = \u221a((x_1 - x_1_adv)^2  + ... + (x_n - x_n_adv)^2)\n\n\t\n\tL_\u221e: This norm gives the largest change among all features of x_adv compared to x and it's defined in the following Eq. <ref>.\n\t\n    L_\u221e = max(|x_1 - x_1_adv|, ..., |x_n - x_n_adv|)\n\n\t\n\t\n\t\n\n  \nAttack frequency\n\tAttack frequency is a property that defines whether the attack is executed in a one-step iteration or requires several. Thus, there are two types of attacks: one-step attacks and iterative attacks. One-step attacks mean that the adversarial examples are generated by an algorithm that executes only once, i.e., it does not iterate multiple times to optimize the adversarial example. Thus, one-step attacks are faster but less optimized. Iterative attacks on the other hand use iterative functions to generate adversarial examples so that it maximizes their efficiency but takes more time.\n\t\n\t\n\t\n\t\n\t\n\n  \nDomain constraints\n\tThe feasibility of adversarial attacks is domain-specific and is influenced by several constraints. These constraints can be divided into two main categories: syntactic constraints and semantic constraints. The following syntactic constraints were originally discussed by Merzouk et al. <cit.>. As for the semantic links, the exact definition of these constraints is difficult since they are specific to each domain and even to each type of feature used. However, we draw on the work of Hashemi et al. <cit.>, and Teuffeunbach et al. <cit.> in the IDS domain to provide a generalization of three different groups with different semantic links.\n\t\n\tSyntactic constraints concern all those related to syntax, e.g., out-of-range values, non-binary values and multiple category membership. Out-of-range values are values that exceed a theoretical maximum value that cannot be exceeded, for example, a float between 0 and 1 or an integer between 0 and 255. Non-binary values are entries that violate the binary nature of a feature and multiple category membership are values that violate the one-hot encoding concept.\n\t\n\tSemantic Links represent the links that certain features may have with each other. These features can be grouped into three distinct groups, each with different semantic properties. The first set includes features that cannot be modified (e.g., IP address, protocol type). The second group includes features that can be directly modified (number of forward packets, size of the forward packet, flow duration, ...). The last group concerns the features that depend on the second group. They must be recalculated based on the latter (number of packets/second or average forward packet size). \n\t\n\t\n\tThis implies that the complexity of generating realistic adversarial examples varies with the different types of data used to represent the domain, such as numerical (continuous or discrete) or categorical data. It also depends on the context in which the model is located (such as network traffic). The NIDS domain is therefore strongly affected by both semantic and syntactic constraints as it uses heterogeneous data types, and its context requires several semantic links most of the time unlike other domains such as computer vision.\n\t\n\t\n\n  \nManipulation space\n\tAn essential property of a realistic adversarial instance is the ability of an attacker to modify its characteristics. In theory, it is possible to directly modify the features of adversarial instances. However, in real-world scenarios, this approach is considered unsuitable for certain domains such as IDSs that analyze network traffic. This is mainly due to the fact that the feature extraction process (i.e., from raw traffic to feature space) is not a fully reversible process, unlike other domains such as computer vision. This means that features can be extracted, and modified but not easily reintroduced into network traffic due to the semantic links between features. Moreover, direct feature modification requires full knowledge of the feature extraction process used by the IDS in order to respect the syntactic or semantic constraints assigned to them. We can therefore deduce that working on the feature space is not very realistic. For this reason, recent studies <cit.> propose to manipulate directly the raw network traffic so that it is not necessary to know the features used, nor to transform the feature values into traffic form. In this way, we can distinguish two manipulation spaces, the feature-based and the traffic-based.\n\t\n\t\n\n \u00a7.\u00a7 Related work\n\n\t\n\t\n\t\n\tNumerous research studies on the real impact of adversarial attacks have already been extensively conducted in the compute vision field, which has also urged researchers to study the issue in the cybersecurity field. Today, the number of papers on this topic is rapidly increasing and the actual impact of these attacks in a real-world scenario seems to be getting clearer. To help the community gain more insight into the topic, we analyze the important aspects of the feasibility of adversarial attacks by comparing the different research and reviews on the topic, especially those related to IDS.\n\t\n\t\n\tIn the review proposed by Reza et al. <cit.>, the authors focus on giving a better understanding of adversarial examples in the computer vision domain. They propose an analysis of numerous attacks and defenses dedicated to this domain. Among these attacks, some are more realistic as they are directly applicable to a real-world scenario. However, this review does not provide any information about the implication of these attacks and defenses in the IDS domain. In addition, the review does not address the topic of domain constraints, nor the attack manipulation space. \n\t\n\tVitorino et al. <cit.> took an interesting approach in their paper to analyze, from the point of view of domain constraints, the suitability of adversarial attacks for the IDS domain. They showed that most of the state-of-the-art attacks, initially dedicated to computer vision, were not suitable for the IDS domain as they did not comply with these constraints. However, the paper does not address the manipulation space used, nor the problems related to the respect of semantic and syntactic constraints found in papers dealing with attacks against IDSs. In addition, defenses against adversarial examples in the IDS domain are also not addressed. \n\t\n\tThe study proposed by McCarthy et al. <cit.> proposes the analysis of several attacks and defenses in different domains of cybersecurity, namely intrusion detection, malware detection, and anomaly detection in industrial systems. They pointed out some constraints related to adversarial algorithms. Our review further elaborates on the manipulation space property, as well as a discussion of semantic and syntactic constraints that are not discussed in detail in their paper. In addition, our work surveys more recent papers.\n\t\n\tThe paper by Apruzzese et al. <cit.> provides interesting insights into the manipulation space used in defining attacks as problem or feature-based. This work also provides an in-depth analysis of the different learning phases of the model by articulating the feasibility at all levels of the machine-learning pipeline. Our work differs by providing an analysis of more recent work on the topic and an explanation of each paper based on the domain constraints analysis. In addition, their work does not include an overview of possible defenses. \n\t\n\tMartins et al. <cit.> provides a comprehensive overview of adversarial attacks against IDS and malware classifiers. They also describe the state of the art of defenses. However, this review does not include a discussion of the feasibility aspect of adversarial attacks and defenses. In our contribution, an analysis of the realistic aspect of the state-of-the-art defenses and attacks is introduced with an explanation of their feasibility.\n\t\n\t\n\n\u00a7 ADVERSARIAL STRATEGIES\n\n\t\n\tIn this section, we present state-of-the-art adversarial attacks, classified into white-box and black-box algorithms. A list of these attacks can be found in <ref>.\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n \u00a7.\u00a7 White-Box algorithms\n\n\t\n\n  \nLimited-memory Broyden Fletcher Goldfarb Shanno (L-BFGS)\n\t\n\tThe idea of this iterative attack is to produce an instance x_adv similar to the initial instance x under the distance L_2 but have x_adv classified as another target class using the L-BFGS box constraint. For this, Szegedy et al. <cit.> explain that it's possible to express the initial problem as a constrained minimization problem to generate targeted adversarial examples as illustrated in Eq. <ref>. \n\t\n    Minimize: ||x - x_adv||_2 \n    Such that: C(x_adv) = t \n    \n    \t\t\tx_adv\u2208 [0,1]^n\n\n\tSince this problem is difficult to solve, they adapt it into an easier-to-handle variant, as shown in Eq. <ref>.\n\t\n    Minimize: c.|x - x_adv| + J(x_adv, t)\n    Such that: x_adv\u2208 [0,1]^n\n\n\twhere x is the input element, x_adv is the adversarial element, c is a positive constant, J is the loss function and t is the target label. \n\t\n\tOn the other hand, while L-BFGS is an effective attack, it can be time-consuming due to the use of the linear search method to find an optimal c. \n\t\n\t\n\n  \nFast gradient sign method (FGSM)\n\tThis one-step algorithm was developed by Goodfellow et al.  in their 2014 paper <cit.>. The idea of FGSM is to generate perturbation using gradient ascent to maximize the loss function. FGSM can be used as a targeted or untargeted attack and originally runs under the L_\u221e norm but is easily adaptable for the L_2 norm. FGSM is a very fast algorithm for generating adversarial instances even if the adversarial samples are not optimized because it does not minimize the generated perturbation. This algorithm is very efficient, in most cases, at creating adversarial perturbations in a time-efficient manner. It can be defined by the following Eq. <ref>. \n\t\n    x_adv = x + \u03f5 * sign(\u2207_x J(x, y))\n\n\twhere \u03f5 is the variable allowing control of the amount of perturbation, y is the desired label, and the input x. The main disadvantage of this attack from a network traffic perspective is that all features are modified, making it less practical in real-life scenarios. \n\t\n\t\n\n  \nBasic Iterative Method/Projected Gradient Descent (BIM/PGD)\n\tThis is an improvement of FGSM where the algorithm iteratively increases the amount of perturbation to cause misclassification. It is more efficient than the classical FGSM in terms of misclassification, but on the other hand, this attack takes more time to create adversarial examples. PGD is an algorithm proposed by Aleksander Madry et al. <cit.> and BIM is proposed by Alexey Kurakin et al. <cit.>. Both attacks are quite similar as they use, at each iteration, a projection function to project the adversarial examples into the \u03f5-ball which can be L_2 or L_\u221e, as shown in Eq. <ref>.\n\t\n\t\n    x^t_adv = Proj[x^t-1 + \u03f5 * sign(\u2207_x J(x^t-1, y))]\n\n\twhere x_0_adv = 0 and Proj is the projection function.\n\t\n\tThe main difference between the BIM and PGD versions of the attack concerns the initialization of the attack. Indeed, BIM sets the value of the original point as the initialization point while PGD starts the attack at a random point using the L_\u221e norm. Moreover, at each restart, a new random point is chosen. Since the results of these two attacks are generally quite similar, it is common to use only one of them when testing.\n\t\n\t\n\n  \nDeepFool\n\tThis attack proposed by Moosavi-Dezfooli et al. <cit.> works as an untargeted attack and iteratively generates small perturbations to fool the classification. This algorithm uses the L_2 norm to generate these perturbations. To do so, this attack determines the nearest hyperplane for an input element and projects it beyond this hyperplane. This method is primarily based on the assumption that the model is completely linear. However, in most high-dimensional models, as in many deep neural networks, this is rarely the case. To overcome this problem, a linear approximation is first performed. The main problem with this attack is the inability to introduce domain-specific constraints and the significantly longer time required to generate adversarial instances compared to the FGSM.\n\t\n\t\n\n  \nJacobian-based Saliency Map Attack (JSMA)\n\t\n\tJSMA is an iterative and targeted algorithm proposed by Nicolas Papernot et al. <cit.> that uses a saliency map to tell which feature has the greatest impact on classification. This saliency map is based on a jacobian matrix which is a matrix containing the first-order partial derivatives as defined in Eq. <ref>\n\t\n    J_F(x) = \u2202 F(x)/\u2202 x = [\u2202 F_j(x)/\u2202 x_i] i \u00d7 j\n\n\tThis jacobian matrix, therefore, allows us to obtain the direction of sensitivity and, therefore know what input element influences the most desired output. This algorithm, based on the L_0 norm, has the advantage that it can generate adversarial samples using fewer features. It is therefore an interesting option for practical attacks against IDS.\n\t\n\t\n\n  \nCarlini and Wagner (C&W)\n\tCarlini and Wagner <cit.> proposed an optimization algorithm to generate adversarial examples under the L_0, L_2, and L_\u221e norms. This attack is different from L-BFGS because it uses a different loss function to escape box constraints. They redefine the initial problem of adversarial examples previously defined in Eq. <ref>.\n\tThis redefinition is given in Eq. <ref>\n\t\n    Minimize: D(x, x + \u03b4) + c.f(x + \u03b4) \n    Such that: x + \u03b4\u2208 [0,1]^n \u2014\u2013 constraint 2\n\n\tThis attack is one of the most successful since it was able to break several defenses such as defensive distillation (see section <ref>). This attack can be used in a targeted and non-targeted version. Nevertheless, even if C&W is very efficient, it should also be noted that this algorithm takes significantly more time to generate adversarial instances.\n\t\n\t\n\n  \nElastic-Net Attacks to Deep Neural Networks (EAD)\n\tThis iterative algorithm proposed by Pin-Yu et al. <cit.> introduces the use of the L_1 norm to generate perturbations to create adversarial examples. The authors transformed the problem into an elastic network regularized optimization problem. The elastic network regularization takes advantage of Lasso (using the L_1 norm) and Ridge (using the L_2 norm) regularization. EAD uses an iterative attack under L_2 using a L_1 regularizer. The original Elastic-Net regularization defined in Eq. <ref> is redefined for EAD as shown in Eq. <ref>.\n\t\n\t\n    Minimize_z \u2208 Zf(z) + \u03bb_1 ||z||_1 + \u03bb_2 ||z||^2_2\n\n\t\n    Minimize_xc.f(x,t) + \u03b2 ||x - x_0||_1 + ||x - x_0||^2_2 \n    Such that: x \u2208 [0,1]^n\n\n\t\n\tThe experimental results <cit.> show that the attack is as effective as other state-of-the-art attacks. It is important to note that the results of this attack showed that it was the most effective in terms of transferability, which makes it interesting both for attackers using substitute models (black-box attack) and also for defenders using the adversarial training defense. In addition, the authors showed that this attack, like C&W, can break the defensive distillation defense. However, due to its optimization problem, it takes more time to execute than FGSM.\n\t\n\t\n\n \u00a7.\u00a7 Black-Box algorithms\n\n\t\n\t\n\n  \nZeroth-Order Optimization (ZOO)\n\tIt's a black-box and score-based algorithm inspired by the C&W attack. As the name suggests, instead of using First-Order Optimization, it employs Zeroth-Order Optimization. It uses the logit values thanks to a zeroth order oracle to estimate the gradients. To estimate the gradients and Hessian, the authors <cit.> use the symmetric quotient difference.\n\t\n\tTo avoid detection by other defenses, Oracle queries must be reduced. To this end, ZOO employs three techniques, importance sampling, hierarchical attacks, and attack space reduction. The results of the experiments suggest that this attack is effective against ML models in black-box settings.\n\t\n\tIt is important to note that while this technique has comparable performance to C&W and yields a better attack success rate than substitute model attack, it is much more resource intensive than white-box algorithms, and even than the substitute model attack.\n\t\n\t\n\n  \nBoundary\n\tThis iterative targeted/non-targeted decision-based attack created by Wieland Brendel et al. <cit.> is notably effective as it does not require gradient information and succeeds in defeating many existing defenses like defensive distillation and gradient masking defenses. Moreover, it is more realistic with respect to the other attacks as it doesn't rely on probabilities, but rather on the decision, which is what Machine Learning APIs typically provide. According to the authors, despite being a black-box attack, the Boundary attack produces a similar misclassification efficiency as other white-box attacks such as FGSM, C&W, and DeepFool.\n\t\n\tBoundary uses a relatively simple and flexible algorithm. This attack uses a simple rejection sampling algorithm to track the decision boundary from the adversarial classification region to the non-adversarial region. The main drawback of this attack is that it uses an excessive number of iterations to find adversarial examples due to its brute-force nature. \n\t\n\t\n\n  \nOPT\n\tThe OPT attack, proposed by Minhao Cheng et al. <cit.>, is an iterative decision-based black-box attack that can be targeted or untargeted. Being a decision-based attack means that it just needs the decisions rather than logits or probabilities. This optimization-based attack uses the Randomized Gradient-Free (RGF) method to estimate the gradient at each iteration rather than using the zeroth-order coordinate descent method, which provides lower performance. The RGF method is defined in Eq. <ref> to estimate the gradient:\n\t\n    \u011d = g(\u03b8 + \u03b2 u) - g(\u03b8)/\u03b2.u\n\n\twhere g is the search direction, g(\u03b8) is the distance from x_0 to the nearest adversarial example along the direction \u03b8, \u03b2 > 0 is a parameter and u is a random Gaussian vector. \n\t\n\tThis attack uses the L_2 and L_\u221e norms to determine the perturbation rate to be applied and the binary search to evaluate the objective function. The results showed that the OPT attack was more efficient in terms of the number of queries required than the boundary attack, a similar attack in that it also uses only the model decision to be able to generate adversarial perturbations. In terms of performance, OPT was shown to be as efficient as many other state-of-the-art algorithms.\n\t\n\tDespite this, it requires performing a large number of queries, which can be detected by the victim's model if defense mechanisms are in place.\n\t\n\t\n\n  \nSubstitute Model attack\n\tThis method, used to perform adversarial attacks in a black box setting, was designed by Papernot et al. <cit.>. It allows extracting the architecture of the model, the decision boundaries, and its functionalities. The goal is to try to mimic the original model using several queries to obtain y = F^s(x), i.e., the given prediction of the original model must be equal to the prediction of the copied model. Once the model has similar behavior, state-of-the-art white-box attacks are used to generate adversarial examples. Using the transferability property, which is intrinsic to machine learning models, the original model can be fooled. This type of attack is less effective than white-box attacks but since it does not rely on gradient information, this attack is indeed feasible against non-differentiable models. In addition, it has been shown that the Substitution Model attack also defeats defenses based on gradient masking and defensive distillation.\n\t\n\t\n\t\n\n  \n(Wasserstein) Generative Adversarial Network (GAN/WGAN)\n\tGAN is an algorithmic architecture created by Goodfellow et al. <cit.> in 2014 and used to generate synthetic instances that resemble the original real instances. GAN uses two neural networks called discriminator and generator, both of which play a zero-sum adversarial game. The generator will try to create fake instances using a random normal distribution to fool the discriminator. The discriminator's goal is not to be fooled and to try to identify the false instances by learning from real instances contained in a dataset. As it goes along, the generator will try to learn to create more real instances. \n\t\n\t\n\tWGAN <cit.> uses a different method than GAN to compute the probability distance between the two distributions. Instead of using the Jensen-Shannon divergence <cit.>, which is based on the Kullback\u2013Leibler divergence <cit.>, it uses the Earth-Mover's distance <cit.>. The main advantages of WGAN are that it solves the vanishing gradient and mode collapse problems through more stable training. However, even if this mitigates the stability problem, the attack remains unpredictable and therefore unstable.\n\t\n\t\n\n\u00a7 DEFENSE STRATEGIES\n\n\t\n\tThe following defenses are designed to mitigate the effect of adversarial examples, such as those generated by the adversarial attacks discussed in Section <ref>. One could divide defenses into two different types: proactive, where the idea is to prevent the model to be fooled by adversarial examples, and reactive, where the defense tries to detect adversarial examples during attacks. A list of the well-known defenses is in <ref>.\n\t\n\t\n\t\n\t\n\t\n\t\n\n \u00a7.\u00a7 Proactive defenses\n\n\t\n\t\n\n  \nAdversarial Training\n\tThe purpose of this defense proposed by Ian J. Goodfellow et al. <cit.> is to strengthen the model against adversarial attacks by taking them into account during the learning phase. This defense can be seen in two ways. One can either provide the adversarial examples directly to the model with the training data or incorporate them into the loss function of the model which acts as a regularizer. This kind of defense is easy to implement and can be used very well in the IDS domain.\n\t\n\tA variant, called Ensemble Adversarial Training <cit.> allows to improve robustness against attacks. The effectiveness of this defense can be improved by taking into account adversarial examples generated with different algorithms rather than using only one. If the model is trained with adversarial examples generated based on some adversarial attacks, an attacker could use other attacks to fool the classifier by exploiting the lack of generalization.\n\t\n\tAlthough the models become more robust to adversarial examples, they are not completely immune because some adversarial examples may go undetected. Moreover, this defense is limited by a trade-off between robustness and accuracy, as the more, the model is trained with adversarial examples, the more its overall performance decreases. Athalye et al. <cit.> also showed that, if the model is trained with adversarial examples generated using the L_\u221e norm, the model is less robust as compared to training based on other norms (L_0, L_1 and L_2)\n\t\n\t\n\n  \nObfuscated Gradients\n\tThis technique is based on a gradient masking method so as to disrupt the descent of the gradient and, in this way, prevent gradient-based attacks from being able to successfully exploit the gradient by trying to make the model non-differentiable. \n\t\n\tGradient masking was broken by several attacks. One of them is in a white-box setting by using a random step and then switching to a gradient-based algorithm like FGSM for example. It was also broken in the black-box setting via transferability, which ensures the effectiveness of adversarial examples against other models than the one on which they were generated. Papernot et al. <cit.> show in particular that black box attacks are more effective than white box attacks when gradient masking defense is used. \tFurthermore, Athaye et al. <cit.> have shown how to bypass three types of obfuscated gradients, namely: shattering gradients, stochastic gradients, and vanishing/exploding gradients.\n\t\n\tIt can be noted that this defense could be used in the IDS domain but may not be as effective since, in theory, an attacker has limited knowledge of the defender's model, which limits the possibility of using white box attacks directly.\n\t\n\t\n\t\n\t\n\n  \nDefensive Distillation\n\t\n\tInitially used to reduce the dimensionality of DNN, a defense based on distillation is proposed by Papernot et al. <cit.> defensive distillation aims to smooth the decision surface of the model. The distillation method uses two neural network models. An initial network, taking as input the training data and the corresponding labels. The model provides predictions in a probability vector and transfers this knowledge to the second network, called the distilled network. This network, therefore, takes the same training data as the initial network, but the corresponding labels are taken from the probability vector of the initial network, then new predictions are made. The authors showed that defensive distillation is less sensitive to small perturbations.\n\t\n\tThis defense could be used in the IDS domain, however, Carlini and Wagner <cit.> have shown that this defense is broken by their attack. Therefore, it is not wise to use defenses that have already been shown to be broken when protecting a model. \n\t\n\t\n\t\n\n  \nFeature Squeezing\n\tThis defense technique, proposed by Weilin Xu et al. <cit.>, compresses the features of the instances and classifies them. Then a comparison is made between this classification and the classification of the original samples. If the results are different, then the instance is considered adversarial. Of the compression methods used by the author, such as bit depth compression, median smoothing, or non-local means, none consistently gives the best results, all need to be evaluated collectively as performance differs depending on the dataset used. \n\t\n\tThis defense is not suitable for the IDS use case because network traffic is often represented in tabular form and these compression techniques result in significant information loss for the underlying data.\n\t\n\t\n\n  \nEnsemble Defense\n\tThis technique is expected to be effective by assuming that several different defense techniques improve the robustness of the model. Such a defense can be either proactive or reactive, or a mixture of both, in case the different defense mechanisms used are both reactive and proactive. It can therefore be used in the IDS domain and is potentially effective against various types of attacks.\n\t\n\tThis technique is ineffective because an attacker can exploit any of the defenses' flaws to bypass them all. In addition, Warren He et al. <cit.> demonstrated that employing multiple weak defenses does not result in a stronger defense.\n\t\n\t\n\n  \nFeature Removal\n\t\n\tThis defense consists of identifying the most vulnerable features and removing them from the data used to train the model. These vulnerabilities often come from the complexity of the model with high dimensions. This defense will therefore reduce the complexity of learning the model and remove some dimensions that are too vulnerable to escape the attack. The removal of features will reduce the attack surface by reducing the possible vectors that can be perturbed.\n\t\n\tHowever, the work of Apruzzese et al. <cit.> shows that feature removal decreases the performance of an IDS. It results in a loss of precision that increases the number of false positives.\n\t\n\t\n\n \u00a7.\u00a7 Reactive defenses\n\n\t\n\t\n\n  \nAdversarial Detection \n\t\n\tThis attack mechanism uses different methods to detect adversarial examples, based on statistical tools such as principal component analysis (PCA), distributions, and normalization. \n\t\n\tOne example of this method is the defense proposed by Feinman et al. <cit.> which is based on two techniques: density estimates and Bayesian uncertainty estimates. The general idea of the first method is to check whether the density estimates of the last hidden layer for an input instance are significantly different from those associated with the training set containing the benign examples and, if so, the instance will be considered adversarial. Density estimates are made in the feature space of the last hidden layer because it is considered more linear than that of the input. Bayesian uncertainty estimates can be used to overcome situations where density estimates cannot detect adversarial examples. This method allows detection in low-confidence regions in the input space.\n\t\n\tThe main advantage of this reactive defense is that it does not change the initial accuracy of the model. It could also be used in the IDS domain because it has no particular restrictions. However, it has two main problems: The first one is that it provides many false positives, which makes it less effective. The second problem is that most of these detection methods have been proven to be broken by Carlini and Wagner <cit.>. Tianyu et al. <cit.> proposed a more robust alternative to detect adversarial examples using kernel density estimates and the reverse cross-entropy training procedure.\n\t\n\t\n\t\n\n  \nAdversarial Query Detection\n\tIntroduced in the Titi-taka framework proposed by Zhang et al. <cit.>, this defense consists of detecting the number of abnormal queries that signal that an attack is being conducted. This defense reduces the number of possible queries sent to the model, making it more difficult to exploit for attacks using a large number of queries, while maintaining the initial accuracy. The main drawback is that this defense is only effective against black-box attacks that use large numbers of queries.\n\t\n\t\n\t\n\n\u00a7 ADVERSARIAL ATTACKS AGAINST IDS\n\n\t\n\tIn recent years, many researchers have been interested in the implications of adversarial learning in the context of IDS by proposing several studies. We have selected several of them for their contributory aspect, and more precisely their contribution in terms of feasibility, a very important topic for IDS. These contributions are all listed in <ref>. \n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\tIn 2017, Maria Rigaki et al. <cit.> demonstrated in their work that adversarial examples can be generated by adversarial algorithms to fool an IDS using a DNN trained on the NSL-KDD dataset. They showed that not all algorithms are suitable for fooling an IDS and pointed out the fact that FGSM is incompatible with this goal, but JSMA may be suitable. Furthermore, they showed that adversarial examples are capable of transferring to several machine learning models such as Decision Trees, Random Forests, Linear SVM, and Ensemble Voting. In addition, they showed that creating a feature-based adversarial instance requires knowing the mapping between features and network traffic and how the data is preprocessed because, unlike images, features extracted from network traffic are highly correlated. \n\t\n\t\n\t\n\tZilong et al. <cit.> proposed to study the effectiveness of adversarial examples with a WGAN using the NSL-KDD dataset. The performances of several classifiers were studied, namely decision tree, Random Forest, SVM, MLP, Naive Bayes, logistic regression, and KNN. The results showed that adversarial examples can fool all trained classifiers. They also showed that the attack remains effective even when using a limited feature space.\n\t\n\t\n\tWarzy\u0144ski and Ko\u0142aczek <cit.> proposed a study on using FGSM to generate adversarial examples to fool a neural network-based classifier trained with the NSL-KDD dataset. The results showed that this attack is effective. It may be noted that this study does not address the feasibility of adversarial attacks in a realistic scenario and is limited to the study of a particular dataset and a particular attack. \n\t\n\t\n\tZheng Wang <cit.> provides an in-depth analysis of the NSL-KDD dataset by investigating feature importance when generating adversarial examples against a multilayer perceptron (MLP) based IDS. He showed that the feasibility of adversarial attacks against IDSs is different from that of image classifiers by illustrating that not all adversarial algorithms are suitable for creating adversarial attacks against IDSs. Among these algorithms, JSMA seems to be the most suitable as it does not modify all features to create adversarial examples but only those it considers most important. This is particularly relevant since adversaries are usually limited in their ability to manipulate features due to restricted access or the complexity of manipulating them all at once. Feature importance shows that some features are more vulnerable than others in that they are more often selected by the algorithm during adversarial examples generation.\n\t\n\tYang et al. <cit.> proposed a more realistic approach to the problem by using three black-box attacks that assume no knowledge of the target model information. These three attacks are WGAN, ZOO, and Substitute Model. To analyze their performance, they took 5 classifiers, namely Random Forest, SVM, MLP, and Naive Bayes trained with the NSL-KDD dataset. The results showed that ZOO was the most efficient, the Substitute Model was the least efficient while WGAN provided good performance but was unstable due to its intrinsic properties. However, the paper does not discuss the feasibility of these attacks in real-world settings besides the black-box perspective.\n\t\n\t\n\tInstead of using state-of-the-art attacks, Apruzzese et al. <cit.> proposed an attack that iteratively produces manually defined perturbations. They studied the performance of this attack on a Random Forest classifier trained on the CTU-13 dataset which is based on a collection of Botnet attacks. This attack follows a fairly simple strategy, it clusters specific features and applies an iterative perturbation. The features are not chosen trivially, they are the ones that are easiest to manipulate, namely time, packet size, and the number of packets. The results showed that changing only a few features can lead to a decrease in classifier performance. From a feasibility point of view, this work is interesting because the modified features are at most four, and chosen in advance, which can be adjusted to modify only those features we have access to. \n\t\n\tTo generate adversarial examples, Martins et al. <cit.> chose to work on the NSL-KDD dataset and a more realistic and recent dataset, namely CIC-IDS 2017. They also used adversarial training, first described in the image classification literature, to see if it could be applied to the IDS domain. The results showed that among the attacks used, JSMA is the least effective but disrupts the fewest features. They also showed that Adversarial Training improves the overall robustness of all classifiers, namely Decision Tree, Random Forest, Naive Bayes, SVM, DNN, and Denoising Autoencoder. Nevertheless, we can note that the feasibility of adversarial attacks has not been addressed in this work, except for the use of a more realistic dataset.\n\t\n\t\n\tJoseph et al. <cit.> used Kitsune's classifier called KitNET, and its dataset, to evaluate its robustness to adversarial examples generated based on four attacks (FGSM, JSMA, C&W, EAD) using different norms (L_0, L_1, L_2, L_\u221e). In a white box setting, the results showed that the classifier was vulnerable to all four attacks. \n\t\n\tThe study provided by Olakunle et al. <cit.> analyzed the impact of adversarial examples generated with FGSM, BIM, and PGD against two DNNs trained with the Bot-IoT dataset containing network attacks such as DOS or DDOS. The results showed that these adversarial attacks performed well in the IoT domain. In addition, they proposed feature normalization as a defense mechanism. The results showed that this defense was not effective as it increased the accuracy of the classifier however it also made the model more vulnerable to adversarial examples.\n\t\n\tAccording to Apruzzese et al. <cit.> on evaluating the effectiveness of adversarial attacks against botnet detectors, their results show that it is possible to fool this type of NIDS detector based on machine learning algorithms. They found that all the machine learning algorithms studied in this paper, namely Random Forest, Decision Trees, AdaBoost, Multi-Layer Perceptron, K-Nearest Neighbor, Gradient Boosting, Linear Regression, Support Vector Machines, Naive Bayes, ExtraTrees, Bagging, and Stochastic Gradient Descent Linear Classifier are susceptible to be fooled. In this study, the experiments are conducted from a more realistic perspective by taking into account some important domain constraints and assuming the gray box parameters, and using known realistic datasets containing botnet attacks to train their models. These datasets are as follows: CTU-13, IDS2017, CIC-IDS2018, and UNB-CA Botnet. To generate adversarial examples, the authors manually add small perturbations to a maximum of 4 features of each malicious instance keeping a realistic perspective. These features are duration, sent bytes, received bytes, and exchanged packets. Furthermore, they showed that using the defense called \"feature removal\" (see Section <ref>) does not guarantee robust protection for botnet detectors.\n\t\n\tHashemi et al. <cit.> propose a bottom-up approach by first analyzing the characteristics of the IDS datasets to understand their domain constraints. Once these constraints were identified, they showed that it is possible to fool different IDS models (Kitsune, DAGMM, and BiGAN) trained on the Kitsune and CICIDS2017 datasets, respectively, under these domain constraints for both packet-based and flow-based IDSs. To approach the problem more realistically, they proposed two algorithms for each of these network traffic types. For packets, the algorithm is divided into three parts: one function that generates delays between packets, another one that splits packets to have more packets, and the last one used to generate new packets. For flows, the algorithm uses a system of groups, only one of which can be modified and on which the attacker can apply perturbations. Recent work by Teuffenbach et al. <cit.> building on this work, also uses this grouping method to modify only relevant features. However, these groupings are slightly different from those proposed by Hashemi. Their results also showed that their method, involving domain constraints directly in their optimization problem, was effective in fooling the models (DNN, DBN, and AE) trained on CIC-IDS2017 and NSL-KDD.\n\t\n\t\n\t\n\tIn their paper, Aiken et al. <cit.> investigated the effectiveness of a novel adversarial example generation method focused on a SYN Flood DDoS attack. The results showed that the proposed algorithm is effective in fooling the classifiers (Random Forest, SVM, Logistic Regression, and KNN) trained on the SYN Flood attack present in the CIC-IDS 2017 dataset. The accuracy of the model fell to 0% using the proposed algorithm. However, some characteristics of the instances are manipulated when they are not supposed to be since they are not easily modified in reality, such as the traffic from the victim. \n\t\n\tSheatsley et al. <cit.> study showed that, even when several NIDS-related domain constraints are considered, limiting the number of features that can be modified, it is possible to create realistic adversarial examples capable of fooling attack detectors using the AJSMA (Adapted JSMA) and HSG (Histogram Sketch Generation) adversarial algorithms. The experiments were conducted on the NSL-KDD and UNSW-NB15 datasets. They showed that domains with more restrictive constraints, such as NIDS, are no more robust than those with fewer constraints, such as image recognition. They also showed that these attacks were effective because of their transferability.\n\t\n\tJiming Chen et al. <cit.> studied the impact of adversarial examples on the domain of Industrial Control Systems (ICS). They took a more realistic approach by limiting their knowledge of the models used by the defender. They reproduced an ICS system to create a realistic environment and trained their MLP model directly on the extracted traffic. They then used two attack algorithms, GAN and OPT, to produce adversarial instances while taking into account domain constraints such as constraints related to the protocols used during the attacks. Their results showed that the ICS domain was also vulnerable to adversarial examples. In this study, several models were tested, namely, Random Forest, OCSVM (One-Class SVM), DNN, Stacking Model, and Naive Bayes. They proposed to use adversarial training and they found that this defense improved the robustness of the models studied. \n\t\n\tIn their paper, Sadeghzadeh et al. <cit.> proposed a problem-based approach as opposed to a feature-based approach <cit.>. The authors used three new attacks to manipulate network traffic called Adversarial Pad (AdvPad) which adds the perturbation to the packet, Adversarial Payload (AdvPay) which adds perturbation to the payload and Adversarial Burst (AdvBurst) which adds newly crafted packets. They propose to manipulate traffic concerning different types of services such as VoIP, mail protocols, file transfer, or P2P present in the ISCXVPN2016 dataset. The results showed that the classifier used (CNN) decreased its robustness due to the use of the three attacks. \n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\tHan et al. <cit.> proposed a novel attack using a GAN and a Particle Swarm Optimization (PSO) technique to directly manipulate the traffic under a black-box assumption. This approach is more realistic because the feature space is not easily reversible in the IDS domain, which means that the change in feature value cannot be transferred directly into the network traffic due to the numerous domain constraints. The attack process is divided into two parts, the GAN is used to generate adversarial features first, and then the PSO is used to add mutations to the malicious traffic. To evaluate the effectiveness of their attack, the authors used three new metrics to test the evasion effectiveness, namely the Detection Evasion Rate (DER), the Malicious Traffic Evasion Rate (MER), and the Probability Drop Rate (PDR) of the malicious traffic. They also proposed a new metric to provide an interpretability indicator called Malicious features Mimicry Rate (MMR) which provides a measure of how far the adversarial features are from the malicious features during mutation. The results showed that the attack was able to fool packet-based IDSs trained with the Kitsune dataset, as well as flow-based IDSs trained with the CIC-IDS2017 dataset.  However, the effectiveness of the attack varies depending on the knowledge of the extracted features. If the substitute model does not know any of the features extracted by the extractor, it will still be able to generate adversarial traffic capable of fooling the classifiers by randomly choosing less effective features. Despite the black-box assumption, the authors assumed that the extractor used by the IDS is known, which is not always the case in real settings.\n\t\n\tZhang et al. <cit.> proposed a new version of their original paper <cit.>. Their goal was to evaluate their framework's performance in terms of improving IDS protection against evasion attacks subject to limited defense knowledge. In particular, they used multiple decision-based black-box algorithms to provide a more realistic representation of the problem. The results showed that the classifiers, using MLP, CNN, and C-LSTM, trained on the CIC-IDS2018 dataset were all vulnerable to the used black-box algorithms, namely NES, Boundary, Pointwise, HopSkipJumpAttack, and Opt-Attack. To improve the robustness of the classifiers, the authors propose to use their Tiki-Taka framework combining several defenses, Adversarial Training, Ensemble Voting, and Adversarial Query Detection. Thus, the authors combined two proactive defenses and a reactive one respectively. The results indicate that this combination of defenses is effective against the attacks studied on the CIC-IDS2017 and CIC-IDS-2018 datasets. \n\t\n\tIn their paper, Mohamed Amine Merzouk et al. <cit.> provide an in-depth analysis of the feasibility of state-of-the-art attacks against an IDS trained with NSL-KDD. The adversarial algorithms studied are FGSM, BIM, DeepFool, C&W, and JSMA. They showed that these adversarial algorithms produce invalid Adversarial Examples (AEs) if applied directly without taking domain constraints. For example, some of the generated values were negative or out of bounds, exceeding their feasible limit. These AEs must meet certain criteria to be valid. In particular, they show four constraints, namely out-of-range values, non-binary values, membership in multiple categories, and semantic links.\n\t\n\tDebicha et al. <cit.> proposed an adversarial detector design based on transfer learning. They evaluated the effectiveness of using multiple strategically placed adversarial detectors versus a single adversarial detector for intrusion detection systems. Their experiments were conducted on two IDS architectures: a serial architecture and a Kitsune-inspired parallel architecture. They chose four evasion attacks to generate adversarial traffic, namely FGSM, PGD, DeepFool, and C&W. Although the attacks are feature-based and not traffic-based, the author has taken into account the domain constraints to make them more feasible. Their defense is based on the implementation of multiple adversarial detectors, each receiving a subset of the information passed by the IDS and using a suitable fusion rule to combine their respective decisions. Using this defense, they were able to improve the detection rate over adversarial training.\n\t\n\t\n\t\n\t\n\n\u00a7 FEASIBILITY OF THE EXISTING EVASION ATTACKS\n\n\t\n\tAfter reviewing all these papers, we can see that most of them do not consider the realistic aspect, or only in a limited way regardless of the domain, whether in an IoT context or a traditional enterprise network. Here are the realistic aspects that should be taken into account for future studies of the impact of adversarial attacks on intrusion detection systems:\n\t\n\tFirst, it seems that most attacks based on feature-space manipulation do not provide a sufficiently realistic approach, as features cannot be easily transcribed back into network traffic once extracted and modified. In addition, semantic and syntactic constraints restrict the modifications applied to any feature. When working on the feature space, it is necessary to ensure that the generated adversarial network traffic is valid. An additional step must be performed after adding adversarial perturbations to the feature space. This step is called problem-space projection, which is the projection of the adversarial example into the realistic problem space. The objective of this step is to ensure that the reverse feature-mapping is doable as shown in <cit.>.\n\t\n\t\n\tSecond, the attacker is not supposed to know the mapping of raw network traffic into features, or at least not all of them, nor the semantic or syntactic links that exist between these features. This means that the assumptions about the feature extractor used in the model learning pipeline must remain limited if a truly realistic scenario is to be performed for the attacker. In other words, the attacker's knowledge should be limited and assumptions of full knowledge of the IDS should be avoided. \n\t\n\tThird, some papers <cit.> have addressed the black box hypothesis using attacks such as Boundary, NES, OPT, or ZOO. In reality, these black box attacks are easily detected by simple defenses such as Query Detection. In addition to the explanation given in the previous two points, one cannot query the IDS like an oracle repeatedly because the attacker could easily reveal himself, in addition to the fact that IDSs are not designed to deliver feedback when queried.\n\t\n\t\n\t\n\t\n\n\u00a7 CONCLUSION\n\n\t\n\tCurrent research on the impact of using evasion attacks to bypass machine learning-based NIDS has shown that a slight perturbation can allow the attacker to circumvent detection. This of course raises a security concern, as the use of machine learning models is becoming more prevalent in the cybersecurity field. However, while it is theoretically possible to exploit these models, their exploitation is a bit different in a real-world setting. In this paper, we have reviewed the most recent attacks based on two possible adversarial strategies, namely the white-box and the black-box settings. We then explored some popular defense mechanisms. For each of the attacks and defenses, we elaborated on their suitability in the IDS domain. Finally, a set of related research papers are highlighted to clarify the feasibility of adversarial attacks in a more realistic context in the cybersecurity domain, specifically in the IDS domain. Concerning feasibility, we have provided several criticisms regarding recently published work by identifying their manipulation space. Thus, future research should focus on manipulating the traffic space by limiting the attacker's knowledge. We believe that this review has highlighted various points that may have been overlooked in some previous research, and that it will allow future research in this area to better address the various realistic constraints. \n\t\n\t\n\t\n"}