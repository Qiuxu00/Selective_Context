{"entry_id": "http://arxiv.org/abs/2303.06905v1", "published": "20230313074718", "title": "DEHRFormer: Real-time Transformer for Depth Estimation and Haze Removal from Varicolored Haze Scenes", "authors": ["Sixiang Chen", "Tian Ye", "Jun Shi", "Yun Liu", "JingXia Jiang", "Erkang Chen", "Peng Chen"], "primary_category": "cs.CV", "categories": ["cs.CV"], "text": "\n\n\n\nEffects of Nb Doping on the Charge-Density Wave and Electronic Correlations in the Kagome Metal Cs(V_1-xNb_x)_3Sb_5\n    Hai-Hu\u00a0Wen\n    March 30, 2023\n===================================================================================================================\n\n\n\n\n\n\n\nVaricolored haze caused by chromatic casts poses haze removal and depth estimation challenges. Recent learning-based depth estimation methods are mainly targeted at dehazing first and estimating depth subsequently from haze-free scenes. This way, the inner connections between colored haze and scene depth are lost. In this paper,\nwe propose a real-time transformer for simultaneous single image Depth Estimation and Haze\nRemoval (DEHRFormer). DEHRFormer consists of a single encoder and two task-specific decoders. The transformer decoders with learnable queries are designed to decode coupling features from the task-agnostic encoder and project them into clean image and depth map, respectively. In addition, we introduce a novel learning paradigm that utilizes contrastive learning and domain consistency learning to tackle weak-generalization problem for real-world dehazing, while predicting the same depth map from the same scene with varicolored haze. Experiments demonstrate that DEHRFormer achieves significant performance improvement across diverse varicolored haze scenes over previous depth estimation networks and dehazing approaches.\n\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nWith the development of deep learning technology, the computer vision community has entered a prosperous era\u00a0<cit.>.\n\n\nLow-level vision tasks are further developed as deep learning advances\u00a0<cit.>.\nHaze, as a common weather phenomenon, would result in severe visibility degradation, which also seriously harms high-level vision tasks, such as object detection, depth estimation, etc.\nTherefore, single image dehazing, as a long-standing low-level vision task, the haze effect can be formulated by the following well-known atmosphere scattering model mathematically:\n\n    \ud835\udc08(\ud835\udc31)=\ud835\udc09(\ud835\udc31) t(\ud835\udc31)+\ud835\udc00(\ud835\udc31)(1-t(\ud835\udc31)),\n\nwhere \ud835\udc08(\ud835\udc31) is the observed hazy image, \ud835\udc09(\ud835\udc31) is the clean one, t(\ud835\udc31) is the transmission map and \ud835\udc00(\ud835\udc31) stands for the global atmospheric light. Single image dehazing is a classical ill-posed problem, due to uncertain parameters: t(\ud835\udc31) and A(\ud835\udc31), while the transmission map t(\ud835\udc31) is a parameter associated with depth:\n\n    t(\ud835\udc31) = e^-\u03b2 d(x),\n\nwhere \u03b2 is the scattering coefficient of the atmosphere and d(x) is the depth map.\nAs previously mentioned, due to the influence of the scattering particles in the atmosphere, existing widely-employed depth sensors, like LiDAR or Kinect, etc, are not reliable and robust in haze scenes.\n\nVaricolored haze, as a more challenge ill-posed problem, offers diverse hazy conditions with vary colors. To the best of our knowledge, the varicolored haze removal is a less-touched topic in the vision community, but it is worth exploring for many applications. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this work, we present a new task: jointly perform depth estimation and haze removal from varicolored haze scenes.\nTo handle this new task, we present a novel end-to-end transformer, namely DEHRFormer, perform Depth Estimation and Haze Removal by a unified model. Our DEHRFormer aims to tackle several long-standing but less-touch problems as follows: (i) Varicolored haze scenes. Compared with common haze scenes, varicolored haze is a larger collection of haze conditions with more challenging degradations. However, most existing dehazing manners often meet difficulties when trying to handle it\u00a0<cit.> (ii) Domain gap problem for varicolored dehazing. The domain gap between real and synthetic varicolored haze domains makes networks only supervised by synthetic data hard to generalize well for the real varicolored haze images. Previous arts usually utilize complex image translation paradigm\u00a0<cit.> or unsupervised learning based on hand-craft priors to bridge it\u00a0<cit.>, which are inefficient and unstable when training. (iii) Domain consistency problem for varicolored image depth estimation. Estimating depth maps from haze scenes is an existing topic, but previous manners\u00a0<cit.> ignore the domain consistency between clean and hazy domains, which means different depth maps may be produced from the same scene with or without haze.\n\n\nFor challenging varicolored haze scenes, we introduce haze type queries in the transformer-based dehazing decoder to learn diverse varicolored haze degradations, which employ multiple head self-attention mechanism to match learnable haze queries with sample-wise degraded features, to project degraded features into the clean feature space.\nIn depth decoder, we further employ learnable queries as the medium to effectively capture depth information from clean features. For domain consistency problem, we present domain consistency learning to maintain the consistency of depth maps over haze and clean scenes. For domain gap problem, we introduce a novel semi-supervised contrastive learning paradigm, which explicitly exploits the knowledge from real-negative samples to boost generalization of DEHRFormer on real varicolored scenes. Moreover, we propose the first varicolored haze scene depth estimation dataset, which consists of 8000 paired data for varicolored haze removal and depth estimation tasks. We summarize the contributions as follows:\n\n    \n  * This work focuses on a novel and practical tasks: varicolored image dehazing and depth estimation. Compared with prior arts which only consider common haze removal (grayish haze scenes) or depth estimation from clean scenes, we are the first to joint consider dehazing and estimating depth maps from varicolored haze scenes in a unified way.\n    \n    \n  * We propose a real-time transformer for depth estimation and haze removal, which unifies the challenging varicolored haze removal and depth estimation to a sequence-to-sequence translation task with learnable queries, significantly easing the task pipeline.\n    \n    \n  * A semi-supervised learning paradigm is proposed to boost the generalization of DEHRFormer in the real haze domain. Furthermore, we considered the domain consistency of depth estimation over the haze and clean domains.\n    \n    \n    \n    \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 METHOD\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Coupling Learning Encoder\n\nIn our architecture, we first offer a Coupling Learning Encoder to capture features from degraded images. Different from previous approaches\u00a0<cit.>, in the encoder, we apply a CNNs-based encoder to extract coupling features of dehazing and depth estimation, which is the basis for achieving real-time efficiency for inference due to its O(N) complexity compared to O(N^2) computational complexity of self-attention.\n\nInspired from NAFNet\u00a0<cit.>, the high-dimension space is crucial for extracting features. Nevertheless, it only adopts the simple 3\u00d73 depth-wise convolution to perform modeling. To enhance the coupling learning ability of features for the encoder, we propose Multi-scale Feature Modeling block (MSFM), in which multi-scale convolution is presented in high-dimension space to boost the performance of excavating coupling features of haze removal and depth estimation. As shown in Fig.<ref>, given an input feature X_i^e, our Coupling Learning Encoder can be expressed as:\n\n\n\n\n    X_i+1^e=MSFM^N(X_i^e \u2208\u211d^\u03bc/2^i\u00d7w/2^i\u00d7 C_i\u2193),\n\nwhere the e denotes the encoder, X_i indicates the feature of i-th layer encoder. H and W mean the height and width of input image. \u2193 is the down-sampling operation, we perform overlapped patch merging as follows <cit.>. There are four stages in our coupling encoder.\n\n\n\n \u00a7.\u00a7 Task-specific Decoder For Features Decoupling\n\n\n\nMotivated by DETR<cit.>, we attempt to use learnable queries to decode coupling features via a unified task-specific decoder. The decoding can be seen as sequence-to-sequence translation, which exploits learnable queries to translate the features from the coupling encoder. For the dehazing decoder, we aim to utilize the learnable haze queries Q_h as prototypes to study varicolored degradations. Specifically, the degradation-wise type queries Q_h are used to adaptively decode the varicolored information from encoder via a sequence-to-sequence manner. Given the coupling feature X_4^e\u2208\u211d^H/16\u00d7W/16\u00d7 C_4, we feed it into our Task-specific Decoder of dehazing and reshape it into 3d sequence X_4^s\u2208\u211d^N \u00d7 C_4, where N=H/16\u00d7W/16. We employ the linear layer to project X_4^s into Key (K) and Value (V). Therefore, the decoupling can be expressed as follows self-attention:\n\n    X_h^' =Softmax(\ud835\udc10_\ud835\udc21 \ud835\udc0a^T/\u221a(C_4)) \ud835\udc15,\n\nwhere X_h^' denotes the dehazing feature decoupled from the encoder. For the decoupling feature, we then use up-sampling layer to go back to the original resolution. We add Residual Block\u00a0<cit.> in each stage and have skip connections across each stage.\nFor the depth decoder, we devise the decoder to decouple the features into depth estimation space. The learnable depth queries X_d in this decoder are leveraged to decode the depth information for various scenes. Unlike the dehazing decoder, we utilize X_d to decouple the depth feature from the dehazing feature X_h^' instead of encoder feature X_4^e. This can promote the network to process depth estimation from the clean feature to some extent. Therefore, our unified Task-specific decoder is serial. The overall self-attention and recover original feature size are consistent with the method of dehazing decoder.\n\n\n\n\n \u00a7.\u00a7 Semi-supervised Contrastive Learning\n\n\n\nFor image restoration, conventional contrastive learning paradigm usually only exploit synthetic negative samples to boost model performance. However, real degraded samples are accessible for us, these manners ignore this point and only focus on how to boost the model performance of synthetic domain.\nFor bridging the gap between synthetic and real domain,\nwe introduce the semi-supervised contrastive learning:\n\n    !0.7cm\ud835\udc9e(v, v^+, v^-)= \n       -log[exp(\u03b4(v) \u00b7\u03b4(v^+) )/exp(\u03b4(v) \u00b7\u03b4(v^+) / )+\u2211_n=1^N exp(\u03b4(v) \u00b7\u03b4(v_r^-))]\n\nwhere \u03b4(v),\u03b4(v)_+,\u03b4(v)_- denote the anchor sample, positive sample, and negative sample, respectively. \u03b4(\u00b7) is the feature extraction operation by the VGG-19 network. And N denotes the total number of negative samples. Our contrastive learning loss is defined as:\n\n    \u2112_CR=\ud835\udc9e(J_syn, J_g t,{\u00ce_n}_n=1^N)\n\nwhere J_gt is the ground truth of the input image, J_syn is the output result of DEHRFormer and N is the total number of real-world varicolored haze images in a single batch.\n\nIn our semi-supervised paradigm, we exploit real-world hazy samples as negative samples, and predicated results as anchor samples. Different from previous contrastive learning manners, we leverage the set of real hazy images by all varicolored types as the negative samples. The positive sample guides our DEHRFormer to mine clean knowledge by the feature space, while real-negative samples enhance the discriminative knowledge of our network for diverse varicolored haze images. Our semi-supervised learning paradigm provides a lower bound to limit the output of DEHRFormer away from real-world negative samples, which enhances the generalization of our model on the real domain.\n\n\n\n\n\n \u00a7.\u00a7 Domain Consistency Learning\n\nPopular depth estimation networks that only learn from the clean domain usually meet failures in haze scenes. Vice versa, the model that only learns knowledge from the haze domain would meet the generalization decline problem in the clean domain, which is not obviously neglectable. To achieve the generalization consistency between both domains, we present an additional constraint, which enforces our DEHRFormer to predict the same depth maps from the same scenes with or without hazy degradations.\nLet's denote J_gt^d and J_clean^d as the ground-truth depth map and the depth map predicted from the clean scene by our network. The constraint to perform domain consistency learning can be introduced as follows:\n\n\n    \u2112_DC = D(J_gt^d,J_clean^d)\n\nwhere D(\u00b7) denotes the Norm-based function to measure distance. \n\n\n\n\n\n \u00a7.\u00a7 Loss Functions\n\nWe use Charbonnied loss\u00a0<cit.> as our reconstruction loss:\n\n    \u2112_char =1/N\u2211_i=1^N \u221a(X^i-Y^i^2+\u03f5^2),\n\nwhere X^i and Y^i denote the predicted results and corresponding ground-truth. The constant \u03f5 is empirically set to 1e^-3 for all experiments.\nOur overall loss functions can be formulated as follows:\n\n    \u2112 = \u03bb_1\u2112_char(J^d_gt,J_syn^d)+\u03bb_2\u2112_char(J_gt^h,J_syn^h) + \u03bb_3\u2112_CR + \u03bb_4\u2112_DC,\n\nwhere J^{d,h}_syn and J^{d,h}_gt are the estimated depth map and dehazing image, and ground-truth of depth map and haze image, respectively. \u03bb_1,\u03bb_2, \u03bb_3 and \u03bb_4 are set to 1, 1, 0.5 and 1 in our all experiments.\n\n\n\n\u00a7 EXPERIMENTS\n\n\nImplementation Details. \nWe implement our framework using PyTorch with a RTX3090 GPU. We train our model 200 epoch with the patch size of 256\u00d7256. We adopt Adam optimizer, its initial learning rate is set to 2\u00d7 10^-4, and we employ CyclicLR to adjust the learning rate. The initial momentum is set to 0.9 and 0.999. For data augmentation, we apply horizontal flipping and randomly rotate the image to 0,90,180,270 degrees.\n\n\nDatasets. \nTo facilitate the development of this task, we propose the first varicolored haze scene depth estimation\ndataset, which includes 8000 paired data, named varicolored haze removal and depth estimation (VHRDE) dataset. We utilize 6,000 paired haze image from VHRDE for training and 2,000 paired data from VHRDE for testing. For real-world varicolored hazy samples, we utilize 2,000 real hazy images from the URHI (Unannotated Real Hazy Images) dataset\u00a0<cit.> for semi-supervised training and 1,000 real hazy samples for testing by No-reference image quality assessment.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompared with SOTA Methods. \nWe conduct extensive experiments to demonstrate the superiority of our algorithm compared to previous SOTA dehazing methods and depth estimation methods. For varicolored haze removal, we compared with DCP\u00a0<cit.>, GDCP\u00a0<cit.>, PSD\u00a0<cit.>, SDDE<cit.>, PMNet\u00a0<cit.> and NAFNet\u00a0<cit.>. We retrain the DL-based model on our proposed varicolored haze training set and perform inference to ensure a fair comparison. We use PSNR and SSIM to compare the performance of dehazing quantitatively. We can observe that the proposed DEHRFormer achieves the best results on PSNR and SSIM metrics in Table.<ref>. Compared to the second best approach NAFNet\u00a0<cit.>, we exceed the 0.41dB and 0.1 on PSNR and SSIM. We also present the visual comparison with previous SOTA methods in Fig.<ref>. It can be seen that our method can remove the varicolored haze thoroughly, while the previous methods still have various residual haze. Also, as shown in Table.\u00a0<ref>, we employ the well-known no-reference image quality assessment indicator to highlight our merits in real-domain, i.e., NIMA\u00a0<cit.>, which predicts aesthetic qualities of images. Fig.<ref> presents the different dehazing results in real-world images, our method obtains the best results of removing all varicolored haze compared to other algorithms.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the depth estimation,  We use the most common depth estimation metrics\u00a0<cit.> to quantitatively measure the performance of our model in depth estimation, including root mean square error (RMSE), Abs relative error, and accuracy \u03b4_1, \u03b4_2, \u03b4_3\u00a0<cit.>. For a fairer and more diverse comparison, for the dehazing algorithms \u00a0<cit.>\u00a0<cit.>\u00a0<cit.>\u00a0<cit.>\u00a0<cit.>, we first perform the dehazing manner and then use the depth estimation model\u00a0<cit.> to acquire the depth map. For SDDE\u00a0<cit.> and our DEHRFormer, the depth map is directly obtained from the haze map. The quantitative metrics are presented in Table.<ref>. We found that our framework achieves the best results on five metrics. It is worth mentioning that the paradigm of dehazing first and then depth estimation does not perform well, due to the gap between the dehazing image and the clean image. This has a significant impact on clean-to-depth networks. Our method can explicitly extract the relationship between haze and depth and facilitate depth estimation directly from haze images. The visual comparison is presented in Fig.<ref>. It can be seen from Fig.<ref> that the predicted depth map better reflects the real structure scene, and the transition of details is smoother than the SOTA approaches. We show the inference time cost[Worth noting that we compare DEHRFormer with other Dehazing-DepthEstimation pipelines and single-stage manner, (i.e, SDDE) for a fair comparison. And the time reported in the table corresponds to the time taken by each model or pipeline feed forward an image of dimension 512\u00d7512 during the inference stage. We perform all inference testing on an RTX3090 GPU for a fair comparison. Notably, we utilize the torch.cuda.synchronize() API function to get accurate feed forward run-time.] in Table.<ref>. It can be seen that DEHRFormer attracts real-time performance in the inference stage and surpass the previous SDDE method or dehazing-to-depth approaches.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 ABLATION STUDY\n\n\n\nFor ablation studies, we follow the basic settings presented above and conduct experiments to demonstrate the effectiveness of the components of our proposed comprehensive manner. Next, we analyse the influence of each element individually. \n\n\nImprovements of Learnable Queries. This part aims to demonstrate the effectiveness of proposed learnable queries in the Task-specific Decoder. We present the results in Table.<ref>. We observe that learnable queries can facilitate the decoder adaptively decouples the information we need from the coupling encoder via sequence-to-sequence. In addition, we notice that the number of learnable queries also affects the performance of the decoupling decoder. \n\n\n\n\n\nBenefits of Semi-supervised Contrastive Learning. To boost the generalization of our model in the real-domain, we propose Semi-supervised Contrastive Learning. We tend to verify the gains of Contrastive Learning in this part.\n\nWe found that real-domain negative samples can enhance the generalization of our model. It is worth mentioning that although synthetic samples can slightly improve our metrics on synthetic datasets, the generalization ability on real-world datasets drops significantly. We also explored the effect of the ratio of positive and negative samples on model performance. We found that more negative samples can facilitate the model to exploit negative information, we only choose the 1:1 ratio to conduct the experiments due to the best trade-off between performance and graphics memory.\n\n\n\n\n\n\n\n \nEffectiveness of Domain Consistency Learning. To demonstrate the gain of proposed Domain Consistency Learning (DCL), we remove the domain consistency learning and observe the effect on estimating clean image depth maps directly from haze maps. From Table.<ref>, We believe that Domain Consistency Learning can maintain the consistency between the depth maps of haze and clean scenes. \n\n\n\n\n\n\n\u00a7 CONCLUSIONS\n\nIn this work, we propose a novel real-time transformer to tackle a new task: depth estimation and haze removal from varicolored haze scenes. Moreover, we present a semi-supervised contrastive learning paradigm for the domain gap problem to achieve domain adaptation in real-world haze scenes. To maintain depth estimation performance in clean scenes, we propose domain consistency learning to simultaneously enforce network learns from hazy and clean domains. Extensive experiments on synthetic and natural varicolored haze data demonstrate the superiority of our DEHRFormer. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIEEEbib\n\n\n"}