{"entry_id": "http://arxiv.org/abs/2303.07280v1", "published": "20230313165411", "title": "Vision-Language Models as Success Detectors", "authors": ["Yuqing Du", "Ksenia Konyushkova", "Misha Denil", "Akhil Raju", "Jessica Landon", "Felix Hill", "Nando de Freitas", "Serkan Cabi"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.LG"], "text": "\nAlign and Attend: Multimodal Summarization with Dual Contrastive Losses\n    Bo He^1Part of this work was done when Bo was an intern at Adobe Research., Jun Wang^1, Jielin Qiu^2, Trung Bui^3, Abhinav Shrivastava^1, Zhaowen Wang^3\n\n^1University of Maryland, College Park     ^2Carnegie Mellon University     ^3Adobe Research\n\n{bohe,abhinav}@cs.umd.edu, junwong@umd.edu, jielinq@andrew.cmu.edu, {bui,zhawang}@adobe.com\n\n    March 30, 2023\n========================================================================================================================================================================================================================================================================================================================================================\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\n\nBeing able to detect successful (i.e., preferred) behaviour is a crucial prerequisite for training intelligent agents. For example, a signal of successful behaviour is necessary as a reward for policy learning, or as an evaluation metric for identifying performant policies. As such, in this work we are  concerned with developing accurate and generalisable success detectors, which classify if a behaviour is successful or not. While it is possible to engineer success detectors in specific domains, such as games <cit.> or control tasks <cit.>, in most real-world tasks they can be challenging to define. Success detection in realistic settings can be difficult not only due to challenges with identifying the environment state (e.g., detecting a particular object configuration from pixels), but also due to ambiguities about what a successful state is (e.g., subjective goals, such as \u201cgenerate an entertaining story\u201d). \nOne possible approach for developing success detectors is through reward modelling with preference data <cit.>. However, the trained preference models are often accurate only for the fixed set of tasks and narrow environment conditions observed in the preference-annotated training data, and thus they require extensive labour-intensive annotations for better coverage. This presents a significant bottleneck, as we would like success detectors to be able to generalise broadly \u2013 for instance, once a success detector learns what \u201csuccessfully picking up a block\u201d looks like, it should be able to detect this behaviour even if the background or agent morphology changes thanks to a semantic understanding of \u201cpicking up a block\u201d.\n\nConsider success detection in robotic manipulation, where tasks are specified with language instructions and observations consist of images. We posit that generalisable success detection is useful for learning generalisable policies in this domain. Here, effective success detectors should generalise to task variations along two axes. Firstly, they should generalise to language variations in the task specification. For instance, a model that is trained on detecting success for the instruction \u201clift a rubber duck\u201d should also accurately measure success for \u201clift a toy duck object\u201d. Secondly, success detectors should generalise to visual variations. For example, if a camera moves or additional objects are introduced in the scene, the model should still reliably detect success on accomplishing a known task. Standard reward models are typically trained for fixed conditions and tasks, and are thus unable to generalise to such variations. As such, adapting success detectors to new conditions typically requires collecting a new annotated dataset and re-training the model.\n\nIn this work, we aim to train success detectors that are robust with respect to variations in both language specifications and perceptual conditions. To this end, we leverage large pretrained vision-language models (VLMs), such as Flamingo <cit.>, as a foundation for learning success detectors. We hypothesize that Flamingo's pretraining on vast amounts of diverse language and visual data will enable learning more robust success detectors. In particular, we show that the same simple approach of finetuning Flamingo with human annotations leads to generalisable success detection across vastly different domains.\n\nThis simple approach allows us to use a unified architecture and training scheme, where we require only 1) videos describing the world state, and 2) text describing the desired behaviour or task. We reframe the problem of success detection as a visual question answering (VQA) task and refer to this formulation as SuccessVQA (Figure <ref>). \n\n\n\nConcretely, we finetune Flamingo for success detection on three diverse domains: a simulated household <cit.>, real-world robotic manipulation, and in-the-wild egocentric human videos <cit.>. The universality of the SuccessVQA task formulation is instrumental in enabling use of the same training architecture in a wide range of tasks and environments. We demonstrate that the resulting success detectors are capable of zero-shot generalisation to unseen conditions (both in language and vision) where bespoke learned reward models fail. \n\n\n\n\u00a7 RELATED WORK\n\n\n\n\n  \nVision-Language Models (VLMs)  Multimodal vision-language models (VLMs) have shown remarkable success in recent years, where VLMs can serve as a foundation for various tasks using language, vision, or arbitrary combinations of modalities. VLMs can be trained with contrastive objectives <cit.> and/or generative objectives <cit.>. In this work we rely on the Flamingo model <cit.>, which leverages a contrastive objective for pretraining the vision encoder on text-and-image pairs. This is combined with a frozen pretrained language model though the Perceiver Resampler and interleaved cross-attention layers, and optimized with a generative objective. We approach success detection as a closed-form visual question answering (VQA) task. However, unlike other applications of VLMs in single-image VQA tasks <cit.>, we rely on videos to specify the world state, making our work more similar to video QA tasks <cit.>. While the original Flamingo work demonstrates capabilities on video understanding, we extend this approach to training video-based reward models. Variants of our approach (e.g., by reducing the video input to a single frame) can also be applied with other VLMs built on large language models <cit.>.\n\n\n\n\n\n  \nReward Modelling Reward modelling is often necessary when it is challenging to hard-code a reward function for an agent to learn from. To circumvent this, there has been a rich body of prior work on learning reward functions from data. When rewards are accessible through a simulator, one can use supervised learning to train a reward model for model-based agent learning <cit.>. However, many tasks can be difficult to simulate and hand-engineer simulated rewards for.  To overcome this challenge, one can learn reward models from human data. When demonstrations of desirable behaviour are available, one can leverage inverse reinforcement learning (IRL), where the key idea is to recover a reward function that best explains expert behaviour\u00a0<cit.>. However, IRL relies on access to such expert demonstrations, makes assumptions about the relationship between the expert actions and the true reward, and can be difficult to learn.\n\nWhen demonstrations are difficult to acquire, a more natural way of providing human feedback is through comparative preferences that indicate the degree to which certain agent behaviour is desirable. This can be done with comparisons of whole episodes\u00a0<cit.>, trajectory segments\u00a0<cit.>, or even synthesized hypothetical trajectories\u00a0<cit.>. These methods then fit a reward function as a preference-predictor, e.g., using a Bradley-Terry model <cit.>. Nevertheless, preferences are not always the most natural form of feedback from humans, and in many cases we would like the exploit the goal-oriented nature of many tasks we care about. In other words, sometimes it can be easier for a person to provide direct success labels or scalar rewards with respect to a given goal. This can be done online in response to observed agent actions and state transitions <cit.>. In robotics, proposed methods vary from sparse, single frame annotations <cit.> to dense, whole trajectory annotations <cit.>. In this work we learn from reward annotations, focusing on training success detectors which can be viewed as binary reward functions. Since collecting human annotations for each new task and environment can be expensive, we aim to study whether pretrained, large VLMs can enable learning more generalisable success detectors from human annotations.\n\n\nLarge-Scale Pretraining for Success Detectors\nOur work falls under the general category of using foundation models as reward models. In language modelling, reward models are typically trained by finetuning a pretrained LLM with human preferences over LLM generations. This reward model can then be used to finetune an LLM with filtered supervised finetuning or reinforcement learning from human feedback (RLHF) <cit.>. For embodied agents, large-scale datasets of in-the-wild human videos have been used to train reward models <cit.>. Rather than using human reward annotations of agent behaviours, these methods rely on task-annotated human videos of successful behaviours.  Most similar to our work, some prior approaches propose using contrastive VLMs as reward models. In simulated robot domains, <cit.> propose using CLIP <cit.> to generate task rewards from a text-based goal description and pixel observations. <cit.> leverage large-scale Minecraft data to finetune a Minecraft-specific video CLIP model for detecting alignment (i.e., reward) with text task descriptions. Our work differs in that we leverage a generative VLM built on a frozen large language model, which we hypothesize enables better language generalisation. We also apply our method to three vastly different domains, including real-world domains where ground truth rewards are difficult to obtain, and thus directly make use of human annotations.\n\n\n\n\n\u00a7 SUCCESSVQA: SUCCESS DETECTION AS A VQA TASK\n\n\nOur primary contribution is SuccessVQA, a framework that allows us to train multi-task success detectors by directly leveraging powerful pretrained VLMs, such as Flamingo. In SuccessVQA, the VLM is given a visual input representing the state of the world (e.g., a single image or a short video clip) and a question asking if the specified task is successfully accomplished. This problem formulation has several advantages:\n\n    \n  * It allows us to unify success detection across domains, using the same architecture and training scheme. We consider three domains: a simulated 3D playroom used in prior research on language-conditioned interactive agents  ()\u00a0<cit.>, real robotic manipulation, and \u201cin-the-wild\" human videos from Ego4D\u00a0 <cit.>.\n    \n  * Relying on a pretrained vision-language model enables us to harness the advantages of pretraining on a large multimodal dataset. We hypothesize that this is the reason for better generalisation to both language and visual variations.\n    \n  * The task and state specification allows us to unify treatment of success detection in tasks defined either by singular successful states or target behaviours (i.e., detecting success requires reasoning across multiple frames).\n\n\n\n\n  \nSuccessVQA Datasets\n\nTo create the SuccessVQA datasets, we use behaviour trajectories annotated by humans to indicate  whether a task is completed successfully, and if so, when a success occurs. There may be multiple annotations per trajectory from different human raters. In the cases where raters disagree, success or failure is determined by a majority vote, and the median (across the raters who annotated success) of the first annotated success frame is used as the 'point of success'. All subsequent frames are also successful, unless the task is reversed (e.g. removing a gear after inserting it for the robotics domain). To generate SuccessVQA examples, a trajectory is split into non-overlapping subsequences (Figure <ref>). For simplicity, we make the clip lengths the same as the pretraining clip lengths used for Flamingo: by first creating subsequences of length 211 frames, then downsampling from 30 FPS to 1 FPS to create 8-frame subsequences. We then generate the VQA question using one of two methods. When trajectories correspond to some known task, we use the template: , for example,  (see Figure <ref>, first and second rows). When no task is provided but there is a narration corresponding the  actions in the clip, as in Ego4D, we use a frozen Flamingo model to rephrase the narrations into questions. For example, given a narration , we convert it to the question  (see Figure <ref>, last row). Finally, the answer is generated:  if the given subsequence ends in success frames, and  otherwise. \n\n\n\n  \nTraining and Evaluation\n\n\n \nWe finetune the Flamingo (3B) vision-language model on the SuccessVQA dataset for each domain. Specifically, we finetune all the vision layers (vision encoder, perceiver, and cross attention layers) and keep the language layers frozen. In the experiments we refer to this model as the . For evaluation we compute clip-level success detection accuracy against the ground truth human annotations on held-out trajectories. In the simulated household and robotics domains (Sections <ref> and <ref>) we also compute episode-level accuracy to directly compare against baseline bespoke success detection models, denoted . Note that these baselines were hand-designed independently and tuned specifically for each domain. While these models differ from Flamingo in both pretraining schemes and architecture, they represent a best attempt at designing an accurate reward model for in-distribution evaluations. Episode-level success detection is computed as follows: first, we generate subsequences from the test trajectories in the same way as during training. Next, the success detection model classifies each clip individually for success, as illustrated in Figure <ref>. We consolidate the classifications in one of two ways. 1) When the success is completely defined by the observed environment state (as in the robotics tasks), we only look at the first and the last clip of an episode. Then, the entire episode as successful if the first clip is in a failure state and the last clip is in a success state. 2) When the success is defined by a particular behaviour (as in the simulated household domain), if any subsequence in an episode is classified as success we classify the episode as successful. We report balanced accuracy on the test episodes, as there can be a large imbalance between the number of successful and failure episodes in the dataset. A random model would achieve 50% balanced accuracy.\n\n\n\n  \nExperiments overview\n\nWe use the SuccessVQA problem formulation to train success detectors across a diverse range of tasks in vastly different domains: simulated household or  (Section <ref>), robotics (Section <ref>), and Ego4D videos (Section <ref>). We investigate whether Flamingo as a success detector model backbone enables generalisation across the following axes:\n\n    \n  * language generalisation (Section <ref>). Can we accurately detect success for novel tasks specified with language? To answer this question, we evaluate generalisation to unseen tasks specified with language. For example, if we train on detecting success for the task , can we accurately detect success for the task ? For these experiments, we use simulated tasks in the  environment where the trajectory dataset contains a large and diverse set of language-specified tasks.\n    \n  * visual robustness (Section <ref>). Can we detect success in the presence of unseen visual variations? To answer this question, we evaluate success detection accuracy for a known semantic task, but in the presence of naturalistic visual perturbations. In these experiments, we use real-world robotic manipulation tasks where we introduce visual variations at test-time using different camera viewpoints and distractor objects.\n\nWe compare our model against bespoke evaluation models designed and trained specifically for each domain. We do not necessarily expect the Flamingo-based models to outperform the bespoke models in a given in-distribution scenario. Rather, we aim to investigate whether the Flamingo-based models have better robustness to both aforementioned language and visual changes, while also not requiring any domain-specific architectural or training changes. We emphasize that the benefit of SuccessVQA is the simple task formulation that can be applied across a wide range of domains and is directly amenable for use with large pretrained VLMs. Finally, in Section <ref> we show an example of an in-the-wild SuccessVQA dataset derived from Ego4D <cit.>. Initial results for success detection in this domain are promising, and we hope to encourage further work on accurate reward modelling in unstructured real-world settings.\n\n\n\u00a7 LANGUAGE ROBUSTNESS WITH INTERACTIVE AGENTS ()\n \n\nIn this section we train and evaluate success detectors in the simulated  environment, a diverse 3D house environment designed for training language-conditioned interactive agents <cit.>. The environment consists of \u201ca randomised set of rooms, with children\u2019s toys and domestic objects, as well as containers, shelves, furniture, windows, and doors\" (see Figure 1 in <cit.>). The tasks are generated from human-human interactions in the , where a setter is instructed to provide a task via language for a solver, e.g., . Success detectors in this environment can serve as automated evaluators for trained policies.\n\nThere are two properties in this environment that are particularly challenging for automated success detection: large language variety and the environment's multi-task nature. Large language variations are present because the tasks were originally generated from human interactions, and people are likely to use diverse language to specify even semantically similar tasks. For example, the task of bringing an object to the setter can be phrased in many ways: , , . Moreover, success detection in this environment is intrinsically multi-task in its nature because: (1) there is a vast set of possible tasks that can be specified with different utterances, and (2) the behaviour of different people and trained agents can vary greatly for the same task. For automated evaluation, it is not scalable to train a new model for each language and task variation. \n\n\n\n \u00a7.\u00a7 Methodology\n\n\n\n\n  \nTraining Dataset\nWe use tasks and trajectories from the Standardized Test Suite (STS), designed specifically for evaluating learned Interactive Agents <cit.>. We focus on the movement-based tasks: tasks that require the solver agent to move around and interact with the environment. The STS consists of a set of \"scenarios that typify the behaviour [the Interactive Agents team] wishes to evaluate\" <cit.>, and various trained agent policies are tasked with accomplishing the given scenarios. These test episodes are then annotated by human raters to indicate if a task is successfully completed and if so, at which frame success occurred. We use these annotations to create a SuccessVQA dataset for  finetuning and to train a  model for comparison. The training set consists of STS and human interaction data collected between September 2021 to April 2022 (see Figure <ref>), 546,887 trajectories in total (1,421,111 clips).\n\n\n\n\n\n  \nBaseline Success Detectors\nFor the  baseline, we use a success detection model specifically designed for the STS independently of this work.  There are two types of baseline models: whole episode evaluation and autoregressive evaluation. As the whole episode model consistently outperformed the autoregressive model, in this section we only report the results from that baseline (see Appendix <ref> for additional results). This model creates a downsampled set of 32 frames from the entire evaluation episode and embeds the images with a ResNet-101. The agent input and output text are embedded using a learned text embedding. All embeddings are then concatenated together and fed to a transformer with an MLP head that predicts the likelihood the episode was successful. In addition to training on success detection, an auxiliary instruction-matching contrastive loss is applied. \n\n\n\n  \nEvaluation \nTo select the best success detection model, we use the model and checkpoint with the highest balanced accuracy on a held-out validation split from the same distribution as the training data. We then evaluate the chosen success detector model across three different test sets: \n\n    \n  * Test 1: unseen episodes (in distribution) \u2013 a randomly held-out 10% of training dataset trajectories, which includes rephrasings of training tasks. This dataset contains 175,952 clips. \n    \n  * Test 2: unseen behaviour (out of distribution agents) \u2013 trajectories generated by new agents on tasks seen in the training dataset, including rephrasings of training tasks. These agents potentially demonstrate novel behaviour. This allows us to assess success detector robustness to unseen behaviours on known tasks, which is important as it determines if we can reuse the same models even as agent behaviour evolves over time (i.e. the success detector should be accurate even when the agent solves a known task in a novel way). \n\n    This dataset contains 462,061 clips. \n    \n  * Test 3: unseen tasks (out of distribution tasks and agents) \u2013 the most challenging setting: trajectories generated by new agents on new tasks not seen during training. For examples of how these tasks differ from the training set, see Table <ref>. Note that this set comprises completely new tasks as well as rephrasings of said tasks.  As the tasks are new, the success detector models need to master a semantic understanding of language to properly generalise to success detection in this set. This dataset contains 272031 clips.\n\n\n\n\n\n\n \u00a7.\u00a7 Experimental Results\n\n\n\n\nTable <ref> presents the episode-level balanced accuracy on each test set. We find that without finetuning, the accuracy of the Flamingo model is close to random chance (see Appendix <ref> for details). This is unsurprising, as the IA domain differs greatly from Flamingo's pretraining data. With finetuning on the same training set,  matches the performance of  in both Test 1 (unseen episodes) and Test 2 (unseen behaviour). More importantly, in Test 3 (unseen tasks), the performance of a bespoke model drops to a random chance, while  outperforms it by a significant margin (10%), see Table <ref>. \n\nAs the instructions in Test 3 are for novel tasks, not just rephrasings of tasks seen during training, this experiment demonstrates that the success detector exhibits some amount of semantic understanding of the scenes. We hypothesize that this is possible due to Flamingo's large language model backbone and web-scale pretraining. That said, there is still a large margin for improvement on the most challenging test set. For future work, it would be interesting to investigate how different model scales, dataset sizes, or cross-finetuning with different datasets can affect generalisation.\n\n\n\u00a7 VISUAL ROBUSTNESS WITH ROBOTIC MANIPULATION\n \n\n\n\nIn this section we train and evaluate success detectors on a family of real-life robotic gear manipulation tasks with a Panda robot arm. There are six tasks corresponding to inserting or removing a small, medium, or large gear within a basket (Figure <ref>). We consider visual observations from a basket camera. \n\nIdeally, a success detector should remain accurate under naturalistic visual changes, such as different camera view angles, lighting conditions, or backgrounds. Furthermore, as the performance of learned policies improves, we may want to introduce new objects or tasks to the environment. It quickly becomes impractical to re-annotate and re-train success detectors from previous tasks in new conditions, thus making it important to train visually robust success detectors. For example, a model that has learned to detect successful gear insertion should still be able to robustly detect success even if the basket has additional task-irrelevant distractor objects or the camera angle changes. To investigate this, we experiment with zero-shot evaluations on episodes with such visual changes.\n\n\n\n\n \u00a7.\u00a7 Methodology\n\n\n\n  \nTraining dataset\nHuman operators provide 101,789 demonstrations for 6 tasks using a 6DoF control device. Each episode is then annotated by humans with rewards for each task (e.g., every episode has 6 reward annotations, one for each task). Human annotators label positive rewards for all frames with a success state (i.e., if the task is solved), and zero rewards otherwise. Note that it is possible for a task to be accidentally undone in the same episode, at which point the reward annotation would revert to zero. The reward annotations and corresponding episode frames are then converted into SuccessVQA examples (see Figure <ref>). The ground truth VQA answer is obtained from the human annotations: clip answers are labelled successful if they contain only a single transition from zero to positive reward or only have positive rewards throughout, otherwise they are labelled as unsuccessful. We train a single  success detector model for all 6 tasks.\n\n\n\n\n\n\n  \nBaseline Success Detector\n\nAs a baseline, we consider a ResNet-based <cit.> per-frame success classification model, tuned specifically for this task by the robotics team. The ResNet-18 is pretrained on ImageNet, and the classification layer is swapped out for a binary classification layer. We finetune a separate success classification model for each of the 6 gear tasks, with image augmentations applied during training. This is distinct from our method where we train a single multi-task model across all 6 conditions. We consider an episode successful if the first and last frames[We find that incorporating more frames does not improve episode-level accuracy.] of the episode are classified as a failure (output < 0.5) and success (output > 0.5) correspondingly. We will further refer to the baseline model as bespoke success detector (). \n\n\n\n  \nEvaluation\nTo compare against the , we look at episode-level balanced accuracy. Given an evaluation episode, we consider the episode successful under  if the first clip is classified as unsuccessful and the last clip is classified as successful (see Figure <ref> in the Appendix). This matches the episode-level classification scheme of .\n\nWe conduct the evaluation on three test sets (see Figure <ref>):\n\n    \n  * Test 1: In-domain episodes (first row),\n    \n  * Test 2: Episodes with a viewpoint variation, using a different (back) camera (second row),\n    \n  * Test 3: Episodes with distractor objects in the basket, but the original camera (last row).\n\nThe last two settings are designed to test the robustness of the models to naturalistic visual perturbations in the environment. The trained success detectors can then either be used as automated evaluators or reward models for agent training.\n\n\n\n\n\n \u00a7.\u00a7 Experimental results\n\n\n\n\n  \u00a7.\u00a7.\u00a7 In-Domain Performance\n\n\n\n\nIn Test 1, we conduct an in-domain evaluation where the test set comes from the same visual conditions as the training set (see Figure <ref>, top row). The test set includes all the training episodes and an additional held out 2076 episodes. \n\nThe results in Table <ref> show that while the  consistently outperforms the , the performance of the  model is still comparable for the insertion task. Note that the accuracy of the Flamingo model on the remove tasks is lower, which we hypothesize is likely due to a data balancing issue. We have 5 times more training data available for insertion than removal, and training a single model across all tasks likely led to a tradeoff in accuracy between the insertion and removal tasks, which are temporal opposites of each other.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe further verify that the FT Flamingo 3B success detector can be used to train useful policies using reward-filtered behaviour cloning (BC). In filtered BC, we first use FT Flamingo 3B to classify demonstration episodes as successes or failures for a particular task. Then, we use only the episodes classified as success for BC training. Table <ref> shows the average success rates of the policies evaluated on 20 episodes with manual resets. In manual resets no extra gears are pre-inserted on the pegs for the insert task and only the one relevant gear is pre-inserted for the remove tasks. The success rates vary between 50% and 75%, suggesting that the accuracy of the success detector models is sufficient for some amount of policy training. To compare with the  model, we also conduct filtered BC training with the  reward model and evaluate an insert large gear policy over 100 episodes with automated resets. In automated resets, policies for different tasks are run in sequence one after another and any number of gears might be already inserted at the start of the episode, presenting a harder scenario. In this case, the success rate is 30%  with  is and 33% with . This provides a preliminary proof-of-concept that the difference in reward model accuracy does not lead to a large difference in policy performance. We leave more detailed policy evaluations to future work.\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Visual Robustness\n\n\nNext, we focus on testing the generalisation capabilities of success detectors. We measure zero-shot accuracy on two natural visual variations described above: Test 2 and Test 3.\n\nIn Test 2, we look at zero-shot robustness to different viewpoints (Figure <ref>, middle row). Given that the success detectors were only trained on frames from the front basket camera, we evaluate robustness by measuring success detector accuracy on episodes recorded with the back basket camera. As we can see in Table <ref>, changing the camera angle drastically hurts the quality of  (accuracy decreases of 10-50 absolute percentage points) while the performance of  is more stable (accuracy decreases by less than 10%). Note that in some tasks the performance of the bespoke model drops to the level of random guessing, essentially rendering the model useless for success detection. With this, FT Flamingo 3B becomes the best performing model in 5 out of 6 tasks. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, in Test 3 we look at zero-shot robustness in the setting where some distractor objects (two pegs and a board, see Figure <ref>, last row) are introduced. Table <ref> shows that detecting success on known tasks across this novel visual setting causes a 4-30% (absolute percentage points) drop in balanced accuracy for the bespoke model, while the accuracy mostly stays stable for the Flamingo-based models, with a 4.5% drop in accuracy at most. \n\nThese two experiments demonstrate that Flamingo-based success detection models are robust to natural visual variations. We hypothesize that the pretrained Flamingo-based success detection model is better suited to zero-shot visual generalisation than the bespoke baseline reward model, as Flamingo is pretrained on a diverse set of visual data with corresponding language grounding. While the baseline model was also pretrained and used image augmentations during task finetuning, it was not exposed to such a diverse set of visual data or language. Large-scale diverse pretraining might contribute to better semantic tasks recognition under naturalistic visual changes. These encouraging results suggest that pretrained VLM-based success detectors are likely better suited to the real-world tasks involving unstructured, open, and evolving settings.\n\n\n\n\u00a7 REAL WORLD SUCCESS DETECTION WITH EGO4D\n \n\nIn this section we describe creating a SuccessVQA dataset using \u201cin-the-wild\" egocentric videos of humans performing tasks. This present a much more diverse setting than the prior two domains, in both visuals and language. We construct this dataset using annotations from the Ego4D dataset <cit.>, where unlike prior benchmarks in action recognition, the focus is on detecting a temporal point of success for a given action. It is an example of a realistic, unstructured setting where the ground-truth success labels can be obtained only from human annotations. While the  success detector model shows initial promising results, our experiments show that the benchmark is nonetheless very challenging with much room for future progress.\n\n\n\nEgo4D is a publicly available dataset of egocentric human-in-the-wild videos. The videos show people executing common tasks (e.g., washing dishes, cleaning cars, gardening). To generate \u201csuccessful\" and \u201cunsuccessful\" action sequences, we make use of annotations from the Ego4D Forecasting + Hands & Objects (FHO) dataset, where corresponding narrations describe the actions of the camera wearer in the videos. Additionally, critical state changes are annotated: \u201chow the camera wearer changes the state of an object by using or manipulating it\u2013which we call an object state change\u201d <cit.>. Each narration is centered on an 8-second clip, which is further annotated with action verbs, object nouns, and state change types corresponding to the narration and clip, as well as the critical frames PRE, Point of No Return (PNR), and POST for indicating when the state change has occurred. The PNR frame annotates the start of the state change, the PRE frame indicates a point before the state change, and the POST frame is a point after the state change is completed. \n\nWe propose using the critical frame annotations as annotations of \u201csuccess\" for the behaviour described in the narration. Specifically, we treat PNR frame as a point at which \u201csuccess\" occurs. To generate a negative example for a clip, we use the frames in the 8-second clip prior to the PRE frame. These frames do not contain the point of success, but they often demonstrate the beginning of the relevant action. We then generate the questions for SuccessVQA by rephrasing the narrations into questions using Flamingo, as shown in Figure <ref>. \n\nUnlike the  and robotics domains where there is only one relevant task per episode, a single Ego4D \u201cepisode\" (i.e. video) can have multiple narrations corresponding to different actions. Thus, instead of episode-level accuracy we evaluate success detection accuracy on clips taken from held out videos. In our experiments,  finetuned on the SuccessVQA dataset attains  99% training balanced accuracy and  62% test set balanced accuracy. For context, zero shot and 4-shot Flamingo models only achieve 50% and 52%.\nThat is, without finetuning, the Flamingo model is not capable of detecting success. Providing a few examples with few-shot prompting improves performance, but very slightly. However, finetuning Flamingo on the in-domain Ego4D SuccessVQA examples achieves a significant improvement over random chance. That said, there is still a large gap between train and test performance. We find that it is currently difficult to generalise to completely unseen videos and language tasks, so this domain provides an exciting avenue for future work.\n\n\n\n\n\n\u00a7 CONCLUSION\n\n\nIn this work we propose SuccessVQA \u2013 a reformulation of success detection that is amenable to pretrained VLMs such as Flamingo. We investigate success detection across a wide range of domains: simulated language-conditioned interactive agents, real-world robotic manipulation, and \u201cin-the-wild\" human videos. We find that the pretrained VLM has comparable performance on most in-distribution tasks, and increased robustness across language and visual changes compared to task-specific reward models, and emphasize that our contribution is a more universal success detection task formulation that can be applied easily across vastly different domains.  VLMs can be used as policies, see e.g., <cit.>, but in this work we have demonstrated that there is also great value in using them as reward models. In contrast to VLMs as policies, VLMs as rewards focuses on the `what to do' and not on `how to do it'. We therefore expect such models to transfer more easily than policies when the same task can be accomplished in many ways, and where fine visual details are not necessary (e.g., grasp angle for fine motor control).\n\nThat said, this method has some limitations. There still exist some gaps between the Flamingo-based reward models and the bespoke reward models in our experiments, especially in some tasks in the robotics environment. Furthermore, inference with a larger VLM is expensive, making online success detection challenging. Lastly, we find that finetuning on a sufficient amount of in-domain data is necessary for robust success detection, as zero-shot or few-shot performance is not sufficient yet. Nonetheless, we are optimistic that further progress on broadly improving VLMs will result in more accurate few-shot success detection.\n\nTo address the limitations of the current approach, improving inference speed or distillation to a smaller model can help with efficient online success detection. Before deployment as a reward model for learning policies, we need further investigations into model accuracy and thorough characterizations of the effects of false positives and false negatives. So far we have experimented with a Flamingo 3B, but larger models might bring further improvements in robustness and generalisation. Another interesting avenue would be to investigate the practicality of in-domain few-shot generalisation to novel tasks (e.g., train on `insert gear' tasks, then detect success on `remove gear' after prompting with a few examples). An interesting question is when to choose few-shot or finetuning and how to combine the two. The shared SuccessVQA format can enable shared finetuning across different datasets (e.g., combining Ego4D SuccessVQA and VQAv2 <cit.>) to study the impact of cross-task transfer. Lastly, the flexibility in input format of VLM models allows us to consider success detection tasks where the task is specified visually (e.g., with a goal image) or the state is described in language (e.g., a dialogue agent) in the same framework as the current work.\n\n\n\n\u00a7 ACKNOWLEDGEMENTS\n\n\nWe would like to thank Olivia Watkins and Antoine Miech for careful proofreading of the paper and detailed comments. We would also like to thank the DM Robotics Team, the Interactive Agents team, and the Flamingo team for insightful discussions and research support.\n\n\n\n\n\n\n\n\n\u00a7 SIMULATED HOUSEHOLD DOMAIN\n\n\nTo evaluate agent policies on the standardized set of scenarios (STS), each agent is first given a period of context to replay up to a \"continuation point\", after which the agent policy is used to complete the trajectory. Each continuation is then evaluated offline by human annotators as either successful or failure, along with the point at which success or failure occurs. These human annotations are then used to rank agent policies, using the proportion of successful annotations they receive. For more details on the evaluation procedure, see <cit.>.\n\n\n\n \u00a7.\u00a7 Baseline Evaluation Models\n\nWhile human evaluations provide the ground truth signal for assessing agent capabilities, the cost of annotations scales directly with the number of evaluations for each new task and agent. Thus, there has been interest in automating the evaluation protocol to enable evaluation to scale over time. Ideally, an automated evaluation model will condition on an episode of agent behaviour and the input task utterance, and output a classification whether or not the task is successful. \n\nCurrently two baseline evaluation models have been developed for the STS: whole-episode and autoregressive models. In both cases, the reward annotations for a particular episode are aggregated using majority voting.  \n\nWhole episode evaluation models.\n\n\nFor these models, we first preprocess an STS episode by downsampling it to 32 frames and tokenizing the text instruction and agent responses. The images are then embedded with a ResNet-101, the input and output text are embedded, and these embeddings are concatenated together and fed to a transformer with 16 layers and 16 attention heads. The transformer output is fed through two MLP heads: one to predict the likelihood of the episode being successful, P(success), and an auxiliary contrastive loss, P(matching). P(success) is supervised with the aggregated reward annotations, and P(matching) is trained to predict whether an instruction matches the episode or has been shuffled.  \n\nAutoregressive evaluation models.\n\nThe autoregressive evaluation models use the same architecture as the Playhouse agents, which takes inputs on a per-frame basis, rather than at the episode level. The model embeds the images and language for each frame, passes the embeddings to a multimodal transformer followed by an LSTM, and is asked to predict success or no-success on a per frame basis. The success of an entire episode is then determined by whether or not any single frame was predicted to be successful.\n\n\n\n\n\u00a7 ROBOTICS DOMAIN\n\n\n\n \u00a7.\u00a7 Ground truth in robotics domain\n\n\nFigure <ref> shows how the ground truth success and failure labels are assigned to the full episodes. For an episode to be successful, it must start in a failure state and terminate in a success state.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Data Efficiency in robotics domain\n\n\nWe investigate whether the pretraining used for Flamingo makes it more amenable to accurate success detection in the lower-data regime. For this set of experiments, we train on only 100-200 episodes (100x less than the tens of thousands of episodes used in the above experiments) per task and evaluate on the same in-domain test set. As shown in Table <ref>, for five of the six tasks the Flamingo-based model is less affected by the smaller dataset than the ResNet-based model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n"}