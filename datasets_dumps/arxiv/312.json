{"entry_id": "http://arxiv.org/abs/2303.06902v1", "published": "20230313073237", "title": "Molecular Property Prediction by Semantic-invariant Contrastive Learning", "authors": ["Ziqiao Zhang", "Ailin Xie", "Jihong Guan", "Shuigeng Zhou"], "primary_category": "q-bio.BM", "categories": ["q-bio.BM", "cs.LG"], "text": "\nResource-efficient Direct Characterization of General Density Matrix\n    Lijian Zhang\n    March 30, 2023\n====================================================================\n\n\n\n\n\n    Contrastive learning have been widely used as pretext tasks for self-supervised pre-trained molecular representation learning models in AI-aided drug design and discovery.\n    However, exiting methods that generate molecular views by noise-adding operations for contrastive learning may face the semantic inconsistency problem, which leads to false positive pairs and consequently poor prediction performance.\n    To address this problem, in this paper we first propose a semantic-invariant view generation method by properly breaking molecular graphs into fragment pairs. Then, \n    we develop a Fragment-based Semantic-Invariant Contrastive Learning (FraSICL) model based on this view generation method for molecular property prediction.\n    The FraSICL model consists of two branches to generate representations of views for contrastive learning, meanwhile a multi-view fusion and an auxiliary similarity loss are introduced to make better use of the information contained in different fragment-pair views.\n    Extensive experiments on various benchmark datasets show that with the least number of pre-training samples, FraSICL can achieve state-of-the-art performance, compared with major existing counterpart models.\n\n    \n\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n \n\nNowadays molecular property prediction (MPP) based on deep learning techniques has been a hot research topic of the AI-aided Drug Discovery (AIDD) community\u00a0<cit.>.\nAs most of the molecular properties that drug discovery studies concern require in vivo or in vitro wet-lab experiments to measure, labeled data for MPP tasks are typically scarce, because it is expensive and time-consuming to acquire such data\u00a0<cit.>.\nOn the contrary, there are large amounts of public available unlabeled data\u00a0<cit.>.\nTherefore, how to use these large-scale unlabeled molecular data to train deep neural networks to learn better molecular representations for MPP tasks, is of great interest to the AIDD community.\n\nRecently, as self-supervised pre-trained models (e.g. BERT\u00a0<cit.>, MoCo\u00a0<cit.> and SimCLR\u00a0<cit.>) have shown significant superiority in the fields of Natural Language Processing (NLP) and Computer Vision (CV), self-supervised learning (SSL) has become a mainstream method of utilizing large-scale unlabeled molecular data in MPP study.\nThese SSL methods typically use some inherent features within or between samples to construct pretext tasks, so that unlabeled data can be leveraged to train deep models in a self-supervised learning manner\u00a0<cit.>.\nContrastive learning, masked language model and predictive learning are the currently three categories of methods to design pretext tasks in MPP studies\u00a0<cit.>.\nInspired by SimCLR, contrastive learning methods aim at learning representations through contrasting positive data pairs against negative ones\u00a0<cit.>.\nOriginal molecular structures are augmented into multiple views, and views generated from the same molecule are typically used as positive data pairs, while views of different molecules are taken as negative ones\u00a0<cit.>.\n\n\n\n\n\n\n\n\nThe way to generate molecular views is crucial to the design of contrastive learning pretext tasks for molecular representation learning.\n\nAs a kind of special objects, molecules can be represented by different methods, including molecular fingerprints\u00a0<cit.>, SMILES\u00a0<cit.>, IUPAC\u00a0<cit.>, and molecular graph.\nThese different molecular representation methods therefore can naturally be leveraged to generate views for contrastive learning.\nFor instance, the DMP\u00a0<cit.> and MEMO\u00a0<cit.> models are designed in this way.\nFollowing the practice in CV, another widely used category of methods tries to add noise into molecular structures to generate transformations of the original molecules.\nThese noise-adding operations include deleting atoms, replacing atoms, deleting bonds, deleting subgraph structures etc. \nMolCLR\u00a0<cit.> and GraphLoG\u00a0<cit.> are such representative models.\n\nAlthough the noise-adding methods for view generation have been widely used in CV studies\u00a0<cit.>, when applying these methods into MPP tasks, a fact that has not been noticed by the researchers is that molecules are very sensitive to noise.\nArbitrarily modifying the topological structure of a molecule with noise, the generated new structure may represent a totally different molecule.\n\nFor instance, as shown in Fig.\u00a0<ref>(a), adding noise into an dog image by randomly masking some area will not change the semantic of the generated view, which is still a yellow dog.\nHowever, in Fig.\u00a0<ref>(b), for an acetophenone molecule, deleting a subgraph leads to a benzene molecule, indicating that acetophenone's chemical semantic is completely changed.\nAnd small modification to molecular structure can lead to dramatic changes in the properties of modified molecules, including both bio-activity and other physio-chemical properties.\n\n\n\nConcretely, from the PubChem database we can find that the LogP value of acetophenone in Fig.\u00a0<ref>(b) is 1.58, while that of benzene is 2.13. The difference is almost 35%.\nTherefore, \n\nit is unreasonable to treat these two views (molecules) as a positive pair for contrastive learning.\n\n\n\nAiming at solving this semantic inconsistency problem, this paper proposes a Fragment-based Semantic-Invariant Contrastive Learning molecular representation model, named FraSICL.\n\n\nA semantic-invariant molecular view generation method is developed, in which a molecular graph is properly broken into fragments by changing the message passing topology while preserving the topological information.\nA multi-view fusion mechanism is introduced to FraSICL to make better use of the information contained in views of different fragments and avoid the impact of randomness.\n\n\n\n\nIn addition, an auxiliary similarity loss is designed to train the backbone Graph Neural Network (GNN) to generate better representation vectors.\n\n\n\nOur contribution are summarized as follows:\n\n    \n  * We raise the semantic inconsistency problem in molecular view construction for molecular constrastive learning and develop an effective method to generate semantic-invariant graph views by changing message passing topology while preserving the topological information.\n    \n  * We propose a novel Fragment-based Semantic-Invariant Contrastive Learning molecular representation model for effective molecular property prediction, which is also equipped with a multi-view fusion mechanism and an auxiliary similarity loss to better leverage the information contained in unlabeled pre-training data.\n    \n  * Extensive experiments show that compared with SOTA pre-trained molecular property prediction models, the proposed FraSICL can achieve better prediction accuracy on downstream target tasks with less amounts of unlabeled pre-training data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 METHOD\n\nHere, we first formally define \nsemantic-invariant molecular view in Sec.\u00a0<ref>, then propose a semantic-invariant molecular view generation method in Sec.\u00a0<ref> and a multi-view fusion scheme in Sec.\u00a0<ref>. Finally, \nwe present the structure of the Fragment-based Semantic-Invariant Contrastive Learning (FraSICL) molecular representation model in Sec.\u00a0<ref> and its loss functions in Sec.\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Semantic-invariant Molecular View\n\nIn Sec.\u00a0<ref>, we give an example to illustrate how noise-adding operations may lead to semantic inconsistency and consequently false positive pairs.\nHere, we will formally define semantic-invariant molecular view. \n\n\n\n\n\n\n\nGiven a molecule m and its molecular graph G = {V,E,X_atom,X_bond} (hydrogen-depleted) where\nV denotes the set of nodes that represent the atoms, E denotes the set of edges between nodes, representing the bonds. X_atom and X_bond are feature matrix of atoms and bonds respectively.\nA transformation function F(\u00b7) is used to generate a molecular graph view (or simply molecular view) G' of G, i.e.,  G'=F(G) and G'={V',E',X'_atom,X'_bond}. In what follows, we first define two types of semantic inconsistent views.\n\n\nIf there is another molecule m_2 whose molecular graph is G_2, and G_2=G'=F(G), i.e., the view G' of m is \n\nthe same as\nthe molecular graph G_2 of m_2, then we say G' is a semantic-conflict view of m with regard to (w.r.t.) m_2.\n\n\n\n\n\nIf there exists another molecule m_2 whose molecular graph is G_2, and  G'_2=F(G_2)=G'=F(G), i.e., the view G' of m is\n\nthe same as\na view G'_2 of m_2. Then, we say \nG' is a semantic-ambiguity view of molecule m w.r.t. molecule m_2.\n\n\n\n\n\nBoth semantic-conflict views and semantic-ambiguity views will lead to false positive pairs for molecular representation contrastive learning.\nFor example, assume that a Graph Neural Network g(\u00b7) serves as an encoder to embed the molecular graphs into latent graph embeddings _G=g(G).\nIf we ignore the randomness in the encoder, it is obvious that, for molecule m, if it has a semantic-conflict view w.r.t. molecule m_2, i.e., G'=F(G)=G_2, then the representation of G' embedded by the graph neural network will be the same as that of G_2.\nThat is, _G'=g(G')=_G_2=g(G_2).\n\nIn this case, as _G and _G' are considered as a positive pair in contrastive learning, _G and _G_2 are consequently used as a positive pair.\nIn another word, the contrastive loss will implicitly make the representations of molecule m and m_2 to be close.\nHowever, as claimed before, the molecular properties of different molecules may be greatly different, so that they cannot be used as a positive pair for contrastive learning.\nTherefore, semantic-conflict views will lead to false positive pairs and degrade learning performance.\n\nOn the other hand, if a semantic-ambiguity view is generated as defined in Def.\u00a0<ref>, i.e., F(G_2)=G'=F(G), \n\nindicating that the contrastive loss will make _G and _G', _G_2 and _G' to be close in the embedding space, thus _G and _G_2 to be close too.\nThat is, the representations of molecules m and m_2 are consequently close by contrastive learning.\n\n\n\nSo semantic-ambiguity views will also lead to false positive pairs.\n\nTo boost the performance of contrastive learning for MPP, we should avoid the generation of both semantic-conflict views and semantic-ambiguity views. That is, we generate only semantic-invariant views, which are defined as follows:\n\n\nGiven a view G' of molecule m with graph G, if G' is neither a semantic-conflict view nor a semantic-ambiguity view w.r.t. any other molecules, then we say \nG' is a semantic-invariant view of m.\n \n\nIn next section, we will give a method to generate semantic-invariant views.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Semantic-invariant View Generation\n\n\n\n\n\n\nAccording to Def.\u00a0<ref> in Sec.\u00a0<ref>, semantic-invariant views should be neither  \nsemantic-conflict views nor semantic-ambiguity views. Besides, from the perspective of prediction, they should also be discriminative. That is, they can be encoded into different representations by neural network encoders. \nIn this section, to achieve these goals, we propose a semantic-invariant view generation method.\n\nIn our previous study\u00a0<cit.>, to better capture the hierarchical structural information of molecules, a chemical-interpretable molecule fragmentization method FraGAT is proposed.\nBy considering acyclic single bonds as boundaries between functional groups, the FraGAT model proposes to randomly breaking one of the acyclic single bonds to generate two graph fragments corresponding to some chemical meaningful functional groups.\nThe experimental results show that learning representations by chemical meaningful molecular graph fragments can achieve good predictive performance for MPP tasks.\n\nInspired by these findings, our semantic-invariant view generation method is designed as follows: \n \n\n\n\n\n\nGiven a molecule m, its molecular graph can be denoted as an annotated graph G = {V, E, X_atom, X_bond}.\nThe atom feature matrix X_atom and the bond feature matrix X_bond are computed according to Tab.\u00a0<ref>.\nThen, remove one of the acyclic single bonds e_ij from E, we obtain G' = {V, E', X_atom, X_bond} where E' = E - {e_ij}.\n\n\nWe accept G' as a view to be generated, i.e., a semantic-invariant view.\nAs the graph G' consists of two disconnected molecular graph fragments, it is also called fragment-pair view.\n\nFrom the discrimination perspective, G' is a different graph from the original molecular graph G, so that it will make GNN encoders to generate a different representation. Furthermore, as all the acyclic single bonds in a molecule are unique, breaking different acyclic single bonds will lead to different fragment-pair views, whose representations after a GNN encoder will also be different. That is, the generated views for a molecule are discriminative. \n\n\n\n\n\n\n\n\n\n\n\n\n\nThen, is G' a real semantic-invariant view of molecule m according to Def.\u00a0<ref>? Let us check.  \n\nOn the one hand, as the atom feature matrix X_atom is not modified, the numbers of bonds of the two vertex i and j of the broken bond e_ij encoded in the atom feature vectors of the generated G' remain the same as that in the original molecular graph G.\nHowever, the modified E' indicates that there is no edge between i and j, so that the numbers of bonds encoded in the atom feature vectors are not consistent with that in the topological structure.\nThe degrees of node i and j in graph G' are lower than the numbers of bonds of i and j encoded in X_atom.\nIn another word, G' is not a valid molecular graph of any molecule. Furthermore, from the graph perspective, because G' consists of two disconnected subgraphs, and no any valid molecule corresponds to a disconnected graph. \nTherefore, G' cannot be a semantic-conflict view w.r.t. any molecule according to Def.\u00a0<ref>.\n\n\n\n\n\n\n\n\n\nOn the other hand, since only one single bond is removed in view G', this discrepancy can only be discovered at nodes i and j, and the numbers of bonds of i and j encoded in X_atom must be only 1 larger than the degrees of i and j, so the removed edge can only be between i and j, and the removed edge can only be a single bond.\nThus, there is no other molecular graph G_2 that can generate the same G'. Similarly, from graph perspective, the graph G of molecule m cannot be equal to the disconnected graph of any view generated from any other molecule. In summary, G' cannot be a semantic-ambiguity view w.r.t. any molecule according to Def.\u00a0<ref>.\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, from the perspective of graph rewiring\u00a0<cit.>, the topological information about the broken edge is encoded in the atom feature matrix X_atom.\nSo our method preserves the topological structural information of the original molecular graph, but propagates message between nodes through a different topology.\nThus, it realizes local decoupling of the input graph topology and the message passing topology.\nMoreover, compared with randomly breaking any edges in the molecular graph, our method can generate chemical meaningful graph fragments to benefit the prediction of molecular properties, which has been demonstrated in the experiments of previous work\u00a0<cit.>.\n\nIn conclusion, our method is expected to generate better positive pairs, which will help to train neural networks to generate better molecular representations by contrastive learning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Multi-view Fusion\n\n\nThe number of acyclic single bonds of an organic molecule is often large, so there are various fragment-pair views can be generated from one molecule by our proposed view generation method.\nAs demonstrated in the experiments of some existing work\u00a0<cit.>, different fragment pairs contain different information about functional groups that constitute a molecule, which shows different predictive performance.\nSo, to ensure that the information of the functional groups that determine molecular properties and are contained in the fragment pairs can be obtained by the neural network, in FraSICL, we no longer randomly generate fragment-pair views as other contrastive learning models do.\nInstead, a multi-view fusion mechanism is introduced as follows: \nGiven a molecule with N_b breakable acyclic single bonds, all of the N_b fragment-pair views are generated and the representations of these fragment-pair views are calculated by a GNN encoder.\nThen, a Transformer encoder is exploited to fuse these representations by the multi-head attention (MHA) mechanism to produce a representation vector that contains information of all of the fragment pairs, named fragment view. The details of fragment-pair view fusion are delayed to the next section.  \nThe fragment view and the molecule view ( i.e., the original molecular graph) are used as two views of a molecule for contrastive learning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Model Structure\n\n\n\n\n\n\n\nThe structure of the FraSICL model is shown in Fig.\u00a0<ref>.\nGiven a molecule m with molecular graph G_mol={V, E, X_atom, X_bond}, \n\n\nthe model computes the representations of two views via two branches: the left branch is the molecule view branch for generating molecular view, and the right one is the fragment view branch for generating the fragment view.\n\nIn the molecule view branch, a GNN g_mol(\u00b7) is used as an encoder to capture the representation of the molecular graph _mol=g_mol(G_mol).\nAttentive FP\u00a0<cit.> is employed as the graph encoder in this work.\n\nThen, following the structure of MolCLR\u00a0<cit.>, _mol is fed to a projection head l_mol(\u00b7) and a regularization function norm(\u00b7) to produce the projection of the molecule view _mol=norm(l_mol(_mol)).\nThe structure of a projection head is shown in Fig.\u00a0<ref>.\nAnd the regularization function is norm()\u030c = /, which can make the length of the projection vector be 1.\n\n\nAnd for the fragment view branch on the right, all of the N_b breakable acyclic single bonds are enumerated and broken by the method proposed in Sec.\u00a0<ref> to generate N_b fragment-pair views G_frag^1={V,E_1,X_atom,X_bond}, \u2026, G_frag^N_b={V,E_N_b,X_atom,X_bond}.\nThen, a GNN g_frag(\u00b7) is used as an encoder to compute the representation of each fragment-pair view _frag^i=g_frag(G_frag^i).\n\n\nAttentive FP is also used here.\nNote that since there are two disconnected components in each fragment-pair view G_frag^i, g_frag(\u00b7) will read out these two subgraphs separately and produce two subgraph embeddings.\nThe representation of a fragment-pair view is obtained by element-wisely adding its two corresponding subgraph embeddings for permutation-invariant property.\n\n\n\nThen, as described in Sec.\u00a0<ref>, a multi-view fusion mechanism is introduced for leveraging all of the information related to functional groups contained in the N_b fragment-pair views.\n\nSpecifically, a Transformer encoder T(\u00b7) is employed, which uses the representations of fragment-pair views _frag^i as input tokens, and computes the interaction relationships between the fragment-pair views by the multi-head attention (MHA) mechanism.\nThe resulting attention scores serve as weights to fuse the representations and obtain _frag^i.\n\nBy summing up all of the representations of N_b fragment-pair views, we can get the representation of fragment view _fv = \u2211_i=1^N_b_frag^i.\n\n\n\nFinally, the representation _fv of the fragment view goes through a projection head l_fv(\u00b7) and a normalization layer to get _fv = norm(l_fv(_fv)).\nFollowing the structure of MolCLR, the two projections _mol and _fv are used to calculate contrastive loss.\nAnd when finetuning on the downstream tasks, the model will output one of the representations _mol or _fv of a molecule to serve as learned molecular representation.\nA downstream prediction head f(\u00b7) will use this representation as input, and predict the molecular property by y=f(_mol) or y=f(_fv).\n\n\n\nIn addition, representations _frag^i of N_b fragment-pair views of a molecule goes through another projection head and a normalization layer to produce projection _frag^i = norm(l_frag(_frag^i)).\nInner product of these projections are computed to generate a similarity matrix S={s_ij | s_ij = <_frag^i, _frag^j>}, S \u2208\u211d^N_b \u00d7 N_b, where <\u00b7,\u00b7> denotes the inner product of two vectors.\nThe usage of this similarity matrix S will be introduced in the next section.\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Loss Functions\n\n\n\nThe training of FraSICL in the pre-training phase is illustrated in Fig.\u00a0<ref>.\nHere, given a batch of N molecules, the model will calculate the projections _mol and _fv of each molecule.\nThen, contrastive learning is performed between all samples in a batch.\nThe view pair (i.e. molecule view and fragment view) of each sample is a positive pair, as shown by the red line, and the view pairs of other samples in the batch are negative pairs, as shown by the blue line in the figure.\nThe NT-Xent Loss is used for contrastive learning:\n\n\n    \u2112_mol^i = loge^(sim(_mol^i,_fv^i)/\u03c4)/\u2211_k=1^N1{k  i}(e^(sim(_mol^i,_mol^k)/\u03c4)+e^(sim(_mol^i,_fv^k)/\u03c4))\n\n\n    \u2112_fv^i = loge^(sim(_mol^i,_fv^i)/\u03c4)/\u2211_k=1^N1{k  i}(e^(sim(_fv^i,_mol^k)/\u03c4)+e^(sim(_fv^i,_fv^k)/\u03c4))\n\nwhere inner product similarity is adopted for sim(_mol^i,_fv^i), and \u03c4 is a temperature parameter.\n\nThe sum of all contrastive losses of a batch of molecules is denoted as \u2112_clr=\u2211_i=1^N \u2112_mol^i + \u2211_i=1^N \u2112_fv^i.\n\nIn addition, although the representations of different fragment-pair views have been fused, from the perspective of contrastive learning, the representations of different fragment-pair views of the same molecule should also be as close as possible.\nAnd as demonstrated in previous study\u00a0<cit.>, representations of some fragment pairs of a molecule are highly predictive on the downstream tasks, while some others are less effective.\nSo, we hope that the representations of fragment-pair views can use information from each other to train the GNN encoder to extract better representations.\nTo this end, an additional auxiliary loss \u2112_sim is introduced to improve the similarity between representations of fragment-pair views of a molecule, based on the similarity matrix S.\nSince the inner product of two normalized vectors is equivalent to cosine similarity, and the maximum value of cosine similarity is 1, assuming a molecule k have N_b^k fragment-view pairs, the elements of the similarity matrix are s_ij^k = <_frag^k,i, _frag^k,j>, our auxiliary similarity loss of molecule k is:\n\n\n\n\n\n\n\n\n\n\n\n\n    \u2112_sim^k = 1/(N_b^k)^2\u2211_i=1^N_b^k\u2211_j=1^N_b^k (s_ij^k-1)^2\n\ni.e., as shown in Fig.\u00a0<ref>, the sum of L2 loss between each element of the similarity matrix S and that of an all-one matrix.\nDenote the similarity loss of a batch of molecules as \u2112_sim=\u2211_k=1^N \u2112_sim^k, then the loss for pre-training the FraSICL model is:\n\n\n    \u2112 = \u03b3\u2112_sim + \u2112_clr\n\nwhere \u03b3 is a hyper-parameter to adjust the influence of the auxiliary similarity loss.\n\n\n\n\n\u00a7 EXPERIMENTS\n\n\n\n\n \u00a7.\u00a7 Baseline experiments\n\nExperimental setting. To construct the pre-training dataset, 200K molecules are randomly sampled from the pre-training dataset of MolCLR, where 10 million molecules are gathered from the PubChem database\u00a0<cit.>.\n\n\n\n\nThe amount of pre-training data is generally smaller than that of the other baseline models, as shown in Tab.\u00a0<ref>.\n5% of the pre-training data are randomly selected as a validation set for model selection.\n\n7 downstream tasks from MoleculeNet\u00a0<cit.> are used as downstream target tasks for the baseline experiments.\n\nScaffold splitting is used on each downstream task, with an 8:1:1 ratio for the training/validation/test sets.\n\nWhen transferring a pre-trained FraSICL model to the target tasks, different strategies can be applied, including using which branch of the model for producing molecular representations, and whether to finetune the pre-trained model (PTM) on target tasks.\n\nIn the baseline experiments, we adopt the more complex fragment view branch for molecular representations, and finetune the model together with prediction head on the target tasks.\n\n\n\nCompared baseline models. Seven state-of-the-art self-supervised pre-training models for molecular representation learning are used as baseline models for comparison, including MolCLR\u00a0<cit.>, DMP\u00a0<cit.>, MEMO\u00a0<cit.>, GROVER\u00a0<cit.>, GraphLoG\u00a0<cit.>, PretrainGNNs\u00a0<cit.> and KPGT\u00a0<cit.>.\nThe experimental results are shown in Tab.\u00a0<ref>, where the data of baseline models are cited from the original papers of these models.\nThe best score on each dataset is bold, and the second-best is underlined.\n\n\n\n\n\nResults and analysis. As shown in Tab.\u00a0<ref>, FraSICL achieves the best predictive performance on 5 of the 7 downstream MPP tasks, and the second on another one.\nAs the number of pre-training samples used by FraSICL is only 200K, which is the least among these compared baseline models, the experimental results show that FraSICL can make better use of the information contained in the graph fragments of molecules to produce molecular representations with better predictive performance.\nCompared with the MEMO model that uses the same amount of pre-training data, the predictive performance of FraSICL on the 7 downstream tasks is significantly improved, even exceeds 20% on the BBBP dataset.\nAnd compared with the models such as GROVER and DMP-TF, FraSICL can achieve comparable or even higher predictive performance with only about 1/50 training samples.\nThese results show the superiority of FraSICL to the existing models on molecular property prediction tasks.\n\n\n\n \u00a7.\u00a7 Experiments with different transferring settings\n\nExperimental setting. In the baseline experiments, we choose finetuning the more complex and predictive fragment view branch as the transferring setting.\n\n\nIn this section, other transferring settings are tested, i.e., the combinations of different branches and different fine-tuning strategies.\nExperiments are carried out on the BBBP, ClinTox, ESOL and FreeSolv datasets.\nFour transferring settings are evaluated, which are denoted as FraSICL-ft-mol, FraSICL-ft-frag, FraSICL-fr-mol, FraSICL-fr-frag, where ft represents finetuning the PTM, fr represents freezing the PTM, mol indicates using molecule views and frag indicates using fragment views.\n\nResults and analysis. The experimental results are shown in Tab.\u00a0<ref>.\nSince the two branches of FraSICL are asymmetric, the structure of the fragment view branch is more complex and has stronger learning capability.\nThus, as is revealed by the experimental results, FraSICL-ft-frag achieves the best performance on 3 of the 4 target tasks.\n\n\n\n\n\n\n\nHowever, a more complex model structure indicates that it is more likely to suffer overfitting on the downstream tasks.\n\nSo, when transferring to the FreeSolv dataset with only 642 samples, the performance of FraSICL-ft-frag is slightly inferior to that of FraSICL-ft-mol.\n\nIn addition, compared with freezing the pre-trained model, finetuning model allows the PTM to obtain information about specific molecular properties from the supervised loss, thereby the generated molecular representations are more relevant to the target task.\nThus a large improvement on the performance is achieved.\n\n\n\n\n\n \u00a7.\u00a7 Influence of the auxiliary similarity loss\n\nMotivation. The auxiliary similarity loss, i.e., Equ.\u00a0(<ref>) introduced in Sec.\u00a0<ref>, is designed for making the fragment-pair views to learn from each other to better leverage the information encoded in different fragment-pair views.\n\nHowever, it is intuitive that when the auxiliary loss takes an excessively dominant role in the total training loss, the model may tend to generate\n\nexactly the same representation vectors for different fragment-pair views to decrease the similarity loss.\nOn this occasion, the representation vectors of fragment-pair views will not contain any information about the topological structure, showing a model collapse phenomenon.\nThus, the influence of the hyperparameter \u03b3 is crucial.\n\n\n\nExperimental setting. In this section, experiments are conducted to test the influence of the auxiliary similarity loss by setting different \u03b3.\n\nIn these experiments, \u03b3 is set to 0.1, 0.01, 0.005 and 0 respectively, where \u03b3=0 indicates training without the similarity loss, which can be regarded as an ablation study.\n\nHere, the transferring setting is the same as the baseline experiments, i.e., finetuning the fragment view branch.\n\n\n\n\n\nResults and analysis. Results are shown in Tab.\u00a0<ref>. As shown in Tab.\u00a0<ref>, the auxiliary similarity loss has an obvious impact on the predictive performance.\n\nWhen \u03b3=0, i.e., training without the similarity loss, the predictive performance is not superior to that of the other three models trained with the auxiliary similarity loss, which demonstrates that the auxiliary similarity loss can indeed promote the model to produce more predictive representations.\nAnd when \u03b3=0.1, the model achieves even worse results than \u03b3=0 on 3 of the 4 downstream tasks, which reveals that a large value of \u03b3 may make the similarity loss be harmful to the model and lead to performance degradation.\n\n\n\n\n\u00a7 CONCLUSION\n\nThis paper focuses on the semantic inconsistency problem that may occur when using noise-adding operations to generate new views for contrastive learning in self-supervised molecular property prediction studies.\nTo solve this problem, this paper first defines semantic-invariant molecular view by introducing two types of semantic inconsistent views that may lead to false positive pairs and consequently poor performance.\n\nThen, a semantic-invariant view generation method is proposed.\nThe views generated by this method will not cause semantic inconsistency, which realizes the decoupling of the input graph topology and the message passing topology of GNNs.\nThus, this method is expected to promote the GNN encoders to extract better molecular representations.\n\nBased on the semantic-invariant views, a Fragment-based Semantic-Invariant Contrastive Learning (FraSICL) molecular representation model is developed.\nFraSICL is an asymmetric model with two branches, the molecule view branch and the fragment view branch.\nA multi-view fusion mechanism is also introduced to make better use of the information contained in the views of different fragment pairs. Furthermore, an auxiliary similarity loss is designed to train the backbone GNN to produce better representations.\n\n\n\n\nBaseline experiments are conducted on 7 target tasks, and experimental results show that FraSICL achieves state-of-the-art predictive performance with the least number of pre-training data.\nFurther experiments demonstrate that in our model finetuning is effective in boosting performance and the auxiliary similarity loss can improve the predictive accuracy if a proper hyperparameter \u03b3 is selected.\n\nThese findings reveal that FraSICL can make better use of the information of pre-training samples and generate representations with superior predictive performance.\n\n\n\nunsrt  \n  \n\n\n\n"}