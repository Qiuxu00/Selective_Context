{"entry_id": "http://arxiv.org/abs/2303.06832v2", "published": "20230313032836", "title": "ODIN: On-demand Data Formulation to Mitigate Dataset Lock-in", "authors": ["SP Choi", "Jihun Lee", "Hyeongseok Ahn", "Sanghee Jung", "Bumsoo Kang"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "text": "\n\n\n\n\n\n\n\n\nODIN: On-demand Data Formulation to MItigate Dataset Lock-iN\n    \n SP Choi, Jihun Lee, Hyeongseok Ahn, Sanghee Jung, Bumsoo Kang\n\n Lotte AI Research\n\n \n {seungpyo.choi, jihoon8798, hyeongseok_ahn, sanghee.jung, bumsoo.kang}@lotte.net\n \n\n\n\n\n\n\n\n\n \n \n    Received: date / Accepted: date\n==========================================================================================================================================================================================\n\n\n\nempty\n\n\n\n   is an innovative approach that addresses the problem of dataset constraints by integrating generative AI models. Traditional zero-shot learning methods are constrained by the training dataset. To fundamentally overcome this limitation, attempts to mitigate the dataset constraints by generating on-demand datasets based on user requirements.\n   consists of three main modules: a prompt generator, a text-to-image generator, and an image post-processor. To generate high-quality prompts and images, we adopted a large language model (e.g., ChatGPT), and a text-to-image diffusion model (e.g., Stable Diffusion), respectively. We evaluated on various datasets in terms of model accuracy and data diversity to demonstrate its potential, and conducted post-experiments for further investigation.\n Overall, is a feasible approach that enables AI to learn unseen knowledge beyond the training dataset.\n\n\n\n\n\n\n\n\n\u00a7 INTRODUCTION\n\n\nThe primary drawback of traditional supervised machine learning is its dependency on the training dataset for acquiring knowledge. This means that a model designed for a particular classification task is unable to classify classes that were not included in the dataset. \nResearchers have been exploring ways to enable machine learning techniques to obtain learning-ability for new things, similar to how humans learn.\n\nZero-shot learning\u00a0<cit.> is a popular approach in this direction, which tackles the challenge of handling unseen classes not present in the training dataset. \nEmbedding-based zero-shot learning\u00a0<cit.> enables a model to represent unseen classes as a combination of features solely learned from the training data, leading to a strong dependency on the dataset.\nOn the other hand, generative zero-shot learning\u00a0<cit.> is the method that trains a generative model to generate latent features for unseen classes based on the training dataset. The latent features are integrated with the training dataset at the feature level, allowing the model to learn the knowledge of the unseen classes. However, since the generative model is trained on the training dataset, the dependency on the dataset still persists. \nIn conclusion, both zero-shot learning methods are still fundamentally constrained by the prior knowledge of the training dataset and may not perform well on unseen classes in completely different domains.\n \nIn this study, we address the challenge of dataset lock-in, which is being constrained by the dataset, in a different way. While an ideal solution would be creating a large dataset that contains all the necessary knowledge, constructing such a dataset and training a model on it may not be practical due to its massive learning capacity requirement. \nPrevious attempts at tackling such challenges focused on utilizing prior knowledge to deal with new classes, which does not fundamentally solve the underlying problem of being locked in the dataset. \nAs an alternative, we attempt to mitigate the dataset constraints by generating on-demand datasets based on user requirements. In this light, we propose , a system that dynamically formulates customized datasets for users by leveraging text-to-image generation techniques.\n\n\n\n\nFigure\u00a0<ref> highlights the key structural difference between and typical zero-shot learning approaches, where the main task is to classify unseen classes.\n\n\n  \n  * The ideal solution in (a) is that all the data and labels required to train a classification model are already present in the dataset. In other words, unseen data does not actually exist, which eliminates the need for zero-shot learning techniques (ideal but not realistic).\n  \n  \n  * Embedding-based zero-shot learning\u00a0<cit.> in (b) utilizes a semantic embedding model that learns the seen data and converts semantic attributes into feature vectors. This approach allows the visual feature of the image and the semantic feature of the class to be projected to the same latent space. Then, it recognizes the unseen class based on the similarity between the feature vectors of seen and unseen data.\n  \n  \n  * Generative zero-shot learning\u00a0<cit.> in (c) generates latent feature vectors for unseen labels. Firstly, the feature extractor converts the pairs of image-label (i.e., training dataset) into the pairs of feature-label (i.e., feature vectors). The feature generation model is then trained using these feature vectors to generate additional feature vectors for the unseen labels. These converted and generated feature vectors are integrated to train the classification model.\n  \n  \n  * As in (d) for , it dynamically formulates an on-demand dataset by directly generating image data from the unseen labels without relying on the training dataset and incorporating them with the existing dataset. From the model's perspective, the previous unseen classes become seen classes as the unseen data are now included in the dataset.\n  \n  This approach differs from generative zero-shot learning (c) in the way of handling unseen classes. First, the data generation model in (i.e., image generation model) is independent of the training dataset (i.e., seen classes), while the one in the generative zero-shot learning relies on the dataset. Second, handles the unseen classes at the data level (i.e., image-label), while zero-shot learning handles them mostly in the feature level (i.e., feature-label). These fundamental differences mitigate the dataset constraints and enable to create new classes, even if they were not in the original dataset.\n\n\nTo evaluate the feasibility of , we compare the performance of the model trained on the dataset with the one trained on the conventional dataset. \nWe followed a common training hyperparameter setting in the experiment, which is likely already tuned to the conventional datasets.\nSince -generated images and real images may differ in various perspectives, there could be other optimization settings for . However, hyperparameter optimization was out of our scope in this study.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\u00a7 RELATED WORK\n\n\ncan be viewed as an advanced approach that addresses multiple research problems, including data augmentation/generation and zero-shot learning for unseen classes. We discuss the previous angles and approaches taken toward each problem, and position among them.\n\n\n\n \u00a7.\u00a7 Data augmentation and generation\n\n\n\n\n\n\nResearch on data augmentation has become a critical aspect in deep learning field, since deep neural networks heavily rely on the size of training data. \nData augmentation aims to artificially inflate the size of a dataset while keeping the labels fixed, spanning from common methods like rotating, flipping, and cropping, to more advanced methods like mixup\u00a0<cit.>, cutmix\u00a0<cit.>,  UniformAugment\u00a0<cit.> and TrivialAugment\u00a0<cit.>. Nevertheless, such augmentation techniques require a minimum amount of data to be performed, which is dependent on task and domain.\n\n\n\n\n\n\n\nOn the other hand, recent advances in image generation have been gaining attention as a breakthrough for the long-standing problem of insufficient training data. Ravuri and Vinyals\u00a0<cit.> implement BigGAN\u00a0<cit.> as a large scale, high-fidelity image generative model. Their conclusion is that the mere addition of generated samples results in a lengthy training time with a minor improvement. Several attempts have been introduced to overcome such performance issues in investigating learning strategies\u00a0<cit.> or generative models\u00a0<cit.>. These efforts aimed at training a model with generated images, assuming that a gap between real and generated images is the reason for the poor performance.\n\n\n\nMany attempts implemented state-of-the-art image generation models to reduce this gap by improving the quality of generated images. In contrast, takes a different approach by reducing the gap in the feature space, which is inserting pixel noise to the generated images. While this may seem to manipulate the images more unrealistic, note that the key idea is to reduce the distance between the real and generated images in the feature space.\n\n\n\n\n\n\n\n \u00a7.\u00a7 Zero-shot learning for unseen label\n\n\nZero-shot learning\u00a0<cit.> refers to the capability of a model to classify objects that were not presented in the training dataset by transferring knowledge from seen classes to unseen classes.\n\n\nA number of proposed approaches enabling models to recognize unseen classes without prior knowledge of and access to unseen data. These can be broadly categorized into two approaches, embedding-based zero-shot learning and generative zero-shot learning.\n\n\n\nEmbedding-based zero-shot learning\u00a0<cit.> firstly trains a model to embed semantic attributes of images, and maps them to classes in a latent space to find class representation.\nBy this means, it is possible to recognize unseen classes based on the similarity between the representation of the image to be recognized and those of the class.\nThe advantage of this approach is that it is easy to implement and relatively requires low computing resources. \nHowever, in generalized zero-shot learning\u00a0<cit.> (i.e., recognizing not only unseen but also seen), \nthe model has a risk to be biased when the images of unseen classes are insufficient, and eventually get tendency to recognize only the seen classes.\n\n\nGenerative zero-shot learning emerged with the development of deep learning-based generation models. The idea is to convert zero-shot learning into supervised learning by generating images\u00a0<cit.> or features\u00a0<cit.>.\nHowever, due to the unpleasant performance of existing image generation models, most of the studies are directed toward to generating features rather than images themselves. This feature generation approach overcomes the bias problem that occured in embedding-based methods, as it can generate a large number of samples for unseen classes\u00a0<cit.>. However, there is a potential generalization problem on unseen classes because the generative model is only trained on seen data. \n\n\n\n\n\nIn other words, zero-shot learning methods that attempt to overcome the drawback of supervised learning (i.e., dataset lock-in) also fundamentally suffer from the same drawback.\nIn contrast, directly formulates datasets that are independent of existing datasets and aligned with the user's requirements, which fundamentally solves the problem of dataset lock-in. Furthermore, has the potential to become the foundational approach for ideal supervised learning that is free from the data constraints.\n\n\n\u00a7 DESIGN\n \n\nIn this section, we present an overview of the design process behind , which aims to dynamically formulate on-demand dataset based on the user request, namely dynamic data formulation. consists of three main components, including prompt generator, text-to-image generator, and image post-processor.\nFigure\u00a0<ref> illustrates the overall process of dynamic data formation in . First, the prompt generator receives a label from the user and generates a prompt.\nSecond, a set of images is generated by the text-to-image (denoted txt2img, hereafter) generator based on the given prompt.\nFinally, the image post-processor reduces the invisible gap that may exist between the generated and the real images, although the generated images seem realistic to the human eye.\nThe output images generated by are then used as input to train the models.\nWe name this newly generated dataset as dataset. \n\n\n\n\n\n\n\n\nTo begin the design process, we carefully selected a suitable dataset that would meet our criteria for fine-grained classification with a sufficient number of classes and a relatively high image resolution.\nOur choice was Oxford-IIIT Pet dataset (denoted Oxford Pet, hereafter)\u00a0<cit.>, which includes 37 classes of pet images, to list a few, Bengal, Egyptian Mau, Pomeranian, etc.\nThe dataset contains images of varying resolutions, with an average of 430x390, and around 200 images included in each class.\nThen, we iteratively designed each component of based on the classification task on the Oxford Pet.\nTo make decisions in each step, we evaluated the performance of Swin-v2 (started from Imagenet-1k checkpoint), using a common training parameter setting to focus solely on the dataset itself rather than parameter optimization.\nThe hyperparameter setting is: learning rate=2e-5, optimizer=adam, loss=cross-entropy, batch size=50, without any additional lr-scheduling. We report the best accuracy within 50 epochs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Iteration 1: Initial design\n\n\nAs an initial step, we designed basic components of with minimal guidance in order to assess its feasibility. Since the primary goal of is to generate images, we incorporated Stable Diffusion-v2\u00a0<cit.>, a publicly available txt2img generation model. Subsequently, we initially designed with a prompt generator and an image post-processor on top of the txt2img generator.\n\n\n    \n  * Prompt generator: It generates simple prompts in a style of \u201ca photo of one {class} {context}\", where {class} represents the user-provided label and {context} is the word `pet', as the design process is based on the Oxford Pet. The reason we added the `pet' to the prompt is to avoid homophone errors in some classes (e.g., boxer).\n    \n  * Txt2Img generator: It uses Stable Diffusion-v2, a generalized pre-trained txt2img generation model released under an open-source license by Stability AI, taking the output of the prompt generator as input.\n    \n  * Image post-processor: It applies Gaussian blur with a kernel size of 5, which is a widely used technique for image noise reduction.\n\n\n\n\nUsing the images generated by the initial version of , we achieved a classification accuracy of 78.6% on the Oxford Pet, which seems a decent result for a first attempt, yet still identified rooms for improvement.\nTo improve the accuracy further, we examined the generated images. We observed that they appeared less diverse compared to those in the real dataset. For example, Figure\u00a0<ref>(a) shows a set of images of `yorkshire-terrier' class generated by the initial version of . These images have similar colors, backgrounds, and compositions. We concluded that increasing the diversity in the generated images is our next priority.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Iteration 2: Prompt generator\n\nIn the recent advances in image generation, we design to be flexible enough to accommodate a number of image generation models. To this end, we tried to keep the image generation module untouched so that it could be replaced with other models in the future. Instead, we focused on addressing the diversity issue at the prompt level. We developed the prompt generator to generate various prompts as inputs for the txt2img generator, aiming to finally generate diverse images while maintaining the flexibility of the image generation module. \nTo develop the prompt generator, we tested two different approaches: 1) generating prompts with pre-trained large language models (LLMs) and 2) extracting prompts from real images with pre-trained image caption LLMs. We performed the classification task with changing only the prompt generator to observe the suggested strategies respectively.\n\nPrompt generation: \nFor the generative language model, we chose ChatGPT\u00a0<cit.> which is a text-based interaction service developed on GPT-3.5 by OpenAI. To generate the prompts for each class, we sent queries to ChatGPT with the following input: \u201cCan you recommend 10 simple prompts for image creation? I want to generate photo-realistic class images with txt2img model\".\nWe observed that the ChatGPT-suggested prompts generated much more diverse images than the naive prompts.\n\nPrompt extraction:\nTo obtain diverse prompts by extracting them from real images, we adopted blip-v2 model (blip2-opt, pretrained-opt2.7b)\u00a0<cit.>, a recently developed high-performance image captioning model (hits 96.9% accuracy in img2txt captioning under the Flickr30K dataset)\nHowever, even with a good image captioning model, biased or low-quality source images would lead to insufficient quality of the captioned prompts for generating a good training dataset.\nTo avoid such an issue, we use Oxford Pet training dataset for captioning. Overall, blip-2 image captioning works well; however, the captioned description of the subject tends to be coarse compared to the labels in the dataset. For example, an image of `yorkshire-terrier' is captioned as an image of `dog'. Therefore, in reverse, we replace the word of coarse subjects with the labels of the dataset (e.g., replacing `dog' with `yorkshire-terrier'). Since we directly used Oxford Pet, this may represent the peak performance of image-captioning under optimal settings.\n\n\n\n\n\n\n\nFigure\u00a0<ref> shows sampled generated images using (a) the naive prompt in our initial design, (b) prompts suggested by ChatGPT, and (c) prompts extracted from the training dataset using blip-v2. We observed that ChatGPT-suggested prompts and blip-v2 based extracted prompts generate more diverse images than the naive prompts. As for the image captioning in (c), we provide examples of the source images in the training dataset and the generated images of captioned prompts from the source images in our supplementary.\n\nTable\u00a0<ref> presents the results of each method. ChatGPT-based prompt generation achieved 85.4%, while blip-v2 based prompt extraction achieved 85.8% accuracy. Both approaches demonstrate a significant improvement compared to the naive prompt generator.\nThis indicates that the quality of prompts is heavily related to the quality of dataset, followed by the model performance.\nAlthough there is no significant difference between the two approaches, they have different practical implications.\nThe prompt extraction requires source images to be captioned while the prompt generator does not. \nIn our design study, we used Oxford Pet as the source images, since it was the dataset we tested.\nHowever, using training set of a specific dataset as the source images and captioning them as prompts are difficult to generalize.\nIn contrast, ChatGPT-based prompt generation is easy to incorporate into the pipeline, as the output from ChatGPT can be directly used as a prompt.\nWith these considerations, we decided to use the ChatGPT-based prompt generator for .\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Iteration 3: Image post-processor\n\nDespite generating more diverse images, the accuracy with was still lower than that of real image dataset.\nPrevious research discussed in 2.1 investigated such problems and concluded that there is a gap between the real images and generated ones. \nThey addressed it by increasing the image quality closer to real images. However, we approached the problem differently: reducing the distance in the feature space.\nWe hypothesized that adding noises to the generated images could smooth out the specific features of the generated images.\nWe experimented with several noise functions on the model with ChatGPT-based prompts (from the 3.1) as the base (i.e., the base accuracy: 85.4%). Table\u00a0<ref> (left) presents the accuracy of the model with each noise function. While the salt noise performed the best with 86.4% (the increase of 0.7% from the base), we also observed that most of the functions show similar results (M=84.79, SD=0.95). Since the experiment was conducted on Oxford Pet under a specific training condition, there is a possibility that the performance of the noise functions may significantly vary depending on the task or model architecture.\n\n\n\n\n\n\nWe also hypothesized that applying post-processing to real images in the test dataset could smooth out their specific features. We expected such an attempt would reduce the gap between the generated and the real images in the feature space, leading to a better classification performance. To test our assumption, we applied the same post-processing to the test dataset during the inference. Contrary to our expectations, the result was slightly less than that of applying to the training dataset only (M=82.65, SD=1.14), as shown in Table\u00a0<ref> (right).\nOur interpretation is that the generated images have specific features that differ from those of real images, and applying post-processing smooths out such differences. However, applying post-processing to real images acts as noise and reduces the performance of the model. \nTherefore, we conclude to apply post-processing only to the generated images in the training dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Dynamic dataset formulation\n\nWe built the first prototype of by implementing the prompt generator and the image post-processor as we designed. To evaluate the feasibility of dynamically formulating on-demand datasets, we conducted small-scale Proof of Concept (PoC) experiment. Thorough experiments on other datasets are discussed in the next section.\n\nFor PoC experiment, we conducted 101 rounds of tests. In each round, we created a test dataset randomly selected and combined 10 classes from each of the Oxford Pet and Indian Food, resulting in a total of 20 classes.\nWe ensured the number of data for each class was balanced by randomly sampling images with the minimum number of data in a class.\nAs for the training dataset, based on the selected labels, we generated 10 different prompts for each class using ChatGPT, followed by generating 18 images from each prompt. In total, 180 images for each class were generated as the training set.\nOverall, we achieved an average accuracy of 88.4% with a SD of 3.25 across the 101 rounds. \nThe result demonstrated the potential of for dynamic dataset formulation.\nNote that has, technically, the capability to generate an unlimited amount of data. \n\n\n\n\n\n\n\n\n\u00a7 TECHNICAL PROBE\n\n\n\n\n\n \u00a7.\u00a7 Evaluation\n\nWe conducted evaluations on a number of tasks to examine the quality of dataset. We focused on two main aspects: accuracy and diversity. We measured the accuracy of various models trained on dataset under different datasets. As for diversity, we compared the variance of structural similarity index (SSIM) between real datasets and those generated by .\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Accuracy\n\n\n\n\n\nIn our design process in 3, we conducted several experiments using a fixed dataset (i.e., Oxford Pet) and model structure (i.e., Swin-v2) to verify the feasibility of . To evaluate the final design of , we measured the accuracy with various datasets and models. \nWe used four datasets in our evaluation, including Oxford Pet\u00a0<cit.>, Caltech\u00a0<cit.>, Indian Food\u00a0<cit.>, and CIFAR-100\u00a0<cit.>.\nWe used all classes of the datasets except for the `Face-easy' class (a cropped version of `Face' class) of Caltech. We omitted it due to its loose connection between the label and the photo, resulting in the difficulty of generating proper image with . \nAs for the model structures, we used five models, including ResNet-152\u00a0<cit.>, ResNeXt101\u00a0<cit.>, ViT-b-16\u00a0<cit.>, Swin-v2\u00a0<cit.>, and RegNet\u00a0<cit.>. \nWe applied the hyperparameters globally as described in 3. \nIn the experiments, we generated dataset in 768x768 resolution, which appeared to be the optimal resolution for Stable Diffusion-v2. We resized the images to 224x224 resolution before feeding them to the models, and began the training from the Imagenet-1k checkpoint. Table <ref> shows the results.\n\n\n\nFocusing on the accuracy of the models for each dataset, we found that performs well across various model structures without tuning the parameters, not just on the model we used for our design (i.e., Swin-v2).\nThe top-1 best accuracy also did not show significant variance across models for each dataset, including Oxford Pet (M=83.76, SD=1.54, Caltech (M=86.72, SD=1.52), Indian Food (M=70.6, SD=2.14), and CIFAR-100 (M=63.88, SD=2.24). \nThe results demonstrate the feasibility of for various model structures. \nHowever, we observed that the accuracy of the model for CIFAR-100 tends to be lower than that of the others, possibly due to the low resolution of CIFAR-100 image (32x32). \nTo equalize the resolution of the images as model inputs, we stretched the images from (32x32) to (224x224). At the same time, we reduced the resolution of the other images to (224x224) due to their larger original resolution. We speculate that this process have affected the low accuracy of CIFAR-100.\n\n\n\nIn terms of the impact of post-processing, the results indicate that the post-processing led to an average improvement of 1.29% in top-1 accuracy across 20 results, and up to 4.3% increase with ResNet-152 under Oxford Pet. The findings demonstrate that the image post-processing of improves the model accuracy across different structures and datasets. However, our experiments did not provide clear guidelines for model- or dataset-specific post-processing techniques, and in four out of 20 results, the performance without post-processing was the best. \nInvestigating the relationship between the performance and various post-processing techniques to find optimal post-processing for each model and dataset remains for our future work.\n\n\n\nThis experiment confirms the feasibility of , achieving classification accuracies up to 91.4% (using ResNeXt101 on Caltech). Interestingly, we found that ResNeXt101 outperformed Swin-v2 by an average of 0.9% across datasets, though the was initially designed with the Swin-v2. However, as the accuracy of the model depends on the hyperparameter setting, further research is necessary to explore the full potential of . Note that parameter optimization for each model was out of the scope.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Diversity\n\n\n\n\nIn this section, we examine the diversity between the real dataset and the dataset from two perspectives: 1) the impact of different prompt generators on reproducing single dataset (Oxford Pet), 2) the impact of 's prompt generator on reproducing additional datasets.\n\n\n\n\nSpecifically, we verify whether the ChatGPT-based prompt generator of help generates a more diverse dataset in general cases. In the first perspective, we reproduced Oxford Pet into two different datasets - one by a prompt generator using fixed prompt (denoted as -Naive), and the other by a ChatGPT-based prompt generator (denoted as -ChatGPT). These datasets were then compared with the original dataset to measure the effects of the generators respectively. Furthermore, in the second perspective, we only use -ChatGPT to reproduce additional datasets, then compared them with the real ones to evaluate how generally diverse the datasets are. The average SSIM score is used as the measurement standard in every cases. SSIM\u00a0<cit.> is a well-known method to assess the structural similarity between two images. We assumed that the lower average SSIM score over all pairs in a class indicates that the images in the class are composed of images with more different structures (i.e., diverse). \n\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent prompt generators on reproducing single dataset:\n\n\nFigure\u00a0<ref> presents SSIM scores of images generated under classes of Oxford Pet, each based on the prompts of -Naive and -ChatGPT. The results show that the dataset with -ChatGPT looks more similar to the real ones than those with -Naive. \nThe mean and variance of SSIM scores for each class are as follows: -Naive (M=0.18, SD=0.06), Real (M=0.15, SD=0.03), and -ChatGPT (M=0.12, SD=0.03). Dataset with -ChatGPT tends to have lower SSIM scores than the real dataset, probably because the increased prompt complexity generated more images drawn in unexpected compositions. \n\n\n\n\n\n\n-ChatGPT on reproducing additional datasets:\n\nWe reproduced three additional datasets (Indian Food, CIFAR-100, and Caltech) with -ChatGPT, then compared SSIM scores with the real ones. Table\u00a0<ref> shows the results of all our runs. In the case of Indian Food, generated images with -ChatGPT obtained SSIM score of (M=0.08, SD=0.01), whereas the real dataset's SSIM score is in distribution of (M=0.11,SD=0.03).\nA larger difference in SSIM scores between the real and the generated datasets was shown in the case of Caltech, with -ChatGPT's SSIM scores of (M=0.14, SD=0.08) and real dataset (M=0.20, SD=0.08).\nThis might be due to the presence of confusing labels in Caltech that affects the prompt generator. For example, we asked ChatGPT to suggest a prompt with the word 'airplanes' (a class in Caltech), but it rather gave an answer including 'helicopter'. We assume that ChatGPT inadvertently judged helicopter as a kind of airplane.\nLastly, SSIM score of -ChatGPT in CIFAR-100 classes achieved (M=0.10, SD=0.05), while the real presenting (M=0.40, SD=0.06). \nHere, the resolution of the actual image is inherently low (32x32), which led to an underestimation in terms of structural similarity.\n\n\n\n\n\n\n\n\n\n  \u00a7.\u00a7.\u00a7 Summary of findings\n \n\n\n(1) A dataset generated by is useful to train various models on different datasets, however, the accuracy is slightly lower compared to the models trained on real images; (2) The diversity of images generated by is strongly correlated with the quality of the prompts; (3) While the model trained on dataset performs well for common resolution images, we found that it has limited performance in low resolution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Gap between the real and generated images\n\nWe further investigate the reason for the differences in model accuracy between the real and the datasets. Our initial hypothesis was the presence of an invisible gap between the real and generated images. \nTo verify this, we performed a binary classification to distinguish the real and the generated images using several model structures with similar parameter sizes (ResNeXt101: 83.5M, ViT-b-16: 86.6M, Swin-v2: 87.9M). We tested on different datasets used in 4.1 without image post-processing.\n\n\n\n\n\n\nTable\u00a0<ref> shows the binary classification results for real and generated images across various models and datasets. \nWe observed that the real and generated images were well distinguished regardless of the model structures, indicating the presence of invisible differences between them. \nResNeXt101 and Swin-v2 in Caltech show the largest difference, implying that Swin-v2 performs better at identifying the generated and real images than ResNeXt101. \nTo investigate the impact of such a difference on the model performance, we evaluated the performance of each model on both real and datasets under the same settings. \nResNeXt101 achieved 89.6% and Swin-v2 achieved 86.4% of accuracy (see Table\u00a0<ref>). In comparison, the accuracy for the real dataset was 96.5% for ResNeXt101 and 94.8% for Swin-v2. The performance difference was 6.9% for ResNeXt101 and 8.4% for Swin-v2. \nAs Swin-v2 outperforms in distinguishing between real and generated images, we think this contributes to the larger performance difference between real and datasets.\n\nThis finding also explains why the post-processor was effective in improving the model performance, which reduces the gap between real and generated images.\nMoreover, although the generated images exhibit clear differences from real images, the model performance on dataset still achieved reasonable accuracy. This is probably because the model was initially trained using a pre-trained ImageNet-1k checkpoint, which already has knowledge of real images, compensating for the difference between the real and generated images in the model performance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a7 DISCUSSION\n\n\n\n \u00a7.\u00a7 Limitation\n\n\nLocking in generalized pre-trained models: addresses the dataset lock-in of traditional machine learning and mitigates the problem by utilizing generalized pre-trained models (i.e., Stable Diffusion and ChatGPT). However, it is still constrained by the knowledge boundaries of these models, which is unable to generate data beyond their knowledge. \nAs such models continue to learn and expand their knowledge boundaries, can generate an unlimited amount of data within their knowledge range, providing the necessary data for personalized model training.\n\n\nOptimization for generated images:\nIn 2.1, we highlighted that previous research discussed an invisible gap between real and generated images\u00a0<cit.>. \nAs we mentioned in 4.2, we achieved 98.5% accuracy in detecting real and generated images under CIFAR-100. It indicates that there are obvious differences between the generated and the real images.  \n\n\n\nWe conducted experiments in 4.1 and found that the model trained on real images consistently outperformed those trained on generated images.\nHowever, the hyperparameters may not be optimal for dataset. Given the difference between real and generated images, there is room for improvement in hyperparameter optimization for generated images, as well as in generating better prompts and image post-processing.\n\n\n\n\n \u00a7.\u00a7 Fueling AI adoption\n\n\nData is often referred to as the fuel\u00a0<cit.> that drives model training and boosts performance without directly impacting the model architecture. We discuss the potential of as the fuel for future adoption, highlighting its advantages in terms of high versatility and scalability.\n\nWithin-domain versatility:\nLack of data often discourages thorough experiments in certain domains. For example, in the case of human detection, deep learning model may detect humans accurately when they are facing forward or sideways, but perform poorly in angled photos taken from the ceiling\u00a0<cit.>. This is because there are simply insufficient data consisting of ceiling-perspective photos. addresses such issues straightforwardly without requiring additional manipulation or optimization of model structures, which ultimately facilitates practical utilization.\n\n\n\n\n\n\nScalability to other domains: \nalso has an advantage in facilitating the adoption of AI in other domains where data collection is challenging.\nSeveral domains in our lives suffer from data depletion due to the difficulties in collecting data, such as in a disaster or medical emergency. \novercomes limited availability of data in such domains by generating synthetic data to train AI models for any situation in any domain. \nImagine an earthquake where a building collapses under a huge fire breakout, and using traditional methods (e.g., infrared sensors) for survivor search impossible.\nIn this case, a camera with a built-in detection system pre-trained with the dataset under earthquake scenarios can be served as a viable substitution.\nWhile taking a disaster as an initial example, we believe that the capability of (i.e., fueling data) opens up new opportunities for the adoption of AI in previously inaccessible areas.\n\n\n\n\n\n\n\n\n\n\n\n\n \u00a7.\u00a7 Intensifying user customization\n\n\n\n\n\n\n\n\n\n\n\n\n\nis designed to formulate on-demand dataset, in which the `customization' is the underlying pursuit. We discuss design flexibility and task extensibility, which have the potential to further enhance the level of customization.\n\nDesign flexibility:\nWe designed each component of to be modular and independent of the others so that they can be easily replaced by other models as needed. In particular, currently uses Stable Diffusion, which is one of the generalized pre-trained models. Although such models are referred to as \u201cgeneralized\u201d, they are not always suitable for all use cases. However, there are opportunities to improve task performance by replacing them with more specialized models. For example in the classification of specific painting styles, replacing the Stable Diffusion with a model primarily trained for painting (e.g., Novel AI\u00a0<cit.>) could potentially enhance the quality of the generated images. The flexibility in our design helps create a custom dataset that better meets user requirements and improves performance.\n\n\n\nTask extensibility:\ncan expand the scope of the task by adding a few more components to its design beyond replacing them. Figure\u00a0<ref> shows heatmaps of the parts of the generated images that correspond to given labels. In this manner, it is possible to create a dataset for image recognition or image segmentation tasks by providing the bounding box of a given label or its pixel coordinates along with the entire image. This functionality allows users to create a custom dataset tailored to specific tasks and requirements.\n\n\n\n\nchatGPT, Stable Diffusion\uc744 \uc608\uc2dc\ub85c \ub4e4\uba74\uc11c \uac1c\uc778\uc774 AI\uc744 \ub9cc\ub4dc\ub294 \uc0ac\ud68c\uac00 \ub2e4\uac00\uc628\ub2e4\ub294 \uc880 \uc774\uc0c1\ud55c\ub4ef \ud568.. \ub9c8\uce58 \ub098\uc0ac\uc5d0\uc11c \uc6b0\uc8fc\uc120\uc744 \uc3d8\uc544\uc62c\ub9b0\ub2e4\uace0 \uac1c\uc778\uc774 \uc6b0\uc8fc\uc120\uc744 \ub9cc\ub4e4\uc5b4 \uc6b0\uc8fc\uc5ec\ud589\uc744 \ud558\ub294 \uc0ac\ud68c\uac00 \ub2e4\uac00\uc628\ub2e4 \uadf8\ub7f0 \ub290\ub08c.. \u2013> \uac1c\uc778\uc774 \ub9cc\ub4dc\ub294 customization\ub9d0\uace0 \uac1c\uc778\uc744 \uc704\ud55c customization\uc73c\ub85c \uac00\ub294\uac74 \uc5b4\ub5a8\uae4c \uc2f6\uc74c\nAs the public is easily exposed to AI models due to the great advance of the latest AI technology such as ChatGPT\u00a0<cit.> and Stable diffusion models\u00a0<cit.>, the society in which individuals model AI has become closer.\nHowever, a gap between the dataset and the actual task makes it difficult to become an AI ubiquitous \ub72c\uae08\uc5c6\uc74c\"AI ubiquitous\" society.\nThis gap occurs because it is difficult to configure the dataset that precisely fits the problem situation the user wants to solve.\nAI\uc5d0 \uc775\uc219\ud558\uc9c0 \uc54a\uc740 \uc0ac\ub78c\ub4e4\uc740 \ub370\uc774\ud130 \uc218\uc9d1\uc774 \ubb38\uc81c\uac00 \uc544\ub2c8\ub77c \uadf8\ub0e5 \ubaa8\ub378\uc744 \ubabb\ub9cc\ub4e4\uc9c0\uc54a\uc744\uae4c?.. \ub370\uc774\ud130 \uc218\uc9d1\uc744 \uc624\ud788\ub824 \uc798\ud560\ub4ef\nEspecially for ordinary individuals who are not familiar with AI, collecting a well-suited dataset for AI modeling is a considerable challenge.\ncircumstance-fit, task-fit\ub3c4 \uac11\uc790\uae30 \ub098\uc628\ub290\ub08c?.. \nThe circumstance-fit and task-fit characteristics of the will ease such adversity.\n\nThink of a situation where user wants to build a deep learning model that can identify aggressive behaviors of his dog, Max. What user needs are the photos of a specific dog acting unusual. \nTo achieve this, the existing txt2img generator can be replaced with Dreambooth<cit.>, which generates fine-tuned outputs based on the input object. Taking a step further, can be updated to reflect changes in the task at hand. If one wants to determine whether Max is being aggressive while playing with other dogs, the scope of the task is expanded from simple classification to object detection(detecting Max in a pack). In this case, can be utilized to draw pixel coordinates of Max in images, thereby creating a dataset suitable for segmentation like Figure 6. \n\uc774\ub7ec\uc774\ub7ec \ud560\ub54c\ub294 figure 6\ucc98\ub7fc \ud560\uc218\uc788\uc5b4! \ubcf4\ub2e4\ub294 NUO\ub294 figure 6\uacfc\uac19\uc740 \uae30\ub2a5\ub3c4 \ud560 \uc218 \uc788\uc73c\ub2c8 \uc774\ub7ec\uc774\ub7ec\ud55c \uc0c1\ud669\uc5d0\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc744\uac70\uc57c \uc774\ub7f0\ub290\ub08c\uc774 \ub098\uc744\uac83 \uac19\uc74c \uac11\uc790\uae30 figure 6 \ub098\uc624\ub2c8\uae4c \uc774\uc0c1\ud568 \uc544\ubb34\ub798\ub3c4 \uadf8\ub807\uc9c0...\n\nWith 's significant flexibility to construct datasets customized for user's needs, we envision a genuine \"AI ubiquitous\" society where individuals can design their own AI service.\n\n\n\n\ud615\uc11d\n\ud0c8\ubd80\ucc29\uc774 \uc26c\uc6b4 \uc11c\ube44\uc2a4\n\n\u2013 5.3\uc5d0\uc11c \uc8fc\uc694 \ub17c\uc810\uc740 \uc720\uc800\uac00 \uc785\ub9db\uc5d0 \ub9de\uac8c \ub370\uc774\ud130\uc14b\uc744 control\ud55c\ub2e4\ub294 \uc810\n \n//\ucd08\ubc18  40\n-> \ubb38\uc81c \uc0c1\ud669 \uc81c\uc2dc(\uad34\ub9ac) \uadf8\ub7ec\ub098 nuo \ucd5c\uace0\n\n\n//\uc911\ubc18 50 \n\n\ntask\uc758 \ubc94\uc704\uac00 \ub2e8\uc21c classification(\uacf5\uaca9\uc801\uc778 \ud589\uc704\ub97c \ud558\uace0 \uc788\ub294\uc9c0 \uc544\ub2cc\uc9c0\ub97c \ud310\ubcc4)\uc5d0\uc11c \ubb34\ub9ac\ub4e4 \uc0ac\uc774\uc5d0\uc11c Max\ub97c \uac80\ucd9c\ud574\ub0b4\ub294 object detection\ub85c \ud655\uc7a5\ub418\ub294 \uac83\uc774\ub77c\uace0 \ud560 \uc218 \uc788\ub2e4. \uc774\ub54c Dreambooth\uac00 bounding box\uae4c\uc9c0 \uadf8\ub824\ub0bc \uc218 \uc788\ub3c4\ub85d \ud574 bounding box prediction\uc5d0 \ubc14\ub85c \uc4f8 \uc218 \uc788\ub294 \ub370\uc774\ud130\uc14b\uc744 \ud68d\ub4dd\ud560 \uc218 \uc788\ub2e4.\n\n\n\n//\ud6c4\ubc18 10\n-> \ubd10 nuo \uc9f1\uc774\uc9c0\n\uc774\ub807\uac8c \ub370\uc774\ud130\uc14b\uc5d0 Control ability\ub97c \ubd80\uc5ec\ud558\ub294 NUO\ub9cc\uc758 \uc7a5\uc810\uc744 \ud1b5\ud574, \uac1c\uc778\uc774 \uc9c1\uc811 AI \ubaa8\ub378 \ub514\uc790\uc778\uc744 \ud558\ub294 \ub0a0\uc744 \uc55e\ub2f9\uae38 \uc218 \uc788\uc744\uac83\uc774\ub2e4.\n\n\n\n\n\n\n\n\n\u00a7 CONCLUSION\n\nWe address the problem of dataset lock-in by dynamically formulating on-demand dataset. We propose , a system that consists of 3 main modules: a prompt generator, a txt2img generator, and an image post-processor. We evaluate in terms of accuracy and diversity. Our results demonstrate the potential of to enable models to learn new knowledge beyond the training dataset. We believe represents a step forward from the zero-shot learning and has the potential to change the way we approach deep learning problems.\n\n\nieee_fullname\n\n\n\n[pages=1, pagecommand=]pdfs/_Arxiv__ODIN__Supplemental_.pdf\n[pages=2, pagecommand=]pdfs/_Arxiv__ODIN__Supplemental_.pdf\n[pages=3, pagecommand=]pdfs/_Arxiv__ODIN__Supplemental_.pdf\n[pages=4, pagecommand=]pdfs/_Arxiv__ODIN__Supplemental_.pdf\n[pages=5, pagecommand=]pdfs/_Arxiv__ODIN__Supplemental_.pdf\n\n\n"}